2025-09-23 19:00:32,487 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-23 19:00:32,596 - INFO - __main__ - Logging system initialized successfully
2025-09-23 19:00:32,596 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-23 19:00:32,596 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-23 19:00:32,596 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-23 19:00:32,596 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-23 19:00:32,596 - INFO - __main__ - Attempting to load: X.npy
2025-09-23 19:00:32,637 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-23 19:00:32,674 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-23 19:00:32,674 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-23 19:00:32,674 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-23 19:00:32,674 - INFO - __main__ - Flow: Code Generation â†’ BO â†’ Evaluation
2025-09-23 19:00:32,676 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-23 19:00:32,676 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-23 19:00:32,676 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-23 19:00:32,676 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-23 19:00:32,676 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation â†’ JSON Storage â†’ BO â†’ Training Execution â†’ Evaluation
2025-09-23 19:00:32,676 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-23 19:00:32,676 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-23 19:00:32,676 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-23 19:00:32,676 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-23 19:00:32,775 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-23 19:00:32,776 - INFO - class_balancing - Class imbalance analysis:
2025-09-23 19:00:32,776 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-23 19:00:32,776 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-23 19:00:32,776 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-23 19:00:32,776 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-23 19:00:32,776 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-23 19:00:32,776 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-23 19:00:32,776 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-23 19:00:32,776 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-23 19:00:32,947 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ¤– STEP 1: AI Training Code Generation
2025-09-23 19:00:32,948 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-23 19:03:55,882 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Prompt length: 3998 characters
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-23 19:09:42,551 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-23 19:09:42,561 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-23 19:09:42,561 - WARNING - _models.ai_code_generator - Initial JSON parse failed: Expecting ',' delimiter: line 21 column 17 (char 16862), attempting to fix common issues
2025-09-23 19:09:42,562 - INFO - _models.ai_code_generator - Successfully fixed JSON formatting issues
2025-09-23 19:09:42,562 - INFO - _models.ai_code_generator - AI generated training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:09:42,562 - INFO - _models.ai_code_generator - Confidence: 0.90
2025-09-23 19:09:42,562 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-09-23 19:09:42,562 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:09:42,562 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'd_model', 'n_heads', 'num_layers', 'mlp_ratio', 'dropout', 'weight_decay', 'patch_size', 'label_smoothing', 'use_focal_loss', 'focal_gamma', 'grad_clip_norm', 'scheduler', 'class_weights', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-23 19:09:42,562 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.90
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ’¾ STEP 2: Save Training Function to JSON
2025-09-23 19:09:42,563 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_Tiny1D-ViT-SE-Transformer-MITBIH_1758672582.json
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_Tiny1D-ViT-SE-Transformer-MITBIH_1758672582.json
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ” STEP 3: Bayesian Optimization
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ“¦ Installing dependencies for GPT-generated training code...
2025-09-23 19:09:42,564 - INFO - package_installer - ğŸ” Analyzing GPT-generated code for package dependencies...
2025-09-23 19:09:42,566 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-23 19:09:42,567 - INFO - package_installer - Available packages: {'torch'}
2025-09-23 19:09:42,567 - INFO - package_installer - Missing packages: set()
2025-09-23 19:09:42,567 - INFO - package_installer - âœ… All required packages are already available
2025-09-23 19:09:42,567 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-09-23 19:09:42,567 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-23 19:09:42,567 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-23 19:09:42,567 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'd_model', 'n_heads', 'num_layers', 'mlp_ratio', 'dropout', 'weight_decay', 'patch_size', 'label_smoothing', 'use_focal_loss', 'focal_gamma', 'grad_clip_norm', 'scheduler', 'class_weights', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-23 19:09:42,567 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-23 19:09:42,567 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-23 19:09:42,567 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-23 19:09:42,602 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-23 19:09:42,735 - INFO - bo.run_bo - Converted GPT search space: 20 parameters
2025-09-23 19:09:42,735 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-23 19:09:42,735 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-23 19:09:42,736 - INFO - bo.run_bo - ğŸ”BO Trial 1: Initial random exploration
2025-09-23 19:09:42,736 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:09:42,736 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:09:42,736 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:09:42,736 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': 12, 'd_model': 92, 'n_heads': 3, 'num_layers': 2, 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': 39188, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-23 19:09:42,737 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': 12, 'd_model': 92, 'n_heads': 3, 'num_layers': 2, 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': 39188, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - Model: 10,856 parameters, 46.6KB storage
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.25489993971720476, 0.19067650278050832, 0.17220858917052453, 0.1592238353429326, 0.13939441082300041, 0.1419021197516895, 0.12744052242158002, 0.12366793638876132, 0.12623549760726707, 0.10909633015226468, 0.11421404015274607, 0.10665167654676813], 'val_losses': [0.21870903324514587, 0.17624613041399625, 0.15134156606704008, 0.1344638961919148, 0.13005695043183288, 0.12962597129917125, 0.113186163550749, 0.12376264798459724, 0.10881336247591637, 0.10537854445307733, 0.09983899139891014, 0.10109018617717003], 'val_acc': [0.7403834106001754, 0.8061646410224282, 0.8486405212379401, 0.8585390301967172, 0.8621726600676607, 0.8735747400075179, 0.8887357474000752, 0.8728229545169778, 0.8901140207993986, 0.8986342563588523, 0.8966294950507455, 0.8990101491041222], 'model_size_bytes': 199413, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': 12, 'd_model': 92, 'n_heads': 3, 'num_layers': 2, 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': 39188, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 10856, 'model_storage_size_kb': 46.646875, 'model_size_validation': 'PASS'}
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - BO Objective: base=0.8990, size_penalty=0.0000, final=0.8990
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - Model: 10,856 parameters, 46.6KB (PASS 256KB limit)
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 24.874s
2025-09-23 19:10:07,610 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8990
2025-09-23 19:10:07,610 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-23 19:10:07,610 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': np.int64(12), 'd_model': np.int64(92), 'n_heads': 3, 'num_layers': np.int64(2), 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': np.int64(39188), 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, value=0.8990
2025-09-23 19:10:07,610 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': np.int64(12), 'd_model': np.int64(92), 'n_heads': 3, 'num_layers': np.int64(2), 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': np.int64(39188), 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True} -> 0.8990
2025-09-23 19:10:07,611 - INFO - bo.run_bo - ğŸ”BO Trial 2: Initial random exploration
2025-09-23 19:10:07,611 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:10:07,611 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:10:07,611 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:10:07,611 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': 46, 'd_model': 91, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': 43021, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:10:07,612 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': 46, 'd_model': 91, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': 43021, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - Model: 20,293 parameters, 87.2KB storage
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.1500932466258225, 2.112802879174685, 2.0973590202649404, 2.0528743468934283, 2.0106371819097304, 1.9918745874451151, 1.984209218725907, 1.9709511053195086, 1.9556792418401021, 1.9403461090578784, 1.9300434519032228, 1.9193303539270268, 1.9192023465116792, 1.9031998253412363, 1.8967574969294883, 1.8873678382706778, 1.8853790417495155, 1.8813880054595884, 1.8779873931982634, 1.8740331361613671, 1.8628751037600524, 1.8585746137136072, 1.8497283426594944, 1.854211062864153, 1.8471998022944207, 1.8466184386498117, 1.8376032263138566, 1.8377672057027368, 1.839443708397454, 1.826800062024339, 1.8281496797846108, 1.823567141787665, 1.8192270049451942, 1.8226270880820297, 1.8179370276638094, 1.8157651135225634, 1.8118047988030508, 1.8076471641197154, 1.8067987621217034, 1.7976774092706131, 1.8038279405868176, 1.7961453804750063, 1.7989135244841699, 1.79160667988597, 1.798327983756959, 1.7876412005209854], 'val_losses': [2.095532409918665, 2.069051769439655, 2.0258972375647732, 1.9879878547134562, 1.980980456375774, 1.972381000067115, 1.956298968887855, 1.9238942562077461, 1.9087476630868685, 1.893755261662938, 1.8862539637194826, 1.867320267154286, 1.8567437609400221, 1.8453974532029516, 1.8366496933087597, 1.8249355011335249, 1.8179281500852313, 1.8116441185858865, 1.8050985577381011, 1.7996862507033746, 1.7949920958287284, 1.7888045010747744, 1.7846533663364568, 1.7763447950633453, 1.768478030923046, 1.766736144903623, 1.7645365375128774, 1.7557303018997732, 1.757565603595586, 1.7508552980040595, 1.7510244126099421, 1.7477209429859801, 1.742604725821815, 1.7363454076135805, 1.7450731862922193, 1.73276772049193, 1.730522380722389, 1.727269389160055, 1.724862581147181, 1.724137035143314, 1.7215917497451347, 1.717771629016959, 1.7085448488408261, 1.7128316829294479, 1.7052855918527232, 1.7058867993263385], 'val_acc': [0.10349580253101115, 0.08181932088710688, 0.09986217266006767, 0.1055005638391179, 0.10161633880466107, 0.1032452073674978, 0.1032452073674978, 0.1086330033830347, 0.10499937351209121, 0.10662824207492795, 0.11013657436411477, 0.10951008645533142, 0.1252975817566721, 0.12680115273775217, 0.13030948502693898, 0.14208745771206616, 0.16551810550056384, 0.15912792883097357, 0.14985590778097982, 0.15073299085327654, 0.16990352086204735, 0.15524370379651672, 0.17053000877083072, 0.18957524119784488, 0.17253477007893747, 0.192206490414735, 0.25297581756672094, 0.20448565342688887, 0.19295827590527503, 0.21024934218769578, 0.2181430898383661, 0.21212880591404587, 0.22165142212755293, 0.22754040847011653, 0.23330409723092344, 0.23505826337551686, 0.2240320761809297, 0.2118782107505325, 0.2411978448815938, 0.2554817692018544, 0.24633504573361734, 0.2698909911038717, 0.2361859416113269, 0.25209873449442427, 0.2557323643653677, 0.24270141586267385], 'model_size_bytes': 229237, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': 46, 'd_model': 91, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': 43021, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 20293, 'model_storage_size_kb': 87.19648437500001, 'model_size_validation': 'PASS'}
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - BO Objective: base=0.2427, size_penalty=0.0000, final=0.2427
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - Model: 20,293 parameters, 87.2KB (PASS 256KB limit)
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 30.548s
2025-09-23 19:10:38,159 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2427
2025-09-23 19:10:38,159 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-23 19:10:38,159 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': np.int64(46), 'd_model': np.int64(91), 'n_heads': 2, 'num_layers': np.int64(2), 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': np.int64(43021), 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, value=0.2427
2025-09-23 19:10:38,159 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': np.int64(46), 'd_model': np.int64(91), 'n_heads': 2, 'num_layers': np.int64(2), 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': np.int64(43021), 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True} -> 0.2427
2025-09-23 19:10:38,159 - INFO - bo.run_bo - ğŸ”BO Trial 3: Initial random exploration
2025-09-23 19:10:38,159 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:10:38,159 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:10:38,159 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:10:38,159 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': 11, 'd_model': 75, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': 37065, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:10:38,161 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': 11, 'd_model': 75, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': 37065, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - Model: 10,275 parameters, 44.2KB storage
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4018279313434318, 0.23614479681016642, 0.19631894869842548, 0.16708861271570197, 0.1472443863569792, 0.12918425497158098, 0.11135982659004746, 0.09408311714894849, 0.08428363786484071, 0.07432211902915276, 0.07300470031512447], 'val_losses': [0.2602476288506507, 0.16657838058354427, 0.16082730029343573, 0.1483654274737427, 0.11402542149287957, 0.10026475431204081, 0.0957042981833481, 0.08266359348120345, 0.0698509370599373, 0.06935254825763858, 0.06816789332267083], 'val_acc': [0.7893747650670342, 0.8602931963413106, 0.8759553940608946, 0.884225034456835, 0.915549429896003, 0.9196842500939731, 0.9233178799649167, 0.9360982333040972, 0.9485026938980078, 0.9473750156621977, 0.9483773963162511], 'model_size_bytes': 205881, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': 11, 'd_model': 75, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': 37065, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 10275, 'model_storage_size_kb': 44.15039062500001, 'model_size_validation': 'PASS'}
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - BO Objective: base=0.9484, size_penalty=0.0000, final=0.9484
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - Model: 10,275 parameters, 44.2KB (PASS 256KB limit)
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 14.959s
2025-09-23 19:10:53,203 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9484
2025-09-23 19:10:53,203 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-09-23 19:10:53,203 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': np.int64(11), 'd_model': np.int64(75), 'n_heads': 2, 'num_layers': np.int64(3), 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': np.int64(37065), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, value=0.9484
2025-09-23 19:10:53,203 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': np.int64(11), 'd_model': np.int64(75), 'n_heads': 2, 'num_layers': np.int64(3), 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': np.int64(37065), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False} -> 0.9484
2025-09-23 19:10:53,203 - INFO - bo.run_bo - ğŸ”BO Trial 4: Using RF surrogate + Expected Improvement
2025-09-23 19:10:53,203 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:10:53,203 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:10:53,203 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:10:53,203 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.842797040686455e-05, 'batch_size': 64, 'epochs': 35, 'd_model': 36, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': 100, 'label_smoothing': 0.04065048351171404, 'use_focal_loss': True, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 34569, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:10:53,204 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.842797040686455e-05, 'batch_size': 64, 'epochs': 35, 'd_model': 36, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': 100, 'label_smoothing': 0.04065048351171404, 'use_focal_loss': True, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 34569, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - Model: 46,321 parameters, 199.0KB storage
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.15767041525849573, 0.14616018050106266, 0.1389311932112995, 0.1333474838427089, 0.12963859027103572, 0.1252417524818101, 0.1215065193470157, 0.1193185193184731, 0.11662642792214077, 0.11290705654118621, 0.11056507623388337, 0.10704908485044473, 0.1034095215018081, 0.1022642092663808, 0.10037222819368563, 0.09870833570827119, 0.09567042898371662, 0.09546559434628586, 0.09299017942857687, 0.09258078671500534, 0.09045722395162051, 0.08824532288669722, 0.08699650520995211, 0.08537494567063983, 0.08436709075010394, 0.08327295449147662, 0.08190273695085908, 0.08266386755644721, 0.07967210366259567, 0.07892636184671677, 0.07929298308317176, 0.07824130221679482, 0.07677836464239822, 0.07556479323354408, 0.07567030588841822], 'val_losses': [0.14489994995700223, 0.1372329736017125, 0.1296484014529316, 0.1236207095768208, 0.11950206941666049, 0.11542725467174786, 0.11310341750672924, 0.11072972302005682, 0.10862877832784223, 0.10650023672197147, 0.10397165430301997, 0.10099560703978161, 0.09875633772063518, 0.09628058232151392, 0.09497356113028203, 0.093659577856923, 0.09215870446072397, 0.0906902638082777, 0.08925786165758835, 0.08759404240296742, 0.08547395726045284, 0.08420684236471604, 0.08323384095471113, 0.08098939327387049, 0.08055997367020822, 0.07787574320922347, 0.07680459707707461, 0.07642832227716873, 0.07430307711584871, 0.072861900637826, 0.07287679730565973, 0.07220860632842921, 0.07101894565003175, 0.06987220781966492, 0.0697316282709213], 'val_acc': [0.024057135697281042, 0.02806665831349455, 0.029570229294574615, 0.031324395439168026, 0.0323267760932214, 0.03295326400200476, 0.03320385916551811, 0.03658689387294825, 0.044480641523618594, 0.059015161007392555, 0.06227289813306603, 0.07179551434657311, 0.07730860794386668, 0.08269640395940359, 0.0875830096479138, 0.08958777095602055, 0.0943490790627741, 0.09497556697155744, 0.0969803282796642, 0.09998747024182433, 0.09948627991479765, 0.1006139581506077, 0.10211752913168776, 0.10274401704047112, 0.10424758802155118, 0.10186693396817441, 0.10487407593033454, 0.10499937351209121, 0.10449818318506453, 0.1055005638391179, 0.10662824207492795, 0.10362110011276783, 0.11176544292695151, 0.12604936724721214, 0.115399072797895], 'model_size_bytes': 204975, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.842797040686455e-05, 'batch_size': 64, 'epochs': 35, 'd_model': 36, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': 100, 'label_smoothing': 0.04065048351171404, 'use_focal_loss': True, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 34569, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 46321, 'model_storage_size_kb': 199.03554687500002, 'model_size_validation': 'PASS'}
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - BO Objective: base=0.1154, size_penalty=0.0000, final=0.1154
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - Model: 46,321 parameters, 199.0KB (PASS 256KB limit)
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 56.358s
2025-09-23 19:11:49,646 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1154
2025-09-23 19:11:49,646 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.085s
2025-09-23 19:11:49,646 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 1.842797040686455e-05, 'batch_size': np.int64(64), 'epochs': np.int64(35), 'd_model': np.int64(36), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': np.int64(100), 'label_smoothing': 0.04065048351171404, 'use_focal_loss': np.True_, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': np.int64(34569), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.1154
2025-09-23 19:11:49,646 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 1.842797040686455e-05, 'batch_size': np.int64(64), 'epochs': np.int64(35), 'd_model': np.int64(36), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': np.int64(100), 'label_smoothing': 0.04065048351171404, 'use_focal_loss': np.True_, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': np.int64(34569), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.1154
2025-09-23 19:11:49,646 - INFO - bo.run_bo - ğŸ”BO Trial 5: Using RF surrogate + Expected Improvement
2025-09-23 19:11:49,646 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:11:49,647 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:11:49,647 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:11:49,647 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 6.218414569281548e-05, 'batch_size': 32, 'epochs': 6, 'd_model': 60, 'n_heads': 3, 'num_layers': 1, 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': 100, 'label_smoothing': 0.16645169790002662, 'use_focal_loss': True, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 44158, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:11:49,648 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 6.218414569281548e-05, 'batch_size': 32, 'epochs': 6, 'd_model': 60, 'n_heads': 3, 'num_layers': 1, 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': 100, 'label_smoothing': 0.16645169790002662, 'use_focal_loss': True, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 44158, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:11:59,145 - INFO - _models.training_function_executor - Model: 13,140 parameters, 14.1KB storage
2025-09-23 19:11:59,146 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4184733569282221, 0.334049597420355, 0.3070350216934844, 0.2863255955977416, 0.2730059827656404, 0.26300052871038804], 'val_losses': [0.34017408296210083, 0.31310684578566755, 0.28217635433030985, 0.25060428626431125, 0.22825787404253137, 0.2178835974614836], 'val_acc': [0.7675729858413732, 0.7804786367623104, 0.7869941110136575, 0.8031574990602681, 0.823079814559579, 0.8341060017541662], 'model_size_bytes': 107569, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 6.218414569281548e-05, 'batch_size': 32, 'epochs': 6, 'd_model': 60, 'n_heads': 3, 'num_layers': 1, 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': 100, 'label_smoothing': 0.16645169790002662, 'use_focal_loss': True, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 44158, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 13140, 'model_storage_size_kb': 14.115234375000002, 'model_size_validation': 'PASS'}
2025-09-23 19:11:59,146 - INFO - _models.training_function_executor - BO Objective: base=0.8341, size_penalty=0.0000, final=0.8341
2025-09-23 19:11:59,146 - INFO - _models.training_function_executor - Model: 13,140 parameters, 14.1KB (PASS 256KB limit)
2025-09-23 19:11:59,146 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 9.499s
2025-09-23 19:11:59,230 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8341
2025-09-23 19:11:59,230 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-09-23 19:11:59,230 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 6.218414569281548e-05, 'batch_size': np.int64(32), 'epochs': np.int64(6), 'd_model': np.int64(60), 'n_heads': np.int64(3), 'num_layers': np.int64(1), 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': np.int64(100), 'label_smoothing': 0.16645169790002662, 'use_focal_loss': np.True_, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(44158), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.8341
2025-09-23 19:11:59,230 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 6.218414569281548e-05, 'batch_size': np.int64(32), 'epochs': np.int64(6), 'd_model': np.int64(60), 'n_heads': np.int64(3), 'num_layers': np.int64(1), 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': np.int64(100), 'label_smoothing': 0.16645169790002662, 'use_focal_loss': np.True_, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(44158), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.8341
2025-09-23 19:11:59,230 - INFO - bo.run_bo - ğŸ”BO Trial 6: Using RF surrogate + Expected Improvement
2025-09-23 19:11:59,230 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:11:59,230 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:11:59,230 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:11:59,230 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00022687383616628082, 'batch_size': 256, 'epochs': 15, 'd_model': 43, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': 50, 'label_smoothing': 0.13151121598021862, 'use_focal_loss': True, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42992, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:11:59,232 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00022687383616628082, 'batch_size': 256, 'epochs': 15, 'd_model': 43, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': 50, 'label_smoothing': 0.13151121598021862, 'use_focal_loss': True, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42992, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - Model: 5,891 parameters, 25.3KB storage
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8403377209253278, 0.6258667548083013, 0.4839695855959747, 0.3857827268561164, 0.33141840789299637, 0.2924884982782621, 0.2661674457498314, 0.2460892591710057, 0.22924609782757535, 0.22009057536801982, 0.20791382570437827, 0.20424145248955483, 0.1996998912715353, 0.19642767790370397, 0.19749426222639574], 'val_losses': [0.6704454867556914, 0.5590170607682144, 0.41072977507059144, 0.33516997183688135, 0.28310740360930187, 0.2431544594773821, 0.22633619276208547, 0.19714266706672084, 0.18249125922626905, 0.1765057689738653, 0.16564461565707772, 0.16262220376312486, 0.15773755699945055, 0.1576868868302827, 0.15757392703471157], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7675729858413732, 0.800150357098108, 0.8240821952136324, 0.8531512341811803, 0.8646786117027941, 0.8893622353088585, 0.8982583636135822, 0.9013908031574991, 0.909033955644656, 0.907906277408846, 0.9125422879338428, 0.9122916927703295, 0.9124169903520862], 'model_size_bytes': 118137, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00022687383616628082, 'batch_size': 256, 'epochs': 15, 'd_model': 43, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': 50, 'label_smoothing': 0.13151121598021862, 'use_focal_loss': True, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42992, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 5891, 'model_storage_size_kb': 25.312890625, 'model_size_validation': 'PASS'}
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - BO Objective: base=0.9124, size_penalty=0.0000, final=0.9124
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - Model: 5,891 parameters, 25.3KB (PASS 256KB limit)
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 9.893s
2025-09-23 19:12:09,208 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9124
2025-09-23 19:12:09,209 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.085s
2025-09-23 19:12:09,209 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 0.00022687383616628082, 'batch_size': np.int64(256), 'epochs': np.int64(15), 'd_model': np.int64(43), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.13151121598021862, 'use_focal_loss': np.True_, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(42992), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9124
2025-09-23 19:12:09,209 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 0.00022687383616628082, 'batch_size': np.int64(256), 'epochs': np.int64(15), 'd_model': np.int64(43), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.13151121598021862, 'use_focal_loss': np.True_, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(42992), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9124
2025-09-23 19:12:09,209 - INFO - bo.run_bo - ğŸ”BO Trial 7: Using RF surrogate + Expected Improvement
2025-09-23 19:12:09,209 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:12:09,209 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:12:09,209 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:12:09,209 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00015566751798448127, 'batch_size': 256, 'epochs': 5, 'd_model': 70, 'n_heads': 2, 'num_layers': 1, 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': 40, 'label_smoothing': 0.08349945433375607, 'use_focal_loss': False, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 54321, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:12:09,210 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00015566751798448127, 'batch_size': 256, 'epochs': 5, 'd_model': 70, 'n_heads': 2, 'num_layers': 1, 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': 40, 'label_smoothing': 0.08349945433375607, 'use_focal_loss': False, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 54321, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:12:11,592 - INFO - _models.training_function_executor - Model: 45,922 parameters, 197.3KB storage
2025-09-23 19:12:11,593 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0648087401798307, 0.905266115729516, 0.8634229612295197, 0.8423048275509555, 0.8254659642918689], 'val_losses': [0.9437481293387318, 0.862746339105093, 0.8478108550534035, 0.8129762783039782, 0.7909928219477614], 'val_acc': [0.7406340057636888, 0.7572985841373262, 0.7668212003508332, 0.7767197093096103, 0.7862423255231175], 'model_size_bytes': 191387, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00015566751798448127, 'batch_size': 256, 'epochs': 5, 'd_model': 70, 'n_heads': 2, 'num_layers': 1, 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': 40, 'label_smoothing': 0.08349945433375607, 'use_focal_loss': False, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 54321, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 45922, 'model_storage_size_kb': 197.32109375000002, 'model_size_validation': 'PASS'}
2025-09-23 19:12:11,593 - INFO - _models.training_function_executor - BO Objective: base=0.7862, size_penalty=0.0000, final=0.7862
2025-09-23 19:12:11,593 - INFO - _models.training_function_executor - Model: 45,922 parameters, 197.3KB (PASS 256KB limit)
2025-09-23 19:12:11,593 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 2.384s
2025-09-23 19:12:11,678 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7862
2025-09-23 19:12:11,678 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.086s
2025-09-23 19:12:11,678 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.00015566751798448127, 'batch_size': np.int64(256), 'epochs': np.int64(5), 'd_model': np.int64(70), 'n_heads': np.int64(2), 'num_layers': np.int64(1), 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': np.int64(40), 'label_smoothing': 0.08349945433375607, 'use_focal_loss': np.False_, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(54321), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7862
2025-09-23 19:12:11,678 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.00015566751798448127, 'batch_size': np.int64(256), 'epochs': np.int64(5), 'd_model': np.int64(70), 'n_heads': np.int64(2), 'num_layers': np.int64(1), 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': np.int64(40), 'label_smoothing': 0.08349945433375607, 'use_focal_loss': np.False_, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(54321), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7862
2025-09-23 19:12:11,679 - INFO - bo.run_bo - ğŸ”BO Trial 8: Using RF surrogate + Expected Improvement
2025-09-23 19:12:11,679 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:12:11,679 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:12:11,679 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:12:11,679 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008105504003066627, 'batch_size': 64, 'epochs': 46, 'd_model': 90, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': 50, 'label_smoothing': 0.1210834125192326, 'use_focal_loss': True, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 38713, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:12:11,680 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008105504003066627, 'batch_size': 64, 'epochs': 46, 'd_model': 90, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': 50, 'label_smoothing': 0.1210834125192326, 'use_focal_loss': True, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 38713, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - Model: 240,581 parameters, 1033.7KB storage
2025-09-23 19:13:16,288 - WARNING - _models.training_function_executor - Model storage 1033.7KB exceeds 256KB limit!
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.19402191426316864, 0.10380616666932858, 0.08323842161195162, 0.0740543108293333, 0.06209955993977108, 0.058803237813369665, 0.0540743937219171, 0.0524321887224373, 0.048090012885446454, 0.04609093761623944, 0.04341533979228224, 0.041409051960848056, 0.040509571708418825, 0.03769778325030905, 0.03750676702865196, 0.03478473435507507, 0.03529374467422127, 0.03312316440543017, 0.031596071257608715, 0.03194808653796992, 0.029779182051760097, 0.0294667512974697, 0.028883320897296214, 0.02682374300175478, 0.025242142850794586, 0.026583471257430144, 0.025032384885494353, 0.02498025796611749, 0.022101100295090315, 0.024823000523156495, 0.023228524828667694, 0.02181378414188539, 0.021685687679880935, 0.019311544874425882, 0.020380833995059196, 0.02047515753484499, 0.019194364171267804, 0.01843690606994611, 0.019655034668590504, 0.01965944219712151, 0.01846216628736621, 0.016063565371459285, 0.01734960416275698, 0.017008920280368496, 0.017059166247323117, 0.015128577917764542], 'val_losses': [0.10498044782003922, 0.09484021347065107, 0.06796667309422613, 0.05482126874532964, 0.062149272068201665, 0.05053795231189992, 0.0484423271302531, 0.048714250493260225, 0.04929099869892765, 0.05149665904506647, 0.04379175399598017, 0.044720863265518315, 0.03989090675822088, 0.048944122162308555, 0.0400333539652221, 0.04094765249902541, 0.04079150805260779, 0.038296187481117225, 0.037811013179238524, 0.037465563073125104, 0.03897783275408458, 0.0372100897770958, 0.03508899121162154, 0.03584909024781981, 0.03972865535627106, 0.03851599444267281, 0.039223804722646624, 0.033822349304058634, 0.039980545445815495, 0.03335312195005549, 0.03549564303334323, 0.03550892116383134, 0.033364330069147366, 0.03884592070725609, 0.03892813040422715, 0.03810818213864446, 0.0359345229272331, 0.03369371062421198, 0.03765217835906753, 0.03537922469898358, 0.03821704734680184, 0.036669804860369005, 0.03603055560255376, 0.03475600990465957, 0.03330556340324238, 0.03676727135002785], 'val_acc': [0.8986342563588523, 0.9142964540784363, 0.9383535897757174, 0.9473750156621977, 0.9451196591905776, 0.9517604310236812, 0.9493797769703045, 0.9515098358601679, 0.9408595414108508, 0.9515098358601679, 0.9561458463851648, 0.9582759052750282, 0.9594035835108382, 0.9517604310236812, 0.9597794762561083, 0.9607818569101616, 0.9566470367121914, 0.9610324520736749, 0.960656559328405, 0.9602806665831349, 0.9641648916175918, 0.9624107254729983, 0.9637889988723217, 0.9660443553439418, 0.953765192331788, 0.9601553690013783, 0.9609071544919183, 0.9645407843628618, 0.9570229294574615, 0.9666708432527252, 0.9672973311615086, 0.9650419746898885, 0.967547926325022, 0.9647913795263752, 0.9680491166520486, 0.9669214384162386, 0.9679238190702919, 0.9674226287432652, 0.9679238190702919, 0.9676732239067786, 0.9670467359979953, 0.9695526876331286, 0.9684250093973187, 0.9666708432527252, 0.970930961032452, 0.968299711815562], 'model_size_bytes': 496147, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008105504003066627, 'batch_size': 64, 'epochs': 46, 'd_model': 90, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': 50, 'label_smoothing': 0.1210834125192326, 'use_focal_loss': True, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 38713, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 240581, 'model_storage_size_kb': 1033.7464843750001, 'model_size_validation': 'FAIL'}
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - BO Objective: base=0.9683, size_penalty=0.8000, final=0.1683
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - Model: 240,581 parameters, 1033.7KB (FAIL 256KB limit)
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 64.609s
2025-09-23 19:13:16,378 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1683
2025-09-23 19:13:16,378 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.089s
2025-09-23 19:13:16,378 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 0.0008105504003066627, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'd_model': np.int64(90), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.1210834125192326, 'use_focal_loss': np.True_, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(38713), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.1683
2025-09-23 19:13:16,378 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 0.0008105504003066627, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'd_model': np.int64(90), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.1210834125192326, 'use_focal_loss': np.True_, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(38713), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.1683
2025-09-23 19:13:16,378 - INFO - bo.run_bo - ğŸ”BO Trial 9: Using RF surrogate + Expected Improvement
2025-09-23 19:13:16,378 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:13:16,378 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:13:16,378 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:13:16,378 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005127798466822118, 'batch_size': 32, 'epochs': 12, 'd_model': 66, 'n_heads': 4, 'num_layers': 1, 'mlp_ratio': 2.8893049947637337, 'dropout': 0.14583261727127086, 'weight_decay': 1.698922866678694e-06, 'patch_size': 100, 'label_smoothing': 0.14154242924170948, 'use_focal_loss': True, 'focal_gamma': 1.3669523491593882, 'grad_clip_norm': 0.07563102648852206, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 11383, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:13:16,379 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005127798466822118, 'batch_size': 32, 'epochs': 12, 'd_model': 66, 'n_heads': 4, 'num_layers': 1, 'mlp_ratio': 2.8893049947637337, 'dropout': 0.14583261727127086, 'weight_decay': 1.698922866678694e-06, 'patch_size': 100, 'label_smoothing': 0.14154242924170948, 'use_focal_loss': True, 'focal_gamma': 1.3669523491593882, 'grad_clip_norm': 0.07563102648852206, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 11383, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - Model: 57,825 parameters, 248.5KB storage
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.12990208620046406, 0.11292482934619343, 0.11742104445212587, 0.1131646870545448, 0.10649996532210523, 0.1053118714544315, 0.09977679508686144, 0.09466852618867012, 0.09271588285360523, 0.09158203387515394, 0.0916983009147197, 0.08971676959688007], 'val_losses': [0.10005818082930121, 0.10181149605639693, 0.10308305690126407, 0.08786725732006012, 0.08330204460922526, 0.08928685805886002, 0.0813044332539157, 0.08669461998558742, 0.07528372267964803, 0.0722673459228489, 0.07112939409606366, 0.07200135766389566], 'val_acc': [0.08206991605062022, 0.1136449066533016, 0.10938478887357474, 0.11489788247086831, 0.13218894875328907, 0.0799398571607568, 0.17403834106001753, 0.19458714446811176, 0.14897882470868312, 0.15474251346949003, 0.17128179426137075, 0.1458463851647663], 'model_size_bytes': 239195, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005127798466822118, 'batch_size': 32, 'epochs': 12, 'd_model': 66, 'n_heads': 4, 'num_layers': 1, 'mlp_ratio': 2.8893049947637337, 'dropout': 0.14583261727127086, 'weight_decay': 1.698922866678694e-06, 'patch_size': 100, 'label_smoothing': 0.14154242924170948, 'use_focal_loss': True, 'focal_gamma': 1.3669523491593882, 'grad_clip_norm': 0.07563102648852206, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 11383, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 57825, 'model_storage_size_kb': 248.46679687500003, 'model_size_validation': 'PASS'}
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - BO Objective: base=0.1458, size_penalty=0.0000, final=0.1458
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - Model: 57,825 parameters, 248.5KB (PASS 256KB limit)
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 19.080s
2025-09-23 19:13:35,687 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1458
2025-09-23 19:13:35,687 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.229s
2025-09-23 19:13:35,687 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 0.005127798466822118, 'batch_size': np.int64(32), 'epochs': np.int64(12), 'd_model': np.int64(66), 'n_heads': np.int64(4), 'num_layers': np.int64(1), 'mlp_ratio': 2.8893049947637337, 'dropout': 0.14583261727127086, 'weight_decay': 1.698922866678694e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.14154242924170948, 'use_focal_loss': np.True_, 'focal_gamma': 1.3669523491593882, 'grad_clip_norm': 0.07563102648852206, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': np.int64(11383), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.1458
2025-09-23 19:13:35,688 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 0.005127798466822118, 'batch_size': np.int64(32), 'epochs': np.int64(12), 'd_model': np.int64(66), 'n_heads': np.int64(4), 'num_layers': np.int64(1), 'mlp_ratio': 2.8893049947637337, 'dropout': 0.14583261727127086, 'weight_decay': 1.698922866678694e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.14154242924170948, 'use_focal_loss': np.True_, 'focal_gamma': 1.3669523491593882, 'grad_clip_norm': 0.07563102648852206, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': np.int64(11383), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.1458
2025-09-23 19:13:35,688 - INFO - bo.run_bo - ğŸ”BO Trial 10: Using RF surrogate + Expected Improvement
2025-09-23 19:13:35,688 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:13:35,688 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:13:35,688 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:13:35,688 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0038282267185434835, 'batch_size': 64, 'epochs': 9, 'd_model': 73, 'n_heads': 1, 'num_layers': 4, 'mlp_ratio': 2.3151972141488244, 'dropout': 0.1930092035274023, 'weight_decay': 0.001058348356623613, 'patch_size': 40, 'label_smoothing': 0.11404331835739179, 'use_focal_loss': False, 'focal_gamma': 1.20805104102331, 'grad_clip_norm': 1.4355854272147706, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 64454, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:13:35,689 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0038282267185434835, 'batch_size': 64, 'epochs': 9, 'd_model': 73, 'n_heads': 1, 'num_layers': 4, 'mlp_ratio': 2.3151972141488244, 'dropout': 0.1930092035274023, 'weight_decay': 0.001058348356623613, 'patch_size': 40, 'label_smoothing': 0.11404331835739179, 'use_focal_loss': False, 'focal_gamma': 1.20805104102331, 'grad_clip_norm': 1.4355854272147706, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 64454, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:13:50,673 - INFO - _models.training_function_executor - Model: 9,198 parameters, 39.5KB storage
2025-09-23 19:13:50,673 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.898266415292337, 0.7981167602064062, 0.7357660217384936, 0.6766243188956199, 0.6432767724108062, 0.6209858959003397, 0.608937042158237, 0.5933801885969084, 0.5876029965251892], 'val_losses': [0.8239398609798753, 0.726933329449816, 0.6717660419401138, 0.6345121081558599, 0.604435100649222, 0.5974107103424852, 0.5968737707462485, 0.5752076836639651, 0.5786074189939022], 'val_acc': [0.8141836862548553, 0.8590402205237438, 0.8942488409973688, 0.9114146096980328, 0.9260744267635634, 0.9293321638892369, 0.9305851397068037, 0.936975316376394, 0.9376018042851773], 'model_size_bytes': 261693, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0038282267185434835, 'batch_size': 64, 'epochs': 9, 'd_model': 73, 'n_heads': 1, 'num_layers': 4, 'mlp_ratio': 2.3151972141488244, 'dropout': 0.1930092035274023, 'weight_decay': 0.001058348356623613, 'patch_size': 40, 'label_smoothing': 0.11404331835739179, 'use_focal_loss': False, 'focal_gamma': 1.20805104102331, 'grad_clip_norm': 1.4355854272147706, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 64454, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 9198, 'model_storage_size_kb': 39.522656250000004, 'model_size_validation': 'PASS'}
2025-09-23 19:13:50,673 - INFO - _models.training_function_executor - BO Objective: base=0.9376, size_penalty=0.0000, final=0.9376
2025-09-23 19:13:50,673 - INFO - _models.training_function_executor - Model: 9,198 parameters, 39.5KB (PASS 256KB limit)
2025-09-23 19:13:50,673 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 14.986s
2025-09-23 19:13:50,766 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9376
2025-09-23 19:13:50,766 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-09-23 19:13:50,766 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 0.0038282267185434835, 'batch_size': np.int64(64), 'epochs': np.int64(9), 'd_model': np.int64(73), 'n_heads': np.int64(1), 'num_layers': np.int64(4), 'mlp_ratio': 2.3151972141488244, 'dropout': 0.1930092035274023, 'weight_decay': 0.001058348356623613, 'patch_size': np.int64(40), 'label_smoothing': 0.11404331835739179, 'use_focal_loss': np.False_, 'focal_gamma': 1.20805104102331, 'grad_clip_norm': 1.4355854272147706, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(64454), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9376
2025-09-23 19:13:50,766 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 0.0038282267185434835, 'batch_size': np.int64(64), 'epochs': np.int64(9), 'd_model': np.int64(73), 'n_heads': np.int64(1), 'num_layers': np.int64(4), 'mlp_ratio': 2.3151972141488244, 'dropout': 0.1930092035274023, 'weight_decay': 0.001058348356623613, 'patch_size': np.int64(40), 'label_smoothing': 0.11404331835739179, 'use_focal_loss': np.False_, 'focal_gamma': 1.20805104102331, 'grad_clip_norm': 1.4355854272147706, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(64454), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9376
2025-09-23 19:13:50,766 - INFO - bo.run_bo - ğŸ”BO Trial 11: Using RF surrogate + Expected Improvement
2025-09-23 19:13:50,766 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:13:50,766 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:13:50,766 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:13:50,766 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0009656720622207006, 'batch_size': 256, 'epochs': 8, 'd_model': 46, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 1.966979796110768, 'dropout': 0.22575263723352001, 'weight_decay': 0.0010826230178144604, 'patch_size': 50, 'label_smoothing': 0.1510123515426017, 'use_focal_loss': True, 'focal_gamma': 1.8453740141190789, 'grad_clip_norm': 1.972061586759113, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 29463, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-23 19:13:50,768 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0009656720622207006, 'batch_size': 256, 'epochs': 8, 'd_model': 46, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 1.966979796110768, 'dropout': 0.22575263723352001, 'weight_decay': 0.0010826230178144604, 'patch_size': 50, 'label_smoothing': 0.1510123515426017, 'use_focal_loss': True, 'focal_gamma': 1.8453740141190789, 'grad_clip_norm': 1.972061586759113, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 29463, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-23 19:13:55,886 - INFO - _models.training_function_executor - Model: 57,741 parameters, 248.1KB storage
2025-09-23 19:13:55,886 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.41223248218461633, 0.2792316266282946, 0.22978910902333066, 0.19817265927888686, 0.1818280761889175, 0.1648172570954206, 0.1560489806477499, 0.14401378683694283], 'val_losses': [0.2984842316759453, 0.2329984595622205, 0.18202567361169195, 0.15558126267634334, 0.13503478208500524, 0.13475614227984667, 0.1385451905075085, 0.1420619362609781], 'val_acc': [0.7873700037589274, 0.8159378523994487, 0.8635509334669841, 0.891116401453452, 0.9105375266257362, 0.908658062899386, 0.9055256233554693, 0.8863550933466984], 'model_size_bytes': 245459, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0009656720622207006, 'batch_size': 256, 'epochs': 8, 'd_model': 46, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 1.966979796110768, 'dropout': 0.22575263723352001, 'weight_decay': 0.0010826230178144604, 'patch_size': 50, 'label_smoothing': 0.1510123515426017, 'use_focal_loss': True, 'focal_gamma': 1.8453740141190789, 'grad_clip_norm': 1.972061586759113, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 29463, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 57741, 'model_storage_size_kb': 248.10585937500002, 'model_size_validation': 'PASS'}
2025-09-23 19:13:55,886 - INFO - _models.training_function_executor - BO Objective: base=0.8864, size_penalty=0.0000, final=0.8864
2025-09-23 19:13:55,886 - INFO - _models.training_function_executor - Model: 57,741 parameters, 248.1KB (PASS 256KB limit)
2025-09-23 19:13:55,886 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.120s
2025-09-23 19:13:55,980 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8864
2025-09-23 19:13:55,980 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.094s
2025-09-23 19:13:55,980 - INFO - bo.run_bo - Recorded observation #11: hparams={'lr': 0.0009656720622207006, 'batch_size': np.int64(256), 'epochs': np.int64(8), 'd_model': np.int64(46), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 1.966979796110768, 'dropout': 0.22575263723352001, 'weight_decay': 0.0010826230178144604, 'patch_size': np.int64(50), 'label_smoothing': 0.1510123515426017, 'use_focal_loss': np.True_, 'focal_gamma': 1.8453740141190789, 'grad_clip_norm': 1.972061586759113, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(29463), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.8864
2025-09-23 19:13:55,980 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 11: {'lr': 0.0009656720622207006, 'batch_size': np.int64(256), 'epochs': np.int64(8), 'd_model': np.int64(46), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 1.966979796110768, 'dropout': 0.22575263723352001, 'weight_decay': 0.0010826230178144604, 'patch_size': np.int64(50), 'label_smoothing': 0.1510123515426017, 'use_focal_loss': np.True_, 'focal_gamma': 1.8453740141190789, 'grad_clip_norm': 1.972061586759113, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(29463), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.8864
2025-09-23 19:13:55,981 - INFO - bo.run_bo - ğŸ”BO Trial 12: Using RF surrogate + Expected Improvement
2025-09-23 19:13:55,981 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:13:55,981 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:13:55,981 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:13:55,981 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00518451490363407, 'batch_size': 256, 'epochs': 6, 'd_model': 93, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 2.296385520287719, 'dropout': 0.17205091410282083, 'weight_decay': 0.00013197831391939811, 'patch_size': 25, 'label_smoothing': 0.09407771562499165, 'use_focal_loss': False, 'focal_gamma': 2.187627493151842, 'grad_clip_norm': 1.992295167581806, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 33301, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:13:55,982 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00518451490363407, 'batch_size': 256, 'epochs': 6, 'd_model': 93, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 2.296385520287719, 'dropout': 0.17205091410282083, 'weight_decay': 0.00013197831391939811, 'patch_size': 25, 'label_smoothing': 0.09407771562499165, 'use_focal_loss': False, 'focal_gamma': 2.187627493151842, 'grad_clip_norm': 1.992295167581806, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 33301, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:14:06,076 - INFO - _models.training_function_executor - Model: 310,373 parameters, 333.4KB storage
2025-09-23 19:14:06,076 - WARNING - _models.training_function_executor - Model storage 333.4KB exceeds 256KB limit!
2025-09-23 19:14:06,077 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9428122003204754, 0.7653920046911165, 0.6969138762334219, 0.6487980540789888, 0.6376182154034312, 0.5883914690748423], 'val_losses': [0.7989055756159764, 0.7474598162172134, 0.639358535698371, 0.6092367613282722, 0.5910918383337833, 0.5477994599224048], 'val_acc': [0.8076682120035084, 0.8298458839744393, 0.8921187821075053, 0.9013908031574991, 0.9046485402831725, 0.9256985340182935], 'model_size_bytes': 639023, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00518451490363407, 'batch_size': 256, 'epochs': 6, 'd_model': 93, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 2.296385520287719, 'dropout': 0.17205091410282083, 'weight_decay': 0.00013197831391939811, 'patch_size': 25, 'label_smoothing': 0.09407771562499165, 'use_focal_loss': False, 'focal_gamma': 2.187627493151842, 'grad_clip_norm': 1.992295167581806, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 33301, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 310373, 'model_storage_size_kb': 333.40849609375005, 'model_size_validation': 'FAIL'}
2025-09-23 19:14:06,077 - INFO - _models.training_function_executor - BO Objective: base=0.9257, size_penalty=0.1512, final=0.7745
2025-09-23 19:14:06,077 - INFO - _models.training_function_executor - Model: 310,373 parameters, 333.4KB (FAIL 256KB limit)
2025-09-23 19:14:06,077 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 10.096s
2025-09-23 19:14:06,172 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7745
2025-09-23 19:14:06,172 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-09-23 19:14:06,172 - INFO - bo.run_bo - Recorded observation #12: hparams={'lr': 0.00518451490363407, 'batch_size': np.int64(256), 'epochs': np.int64(6), 'd_model': np.int64(93), 'n_heads': np.int64(2), 'num_layers': np.int64(4), 'mlp_ratio': 2.296385520287719, 'dropout': 0.17205091410282083, 'weight_decay': 0.00013197831391939811, 'patch_size': np.int64(25), 'label_smoothing': 0.09407771562499165, 'use_focal_loss': np.False_, 'focal_gamma': 2.187627493151842, 'grad_clip_norm': 1.992295167581806, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(33301), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7745
2025-09-23 19:14:06,172 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 12: {'lr': 0.00518451490363407, 'batch_size': np.int64(256), 'epochs': np.int64(6), 'd_model': np.int64(93), 'n_heads': np.int64(2), 'num_layers': np.int64(4), 'mlp_ratio': 2.296385520287719, 'dropout': 0.17205091410282083, 'weight_decay': 0.00013197831391939811, 'patch_size': np.int64(25), 'label_smoothing': 0.09407771562499165, 'use_focal_loss': np.False_, 'focal_gamma': 2.187627493151842, 'grad_clip_norm': 1.992295167581806, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(33301), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7745
2025-09-23 19:14:06,172 - INFO - bo.run_bo - ğŸ”BO Trial 13: Using RF surrogate + Expected Improvement
2025-09-23 19:14:06,172 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:14:06,172 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:14:06,172 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:14:06,172 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003840474526989731, 'batch_size': 128, 'epochs': 22, 'd_model': 61, 'n_heads': 1, 'num_layers': 4, 'mlp_ratio': 2.33711319621712, 'dropout': 0.20750457063658334, 'weight_decay': 0.000266845419499171, 'patch_size': 50, 'label_smoothing': 0.15474085937304935, 'use_focal_loss': True, 'focal_gamma': 1.19245568490054, 'grad_clip_norm': 1.5641490902752075, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 26599, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:14:06,174 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003840474526989731, 'batch_size': 128, 'epochs': 22, 'd_model': 61, 'n_heads': 1, 'num_layers': 4, 'mlp_ratio': 2.33711319621712, 'dropout': 0.20750457063658334, 'weight_decay': 0.000266845419499171, 'patch_size': 50, 'label_smoothing': 0.15474085937304935, 'use_focal_loss': True, 'focal_gamma': 1.19245568490054, 'grad_clip_norm': 1.5641490902752075, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 26599, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:14:27,709 - INFO - _models.training_function_executor - Model: 8,601 parameters, 37.0KB storage
2025-09-23 19:14:27,709 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4689952664765089, 0.30669867365182807, 0.2544418694519427, 0.2260426870412655, 0.208669728793992, 0.18934135108880895, 0.17636842306322784, 0.16589072826448414, 0.15451000666517894, 0.14835461273780537, 0.14068950369010738, 0.1330050403413343, 0.12867802710948656, 0.12437218971485348, 0.11983572136829786, 0.11555092507009462, 0.1111268271757943, 0.10709979664951519, 0.10699882784043399, 0.10252106673549904, 0.10160853772659041, 0.0969158117040463], 'val_losses': [0.33240043749719345, 0.2323765958528772, 0.19355408739601515, 0.18358031414143947, 0.16372509480508524, 0.15633524010323624, 0.14874528513682425, 0.12536742075364732, 0.1256990192385451, 0.1264848881895314, 0.11577967708878328, 0.11139903244860129, 0.09662369430175313, 0.09864479700021017, 0.1112560805877585, 0.09720616513678132, 0.09546286858643464, 0.09284206236787435, 0.08789443723808858, 0.0894482553746449, 0.08237033836471677, 0.08204000295814215], 'val_acc': [0.8006515474251347, 0.8738253351710312, 0.8990101491041222, 0.9031449693020924, 0.9094098483899261, 0.9201854404209999, 0.9199348452574865, 0.9319634131061271, 0.9335922816689638, 0.9314622227791004, 0.9365994236311239, 0.9412354341561208, 0.9477509084074678, 0.9475003132439543, 0.9365994236311239, 0.9467485277534143, 0.9483773963162511, 0.9496303721338178, 0.9492544793885478, 0.945746147099361, 0.9550181681493547, 0.9560205488034081], 'model_size_bytes': 202621, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003840474526989731, 'batch_size': 128, 'epochs': 22, 'd_model': 61, 'n_heads': 1, 'num_layers': 4, 'mlp_ratio': 2.33711319621712, 'dropout': 0.20750457063658334, 'weight_decay': 0.000266845419499171, 'patch_size': 50, 'label_smoothing': 0.15474085937304935, 'use_focal_loss': True, 'focal_gamma': 1.19245568490054, 'grad_clip_norm': 1.5641490902752075, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 26599, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 8601, 'model_storage_size_kb': 36.957421875, 'model_size_validation': 'PASS'}
2025-09-23 19:14:27,709 - INFO - _models.training_function_executor - BO Objective: base=0.9560, size_penalty=0.0000, final=0.9560
2025-09-23 19:14:27,709 - INFO - _models.training_function_executor - Model: 8,601 parameters, 37.0KB (PASS 256KB limit)
2025-09-23 19:14:27,709 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 21.537s
2025-09-23 19:14:27,804 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9560
2025-09-23 19:14:27,804 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-09-23 19:14:27,804 - INFO - bo.run_bo - Recorded observation #13: hparams={'lr': 0.0003840474526989731, 'batch_size': np.int64(128), 'epochs': np.int64(22), 'd_model': np.int64(61), 'n_heads': np.int64(1), 'num_layers': np.int64(4), 'mlp_ratio': 2.33711319621712, 'dropout': 0.20750457063658334, 'weight_decay': 0.000266845419499171, 'patch_size': np.int64(50), 'label_smoothing': 0.15474085937304935, 'use_focal_loss': np.True_, 'focal_gamma': 1.19245568490054, 'grad_clip_norm': 1.5641490902752075, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(26599), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9560
2025-09-23 19:14:27,804 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 13: {'lr': 0.0003840474526989731, 'batch_size': np.int64(128), 'epochs': np.int64(22), 'd_model': np.int64(61), 'n_heads': np.int64(1), 'num_layers': np.int64(4), 'mlp_ratio': 2.33711319621712, 'dropout': 0.20750457063658334, 'weight_decay': 0.000266845419499171, 'patch_size': np.int64(50), 'label_smoothing': 0.15474085937304935, 'use_focal_loss': np.True_, 'focal_gamma': 1.19245568490054, 'grad_clip_norm': 1.5641490902752075, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(26599), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9560
2025-09-23 19:14:27,804 - INFO - bo.run_bo - ğŸ”BO Trial 14: Using RF surrogate + Expected Improvement
2025-09-23 19:14:27,804 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:14:27,804 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:14:27,804 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:14:27,804 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00019132652427760765, 'batch_size': 64, 'epochs': 20, 'd_model': 71, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 2.357698426691005, 'dropout': 0.43676798277060064, 'weight_decay': 0.005927277344027731, 'patch_size': 50, 'label_smoothing': 0.187628624340883, 'use_focal_loss': False, 'focal_gamma': 1.39245431717818, 'grad_clip_norm': 1.0427866655963234, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 6737, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-09-23 19:14:27,806 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00019132652427760765, 'batch_size': 64, 'epochs': 20, 'd_model': 71, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 2.357698426691005, 'dropout': 0.43676798277060064, 'weight_decay': 0.005927277344027731, 'patch_size': 50, 'label_smoothing': 0.187628624340883, 'use_focal_loss': False, 'focal_gamma': 1.39245431717818, 'grad_clip_norm': 1.0427866655963234, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 6737, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-09-23 19:14:55,678 - INFO - _models.training_function_executor - Model: 9,727 parameters, 41.8KB storage
2025-09-23 19:14:55,678 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0884245025093267, 0.9903191964051906, 0.9601241836600729, 0.9406643414099302, 0.9266689361771766, 0.9148909192565793, 0.8998634538991677, 0.8896364244035143, 0.8858639241474798, 0.8768213233362817, 0.8705035528622672, 0.8662204068432183, 0.8600076692682331, 0.85649330505164, 0.8547231293809789, 0.8518494678048514, 0.8503860623560255, 0.8487493565061414, 0.8490733304250189, 0.8510321553528012], 'val_losses': [1.0177218942112858, 0.9571554099926448, 0.9464748441956423, 0.8996576676316734, 0.9015601317008147, 0.8731786286960851, 0.8779745348084468, 0.8606465022122706, 0.8790085592032106, 0.8768585247737156, 0.8470423539813159, 0.8615394903818567, 0.8446668057479949, 0.8489232346566753, 0.8462789482811074, 0.8439114810141566, 0.844544815279281, 0.8392747573693613, 0.8422732932005083, 0.8437682659072455], 'val_acc': [0.7842375642150107, 0.8079188071670217, 0.8264628492670092, 0.8506452825460469, 0.8584137326149606, 0.8745771206615712, 0.8720711690264378, 0.8838491417115649, 0.8716952762811677, 0.876205989224408, 0.8966294950507455, 0.8877333667460218, 0.8963788998872322, 0.8950006264879088, 0.8956271143966922, 0.8982583636135822, 0.8976318757047989, 0.9001378273399323, 0.8993860418493923, 0.898383661195339], 'model_size_bytes': 203193, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00019132652427760765, 'batch_size': 64, 'epochs': 20, 'd_model': 71, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 2.357698426691005, 'dropout': 0.43676798277060064, 'weight_decay': 0.005927277344027731, 'patch_size': 50, 'label_smoothing': 0.187628624340883, 'use_focal_loss': False, 'focal_gamma': 1.39245431717818, 'grad_clip_norm': 1.0427866655963234, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 6737, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 9727, 'model_storage_size_kb': 41.795703125, 'model_size_validation': 'PASS'}
2025-09-23 19:14:55,678 - INFO - _models.training_function_executor - BO Objective: base=0.8984, size_penalty=0.0000, final=0.8984
2025-09-23 19:14:55,678 - INFO - _models.training_function_executor - Model: 9,727 parameters, 41.8KB (PASS 256KB limit)
2025-09-23 19:14:55,678 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 27.874s
2025-09-23 19:14:55,776 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8984
2025-09-23 19:14:55,776 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-09-23 19:14:55,776 - INFO - bo.run_bo - Recorded observation #14: hparams={'lr': 0.00019132652427760765, 'batch_size': np.int64(64), 'epochs': np.int64(20), 'd_model': np.int64(71), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 2.357698426691005, 'dropout': 0.43676798277060064, 'weight_decay': 0.005927277344027731, 'patch_size': np.int64(50), 'label_smoothing': 0.187628624340883, 'use_focal_loss': np.False_, 'focal_gamma': 1.39245431717818, 'grad_clip_norm': 1.0427866655963234, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(6737), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.8984
2025-09-23 19:14:55,776 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 14: {'lr': 0.00019132652427760765, 'batch_size': np.int64(64), 'epochs': np.int64(20), 'd_model': np.int64(71), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 2.357698426691005, 'dropout': 0.43676798277060064, 'weight_decay': 0.005927277344027731, 'patch_size': np.int64(50), 'label_smoothing': 0.187628624340883, 'use_focal_loss': np.False_, 'focal_gamma': 1.39245431717818, 'grad_clip_norm': 1.0427866655963234, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(6737), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.8984
2025-09-23 19:14:55,776 - INFO - bo.run_bo - ğŸ”BO Trial 15: Using RF surrogate + Expected Improvement
2025-09-23 19:14:55,776 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:14:55,776 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:14:55,776 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:14:55,776 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0006380542088924777, 'batch_size': 128, 'epochs': 14, 'd_model': 68, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.9956487482498506, 'dropout': 0.25723832342248043, 'weight_decay': 4.81771906763772e-05, 'patch_size': 50, 'label_smoothing': 0.14027812208965387, 'use_focal_loss': False, 'focal_gamma': 1.9194909497560813, 'grad_clip_norm': 1.3966237621116846, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 7313, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:14:55,778 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0006380542088924777, 'batch_size': 128, 'epochs': 14, 'd_model': 68, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.9956487482498506, 'dropout': 0.25723832342248043, 'weight_decay': 4.81771906763772e-05, 'patch_size': 50, 'label_smoothing': 0.14027812208965387, 'use_focal_loss': False, 'focal_gamma': 1.9194909497560813, 'grad_clip_norm': 1.3966237621116846, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 7313, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:15:09,743 - INFO - _models.training_function_executor - Model: 9,316 parameters, 40.0KB storage
2025-09-23 19:15:09,744 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.169523529106, 0.969551489748117, 0.8170485849932867, 0.7298950413353136, 0.6886814220434173, 0.6706389446282146, 0.6528843179201828, 0.6403951109159569, 0.630140383112189, 0.6221797233445974, 0.6152094515464183, 0.6097150063546825, 0.6081066349169382, 0.6060317018887746], 'val_losses': [1.0620899771765768, 0.9220598941159389, 0.7253125864317387, 0.669339289373659, 0.6429294613336445, 0.6381138275103109, 0.6233321835695689, 0.6141632686296422, 0.6092480366174148, 0.6026273856947497, 0.6036906593680277, 0.6017791701205586, 0.5988686052708231, 0.5986899891050219], 'val_acc': [0.7200852023555946, 0.76882596165894, 0.8827214634757549, 0.9208119283297832, 0.9353464478135572, 0.9383535897757174, 0.9458714446811176, 0.9481268011527377, 0.9507580503696279, 0.9551434657311114, 0.9551434657311114, 0.9558952512216514, 0.9577747149480015, 0.9571482270392181], 'model_size_bytes': 236985, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0006380542088924777, 'batch_size': 128, 'epochs': 14, 'd_model': 68, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.9956487482498506, 'dropout': 0.25723832342248043, 'weight_decay': 4.81771906763772e-05, 'patch_size': 50, 'label_smoothing': 0.14027812208965387, 'use_focal_loss': False, 'focal_gamma': 1.9194909497560813, 'grad_clip_norm': 1.3966237621116846, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 7313, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 9316, 'model_storage_size_kb': 40.0296875, 'model_size_validation': 'PASS'}
2025-09-23 19:15:09,744 - INFO - _models.training_function_executor - BO Objective: base=0.9571, size_penalty=0.0000, final=0.9571
2025-09-23 19:15:09,744 - INFO - _models.training_function_executor - Model: 9,316 parameters, 40.0KB (PASS 256KB limit)
2025-09-23 19:15:09,744 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 13.968s
2025-09-23 19:15:09,843 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9571
2025-09-23 19:15:09,843 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-23 19:15:09,843 - INFO - bo.run_bo - Recorded observation #15: hparams={'lr': 0.0006380542088924777, 'batch_size': np.int64(128), 'epochs': np.int64(14), 'd_model': np.int64(68), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 3.9956487482498506, 'dropout': 0.25723832342248043, 'weight_decay': 4.81771906763772e-05, 'patch_size': np.int64(50), 'label_smoothing': 0.14027812208965387, 'use_focal_loss': np.False_, 'focal_gamma': 1.9194909497560813, 'grad_clip_norm': 1.3966237621116846, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(7313), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9571
2025-09-23 19:15:09,843 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 15: {'lr': 0.0006380542088924777, 'batch_size': np.int64(128), 'epochs': np.int64(14), 'd_model': np.int64(68), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 3.9956487482498506, 'dropout': 0.25723832342248043, 'weight_decay': 4.81771906763772e-05, 'patch_size': np.int64(50), 'label_smoothing': 0.14027812208965387, 'use_focal_loss': np.False_, 'focal_gamma': 1.9194909497560813, 'grad_clip_norm': 1.3966237621116846, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(7313), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9571
2025-09-23 19:15:09,844 - INFO - bo.run_bo - ğŸ”BO Trial 16: Using RF surrogate + Expected Improvement
2025-09-23 19:15:09,844 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:15:09,844 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:15:09,844 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:15:09,844 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0012519052827756568, 'batch_size': 32, 'epochs': 17, 'd_model': 56, 'n_heads': 4, 'num_layers': 1, 'mlp_ratio': 3.049040420483303, 'dropout': 0.34935032591577725, 'weight_decay': 4.246528221346579e-06, 'patch_size': 50, 'label_smoothing': 0.18704589273685368, 'use_focal_loss': False, 'focal_gamma': 1.881437816044532, 'grad_clip_norm': 1.6368682046358407, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 33698, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:15:09,845 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0012519052827756568, 'batch_size': 32, 'epochs': 17, 'd_model': 56, 'n_heads': 4, 'num_layers': 1, 'mlp_ratio': 3.049040420483303, 'dropout': 0.34935032591577725, 'weight_decay': 4.246528221346579e-06, 'patch_size': 50, 'label_smoothing': 0.18704589273685368, 'use_focal_loss': False, 'focal_gamma': 1.881437816044532, 'grad_clip_norm': 1.6368682046358407, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 33698, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:15:37,211 - INFO - _models.training_function_executor - Model: 39,555 parameters, 170.0KB storage
2025-09-23 19:15:37,211 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0150306614606717, 0.9509134387826574, 0.929251247947461, 0.909283339092241, 0.8974829403390577, 0.8862145129153967, 0.8770930561935552, 0.8682189752026181, 0.8586508136242511, 0.8546431863815441, 0.8484256788441753, 0.8428072013476474, 0.8402006679417268, 0.8391473221718374, 0.8341624857645729, 0.8326934878921665, 0.8324645187096484], 'val_losses': [0.9285661543002629, 0.8966918419572797, 0.8590995245388525, 0.8382058509413931, 0.8463119636082647, 0.8332967274829179, 0.8276692316311371, 0.8117998627220654, 0.8114627396951357, 0.8042454487353455, 0.8439657821898442, 0.8063210632146532, 0.8098981545139056, 0.8050585617369062, 0.8067902752726079, 0.8111677568906711, 0.8061717233103987], 'val_acc': [0.8259616589399824, 0.8486405212379401, 0.8666833730109009, 0.8828467610575116, 0.8808419997494048, 0.8858539030196717, 0.891868186943992, 0.8993860418493923, 0.8993860418493923, 0.9075303846635759, 0.8871068788372385, 0.9046485402831725, 0.9050244330284426, 0.9066533015912793, 0.9060268136824959, 0.9046485402831725, 0.907154491918306], 'model_size_bytes': 166043, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0012519052827756568, 'batch_size': 32, 'epochs': 17, 'd_model': 56, 'n_heads': 4, 'num_layers': 1, 'mlp_ratio': 3.049040420483303, 'dropout': 0.34935032591577725, 'weight_decay': 4.246528221346579e-06, 'patch_size': 50, 'label_smoothing': 0.18704589273685368, 'use_focal_loss': False, 'focal_gamma': 1.881437816044532, 'grad_clip_norm': 1.6368682046358407, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 33698, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 39555, 'model_storage_size_kb': 169.962890625, 'model_size_validation': 'PASS'}
2025-09-23 19:15:37,211 - INFO - _models.training_function_executor - BO Objective: base=0.9072, size_penalty=0.0000, final=0.9072
2025-09-23 19:15:37,211 - INFO - _models.training_function_executor - Model: 39,555 parameters, 170.0KB (PASS 256KB limit)
2025-09-23 19:15:37,211 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 27.367s
2025-09-23 19:15:37,311 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9072
2025-09-23 19:15:37,311 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-23 19:15:37,311 - INFO - bo.run_bo - Recorded observation #16: hparams={'lr': 0.0012519052827756568, 'batch_size': np.int64(32), 'epochs': np.int64(17), 'd_model': np.int64(56), 'n_heads': np.int64(4), 'num_layers': np.int64(1), 'mlp_ratio': 3.049040420483303, 'dropout': 0.34935032591577725, 'weight_decay': 4.246528221346579e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.18704589273685368, 'use_focal_loss': np.False_, 'focal_gamma': 1.881437816044532, 'grad_clip_norm': 1.6368682046358407, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(33698), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9072
2025-09-23 19:15:37,311 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 16: {'lr': 0.0012519052827756568, 'batch_size': np.int64(32), 'epochs': np.int64(17), 'd_model': np.int64(56), 'n_heads': np.int64(4), 'num_layers': np.int64(1), 'mlp_ratio': 3.049040420483303, 'dropout': 0.34935032591577725, 'weight_decay': 4.246528221346579e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.18704589273685368, 'use_focal_loss': np.False_, 'focal_gamma': 1.881437816044532, 'grad_clip_norm': 1.6368682046358407, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(33698), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9072
2025-09-23 19:15:37,311 - INFO - bo.run_bo - ğŸ”BO Trial 17: Using RF surrogate + Expected Improvement
2025-09-23 19:15:37,311 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:15:37,311 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:15:37,311 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:15:37,311 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0012335538376642553, 'batch_size': 128, 'epochs': 29, 'd_model': 38, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3.4108487070361657, 'dropout': 0.24156090620477688, 'weight_decay': 5.766739245797547e-06, 'patch_size': 25, 'label_smoothing': 0.1284753624288166, 'use_focal_loss': True, 'focal_gamma': 1.979004447294647, 'grad_clip_norm': 1.9784186578274159, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 36951, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:15:37,313 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0012335538376642553, 'batch_size': 128, 'epochs': 29, 'd_model': 38, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3.4108487070361657, 'dropout': 0.24156090620477688, 'weight_decay': 5.766739245797547e-06, 'patch_size': 25, 'label_smoothing': 0.1284753624288166, 'use_focal_loss': True, 'focal_gamma': 1.979004447294647, 'grad_clip_norm': 1.9784186578274159, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 36951, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:16:09,992 - INFO - _models.training_function_executor - Model: 51,970 parameters, 223.3KB storage
2025-09-23 19:16:09,992 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6289196936583731, 0.4426034144930936, 0.31822427226345973, 0.24228068273460246, 0.1988436572683846, 0.17359722768200245, 0.1596815766212886, 0.14310881305261092, 0.1353423073862152, 0.12774501502358912, 0.1208957998763022, 0.11459628128525204, 0.11029920421013971, 0.10382797919614974, 0.0983317605125345, 0.09429872911819628, 0.09004849040134469, 0.08875503614127096, 0.08441791065955416, 0.08138404151666502, 0.07898503143942617, 0.07543599802695061, 0.07431627168692001, 0.07110684335603341, 0.069273533749954, 0.06754110680064346, 0.0660667736011501, 0.06528629274039978, 0.06480139709415815], 'val_losses': [0.49813210329395863, 0.3642677985409002, 0.2409962925855983, 0.20684765656536466, 0.15508542538936418, 0.14934481978005684, 0.1360517086967865, 0.1597272487075563, 0.1229436696443906, 0.09926195551994046, 0.1035560670814864, 0.12475907248572633, 0.08913301693548864, 0.09945564708419125, 0.08588367634808908, 0.08316279589698557, 0.08128859368598099, 0.07661870024120908, 0.0716187206272336, 0.07404678459436191, 0.07532178896226437, 0.07362299124025161, 0.0668277658392367, 0.06824884796509273, 0.06919904201830737, 0.06778368721658343, 0.06624672314215836, 0.06529944971560417, 0.06631827835017406], 'val_acc': [0.7200852023555946, 0.7460218017792256, 0.8253351710311991, 0.8422503445683498, 0.891868186943992, 0.8909911038716952, 0.8993860418493923, 0.8807167021676482, 0.908658062899386, 0.9327151985966671, 0.9260744267635634, 0.9163012153865431, 0.9340934719959905, 0.9261997243453202, 0.9303345445432903, 0.9397318631750408, 0.9414860293196341, 0.9402330535020674, 0.9473750156621977, 0.9429896003007142, 0.9462473374263877, 0.9458714446811176, 0.9500062648790878, 0.947249718080441, 0.9439919809547676, 0.9463726350081444, 0.9475003132439543, 0.9486279914797644, 0.9473750156621977], 'model_size_bytes': 223379, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0012335538376642553, 'batch_size': 128, 'epochs': 29, 'd_model': 38, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3.4108487070361657, 'dropout': 0.24156090620477688, 'weight_decay': 5.766739245797547e-06, 'patch_size': 25, 'label_smoothing': 0.1284753624288166, 'use_focal_loss': True, 'focal_gamma': 1.979004447294647, 'grad_clip_norm': 1.9784186578274159, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 36951, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 51970, 'model_storage_size_kb': 223.30859375000003, 'model_size_validation': 'PASS'}
2025-09-23 19:16:09,992 - INFO - _models.training_function_executor - BO Objective: base=0.9474, size_penalty=0.0000, final=0.9474
2025-09-23 19:16:09,992 - INFO - _models.training_function_executor - Model: 51,970 parameters, 223.3KB (PASS 256KB limit)
2025-09-23 19:16:09,993 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 32.681s
2025-09-23 19:16:10,223 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9474
2025-09-23 19:16:10,223 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.231s
2025-09-23 19:16:10,223 - INFO - bo.run_bo - Recorded observation #17: hparams={'lr': 0.0012335538376642553, 'batch_size': np.int64(128), 'epochs': np.int64(29), 'd_model': np.int64(38), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': 3.4108487070361657, 'dropout': 0.24156090620477688, 'weight_decay': 5.766739245797547e-06, 'patch_size': np.int64(25), 'label_smoothing': 0.1284753624288166, 'use_focal_loss': np.True_, 'focal_gamma': 1.979004447294647, 'grad_clip_norm': 1.9784186578274159, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(36951), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9474
2025-09-23 19:16:10,223 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 17: {'lr': 0.0012335538376642553, 'batch_size': np.int64(128), 'epochs': np.int64(29), 'd_model': np.int64(38), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': 3.4108487070361657, 'dropout': 0.24156090620477688, 'weight_decay': 5.766739245797547e-06, 'patch_size': np.int64(25), 'label_smoothing': 0.1284753624288166, 'use_focal_loss': np.True_, 'focal_gamma': 1.979004447294647, 'grad_clip_norm': 1.9784186578274159, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(36951), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9474
2025-09-23 19:16:10,223 - INFO - bo.run_bo - ğŸ”BO Trial 18: Using RF surrogate + Expected Improvement
2025-09-23 19:16:10,223 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:16:10,224 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:16:10,224 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:16:10,224 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00030072431181649525, 'batch_size': 128, 'epochs': 23, 'd_model': 51, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 3.7193838658030662, 'dropout': 0.21800899545803665, 'weight_decay': 7.283601722804179e-05, 'patch_size': 10, 'label_smoothing': 0.12128653919109393, 'use_focal_loss': True, 'focal_gamma': 1.6690384211067433, 'grad_clip_norm': 1.5521018886764468, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('auto'), 'seed': 634, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:16:10,225 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00030072431181649525, 'batch_size': 128, 'epochs': 23, 'd_model': 51, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 3.7193838658030662, 'dropout': 0.21800899545803665, 'weight_decay': 7.283601722804179e-05, 'patch_size': 10, 'label_smoothing': 0.12128653919109393, 'use_focal_loss': True, 'focal_gamma': 1.6690384211067433, 'grad_clip_norm': 1.5521018886764468, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('auto'), 'seed': 634, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:17:20,728 - INFO - _models.training_function_executor - Model: 7,191 parameters, 30.9KB storage
2025-09-23 19:17:20,728 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.09745535930055643, 0.06436094159590468, 0.0550727149929031, 0.045992400814144775, 0.04200512825410191, 0.0385476518562984, 0.03642780601224912, 0.035622960005014, 0.03444893052897764, 0.032115026970891344, 0.030491564520170406, 0.0294254420643332, 0.028912483373505687, 0.028908307605544078, 0.02703641632482719, 0.02695948161738861, 0.026164357899859582, 0.02577003293458006, 0.025365681076255517, 0.025188868990760734, 0.02567415354024855, 0.024855489571917015, 0.02484859008143294], 'val_losses': [0.07202740023798317, 0.052837891350134454, 0.049349253703608445, 0.04645739442892555, 0.04434798814495425, 0.034576903662239755, 0.03329838451771506, 0.036323044935698545, 0.030855868129222343, 0.027486578637682477, 0.027890843091993058, 0.026391536534759556, 0.026678823539444587, 0.026805888498737586, 0.025529012122865853, 0.02561718713510359, 0.02496107913228843, 0.025552289714062836, 0.025242269340687386, 0.024989257069987704, 0.02493313367702249, 0.024893791552494417, 0.024942672994772437], 'val_acc': [0.0929708056634507, 0.10675353965668463, 0.12178924946748527, 0.12880591404585892, 0.13770204235058264, 0.14785114647287306, 0.15436662072422003, 0.15399072797895, 0.15837614334043354, 0.17854905400325774, 0.18393685001879465, 0.18995113394311489, 0.19721839368500188, 0.19746898884851521, 0.19345946623230173, 0.1984713695025686, 0.19997494048364867, 0.20736749780729233, 0.20060142839243203, 0.20348327277283548, 0.20060142839243203, 0.2016038090464854, 0.20147851146472873], 'model_size_bytes': 187389, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00030072431181649525, 'batch_size': 128, 'epochs': 23, 'd_model': 51, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 3.7193838658030662, 'dropout': 0.21800899545803665, 'weight_decay': 7.283601722804179e-05, 'patch_size': 10, 'label_smoothing': 0.12128653919109393, 'use_focal_loss': True, 'focal_gamma': 1.6690384211067433, 'grad_clip_norm': 1.5521018886764468, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('auto'), 'seed': 634, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 7191, 'model_storage_size_kb': 30.898828125, 'model_size_validation': 'PASS'}
2025-09-23 19:17:20,728 - INFO - _models.training_function_executor - BO Objective: base=0.2015, size_penalty=0.0000, final=0.2015
2025-09-23 19:17:20,728 - INFO - _models.training_function_executor - Model: 7,191 parameters, 30.9KB (PASS 256KB limit)
2025-09-23 19:17:20,728 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 70.505s
2025-09-23 19:17:20,830 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2015
2025-09-23 19:17:20,830 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.101s
2025-09-23 19:17:20,830 - INFO - bo.run_bo - Recorded observation #18: hparams={'lr': 0.00030072431181649525, 'batch_size': np.int64(128), 'epochs': np.int64(23), 'd_model': np.int64(51), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 3.7193838658030662, 'dropout': 0.21800899545803665, 'weight_decay': 7.283601722804179e-05, 'patch_size': np.int64(10), 'label_smoothing': 0.12128653919109393, 'use_focal_loss': np.True_, 'focal_gamma': 1.6690384211067433, 'grad_clip_norm': 1.5521018886764468, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('auto'), 'seed': np.int64(634), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.2015
2025-09-23 19:17:20,830 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 18: {'lr': 0.00030072431181649525, 'batch_size': np.int64(128), 'epochs': np.int64(23), 'd_model': np.int64(51), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 3.7193838658030662, 'dropout': 0.21800899545803665, 'weight_decay': 7.283601722804179e-05, 'patch_size': np.int64(10), 'label_smoothing': 0.12128653919109393, 'use_focal_loss': np.True_, 'focal_gamma': 1.6690384211067433, 'grad_clip_norm': 1.5521018886764468, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('auto'), 'seed': np.int64(634), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.2015
2025-09-23 19:17:20,830 - INFO - bo.run_bo - ğŸ”BO Trial 19: Using RF surrogate + Expected Improvement
2025-09-23 19:17:20,830 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:17:20,830 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:17:20,830 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:17:20,830 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007608089254491944, 'batch_size': 128, 'epochs': 15, 'd_model': 70, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 1.6992633810546987, 'dropout': 0.15346683822950966, 'weight_decay': 1.6262563018009313e-06, 'patch_size': 100, 'label_smoothing': 0.07674740545555102, 'use_focal_loss': False, 'focal_gamma': 1.283945474817634, 'grad_clip_norm': 1.9152857692583614, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 24012, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:17:20,832 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007608089254491944, 'batch_size': 128, 'epochs': 15, 'd_model': 70, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 1.6992633810546987, 'dropout': 0.15346683822950966, 'weight_decay': 1.6262563018009313e-06, 'patch_size': 100, 'label_smoothing': 0.07674740545555102, 'use_focal_loss': False, 'focal_gamma': 1.283945474817634, 'grad_clip_norm': 1.9152857692583614, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 24012, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:17:32,349 - INFO - _models.training_function_executor - Model: 15,890 parameters, 68.3KB storage
2025-09-23 19:17:32,349 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8290021215652353, 0.6699374129149832, 0.5937544402650315, 0.5608517779513185, 0.5397346956375955, 0.5261376697551531, 0.5087763318848058, 0.4954797587844172, 0.488719333370381, 0.48237727708813244, 0.4741118724840654, 0.4675289662515475, 0.46409478084270966, 0.4600912271143294, 0.45807837321899203], 'val_losses': [0.707667852132157, 0.5767582265293519, 0.5208286921297782, 0.508148575102442, 0.5033234705558741, 0.48793199484008637, 0.47025128020527457, 0.4756235411525325, 0.47856602750465604, 0.45170082112494425, 0.4661140186627427, 0.44540196964875484, 0.4412753987231659, 0.43879605748124356, 0.44057786421374917], 'val_acc': [0.8165643403082321, 0.8927452700162887, 0.9190577621851899, 0.9253226412730234, 0.9260744267635634, 0.9313369251973437, 0.938478887357474, 0.9376018042851773, 0.9354717453953139, 0.9456208495176043, 0.9359729357223405, 0.9481268011527377, 0.9480015035709811, 0.9523869189324646, 0.9482520987344945], 'model_size_bytes': 204153, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007608089254491944, 'batch_size': 128, 'epochs': 15, 'd_model': 70, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 1.6992633810546987, 'dropout': 0.15346683822950966, 'weight_decay': 1.6262563018009313e-06, 'patch_size': 100, 'label_smoothing': 0.07674740545555102, 'use_focal_loss': False, 'focal_gamma': 1.283945474817634, 'grad_clip_norm': 1.9152857692583614, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 24012, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 15890, 'model_storage_size_kb': 68.27734375, 'model_size_validation': 'PASS'}
2025-09-23 19:17:32,349 - INFO - _models.training_function_executor - BO Objective: base=0.9483, size_penalty=0.0000, final=0.9483
2025-09-23 19:17:32,349 - INFO - _models.training_function_executor - Model: 15,890 parameters, 68.3KB (PASS 256KB limit)
2025-09-23 19:17:32,349 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 11.519s
2025-09-23 19:17:32,452 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9483
2025-09-23 19:17:32,452 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.103s
2025-09-23 19:17:32,452 - INFO - bo.run_bo - Recorded observation #19: hparams={'lr': 0.0007608089254491944, 'batch_size': np.int64(128), 'epochs': np.int64(15), 'd_model': np.int64(70), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 1.6992633810546987, 'dropout': 0.15346683822950966, 'weight_decay': 1.6262563018009313e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.07674740545555102, 'use_focal_loss': np.False_, 'focal_gamma': 1.283945474817634, 'grad_clip_norm': 1.9152857692583614, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(24012), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9483
2025-09-23 19:17:32,452 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 19: {'lr': 0.0007608089254491944, 'batch_size': np.int64(128), 'epochs': np.int64(15), 'd_model': np.int64(70), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 1.6992633810546987, 'dropout': 0.15346683822950966, 'weight_decay': 1.6262563018009313e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.07674740545555102, 'use_focal_loss': np.False_, 'focal_gamma': 1.283945474817634, 'grad_clip_norm': 1.9152857692583614, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(24012), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9483
2025-09-23 19:17:32,453 - INFO - bo.run_bo - ğŸ”BO Trial 20: Using RF surrogate + Expected Improvement
2025-09-23 19:17:32,453 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:17:32,453 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:17:32,453 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:17:32,453 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001012809412516056, 'batch_size': 32, 'epochs': 23, 'd_model': 45, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 2.3395492179845054, 'dropout': 0.21040946794631887, 'weight_decay': 2.748447073383088e-06, 'patch_size': 50, 'label_smoothing': 0.13748780780707026, 'use_focal_loss': True, 'focal_gamma': 1.1269681703877588, 'grad_clip_norm': 1.0083738131101287, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 20020, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:17:32,454 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001012809412516056, 'batch_size': 32, 'epochs': 23, 'd_model': 45, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 2.3395492179845054, 'dropout': 0.21040946794631887, 'weight_decay': 2.748447073383088e-06, 'patch_size': 50, 'label_smoothing': 0.13748780780707026, 'use_focal_loss': True, 'focal_gamma': 1.1269681703877588, 'grad_clip_norm': 1.0083738131101287, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 20020, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:18:42,869 - INFO - _models.training_function_executor - Model: 6,345 parameters, 27.3KB storage
2025-09-23 19:18:42,869 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3726465843819334, 0.25777929376504927, 0.20804351540401692, 0.18450610955236954, 0.16665422643110037, 0.15334210638239845, 0.1431072221152433, 0.13326116331946009, 0.12711590158841524, 0.12296755270747073, 0.11558509278551542, 0.10848782427920078, 0.10476457465947546, 0.09964110575800973, 0.0933130068685056, 0.09151059962512444, 0.0872769411344619, 0.08400572539500806, 0.08045511259749745, 0.08037904932435878, 0.0772373576921048, 0.07924278934188898, 0.0774851558611618], 'val_losses': [0.27293412614127865, 0.19204852560855043, 0.17716167428326657, 0.13858985109657307, 0.12823535378659678, 0.11929812618911588, 0.11603530638070203, 0.10382495341686442, 0.099509713201576, 0.09618975353092207, 0.0885249903070129, 0.09318997103255534, 0.08226982813037707, 0.08387728341592668, 0.07809854152369318, 0.07252901879368778, 0.07358344280250934, 0.07382630971640203, 0.06843499593784928, 0.06903483451501258, 0.06922491327956899, 0.06813263361383211, 0.06739770178376785], 'val_acc': [0.8531512341811803, 0.8987595539406089, 0.9129181806791129, 0.9305851397068037, 0.9372259115399073, 0.9379776970304473, 0.9408595414108508, 0.9495050745520611, 0.9510086455331412, 0.9523869189324646, 0.953389299586518, 0.9523869189324646, 0.9591529883473249, 0.9568976318757048, 0.9599047738378649, 0.9627866182182684, 0.9612830472371883, 0.9610324520736749, 0.9647913795263752, 0.9654178674351584, 0.9651672722716451, 0.9652925698534018, 0.9655431650169152], 'model_size_bytes': 132605, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001012809412516056, 'batch_size': 32, 'epochs': 23, 'd_model': 45, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 2.3395492179845054, 'dropout': 0.21040946794631887, 'weight_decay': 2.748447073383088e-06, 'patch_size': 50, 'label_smoothing': 0.13748780780707026, 'use_focal_loss': True, 'focal_gamma': 1.1269681703877588, 'grad_clip_norm': 1.0083738131101287, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 20020, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 6345, 'model_storage_size_kb': 27.263671875000004, 'model_size_validation': 'PASS'}
2025-09-23 19:18:42,869 - INFO - _models.training_function_executor - BO Objective: base=0.9655, size_penalty=0.0000, final=0.9655
2025-09-23 19:18:42,869 - INFO - _models.training_function_executor - Model: 6,345 parameters, 27.3KB (PASS 256KB limit)
2025-09-23 19:18:42,869 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 70.416s
2025-09-23 19:18:42,971 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9655
2025-09-23 19:18:42,971 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.103s
2025-09-23 19:18:42,972 - INFO - bo.run_bo - Recorded observation #20: hparams={'lr': 0.001012809412516056, 'batch_size': np.int64(32), 'epochs': np.int64(23), 'd_model': np.int64(45), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 2.3395492179845054, 'dropout': 0.21040946794631887, 'weight_decay': 2.748447073383088e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.13748780780707026, 'use_focal_loss': np.True_, 'focal_gamma': 1.1269681703877588, 'grad_clip_norm': 1.0083738131101287, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(20020), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9655
2025-09-23 19:18:42,972 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 20: {'lr': 0.001012809412516056, 'batch_size': np.int64(32), 'epochs': np.int64(23), 'd_model': np.int64(45), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 2.3395492179845054, 'dropout': 0.21040946794631887, 'weight_decay': 2.748447073383088e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.13748780780707026, 'use_focal_loss': np.True_, 'focal_gamma': 1.1269681703877588, 'grad_clip_norm': 1.0083738131101287, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(20020), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9655
2025-09-23 19:18:42,972 - INFO - bo.run_bo - ğŸ”BO Trial 21: Using RF surrogate + Expected Improvement
2025-09-23 19:18:42,972 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:18:42,972 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:18:42,972 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:18:42,972 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0030820877128057656, 'batch_size': 128, 'epochs': 40, 'd_model': 66, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 1.9296560888910568, 'dropout': 0.1597547528998792, 'weight_decay': 2.4737289037535917e-06, 'patch_size': 100, 'label_smoothing': 0.11124350253669368, 'use_focal_loss': False, 'focal_gamma': 1.1505371800446618, 'grad_clip_norm': 1.6067792459953287, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 59584, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:18:42,973 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0030820877128057656, 'batch_size': 128, 'epochs': 40, 'd_model': 66, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 1.9296560888910568, 'dropout': 0.1597547528998792, 'weight_decay': 2.4737289037535917e-06, 'patch_size': 100, 'label_smoothing': 0.11124350253669368, 'use_focal_loss': False, 'focal_gamma': 1.1505371800446618, 'grad_clip_norm': 1.6067792459953287, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 59584, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:19:14,686 - INFO - _models.training_function_executor - Model: 14,982 parameters, 64.4KB storage
2025-09-23 19:19:14,686 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0076429702821645, 0.8197397992761273, 0.7157901330418863, 0.6622500359868285, 0.6336372744987413, 0.6200304714856588, 0.6112055443912298, 0.6030286612268103, 0.5936430312784842, 0.586168466420773, 0.5834083701757412, 0.5684279406186098, 0.5633440940962569, 0.554206957693249, 0.5533266781666477, 0.5462876294794654, 0.5449817374703864, 0.5359901426650526, 0.532058969091974, 0.5299423094049408, 0.5249728410124417, 0.5193410383834578, 0.5183534007455884, 0.5139013000078392, 0.5135523669310345, 0.50685481838358, 0.5032986619006242, 0.500428548543834, 0.49751187283880066, 0.4922035840957777, 0.49191305018440734, 0.4904410517821055, 0.48562671189483975, 0.4844692858555739, 0.4842333796608886, 0.4830554213544179, 0.48164071136558617, 0.47962558048805026, 0.47960520079869084, 0.47866289098732634], 'val_losses': [0.8867982739629101, 0.7177059299858303, 0.6383292052459334, 0.612714177682994, 0.5964949597483096, 0.5657414116143672, 0.5812534631314306, 0.5922991850967321, 0.5754686915296433, 0.5400904746540147, 0.5536174397094553, 0.5376282142354587, 0.5320380195506667, 0.5345116317190035, 0.5414321335869734, 0.5327915000917021, 0.5303926228386554, 0.5178450048007159, 0.522871908534665, 0.5128679489928101, 0.5117281884526328, 0.5159196553486001, 0.5116964148752164, 0.5121202367377631, 0.502369824835657, 0.5027825084489593, 0.49935288500059904, 0.502660910709208, 0.49745822515184157, 0.49567448790398116, 0.4961199825336694, 0.4931387348127849, 0.49400254722571196, 0.49185177419825227, 0.4922416136312897, 0.4915823235262039, 0.4906192310037746, 0.4913120305996732, 0.4909630710357802, 0.49098727021535793], 'val_acc': [0.7755920310738003, 0.8565342688886104, 0.9015161007392557, 0.9173035960405964, 0.9219396065655933, 0.9349705550682872, 0.9318381155243703, 0.92544793885478, 0.931587520360857, 0.9486279914797644, 0.9452449567723343, 0.9486279914797644, 0.9520110261871946, 0.9480015035709811, 0.9482520987344945, 0.9508833479513845, 0.9540157874953014, 0.9576494173662449, 0.9528881092594913, 0.9575241197844881, 0.9591529883473249, 0.9567723342939481, 0.9601553690013783, 0.9601553690013783, 0.9655431650169152, 0.9646660819446184, 0.9671720335797519, 0.9660443553439418, 0.9676732239067786, 0.9696779852148854, 0.9689261997243453, 0.970179175541912, 0.970179175541912, 0.970179175541912, 0.9688009021425886, 0.9693020924696153, 0.9713068537777221, 0.9705550682871821, 0.9706803658689387, 0.9704297707054254], 'model_size_bytes': 194617, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0030820877128057656, 'batch_size': 128, 'epochs': 40, 'd_model': 66, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 1.9296560888910568, 'dropout': 0.1597547528998792, 'weight_decay': 2.4737289037535917e-06, 'patch_size': 100, 'label_smoothing': 0.11124350253669368, 'use_focal_loss': False, 'focal_gamma': 1.1505371800446618, 'grad_clip_norm': 1.6067792459953287, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 59584, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 14982, 'model_storage_size_kb': 64.37578125, 'model_size_validation': 'PASS'}
2025-09-23 19:19:14,686 - INFO - _models.training_function_executor - BO Objective: base=0.9704, size_penalty=0.0000, final=0.9704
2025-09-23 19:19:14,687 - INFO - _models.training_function_executor - Model: 14,982 parameters, 64.4KB (PASS 256KB limit)
2025-09-23 19:19:14,687 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 31.715s
2025-09-23 19:19:14,791 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9704
2025-09-23 19:19:14,791 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.104s
2025-09-23 19:19:14,791 - INFO - bo.run_bo - Recorded observation #21: hparams={'lr': 0.0030820877128057656, 'batch_size': np.int64(128), 'epochs': np.int64(40), 'd_model': np.int64(66), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 1.9296560888910568, 'dropout': 0.1597547528998792, 'weight_decay': 2.4737289037535917e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.11124350253669368, 'use_focal_loss': np.False_, 'focal_gamma': 1.1505371800446618, 'grad_clip_norm': 1.6067792459953287, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(59584), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9704
2025-09-23 19:19:14,791 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 21: {'lr': 0.0030820877128057656, 'batch_size': np.int64(128), 'epochs': np.int64(40), 'd_model': np.int64(66), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 1.9296560888910568, 'dropout': 0.1597547528998792, 'weight_decay': 2.4737289037535917e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.11124350253669368, 'use_focal_loss': np.False_, 'focal_gamma': 1.1505371800446618, 'grad_clip_norm': 1.6067792459953287, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(59584), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9704
2025-09-23 19:19:14,791 - INFO - bo.run_bo - ğŸ”BO Trial 22: Using RF surrogate + Expected Improvement
2025-09-23 19:19:14,791 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:19:14,791 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:19:14,791 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:19:14,791 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005205842923052191, 'batch_size': 128, 'epochs': 34, 'd_model': 73, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 2.20135518988508, 'dropout': 0.025255215763536905, 'weight_decay': 1.0473017719674195e-06, 'patch_size': 10, 'label_smoothing': 0.11955973496937371, 'use_focal_loss': True, 'focal_gamma': 1.1731567082923111, 'grad_clip_norm': 1.4903099672075137, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42184, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:19:14,793 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005205842923052191, 'batch_size': 128, 'epochs': 34, 'd_model': 73, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 2.20135518988508, 'dropout': 0.025255215763536905, 'weight_decay': 1.0473017719674195e-06, 'patch_size': 10, 'label_smoothing': 0.11955973496937371, 'use_focal_loss': True, 'focal_gamma': 1.1731567082923111, 'grad_clip_norm': 1.4903099672075137, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42184, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:21:10,477 - INFO - _models.training_function_executor - Model: 10,293 parameters, 44.2KB storage
2025-09-23 19:21:10,477 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4795240114632023, 0.26434895142220155, 0.17881585715384507, 0.16177225842545873, 0.15452752810148832, 0.15391569221799836, 0.13916546700788546, 0.12917851763592325, 0.12185922425976747, 0.11207711388208576, 0.10126781038849163, 0.09506424413910225, 0.09120180801827257, 0.08552118176404454, 0.08332060141581041, 0.07822388408187636, 0.07345661846353413, 0.06754841986466323, 0.07134677728609054, 0.062002140996084434, 0.05648643115333319, 0.05341480804506155, 0.04868856751060469, 0.04370987142130287, 0.04204043203030852, 0.03888250669195543, 0.03375657274041277, 0.02949756865885866, 0.02656089997087837, 0.0244279001114105, 0.02141515568082955, 0.019775814931515638, 0.019232391849796422, 0.018454018676431722], 'val_losses': [0.34033792803959956, 0.19711632904399534, 0.20884311970852174, 0.1531438908919313, 0.1340828151007431, 0.13390006998766485, 0.1420612108980395, 0.1308673444606505, 0.13509711963093649, 0.12385052509205396, 0.1045145284482044, 0.09247536852147373, 0.08160373981066059, 0.09585985598289612, 0.07925016849079222, 0.07948835440378652, 0.0796460563697592, 0.07014278177106131, 0.07075991082827768, 0.06699284953631758, 0.07176089394067914, 0.061083993668761445, 0.06564214428969156, 0.06435329902834529, 0.06402099528545396, 0.06422054584448147, 0.05562364123504931, 0.05797576070365833, 0.055481908761003146, 0.05414866232331303, 0.054919789433329945, 0.05578919084407291, 0.05651470827975678, 0.05622918442285624], 'val_acc': [0.7807292319258239, 0.908658062899386, 0.8936223530885854, 0.9218143089838366, 0.9318381155243703, 0.9328404961784238, 0.9278285929081569, 0.9287056759804536, 0.930459842125047, 0.9335922816689638, 0.9456208495176043, 0.9447437664453076, 0.953765192331788, 0.9485026938980078, 0.9538904899135446, 0.9568976318757048, 0.9547675729858414, 0.9585265004385415, 0.9609071544919183, 0.9611577496554317, 0.9592782859290816, 0.9644154867811051, 0.9616589399824583, 0.9642901891993485, 0.9629119158000251, 0.9649166771081318, 0.9667961408344818, 0.9670467359979953, 0.9704297707054254, 0.9691767948878587, 0.9704297707054254, 0.9700538779601554, 0.9699285803783987, 0.9696779852148854], 'model_size_bytes': 260925, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005205842923052191, 'batch_size': 128, 'epochs': 34, 'd_model': 73, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 2.20135518988508, 'dropout': 0.025255215763536905, 'weight_decay': 1.0473017719674195e-06, 'patch_size': 10, 'label_smoothing': 0.11955973496937371, 'use_focal_loss': True, 'focal_gamma': 1.1731567082923111, 'grad_clip_norm': 1.4903099672075137, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42184, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 10293, 'model_storage_size_kb': 44.227734375000004, 'model_size_validation': 'PASS'}
2025-09-23 19:21:10,477 - INFO - _models.training_function_executor - BO Objective: base=0.9697, size_penalty=0.0000, final=0.9697
2025-09-23 19:21:10,477 - INFO - _models.training_function_executor - Model: 10,293 parameters, 44.2KB (PASS 256KB limit)
2025-09-23 19:21:10,477 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 115.686s
2025-09-23 19:21:10,582 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9697
2025-09-23 19:21:10,582 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-09-23 19:21:10,582 - INFO - bo.run_bo - Recorded observation #22: hparams={'lr': 0.005205842923052191, 'batch_size': np.int64(128), 'epochs': np.int64(34), 'd_model': np.int64(73), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 2.20135518988508, 'dropout': 0.025255215763536905, 'weight_decay': 1.0473017719674195e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.11955973496937371, 'use_focal_loss': np.True_, 'focal_gamma': 1.1731567082923111, 'grad_clip_norm': 1.4903099672075137, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(42184), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9697
2025-09-23 19:21:10,582 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 22: {'lr': 0.005205842923052191, 'batch_size': np.int64(128), 'epochs': np.int64(34), 'd_model': np.int64(73), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 2.20135518988508, 'dropout': 0.025255215763536905, 'weight_decay': 1.0473017719674195e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.11955973496937371, 'use_focal_loss': np.True_, 'focal_gamma': 1.1731567082923111, 'grad_clip_norm': 1.4903099672075137, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(42184), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9697
2025-09-23 19:21:10,583 - INFO - bo.run_bo - ğŸ”BO Trial 23: Using RF surrogate + Expected Improvement
2025-09-23 19:21:10,583 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:21:10,583 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:21:10,583 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:21:10,583 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.007207743144938571, 'batch_size': 64, 'epochs': 59, 'd_model': 58, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 2.1614864438579526, 'dropout': 0.3780279960276556, 'weight_decay': 1.629287774149388e-05, 'patch_size': 25, 'label_smoothing': 0.0831466879483132, 'use_focal_loss': True, 'focal_gamma': 1.1869127904080639, 'grad_clip_norm': 0.4964436672125857, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 23143, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:21:10,584 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.007207743144938571, 'batch_size': 64, 'epochs': 59, 'd_model': 58, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 2.1614864438579526, 'dropout': 0.3780279960276556, 'weight_decay': 1.629287774149388e-05, 'patch_size': 25, 'label_smoothing': 0.0831466879483132, 'use_focal_loss': True, 'focal_gamma': 1.1869127904080639, 'grad_clip_norm': 0.4964436672125857, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 23143, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:22:35,431 - INFO - _models.training_function_executor - Model: 6,206 parameters, 26.7KB storage
2025-09-23 19:22:35,431 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4858522690031846, 0.35140323421954583, 0.30214763651873144, 0.27916446943851886, 0.2693290912296167, 0.25856062443280725, 0.25880287596744966, 0.24654178609040822, 0.24247932467037017, 0.23238018965949295, 0.22598294500524463, 0.2115348550913679, 0.20585837777683813, 0.2007886742143685, 0.19503573968754942, 0.19055744479914916, 0.18519015475223974, 0.18246040230006783, 0.18036834179890154, 0.17773672428851595, 0.17078567045113163, 0.1670649861103236, 0.1665103210962678, 0.16320713595010036, 0.1620146910923206, 0.1532579237228833, 0.15169319056045202, 0.15063332827513265, 0.1478872099809676, 0.1426945437594613, 0.14271452855241182, 0.13681088218663423, 0.13409951082722374, 0.13071461456126446, 0.12901705069004082, 0.12853041364148615, 0.12366444351419345, 0.12175911541941717, 0.11549732665171417, 0.11823913340857314, 0.11423736369572864, 0.10960804066679902, 0.10636921062856876, 0.10437586418022858, 0.103381916826694, 0.09931198341862224, 0.0992564335861031, 0.09603167205271303, 0.09189423718473515, 0.09302235898400064, 0.0895764511425767, 0.08789166105687721, 0.08568947076147722, 0.08664510334430174, 0.08518083693646168, 0.08434513471564344, 0.0827801467758206, 0.08364214186263992, 0.08382403535732623], 'val_losses': [0.3864304847476656, 0.29991336940202984, 0.2561939964793243, 0.2680733121213661, 0.27736899317199615, 0.21691578015070812, 0.20755733954377706, 0.2360846162563039, 0.22933394045791536, 0.20944852477352416, 0.19044958261751752, 0.18340428425691105, 0.20344648720614184, 0.1990851467782663, 0.21242429496847162, 0.16234531079704503, 0.14928466681745278, 0.19459530495269609, 0.15892150522150308, 0.14185337570367892, 0.19103492943173497, 0.1410767405122694, 0.15868789449991358, 0.1454930320656229, 0.16472624349802856, 0.14866295361665238, 0.15268211063688866, 0.14283343655583652, 0.14401750781539163, 0.13926229769355364, 0.12945511944913138, 0.13587679990478932, 0.1462981506202838, 0.12813028139423388, 0.13242621623355857, 0.11548903193793908, 0.13712910741899953, 0.13697600676393976, 0.11303344642461773, 0.12078994205493428, 0.12639763995458003, 0.1043878923576475, 0.10008951935993222, 0.09539349301445232, 0.10063882377766499, 0.08982835168688985, 0.10522174466643593, 0.09453138424245326, 0.09954694393485908, 0.10486180641951082, 0.1013441060469826, 0.10224835322104425, 0.10150111093790426, 0.10283876780377132, 0.10167409381222985, 0.10564790607197005, 0.1037973459474411, 0.10320443428876391, 0.10326539496814767], 'val_acc': [0.7740884600927203, 0.8159378523994487, 0.8633003383034707, 0.8502693898007768, 0.838742012279163, 0.8941235434156121, 0.9043979451196592, 0.8884851522365619, 0.8886104498183185, 0.8876080691642652, 0.9000125297581757, 0.9026437789750658, 0.8965041974689888, 0.8943741385791254, 0.8958777095602055, 0.9126675855155996, 0.9267009146723468, 0.900263124921689, 0.9150482395689763, 0.9270768074176169, 0.9119158000250596, 0.9279538904899135, 0.916677108131813, 0.936975316376394, 0.9216890114020799, 0.9263250219270768, 0.9245708557824834, 0.9285803783986969, 0.9314622227791004, 0.930459842125047, 0.9372259115399073, 0.9278285929081569, 0.9290815687257236, 0.9362235308858539, 0.9270768074176169, 0.9438666833730109, 0.9383535897757174, 0.938478887357474, 0.9437413857912542, 0.9345946623230171, 0.9382282921939606, 0.9448690640270643, 0.9503821576243578, 0.9530134068412479, 0.9503821576243578, 0.9538904899135446, 0.9487532890615211, 0.9522616213507079, 0.9517604310236812, 0.9511339431148979, 0.9523869189324646, 0.9488785866432777, 0.9512592406966546, 0.9532640020047614, 0.9532640020047614, 0.9502568600426011, 0.952637514095978, 0.9523869189324646, 0.9523869189324646], 'model_size_bytes': 139513, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.007207743144938571, 'batch_size': 64, 'epochs': 59, 'd_model': 58, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 2.1614864438579526, 'dropout': 0.3780279960276556, 'weight_decay': 1.629287774149388e-05, 'patch_size': 25, 'label_smoothing': 0.0831466879483132, 'use_focal_loss': True, 'focal_gamma': 1.1869127904080639, 'grad_clip_norm': 0.4964436672125857, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 23143, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 6206, 'model_storage_size_kb': 26.66640625, 'model_size_validation': 'PASS'}
2025-09-23 19:22:35,432 - INFO - _models.training_function_executor - BO Objective: base=0.9524, size_penalty=0.0000, final=0.9524
2025-09-23 19:22:35,432 - INFO - _models.training_function_executor - Model: 6,206 parameters, 26.7KB (PASS 256KB limit)
2025-09-23 19:22:35,432 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 84.849s
2025-09-23 19:22:35,537 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9524
2025-09-23 19:22:35,537 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.106s
2025-09-23 19:22:35,537 - INFO - bo.run_bo - Recorded observation #23: hparams={'lr': 0.007207743144938571, 'batch_size': np.int64(64), 'epochs': np.int64(59), 'd_model': np.int64(58), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 2.1614864438579526, 'dropout': 0.3780279960276556, 'weight_decay': 1.629287774149388e-05, 'patch_size': np.int64(25), 'label_smoothing': 0.0831466879483132, 'use_focal_loss': np.True_, 'focal_gamma': 1.1869127904080639, 'grad_clip_norm': 0.4964436672125857, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(23143), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9524
2025-09-23 19:22:35,537 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 23: {'lr': 0.007207743144938571, 'batch_size': np.int64(64), 'epochs': np.int64(59), 'd_model': np.int64(58), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 2.1614864438579526, 'dropout': 0.3780279960276556, 'weight_decay': 1.629287774149388e-05, 'patch_size': np.int64(25), 'label_smoothing': 0.0831466879483132, 'use_focal_loss': np.True_, 'focal_gamma': 1.1869127904080639, 'grad_clip_norm': 0.4964436672125857, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(23143), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9524
2025-09-23 19:22:35,538 - INFO - bo.run_bo - ğŸ”BO Trial 24: Using RF surrogate + Expected Improvement
2025-09-23 19:22:35,538 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:22:35,538 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:22:35,538 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:22:35,538 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.000420629716996232, 'batch_size': 128, 'epochs': 53, 'd_model': 60, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 2.143488649155732, 'dropout': 0.14347246274225886, 'weight_decay': 7.63001415097577e-05, 'patch_size': 10, 'label_smoothing': 0.13753663952659145, 'use_focal_loss': False, 'focal_gamma': 1.0681802900743391, 'grad_clip_norm': 0.8871569658801739, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 15021, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:22:35,539 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.000420629716996232, 'batch_size': 128, 'epochs': 53, 'd_model': 60, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 2.143488649155732, 'dropout': 0.14347246274225886, 'weight_decay': 7.63001415097577e-05, 'patch_size': 10, 'label_smoothing': 0.13753663952659145, 'use_focal_loss': False, 'focal_gamma': 1.0681802900743391, 'grad_clip_norm': 0.8871569658801739, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 15021, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:24:13,114 - INFO - _models.training_function_executor - Model: 8,220 parameters, 35.3KB storage
2025-09-23 19:24:13,115 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.2219291444455562, 1.0829295369163698, 1.0281366163098797, 0.9441359760401052, 0.8967461114172776, 0.8511306693338606, 0.8114844786640217, 0.7650864896210267, 0.7365684087304167, 0.7142777532368036, 0.6954545905666013, 0.6783447021067271, 0.6647636297184307, 0.6567816319547941, 0.647369200650886, 0.6414570159318808, 0.6336706641831464, 0.6303138046961017, 0.6239244523821545, 0.6225691402787101, 0.6182301522899539, 0.6151967516101676, 0.6100020105369826, 0.6073558887167054, 0.6052240796175509, 0.6038150781285317, 0.6011766007220436, 0.599818685139323, 0.5974188519555577, 0.5935584697844977, 0.5917001250927428, 0.590890469919062, 0.5857056975009929, 0.5853569655588231, 0.5835735560811575, 0.5834919763952741, 0.5801925597041336, 0.5793217143382703, 0.5779898023764427, 0.5752405829064116, 0.576125971981111, 0.5738293001109024, 0.5730886803258725, 0.5719584452793368, 0.5713068061584003, 0.5715106324706203, 0.5712574914340981, 0.5698285867269186, 0.5689131502586398, 0.5682445492238227, 0.5683008918385642, 0.5675992931454932, 0.5677131003344361], 'val_losses': [1.0944109617386706, 1.0604017081303698, 0.9700561029811563, 0.9154886296183494, 0.8537203832432047, 0.8174230986976218, 0.7470674710365752, 0.7618100623590967, 0.7403828548602391, 0.6923912184278165, 0.6755197562082207, 0.6645775715027099, 0.6464214273494462, 0.646058761308761, 0.6253031232485181, 0.626337789067229, 0.6241299334048568, 0.6160960094834044, 0.6242702769211489, 0.6134853649148366, 0.6111107666216732, 0.6037147313472159, 0.6073443264594399, 0.6041812537054327, 0.6037877684113914, 0.5985627115228364, 0.5971265723197148, 0.5975117470264854, 0.5976628650924105, 0.589768707804506, 0.5955800887502056, 0.5918404667647228, 0.5884430557097309, 0.5913981492470686, 0.5917045052167154, 0.5889873602174903, 0.5828649580082848, 0.5882549638766691, 0.5869047828387414, 0.5859135379171091, 0.5815950502893438, 0.582733272341107, 0.5831771804162874, 0.5819302754748094, 0.5794158254018061, 0.5831441743573126, 0.5812385788299643, 0.5812970122557926, 0.5812561392724372, 0.5816283440951371, 0.5804825378172291, 0.5805028702080243, 0.5804211629122634], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.753915549429896, 0.7807292319258239, 0.822328029069039, 0.8356095727352462, 0.8824708683122415, 0.870066407718331, 0.8903646159629119, 0.9104122290439794, 0.9183059766946498, 0.92319258238316, 0.9310863300338303, 0.9302092469615336, 0.9402330535020674, 0.9386041849392307, 0.9409848389926074, 0.946497932589901, 0.9419872196466608, 0.945746147099361, 0.9477509084074678, 0.9482520987344945, 0.9501315624608445, 0.9490038842250345, 0.9508833479513845, 0.9525122165142212, 0.953765192331788, 0.953765192331788, 0.9528881092594913, 0.9572735246209748, 0.9555193584763814, 0.9548928705675981, 0.9557699536398947, 0.9561458463851648, 0.9553940608946248, 0.9572735246209748, 0.960656559328405, 0.9562711439669215, 0.9582759052750282, 0.9587770956020549, 0.961408344818945, 0.960656559328405, 0.9604059641648917, 0.9612830472371883, 0.9622854278912417, 0.9596541786743515, 0.9604059641648917, 0.9609071544919183, 0.9605312617466483, 0.960656559328405, 0.961408344818945, 0.9609071544919183, 0.9609071544919183], 'model_size_bytes': 153401, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.000420629716996232, 'batch_size': 128, 'epochs': 53, 'd_model': 60, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 2.143488649155732, 'dropout': 0.14347246274225886, 'weight_decay': 7.63001415097577e-05, 'patch_size': 10, 'label_smoothing': 0.13753663952659145, 'use_focal_loss': False, 'focal_gamma': 1.0681802900743391, 'grad_clip_norm': 0.8871569658801739, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 15021, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 8220, 'model_storage_size_kb': 35.3203125, 'model_size_validation': 'PASS'}
2025-09-23 19:24:13,115 - INFO - _models.training_function_executor - BO Objective: base=0.9609, size_penalty=0.0000, final=0.9609
2025-09-23 19:24:13,115 - INFO - _models.training_function_executor - Model: 8,220 parameters, 35.3KB (PASS 256KB limit)
2025-09-23 19:24:13,115 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 97.577s
2025-09-23 19:24:13,220 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9609
2025-09-23 19:24:13,220 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-09-23 19:24:13,220 - INFO - bo.run_bo - Recorded observation #24: hparams={'lr': 0.000420629716996232, 'batch_size': np.int64(128), 'epochs': np.int64(53), 'd_model': np.int64(60), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': 2.143488649155732, 'dropout': 0.14347246274225886, 'weight_decay': 7.63001415097577e-05, 'patch_size': np.int64(10), 'label_smoothing': 0.13753663952659145, 'use_focal_loss': np.False_, 'focal_gamma': 1.0681802900743391, 'grad_clip_norm': 0.8871569658801739, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(15021), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9609
2025-09-23 19:24:13,220 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 24: {'lr': 0.000420629716996232, 'batch_size': np.int64(128), 'epochs': np.int64(53), 'd_model': np.int64(60), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': 2.143488649155732, 'dropout': 0.14347246274225886, 'weight_decay': 7.63001415097577e-05, 'patch_size': np.int64(10), 'label_smoothing': 0.13753663952659145, 'use_focal_loss': np.False_, 'focal_gamma': 1.0681802900743391, 'grad_clip_norm': 0.8871569658801739, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(15021), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9609
2025-09-23 19:24:13,221 - INFO - bo.run_bo - ğŸ”BO Trial 25: Using RF surrogate + Expected Improvement
2025-09-23 19:24:13,221 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:24:13,221 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:24:13,221 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:24:13,221 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004052400926695513, 'batch_size': 128, 'epochs': 54, 'd_model': 50, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 3.0350384340740897, 'dropout': 0.06958043155058526, 'weight_decay': 4.593299924493083e-06, 'patch_size': 100, 'label_smoothing': 0.11321607075539253, 'use_focal_loss': False, 'focal_gamma': 1.1659436482587728, 'grad_clip_norm': 0.8260317390211169, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 11115, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:24:13,222 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004052400926695513, 'batch_size': 128, 'epochs': 54, 'd_model': 50, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 3.0350384340740897, 'dropout': 0.06958043155058526, 'weight_decay': 4.593299924493083e-06, 'patch_size': 100, 'label_smoothing': 0.11321607075539253, 'use_focal_loss': False, 'focal_gamma': 1.1659436482587728, 'grad_clip_norm': 0.8260317390211169, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 11115, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:24:48,744 - INFO - _models.training_function_executor - Model: 62,419 parameters, 268.2KB storage
2025-09-23 19:24:48,744 - WARNING - _models.training_function_executor - Model storage 268.2KB exceeds 256KB limit!
2025-09-23 19:24:48,744 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0171645500231024, 0.8278330824180448, 0.7447939526559386, 0.7018047792624812, 0.6783664696415919, 0.650641080128707, 0.6392772246623731, 0.6258834334680192, 0.6163928071624208, 0.6039295728891575, 0.5967690560564012, 0.5916699488741135, 0.5792255018680006, 0.5690698803966738, 0.5627088725475374, 0.5574893873556364, 0.5531573823619874, 0.54883071123439, 0.5438599827982439, 0.5393930008263306, 0.5381933427994455, 0.5332713664946646, 0.5300629182058748, 0.5249316687245673, 0.523321952722509, 0.5183162855421605, 0.5185390546811955, 0.5165221342088824, 0.5127442891042909, 0.5125546253200268, 0.5052318772669779, 0.5058630125002728, 0.5015091906718486, 0.5005325702812365, 0.49690084455268924, 0.49637906982875335, 0.491330651237945, 0.4909132542819437, 0.4880395873966094, 0.4858518549840156, 0.4842407463827188, 0.48281657550693646, 0.4829486011917331, 0.4792305245639365, 0.4766989487488198, 0.47626890171568625, 0.47422523499169983, 0.47338723665553706, 0.4723277152419236, 0.47239886132453357, 0.4716097629335789, 0.4703082551977985, 0.47093171681806756, 0.46990777759520186], 'val_losses': [0.8765345995713858, 0.7498552154858389, 0.7399704757601288, 0.642217644838862, 0.6177455215522213, 0.6037526420447002, 0.6228795033949477, 0.5823984863076678, 0.5987267104579518, 0.5856303831880036, 0.5839189984993802, 0.5642897414019974, 0.5503901628201624, 0.5498999073875531, 0.5542197498876698, 0.5480537052860147, 0.5333471777982512, 0.5287637438425786, 0.5261989045883985, 0.5390942676401037, 0.5281446275443993, 0.5246505867328102, 0.53063204256817, 0.5188250734705686, 0.512126603792401, 0.5153563962277278, 0.5130370987819379, 0.5145552792714632, 0.5199708177128944, 0.507735788090279, 0.505083015726527, 0.5039072151381843, 0.4989871419532125, 0.5054460738987093, 0.4977321624979909, 0.49810331016536225, 0.4978423077366194, 0.49655097075504046, 0.49520903154358115, 0.492592188308538, 0.49036120126426946, 0.4941102767659823, 0.49428861468466523, 0.49072063831051643, 0.49057951444462866, 0.4881152896979088, 0.4894861136742245, 0.4904990318260341, 0.4881895764879887, 0.48871374646309784, 0.48842275761046394, 0.48862612382643594, 0.4886205055080041, 0.4886065531667201], 'val_acc': [0.7839869690514973, 0.8451321889487533, 0.8522741511088836, 0.9052750281919559, 0.914797644405463, 0.9255732364365368, 0.914797644405463, 0.9340934719959905, 0.9238190702919433, 0.9333416865054505, 0.9342187695777472, 0.9423631123919308, 0.9471244204986844, 0.9453702543540909, 0.9451196591905776, 0.9468738253351711, 0.9538904899135446, 0.9553940608946248, 0.9565217391304348, 0.9492544793885478, 0.9556446560581381, 0.9558952512216514, 0.9557699536398947, 0.9570229294574615, 0.9624107254729983, 0.9624107254729983, 0.9630372133817817, 0.9637889988723217, 0.9615336424007017, 0.9647913795263752, 0.9657937601804285, 0.9661696529256986, 0.9677985214885353, 0.9665455456709685, 0.969051497306102, 0.9708056634506954, 0.970179175541912, 0.9705550682871821, 0.9715574489412354, 0.9725598295952889, 0.9725598295952889, 0.9706803658689387, 0.9718080441047487, 0.9724345320135321, 0.9736875078310988, 0.9738128054128555, 0.9740634005763689, 0.9740634005763689, 0.9753163763939356, 0.9741886981581256, 0.9744392933216389, 0.9741886981581256, 0.9741886981581256, 0.9743139957398822], 'model_size_bytes': 261431, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004052400926695513, 'batch_size': 128, 'epochs': 54, 'd_model': 50, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 3.0350384340740897, 'dropout': 0.06958043155058526, 'weight_decay': 4.593299924493083e-06, 'patch_size': 100, 'label_smoothing': 0.11321607075539253, 'use_focal_loss': False, 'focal_gamma': 1.1659436482587728, 'grad_clip_norm': 0.8260317390211169, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 11115, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 62419, 'model_storage_size_kb': 268.20664062500003, 'model_size_validation': 'FAIL'}
2025-09-23 19:24:48,744 - INFO - _models.training_function_executor - BO Objective: base=0.9743, size_penalty=0.0238, final=0.9505
2025-09-23 19:24:48,744 - INFO - _models.training_function_executor - Model: 62,419 parameters, 268.2KB (FAIL 256KB limit)
2025-09-23 19:24:48,744 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 35.524s
2025-09-23 19:24:48,851 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9505
2025-09-23 19:24:48,851 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.106s
2025-09-23 19:24:48,851 - INFO - bo.run_bo - Recorded observation #25: hparams={'lr': 0.004052400926695513, 'batch_size': np.int64(128), 'epochs': np.int64(54), 'd_model': np.int64(50), 'n_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': 3.0350384340740897, 'dropout': 0.06958043155058526, 'weight_decay': 4.593299924493083e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.11321607075539253, 'use_focal_loss': np.False_, 'focal_gamma': 1.1659436482587728, 'grad_clip_norm': 0.8260317390211169, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(11115), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9505
2025-09-23 19:24:48,851 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 25: {'lr': 0.004052400926695513, 'batch_size': np.int64(128), 'epochs': np.int64(54), 'd_model': np.int64(50), 'n_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': 3.0350384340740897, 'dropout': 0.06958043155058526, 'weight_decay': 4.593299924493083e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.11321607075539253, 'use_focal_loss': np.False_, 'focal_gamma': 1.1659436482587728, 'grad_clip_norm': 0.8260317390211169, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(11115), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9505
2025-09-23 19:24:48,851 - INFO - bo.run_bo - ğŸ”BO Trial 26: Using RF surrogate + Expected Improvement
2025-09-23 19:24:48,852 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:24:48,852 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:24:48,852 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:24:48,852 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0016585894468790462, 'batch_size': 128, 'epochs': 12, 'd_model': 34, 'n_heads': 3, 'num_layers': 1, 'mlp_ratio': 3.180671433495683, 'dropout': 0.38305113847752803, 'weight_decay': 0.0002145264095957784, 'patch_size': 100, 'label_smoothing': 0.03281197499263345, 'use_focal_loss': True, 'focal_gamma': 1.059146796735405, 'grad_clip_norm': 0.042259387558738044, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 8444, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:24:48,853 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0016585894468790462, 'batch_size': 128, 'epochs': 12, 'd_model': 34, 'n_heads': 3, 'num_layers': 1, 'mlp_ratio': 3.180671433495683, 'dropout': 0.38305113847752803, 'weight_decay': 0.0002145264095957784, 'patch_size': 100, 'label_smoothing': 0.03281197499263345, 'use_focal_loss': True, 'focal_gamma': 1.059146796735405, 'grad_clip_norm': 0.042259387558738044, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 8444, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:24:54,955 - INFO - _models.training_function_executor - Model: 19,879 parameters, 85.4KB storage
2025-09-23 19:24:54,955 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7983049369618636, 0.5138362269585202, 0.4863052632625482, 0.4711934655808612, 0.4638165937600621, 0.4523207269392153, 0.43955432977984243, 0.4335517391888744, 0.43044118678566434, 0.42464852338856973, 0.4206868680182642, 0.4216784194144032], 'val_losses': [0.5820494324226544, 0.511278316742674, 0.47399647864553357, 0.48502359752770935, 0.46949137598108104, 0.5069464412253537, 0.45205485712479654, 0.49859356207665967, 0.49605561327761, 0.44906264307161065, 0.4347965722954254, 0.44462270065890025], 'val_acc': [0.7164515724846511, 0.7228417491542414, 0.7377521613832853, 0.7244706177170781, 0.7308607943866683, 0.7258488911164015, 0.7552938228292194, 0.7361232928204485, 0.728354842751535, 0.7581756672096228, 0.7640646535521864, 0.7598045357724595], 'model_size_bytes': 87451, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0016585894468790462, 'batch_size': 128, 'epochs': 12, 'd_model': 34, 'n_heads': 3, 'num_layers': 1, 'mlp_ratio': 3.180671433495683, 'dropout': 0.38305113847752803, 'weight_decay': 0.0002145264095957784, 'patch_size': 100, 'label_smoothing': 0.03281197499263345, 'use_focal_loss': True, 'focal_gamma': 1.059146796735405, 'grad_clip_norm': 0.042259387558738044, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 8444, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 19879, 'model_storage_size_kb': 85.417578125, 'model_size_validation': 'PASS'}
2025-09-23 19:24:54,955 - INFO - _models.training_function_executor - BO Objective: base=0.7598, size_penalty=0.0000, final=0.7598
2025-09-23 19:24:54,955 - INFO - _models.training_function_executor - Model: 19,879 parameters, 85.4KB (PASS 256KB limit)
2025-09-23 19:24:54,955 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 6.103s
2025-09-23 19:24:55,063 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7598
2025-09-23 19:24:55,063 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.108s
2025-09-23 19:24:55,063 - INFO - bo.run_bo - Recorded observation #26: hparams={'lr': 0.0016585894468790462, 'batch_size': np.int64(128), 'epochs': np.int64(12), 'd_model': np.int64(34), 'n_heads': np.int64(3), 'num_layers': np.int64(1), 'mlp_ratio': 3.180671433495683, 'dropout': 0.38305113847752803, 'weight_decay': 0.0002145264095957784, 'patch_size': np.int64(100), 'label_smoothing': 0.03281197499263345, 'use_focal_loss': np.True_, 'focal_gamma': 1.059146796735405, 'grad_clip_norm': 0.042259387558738044, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(8444), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7598
2025-09-23 19:24:55,063 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 26: {'lr': 0.0016585894468790462, 'batch_size': np.int64(128), 'epochs': np.int64(12), 'd_model': np.int64(34), 'n_heads': np.int64(3), 'num_layers': np.int64(1), 'mlp_ratio': 3.180671433495683, 'dropout': 0.38305113847752803, 'weight_decay': 0.0002145264095957784, 'patch_size': np.int64(100), 'label_smoothing': 0.03281197499263345, 'use_focal_loss': np.True_, 'focal_gamma': 1.059146796735405, 'grad_clip_norm': 0.042259387558738044, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(8444), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7598
2025-09-23 19:24:55,063 - INFO - bo.run_bo - ğŸ”BO Trial 27: Using RF surrogate + Expected Improvement
2025-09-23 19:24:55,063 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:24:55,063 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:24:55,063 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:24:55,063 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0034497542954790954, 'batch_size': 128, 'epochs': 35, 'd_model': 40, 'n_heads': 1, 'num_layers': 4, 'mlp_ratio': 2.523170040539223, 'dropout': 0.17069574007817992, 'weight_decay': 3.269943003173989e-05, 'patch_size': 10, 'label_smoothing': 0.1268743240644723, 'use_focal_loss': True, 'focal_gamma': 1.1238771862531722, 'grad_clip_norm': 1.573212336061977, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 44148, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:24:55,065 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0034497542954790954, 'batch_size': 128, 'epochs': 35, 'd_model': 40, 'n_heads': 1, 'num_layers': 4, 'mlp_ratio': 2.523170040539223, 'dropout': 0.17069574007817992, 'weight_decay': 3.269943003173989e-05, 'patch_size': 10, 'label_smoothing': 0.1268743240644723, 'use_focal_loss': True, 'focal_gamma': 1.1238771862531722, 'grad_clip_norm': 1.573212336061977, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 44148, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:26:02,157 - INFO - _models.training_function_executor - Model: 5,640 parameters, 6.1KB storage
2025-09-23 19:26:02,158 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.37766993810960015, 0.2254992037175073, 0.19078506319558258, 0.168458127681483, 0.150413287891928, 0.14189387645130264, 0.13261818932738403, 0.12816627644716852, 0.12247757434600376, 0.12083772296055671, 0.1125713518077903, 0.11463216124015826, 0.10862466445155293, 0.10374406530169009, 0.10186124064876177, 0.09693989413264543, 0.09355002233661536, 0.09496005819765312, 0.09403581687557934, 0.08985635419735181, 0.09089477176382156, 0.08863584017935087, 0.0828602577812618, 0.08212700312072553, 0.07973771911187157, 0.08281649883498206, 0.07752967106581997, 0.07670146325467818, 0.07375806843565472, 0.07164734123310335, 0.07352200572766393, 0.07276805303121454, 0.0712520925076247, 0.06830650222564563, 0.06785477963192915], 'val_losses': [0.25958752673310215, 0.1804856971408502, 0.15364771156890725, 0.14955471830213596, 0.14796575304933426, 0.14572633250141903, 0.12649083795284302, 0.12605024499674217, 0.11892230527459373, 0.12583771286666012, 0.11797626594228787, 0.1230865571936813, 0.10465669716163292, 0.10718197470943726, 0.10097915502217251, 0.10729713374350905, 0.09668917044140225, 0.10096218718231927, 0.10546318718797357, 0.09840222918194512, 0.09111956246790022, 0.08502208944393749, 0.08585647496494818, 0.08397893948545135, 0.08493685209651054, 0.08660768656287184, 0.08770743319282794, 0.08701630199019764, 0.08415948816718291, 0.08158317049028221, 0.08092796789264488, 0.08366574552771353, 0.07969497875183869, 0.08921679584413315, 0.08266787026336567], 'val_acc': [0.869314622227791, 0.9057762185189826, 0.9274527001628868, 0.9173035960405964, 0.9295827590527502, 0.9253226412730234, 0.9389800776845006, 0.9409848389926074, 0.9382282921939606, 0.939230672848014, 0.9409848389926074, 0.9332163889236937, 0.9433654930459842, 0.9431148978824708, 0.947249718080441, 0.9429896003007142, 0.9477509084074678, 0.946122039844631, 0.9406089462473374, 0.9516351334419245, 0.9548928705675981, 0.9556446560581381, 0.9500062648790878, 0.9557699536398947, 0.9599047738378649, 0.954141085077058, 0.9543916802405713, 0.9571482270392181, 0.9553940608946248, 0.953765192331788, 0.9547675729858414, 0.9560205488034081, 0.9548928705675981, 0.9448690640270643, 0.9536398947500313], 'model_size_bytes': 117885, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0034497542954790954, 'batch_size': 128, 'epochs': 35, 'd_model': 40, 'n_heads': 1, 'num_layers': 4, 'mlp_ratio': 2.523170040539223, 'dropout': 0.17069574007817992, 'weight_decay': 3.269943003173989e-05, 'patch_size': 10, 'label_smoothing': 0.1268743240644723, 'use_focal_loss': True, 'focal_gamma': 1.1238771862531722, 'grad_clip_norm': 1.573212336061977, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 44148, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 5640, 'model_storage_size_kb': 6.058593750000001, 'model_size_validation': 'PASS'}
2025-09-23 19:26:02,158 - INFO - _models.training_function_executor - BO Objective: base=0.9536, size_penalty=0.0000, final=0.9536
2025-09-23 19:26:02,158 - INFO - _models.training_function_executor - Model: 5,640 parameters, 6.1KB (PASS 256KB limit)
2025-09-23 19:26:02,158 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 67.094s
2025-09-23 19:26:02,266 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9536
2025-09-23 19:26:02,266 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.108s
2025-09-23 19:26:02,266 - INFO - bo.run_bo - Recorded observation #27: hparams={'lr': 0.0034497542954790954, 'batch_size': np.int64(128), 'epochs': np.int64(35), 'd_model': np.int64(40), 'n_heads': np.int64(1), 'num_layers': np.int64(4), 'mlp_ratio': 2.523170040539223, 'dropout': 0.17069574007817992, 'weight_decay': 3.269943003173989e-05, 'patch_size': np.int64(10), 'label_smoothing': 0.1268743240644723, 'use_focal_loss': np.True_, 'focal_gamma': 1.1238771862531722, 'grad_clip_norm': 1.573212336061977, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(44148), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9536
2025-09-23 19:26:02,266 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 27: {'lr': 0.0034497542954790954, 'batch_size': np.int64(128), 'epochs': np.int64(35), 'd_model': np.int64(40), 'n_heads': np.int64(1), 'num_layers': np.int64(4), 'mlp_ratio': 2.523170040539223, 'dropout': 0.17069574007817992, 'weight_decay': 3.269943003173989e-05, 'patch_size': np.int64(10), 'label_smoothing': 0.1268743240644723, 'use_focal_loss': np.True_, 'focal_gamma': 1.1238771862531722, 'grad_clip_norm': 1.573212336061977, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(44148), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9536
2025-09-23 19:26:02,267 - INFO - bo.run_bo - ğŸ”BO Trial 28: Using RF surrogate + Expected Improvement
2025-09-23 19:26:02,267 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:26:02,267 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:26:02,267 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:26:02,267 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0005080002107669694, 'batch_size': 128, 'epochs': 31, 'd_model': 65, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 2.6854140687678463, 'dropout': 0.4641128162250818, 'weight_decay': 1.125714273116538e-05, 'patch_size': 10, 'label_smoothing': 0.14209961260308926, 'use_focal_loss': True, 'focal_gamma': 1.0824982211135412, 'grad_clip_norm': 1.0770003417098677, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 4606, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:26:02,268 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0005080002107669694, 'batch_size': 128, 'epochs': 31, 'd_model': 65, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 2.6854140687678463, 'dropout': 0.4641128162250818, 'weight_decay': 1.125714273116538e-05, 'patch_size': 10, 'label_smoothing': 0.14209961260308926, 'use_focal_loss': True, 'focal_gamma': 1.0824982211135412, 'grad_clip_norm': 1.0770003417098677, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 4606, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:27:46,350 - INFO - _models.training_function_executor - Model: 9,165 parameters, 9.8KB storage
2025-09-23 19:27:46,351 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5072966349459873, 0.348311052558618, 0.3016244799745444, 0.2743470697429953, 0.243464485688384, 0.22250623935629646, 0.2089430749102672, 0.19985694719059957, 0.18907673496973415, 0.1826757210918982, 0.17761530475597895, 0.1702833679884451, 0.1624066767888466, 0.15715846110789866, 0.15627305556410884, 0.15274929940743476, 0.14660848082521596, 0.14553105215830928, 0.14003835440230947, 0.13960073424784852, 0.13592221950097771, 0.13469932369319051, 0.13150866398147237, 0.13055659005645334, 0.13096016403613814, 0.12706335606382846, 0.12351145440409686, 0.12409617796951497, 0.12073316068671772, 0.11995593164223155, 0.11780939473210505], 'val_losses': [0.4188296886807679, 0.3962235096615667, 0.2917954556643029, 0.2682137740907805, 0.2596750757041439, 0.23462570002243013, 0.27704531304531715, 0.294532726495588, 0.3036848949332641, 0.2723708912186897, 0.27326505079206376, 0.29348561358159087, 0.26921821237095855, 0.3212954074577526, 0.27493316167119514, 0.27320296643797787, 0.2572437325475464, 0.26084367424887256, 0.2961292483674309, 0.27162741388261596, 0.27953632990342453, 0.26630051276738625, 0.28437973081770584, 0.23316973667648289, 0.281727579096373, 0.3277823606597244, 0.22423660953067895, 0.26481834732303194, 0.2935406283289101, 0.2555075809154172, 0.2380789681912484], 'val_acc': [0.7649417366244832, 0.7737125673474502, 0.8604184939230672, 0.876205989224408, 0.8739506327527878, 0.8813431900764315, 0.8627991479764441, 0.8655556947750909, 0.8488911164014534, 0.8668086705926575, 0.8581631374514472, 0.8552812930710437, 0.876581881969678, 0.8596667084325272, 0.8844756296203483, 0.8857286054379151, 0.8795890239318381, 0.8723217641899511, 0.8699411101365744, 0.8857286054379151, 0.8772083698784613, 0.8701917053000877, 0.877333667460218, 0.8803408094223781, 0.8739506327527878, 0.8596667084325272, 0.9028943741385791, 0.8640521237940108, 0.8541536148352337, 0.8950006264879088, 0.8812178924946749], 'model_size_bytes': 236349, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0005080002107669694, 'batch_size': 128, 'epochs': 31, 'd_model': 65, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 2.6854140687678463, 'dropout': 0.4641128162250818, 'weight_decay': 1.125714273116538e-05, 'patch_size': 10, 'label_smoothing': 0.14209961260308926, 'use_focal_loss': True, 'focal_gamma': 1.0824982211135412, 'grad_clip_norm': 1.0770003417098677, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 4606, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 9165, 'model_storage_size_kb': 9.84521484375, 'model_size_validation': 'PASS'}
2025-09-23 19:27:46,351 - INFO - _models.training_function_executor - BO Objective: base=0.8812, size_penalty=0.0000, final=0.8812
2025-09-23 19:27:46,351 - INFO - _models.training_function_executor - Model: 9,165 parameters, 9.8KB (PASS 256KB limit)
2025-09-23 19:27:46,351 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 104.084s
2025-09-23 19:27:46,460 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8812
2025-09-23 19:27:46,461 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.110s
2025-09-23 19:27:46,461 - INFO - bo.run_bo - Recorded observation #28: hparams={'lr': 0.0005080002107669694, 'batch_size': np.int64(128), 'epochs': np.int64(31), 'd_model': np.int64(65), 'n_heads': np.int64(2), 'num_layers': np.int64(4), 'mlp_ratio': 2.6854140687678463, 'dropout': 0.4641128162250818, 'weight_decay': 1.125714273116538e-05, 'patch_size': np.int64(10), 'label_smoothing': 0.14209961260308926, 'use_focal_loss': np.True_, 'focal_gamma': 1.0824982211135412, 'grad_clip_norm': 1.0770003417098677, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(4606), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.8812
2025-09-23 19:27:46,461 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 28: {'lr': 0.0005080002107669694, 'batch_size': np.int64(128), 'epochs': np.int64(31), 'd_model': np.int64(65), 'n_heads': np.int64(2), 'num_layers': np.int64(4), 'mlp_ratio': 2.6854140687678463, 'dropout': 0.4641128162250818, 'weight_decay': 1.125714273116538e-05, 'patch_size': np.int64(10), 'label_smoothing': 0.14209961260308926, 'use_focal_loss': np.True_, 'focal_gamma': 1.0824982211135412, 'grad_clip_norm': 1.0770003417098677, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(4606), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.8812
2025-09-23 19:27:46,461 - INFO - bo.run_bo - ğŸ”BO Trial 29: Using RF surrogate + Expected Improvement
2025-09-23 19:27:46,461 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:27:46,461 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:27:46,461 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:27:46,461 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0035268868882016692, 'batch_size': 128, 'epochs': 38, 'd_model': 76, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 1.7572104186756603, 'dropout': 0.01056380893694653, 'weight_decay': 1.0281566234301136e-06, 'patch_size': 20, 'label_smoothing': 0.18015243757072502, 'use_focal_loss': True, 'focal_gamma': 1.3335837171396505, 'grad_clip_norm': 0.27788114123223334, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 48891, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:27:46,463 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0035268868882016692, 'batch_size': 128, 'epochs': 38, 'd_model': 76, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 1.7572104186756603, 'dropout': 0.01056380893694653, 'weight_decay': 1.0281566234301136e-06, 'patch_size': 20, 'label_smoothing': 0.18015243757072502, 'use_focal_loss': True, 'focal_gamma': 1.3335837171396505, 'grad_clip_norm': 0.27788114123223334, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 48891, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:28:52,198 - INFO - _models.training_function_executor - Model: 8,436 parameters, 9.1KB storage
2025-09-23 19:28:52,199 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4449865710660916, 0.23755733642734256, 0.16565083544553716, 0.13380827807389237, 0.1254199491800067, 0.12241275516162932, 0.11964592964966689, 0.11749713124571799, 0.11268055408350959, 0.10527737731142563, 0.0963635766227618, 0.08716604218651984, 0.07845194122696243, 0.07419373141247217, 0.06698596173340103, 0.06311961753308518, 0.05984312588809292, 0.05218847122899804, 0.04931952050254176, 0.04748273295473866, 0.04293972330550478, 0.04070230182722975, 0.035912818314922865, 0.03415416669869201, 0.030125952509878623, 0.027169121893132888, 0.02354078076169405, 0.020538443094438884, 0.018711217663883676, 0.014100091039560985, 0.011639784481094588, 0.008855535035743864, 0.0075817012825161985, 0.005943638401211785, 0.0056250766674526935, 0.005047549413498971, 0.0041779368830301405, 0.003788287664190039], 'val_losses': [0.3130352211643051, 0.17684758766083442, 0.12729776934183207, 0.1238744961014518, 0.1298563087300958, 0.11616682318081131, 0.10946071901752034, 0.17254428955721773, 0.11888554078559352, 0.09682342328402858, 0.09543061220280703, 0.08813102176566275, 0.08077959166278428, 0.08174226680520094, 0.07609329120868759, 0.06697731215005948, 0.06786733980376594, 0.07260251155758154, 0.07652194234601478, 0.06118049974721441, 0.0574945263184636, 0.05750069541973572, 0.05775452221026921, 0.05458065146544631, 0.056192077264094496, 0.06358801382606064, 0.055014776643573335, 0.0574458239147529, 0.05535130866890398, 0.06417162262314977, 0.06414119951551144, 0.07111574560049079, 0.07357010754662459, 0.07383535311882575, 0.0722602565285407, 0.07360366599039392, 0.07379513491837097, 0.0738578906424914], 'val_acc': [0.8052875579501315, 0.9087833604811427, 0.9285803783986969, 0.9332163889236937, 0.928956271143967, 0.9354717453953139, 0.9391053752662574, 0.9022678862297958, 0.9329657937601804, 0.9467485277534143, 0.9490038842250345, 0.9492544793885478, 0.9520110261871946, 0.9491291818067912, 0.9530134068412479, 0.9592782859290816, 0.9592782859290816, 0.9566470367121914, 0.955268763312868, 0.9646660819446184, 0.9647913795263752, 0.9629119158000251, 0.9642901891993485, 0.9671720335797519, 0.9669214384162386, 0.9677985214885353, 0.9689261997243453, 0.9670467359979953, 0.9718080441047487, 0.9693020924696153, 0.9691767948878587, 0.9684250093973187, 0.9689261997243453, 0.968299711815562, 0.9691767948878587, 0.9699285803783987, 0.9695526876331286, 0.9698032827966421], 'model_size_bytes': 247421, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0035268868882016692, 'batch_size': 128, 'epochs': 38, 'd_model': 76, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 1.7572104186756603, 'dropout': 0.01056380893694653, 'weight_decay': 1.0281566234301136e-06, 'patch_size': 20, 'label_smoothing': 0.18015243757072502, 'use_focal_loss': True, 'focal_gamma': 1.3335837171396505, 'grad_clip_norm': 0.27788114123223334, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 48891, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 8436, 'model_storage_size_kb': 9.062109375, 'model_size_validation': 'PASS'}
2025-09-23 19:28:52,199 - INFO - _models.training_function_executor - BO Objective: base=0.9698, size_penalty=0.0000, final=0.9698
2025-09-23 19:28:52,199 - INFO - _models.training_function_executor - Model: 8,436 parameters, 9.1KB (PASS 256KB limit)
2025-09-23 19:28:52,199 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 65.738s
2025-09-23 19:28:52,310 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9698
2025-09-23 19:28:52,310 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-09-23 19:28:52,310 - INFO - bo.run_bo - Recorded observation #29: hparams={'lr': 0.0035268868882016692, 'batch_size': np.int64(128), 'epochs': np.int64(38), 'd_model': np.int64(76), 'n_heads': np.int64(2), 'num_layers': np.int64(4), 'mlp_ratio': 1.7572104186756603, 'dropout': 0.01056380893694653, 'weight_decay': 1.0281566234301136e-06, 'patch_size': np.int64(20), 'label_smoothing': 0.18015243757072502, 'use_focal_loss': np.True_, 'focal_gamma': 1.3335837171396505, 'grad_clip_norm': 0.27788114123223334, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(48891), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9698
2025-09-23 19:28:52,310 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 29: {'lr': 0.0035268868882016692, 'batch_size': np.int64(128), 'epochs': np.int64(38), 'd_model': np.int64(76), 'n_heads': np.int64(2), 'num_layers': np.int64(4), 'mlp_ratio': 1.7572104186756603, 'dropout': 0.01056380893694653, 'weight_decay': 1.0281566234301136e-06, 'patch_size': np.int64(20), 'label_smoothing': 0.18015243757072502, 'use_focal_loss': np.True_, 'focal_gamma': 1.3335837171396505, 'grad_clip_norm': 0.27788114123223334, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(48891), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9698
2025-09-23 19:28:52,310 - INFO - bo.run_bo - ğŸ”BO Trial 30: Using RF surrogate + Expected Improvement
2025-09-23 19:28:52,310 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:28:52,310 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:28:52,310 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:28:52,310 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.2627030402936304e-05, 'batch_size': 32, 'epochs': 43, 'd_model': 69, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 2.2700386164904396, 'dropout': 0.13560449720609422, 'weight_decay': 4.4723498304723966e-05, 'patch_size': 20, 'label_smoothing': 0.1231431958614279, 'use_focal_loss': False, 'focal_gamma': 1.2515826623734727, 'grad_clip_norm': 0.058378381982730834, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 10650, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:28:52,312 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.2627030402936304e-05, 'batch_size': 32, 'epochs': 43, 'd_model': 69, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 2.2700386164904396, 'dropout': 0.13560449720609422, 'weight_decay': 4.4723498304723966e-05, 'patch_size': 20, 'label_smoothing': 0.1231431958614279, 'use_focal_loss': False, 'focal_gamma': 1.2515826623734727, 'grad_clip_norm': 0.058378381982730834, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 10650, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:30:45,390 - INFO - _models.training_function_executor - Model: 7,383 parameters, 31.7KB storage
2025-09-23 19:30:45,390 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0463432847297762, 0.9215881686810655, 0.831856293649231, 0.7818723176873963, 0.7558969358192389, 0.7375110252098538, 0.7214146048662392, 0.7124915041840024, 0.700632504617287, 0.6922454737453343, 0.6837505286780193, 0.6784035911298062, 0.6714025261228636, 0.6649887543350773, 0.6580213154220784, 0.6540402806302238, 0.6463404055436826, 0.6447572576197915, 0.6394214935179952, 0.6353238406730424, 0.6334513138493167, 0.6291847001847903, 0.625517692762677, 0.6247246158106959, 0.623130389589738, 0.619949076896629, 0.6171217566500241, 0.6145791495682449, 0.6131015039885421, 0.6114042067954972, 0.6083900934920269, 0.6054642883394348, 0.6043566934832303, 0.603598764537738, 0.6002042059652213, 0.5989689581772777, 0.596338916212508, 0.5945300762282089, 0.5930979611845769, 0.5907463937576778, 0.5904000978442514, 0.5903225335300191, 0.586319839431951], 'val_losses': [0.9782875034985664, 0.869814452155911, 0.7686301032308797, 0.7290147588584913, 0.7093559278908297, 0.6949275121132005, 0.6858850870570389, 0.6711891814739599, 0.665295115428048, 0.6566530371381442, 0.6542275090994892, 0.6487478812137294, 0.638866877289975, 0.6342548736353713, 0.6289393327287259, 0.6273904093045846, 0.617662569721037, 0.6186889924884753, 0.6135860072044393, 0.6173377878018271, 0.6065638661907364, 0.6060270011731219, 0.6059529477381016, 0.5987379825842618, 0.6016658218697697, 0.6004671341433977, 0.6021963375700011, 0.5923606126944144, 0.594293321131943, 0.5961180527233239, 0.5870649042680379, 0.5961268776835184, 0.5893472791121368, 0.5887273674306378, 0.5870730126207298, 0.5838622850084825, 0.5896511316418932, 0.5838079022040807, 0.5813672875360265, 0.5804086183040845, 0.5777409118214522, 0.5769958696319352, 0.5808771012451039], 'val_acc': [0.7136950256860043, 0.785239944869064, 0.8520235559453703, 0.8709434907906277, 0.8804661070041349, 0.8883598546548052, 0.8931211627615587, 0.900639017666959, 0.9054003257737125, 0.9111640145345195, 0.9104122290439794, 0.9136699661696529, 0.9165518105500564, 0.9191830597669465, 0.9221902017291066, 0.9226913920561333, 0.9288309735622102, 0.9274527001628868, 0.9295827590527502, 0.9305851397068037, 0.9327151985966671, 0.9338428768324771, 0.9364741260493672, 0.938478887357474, 0.9373512091216639, 0.9352211502318005, 0.9339681744142339, 0.940358351083824, 0.9399824583385541, 0.9386041849392307, 0.9416113269013908, 0.9363488284676106, 0.9416113269013908, 0.9401077559203107, 0.9404836486655808, 0.9433654930459842, 0.9398571607567974, 0.9422378148101742, 0.9427390051372009, 0.9442425761182809, 0.9447437664453076, 0.9456208495176043, 0.9416113269013908], 'model_size_bytes': 183737, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.2627030402936304e-05, 'batch_size': 32, 'epochs': 43, 'd_model': 69, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 2.2700386164904396, 'dropout': 0.13560449720609422, 'weight_decay': 4.4723498304723966e-05, 'patch_size': 20, 'label_smoothing': 0.1231431958614279, 'use_focal_loss': False, 'focal_gamma': 1.2515826623734727, 'grad_clip_norm': 0.058378381982730834, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 10650, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 7383, 'model_storage_size_kb': 31.723828125000004, 'model_size_validation': 'PASS'}
2025-09-23 19:30:45,390 - INFO - _models.training_function_executor - BO Objective: base=0.9416, size_penalty=0.0000, final=0.9416
2025-09-23 19:30:45,390 - INFO - _models.training_function_executor - Model: 7,383 parameters, 31.7KB (PASS 256KB limit)
2025-09-23 19:30:45,390 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 113.080s
2025-09-23 19:30:45,501 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9416
2025-09-23 19:30:45,501 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-09-23 19:30:45,501 - INFO - bo.run_bo - Recorded observation #30: hparams={'lr': 3.2627030402936304e-05, 'batch_size': np.int64(32), 'epochs': np.int64(43), 'd_model': np.int64(69), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 2.2700386164904396, 'dropout': 0.13560449720609422, 'weight_decay': 4.4723498304723966e-05, 'patch_size': np.int64(20), 'label_smoothing': 0.1231431958614279, 'use_focal_loss': np.False_, 'focal_gamma': 1.2515826623734727, 'grad_clip_norm': 0.058378381982730834, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(10650), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9416
2025-09-23 19:30:45,501 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 30: {'lr': 3.2627030402936304e-05, 'batch_size': np.int64(32), 'epochs': np.int64(43), 'd_model': np.int64(69), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 2.2700386164904396, 'dropout': 0.13560449720609422, 'weight_decay': 4.4723498304723966e-05, 'patch_size': np.int64(20), 'label_smoothing': 0.1231431958614279, 'use_focal_loss': np.False_, 'focal_gamma': 1.2515826623734727, 'grad_clip_norm': 0.058378381982730834, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(10650), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9416
2025-09-23 19:30:45,501 - INFO - bo.run_bo - ğŸ”BO Trial 31: Using RF surrogate + Expected Improvement
2025-09-23 19:30:45,501 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:30:45,501 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:30:45,501 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:30:45,502 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.7373000117349443e-05, 'batch_size': 32, 'epochs': 52, 'd_model': 65, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 1.626230203387401, 'dropout': 0.055742299969700496, 'weight_decay': 2.4644597791465034e-06, 'patch_size': 10, 'label_smoothing': 0.13976568438939713, 'use_focal_loss': False, 'focal_gamma': 1.631490888109885, 'grad_clip_norm': 1.7122333550521662, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 60180, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:30:45,503 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.7373000117349443e-05, 'batch_size': 32, 'epochs': 52, 'd_model': 65, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 1.626230203387401, 'dropout': 0.055742299969700496, 'weight_decay': 2.4644597791465034e-06, 'patch_size': 10, 'label_smoothing': 0.13976568438939713, 'use_focal_loss': False, 'focal_gamma': 1.631490888109885, 'grad_clip_norm': 1.7122333550521662, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 60180, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:34:09,842 - INFO - _models.training_function_executor - Model: 9,165 parameters, 39.4KB storage
2025-09-23 19:34:09,842 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0704470279315874, 0.9371322983208894, 0.8853797097031799, 0.8626726871836631, 0.8477077044027773, 0.8324128077434455, 0.8168422854365298, 0.7990177575293084, 0.780114073945, 0.7578149051201417, 0.7439981298451435, 0.7330666835309644, 0.7227769043609671, 0.7156311812169442, 0.709834111146497, 0.705316779458373, 0.7024192407877079, 0.6947578348484762, 0.6929548396389201, 0.6874555179216365, 0.6833786955379556, 0.678714764154417, 0.6748304924667508, 0.6708892350641515, 0.6695973276737224, 0.6648177025599934, 0.6636732353048471, 0.659461279805358, 0.6565962615748925, 0.6560513921254963, 0.6545827378419483, 0.6510505145031442, 0.6504556261140952, 0.6476561786260379, 0.6462363620503887, 0.6440396684025611, 0.6427592281649496, 0.6417305146330031, 0.6401296081546852, 0.6380549058861008, 0.6371354626742154, 0.6364530043819623, 0.6339759433945031, 0.6328563135234805, 0.6314884097796538, 0.627973675074346, 0.6277870135461463, 0.6284035962689166, 0.6274849345476202, 0.6256675471978853, 0.626586777007106, 0.6239077102173407], 'val_losses': [0.9676405613157418, 0.8960961189100937, 0.8727300026305949, 0.8452485163936847, 0.8331580867953743, 0.8167385545354665, 0.7884771582507024, 0.7674550511845669, 0.7564208366461318, 0.7398957726113287, 0.7230787098056735, 0.7152754688693115, 0.6998390911919269, 0.6985083748233482, 0.6852665800839763, 0.6844395200436373, 0.6713947251004065, 0.6762254173154297, 0.6813154275323825, 0.6672416308173827, 0.668472809341076, 0.6643366678632481, 0.6606044819548889, 0.6569492731338126, 0.6586496444621371, 0.6536381408364951, 0.649199751860037, 0.6483833691094277, 0.6443982380865332, 0.642969242053109, 0.6422353909735901, 0.6430498901472222, 0.6371308424137929, 0.6352304499888088, 0.6325417516106367, 0.6386012501800856, 0.6312858329621671, 0.628447595327932, 0.6262936404647272, 0.6264506347973146, 0.625499597103311, 0.6223948992757985, 0.6238843404534257, 0.6238425324516119, 0.6247359849862655, 0.6212164675318617, 0.6242951124088759, 0.6190486454213865, 0.6194236618118348, 0.6154523082969033, 0.6180055208663835, 0.6203224483311228], 'val_acc': [0.7606816188447563, 0.8061646410224282, 0.8168149354717454, 0.8352336799899762, 0.836862548552813, 0.8393685001879464, 0.8507705801278035, 0.868562836737251, 0.8770830722967047, 0.8843503320385917, 0.8985089587770956, 0.8990101491041222, 0.9077809798270894, 0.9091592532264128, 0.9171782984588397, 0.9205613331662699, 0.9250720461095101, 0.9238190702919433, 0.9184312742764065, 0.9243202606189701, 0.9241949630372134, 0.9267009146723468, 0.9277032953264002, 0.930083949379777, 0.9270768074176169, 0.9293321638892369, 0.9317128179426137, 0.9325899010149105, 0.9340934719959905, 0.9359729357223405, 0.9362235308858539, 0.9348452574865305, 0.938102994612204, 0.938478887357474, 0.9398571607567974, 0.9359729357223405, 0.9399824583385541, 0.9394812680115274, 0.9418619220649042, 0.9429896003007142, 0.9441172785365243, 0.9441172785365243, 0.9443678737000376, 0.9431148978824708, 0.9429896003007142, 0.944994361608821, 0.9429896003007142, 0.9468738253351711, 0.944994361608821, 0.9477509084074678, 0.9475003132439543, 0.944994361608821], 'model_size_bytes': 199485, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.7373000117349443e-05, 'batch_size': 32, 'epochs': 52, 'd_model': 65, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 1.626230203387401, 'dropout': 0.055742299969700496, 'weight_decay': 2.4644597791465034e-06, 'patch_size': 10, 'label_smoothing': 0.13976568438939713, 'use_focal_loss': False, 'focal_gamma': 1.631490888109885, 'grad_clip_norm': 1.7122333550521662, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 60180, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 9165, 'model_storage_size_kb': 39.380859375, 'model_size_validation': 'PASS'}
2025-09-23 19:34:09,842 - INFO - _models.training_function_executor - BO Objective: base=0.9450, size_penalty=0.0000, final=0.9450
2025-09-23 19:34:09,842 - INFO - _models.training_function_executor - Model: 9,165 parameters, 39.4KB (PASS 256KB limit)
2025-09-23 19:34:09,842 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 204.341s
2025-09-23 19:34:09,954 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9450
2025-09-23 19:34:09,954 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.112s
2025-09-23 19:34:09,954 - INFO - bo.run_bo - Recorded observation #31: hparams={'lr': 1.7373000117349443e-05, 'batch_size': np.int64(32), 'epochs': np.int64(52), 'd_model': np.int64(65), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 1.626230203387401, 'dropout': 0.055742299969700496, 'weight_decay': 2.4644597791465034e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.13976568438939713, 'use_focal_loss': np.False_, 'focal_gamma': 1.631490888109885, 'grad_clip_norm': 1.7122333550521662, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(60180), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9450
2025-09-23 19:34:09,954 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 31: {'lr': 1.7373000117349443e-05, 'batch_size': np.int64(32), 'epochs': np.int64(52), 'd_model': np.int64(65), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 1.626230203387401, 'dropout': 0.055742299969700496, 'weight_decay': 2.4644597791465034e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.13976568438939713, 'use_focal_loss': np.False_, 'focal_gamma': 1.631490888109885, 'grad_clip_norm': 1.7122333550521662, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(60180), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9450
2025-09-23 19:34:09,954 - INFO - bo.run_bo - ğŸ”BO Trial 32: Using RF surrogate + Expected Improvement
2025-09-23 19:34:09,954 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:34:09,955 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:34:09,955 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:34:09,955 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.515081485101248e-05, 'batch_size': 128, 'epochs': 40, 'd_model': 89, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.968142285304803, 'dropout': 0.10588003273697658, 'weight_decay': 1.2282188950861722e-06, 'patch_size': 20, 'label_smoothing': 0.09332322910231301, 'use_focal_loss': True, 'focal_gamma': 1.0296479815362658, 'grad_clip_norm': 1.3529828666950185, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 38236, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:34:09,956 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.515081485101248e-05, 'batch_size': 128, 'epochs': 40, 'd_model': 89, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.968142285304803, 'dropout': 0.10588003273697658, 'weight_decay': 1.2282188950861722e-06, 'patch_size': 20, 'label_smoothing': 0.09332322910231301, 'use_focal_loss': True, 'focal_gamma': 1.0296479815362658, 'grad_clip_norm': 1.3529828666950185, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 38236, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:35:20,850 - INFO - _models.training_function_executor - Model: 295,933 parameters, 1271.6KB storage
2025-09-23 19:35:20,850 - WARNING - _models.training_function_executor - Model storage 1271.6KB exceeds 256KB limit!
2025-09-23 19:35:20,850 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7023602131165662, 0.6015072801046435, 0.5118588480297995, 0.472666828875008, 0.4487297128476031, 0.43011620407829243, 0.41437164082139694, 0.39832841949411657, 0.3850682158407866, 0.3735154246638279, 0.36463471052622215, 0.3558323287257716, 0.3498976223302157, 0.3439168402913989, 0.3388142283866195, 0.33187250370493554, 0.32735205843404724, 0.3222770034903718, 0.3188167701399207, 0.31512904879021636, 0.31092373205748125, 0.3101933653510252, 0.3068974196226552, 0.30479032762706443, 0.30009557477562954, 0.30208248620724404, 0.29964762771969444, 0.29537487151299907, 0.29493754080413026, 0.2932401783056017, 0.2913173406973588, 0.29059060467752235, 0.2904128197593296, 0.29030122884212795, 0.2884774140497125, 0.28979591582254843, 0.2882576381755391, 0.2893795226193705, 0.28986933567307893, 0.28778206866676936], 'val_losses': [0.6159504580850246, 0.53550918679416, 0.4696074561873499, 0.45067017161820766, 0.43176001262737984, 0.4071103443819066, 0.3939726130154449, 0.38274989112914765, 0.36276867831878057, 0.35953354181659025, 0.33894984112311954, 0.33384369523950036, 0.33098987629502924, 0.3264685744813414, 0.3119903665230606, 0.30402782824333413, 0.2991457441022351, 0.2942932526459746, 0.2866545100175353, 0.28417354546577883, 0.2789665480732335, 0.27587170166272357, 0.27340169915085283, 0.2692169430705931, 0.26871693064665797, 0.2653338578956167, 0.26446984327985085, 0.26078489605771943, 0.26059007803541784, 0.2593581792540814, 0.2576869464845918, 0.25641789427639744, 0.25665271736643186, 0.25626556121224586, 0.2552699235474133, 0.25505813900621715, 0.25508432579083845, 0.25506312544894627, 0.254863447428138, 0.254868385982146], 'val_acc': [0.7200852023555946, 0.7273524620974815, 0.7659441172785365, 0.7639393559704297, 0.7633128680616464, 0.7779726851271771, 0.7747149480015035, 0.7747149480015035, 0.790627740884601, 0.7948878586643278, 0.8113018418744519, 0.814308983836612, 0.8134319007643153, 0.8183185064528254, 0.830722967046736, 0.8364866558075429, 0.838742012279163, 0.8413732614960531, 0.8491417115649668, 0.8481393309109134, 0.8516476632001002, 0.8542789124169904, 0.8532765317629369, 0.8581631374514472, 0.8574113519609071, 0.8591655181055006, 0.8622979576494174, 0.8626738503946874, 0.8615461721588773, 0.8635509334669841, 0.8641774213757675, 0.8651798020298208, 0.8650545044480642, 0.8646786117027941, 0.8654303971933341, 0.8659315875203608, 0.8660568851021175, 0.8659315875203608, 0.8658062899386042, 0.8660568851021175], 'model_size_bytes': 606611, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.515081485101248e-05, 'batch_size': 128, 'epochs': 40, 'd_model': 89, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.968142285304803, 'dropout': 0.10588003273697658, 'weight_decay': 1.2282188950861722e-06, 'patch_size': 20, 'label_smoothing': 0.09332322910231301, 'use_focal_loss': True, 'focal_gamma': 1.0296479815362658, 'grad_clip_norm': 1.3529828666950185, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 38236, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 295933, 'model_storage_size_kb': 1271.5871093750002, 'model_size_validation': 'FAIL'}
2025-09-23 19:35:20,850 - INFO - _models.training_function_executor - BO Objective: base=0.8661, size_penalty=0.8000, final=0.0661
2025-09-23 19:35:20,850 - INFO - _models.training_function_executor - Model: 295,933 parameters, 1271.6KB (FAIL 256KB limit)
2025-09-23 19:35:20,850 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 70.896s
2025-09-23 19:35:21,094 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0661
2025-09-23 19:35:21,094 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.243s
2025-09-23 19:35:21,094 - INFO - bo.run_bo - Recorded observation #32: hparams={'lr': 1.515081485101248e-05, 'batch_size': np.int64(128), 'epochs': np.int64(40), 'd_model': np.int64(89), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 3.968142285304803, 'dropout': 0.10588003273697658, 'weight_decay': 1.2282188950861722e-06, 'patch_size': np.int64(20), 'label_smoothing': 0.09332322910231301, 'use_focal_loss': np.True_, 'focal_gamma': 1.0296479815362658, 'grad_clip_norm': 1.3529828666950185, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(38236), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.0661
2025-09-23 19:35:21,094 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 32: {'lr': 1.515081485101248e-05, 'batch_size': np.int64(128), 'epochs': np.int64(40), 'd_model': np.int64(89), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 3.968142285304803, 'dropout': 0.10588003273697658, 'weight_decay': 1.2282188950861722e-06, 'patch_size': np.int64(20), 'label_smoothing': 0.09332322910231301, 'use_focal_loss': np.True_, 'focal_gamma': 1.0296479815362658, 'grad_clip_norm': 1.3529828666950185, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(38236), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.0661
2025-09-23 19:35:21,095 - INFO - bo.run_bo - ğŸ”BO Trial 33: Using RF surrogate + Expected Improvement
2025-09-23 19:35:21,095 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:35:21,095 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:35:21,095 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:35:21,095 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0026665934327606286, 'batch_size': 128, 'epochs': 57, 'd_model': 54, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': 40, 'label_smoothing': 0.10595857977887042, 'use_focal_loss': True, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 44609, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:35:21,097 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0026665934327606286, 'batch_size': 128, 'epochs': 57, 'd_model': 54, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': 40, 'label_smoothing': 0.10595857977887042, 'use_focal_loss': True, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 44609, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:36:13,920 - INFO - _models.training_function_executor - Model: 6,588 parameters, 28.3KB storage
2025-09-23 19:36:13,920 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3323411243638833, 0.195708709293035, 0.14821105157729567, 0.12817672567896998, 0.1114858505256126, 0.09625981765795227, 0.0902575178629839, 0.08438719305674937, 0.07779558878193014, 0.07299045176228847, 0.06722752943005295, 0.06559919415961545, 0.0646543225490078, 0.060511365195506574, 0.058964269461012034, 0.05522335005444893, 0.050998348268587124, 0.047685231697567575, 0.047321257173792854, 0.044980598605276034, 0.04151381013354626, 0.04086443043084583, 0.040252044169154996, 0.03784173111323444, 0.03427117457553542, 0.033482884577943524, 0.03152127830697395, 0.03000025394142255, 0.029239662880120493, 0.027157588295308427, 0.02587202441473477, 0.024169183032292454, 0.024738227590931146, 0.02271812844112307, 0.019271381010312175, 0.019915793260034717, 0.01764965628857295, 0.017399132487436495, 0.015413584941046176, 0.014328051542082727, 0.014055566255710203, 0.01299603190060076, 0.012385256554777596, 0.010996968116363496, 0.010507404705730916, 0.009837395539450672, 0.009692761955081253, 0.008979766191763265, 0.008765584912010013, 0.007855345086543116, 0.007017816370474175, 0.007348394323868216, 0.00704138548341853, 0.007550102847034049, 0.006659921587873146, 0.005856311368423471, 0.0066723955577647305], 'val_losses': [0.2371538777075649, 0.16718605297850184, 0.14736368723261284, 0.11988807667983592, 0.10095508894108841, 0.08754440919619697, 0.09656593349363522, 0.08975392332644319, 0.0749903733993425, 0.07118603368515569, 0.07435088595446601, 0.07423999564937594, 0.07094037475487057, 0.0655108897619686, 0.0659471095736023, 0.060901434494392746, 0.06351657171685017, 0.06134154691371743, 0.057390234027608206, 0.0560633072310067, 0.05792641113753188, 0.057233699563227415, 0.05182961189120265, 0.05888937580609378, 0.05211553532271259, 0.05415113953167059, 0.05129058665238053, 0.05057966375116447, 0.04743213058190154, 0.04691379942357458, 0.053212558643633584, 0.046570176975408335, 0.04663131568922075, 0.04866723461215948, 0.049685480704867384, 0.049182274400657644, 0.04960062655029463, 0.04643805420999165, 0.049635090196196885, 0.05165819173438136, 0.04940007249673586, 0.051336761163061025, 0.05114889832356119, 0.051860836680753086, 0.05331705838288748, 0.053870702647592264, 0.050612143568191416, 0.05075641504625766, 0.049274094254357255, 0.05461148209491778, 0.0511042285911751, 0.05196665162345458, 0.05254484384098144, 0.05143216243364984, 0.05188547501422427, 0.0517757119499581, 0.051794696529696364], 'val_acc': [0.8575366495426638, 0.9087833604811427, 0.913294073424383, 0.9317128179426137, 0.9428643027189575, 0.9463726350081444, 0.9404836486655808, 0.9433654930459842, 0.9540157874953014, 0.9538904899135446, 0.9567723342939481, 0.9528881092594913, 0.9567723342939481, 0.9592782859290816, 0.9563964415486781, 0.9589023931838115, 0.9611577496554317, 0.9629119158000251, 0.9624107254729983, 0.9636637012905651, 0.9634131061270518, 0.9611577496554317, 0.9654178674351584, 0.9622854278912417, 0.9671720335797519, 0.9646660819446184, 0.9664202480892119, 0.9704297707054254, 0.9689261997243453, 0.9704297707054254, 0.9654178674351584, 0.9698032827966421, 0.9695526876331286, 0.9667961408344818, 0.970930961032452, 0.9708056634506954, 0.9711815561959655, 0.9718080441047487, 0.9720586392682621, 0.9721839368500188, 0.9736875078310988, 0.9729357223405588, 0.9721839368500188, 0.9734369126675855, 0.9723092344317754, 0.9725598295952889, 0.9730610199223155, 0.9739381029946123, 0.9736875078310988, 0.9715574489412354, 0.9731863175040721, 0.9741886981581256, 0.9743139957398822, 0.9744392933216389, 0.9740634005763689, 0.9744392933216389, 0.9744392933216389], 'model_size_bytes': 161657, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0026665934327606286, 'batch_size': 128, 'epochs': 57, 'd_model': 54, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': 40, 'label_smoothing': 0.10595857977887042, 'use_focal_loss': True, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 44609, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 6588, 'model_storage_size_kb': 28.3078125, 'model_size_validation': 'PASS'}
2025-09-23 19:36:13,920 - INFO - _models.training_function_executor - BO Objective: base=0.9744, size_penalty=0.0000, final=0.9744
2025-09-23 19:36:13,920 - INFO - _models.training_function_executor - Model: 6,588 parameters, 28.3KB (PASS 256KB limit)
2025-09-23 19:36:13,920 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 52.825s
2025-09-23 19:36:14,034 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9744
2025-09-23 19:36:14,034 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.114s
2025-09-23 19:36:14,034 - INFO - bo.run_bo - Recorded observation #33: hparams={'lr': 0.0026665934327606286, 'batch_size': np.int64(128), 'epochs': np.int64(57), 'd_model': np.int64(54), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': np.int64(40), 'label_smoothing': 0.10595857977887042, 'use_focal_loss': np.True_, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(44609), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9744
2025-09-23 19:36:14,034 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 33: {'lr': 0.0026665934327606286, 'batch_size': np.int64(128), 'epochs': np.int64(57), 'd_model': np.int64(54), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': np.int64(40), 'label_smoothing': 0.10595857977887042, 'use_focal_loss': np.True_, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(44609), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9744
2025-09-23 19:36:14,035 - INFO - bo.run_bo - ğŸ”BO Trial 34: Using RF surrogate + Expected Improvement
2025-09-23 19:36:14,035 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:36:14,035 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:36:14,035 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:36:14,035 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00011889028246623375, 'batch_size': 64, 'epochs': 55, 'd_model': 71, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 1.703268339052097, 'dropout': 0.1321486217893286, 'weight_decay': 1.1678024057806247e-06, 'patch_size': 40, 'label_smoothing': 0.08689480545384233, 'use_focal_loss': True, 'focal_gamma': 1.1662554487189711, 'grad_clip_norm': 1.1551094984670058, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 62508, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:36:14,036 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00011889028246623375, 'batch_size': 64, 'epochs': 55, 'd_model': 71, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 1.703268339052097, 'dropout': 0.1321486217893286, 'weight_decay': 1.1678024057806247e-06, 'patch_size': 40, 'label_smoothing': 0.08689480545384233, 'use_focal_loss': True, 'focal_gamma': 1.1662554487189711, 'grad_clip_norm': 1.1551094984670058, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 62508, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:37:44,713 - INFO - _models.training_function_executor - Model: 8,946 parameters, 38.4KB storage
2025-09-23 19:37:44,713 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8217805290014543, 0.6268794471670189, 0.5644670181494288, 0.4732410393983683, 0.42575965650386427, 0.3839999349427299, 0.3490134592453676, 0.3201813118946163, 0.2930200811646709, 0.26577861849102496, 0.2397592772021612, 0.2182486489061813, 0.20115979329132927, 0.18604269898295706, 0.17260147756313288, 0.1589413951048818, 0.1496614995292769, 0.14097591802042328, 0.13299449128265223, 0.12912443351077918, 0.12057486768105112, 0.11727190992322865, 0.11585704643721942, 0.11061021624404835, 0.10718797123608044, 0.10304745704918672, 0.09981231180723384, 0.09714404738702201, 0.09606235926078756, 0.09229994576591352, 0.0886501514285787, 0.08816005299800515, 0.08518362751182927, 0.08269934989300746, 0.08135645447760242, 0.0808741962452273, 0.07836979538266702, 0.0762110146013051, 0.07584165480777076, 0.07394575249518427, 0.07191107493368071, 0.07103983535711483, 0.07263053278823114, 0.06912088508060454, 0.06721124833138317, 0.06632091926665017, 0.06702432622401087, 0.06607742687343666, 0.06644621998488509, 0.06530390584705595, 0.06509666215122274, 0.06304065192995434, 0.06400215872761951, 0.06498906209079272, 0.06363603917275201], 'val_losses': [0.6391668115294289, 0.5940656774310805, 0.4868168347111293, 0.43919476155364834, 0.38423175523901265, 0.340878771200483, 0.30549065810914716, 0.27966275413134034, 0.2508708057943076, 0.22379239388029612, 0.2095118793792525, 0.17765899957189762, 0.1591514081767352, 0.15082930206931364, 0.1373790698722257, 0.13016974097385006, 0.12460861469051501, 0.11415872683095359, 0.11369694777317564, 0.10823176808766354, 0.10834997404418543, 0.10750308846911308, 0.10171455115755762, 0.1073210124669032, 0.09472725406903369, 0.09262022620834603, 0.08925104517265156, 0.09187527020975188, 0.08801358069745151, 0.0897824258497224, 0.08723583276495721, 0.08811310364117585, 0.0846190217855789, 0.07910377509745421, 0.07843779185136866, 0.0808142108858834, 0.0821624648364142, 0.07718616857510464, 0.07820747326791115, 0.07747621419039634, 0.07782483057481515, 0.0769698673782455, 0.07413002346287453, 0.07533444725504466, 0.07560862528267338, 0.07465461668062592, 0.07405498108814092, 0.0734771937654664, 0.07264113917717165, 0.0717099332222065, 0.07237247741527836, 0.07299278457314728, 0.07272282955252486, 0.07260706947663421, 0.07259610492832723], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7599298333542163, 0.7729607818569102, 0.7947625610825712, 0.7960155369001378, 0.8232051121413356, 0.838742012279163, 0.8570354592156372, 0.8714446811176544, 0.8792131311865681, 0.8968800902142589, 0.9127928830973562, 0.9176794887858665, 0.9287056759804536, 0.9322140082696404, 0.9352211502318005, 0.9429896003007142, 0.9406089462473374, 0.9433654930459842, 0.9429896003007142, 0.9424884099736875, 0.9503821576243578, 0.9452449567723343, 0.9496303721338178, 0.9542663826588147, 0.9525122165142212, 0.9493797769703045, 0.9551434657311114, 0.9505074552061146, 0.9531387044230046, 0.9517604310236812, 0.9510086455331412, 0.9567723342939481, 0.9582759052750282, 0.9530134068412479, 0.9543916802405713, 0.9571482270392181, 0.9563964415486781, 0.9581506076932715, 0.9563964415486781, 0.9562711439669215, 0.9587770956020549, 0.9594035835108382, 0.9590276907655683, 0.9591529883473249, 0.9586517980202982, 0.9575241197844881, 0.9586517980202982, 0.9600300714196216, 0.9589023931838115, 0.9590276907655683, 0.9586517980202982, 0.9584012028567849, 0.9586517980202982], 'model_size_bytes': 225277, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00011889028246623375, 'batch_size': 64, 'epochs': 55, 'd_model': 71, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 1.703268339052097, 'dropout': 0.1321486217893286, 'weight_decay': 1.1678024057806247e-06, 'patch_size': 40, 'label_smoothing': 0.08689480545384233, 'use_focal_loss': True, 'focal_gamma': 1.1662554487189711, 'grad_clip_norm': 1.1551094984670058, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 62508, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 8946, 'model_storage_size_kb': 38.43984375, 'model_size_validation': 'PASS'}
2025-09-23 19:37:44,713 - INFO - _models.training_function_executor - BO Objective: base=0.9587, size_penalty=0.0000, final=0.9587
2025-09-23 19:37:44,713 - INFO - _models.training_function_executor - Model: 8,946 parameters, 38.4KB (PASS 256KB limit)
2025-09-23 19:37:44,713 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 90.679s
2025-09-23 19:37:44,827 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9587
2025-09-23 19:37:44,827 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.114s
2025-09-23 19:37:44,827 - INFO - bo.run_bo - Recorded observation #34: hparams={'lr': 0.00011889028246623375, 'batch_size': np.int64(64), 'epochs': np.int64(55), 'd_model': np.int64(71), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 1.703268339052097, 'dropout': 0.1321486217893286, 'weight_decay': 1.1678024057806247e-06, 'patch_size': np.int64(40), 'label_smoothing': 0.08689480545384233, 'use_focal_loss': np.True_, 'focal_gamma': 1.1662554487189711, 'grad_clip_norm': 1.1551094984670058, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(62508), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9587
2025-09-23 19:37:44,828 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 34: {'lr': 0.00011889028246623375, 'batch_size': np.int64(64), 'epochs': np.int64(55), 'd_model': np.int64(71), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 1.703268339052097, 'dropout': 0.1321486217893286, 'weight_decay': 1.1678024057806247e-06, 'patch_size': np.int64(40), 'label_smoothing': 0.08689480545384233, 'use_focal_loss': np.True_, 'focal_gamma': 1.1662554487189711, 'grad_clip_norm': 1.1551094984670058, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(62508), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9587
2025-09-23 19:37:44,828 - INFO - bo.run_bo - ğŸ”BO Trial 35: Using RF surrogate + Expected Improvement
2025-09-23 19:37:44,828 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:37:44,828 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:37:44,828 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:37:44,828 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.3596204117008403e-05, 'batch_size': 32, 'epochs': 51, 'd_model': 55, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 1.923884880538775, 'dropout': 0.17562553699715364, 'weight_decay': 2.614332459832888e-06, 'patch_size': 10, 'label_smoothing': 0.0953040316474353, 'use_focal_loss': True, 'focal_gamma': 1.1101211518492664, 'grad_clip_norm': 0.9936978968143209, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 63875, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:37:44,830 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.3596204117008403e-05, 'batch_size': 32, 'epochs': 51, 'd_model': 55, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 1.923884880538775, 'dropout': 0.17562553699715364, 'weight_decay': 2.614332459832888e-06, 'patch_size': 10, 'label_smoothing': 0.0953040316474353, 'use_focal_loss': True, 'focal_gamma': 1.1101211518492664, 'grad_clip_norm': 0.9936978968143209, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 63875, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:40:15,035 - INFO - _models.training_function_executor - Model: 7,535 parameters, 32.4KB storage
2025-09-23 19:40:15,035 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1189897008021068, 0.8163188113640623, 0.709292591572241, 0.653952605254264, 0.6155149326476672, 0.5908348583756498, 0.5716244107861419, 0.5353314886536249, 0.49499251728846094, 0.45788510006102123, 0.4240733777483771, 0.39704726435459037, 0.37457939064062445, 0.3576941362153026, 0.34128627954515056, 0.32634108174498083, 0.3146359471912481, 0.29993101366124975, 0.28906905117464504, 0.2801241627032979, 0.2711752277617167, 0.26070421461368476, 0.25310848375906475, 0.2470534797799291, 0.24263456682268922, 0.23483947838306488, 0.2311261243815037, 0.22862156246108256, 0.2236414466803487, 0.22161735051075507, 0.2187284459262065, 0.21295508824638235, 0.21283067447165366, 0.21293164504798093, 0.21058796369946922, 0.20960956074011988, 0.20599463299307516, 0.20415818237014008, 0.2042405132948932, 0.2034385018819698, 0.20134179531492596, 0.20183505690030965, 0.20011467225220636, 0.2019274141386824, 0.2006563464569925, 0.19990564910995742, 0.1998306303507806, 0.20004669597631886, 0.19857107759520018, 0.19759298774811668, 0.20090398820510907], 'val_losses': [0.8662320381491843, 0.7064897823775864, 0.6578446883125243, 0.6207965838850896, 0.594683300498852, 0.5778009521305494, 0.552434663383633, 0.4996237960760642, 0.46858046555702826, 0.4349350770502487, 0.41236602891852214, 0.3783992517843624, 0.3792532529523686, 0.3489012823877381, 0.3577275926394505, 0.318580831875405, 0.30105040616173445, 0.29727570100164313, 0.3054973860497075, 0.25914881653526406, 0.2646855376982895, 0.24109977509888503, 0.23485610608416174, 0.244795783416936, 0.23063901828175096, 0.23181622411811612, 0.24107767068537655, 0.23624070016173637, 0.23340615269990214, 0.22900028461061792, 0.2271822530786998, 0.21802650100604407, 0.22630787154529167, 0.21636104092138211, 0.21470003065280546, 0.20811271601572565, 0.20777691257241196, 0.2087288794362475, 0.20577957039861627, 0.2067894828904319, 0.2095242983180469, 0.20708754983570862, 0.20807512157723027, 0.20493456878140523, 0.2045888642418386, 0.2051543210272636, 0.2050470436581647, 0.20442792521844516, 0.20498531053720032, 0.2046410260674664, 0.20467336612145953], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7128179426137076, 0.7522866808670593, 0.7501566219771958, 0.7712066157123167, 0.7738378649292069, 0.7893747650670342, 0.7819822077433906, 0.8037839869690515, 0.7918807167021676, 0.8166896378899887, 0.8280917178298459, 0.8319759428643028, 0.8235810048866057, 0.8638015286304974, 0.8566595664703671, 0.874702418243328, 0.8797143215135947, 0.8701917053000877, 0.884976819947375, 0.8837238441298083, 0.8770830722967047, 0.8800902142588648, 0.8832226538027816, 0.8874827715825084, 0.8872321764189951, 0.892995865179802, 0.8891116401453452, 0.8926199724345321, 0.8960030071419621, 0.899511339431149, 0.899511339431149, 0.8990101491041222, 0.899511339431149, 0.900639017666959, 0.8985089587770956, 0.8992607442676356, 0.899887232176419, 0.9020172910662824, 0.9028943741385791, 0.9022678862297958, 0.9028943741385791, 0.9025184813933091, 0.9016413983210124, 0.9018919934845258, 0.9018919934845258], 'model_size_bytes': 131961, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.3596204117008403e-05, 'batch_size': 32, 'epochs': 51, 'd_model': 55, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 1.923884880538775, 'dropout': 0.17562553699715364, 'weight_decay': 2.614332459832888e-06, 'patch_size': 10, 'label_smoothing': 0.0953040316474353, 'use_focal_loss': True, 'focal_gamma': 1.1101211518492664, 'grad_clip_norm': 0.9936978968143209, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 63875, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 7535, 'model_storage_size_kb': 32.376953125, 'model_size_validation': 'PASS'}
2025-09-23 19:40:15,035 - INFO - _models.training_function_executor - BO Objective: base=0.9019, size_penalty=0.0000, final=0.9019
2025-09-23 19:40:15,035 - INFO - _models.training_function_executor - Model: 7,535 parameters, 32.4KB (PASS 256KB limit)
2025-09-23 19:40:15,035 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 150.207s
2025-09-23 19:40:15,149 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9019
2025-09-23 19:40:15,149 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.114s
2025-09-23 19:40:15,150 - INFO - bo.run_bo - Recorded observation #35: hparams={'lr': 2.3596204117008403e-05, 'batch_size': np.int64(32), 'epochs': np.int64(51), 'd_model': np.int64(55), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 1.923884880538775, 'dropout': 0.17562553699715364, 'weight_decay': 2.614332459832888e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.0953040316474353, 'use_focal_loss': np.True_, 'focal_gamma': 1.1101211518492664, 'grad_clip_norm': 0.9936978968143209, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(63875), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9019
2025-09-23 19:40:15,150 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 35: {'lr': 2.3596204117008403e-05, 'batch_size': np.int64(32), 'epochs': np.int64(51), 'd_model': np.int64(55), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 1.923884880538775, 'dropout': 0.17562553699715364, 'weight_decay': 2.614332459832888e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.0953040316474353, 'use_focal_loss': np.True_, 'focal_gamma': 1.1101211518492664, 'grad_clip_norm': 0.9936978968143209, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(63875), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9019
2025-09-23 19:40:15,150 - INFO - bo.run_bo - ğŸ”BO Trial 36: Using RF surrogate + Expected Improvement
2025-09-23 19:40:15,150 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:40:15,150 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:40:15,150 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:40:15,150 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003571083168455777, 'batch_size': 32, 'epochs': 54, 'd_model': 78, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.6655414107132656, 'dropout': 0.028713989907758204, 'weight_decay': 1.9790468156291305e-06, 'patch_size': 10, 'label_smoothing': 0.13498692478578558, 'use_focal_loss': True, 'focal_gamma': 1.1080721300487548, 'grad_clip_norm': 0.026806826887374242, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 17070, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:40:15,152 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003571083168455777, 'batch_size': 32, 'epochs': 54, 'd_model': 78, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.6655414107132656, 'dropout': 0.028713989907758204, 'weight_decay': 1.9790468156291305e-06, 'patch_size': 10, 'label_smoothing': 0.13498692478578558, 'use_focal_loss': True, 'focal_gamma': 1.1080721300487548, 'grad_clip_norm': 0.026806826887374242, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 17070, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:44:41,189 - INFO - _models.training_function_executor - Model: 191,321 parameters, 205.5KB storage
2025-09-23 19:44:41,189 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2936027803837915, 0.14001910073763593, 0.11314917539198797, 0.10153113240627304, 0.09428150836823299, 0.08739143059545003, 0.08242715445169431, 0.07798344011552605, 0.07603627178048726, 0.07355043012940561, 0.06958542874741848, 0.06951400869321145, 0.06809830328687404, 0.05980267319484164, 0.06170726482446257, 0.06105069926681365, 0.059906430094560134, 0.055815175351020964, 0.05386692413260714, 0.053595663331504693, 0.05119626436163537, 0.05071217760603061, 0.04792843028876955, 0.04459532513011783, 0.04519541724309667, 0.04288959296370613, 0.04215369240899436, 0.038325749319174575, 0.038889132782123546, 0.03440448070894869, 0.034254084574411474, 0.03041761178577619, 0.03270518090498932, 0.028697486886535856, 0.026931147288878524, 0.02524683764691179, 0.022604055123394824, 0.02285628065832032, 0.019438698048226958, 0.017927262086412533, 0.01722036618543773, 0.015514134951036807, 0.01629542947996471, 0.012960774893079622, 0.01403913762001488, 0.011036655901291992, 0.011324894544945376, 0.01310631583175681, 0.01031398629869882, 0.009825616766850434, 0.009678719552732399, 0.010665428539894413, 0.009451309616242748, 0.009368995449522908], 'val_losses': [0.1453749942362809, 0.12421266751217075, 0.11667835034309601, 0.09389112323294783, 0.09127007977953681, 0.08836084301516371, 0.08989137966200278, 0.08870774246504874, 0.09919646745572158, 0.08404049170321949, 0.10142772588751675, 0.09117041289262112, 0.10894262356653972, 0.08581196548125013, 0.0698474651424742, 0.07166888242590481, 0.07501166549021962, 0.08409724182038174, 0.07891312116378252, 0.08901449144214472, 0.09689047401608455, 0.09493658218335649, 0.09025041945115275, 0.09677629063220453, 0.1006931590791892, 0.10223431194711875, 0.09946572122093121, 0.10275872709434705, 0.10688603670399786, 0.1014829263363681, 0.10398442239953115, 0.11285885014666383, 0.12044999618623597, 0.12008291078972927, 0.11948196476285003, 0.12439581721811581, 0.11518202458881495, 0.12082336741077607, 0.1188664895868555, 0.1338712100015803, 0.1275643822444049, 0.13337399788323276, 0.13126089805513344, 0.13504281207318977, 0.14142615716143095, 0.14161601435369545, 0.14198288363408393, 0.13905492674927325, 0.13710887653927334, 0.13953072827961435, 0.14057896693904195, 0.13989330723124807, 0.13982248317710685, 0.13982349001549357], 'val_acc': [0.9299586517980203, 0.9423631123919308, 0.9433654930459842, 0.9535145971682747, 0.954516977822328, 0.9528881092594913, 0.9584012028567849, 0.9566470367121914, 0.9594035835108382, 0.9636637012905651, 0.9568976318757048, 0.9609071544919183, 0.9602806665831349, 0.9634131061270518, 0.9676732239067786, 0.967547926325022, 0.9676732239067786, 0.9660443553439418, 0.9695526876331286, 0.9676732239067786, 0.9671720335797519, 0.9674226287432652, 0.9704297707054254, 0.9700538779601554, 0.9685503069790753, 0.969051497306102, 0.970179175541912, 0.9723092344317754, 0.9699285803783987, 0.9671720335797519, 0.9723092344317754, 0.9721839368500188, 0.9696779852148854, 0.969427390051372, 0.9708056634506954, 0.9706803658689387, 0.9713068537777221, 0.9726851271770455, 0.9736875078310988, 0.9705550682871821, 0.9716827465229921, 0.9705550682871821, 0.9718080441047487, 0.9728104247588022, 0.9714321513594788, 0.9723092344317754, 0.9725598295952889, 0.9728104247588022, 0.9735622102493422, 0.9728104247588022, 0.9723092344317754, 0.9730610199223155, 0.9723092344317754, 0.9724345320135321], 'model_size_bytes': 401967, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003571083168455777, 'batch_size': 32, 'epochs': 54, 'd_model': 78, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.6655414107132656, 'dropout': 0.028713989907758204, 'weight_decay': 1.9790468156291305e-06, 'patch_size': 10, 'label_smoothing': 0.13498692478578558, 'use_focal_loss': True, 'focal_gamma': 1.1080721300487548, 'grad_clip_norm': 0.026806826887374242, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 17070, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 191321, 'model_storage_size_kb': 205.52060546875, 'model_size_validation': 'PASS'}
2025-09-23 19:44:41,189 - INFO - _models.training_function_executor - BO Objective: base=0.9724, size_penalty=0.0000, final=0.9724
2025-09-23 19:44:41,189 - INFO - _models.training_function_executor - Model: 191,321 parameters, 205.5KB (PASS 256KB limit)
2025-09-23 19:44:41,189 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 266.039s
2025-09-23 19:44:41,306 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9724
2025-09-23 19:44:41,306 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.117s
2025-09-23 19:44:41,306 - INFO - bo.run_bo - Recorded observation #36: hparams={'lr': 0.0003571083168455777, 'batch_size': np.int64(32), 'epochs': np.int64(54), 'd_model': np.int64(78), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 1.6655414107132656, 'dropout': 0.028713989907758204, 'weight_decay': 1.9790468156291305e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.13498692478578558, 'use_focal_loss': np.True_, 'focal_gamma': 1.1080721300487548, 'grad_clip_norm': 0.026806826887374242, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(17070), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9724
2025-09-23 19:44:41,306 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 36: {'lr': 0.0003571083168455777, 'batch_size': np.int64(32), 'epochs': np.int64(54), 'd_model': np.int64(78), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 1.6655414107132656, 'dropout': 0.028713989907758204, 'weight_decay': 1.9790468156291305e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.13498692478578558, 'use_focal_loss': np.True_, 'focal_gamma': 1.1080721300487548, 'grad_clip_norm': 0.026806826887374242, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(17070), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9724
2025-09-23 19:44:41,307 - INFO - bo.run_bo - ğŸ”BO Trial 37: Using RF surrogate + Expected Improvement
2025-09-23 19:44:41,307 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:44:41,307 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:44:41,307 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:44:41,307 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0006192350246101354, 'batch_size': 128, 'epochs': 46, 'd_model': 41, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3.24379327543148, 'dropout': 0.01571259373763956, 'weight_decay': 1.7162904597714704e-06, 'patch_size': 100, 'label_smoothing': 0.03699459736880331, 'use_focal_loss': False, 'focal_gamma': 1.512793270398372, 'grad_clip_norm': 1.4400157734022578, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 23570, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:44:41,308 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0006192350246101354, 'batch_size': 128, 'epochs': 46, 'd_model': 41, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3.24379327543148, 'dropout': 0.01571259373763956, 'weight_decay': 1.7162904597714704e-06, 'patch_size': 100, 'label_smoothing': 0.03699459736880331, 'use_focal_loss': False, 'focal_gamma': 1.512793270398372, 'grad_clip_norm': 1.4400157734022578, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 23570, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:45:17,282 - INFO - _models.training_function_executor - Model: 9,307 parameters, 40.0KB storage
2025-09-23 19:45:17,283 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7878080495141807, 0.5464905253912435, 0.4675219577690737, 0.4214037279248764, 0.3978749026359108, 0.37744541929613074, 0.3607965827331774, 0.34330078475129444, 0.33290647650983884, 0.3256118818321678, 0.318996192578239, 0.31191216615478456, 0.30619353333457355, 0.3016891316323002, 0.29574804586586423, 0.2905233912150095, 0.2861604593834012, 0.28145789207605715, 0.2797430481367144, 0.2749594200734032, 0.2720335661324964, 0.2677748316841586, 0.26598238360966575, 0.26099378791292094, 0.25833699915734193, 0.2559348379270223, 0.25261971258980453, 0.2506294699124188, 0.2471070877088101, 0.24565811051154454, 0.24310296615950652, 0.24247189310003828, 0.23935063396114695, 0.23781346416176888, 0.2369206416439981, 0.2343154770406428, 0.23220422662496082, 0.232659162169123, 0.23095198568451827, 0.22881964817738856, 0.22908494820762232, 0.2293120765372916, 0.22853344900601416, 0.22813348472211734, 0.22776181309742163, 0.22584632720575482], 'val_losses': [0.6170957666219528, 0.4867584226353547, 0.4495038680117048, 0.4207238204391689, 0.4178420442295708, 0.40262271286311746, 0.3497358137593952, 0.34762073829798096, 0.3412083316873957, 0.3250671096716499, 0.3235495851431525, 0.32616689260136494, 0.31325002709750677, 0.3123738979590505, 0.30840272805084634, 0.30328252408949896, 0.2976180780443269, 0.2926223769199578, 0.29578524301440745, 0.2937263358641561, 0.2913979209166932, 0.28965253631896265, 0.2859302182168295, 0.2876444662585173, 0.2828745357734251, 0.28012765193251404, 0.2831615937591092, 0.2814490484378792, 0.27891382122081615, 0.2798108695264658, 0.28144045711654037, 0.2797159726548622, 0.2853137956998235, 0.28015910278897943, 0.2775283560479827, 0.2780090397632954, 0.27871099418378564, 0.2767555115291938, 0.2771405947864123, 0.27747176040340643, 0.2777889661809254, 0.2760634284735831, 0.27711677134537394, 0.27703714394626155, 0.2769141892953081, 0.2769387852890183], 'val_acc': [0.8250845758676858, 0.8843503320385917, 0.8997619345946624, 0.9040220523743892, 0.9047738378649292, 0.9210625234932965, 0.9353464478135572, 0.9368500187946373, 0.9391053752662574, 0.9432401954642275, 0.9447437664453076, 0.9417366244831474, 0.9496303721338178, 0.9448690640270643, 0.9482520987344945, 0.9506327527878712, 0.9527628116777346, 0.9550181681493547, 0.9535145971682747, 0.954516977822328, 0.9548928705675981, 0.9581506076932715, 0.9577747149480015, 0.9590276907655683, 0.9584012028567849, 0.9619095351459717, 0.9609071544919183, 0.9612830472371883, 0.961784237564215, 0.9611577496554317, 0.9607818569101616, 0.9631625109635384, 0.9586517980202982, 0.9610324520736749, 0.9636637012905651, 0.9640395940358351, 0.9627866182182684, 0.9639142964540784, 0.9646660819446184, 0.9645407843628618, 0.963287808545295, 0.9652925698534018, 0.9639142964540784, 0.9634131061270518, 0.9637889988723217, 0.9639142964540784], 'model_size_bytes': 119865, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0006192350246101354, 'batch_size': 128, 'epochs': 46, 'd_model': 41, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3.24379327543148, 'dropout': 0.01571259373763956, 'weight_decay': 1.7162904597714704e-06, 'patch_size': 100, 'label_smoothing': 0.03699459736880331, 'use_focal_loss': False, 'focal_gamma': 1.512793270398372, 'grad_clip_norm': 1.4400157734022578, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 23570, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 9307, 'model_storage_size_kb': 39.991015625, 'model_size_validation': 'PASS'}
2025-09-23 19:45:17,283 - INFO - _models.training_function_executor - BO Objective: base=0.9639, size_penalty=0.0000, final=0.9639
2025-09-23 19:45:17,283 - INFO - _models.training_function_executor - Model: 9,307 parameters, 40.0KB (PASS 256KB limit)
2025-09-23 19:45:17,283 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 35.976s
2025-09-23 19:45:17,397 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9639
2025-09-23 19:45:17,398 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.115s
2025-09-23 19:45:17,398 - INFO - bo.run_bo - Recorded observation #37: hparams={'lr': 0.0006192350246101354, 'batch_size': np.int64(128), 'epochs': np.int64(46), 'd_model': np.int64(41), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': 3.24379327543148, 'dropout': 0.01571259373763956, 'weight_decay': 1.7162904597714704e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.03699459736880331, 'use_focal_loss': np.False_, 'focal_gamma': 1.512793270398372, 'grad_clip_norm': 1.4400157734022578, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(23570), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9639
2025-09-23 19:45:17,398 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 37: {'lr': 0.0006192350246101354, 'batch_size': np.int64(128), 'epochs': np.int64(46), 'd_model': np.int64(41), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': 3.24379327543148, 'dropout': 0.01571259373763956, 'weight_decay': 1.7162904597714704e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.03699459736880331, 'use_focal_loss': np.False_, 'focal_gamma': 1.512793270398372, 'grad_clip_norm': 1.4400157734022578, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(23570), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9639
2025-09-23 19:45:17,398 - INFO - bo.run_bo - ğŸ”BO Trial 38: Using RF surrogate + Expected Improvement
2025-09-23 19:45:17,398 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:45:17,398 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:45:17,398 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:45:17,398 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003855348595891848, 'batch_size': 256, 'epochs': 35, 'd_model': 67, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 2.600463841886059, 'dropout': 0.023985607975430208, 'weight_decay': 1.4996651151671035e-06, 'patch_size': 100, 'label_smoothing': 0.18740956968901612, 'use_focal_loss': False, 'focal_gamma': 1.1205942459457119, 'grad_clip_norm': 1.244182600775791, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 16161, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:45:17,400 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003855348595891848, 'batch_size': 256, 'epochs': 35, 'd_model': 67, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 2.600463841886059, 'dropout': 0.023985607975430208, 'weight_decay': 1.4996651151671035e-06, 'patch_size': 100, 'label_smoothing': 0.18740956968901612, 'use_focal_loss': False, 'focal_gamma': 1.1205942459457119, 'grad_clip_norm': 1.244182600775791, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 16161, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:45:35,601 - INFO - _models.training_function_executor - Model: 15,209 parameters, 65.4KB storage
2025-09-23 19:45:35,601 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0535897284347535, 0.9005500212859283, 0.8297182055574661, 0.7943486849123407, 0.7759889441093951, 0.762460612221566, 0.7529851719463984, 0.7420725231294184, 0.7363329004330977, 0.7287399908080494, 0.7240433171477842, 0.7213028349031113, 0.719379767709719, 0.7135219429122047, 0.710070708396011, 0.706199422660255, 0.703499685704878, 0.7017191872502487, 0.6989643112708612, 0.6940589469316306, 0.6936063049461176, 0.6904158780450298, 0.6899943914314269, 0.6879363628092952, 0.6851335631476508, 0.6830736777016391, 0.6845677575214993, 0.6821825662318863, 0.6805265770106971, 0.679692003547938, 0.679948838240774, 0.6799311289750547, 0.6784083959684595, 0.6785464385295726, 0.6784955841857225], 'val_losses': [0.9482695757707936, 0.8620268775621851, 0.8126478424915887, 0.7786194696385764, 0.7762372741094584, 0.7477293348808274, 0.7438888265635193, 0.7401027813424982, 0.7308136086358774, 0.7281710355671078, 0.7263960386409958, 0.7318754029026039, 0.7305099287138832, 0.7190292868051384, 0.72168138876787, 0.7118864655569374, 0.7134009820598511, 0.7121326528146981, 0.7073140764741308, 0.7111311901812116, 0.7073332784229943, 0.7073331710435202, 0.7064855548615833, 0.7037791755022881, 0.7020538225312563, 0.7013213763020717, 0.7007333000954075, 0.7014716529350296, 0.7009729814849063, 0.7008878473141575, 0.7011773714056112, 0.7006596393296032, 0.7004304773421264, 0.700353949257581, 0.7004108457484053], 'val_acc': [0.8120536273649919, 0.8655556947750909, 0.8980077684500689, 0.9216890114020799, 0.9238190702919433, 0.9376018042851773, 0.9387294825209873, 0.9402330535020674, 0.9452449567723343, 0.9482520987344945, 0.9486279914797644, 0.9448690640270643, 0.946122039844631, 0.9527628116777346, 0.9491291818067912, 0.955268763312868, 0.9542663826588147, 0.954516977822328, 0.9591529883473249, 0.9556446560581381, 0.9576494173662449, 0.9584012028567849, 0.9586517980202982, 0.9602806665831349, 0.9611577496554317, 0.9616589399824583, 0.9611577496554317, 0.9609071544919183, 0.9624107254729983, 0.9629119158000251, 0.9620348327277284, 0.9622854278912417, 0.962536023054755, 0.9626613206365117, 0.9629119158000251], 'model_size_bytes': 217529, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003855348595891848, 'batch_size': 256, 'epochs': 35, 'd_model': 67, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 2.600463841886059, 'dropout': 0.023985607975430208, 'weight_decay': 1.4996651151671035e-06, 'patch_size': 100, 'label_smoothing': 0.18740956968901612, 'use_focal_loss': False, 'focal_gamma': 1.1205942459457119, 'grad_clip_norm': 1.244182600775791, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 16161, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 15209, 'model_storage_size_kb': 65.351171875, 'model_size_validation': 'PASS'}
2025-09-23 19:45:35,601 - INFO - _models.training_function_executor - BO Objective: base=0.9629, size_penalty=0.0000, final=0.9629
2025-09-23 19:45:35,601 - INFO - _models.training_function_executor - Model: 15,209 parameters, 65.4KB (PASS 256KB limit)
2025-09-23 19:45:35,601 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 18.203s
2025-09-23 19:45:35,717 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9629
2025-09-23 19:45:35,717 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.116s
2025-09-23 19:45:35,717 - INFO - bo.run_bo - Recorded observation #38: hparams={'lr': 0.0003855348595891848, 'batch_size': np.int64(256), 'epochs': np.int64(35), 'd_model': np.int64(67), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 2.600463841886059, 'dropout': 0.023985607975430208, 'weight_decay': 1.4996651151671035e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.18740956968901612, 'use_focal_loss': np.False_, 'focal_gamma': 1.1205942459457119, 'grad_clip_norm': 1.244182600775791, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(16161), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9629
2025-09-23 19:45:35,717 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 38: {'lr': 0.0003855348595891848, 'batch_size': np.int64(256), 'epochs': np.int64(35), 'd_model': np.int64(67), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 2.600463841886059, 'dropout': 0.023985607975430208, 'weight_decay': 1.4996651151671035e-06, 'patch_size': np.int64(100), 'label_smoothing': 0.18740956968901612, 'use_focal_loss': np.False_, 'focal_gamma': 1.1205942459457119, 'grad_clip_norm': 1.244182600775791, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(16161), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9629
2025-09-23 19:45:35,718 - INFO - bo.run_bo - ğŸ”BO Trial 39: Using RF surrogate + Expected Improvement
2025-09-23 19:45:35,718 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:45:35,718 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:45:35,718 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:45:35,718 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0111925400447249e-05, 'batch_size': 32, 'epochs': 50, 'd_model': 72, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 2.798738274395404, 'dropout': 0.03680334963065902, 'weight_decay': 1.8059754733224062e-06, 'patch_size': 40, 'label_smoothing': 0.19601478022972316, 'use_focal_loss': True, 'focal_gamma': 1.1254503775349731, 'grad_clip_norm': 1.6469451388521077, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42920, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:45:35,720 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0111925400447249e-05, 'batch_size': 32, 'epochs': 50, 'd_model': 72, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 2.798738274395404, 'dropout': 0.03680334963065902, 'weight_decay': 1.8059754733224062e-06, 'patch_size': 40, 'label_smoothing': 0.19601478022972316, 'use_focal_loss': True, 'focal_gamma': 1.1254503775349731, 'grad_clip_norm': 1.6469451388521077, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42920, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:48:15,345 - INFO - _models.training_function_executor - Model: 210,413 parameters, 904.1KB storage
2025-09-23 19:48:15,346 - WARNING - _models.training_function_executor - Model storage 904.1KB exceeds 256KB limit!
2025-09-23 19:48:15,346 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0477757191361519, 0.7287933371611729, 0.6519560687181156, 0.6275325301550677, 0.6057305461302163, 0.5426169617901327, 0.5020026679473746, 0.47368563391102314, 0.44521981562878915, 0.419204821255528, 0.39369235899638616, 0.3728080565116909, 0.35000933393594696, 0.330783842594254, 0.31212134274053077, 0.2931844639687768, 0.27591398498847164, 0.25976610103778974, 0.2494298864711598, 0.23493760778692005, 0.22289405309294705, 0.21337716382472874, 0.19900500356284678, 0.19064528444590575, 0.18311956218078174, 0.17708664981471123, 0.17248216620555248, 0.16657807295057583, 0.16335022533934723, 0.15787312829858194, 0.15615644062072886, 0.15185323302641557, 0.14943880778707755, 0.1469176255638683, 0.14470916741496956, 0.14374537050959288, 0.14085001596042065, 0.1397149557915785, 0.13880333952722188, 0.13748355378803406, 0.13659015964597992, 0.13819482389252738, 0.13667524616375412, 0.13516963672060808, 0.13542760065170778, 0.1360184656153228, 0.1347309220231575, 0.13403942769660734, 0.1323563573656235, 0.13463837688477368], 'val_losses': [0.813420169799493, 0.6582775238447209, 0.6305142486323959, 0.6120156952274606, 0.5825095474309244, 0.5048890263999768, 0.4826484800221522, 0.45136415295404625, 0.4243152708920481, 0.3954670622117716, 0.3690482040027257, 0.34574578064655453, 0.3315430849925003, 0.30445025286949634, 0.283219692575358, 0.2651410432421105, 0.24951283258332627, 0.23590609013787817, 0.22075739307796996, 0.21041810906403138, 0.19785883493395678, 0.1830629007264182, 0.175899537114732, 0.16739538339124319, 0.15781337508688803, 0.1614008828486032, 0.1496942698957403, 0.14802898948760337, 0.1415991070477859, 0.13897709353643617, 0.13555878641189092, 0.13343328873647542, 0.1338848771606527, 0.13003526191494247, 0.12905730008929614, 0.12895572165538607, 0.1259720970377888, 0.12448064825776346, 0.12345261756802633, 0.1222604178064913, 0.1226050172338598, 0.1219519806248385, 0.12128855519079801, 0.12120007720115773, 0.12056131852874032, 0.12053463379215916, 0.12026676456442215, 0.12036149432681255, 0.1204248051121859, 0.12042523920816103], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7565467986467861, 0.7560456083197594, 0.76769828342313, 0.7719584012028567, 0.7853652424508207, 0.7996491667710813, 0.8096729733116151, 0.8252098734494424, 0.8308482646284927, 0.8465104623480767, 0.8602931963413106, 0.860543791504824, 0.8715699786994111, 0.8763312868061647, 0.8799649166771081, 0.8923693772710186, 0.9048991354466859, 0.9100363362987094, 0.915925322641273, 0.9189324646034331, 0.915549429896003, 0.9248214509459968, 0.9282044856534268, 0.9275779977446436, 0.9302092469615336, 0.9333416865054505, 0.9329657937601804, 0.9334669840872071, 0.9363488284676106, 0.9354717453953139, 0.9352211502318005, 0.9365994236311239, 0.9386041849392307, 0.9387294825209873, 0.938102994612204, 0.9389800776845006, 0.939230672848014, 0.939606565593284, 0.9402330535020674, 0.9401077559203107, 0.9404836486655808, 0.9411101365743642, 0.9413607317378775, 0.9414860293196341, 0.9414860293196341], 'model_size_bytes': 440495, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0111925400447249e-05, 'batch_size': 32, 'epochs': 50, 'd_model': 72, 'n_heads': 2, 'num_layers': 4, 'mlp_ratio': 2.798738274395404, 'dropout': 0.03680334963065902, 'weight_decay': 1.8059754733224062e-06, 'patch_size': 40, 'label_smoothing': 0.19601478022972316, 'use_focal_loss': True, 'focal_gamma': 1.1254503775349731, 'grad_clip_norm': 1.6469451388521077, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42920, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 210413, 'model_storage_size_kb': 904.1183593750001, 'model_size_validation': 'FAIL'}
2025-09-23 19:48:15,346 - INFO - _models.training_function_executor - BO Objective: base=0.9415, size_penalty=0.8000, final=0.1415
2025-09-23 19:48:15,346 - INFO - _models.training_function_executor - Model: 210,413 parameters, 904.1KB (FAIL 256KB limit)
2025-09-23 19:48:15,346 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 159.628s
2025-09-23 19:48:15,462 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1415
2025-09-23 19:48:15,462 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.116s
2025-09-23 19:48:15,462 - INFO - bo.run_bo - Recorded observation #39: hparams={'lr': 1.0111925400447249e-05, 'batch_size': np.int64(32), 'epochs': np.int64(50), 'd_model': np.int64(72), 'n_heads': np.int64(2), 'num_layers': np.int64(4), 'mlp_ratio': 2.798738274395404, 'dropout': 0.03680334963065902, 'weight_decay': 1.8059754733224062e-06, 'patch_size': np.int64(40), 'label_smoothing': 0.19601478022972316, 'use_focal_loss': np.True_, 'focal_gamma': 1.1254503775349731, 'grad_clip_norm': 1.6469451388521077, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(42920), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.1415
2025-09-23 19:48:15,462 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 39: {'lr': 1.0111925400447249e-05, 'batch_size': np.int64(32), 'epochs': np.int64(50), 'd_model': np.int64(72), 'n_heads': np.int64(2), 'num_layers': np.int64(4), 'mlp_ratio': 2.798738274395404, 'dropout': 0.03680334963065902, 'weight_decay': 1.8059754733224062e-06, 'patch_size': np.int64(40), 'label_smoothing': 0.19601478022972316, 'use_focal_loss': np.True_, 'focal_gamma': 1.1254503775349731, 'grad_clip_norm': 1.6469451388521077, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(42920), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.1415
2025-09-23 19:48:15,462 - INFO - bo.run_bo - ğŸ”BO Trial 40: Using RF surrogate + Expected Improvement
2025-09-23 19:48:15,462 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:48:15,463 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:48:15,463 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:48:15,463 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006622201186735045, 'batch_size': 32, 'epochs': 34, 'd_model': 76, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 3.322541723464661, 'dropout': 0.03834080695669762, 'weight_decay': 0.000101278876963768, 'patch_size': 50, 'label_smoothing': 0.0956008162672402, 'use_focal_loss': True, 'focal_gamma': 1.7139600489344424, 'grad_clip_norm': 0.06788467884584605, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 32869, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:48:15,464 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006622201186735045, 'batch_size': 32, 'epochs': 34, 'd_model': 76, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 3.322541723464661, 'dropout': 0.03834080695669762, 'weight_decay': 0.000101278876963768, 'patch_size': 50, 'label_smoothing': 0.0956008162672402, 'use_focal_loss': True, 'focal_gamma': 1.7139600489344424, 'grad_clip_norm': 0.06788467884584605, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 32869, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:49:41,961 - INFO - _models.training_function_executor - Model: 10,412 parameters, 44.7KB storage
2025-09-23 19:49:41,961 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.45620625043831736, 0.40610038195192466, 0.3736621219928195, 0.35285953678246956, 0.32792719202373155, 0.29967173607560454, 0.27160763442497654, 0.2545175136724585, 0.23953519837508183, 0.2229470775766943, 0.21564237053496713, 0.20912881580035578, 0.20303804333024847, 0.1998848519392884, 0.18763716662975308, 0.1853960050756059, 0.1777151530598674, 0.17644850047418947, 0.17358474094513213, 0.1679353949324492, 0.17463451405741556, 0.16609988375250506, 0.1598374393075696, 0.15381101607838524, 0.14670001417228076, 0.1459043326638017, 0.14938251809449407, 0.1455595641912824, 0.1616447561628832, 0.18616508799840067, 0.18883376012452133, 0.20688214343009323, 0.16847066676228212, 0.17127940294154287], 'val_losses': [0.47538983654578576, 0.3774634837981021, 0.34002075645203966, 0.34550216022536745, 0.299545737603989, 0.2866848517989264, 0.2655771604577277, 0.24776060456710416, 0.22001016638307638, 0.21242485808827288, 0.20427595774805676, 0.1980828447081872, 0.21509145453615217, 0.19807951919881045, 0.19138245391540995, 0.1886737044705079, 0.1764845698082332, 0.19006289919804936, 0.15095921130000223, 0.16143026221494539, 0.16237114775585598, 0.14517259772910374, 0.17283827815120373, 0.15680374111945192, 0.14561102875281867, 0.1437288154345559, 0.14651865961632024, 0.15013470201321436, 0.19152788011117503, 0.18350186003746655, 0.19992551944291975, 0.1848126687160815, 0.16450623093601635, 0.15975447699414536], 'val_acc': [0.7215887733366746, 0.7544167397569227, 0.777220899636637, 0.7636887608069164, 0.8106753539656685, 0.8080441047487783, 0.822328029069039, 0.8373637388798396, 0.8634256358852274, 0.8630497431399574, 0.8718205738629244, 0.874702418243328, 0.8619220649041474, 0.8718205738629244, 0.8822202731487282, 0.8854780102744017, 0.8779601553690014, 0.8775842626237313, 0.9031449693020924, 0.8978824708683122, 0.8965041974689888, 0.9117905024433028, 0.8886104498183185, 0.9022678862297958, 0.9051497306101992, 0.909033955644656, 0.9059015161007392, 0.9100363362987094, 0.8782107505325147, 0.8874827715825084, 0.861295576995364, 0.8804661070041349, 0.8968800902142589, 0.9025184813933091], 'model_size_bytes': 259385, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006622201186735045, 'batch_size': 32, 'epochs': 34, 'd_model': 76, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 3.322541723464661, 'dropout': 0.03834080695669762, 'weight_decay': 0.000101278876963768, 'patch_size': 50, 'label_smoothing': 0.0956008162672402, 'use_focal_loss': True, 'focal_gamma': 1.7139600489344424, 'grad_clip_norm': 0.06788467884584605, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 32869, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 10412, 'model_storage_size_kb': 44.7390625, 'model_size_validation': 'PASS'}
2025-09-23 19:49:41,961 - INFO - _models.training_function_executor - BO Objective: base=0.9025, size_penalty=0.0000, final=0.9025
2025-09-23 19:49:41,961 - INFO - _models.training_function_executor - Model: 10,412 parameters, 44.7KB (PASS 256KB limit)
2025-09-23 19:49:41,961 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 86.498s
2025-09-23 19:49:42,079 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9025
2025-09-23 19:49:42,079 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.118s
2025-09-23 19:49:42,079 - INFO - bo.run_bo - Recorded observation #40: hparams={'lr': 0.006622201186735045, 'batch_size': np.int64(32), 'epochs': np.int64(34), 'd_model': np.int64(76), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 3.322541723464661, 'dropout': 0.03834080695669762, 'weight_decay': 0.000101278876963768, 'patch_size': np.int64(50), 'label_smoothing': 0.0956008162672402, 'use_focal_loss': np.True_, 'focal_gamma': 1.7139600489344424, 'grad_clip_norm': 0.06788467884584605, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(32869), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9025
2025-09-23 19:49:42,079 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 40: {'lr': 0.006622201186735045, 'batch_size': np.int64(32), 'epochs': np.int64(34), 'd_model': np.int64(76), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 3.322541723464661, 'dropout': 0.03834080695669762, 'weight_decay': 0.000101278876963768, 'patch_size': np.int64(50), 'label_smoothing': 0.0956008162672402, 'use_focal_loss': np.True_, 'focal_gamma': 1.7139600489344424, 'grad_clip_norm': 0.06788467884584605, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(32869), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9025
2025-09-23 19:49:42,079 - INFO - bo.run_bo - ğŸ”BO Trial 41: Using RF surrogate + Expected Improvement
2025-09-23 19:49:42,079 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:49:42,080 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:49:42,080 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:49:42,080 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0012506589340297654, 'batch_size': 64, 'epochs': 58, 'd_model': 85, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 1.696947326835118, 'dropout': 0.2211891719272962, 'weight_decay': 1.6674215119965873e-06, 'patch_size': 10, 'label_smoothing': 0.14004858178369625, 'use_focal_loss': True, 'focal_gamma': 1.149609182055955, 'grad_clip_norm': 1.5854358072794756, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 32382, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:49:42,081 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0012506589340297654, 'batch_size': 64, 'epochs': 58, 'd_model': 85, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 1.696947326835118, 'dropout': 0.2211891719272962, 'weight_decay': 1.6674215119965873e-06, 'patch_size': 10, 'label_smoothing': 0.14004858178369625, 'use_focal_loss': True, 'focal_gamma': 1.149609182055955, 'grad_clip_norm': 1.5854358072794756, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 32382, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:51:44,688 - INFO - _models.training_function_executor - Model: 11,305 parameters, 12.1KB storage
2025-09-23 19:51:44,689 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.37281261271852056, 0.2551832684925974, 0.21100484864190008, 0.18784583573160557, 0.1720270939621528, 0.159911844070568, 0.15122231394176203, 0.1445470178446196, 0.14057216183608162, 0.1314185993240687, 0.12750381673679642, 0.12349764020554599, 0.11764364944149466, 0.11244738693550219, 0.1104834481263446, 0.10580214866051717, 0.10451345940099542, 0.09952351412457064, 0.0992670015109305, 0.09589309065962698, 0.09498849319111408, 0.09291640146168775, 0.09006124713359775, 0.08860525655588873, 0.08662805427907329, 0.08625523411469199, 0.08230196184159273, 0.08058918256087269, 0.08019371654569805, 0.07725527012288404, 0.07522220000007568, 0.07481029244003315, 0.07378965457426113, 0.07268329622602127, 0.07087447063601379, 0.07107656642585022, 0.06735648383758022, 0.06734000920644437, 0.06573135597417257, 0.06613049738318465, 0.06300823977797933, 0.06255045199767748, 0.06063594528111368, 0.060945786330588114, 0.05950373572942753, 0.059674414222091375, 0.05726997033207353, 0.05892929678461148, 0.05806907182952345, 0.05668451417559251, 0.055786258614011826, 0.05479762130580158, 0.05432695650184856, 0.05324790057462267, 0.05487582907960539, 0.05266717837347003, 0.05400102653587962, 0.05384255910521984], 'val_losses': [0.24885189295741134, 0.2048281374709631, 0.1971813320279002, 0.15092469715146759, 0.14989137837737743, 0.13838773788929792, 0.12941896702215316, 0.12941191000416977, 0.12976791490373002, 0.1179381675781119, 0.12640217987171928, 0.13192104522356815, 0.12385926063781304, 0.12146306350171872, 0.11049022990078305, 0.11595105759210417, 0.1039475337223531, 0.11014191500862175, 0.11024865725780994, 0.11522936388101514, 0.11484449370797453, 0.10255774446723306, 0.10605178764627894, 0.09645468952822037, 0.09923671231527045, 0.10834418579164924, 0.10219528532229095, 0.09524375825556519, 0.12866568818731336, 0.10026901231465102, 0.0874030033621016, 0.09109297393258839, 0.10015495893983162, 0.09972490490513508, 0.08687335083417734, 0.10168469488030761, 0.09910098199291854, 0.09391044269952689, 0.09083449092126851, 0.08826133880011346, 0.09622641335885755, 0.09026708211920163, 0.09505449867165637, 0.0929879268642775, 0.09397240651496265, 0.09416654194141447, 0.09465848960616269, 0.10009551155822585, 0.09283365143255069, 0.09574381279521173, 0.09469441478973253, 0.09462129410093492, 0.09704220172471979, 0.09326530642160541, 0.09351619829578577, 0.09309403320673847, 0.09369647434377354, 0.09382314266992413], 'val_acc': [0.876205989224408, 0.8938729482520987, 0.9015161007392557, 0.9250720461095101, 0.92319258238316, 0.9292068663074803, 0.9307104372885603, 0.9325899010149105, 0.938102994612204, 0.938102994612204, 0.9358476381405839, 0.9329657937601804, 0.9264503195088335, 0.9373512091216639, 0.944994361608821, 0.9404836486655808, 0.9462473374263877, 0.9406089462473374, 0.9443678737000376, 0.9364741260493672, 0.947249718080441, 0.9462473374263877, 0.9476256108257111, 0.9503821576243578, 0.9495050745520611, 0.9468738253351711, 0.9473750156621977, 0.9502568600426011, 0.9397318631750408, 0.9508833479513845, 0.955268763312868, 0.9543916802405713, 0.9491291818067912, 0.9483773963162511, 0.9557699536398947, 0.9513845382784112, 0.9531387044230046, 0.9531387044230046, 0.953389299586518, 0.955268763312868, 0.9532640020047614, 0.9566470367121914, 0.954516977822328, 0.953389299586518, 0.955268763312868, 0.953765192331788, 0.9553940608946248, 0.9508833479513845, 0.9540157874953014, 0.9527628116777346, 0.9531387044230046, 0.9535145971682747, 0.953389299586518, 0.9555193584763814, 0.9548928705675981, 0.9553940608946248, 0.9551434657311114, 0.9553940608946248], 'model_size_bytes': 176501, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0012506589340297654, 'batch_size': 64, 'epochs': 58, 'd_model': 85, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 1.696947326835118, 'dropout': 0.2211891719272962, 'weight_decay': 1.6674215119965873e-06, 'patch_size': 10, 'label_smoothing': 0.14004858178369625, 'use_focal_loss': True, 'focal_gamma': 1.149609182055955, 'grad_clip_norm': 1.5854358072794756, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 32382, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 11305, 'model_storage_size_kb': 12.144042968750002, 'model_size_validation': 'PASS'}
2025-09-23 19:51:44,689 - INFO - _models.training_function_executor - BO Objective: base=0.9554, size_penalty=0.0000, final=0.9554
2025-09-23 19:51:44,689 - INFO - _models.training_function_executor - Model: 11,305 parameters, 12.1KB (PASS 256KB limit)
2025-09-23 19:51:44,689 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 122.609s
2025-09-23 19:51:44,806 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9554
2025-09-23 19:51:44,806 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.117s
2025-09-23 19:51:44,806 - INFO - bo.run_bo - Recorded observation #41: hparams={'lr': 0.0012506589340297654, 'batch_size': np.int64(64), 'epochs': np.int64(58), 'd_model': np.int64(85), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': 1.696947326835118, 'dropout': 0.2211891719272962, 'weight_decay': 1.6674215119965873e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.14004858178369625, 'use_focal_loss': np.True_, 'focal_gamma': 1.149609182055955, 'grad_clip_norm': 1.5854358072794756, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(32382), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9554
2025-09-23 19:51:44,806 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 41: {'lr': 0.0012506589340297654, 'batch_size': np.int64(64), 'epochs': np.int64(58), 'd_model': np.int64(85), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': 1.696947326835118, 'dropout': 0.2211891719272962, 'weight_decay': 1.6674215119965873e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.14004858178369625, 'use_focal_loss': np.True_, 'focal_gamma': 1.149609182055955, 'grad_clip_norm': 1.5854358072794756, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(32382), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9554
2025-09-23 19:51:44,807 - INFO - bo.run_bo - ğŸ”BO Trial 42: Using RF surrogate + Expected Improvement
2025-09-23 19:51:44,807 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:51:44,807 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:51:44,807 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:51:44,807 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0004259397514062718, 'batch_size': 64, 'epochs': 56, 'd_model': 76, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 3.759278854855002, 'dropout': 0.007554686890401121, 'weight_decay': 7.835564929932141e-06, 'patch_size': 10, 'label_smoothing': 0.13222907672452802, 'use_focal_loss': False, 'focal_gamma': 1.5725884578701557, 'grad_clip_norm': 0.27332187687488224, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 24267, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:51:44,809 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0004259397514062718, 'batch_size': 64, 'epochs': 56, 'd_model': 76, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 3.759278854855002, 'dropout': 0.007554686890401121, 'weight_decay': 7.835564929932141e-06, 'patch_size': 10, 'label_smoothing': 0.13222907672452802, 'use_focal_loss': False, 'focal_gamma': 1.5725884578701557, 'grad_clip_norm': 0.27332187687488224, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 24267, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:54:55,625 - INFO - _models.training_function_executor - Model: 279,469 parameters, 1200.8KB storage
2025-09-23 19:54:55,626 - WARNING - _models.training_function_executor - Model storage 1200.8KB exceeds 256KB limit!
2025-09-23 19:54:55,626 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8202314716780861, 0.6513394295853735, 0.6262470033949032, 0.6097810568955295, 0.6018822370038401, 0.5926204433687027, 0.5851726239103163, 0.5800239442229267, 0.5757608404267243, 0.5707195189775681, 0.568613483697569, 0.5645504245945367, 0.5611243489686848, 0.5580735646800447, 0.5540517877973015, 0.5526387976049626, 0.5492017895652661, 0.547973169316976, 0.5446581927654465, 0.5418808393784156, 0.5397239564071789, 0.5376670214256256, 0.5368494501826541, 0.53242082497315, 0.5307433745826923, 0.5281278426510586, 0.5274275249415866, 0.5243597568151321, 0.5231221678359831, 0.5215137740766046, 0.5191908256883546, 0.5177690946596127, 0.515213361618584, 0.5136297713734469, 0.5123100491262851, 0.5107562697755661, 0.5098409390834527, 0.5084555736722696, 0.5063734822593954, 0.5052709534563106, 0.5036792173252083, 0.5025572764303281, 0.5005133390385559, 0.5013152808599804, 0.4995845071195387, 0.49893376100213815, 0.4987405557183661, 0.49808394167371195, 0.4975971609827339, 0.4966284288522598, 0.49634931460211973, 0.4957396561473667, 0.4957828828191203, 0.49627033031453943, 0.49560198884177165, 0.49515227379291177], 'val_losses': [0.6740131909544643, 0.6413426526939192, 0.6181436579428494, 0.6021623916755029, 0.599388385040093, 0.6161977265162242, 0.5919509447691897, 0.5815148399581498, 0.5785635161588043, 0.5755944981071349, 0.5821006191044191, 0.5882198625064378, 0.5732903106143011, 0.5707362871722103, 0.5700348014148042, 0.5660722751976508, 0.56197225021369, 0.5680167534912668, 0.5562604311982009, 0.5659408667992416, 0.5569457154631868, 0.5636578900759388, 0.5543018024288283, 0.559345123530659, 0.555679159316185, 0.5582668453197315, 0.5614900855906938, 0.5645372855933652, 0.5580137081574026, 0.5596193222305915, 0.5560702127242891, 0.5586415221182988, 0.5625641690864701, 0.5601877264996815, 0.5557309770745422, 0.5587232474500472, 0.5578993832821566, 0.5558579453712805, 0.55350722302143, 0.5588761088304719, 0.5569762494193329, 0.5573699546926766, 0.5608592574689908, 0.5580455108687269, 0.5593580981795164, 0.5588513622306639, 0.5600224219831126, 0.5597574221223028, 0.5595159827131867, 0.5593402410506485, 0.5591769890518747, 0.5609534413888332, 0.5602773921992483, 0.5597111092983758, 0.559767590326678, 0.5597378748234852], 'val_acc': [0.9149229419872197, 0.9253226412730234, 0.937727101866934, 0.9439919809547676, 0.9446184688635509, 0.937727101866934, 0.9469991229169277, 0.953765192331788, 0.9557699536398947, 0.9565217391304348, 0.9542663826588147, 0.9496303721338178, 0.9587770956020549, 0.9579000125297582, 0.9571482270392181, 0.9611577496554317, 0.961784237564215, 0.9619095351459717, 0.9652925698534018, 0.9629119158000251, 0.9664202480892119, 0.9626613206365117, 0.9672973311615086, 0.9651672722716451, 0.9676732239067786, 0.9655431650169152, 0.9647913795263752, 0.9622854278912417, 0.9674226287432652, 0.9657937601804285, 0.9662949505074552, 0.9669214384162386, 0.9645407843628618, 0.9669214384162386, 0.969051497306102, 0.9676732239067786, 0.9684250093973187, 0.9689261997243453, 0.9700538779601554, 0.9681744142338052, 0.9700538779601554, 0.9685503069790753, 0.9684250093973187, 0.969051497306102, 0.9688009021425886, 0.9705550682871821, 0.9693020924696153, 0.9689261997243453, 0.969427390051372, 0.9691767948878587, 0.9699285803783987, 0.9695526876331286, 0.9696779852148854, 0.9698032827966421, 0.9699285803783987, 0.9699285803783987], 'model_size_bytes': 577775, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0004259397514062718, 'batch_size': 64, 'epochs': 56, 'd_model': 76, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 3.759278854855002, 'dropout': 0.007554686890401121, 'weight_decay': 7.835564929932141e-06, 'patch_size': 10, 'label_smoothing': 0.13222907672452802, 'use_focal_loss': False, 'focal_gamma': 1.5725884578701557, 'grad_clip_norm': 0.27332187687488224, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 24267, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 279469, 'model_storage_size_kb': 1200.843359375, 'model_size_validation': 'FAIL'}
2025-09-23 19:54:55,626 - INFO - _models.training_function_executor - BO Objective: base=0.9699, size_penalty=0.8000, final=0.1699
2025-09-23 19:54:55,626 - INFO - _models.training_function_executor - Model: 279,469 parameters, 1200.8KB (FAIL 256KB limit)
2025-09-23 19:54:55,626 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 190.819s
2025-09-23 19:54:55,745 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1699
2025-09-23 19:54:55,745 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.120s
2025-09-23 19:54:55,745 - INFO - bo.run_bo - Recorded observation #42: hparams={'lr': 0.0004259397514062718, 'batch_size': np.int64(64), 'epochs': np.int64(56), 'd_model': np.int64(76), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 3.759278854855002, 'dropout': 0.007554686890401121, 'weight_decay': 7.835564929932141e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.13222907672452802, 'use_focal_loss': np.False_, 'focal_gamma': 1.5725884578701557, 'grad_clip_norm': 0.27332187687488224, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(24267), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.1699
2025-09-23 19:54:55,745 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 42: {'lr': 0.0004259397514062718, 'batch_size': np.int64(64), 'epochs': np.int64(56), 'd_model': np.int64(76), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 3.759278854855002, 'dropout': 0.007554686890401121, 'weight_decay': 7.835564929932141e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.13222907672452802, 'use_focal_loss': np.False_, 'focal_gamma': 1.5725884578701557, 'grad_clip_norm': 0.27332187687488224, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(24267), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.1699
2025-09-23 19:54:55,746 - INFO - bo.run_bo - ğŸ”BO Trial 43: Using RF surrogate + Expected Improvement
2025-09-23 19:54:55,746 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:54:55,746 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:54:55,746 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:54:55,746 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.007870023927856115, 'batch_size': 64, 'epochs': 48, 'd_model': 69, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 2.7057295737960154, 'dropout': 0.03913237883024152, 'weight_decay': 4.7321006941496395e-06, 'patch_size': 50, 'label_smoothing': 0.11223182331948489, 'use_focal_loss': False, 'focal_gamma': 1.4323353800552927, 'grad_clip_norm': 0.30783233416212036, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 44208, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:54:55,748 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.007870023927856115, 'batch_size': 64, 'epochs': 48, 'd_model': 69, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 2.7057295737960154, 'dropout': 0.03913237883024152, 'weight_decay': 4.7321006941496395e-06, 'patch_size': 50, 'label_smoothing': 0.11223182331948489, 'use_focal_loss': False, 'focal_gamma': 1.4323353800552927, 'grad_clip_norm': 0.30783233416212036, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 44208, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:56:18,379 - INFO - _models.training_function_executor - Model: 9,729 parameters, 20.9KB storage
2025-09-23 19:56:18,379 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9679221075153163, 0.9486702252714436, 0.9048423368287849, 0.8771361749998158, 0.8953734668197479, 0.8922830366219725, 0.8567579196471303, 0.8371823009339147, 0.8319608166198161, 0.8200831948652179, 0.8363597939360453, 0.814804536150694, 0.7995392892928521, 0.7786119283730011, 0.7627398276603137, 0.7515641469814737, 0.7560281688825038, 0.7391135538221469, 0.7308952298278039, 0.7263759880986342, 0.7333732652652235, 0.7317372422706317, 0.7161303905901057, 0.7065631569400899, 0.6947179159320312, 0.6764934647801384, 0.6742740481474815, 0.6772180608507647, 0.6746542245542645, 0.6651629760716227, 0.6618043900365143, 0.6541728275713905, 0.6469041207595281, 0.6420996344836737, 0.6333409955364658, 0.6322524238939001, 0.6287459921232648, 0.6238133289743567, 0.6246081641625332, 0.6185503357768907, 0.6150968913460937, 0.6140096722806646, 0.6120182876355688, 0.6120198263514218, 0.6111818651834406, 0.6114111835151538, 0.6103143312987598, 0.6113633789133541], 'val_losses': [0.9419117758647793, 0.9476596103501999, 0.8980097614901807, 0.8751697425943257, 0.9006902288784137, 0.8748128927078123, 0.9125548827291178, 0.8011496932550475, 0.7928182098474468, 0.8274002879533146, 0.8023506019844655, 0.8015621781812218, 0.783424877669336, 0.7592979435364475, 0.7632167147538549, 0.7431983772472486, 0.743758156296305, 0.7342117428704918, 0.7191453445989855, 0.7141247082587069, 0.720719266693599, 0.7069023393282181, 0.6881224620373622, 0.6984923018308051, 0.6646678158848854, 0.6582076114948614, 0.6540767453604363, 0.6607952323149296, 0.6562071363514579, 0.6536287516598235, 0.6471298735268118, 0.6396686830790684, 0.6331361268878416, 0.6249912726731247, 0.6212238700911749, 0.6192522090772775, 0.6232817104967775, 0.6205056510000297, 0.6113869171216664, 0.610578220218603, 0.6085856074065646, 0.6093642500734108, 0.6081731835786987, 0.6071644348095061, 0.6048891496320757, 0.6041353981442028, 0.6043900068116029, 0.6047230177379136], 'val_acc': [0.755419120410976, 0.7443929332163889, 0.7534143591028694, 0.7693271519859667, 0.7668212003508332, 0.7714572108758301, 0.7787244706177171, 0.8153113644906653, 0.832602430773086, 0.7950131562460845, 0.8121789249467485, 0.8193208871068789, 0.8275905275028193, 0.8416238566595665, 0.8427515348953765, 0.8561583761433404, 0.8522741511088836, 0.8556571858163138, 0.8645533141210374, 0.8601678987595539, 0.8575366495426638, 0.8650545044480642, 0.8777095602054881, 0.8715699786994111, 0.8898634256358853, 0.8916175917804786, 0.8952512216514221, 0.8924946748527753, 0.892244079689262, 0.8904899135446686, 0.8965041974689888, 0.901766695902769, 0.9016413983210124, 0.9085327653176294, 0.9092845508081694, 0.9077809798270894, 0.9070291943365493, 0.9097857411351961, 0.9137952637514096, 0.9150482395689763, 0.9160506202230297, 0.915549429896003, 0.9170530008770831, 0.9175541912041097, 0.9175541912041097, 0.9178047863676231, 0.9183059766946498, 0.9176794887858665], 'model_size_bytes': 235901, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.007870023927856115, 'batch_size': 64, 'epochs': 48, 'd_model': 69, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 2.7057295737960154, 'dropout': 0.03913237883024152, 'weight_decay': 4.7321006941496395e-06, 'patch_size': 50, 'label_smoothing': 0.11223182331948489, 'use_focal_loss': False, 'focal_gamma': 1.4323353800552927, 'grad_clip_norm': 0.30783233416212036, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 44208, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 9729, 'model_storage_size_kb': 20.902148437500003, 'model_size_validation': 'PASS'}
2025-09-23 19:56:18,379 - INFO - _models.training_function_executor - BO Objective: base=0.9177, size_penalty=0.0000, final=0.9177
2025-09-23 19:56:18,379 - INFO - _models.training_function_executor - Model: 9,729 parameters, 20.9KB (PASS 256KB limit)
2025-09-23 19:56:18,379 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 82.633s
2025-09-23 19:56:18,501 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9177
2025-09-23 19:56:18,501 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.122s
2025-09-23 19:56:18,501 - INFO - bo.run_bo - Recorded observation #43: hparams={'lr': 0.007870023927856115, 'batch_size': np.int64(64), 'epochs': np.int64(48), 'd_model': np.int64(69), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 2.7057295737960154, 'dropout': 0.03913237883024152, 'weight_decay': 4.7321006941496395e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.11223182331948489, 'use_focal_loss': np.False_, 'focal_gamma': 1.4323353800552927, 'grad_clip_norm': 0.30783233416212036, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(44208), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9177
2025-09-23 19:56:18,501 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 43: {'lr': 0.007870023927856115, 'batch_size': np.int64(64), 'epochs': np.int64(48), 'd_model': np.int64(69), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 2.7057295737960154, 'dropout': 0.03913237883024152, 'weight_decay': 4.7321006941496395e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.11223182331948489, 'use_focal_loss': np.False_, 'focal_gamma': 1.4323353800552927, 'grad_clip_norm': 0.30783233416212036, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(44208), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9177
2025-09-23 19:56:18,502 - INFO - bo.run_bo - ğŸ”BO Trial 44: Using RF surrogate + Expected Improvement
2025-09-23 19:56:18,502 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:56:18,502 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:56:18,502 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:56:18,502 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0004415675856195804, 'batch_size': 64, 'epochs': 36, 'd_model': 74, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 3.5900789816664935, 'dropout': 0.02944373853092181, 'weight_decay': 1.5856321706929264e-06, 'patch_size': 10, 'label_smoothing': 0.07518428120019957, 'use_focal_loss': True, 'focal_gamma': 1.337248166293211, 'grad_clip_norm': 1.342774968183723, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 23494, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:56:18,504 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0004415675856195804, 'batch_size': 64, 'epochs': 36, 'd_model': 74, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 3.5900789816664935, 'dropout': 0.02944373853092181, 'weight_decay': 1.5856321706929264e-06, 'patch_size': 10, 'label_smoothing': 0.07518428120019957, 'use_focal_loss': True, 'focal_gamma': 1.337248166293211, 'grad_clip_norm': 1.342774968183723, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 23494, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:57:47,286 - INFO - _models.training_function_executor - Model: 9,842 parameters, 10.6KB storage
2025-09-23 19:57:47,287 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5820730424241359, 0.4052244463759108, 0.3437709303316801, 0.31084369798559214, 0.2624068481093962, 0.2135054965291156, 0.17422353447193592, 0.14386008313861073, 0.1253426707715064, 0.11436976613512173, 0.10393628875587241, 0.09704164270801367, 0.09145862986566623, 0.08363659792524776, 0.08062621415292967, 0.07572247784596313, 0.07067405897496738, 0.0660731833316933, 0.06177186938145367, 0.058224823480728174, 0.05429367541071177, 0.05201995996034534, 0.04928409171497277, 0.04645447015082691, 0.04293769202500706, 0.04065774905254689, 0.038478331359533526, 0.0353302958604943, 0.03362354202817934, 0.032247568583859, 0.029886980450610938, 0.028255054755728894, 0.027845901674376138, 0.027197721768192767, 0.026514289202088356, 0.02670889891793983], 'val_losses': [0.45504722426356176, 0.35636189370103954, 0.3250837068019423, 0.2770549352372399, 0.23708543950002633, 0.19422592070850345, 0.15113088057283441, 0.14506492513195643, 0.11887732649668015, 0.10364292383421754, 0.09893458907544621, 0.09631208519808049, 0.09813790561232467, 0.08979175425669167, 0.08507121832780619, 0.07524378399265812, 0.07390845593407165, 0.07449283788689932, 0.06940255950879161, 0.07233213598851258, 0.06858539646459902, 0.06186604066841167, 0.05984017125587239, 0.06117614720596913, 0.058621576896303505, 0.06070904446049989, 0.05788616580467836, 0.05993732532012315, 0.05802816100107699, 0.05761849401125916, 0.05720686730122958, 0.05888863721933151, 0.05736837131780515, 0.0568349825594303, 0.05704477887571254, 0.057057619257284545], 'val_acc': [0.7499060268136825, 0.8139330910913419, 0.8255857661947125, 0.8421250469865932, 0.8556571858163138, 0.8782107505325147, 0.9180553815311364, 0.9173035960405964, 0.9303345445432903, 0.9417366244831474, 0.9446184688635509, 0.9432401954642275, 0.9427390051372009, 0.9503821576243578, 0.9491291818067912, 0.9566470367121914, 0.9553940608946248, 0.9530134068412479, 0.9579000125297582, 0.9587770956020549, 0.954516977822328, 0.961784237564215, 0.962536023054755, 0.962160130309485, 0.9631625109635384, 0.9650419746898885, 0.9641648916175918, 0.9657937601804285, 0.9666708432527252, 0.9672973311615086, 0.9661696529256986, 0.9662949505074552, 0.9671720335797519, 0.9680491166520486, 0.967547926325022, 0.9676732239067786], 'model_size_bytes': 186229, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0004415675856195804, 'batch_size': 64, 'epochs': 36, 'd_model': 74, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 3.5900789816664935, 'dropout': 0.02944373853092181, 'weight_decay': 1.5856321706929264e-06, 'patch_size': 10, 'label_smoothing': 0.07518428120019957, 'use_focal_loss': True, 'focal_gamma': 1.337248166293211, 'grad_clip_norm': 1.342774968183723, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 23494, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 9842, 'model_storage_size_kb': 10.5724609375, 'model_size_validation': 'PASS'}
2025-09-23 19:57:47,287 - INFO - _models.training_function_executor - BO Objective: base=0.9677, size_penalty=0.0000, final=0.9677
2025-09-23 19:57:47,287 - INFO - _models.training_function_executor - Model: 9,842 parameters, 10.6KB (PASS 256KB limit)
2025-09-23 19:57:47,287 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 88.785s
2025-09-23 19:57:47,409 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9677
2025-09-23 19:57:47,409 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.123s
2025-09-23 19:57:47,410 - INFO - bo.run_bo - Recorded observation #44: hparams={'lr': 0.0004415675856195804, 'batch_size': np.int64(64), 'epochs': np.int64(36), 'd_model': np.int64(74), 'n_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': 3.5900789816664935, 'dropout': 0.02944373853092181, 'weight_decay': 1.5856321706929264e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.07518428120019957, 'use_focal_loss': np.True_, 'focal_gamma': 1.337248166293211, 'grad_clip_norm': 1.342774968183723, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(23494), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9677
2025-09-23 19:57:47,410 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 44: {'lr': 0.0004415675856195804, 'batch_size': np.int64(64), 'epochs': np.int64(36), 'd_model': np.int64(74), 'n_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': 3.5900789816664935, 'dropout': 0.02944373853092181, 'weight_decay': 1.5856321706929264e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.07518428120019957, 'use_focal_loss': np.True_, 'focal_gamma': 1.337248166293211, 'grad_clip_norm': 1.342774968183723, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(23494), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9677
2025-09-23 19:57:47,410 - INFO - bo.run_bo - ğŸ”BO Trial 45: Using RF surrogate + Expected Improvement
2025-09-23 19:57:47,410 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:57:47,410 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:57:47,410 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:57:47,410 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0014839970812559433, 'batch_size': 128, 'epochs': 50, 'd_model': 83, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 3.076721129928055, 'dropout': 0.029989894064681062, 'weight_decay': 3.5390634632861506e-05, 'patch_size': 25, 'label_smoothing': 0.11057756512840326, 'use_focal_loss': False, 'focal_gamma': 1.450160189669031, 'grad_clip_norm': 0.6541115246902999, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 14976, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:57:47,412 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0014839970812559433, 'batch_size': 128, 'epochs': 50, 'd_model': 83, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 3.076721129928055, 'dropout': 0.029989894064681062, 'weight_decay': 3.5390634632861506e-05, 'patch_size': 25, 'label_smoothing': 0.11057756512840326, 'use_focal_loss': False, 'focal_gamma': 1.450160189669031, 'grad_clip_norm': 0.6541115246902999, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 14976, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:58:39,038 - INFO - _models.training_function_executor - Model: 8,549 parameters, 36.7KB storage
2025-09-23 19:58:39,038 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8473366262642165, 0.6711472628338647, 0.6166690670636962, 0.5913021203976647, 0.5717443764831505, 0.5569252230072255, 0.545265800649016, 0.5351289460575603, 0.5306188815111983, 0.523886860774371, 0.5220474494495768, 0.5184223843099584, 0.5110406892302565, 0.5067946454212225, 0.5060218706191626, 0.5035927740735816, 0.5005300500772466, 0.4974366251265006, 0.4940085791435785, 0.49519345786989516, 0.4911032374915005, 0.48919900454525, 0.4900497453912093, 0.48703195066958094, 0.4846228515495391, 0.4818141200979883, 0.4836208525465921, 0.4821685702042767, 0.48070317012838815, 0.47745609174219694, 0.4763443286694952, 0.47569317517526893, 0.4740132356247903, 0.47514919159320745, 0.47101959639076857, 0.47162353040595234, 0.47118873240389597, 0.47080872431213056, 0.4679451572996996, 0.4689891949910753, 0.46671008585733387, 0.4654858557675613, 0.46639680250752363, 0.4662392394709915, 0.4630897137726869, 0.4637592332974728, 0.4607797336806907, 0.4618456292259952, 0.4625624267505949, 0.4582423239958502], 'val_losses': [0.7172922840274945, 0.6411760381074376, 0.6027010294775992, 0.5850547201844657, 0.5508625419602001, 0.5404407197319913, 0.5404822098224508, 0.5332608185545153, 0.5263885091143046, 0.5278302972634294, 0.5256265674135079, 0.5285309261732599, 0.5115249300829048, 0.5065447796019915, 0.5190051939945713, 0.5169588696417422, 0.5029444083237227, 0.5019671223154941, 0.5029712738832156, 0.5125763234212694, 0.5041614411252436, 0.504862587279566, 0.4956641086374753, 0.4977010150718115, 0.498753656362355, 0.5003239892580921, 0.500232472401516, 0.5001177150583883, 0.49734688506450825, 0.4910927335580568, 0.49663410417090503, 0.4921275739524317, 0.4945950786088466, 0.49686865224946997, 0.4908856209948642, 0.4970362308314832, 0.49406727877733864, 0.49231624947560937, 0.48805376599791234, 0.49302590819785896, 0.49159991805407954, 0.4938488093298967, 0.493664752083783, 0.49366163893789144, 0.4890919440619525, 0.4909477225559544, 0.4931155354691962, 0.4922742932500912, 0.49361613583645414, 0.4918144707496924], 'val_acc': [0.8617967673223906, 0.9015161007392557, 0.9178047863676231, 0.9288309735622102, 0.9423631123919308, 0.9446184688635509, 0.9462473374263877, 0.9475003132439543, 0.9516351334419245, 0.953389299586518, 0.9497556697155745, 0.9503821576243578, 0.9580253101115148, 0.9602806665831349, 0.953765192331788, 0.9547675729858414, 0.9650419746898885, 0.9644154867811051, 0.9634131061270518, 0.9592782859290816, 0.9620348327277284, 0.963287808545295, 0.9679238190702919, 0.9629119158000251, 0.9652925698534018, 0.9644154867811051, 0.9659190577621852, 0.9651672722716451, 0.9659190577621852, 0.9676732239067786, 0.9665455456709685, 0.967547926325022, 0.9693020924696153, 0.9636637012905651, 0.968675604560832, 0.9656684625986719, 0.9676732239067786, 0.9669214384162386, 0.969051497306102, 0.9689261997243453, 0.9681744142338052, 0.968675604560832, 0.9679238190702919, 0.968675604560832, 0.9700538779601554, 0.9706803658689387, 0.9695526876331286, 0.9706803658689387, 0.9703044731236687, 0.9699285803783987], 'model_size_bytes': 199221, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0014839970812559433, 'batch_size': 128, 'epochs': 50, 'd_model': 83, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 3.076721129928055, 'dropout': 0.029989894064681062, 'weight_decay': 3.5390634632861506e-05, 'patch_size': 25, 'label_smoothing': 0.11057756512840326, 'use_focal_loss': False, 'focal_gamma': 1.450160189669031, 'grad_clip_norm': 0.6541115246902999, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 14976, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 8549, 'model_storage_size_kb': 36.733984375000006, 'model_size_validation': 'PASS'}
2025-09-23 19:58:39,038 - INFO - _models.training_function_executor - BO Objective: base=0.9699, size_penalty=0.0000, final=0.9699
2025-09-23 19:58:39,038 - INFO - _models.training_function_executor - Model: 8,549 parameters, 36.7KB (PASS 256KB limit)
2025-09-23 19:58:39,038 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 51.628s
2025-09-23 19:58:39,162 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9699
2025-09-23 19:58:39,162 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.124s
2025-09-23 19:58:39,162 - INFO - bo.run_bo - Recorded observation #45: hparams={'lr': 0.0014839970812559433, 'batch_size': np.int64(128), 'epochs': np.int64(50), 'd_model': np.int64(83), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': 3.076721129928055, 'dropout': 0.029989894064681062, 'weight_decay': 3.5390634632861506e-05, 'patch_size': np.int64(25), 'label_smoothing': 0.11057756512840326, 'use_focal_loss': np.False_, 'focal_gamma': 1.450160189669031, 'grad_clip_norm': 0.6541115246902999, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(14976), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9699
2025-09-23 19:58:39,162 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 45: {'lr': 0.0014839970812559433, 'batch_size': np.int64(128), 'epochs': np.int64(50), 'd_model': np.int64(83), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': 3.076721129928055, 'dropout': 0.029989894064681062, 'weight_decay': 3.5390634632861506e-05, 'patch_size': np.int64(25), 'label_smoothing': 0.11057756512840326, 'use_focal_loss': np.False_, 'focal_gamma': 1.450160189669031, 'grad_clip_norm': 0.6541115246902999, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(14976), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9699
2025-09-23 19:58:39,163 - INFO - bo.run_bo - ğŸ”BO Trial 46: Using RF surrogate + Expected Improvement
2025-09-23 19:58:39,163 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:58:39,163 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:58:39,163 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:58:39,163 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003836825312611888, 'batch_size': 128, 'epochs': 43, 'd_model': 79, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3.727549867368006, 'dropout': 0.014933660469274416, 'weight_decay': 7.148882387167527e-06, 'patch_size': 50, 'label_smoothing': 0.003781773023917246, 'use_focal_loss': True, 'focal_gamma': 1.4658761388224817, 'grad_clip_norm': 1.5047684282492781, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 55366, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:58:39,165 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003836825312611888, 'batch_size': 128, 'epochs': 43, 'd_model': 79, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3.727549867368006, 'dropout': 0.014933660469274416, 'weight_decay': 7.148882387167527e-06, 'patch_size': 50, 'label_smoothing': 0.003781773023917246, 'use_focal_loss': True, 'focal_gamma': 1.4658761388224817, 'grad_clip_norm': 1.5047684282492781, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 55366, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:59:11,194 - INFO - _models.training_function_executor - Model: 10,507 parameters, 45.1KB storage
2025-09-23 19:59:11,194 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.37766971749324785, 0.22059270552748206, 0.17459143128551696, 0.15134567936521026, 0.1337069391254845, 0.12087420591606599, 0.10725977840327108, 0.10214600709493614, 0.09521162454324181, 0.08760016882470092, 0.0833768102267786, 0.07911340984924953, 0.0757841005462398, 0.0708814305622816, 0.06642883396761301, 0.06515868334314079, 0.06290968648508889, 0.061863622998516686, 0.05820034588148423, 0.055066864883863656, 0.0538457912581291, 0.051243868372447286, 0.05100647783728372, 0.04897314497044379, 0.04658859660539439, 0.04616332893708432, 0.04478353060273394, 0.042974072121403716, 0.04291609130106362, 0.041068396129114816, 0.03862358341907546, 0.039715440129298236, 0.03783423843044178, 0.035264397763467105, 0.03439006801664641, 0.0353315247952307, 0.03277254148261525, 0.032324677411130404, 0.03155700387769548, 0.03334012724751145, 0.029364333575612977, 0.029917843255453592, 0.02887317661177439], 'val_losses': [0.25480973367299103, 0.19040960352226033, 0.15394737936025155, 0.13636718384474386, 0.12804261660265184, 0.11094370560995724, 0.11404542656130635, 0.0999965557753629, 0.09479159158847637, 0.09274694055613832, 0.0829123078016108, 0.08403171801582761, 0.09197155806489075, 0.07617064334555775, 0.08057054560997928, 0.07682733115729735, 0.07925428414353003, 0.07399707697093853, 0.06642780361293088, 0.06890196161821453, 0.06604159180979997, 0.06510092934007529, 0.07003748539427725, 0.06087848329966774, 0.06599341994325345, 0.06661550865994406, 0.0730438323563359, 0.06870191259004615, 0.0663534611726805, 0.06342345397826529, 0.06997270036809443, 0.06757134314994737, 0.06267772127181392, 0.0663234867429079, 0.06592454189193561, 0.06136408270193273, 0.06824568178340869, 0.06379076117427258, 0.06524319256880039, 0.062298498941332665, 0.06478666994091024, 0.058773273190203625, 0.06703063736410311], 'val_acc': [0.8302217767197093, 0.8866056885102117, 0.9085327653176294, 0.9150482395689763, 0.9208119283297832, 0.9337175792507204, 0.9322140082696404, 0.9408595414108508, 0.9438666833730109, 0.9434907906277409, 0.9495050745520611, 0.9497556697155745, 0.9344693647412605, 0.9520110261871946, 0.9515098358601679, 0.9502568600426011, 0.9513845382784112, 0.9510086455331412, 0.9584012028567849, 0.9561458463851648, 0.9571482270392181, 0.9579000125297582, 0.9567723342939481, 0.9620348327277284, 0.9572735246209748, 0.9589023931838115, 0.9575241197844881, 0.9562711439669215, 0.9571482270392181, 0.961408344818945, 0.9592782859290816, 0.9581506076932715, 0.9637889988723217, 0.961784237564215, 0.9641648916175918, 0.963287808545295, 0.9595288810925949, 0.9634131061270518, 0.9631625109635384, 0.962536023054755, 0.9641648916175918, 0.9685503069790753, 0.9631625109635384], 'model_size_bytes': 209525, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003836825312611888, 'batch_size': 128, 'epochs': 43, 'd_model': 79, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3.727549867368006, 'dropout': 0.014933660469274416, 'weight_decay': 7.148882387167527e-06, 'patch_size': 50, 'label_smoothing': 0.003781773023917246, 'use_focal_loss': True, 'focal_gamma': 1.4658761388224817, 'grad_clip_norm': 1.5047684282492781, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 55366, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 10507, 'model_storage_size_kb': 45.147265625, 'model_size_validation': 'PASS'}
2025-09-23 19:59:11,195 - INFO - _models.training_function_executor - BO Objective: base=0.9632, size_penalty=0.0000, final=0.9632
2025-09-23 19:59:11,195 - INFO - _models.training_function_executor - Model: 10,507 parameters, 45.1KB (PASS 256KB limit)
2025-09-23 19:59:11,195 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 32.031s
2025-09-23 19:59:11,318 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9632
2025-09-23 19:59:11,318 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.123s
2025-09-23 19:59:11,318 - INFO - bo.run_bo - Recorded observation #46: hparams={'lr': 0.0003836825312611888, 'batch_size': np.int64(128), 'epochs': np.int64(43), 'd_model': np.int64(79), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': 3.727549867368006, 'dropout': 0.014933660469274416, 'weight_decay': 7.148882387167527e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.003781773023917246, 'use_focal_loss': np.True_, 'focal_gamma': 1.4658761388224817, 'grad_clip_norm': 1.5047684282492781, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(55366), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9632
2025-09-23 19:59:11,318 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 46: {'lr': 0.0003836825312611888, 'batch_size': np.int64(128), 'epochs': np.int64(43), 'd_model': np.int64(79), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': 3.727549867368006, 'dropout': 0.014933660469274416, 'weight_decay': 7.148882387167527e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.003781773023917246, 'use_focal_loss': np.True_, 'focal_gamma': 1.4658761388224817, 'grad_clip_norm': 1.5047684282492781, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(55366), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9632
2025-09-23 19:59:11,318 - INFO - bo.run_bo - ğŸ”BO Trial 47: Using RF surrogate + Expected Improvement
2025-09-23 19:59:11,318 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:59:11,318 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:59:11,319 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:59:11,319 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0006714916798985382, 'batch_size': 128, 'epochs': 59, 'd_model': 83, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2.803700842026423, 'dropout': 0.0866685545569781, 'weight_decay': 0.003199936646972401, 'patch_size': 50, 'label_smoothing': 0.19281800582333966, 'use_focal_loss': True, 'focal_gamma': 1.1921076632372198, 'grad_clip_norm': 0.6985856191898651, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 39575, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:59:11,320 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0006714916798985382, 'batch_size': 128, 'epochs': 59, 'd_model': 83, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2.803700842026423, 'dropout': 0.0866685545569781, 'weight_decay': 0.003199936646972401, 'patch_size': 50, 'label_smoothing': 0.19281800582333966, 'use_focal_loss': True, 'focal_gamma': 1.1921076632372198, 'grad_clip_norm': 0.6985856191898651, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 39575, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:59:54,739 - INFO - _models.training_function_executor - Model: 11,039 parameters, 47.4KB storage
2025-09-23 19:59:54,739 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4023587881755109, 0.2570432410859167, 0.2073991768391431, 0.17734855776385167, 0.15725621306654974, 0.14564342992198506, 0.1336631959529612, 0.1256646761549403, 0.11694166452930493, 0.11370286094725246, 0.10700401160679891, 0.10187416053806388, 0.09788570259958697, 0.0928957042965611, 0.09043166141499479, 0.08499837925231864, 0.0839435714424633, 0.07969712835741503, 0.07901543343307657, 0.0764773171270782, 0.07302991040437247, 0.07175083896165355, 0.0688514286505812, 0.06794543263838766, 0.06557948969595602, 0.06474335126176672, 0.06198907169350234, 0.0624557995911972, 0.06051613738246831, 0.06058203259109144, 0.057002144559101776, 0.05638593915795427, 0.056014318773650464, 0.05660266173446316, 0.053992165858335904, 0.05313406288580632, 0.05094233169388285, 0.05065440536838342, 0.04996622902030571, 0.05007302349700712, 0.048267916484412046, 0.046448685971478734, 0.04657959945634564, 0.047239264513725086, 0.04601977944151895, 0.04536713043677831, 0.04313389535934873, 0.0449184764325278, 0.046742103060965524, 0.042733421687252526, 0.04292283100552869, 0.041153511571423156, 0.04298866828940607, 0.041223128958292575, 0.03939594165939132, 0.03780418789538827, 0.04163052238159518, 0.038973468797150045, 0.03744614223291416], 'val_losses': [0.263062117574859, 0.2060269180920067, 0.15640241381067066, 0.13509235256444302, 0.13485591702164154, 0.12495165626785537, 0.10622534819732422, 0.10464423470681748, 0.1097457919355219, 0.09764927922755147, 0.0952063041458287, 0.0891029566009232, 0.08652491227274042, 0.09156251932957495, 0.08666621958992114, 0.08339660793828525, 0.08462509288665063, 0.07644910048966479, 0.07766638596771713, 0.07189190882688595, 0.07712037956008128, 0.07004621958920433, 0.06678271450521489, 0.06918863862446713, 0.07259259029640364, 0.06958306940941565, 0.0702133411157663, 0.06795878844052579, 0.06668591530975611, 0.06245281929545811, 0.06957868421108349, 0.06735606754077077, 0.06631054809040898, 0.06627029687998931, 0.06122077470221715, 0.06338418561061093, 0.06303408958718915, 0.0699944097962748, 0.07300409454002355, 0.05901218368499713, 0.06488672486551796, 0.05745416971654271, 0.06083805832016245, 0.05803887992947939, 0.0661711914105652, 0.0600721072230538, 0.06464925502650855, 0.0610767229019199, 0.05560864108036879, 0.06555757429154352, 0.055865408128036205, 0.06036962642322849, 0.058801940349749254, 0.05605403354907003, 0.05429485260827875, 0.05898582393273407, 0.062179595139589455, 0.057504078319401034, 0.055373546303199836], 'val_acc': [0.8488911164014534, 0.8862297957649418, 0.9178047863676231, 0.9317128179426137, 0.9329657937601804, 0.9317128179426137, 0.9423631123919308, 0.9426137075554442, 0.9423631123919308, 0.9491291818067912, 0.9490038842250345, 0.9532640020047614, 0.952637514095978, 0.9496303721338178, 0.9516351334419245, 0.9550181681493547, 0.9535145971682747, 0.9566470367121914, 0.9589023931838115, 0.9584012028567849, 0.9589023931838115, 0.9605312617466483, 0.9620348327277284, 0.961408344818945, 0.9597794762561083, 0.9629119158000251, 0.9612830472371883, 0.9619095351459717, 0.961784237564215, 0.9642901891993485, 0.9634131061270518, 0.9609071544919183, 0.9631625109635384, 0.961408344818945, 0.9656684625986719, 0.9640395940358351, 0.967547926325022, 0.9620348327277284, 0.9637889988723217, 0.9641648916175918, 0.9626613206365117, 0.9689261997243453, 0.9664202480892119, 0.9670467359979953, 0.9642901891993485, 0.9680491166520486, 0.9642901891993485, 0.9670467359979953, 0.967547926325022, 0.9622854278912417, 0.9699285803783987, 0.9685503069790753, 0.968299711815562, 0.9711815561959655, 0.9708056634506954, 0.9688009021425886, 0.9669214384162386, 0.9704297707054254, 0.9696779852148854], 'model_size_bytes': 201333, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0006714916798985382, 'batch_size': 128, 'epochs': 59, 'd_model': 83, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2.803700842026423, 'dropout': 0.0866685545569781, 'weight_decay': 0.003199936646972401, 'patch_size': 50, 'label_smoothing': 0.19281800582333966, 'use_focal_loss': True, 'focal_gamma': 1.1921076632372198, 'grad_clip_norm': 0.6985856191898651, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 39575, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 11039, 'model_storage_size_kb': 47.433203125000006, 'model_size_validation': 'PASS'}
2025-09-23 19:59:54,740 - INFO - _models.training_function_executor - BO Objective: base=0.9697, size_penalty=0.0000, final=0.9697
2025-09-23 19:59:54,740 - INFO - _models.training_function_executor - Model: 11,039 parameters, 47.4KB (PASS 256KB limit)
2025-09-23 19:59:54,740 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 43.421s
2025-09-23 19:59:54,864 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9697
2025-09-23 19:59:54,864 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.125s
2025-09-23 19:59:54,864 - INFO - bo.run_bo - Recorded observation #47: hparams={'lr': 0.0006714916798985382, 'batch_size': np.int64(128), 'epochs': np.int64(59), 'd_model': np.int64(83), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': 2.803700842026423, 'dropout': 0.0866685545569781, 'weight_decay': 0.003199936646972401, 'patch_size': np.int64(50), 'label_smoothing': 0.19281800582333966, 'use_focal_loss': np.True_, 'focal_gamma': 1.1921076632372198, 'grad_clip_norm': 0.6985856191898651, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(39575), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9697
2025-09-23 19:59:54,864 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 47: {'lr': 0.0006714916798985382, 'batch_size': np.int64(128), 'epochs': np.int64(59), 'd_model': np.int64(83), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': 2.803700842026423, 'dropout': 0.0866685545569781, 'weight_decay': 0.003199936646972401, 'patch_size': np.int64(50), 'label_smoothing': 0.19281800582333966, 'use_focal_loss': np.True_, 'focal_gamma': 1.1921076632372198, 'grad_clip_norm': 0.6985856191898651, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(39575), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9697
2025-09-23 19:59:54,865 - INFO - bo.run_bo - ğŸ”BO Trial 48: Using RF surrogate + Expected Improvement
2025-09-23 19:59:54,865 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:59:54,865 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:59:54,865 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:59:54,865 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0010737168504297517, 'batch_size': 32, 'epochs': 51, 'd_model': 32, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3.913055594912908, 'dropout': 0.015738162180854946, 'weight_decay': 1.596987052204102e-06, 'patch_size': 10, 'label_smoothing': 0.1406614244736677, 'use_focal_loss': True, 'focal_gamma': 1.4332630041133956, 'grad_clip_norm': 1.5352909289233303, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 37377, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:59:54,867 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0010737168504297517, 'batch_size': 32, 'epochs': 51, 'd_model': 32, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3.913055594912908, 'dropout': 0.015738162180854946, 'weight_decay': 1.596987052204102e-06, 'patch_size': 10, 'label_smoothing': 0.1406614244736677, 'use_focal_loss': True, 'focal_gamma': 1.4332630041133956, 'grad_clip_norm': 1.5352909289233303, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 37377, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 20:01:40,618 - INFO - _models.training_function_executor - Model: 4,256 parameters, 4.6KB storage
2025-09-23 20:01:40,618 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.35033798074003814, 0.2150110803037827, 0.17498174442297323, 0.15368606344379324, 0.14053585293330492, 0.13242335457248475, 0.12044753521074945, 0.1141501324247735, 0.10874632153935111, 0.10299074597554268, 0.097127402299378, 0.09416388634117225, 0.08874257705157904, 0.08478426447155869, 0.08235389082299152, 0.0786870563255093, 0.0752200998494945, 0.0729088252171795, 0.06967940645658419, 0.06695980666346431, 0.06627842238005954, 0.06184898356560219, 0.0609740071983934, 0.05766295220300168, 0.0566837354826764, 0.055611195319503405, 0.053960111549650594, 0.05222485492908764, 0.04828130984650857, 0.04782391778551292, 0.04533710263869508, 0.04389440115266167, 0.0442218365508826, 0.041644844682558406, 0.040532944316986015, 0.03826744065257693, 0.037468565911387426, 0.03543310310803974, 0.03592639930117542, 0.03450640560546529, 0.03386916194779019, 0.03366716729384391, 0.03107358866285997, 0.031129298971743196, 0.03129035896850573, 0.02991895504785862, 0.0302706256118362, 0.030235832175938056, 0.028775791837120663, 0.029142541509017416, 0.030303645727967097], 'val_losses': [0.2660087833591548, 0.18099471676127085, 0.16049894635808182, 0.147097819881408, 0.12908345959541973, 0.11659959802172294, 0.11825965185550621, 0.11349076886920759, 0.1024390560284382, 0.10817467778979006, 0.10168179615122187, 0.09708366862931969, 0.10192473088420882, 0.10950823581176897, 0.0872782954759219, 0.08242315945003494, 0.09701615552184666, 0.08626773555376464, 0.07921112988126888, 0.08653698003209186, 0.080327929789568, 0.08212227449558564, 0.08096278945487956, 0.08963100799221567, 0.07925073715091842, 0.07791505190322698, 0.0774667787185559, 0.0780111407557013, 0.07966205667269781, 0.0806023934812881, 0.08147358710954264, 0.07862774331751375, 0.07762668546172047, 0.08105993217708936, 0.07688064742382943, 0.07947522421180642, 0.07985770861355546, 0.08092067971995504, 0.08205548895630542, 0.08026613753212901, 0.08137087608049155, 0.08213146274790117, 0.0824063078396124, 0.08206939169815677, 0.08226983110766865, 0.08223305333603177, 0.08269884404835434, 0.08275470670087193, 0.082430868324972, 0.08253942882249363, 0.08254896817163386], 'val_acc': [0.8275905275028193, 0.9023931838115524, 0.9087833604811427, 0.9144217516601929, 0.9251973436912667, 0.9324646034331537, 0.9333416865054505, 0.9347199599047739, 0.9398571607567974, 0.9398571607567974, 0.9382282921939606, 0.9401077559203107, 0.9451196591905776, 0.9414860293196341, 0.9459967422628743, 0.9508833479513845, 0.946497932589901, 0.9482520987344945, 0.9515098358601679, 0.9511339431148979, 0.9521363237689513, 0.946122039844631, 0.9523869189324646, 0.952637514095978, 0.9525122165142212, 0.9542663826588147, 0.953389299586518, 0.9542663826588147, 0.9527628116777346, 0.9562711439669215, 0.9550181681493547, 0.9546422754040848, 0.9540157874953014, 0.9558952512216514, 0.9575241197844881, 0.9568976318757048, 0.9570229294574615, 0.9581506076932715, 0.9572735246209748, 0.9580253101115148, 0.9595288810925949, 0.9580253101115148, 0.9579000125297582, 0.9591529883473249, 0.9595288810925949, 0.9584012028567849, 0.9579000125297582, 0.9591529883473249, 0.9585265004385415, 0.9585265004385415, 0.9582759052750282], 'model_size_bytes': 63605, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0010737168504297517, 'batch_size': 32, 'epochs': 51, 'd_model': 32, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3.913055594912908, 'dropout': 0.015738162180854946, 'weight_decay': 1.596987052204102e-06, 'patch_size': 10, 'label_smoothing': 0.1406614244736677, 'use_focal_loss': True, 'focal_gamma': 1.4332630041133956, 'grad_clip_norm': 1.5352909289233303, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 37377, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 4256, 'model_storage_size_kb': 4.571875, 'model_size_validation': 'PASS'}
2025-09-23 20:01:40,618 - INFO - _models.training_function_executor - BO Objective: base=0.9583, size_penalty=0.0000, final=0.9583
2025-09-23 20:01:40,618 - INFO - _models.training_function_executor - Model: 4,256 parameters, 4.6KB (PASS 256KB limit)
2025-09-23 20:01:40,618 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 105.753s
2025-09-23 20:01:40,870 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9583
2025-09-23 20:01:40,870 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.252s
2025-09-23 20:01:40,870 - INFO - bo.run_bo - Recorded observation #48: hparams={'lr': 0.0010737168504297517, 'batch_size': np.int64(32), 'epochs': np.int64(51), 'd_model': np.int64(32), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': 3.913055594912908, 'dropout': 0.015738162180854946, 'weight_decay': 1.596987052204102e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.1406614244736677, 'use_focal_loss': np.True_, 'focal_gamma': 1.4332630041133956, 'grad_clip_norm': 1.5352909289233303, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(37377), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9583
2025-09-23 20:01:40,870 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 48: {'lr': 0.0010737168504297517, 'batch_size': np.int64(32), 'epochs': np.int64(51), 'd_model': np.int64(32), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': 3.913055594912908, 'dropout': 0.015738162180854946, 'weight_decay': 1.596987052204102e-06, 'patch_size': np.int64(10), 'label_smoothing': 0.1406614244736677, 'use_focal_loss': np.True_, 'focal_gamma': 1.4332630041133956, 'grad_clip_norm': 1.5352909289233303, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(37377), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9583
2025-09-23 20:01:40,871 - INFO - bo.run_bo - ğŸ”BO Trial 49: Using RF surrogate + Expected Improvement
2025-09-23 20:01:40,871 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 20:01:40,871 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 20:01:40,871 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 20:01:40,871 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0005019972964136975, 'batch_size': 128, 'epochs': 58, 'd_model': 96, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 3.326012975190828, 'dropout': 0.0083768571780124, 'weight_decay': 0.0024262661791049143, 'patch_size': 10, 'label_smoothing': 0.13953277225197738, 'use_focal_loss': False, 'focal_gamma': 1.3053699064822837, 'grad_clip_norm': 1.4859901152242008, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 33010, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 20:01:40,872 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0005019972964136975, 'batch_size': 128, 'epochs': 58, 'd_model': 96, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 3.326012975190828, 'dropout': 0.0083768571780124, 'weight_decay': 0.0024262661791049143, 'patch_size': 10, 'label_smoothing': 0.13953277225197738, 'use_focal_loss': False, 'focal_gamma': 1.3053699064822837, 'grad_clip_norm': 1.4859901152242008, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 33010, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 20:06:04,985 - INFO - _models.training_function_executor - Model: 409,677 parameters, 880.2KB storage
2025-09-23 20:06:04,985 - WARNING - _models.training_function_executor - Model storage 880.2KB exceeds 256KB limit!
2025-09-23 20:06:04,985 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1013913489771985, 0.9443388376730978, 0.8735234290758171, 0.8232057242320601, 0.7649229997079616, 0.6841454819879657, 0.6492836881269434, 0.6330406103644108, 0.619620012672377, 0.6100879589597097, 0.6015756800502359, 0.5985307073554303, 0.5924550813623035, 0.5892630806226306, 0.5842343679224419, 0.580478346493819, 0.5783027615201536, 0.5738525200961127, 0.5698201907116851, 0.5669259606766842, 0.5650092010617842, 0.5625237936353383, 0.5612024705190354, 0.5585856095660776, 0.5578114972789571, 0.5552896630236355, 0.5537742494909446, 0.5493471236697669, 0.548317949471944, 0.5477067345093803, 0.543899801572612, 0.5426939035352226, 0.5401333728813645, 0.5361270717435122, 0.5356398345279697, 0.5333594794792142, 0.5314492362037666, 0.5288785503172985, 0.5279920646108653, 0.5248953221331443, 0.5243963065483634, 0.5217940892969476, 0.5198294553127712, 0.5187681500252912, 0.5172628497351449, 0.517541543883914, 0.515086176035208, 0.5147277772271632, 0.5136074703281858, 0.5126110084357911, 0.5122045286839286, 0.5116380037372805, 0.5114362750107777, 0.511045484852717, 0.510866162158775, 0.5106264965798523, 0.5104144187520718, 0.5105051377066595], 'val_losses': [0.9895611385282844, 0.9326496896491644, 0.8393687904590563, 0.801669801585493, 0.6989629307257443, 0.6653915776628074, 0.6339844746512587, 0.6298820407837797, 0.6396212617252542, 0.6125635639525733, 0.6193048227790782, 0.5970117533330914, 0.6114829186284293, 0.5918104690935451, 0.5928516822520868, 0.5827658898832505, 0.5818868143384341, 0.5932592181779436, 0.5780137761271608, 0.5790223841783083, 0.5762604056646197, 0.577858939324386, 0.5775737750501864, 0.5766519403908728, 0.5700692665814427, 0.5732365289646766, 0.574917684632306, 0.5706953431128738, 0.5720320076082691, 0.5688307115778023, 0.5713139968607871, 0.5755511682413825, 0.5639616485272952, 0.5691973808459688, 0.5634485582572867, 0.5703456032292584, 0.5643279995629699, 0.5673557086750284, 0.5640810703502355, 0.5650778272638582, 0.5666332318286611, 0.5650809841068776, 0.5668655198224728, 0.564619868639014, 0.5655152714501914, 0.5639997142745266, 0.5616369712563359, 0.5640388229275359, 0.5631675610281205, 0.5643283958468517, 0.5648213771930719, 0.565766574610237, 0.5635072439330187, 0.5638288980422947, 0.564041266419239, 0.5642202464414858, 0.5641728184334005, 0.5642156611855893], 'val_acc': [0.7504072171407092, 0.7708307229670467, 0.8323518356095727, 0.8547801027440171, 0.9105375266257362, 0.9253226412730234, 0.9378523994486906, 0.9398571607567974, 0.9365994236311239, 0.9468738253351711, 0.945746147099361, 0.9540157874953014, 0.9459967422628743, 0.9589023931838115, 0.9576494173662449, 0.962160130309485, 0.9631625109635384, 0.9557699536398947, 0.9635384037088084, 0.9642901891993485, 0.9656684625986719, 0.9646660819446184, 0.9650419746898885, 0.9642901891993485, 0.9669214384162386, 0.967547926325022, 0.9665455456709685, 0.9677985214885353, 0.967547926325022, 0.9689261997243453, 0.9684250093973187, 0.9672973311615086, 0.9726851271770455, 0.9693020924696153, 0.9715574489412354, 0.968675604560832, 0.9726851271770455, 0.9698032827966421, 0.9723092344317754, 0.9720586392682621, 0.9714321513594788, 0.9724345320135321, 0.9729357223405588, 0.9725598295952889, 0.9735622102493422, 0.9728104247588022, 0.9736875078310988, 0.9726851271770455, 0.9738128054128555, 0.9739381029946123, 0.9733116150858289, 0.9729357223405588, 0.9748151860669089, 0.9745645909033955, 0.9750657812304222, 0.9745645909033955, 0.9745645909033955, 0.9746898884851523], 'model_size_bytes': 839407, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0005019972964136975, 'batch_size': 128, 'epochs': 58, 'd_model': 96, 'n_heads': 4, 'num_layers': 4, 'mlp_ratio': 3.326012975190828, 'dropout': 0.0083768571780124, 'weight_decay': 0.0024262661791049143, 'patch_size': 10, 'label_smoothing': 0.13953277225197738, 'use_focal_loss': False, 'focal_gamma': 1.3053699064822837, 'grad_clip_norm': 1.4859901152242008, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 33010, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 409677, 'model_storage_size_kb': 880.1654296875, 'model_size_validation': 'FAIL'}
2025-09-23 20:06:04,985 - INFO - _models.training_function_executor - BO Objective: base=0.9747, size_penalty=0.8000, final=0.1747
2025-09-23 20:06:04,985 - INFO - _models.training_function_executor - Model: 409,677 parameters, 880.2KB (FAIL 256KB limit)
2025-09-23 20:06:04,985 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 264.115s
2025-09-23 20:06:05,111 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1747
2025-09-23 20:06:05,112 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.126s
2025-09-23 20:06:05,112 - INFO - bo.run_bo - Recorded observation #49: hparams={'lr': 0.0005019972964136975, 'batch_size': np.int64(128), 'epochs': np.int64(58), 'd_model': np.int64(96), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 3.326012975190828, 'dropout': 0.0083768571780124, 'weight_decay': 0.0024262661791049143, 'patch_size': np.int64(10), 'label_smoothing': 0.13953277225197738, 'use_focal_loss': np.False_, 'focal_gamma': 1.3053699064822837, 'grad_clip_norm': 1.4859901152242008, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(33010), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.1747
2025-09-23 20:06:05,112 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 49: {'lr': 0.0005019972964136975, 'batch_size': np.int64(128), 'epochs': np.int64(58), 'd_model': np.int64(96), 'n_heads': np.int64(4), 'num_layers': np.int64(4), 'mlp_ratio': 3.326012975190828, 'dropout': 0.0083768571780124, 'weight_decay': 0.0024262661791049143, 'patch_size': np.int64(10), 'label_smoothing': 0.13953277225197738, 'use_focal_loss': np.False_, 'focal_gamma': 1.3053699064822837, 'grad_clip_norm': 1.4859901152242008, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(33010), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.1747
2025-09-23 20:06:05,112 - INFO - bo.run_bo - ğŸ”BO Trial 50: Using RF surrogate + Expected Improvement
2025-09-23 20:06:05,112 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 20:06:05,112 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 20:06:05,113 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 20:06:05,113 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.000414311770618384, 'batch_size': 64, 'epochs': 57, 'd_model': 64, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.8871391719402473, 'dropout': 0.009145600851329096, 'weight_decay': 3.512491498908786e-05, 'patch_size': 40, 'label_smoothing': 0.008715040824236865, 'use_focal_loss': True, 'focal_gamma': 1.887127029391676, 'grad_clip_norm': 1.2029227138633813, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 17092, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 20:06:05,114 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.000414311770618384, 'batch_size': 64, 'epochs': 57, 'd_model': 64, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.8871391719402473, 'dropout': 0.009145600851329096, 'weight_decay': 3.512491498908786e-05, 'patch_size': 40, 'label_smoothing': 0.008715040824236865, 'use_focal_loss': True, 'focal_gamma': 1.887127029391676, 'grad_clip_norm': 1.2029227138633813, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 17092, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 20:07:37,263 - INFO - _models.training_function_executor - Model: 8,064 parameters, 34.7KB storage
2025-09-23 20:07:37,263 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2676669597934744, 0.14150291874398416, 0.11027946238691703, 0.09380216232568041, 0.0826305154651143, 0.07382295088368084, 0.07062442464239298, 0.06277067257253086, 0.05846385146127342, 0.056245077130898005, 0.050904637755468385, 0.049128386059369436, 0.04695763185149322, 0.044311095844004694, 0.04192995548241756, 0.04013378075000605, 0.03944723920017519, 0.03741907887003238, 0.035464525820495856, 0.03355622544468777, 0.033386379268325085, 0.03188379418453925, 0.029958861976716315, 0.02969972525318022, 0.027978189454811564, 0.026875906372565664, 0.02723375094539119, 0.026632319887152398, 0.02581726765967921, 0.023864114954444477, 0.02455998424882784, 0.02285983011398887, 0.02209250341175575, 0.022960688080133938, 0.021378533865906272, 0.019364772495707387, 0.020006217444507226, 0.01842176971256152, 0.01808645168928323, 0.019080803761831057, 0.01740028151257796, 0.01712660200276148, 0.01649260645893266, 0.016912602648746753, 0.015193790869950536, 0.01544330176896676, 0.01485537790647802, 0.015246428888525329, 0.015622276912694645, 0.013486058489715841, 0.01448915712757104, 0.011241226516953667, 0.011927137128225506, 0.013295905681498703, 0.011178300751166794, 0.012728900923516636, 0.011791950783756805], 'val_losses': [0.1603342681057082, 0.1299525036158799, 0.11576745733142631, 0.10525908469855076, 0.09277305076062836, 0.08944040228438965, 0.08079173576841527, 0.08702305913561494, 0.07494099894131265, 0.06406294162011866, 0.06800000614229561, 0.05938387590849882, 0.056339917862931375, 0.05432867689615741, 0.05513952202304251, 0.0542484916585114, 0.06069201744169598, 0.05574214491419529, 0.05460330999826611, 0.058852538346026925, 0.05127179674928371, 0.050475466497454366, 0.05752882379896135, 0.04651494334943013, 0.0503961895967677, 0.04884402328665029, 0.052257519760999356, 0.052533469631635014, 0.05259328899871712, 0.0512675417671413, 0.06409407651025974, 0.05685625943197611, 0.05421766476880009, 0.051497138867684734, 0.0500965297423782, 0.051267481394134985, 0.05920357072660311, 0.05348300101680574, 0.05109659672556418, 0.05425208574260122, 0.052052419491241994, 0.05393541537719089, 0.058151976784553285, 0.059643987208220194, 0.05657287342130497, 0.06199778804536631, 0.06002596843147051, 0.05363494771686865, 0.058817691421384814, 0.06140959800940202, 0.05646743815144401, 0.05713652912842392, 0.05793435368236482, 0.05961099223129553, 0.0618205926028757, 0.055311709680685404, 0.05720986532300147], 'val_acc': [0.8862297957649418, 0.9025184813933091, 0.9107881217892495, 0.92319258238316, 0.9368500187946373, 0.9334669840872071, 0.9432401954642275, 0.9431148978824708, 0.9456208495176043, 0.954141085077058, 0.9493797769703045, 0.9546422754040848, 0.9575241197844881, 0.9595288810925949, 0.9587770956020549, 0.9580253101115148, 0.9570229294574615, 0.9577747149480015, 0.9589023931838115, 0.9573988222027315, 0.9609071544919183, 0.9626613206365117, 0.9590276907655683, 0.9630372133817817, 0.9634131061270518, 0.9650419746898885, 0.961408344818945, 0.9622854278912417, 0.9635384037088084, 0.9630372133817817, 0.9547675729858414, 0.9640395940358351, 0.9626613206365117, 0.9635384037088084, 0.9649166771081318, 0.9664202480892119, 0.9595288810925949, 0.9630372133817817, 0.9631625109635384, 0.9650419746898885, 0.9646660819446184, 0.9629119158000251, 0.9636637012905651, 0.9626613206365117, 0.9654178674351584, 0.9649166771081318, 0.9640395940358351, 0.9637889988723217, 0.9611577496554317, 0.9634131061270518, 0.9649166771081318, 0.9654178674351584, 0.9635384037088084, 0.9652925698534018, 0.9644154867811051, 0.9669214384162386, 0.9582759052750282], 'model_size_bytes': 200445, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.000414311770618384, 'batch_size': 64, 'epochs': 57, 'd_model': 64, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.8871391719402473, 'dropout': 0.009145600851329096, 'weight_decay': 3.512491498908786e-05, 'patch_size': 40, 'label_smoothing': 0.008715040824236865, 'use_focal_loss': True, 'focal_gamma': 1.887127029391676, 'grad_clip_norm': 1.2029227138633813, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 17092, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 8064, 'model_storage_size_kb': 34.650000000000006, 'model_size_validation': 'PASS'}
2025-09-23 20:07:37,263 - INFO - _models.training_function_executor - BO Objective: base=0.9583, size_penalty=0.0000, final=0.9583
2025-09-23 20:07:37,263 - INFO - _models.training_function_executor - Model: 8,064 parameters, 34.7KB (PASS 256KB limit)
2025-09-23 20:07:37,263 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 92.151s
2025-09-23 20:07:37,386 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9583
2025-09-23 20:07:37,386 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.123s
2025-09-23 20:07:37,386 - INFO - bo.run_bo - Recorded observation #50: hparams={'lr': 0.000414311770618384, 'batch_size': np.int64(64), 'epochs': np.int64(57), 'd_model': np.int64(64), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 1.8871391719402473, 'dropout': 0.009145600851329096, 'weight_decay': 3.512491498908786e-05, 'patch_size': np.int64(40), 'label_smoothing': 0.008715040824236865, 'use_focal_loss': np.True_, 'focal_gamma': 1.887127029391676, 'grad_clip_norm': 1.2029227138633813, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(17092), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9583
2025-09-23 20:07:37,386 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 50: {'lr': 0.000414311770618384, 'batch_size': np.int64(64), 'epochs': np.int64(57), 'd_model': np.int64(64), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 1.8871391719402473, 'dropout': 0.009145600851329096, 'weight_decay': 3.512491498908786e-05, 'patch_size': np.int64(40), 'label_smoothing': 0.008715040824236865, 'use_focal_loss': np.True_, 'focal_gamma': 1.887127029391676, 'grad_clip_norm': 1.2029227138633813, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(17092), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9583
2025-09-23 20:07:37,386 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.9744
2025-09-23 20:07:37,386 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.0026665934327606286, 'batch_size': np.int64(128), 'epochs': np.int64(57), 'd_model': np.int64(54), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': np.int64(40), 'label_smoothing': 0.10595857977887042, 'use_focal_loss': np.True_, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(44609), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}
2025-09-23 20:07:37,386 - INFO - visualization - Generating BO visualization charts with 50 trials...
2025-09-23 20:07:38,701 - INFO - visualization - BO summary saved to: charts/20250923_200737_BO_Tiny1D-ViT-SE-Transformer-MITBIH/bo_summary.txt
2025-09-23 20:07:38,701 - INFO - visualization - BO charts saved to: charts/20250923_200737_BO_Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 20:07:38,701 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ“Š BO charts saved to: charts/20250923_200737_BO_Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 20:07:38,724 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸš€ STEP 4: Final Training Execution
2025-09-23 20:07:38,724 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-23 20:07:38,817 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-23 20:07:38,830 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-23 20:07:38,845 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-23 20:07:38,845 - INFO - _models.training_function_executor - Loaded training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 20:07:38,845 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-23 20:07:38,845 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.0026665934327606286, 'batch_size': np.int64(128), 'epochs': np.int64(57), 'd_model': np.int64(54), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': np.int64(40), 'label_smoothing': 0.10595857977887042, 'use_focal_loss': np.True_, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(44609), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}
2025-09-23 20:07:38,845 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 20:07:38,868 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 20:07:38,868 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0026665934327606286, 'batch_size': np.int64(128), 'epochs': np.int64(57), 'd_model': np.int64(54), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': np.int64(40), 'label_smoothing': 0.10595857977887042, 'use_focal_loss': np.True_, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': np.int64(44609), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}
2025-09-23 20:07:38,870 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0026665934327606286, 'batch_size': 128, 'epochs': 57, 'd_model': 54, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': 40, 'label_smoothing': 0.10595857977887042, 'use_focal_loss': True, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 44609, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 20:08:44,209 - INFO - _models.training_function_executor - Model: 6,588 parameters, 28.3KB storage
2025-09-23 20:08:44,209 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.32025029588529563, 0.18627951794274827, 0.14145016709660757, 0.11577493797184088, 0.10002713778325817, 0.08337340516478134, 0.07675465105554058, 0.06781988983201617, 0.0625656038498821, 0.05869577786980052, 0.05328891631260622, 0.05224279664403071, 0.0478073075852852, 0.04759418799929506, 0.04563741660628351, 0.04274512229766622, 0.04159391470964469, 0.038832437961026964, 0.03800642771128066, 0.03540299696894101, 0.03542650909534242, 0.033044464256593534, 0.031177829310250166, 0.031095775349906617, 0.029223669017123623, 0.026481510379740784, 0.02660676269883883, 0.025355075252240147, 0.023632271750064594, 0.022565589298336575, 0.020885604432968493, 0.019383298400445947, 0.018629773191319005, 0.01783769940897679, 0.01797620211178982, 0.01537287323463054, 0.014281613850270032, 0.014912623248145929, 0.013258665784410744, 0.012494158374112765, 0.011040461171043509, 0.011184085921347798, 0.010483306269672995, 0.008800658328853003, 0.009005943579306496, 0.008385208594249061, 0.007519742235901326, 0.007290818553743548, 0.007110855224851889, 0.006971991313488836, 0.006138813070218271, 0.0059255033538866495, 0.005457074017321456, 0.005504856202439588, 0.005518116869072339, 0.005883489685105466, 0.005299354630192696], 'val_losses': [0.24421167719563705, 0.15035244677627663, 0.1288605093032979, 0.09677520307475249, 0.08348392548895174, 0.0836958394202821, 0.08358353639626964, 0.06930945108998356, 0.06451119967286652, 0.060101800623874546, 0.05608192624869059, 0.05721585392366493, 0.049757377271322685, 0.050910525316673276, 0.055355783494803044, 0.05261464413620253, 0.04856463556817434, 0.04644625383971855, 0.04455428738458799, 0.04396610895418757, 0.04498609538699088, 0.044270091547395585, 0.04412895623791298, 0.04159533218411103, 0.04007908665491355, 0.043309132520464505, 0.04147112825033157, 0.044079528116171426, 0.03975331950535083, 0.04469231460189322, 0.03675422141225959, 0.03837974800415456, 0.038730109086017685, 0.04325570431888444, 0.040970696723881136, 0.04045102335016032, 0.040899143812934684, 0.040977130000022774, 0.042262197738660655, 0.04193743838373879, 0.04361602989700032, 0.04175902065160376, 0.04231398783260493, 0.0441473955746863, 0.042300983784736915, 0.04317274664246667, 0.04425094091914347, 0.044207035634549814, 0.045339695345343406, 0.04500141153962445, 0.04568325366962645, 0.04605873421706775, 0.04637794389124443, 0.046217275462492775, 0.04603453894123664, 0.046008666807208186, 0.04596786677293719], 'val_acc': [0.8491530520196452, 0.925929638167786, 0.9297383983161271, 0.9493835822391501, 0.9559987972336373, 0.9558985667034179, 0.9504861180715646, 0.9582038688984664, 0.9605091710935151, 0.9630149343490028, 0.9654204670742709, 0.9662223113160269, 0.9662223113160269, 0.9654204670742709, 0.9629147038187832, 0.9683271524506365, 0.96963014934349, 0.96963014934349, 0.9707326851759046, 0.9707326851759046, 0.9721359125989777, 0.9735391400220507, 0.9719354515385387, 0.9747419063846847, 0.9760449032775383, 0.9720356820687581, 0.9732384484313922, 0.9747419063846847, 0.9751428285055628, 0.9726370652500752, 0.9770472085797334, 0.9764458253984164, 0.9774481307006114, 0.9745414453242458, 0.9772476696401724, 0.9754435200962213, 0.9774481307006114, 0.9779492833517089, 0.9762453643379774, 0.9763455948681968, 0.9759446727473189, 0.9772476696401724, 0.9771474391099529, 0.978350205472587, 0.9771474391099529, 0.978951588653904, 0.9790518191841234, 0.978350205472587, 0.978350205472587, 0.9784504360028065, 0.9774481307006114, 0.979352510774782, 0.9788513581236845, 0.979552971835221, 0.979152049714343, 0.9792522802445625, 0.979352510774782], 'model_size_bytes': 161657, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0026665934327606286, 'batch_size': 128, 'epochs': 57, 'd_model': 54, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3.8907241640412216, 'dropout': 0.027113858497890504, 'weight_decay': 1.5574130714285916e-05, 'patch_size': 40, 'label_smoothing': 0.10595857977887042, 'use_focal_loss': True, 'focal_gamma': 1.4828268480269653, 'grad_clip_norm': 0.87219932560886, 'scheduler': np.str_('cosine'), 'class_weights': np.str_('none'), 'seed': 44609, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 6588, 'model_storage_size_kb': 28.3078125, 'model_size_validation': 'PASS'}
2025-09-23 20:08:44,209 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final validation metrics from training (avoids preprocessing mismatch)
2025-09-23 20:08:44,209 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 0.979352510774782, 'macro_f1': None}
2025-09-23 20:08:44,217 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ“Š STEP 5: Performance Analysis
2025-09-23 20:08:44,217 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 20:08:44,217 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.9744
2025-09-23 20:08:44,217 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: 0.9794
2025-09-23 20:08:44,217 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-09-23 20:08:44,217 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 20:08:44,217 - INFO - evaluation.code_generation_pipeline_orchestrator - Score: 0.9794
2025-09-23 20:08:44,217 - INFO - __main__ - AI-enhanced training completed!
2025-09-23 20:08:44,217 - INFO - __main__ - Final model achieved: {'acc': 0.979352510774782, 'macro_f1': None}
2025-09-23 20:08:44,217 - INFO - __main__ - Pipeline completed successfully in single attempt
2025-09-23 20:08:44,217 - INFO - __main__ - Pipeline completed: Tiny1D-ViT-SE-Transformer-MITBIH, metrics: {'acc': 0.979352510774782, 'macro_f1': None}
2025-09-23 20:08:44,217 - INFO - __main__ - Pipeline summary saved to charts/pipeline_summary_20250923_200844.json
2025-09-23 20:08:44,221 - INFO - __main__ - Model saved: trained_models/best_model_Tiny1D-ViT-SE-Transformer-MITBIH_20250923_200844.pth, performance: {'acc': 0.979352510774782, 'macro_f1': None}
2025-09-23 20:08:44,221 - INFO - __main__ - AI-enhanced processing completed successfully
