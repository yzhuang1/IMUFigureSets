{
  "model_name": "TinyCATNet1D-Transformer (<=256k)",
  "training_code": "import math\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\\nfrom typing import Dict, Tuple\\n\\n# --------------------\\n# Model building blocks\\n# --------------------\\nclass SE1D(nn.Module):\\n    def __init__(self, channels: int, reduction: int = 8):\\n        super().__init__()\\n        r = max(1, channels // reduction)\\n        self.avg = nn.AdaptiveAvgPool1d(1)\\n        self.fc = nn.Sequential(\\n            nn.Conv1d(channels, r, kernel_size=1, bias=True),\\n            nn.ReLU(inplace=True),\\n            nn.Conv1d(r, channels, kernel_size=1, bias=True),\\n            nn.Sigmoid(),\\n        )\\n\\n    def forward(self, x):\\n        w = self.avg(x)\\n        w = self.fc(w)\\n        return x * w\\n\\nclass ConvStem1D(nn.Module):\\n    def __init__(self, in_ch: int = 2, c1: int = 32, c2: int = 64):\\n        super().__init__()\\n        self.conv1 = nn.Conv1d(in_ch, c1, kernel_size=7, stride=2, padding=3, bias=True)\\n        self.act1 = nn.ReLU(inplace=True)\\n        self.conv2 = nn.Conv1d(c1, c2, kernel_size=5, stride=2, padding=2, bias=True)\\n        self.act2 = nn.ReLU(inplace=True)\\n        self.se = SE1D(c2, reduction=8)\\n\\n    def forward(self, x):\\n        x = self.act1(self.conv1(x))\\n        x = self.act2(self.conv2(x))\\n        x = self.se(x)\\n        return x\\n\\nclass Tokenizer1D(nn.Module):\\n    def __init__(self, in_ch: int, d_model: int, k: int = 5, s: int = 5):\\n        super().__init__()\\n        pad = (k // 2) if (k % 2 == 1) else 0\\n        self.proj = nn.Conv1d(in_ch, d_model, kernel_size=k, stride=s, padding=pad, bias=True)\\n\\n    def forward(self, x):\\n        # x: (B, C, L) -> (B, d_model, T)\\n        return self.proj(x)\\n\\nclass LearnablePositionalEncoding(nn.Module):\\n    def __init__(self, d_model: int, max_len: int = 64, dropout: float = 0.0):\\n        super().__init__()\\n        self.pe = nn.Parameter(torch.zeros(1, max_len, d_model))\\n        nn.init.trunc_normal_(self.pe, std=0.02)\\n        self.dropout = nn.Dropout(dropout)\\n\\n    def forward(self, x):\\n        # x: (B, T, d_model)\\n        T = x.size(1)\\n        pe = self.pe[:, :T, :]\\n        return self.dropout(x + pe)\\n\\nclass TinyCATNet(nn.Module):\\n    def __init__(self,\\n                 input_len: int = 1000,\\n                 in_ch: int = 2,\\n                 stem_c1: int = 32,\\n                 stem_c2: int = 64,\\n                 d_model: int = 64,\\n                 nhead: int = 4,\\n                 ff_dim: int = 128,\\n                 num_layers: int = 2,\\n                 token_kernel: int = 5,\\n                 token_stride: int = 5,\\n                 dropout: float = 0.1,\\n                 num_classes: int = 5):\\n        super().__init__()\\n        self.input_len = input_len\\n        self.in_ch = in_ch\\n\\n        self.stem = ConvStem1D(in_ch=in_ch, c1=stem_c1, c2=stem_c2)\\n        self.tokenizer = Tokenizer1D(in_ch=stem_c2, d_model=d_model, k=token_kernel, s=token_stride)\\n        # Estimate max tokens conservatively (for 1000 samples, <=64 tokens with the default strides)\\n        self.posenc = LearnablePositionalEncoding(d_model=d_model, max_len=64, dropout=dropout)\\n\\n        enc_layer = nn.TransformerEncoderLayer(\\n            d_model=d_model, nhead=nhead, dim_feedforward=ff_dim, dropout=dropout, batch_first=True, activation='gelu'\\n        )\\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\\n        self.norm = nn.LayerNorm(d_model)\\n        self.head = nn.Linear(d_model, num_classes)\\n\\n    def forward(self, x):\\n        # Accept (B, 1000, 2) or (B, 2, 1000)\\n        if x.dim() != 3:\\n            raise ValueError(f\"Expected 3D input (B, L, C) or (B, C, L), got shape {tuple(x.shape)}\")\\n        if x.size(1) == self.input_len and x.size(2) == self.in_ch:\\n            # (B, L, C) -> (B, C, L)\\n            x = x.transpose(1, 2).contiguous()\\n        elif x.size(1) == self.in_ch:\\n            # already (B, C, L)\\n            pass\\n        else:\\n            raise ValueError(f\"Unexpected input shape {tuple(x.shape)}; expected (B, {self.input_len}, {self.in_ch}) or (B, {self.in_ch}, {self.input_len})\")\\n\\n        x = self.stem(x)           # (B, C', L')\\n        x = self.tokenizer(x)      # (B, d_model, T)\\n        x = x.transpose(1, 2)      # (B, T, d_model)\\n        x = self.posenc(x)         # add positional embedding\\n        x = self.encoder(x)        # (B, T, d_model)\\n        x = self.norm(x)\\n        x = x.mean(dim=1)          # global average pooling over tokens\\n        logits = self.head(x)      # (B, num_classes)\\n        return logits\\n\\n# --------------------\\n# Utilities\\n# --------------------\\n@torch.no_grad()\\ndef _evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Dict[str, float]:\\n    model.eval()\\n    total_loss = 0.0\\n    total = 0\\n    correct = 0\\n    all_preds = []\\n    all_targets = []\\n    for xb, yb in loader:\\n        xb = xb.to(device)\\n        yb = yb.to(device)\\n        logits = model(xb)\\n        loss = F.cross_entropy(logits, yb)\\n        total_loss += loss.item() * yb.size(0)\\n        preds = logits.argmax(dim=1)\\n        correct += (preds == yb).sum().item()\\n        total += yb.size(0)\\n        all_preds.append(preds.cpu())\\n        all_targets.append(yb.cpu())\\n    avg_loss = total_loss / max(1, total)\\n    acc = correct / max(1, total)\\n    # F1 per-class and macro\\n    preds = torch.cat(all_preds) if all_preds else torch.tensor([])\\n    targets = torch.cat(all_targets) if all_targets else torch.tensor([])\\n    f1_per_class, f1_macro = _f1_scores(preds, targets, num_classes=5)\\n    return {\\n        'loss': avg_loss,\\n        'accuracy': acc,\\n        'f1_macro': f1_macro,\\n        **{f'f1_c{i}': f1_per_class[i] for i in range(len(f1_per_class))}\\n    }\\n\\ndef _f1_scores(preds: torch.Tensor, targets: torch.Tensor, num_classes: int) -> Tuple[list, float]:\\n    if preds.numel() == 0:\\n        return [0.0]*num_classes, 0.0\\n    f1s = []\\n    for c in range(num_classes):\\n        tp = ((preds == c) & (targets == c)).sum().item()\\n        fp = ((preds == c) & (targets != c)).sum().item()\\n        fn = ((preds != c) & (targets == c)).sum().item()\\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\\n        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\\n        f1s.append(f1)\\n    f1_macro = float(sum(f1s) / max(1, num_classes))\\n    return f1s, f1_macro\\n\\ndef _count_params(model: nn.Module) -> int:\\n    return sum(p.numel() for p in model.parameters())\\n\\ndef _make_class_weights(y: torch.Tensor, num_classes: int = 5) -> torch.Tensor:\\n    counts = torch.bincount(y.cpu(), minlength=num_classes).float()\\n    counts = torch.clamp(counts, min=1.0)\\n    total = counts.sum()\\n    weights = total / (counts * num_classes)\\n    # normalize to mean 1 for scale stability\\n    weights = weights * (num_classes / weights.sum())\\n    return weights\\n\\n# --------------------\\n# Quantization helpers\\n# --------------------\\nfrom torch.ao.quantization import get_default_qconfig, QConfigMapping\\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\\nfrom torch.ao.quantization import quantize_dynamic\\n\\ndef _quantize_model_post_training(model: nn.Module,\\n                                  X_calib: torch.Tensor,\\n                                  bits: int = 8,\\n                                  quantize_weights: bool = True,\\n                                  quantize_activations: bool = True,\\n                                  calib_batches: int = 32,\\n                                  batch_size: int = 128) -> Tuple[nn.Module, str]:\\n    \"\"\"\\n    Returns a quantized copy of model (operates on CPU).\\n    Strategies:\\n      - If quantize_activations and bits==8: FX static int8 quantization (Conv/Linear supported parts), with calibration.\\n      - Else if quantize_weights and bits in {8,16}: dynamic quantization on Linear (and MHA internals).\\n      - Else: return fp32 copy.\\n    \"\"\"\\n    model_cpu = model.to('cpu').eval()\\n    approach = 'none'\\n\\n    if quantize_activations and bits == 8:\\n        try:\\n            torch.backends.quantized.engine = 'fbgemm' if torch.backends.quantized.engine in ['fbgemm', 'qnnpack'] else 'fbgemm'\\n            qconfig = get_default_qconfig(torch.backends.quantized.engine)\\n            qconfig_mapping = QConfigMapping().set_global(qconfig)\\n            example_input = torch.randn(1, 1000, 2)\\n            prepared = prepare_fx(model_cpu, qconfig_mapping, example_inputs=(example_input,))\\n            # Calibration\\n            model_inputs = X_calib\\n            if model_inputs.dim() == 2:\\n                model_inputs = model_inputs.unsqueeze(0)\\n            n = model_inputs.size(0)\\n            steps = min(calib_batches, max(1, math.ceil(n / batch_size)))\\n            with torch.no_grad():\\n                for i in range(steps):\\n                    s = i * batch_size\\n                    e = min(n, s + batch_size)\\n                    xb = model_inputs[s:e].to('cpu')\\n                    prepared(xb)\\n            qmodel = convert_fx(prepared)\\n            approach = 'fx_static_int8'\\n            return qmodel, approach\\n        except Exception as e:\\n            # Fallback to dynamic quantization of Linear layers\\n            pass\\n\\n    if quantize_weights and bits in (8, 16):\\n        dtype = torch.qint8 if bits == 8 else torch.float16\\n        try:\\n            qmodel = quantize_dynamic(\\n                model_cpu, {nn.Linear, nn.MultiheadAttention}, dtype=dtype\\n            )\\n            approach = f'dynamic_{\"int8\" if bits==8 else \"fp16\"}'\\n            return qmodel, approach\\n        except Exception:\\n            pass\\n\\n    # No quantization\\n    return model_cpu, approach\\n\\n# --------------------\\n# Training entry point\\n# --------------------\\n\\ndef train_model(\\n    X_train: torch.Tensor,\\n    y_train: torch.Tensor,\\n    X_val: torch.Tensor,\\n    y_val: torch.Tensor,\\n    device: torch.device,\\n    # Optimization\\n    epochs: int = 20,\\n    lr: float = 1e-3,\\n    batch_size: int = 128,\\n    weight_decay: float = 1e-5,\\n    grad_clip: float = 0.5,\\n    use_amp: bool = False,\\n    # Architecture\\n    stem_c1: int = 32,\\n    stem_c2: int = 64,\\n    d_model: int = 64,\\n    nhead: int = 4,\\n    ff_dim: int = 128,\\n    num_layers: int = 2,\\n    token_kernel: int = 5,\\n    token_stride: int = 5,\\n    dropout: float = 0.1,\\n    # Data handling / loss\\n    use_weighted_sampler: bool = True,\\n    label_smoothing: float = 0.05,\\n    seed: int = 42,\\n    # Quantization params\\n    quantization_bits: int = 8,\\n    quantize_weights: bool = True,\\n    quantize_activations: bool = True,\\n    calib_batches: int = 32\\n):\\n    \\\"\\\"\\\"\\n    Train a lightweight CAT-Net style 1D CNN + Transformer classifier (5 classes) on ECG beats.\\n\\n    Inputs are torch tensors:\\n      - X_*: shape (N, 1000, 2) or (N, 2, 1000), dtype float32\\n      - y_*: shape (N,), long with 0..4 encoding AAMI N/S/V/F/Q.\\n\\n    Returns: (quantized_model, metrics_dict)\\n      - quantized_model is on CPU (post-training quantized)\\n      - metrics include pre/post-quantization scores and parameter count (<=256k).\\n    \\\"\\\"\\\"\\n    torch.manual_seed(seed)\\n\\n    num_classes = 5\\n\\n    # Datasets & loaders\\n    X_train = X_train.contiguous()\\n    X_val = X_val.contiguous()\\n    y_train = y_train.long().contiguous()\\n    y_val = y_val.long().contiguous()\\n\\n    class_weights = _make_class_weights(y_train, num_classes=num_classes).to(device)\\n\\n    if use_weighted_sampler:\\n        sample_weights = class_weights.cpu()[y_train.cpu()]\\n        sampler = WeightedRandomSampler(weights=sample_weights.double(), num_samples=len(sample_weights), replacement=True)\\n        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, sampler=sampler, drop_last=False)\\n    else:\\n        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=False)\\n\\n    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False, drop_last=False)\\n\\n    # Build model with budget enforcement (<= 256k parameters)\\n    def build_model_with_budget(d_model_, nhead_, ff_dim_, num_layers_):\\n        # Ensure divisibility\\n        nhead_ = max(1, min(nhead_, 8))\\n        if d_model_ % nhead_ != 0:\\n            d_model_ = (d_model_ // nhead_) * nhead_ or nhead_\\n        model_ = TinyCATNet(\\n            input_len=1000, in_ch=2,\\n            stem_c1=stem_c1, stem_c2=stem_c2,\\n            d_model=d_model_, nhead=nhead_, ff_dim=ff_dim_, num_layers=num_layers_,\\n            token_kernel=token_kernel, token_stride=token_stride, dropout=dropout, num_classes=num_classes\\n        )\\n        return model_\\n\\n    model = build_model_with_budget(d_model, nhead, ff_dim, num_layers)\\n    # Reduce dimensions if needed to satisfy parameter budget\\n    def within_budget(m):\\n        return _count_params(m) <= 256_000\\n\\n    if not within_budget(model):\\n        # Iteratively shrink until under budget\\n        dm, ff, nl, nh = d_model, ff_dim, num_layers, nhead\\n        while True:\\n            # Prefer reducing FF, then d_model, then layers\\n            if ff > max(64, dm):\\n                ff = max(64, (ff * 3) // 4)\\n            elif dm > max(32, nh):\\n                dm = max(nh, (dm * 3) // 4)\\n            elif nl > 1:\\n                nl -= 1\\n            else:\\n                break\\n            m2 = build_model_with_budget(dm, nh, ff, nl)\\n            if within_budget(m2):\\n                model = m2\\n                break\\n        # Final safety: if still too big, force minimal config\\n        if not within_budget(model):\\n            model = build_model_with_budget(48, max(1, min(nhead, 4)), 96, 1)\\n\\n    model.to(device)\\n    param_count = _count_params(model)\\n\\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\\n\\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == 'cuda')\\n\\n    best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\\n    best_val_loss = float('inf')\\n\\n    for epoch in range(epochs):\\n        model.train()\\n        running = 0.0\\n        seen = 0\\n        for xb, yb in train_loader:\\n            xb = xb.to(device)\\n            yb = yb.to(device)\\n            optimizer.zero_grad(set_to_none=True)\\n            with torch.cuda.amp.autocast(enabled=use_amp and device.type == 'cuda'):\\n                logits = model(xb)\\n                loss = F.cross_entropy(logits, yb, weight=class_weights, label_smoothing=float(label_smoothing))\\n            scaler.scale(loss).backward()\\n            if grad_clip and grad_clip > 0:\\n                scaler.unscale_(optimizer)\\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\\n            scaler.step(optimizer)\\n            scaler.update()\\n            running += loss.item() * yb.size(0)\\n            seen += yb.size(0)\\n        train_loss = running / max(1, seen)\\n\\n        # Validation\\n        val_metrics = _evaluate(model, val_loader, device)\\n        if val_metrics['loss'] < best_val_loss:\\n            best_val_loss = val_metrics['loss']\\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\\n\\n    # Load best weights\\n    model.load_state_dict(best_state)\\n\\n    # Final eval on validation (fp32)\\n    final_val = _evaluate(model, val_loader, device)\\n\\n    # Post-training quantization on CPU\\n    qmodel, qapproach = _quantize_model_post_training(\\n        model, X_train[: min(len(X_train), batch_size * calib_batches)].detach().cpu(),\\n        bits=int(quantization_bits),\\n        quantize_weights=bool(quantize_weights),\\n        quantize_activations=bool(quantize_activations),\\n        calib_batches=int(calib_batches),\\n        batch_size=int(batch_size)\\n    )\\n\\n    # Evaluate quantized model (CPU)\\n    q_val_loader = DataLoader(TensorDataset(X_val.cpu(), y_val.cpu()), batch_size=batch_size, shuffle=False)\\n    q_metrics = _evaluate(qmodel, q_val_loader, torch.device('cpu'))\\n\\n    metrics = {\\n        'param_count': int(param_count),\\n        'train_loss_last_epoch': float(train_loss),\\n        'val_loss': float(final_val['loss']),\\n        'val_accuracy': float(final_val['accuracy']),\\n        'val_f1_macro': float(final_val['f1_macro']),\\n        'val_f1_per_class': [float(final_val[f'f1_c{i}']) for i in range(5)],\\n        'quantization': {\\n            'approach': qapproach,\\n            'bits': int(quantization_bits),\\n            'quantize_weights': bool(quantize_weights),\\n            'quantize_activations': bool(quantize_activations)\\n        },\\n        'quantized_val_loss': float(q_metrics['loss']),\\n        'quantized_val_accuracy': float(q_metrics['accuracy']),\\n        'quantized_val_f1_macro': float(q_metrics['f1_macro']),\\n        'quantized_val_f1_per_class': [float(q_metrics[f'f1_c{i}']) for i in range(5)]\\n    }\\n\\n    return qmodel, metrics\\n",
  "bo_config": {
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 100
    },
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 128,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128,
        256
      ]
    },
    "weight_decay": {
      "default": 1e-05,
      "type": "Real",
      "low": 1e-08,
      "high": 0.001,
      "prior": "log-uniform"
    },
    "grad_clip": {
      "default": 0.5,
      "type": "Real",
      "low": 0.0,
      "high": 2.0
    },
    "use_amp": {
      "default": false,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "stem_c1": {
      "default": 32,
      "type": "Categorical",
      "categories": [
        32,
        48,
        64
      ]
    },
    "stem_c2": {
      "default": 64,
      "type": "Categorical",
      "categories": [
        48,
        64,
        80
      ]
    },
    "d_model": {
      "default": 64,
      "type": "Integer",
      "low": 32,
      "high": 96
    },
    "nhead": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        2,
        4,
        8
      ]
    },
    "ff_dim": {
      "default": 128,
      "type": "Integer",
      "low": 64,
      "high": 256
    },
    "num_layers": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 4
    },
    "token_kernel": {
      "default": 5,
      "type": "Integer",
      "low": 3,
      "high": 9
    },
    "token_stride": {
      "default": 5,
      "type": "Integer",
      "low": 4,
      "high": 10
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "use_weighted_sampler": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "label_smoothing": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "seed": {
      "default": 42,
      "type": "Integer",
      "low": 0,
      "high": 1000000
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "calib_batches": {
      "default": 32,
      "type": "Integer",
      "low": 10,
      "high": 200
    }
  },
  "confidence": 0.88,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758307184,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}