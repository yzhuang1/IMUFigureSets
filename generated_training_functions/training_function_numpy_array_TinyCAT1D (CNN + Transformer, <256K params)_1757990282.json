{
  "model_name": "TinyCAT1D (CNN + Transformer, <256K params)",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    \"\"\"\n    Train a lightweight 1D CNN + Transformer model for 5-class ECG classification.\n    Inputs are PyTorch tensors (no numpy). X can be (N, L, C) or (N, C, L).\n    Returns: (model, metrics_dict)\n    \"\"\"\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # ---------------------- Hyperparameters ----------------------\n    defaults = {\n        'lr': 1e-3,\n        'epochs': 12,\n        'batch_size': 64,\n        'hidden_size': 64,      # d_model for Transformer (must be divisible by nheads)\n        'dropout': 0.2,\n        'nheads': 4,\n        'mlp_ratio': 4.0,\n        'patch_stride': 4,\n        'patch_kernel': 8,\n        'weight_decay': 1e-4,\n        'use_focal': True,\n        'gamma': 2.0,            # Focal loss gamma\n        'grad_clip': 1.0,\n        'num_workers': 0,        # keep simple/portable\n        'num_classes': 5\n    }\n    cfg = {**defaults, **hyperparams}\n\n    # ---------------------- Utilities ----------------------\n    def count_params(model: nn.Module) -> int:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    def sinusoidal_pe(length: int, d_model: int, device):\n        pe = torch.zeros(length, d_model, device=device)\n        position = torch.arange(0, length, dtype=torch.float, device=device).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return pe  # (L, D)\n\n    class TransformerBlock(nn.Module):\n        def __init__(self, d_model, nhead, mlp_ratio=4.0, dropout=0.2):\n            super().__init__()\n            self.norm1 = nn.LayerNorm(d_model)\n            self.attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n            self.norm2 = nn.LayerNorm(d_model)\n            hidden = int(d_model * mlp_ratio)\n            self.mlp = nn.Sequential(\n                nn.Linear(d_model, hidden),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden, d_model),\n                nn.Dropout(dropout),\n            )\n        def forward(self, x):  # x: (B, L, D)\n            x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x), need_weights=False)[0]\n            x = x + self.mlp(self.norm2(x))\n            return x\n\n    class TinyCAT1D(nn.Module):\n        def __init__(self, in_ch, num_classes, d_model=64, nhead=4, mlp_ratio=4.0, dropout=0.2, patch_kernel=8, patch_stride=4):\n            super().__init__()\n            self.dropout_p = dropout\n            # CNN stem (captures local morphology)\n            self.conv1 = nn.Conv1d(in_ch, 32, kernel_size=7, padding=3)\n            self.bn1 = nn.BatchNorm1d(32)\n            self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n            self.bn2 = nn.BatchNorm1d(64)\n            self.conv3 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n            self.bn3 = nn.BatchNorm1d(64)\n            # Patch embedding (sequence reduction for attention)\n            self.patch_embed = nn.Conv1d(64, d_model, kernel_size=patch_kernel, stride=patch_stride, padding=patch_kernel // 2)\n            # Light Transformer encoder (captures rhythm/context)\n            self.blocks = nn.ModuleList([\n                TransformerBlock(d_model, nhead, mlp_ratio=mlp_ratio, dropout=dropout),\n                TransformerBlock(d_model, nhead, mlp_ratio=mlp_ratio, dropout=dropout),\n            ])\n            self.head = nn.Sequential(\n                nn.LayerNorm(d_model),\n                nn.Linear(d_model, num_classes)\n            )\n        def _to_NCL(self, x):\n            # Accepts (N, C, L) or (N, L, C) or (N, L)\n            if x.dim() == 2:\n                x = x.unsqueeze(1)  # (N, 1, L)\n            if x.dim() == 3:\n                N, A, B = x.shape\n                # Heuristic: if middle dim looks like length and last dim is small (channels), swap\n                if A < B and A <= 8:\n                    x = x.permute(0, 2, 1)  # (N, C, L)\n            return x\n        def forward(self, x):\n            x = self._to_NCL(x)  # (N, C, L)\n            x = F.silu(self.bn1(self.conv1(x)))\n            x = F.silu(self.bn2(self.conv2(x)))\n            x = F.silu(self.bn3(self.conv3(x)))\n            x = F.dropout(x, p=self.dropout_p, training=self.training)\n            x = self.patch_embed(x)  # (N, D, L')\n            x = x.transpose(1, 2)    # (N, L', D)\n            pe = sinusoidal_pe(x.size(1), x.size(2), x.device)\n            x = x + pe.unsqueeze(0)\n            for blk in self.blocks:\n                x = blk(x)\n            x = x.mean(dim=1)  # GAP over time\n            logits = self.head(x)\n            return logits\n\n    class FocalLoss(nn.Module):\n        def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n            super().__init__()\n            self.alpha = alpha  # Tensor[num_classes] or None\n            self.gamma = gamma\n            self.reduction = reduction\n        def forward(self, logits, target):\n            # Cross-entropy per-sample\n            ce = F.cross_entropy(logits, target, weight=self.alpha, reduction='none')\n            pt = torch.softmax(logits, dim=1).gather(1, target.view(-1, 1)).squeeze(1).clamp_min(1e-8)\n            loss = ((1 - pt) ** self.gamma) * ce\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            else:\n                return loss\n\n    def macro_f1_from_logits(logits, targets, num_classes):\n        preds = logits.argmax(dim=1)\n        cm = torch.zeros(num_classes, num_classes, device=logits.device)\n        for t, p in zip(targets.view(-1), preds.view(-1)):\n            cm[t.long(), p.long()] += 1\n        tp = torch.diag(cm)\n        fp = cm.sum(0) - tp\n        fn = cm.sum(1) - tp\n        f1 = (2 * tp) / (2 * tp + fp + fn + 1e-12)\n        macro_f1 = torch.nanmean(f1).item()\n        acc = (preds == targets).float().mean().item()\n        return acc, macro_f1\n\n    # ---------------------- Prep data ----------------------\n    num_classes = int(cfg['num_classes'])\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    # Class weights (inverse frequency) for imbalance mitigation\n    with torch.no_grad():\n        counts = torch.bincount(y_train.view(-1), minlength=num_classes).float()\n        weights = 1.0 / (counts + 1e-12)\n        weights = weights * (num_classes / weights.sum())  # normalize to mean=1\n\n    # DataLoaders; pin_memory only if tensors are on CPU\n    pin_mem = (X_train.device.type == 'cpu')\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=int(cfg['batch_size']),\n        shuffle=True,\n        num_workers=int(cfg['num_workers']),\n        pin_memory=pin_mem,\n        drop_last=False,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=int(cfg['batch_size']),\n        shuffle=False,\n        num_workers=int(cfg['num_workers']),\n        pin_memory=pin_mem,\n        drop_last=False,\n    )\n\n    # ---------------------- Build model ----------------------\n    # Infer input channels from a small batch\n    sample_X = next(iter(train_loader))[0]\n    if sample_X.dim() == 2:\n        in_ch = 1\n    elif sample_X.dim() == 3:\n        # heuristic consistent with forward\n        N, A, B = sample_X.shape\n        in_ch = B if (A < B and A <= 8) else A\n    else:\n        raise ValueError('X_train must be 2D or 3D tensor per sample.')\n\n    d_model = int(cfg['hidden_size'])\n    nheads = int(cfg['nheads'])\n    assert d_model % nheads == 0, 'hidden_size must be divisible by nheads'\n\n    model = TinyCAT1D(\n        in_ch=in_ch,\n        num_classes=num_classes,\n        d_model=d_model,\n        nhead=nheads,\n        mlp_ratio=float(cfg['mlp_ratio']),\n        dropout=float(cfg['dropout']),\n        patch_kernel=int(cfg['patch_kernel']),\n        patch_stride=int(cfg['patch_stride']),\n    ).to(device)\n\n    param_count = count_params(model)\n\n    # ---------------------- Loss/Optimizer ----------------------\n    class_weights = weights.to(device)\n    if bool(cfg['use_focal']):\n        criterion = FocalLoss(alpha=class_weights, gamma=float(cfg['gamma']))\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(cfg['lr']), weight_decay=float(cfg['weight_decay']))\n\n    # ---------------------- Train loop ----------------------\n    train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n\n    for epoch in range(int(cfg['epochs'])):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if cfg['grad_clip'] is not None and float(cfg['grad_clip']) > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(cfg['grad_clip']))\n            optimizer.step()\n            bs = yb.size(0)\n            running_loss += loss.item() * bs\n            n_train += bs\n        epoch_train_loss = running_loss / max(n_train, 1)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        all_logits = []\n        all_targets = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=True)\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.item() * yb.size(0)\n                n_val += yb.size(0)\n                all_logits.append(logits)\n                all_targets.append(yb)\n        val_loss = val_running_loss / max(n_val, 1)\n        all_logits = torch.cat(all_logits, dim=0)\n        all_targets = torch.cat(all_targets, dim=0)\n        acc, macro_f1 = macro_f1_from_logits(all_logits, all_targets, num_classes)\n        val_losses.append(val_loss)\n        val_accs.append(acc)\n        val_f1s.append(macro_f1)\n\n    metrics = {\n        'train_loss': train_losses,\n        'val_loss': val_losses,\n        'val_acc': val_accs,\n        'val_macro_f1': val_f1s,\n        'param_count': int(param_count)\n    }\n\n    return model, metrics\n",
  "hyperparameters": {
    "lr": 0.001,
    "epochs": 12,
    "batch_size": 64
  },
  "reasoning": "The model is a lightweight CNN + Transformer (CAT) architecture, aligning with recent literature for ECG arrhythmia classification. A small CNN stem captures local beat morphology; a strided 1D patch embedding reduces sequence length before attention, keeping compute low. Two small Transformer blocks (d_model=64, 4 heads) model rhythm/context while staying under 256K parameters. Class imbalance is handled via inverse-frequency class weights and optional focal loss, and metrics include macro-F1â€”recommended for AAMI five-class evaluation. The training loop is simple (AdamW, optional grad clipping), with conditional pin_memory used only when tensors reside on CPU, per requirement.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "batch_size",
    "epochs",
    "hidden_size",
    "dropout"
  ],
  "bo_search_space": {
    "lr": {
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "type": "Categorical",
      "categories": [
        8,
        16,
        32,
        64,
        128
      ]
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "hidden_size": {
      "type": "Integer",
      "low": 32,
      "high": 512
    },
    "dropout": {
      "type": "Real",
      "low": 0.0,
      "high": 0.7
    }
  },
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757990282,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}