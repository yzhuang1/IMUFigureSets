{
  "model_name": "STCT_ECG_TinyTransformer",
  "training_code": "import math\nfrom typing import Dict, Any, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# ------------------------------\n# Utility blocks\n# ------------------------------\nclass DepthwiseSeparableConv1d(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, kernel_size: int = 7, stride: int = 1, padding: int = None, bias: bool = False):\n        super().__init__()\n        if padding is None:\n            padding = kernel_size // 2\n        self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_ch, bias=bias)\n        self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=bias)\n    def forward(self, x):\n        x = self.dw(x)\n        x = self.pw(x)\n        return x\n\ndef sinusoidal_positional_encoding(seq_len: int, dim: int, device: torch.device):\n    pe = torch.zeros(seq_len, dim, device=device)\n    position = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, dim, 2, device=device).float() * (-math.log(10000.0) / dim))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    return pe.unsqueeze(0)  # (1, L, D)\n\nclass STCTClassifier(nn.Module):\n    def __init__(\n        self,\n        num_classes: int = 5,\n        input_channels: int = 2,\n        embed_dim: int = 64,\n        stem_channels: int = 32,\n        num_layers: int = 2,\n        num_heads: int = 4,\n        mlp_ratio: int = 4,\n        dropout: float = 0.1,\n        kernel_size: int = 7,\n        stride1: int = 2,\n        stride2: int = 2,\n        use_cls_token: bool = True,\n    ):\n        super().__init__()\n        assert input_channels == 2, \"This STCT variant expects 2-lead input\"\n        self.use_cls_token = use_cls_token\n        k = kernel_size\n        # Temporal CNN stem with depthwise separable convs per lead\n        self.dw1 = nn.Conv1d(input_channels, input_channels, kernel_size=k, stride=stride1, padding=k//2, groups=input_channels, bias=False)\n        self.bn1 = nn.BatchNorm1d(input_channels)\n        self.act = nn.SiLU()\n        # Lightweight spatial lead mixer (mixes the two leads)\n        self.lead_mixer = nn.Conv1d(input_channels, input_channels, kernel_size=1, bias=False)\n        # Pointwise to expand channels\n        self.pw1 = nn.Conv1d(input_channels, stem_channels, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm1d(stem_channels)\n        # Second depthwise separable conv to reduce temporal tokens further\n        self.dw2 = nn.Conv1d(stem_channels, stem_channels, kernel_size=k, stride=stride2, padding=k//2, groups=stem_channels, bias=False)\n        self.bn3 = nn.BatchNorm1d(stem_channels)\n        self.pw2 = nn.Conv1d(stem_channels, embed_dim, kernel_size=1, bias=False)\n        self.bn4 = nn.BatchNorm1d(embed_dim)\n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=embed_dim * mlp_ratio,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True,\n            norm_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        # Class token (optional)\n        if self.use_cls_token:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n            nn.init.trunc_normal_(self.cls_token, std=0.02)\n        # Head\n        self.head = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim, num_classes)\n        )\n\n    def forward(self, x: torch.Tensor):\n        # Expect x: (B, C=2, T) or (B, T, C=2)\n        if x.dim() == 3 and x.shape[1] != 2 and x.shape[2] == 2:\n            x = x.permute(0, 2, 1)  # (B, 2, T)\n        # CNN stem\n        x = self.dw1(x)\n        x = self.bn1(x)\n        x = self.act(x)\n        x = self.lead_mixer(x)\n        x = self.pw1(x)\n        x = self.bn2(x)\n        x = self.act(x)\n        x = self.dw2(x)\n        x = self.bn3(x)\n        x = self.act(x)\n        x = self.pw2(x)\n        x = self.bn4(x)\n        x = self.act(x)  # (B, E, L)\n        x = x.transpose(1, 2)  # (B, L, E)\n        B, L, E = x.shape\n        # Add positional encoding (sinusoidal, parameter-free)\n        pe = sinusoidal_positional_encoding(L + (1 if self.use_cls_token else 0), E, x.device)\n        if self.use_cls_token:\n            cls_tok = self.cls_token.expand(B, -1, -1)\n            x = torch.cat([cls_tok, x], dim=1)  # (B, L+1, E)\n        x = x + pe\n        x = self.transformer(x)\n        if self.use_cls_token:\n            pooled = x[:, 0]  # (B, E)\n        else:\n            pooled = x.mean(dim=1)  # (B, E)\n        logits = self.head(pooled)\n        return logits\n\n# ------------------------------\n# Losses\n# ------------------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha: torch.Tensor = None, gamma: float = 2.0, reduction: str = 'mean'):\n        super().__init__()\n        self.register_buffer('alpha', alpha if alpha is not None else None)\n        self.gamma = gamma\n        self.reduction = reduction\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        ce = F.cross_entropy(logits, targets, weight=self.alpha, reduction='none')\n        pt = torch.exp(-ce)\n        loss = (1 - pt) ** self.gamma * ce\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss\n\n# ------------------------------\n# Training function\n# ------------------------------\n\ndef train_model(\n    X_train: torch.Tensor,\n    y_train: torch.Tensor,\n    X_val: torch.Tensor,\n    y_val: torch.Tensor,\n    device: torch.device,\n    # Hyperparameters\n    lr: float = 1e-3,\n    batch_size: int = 128,\n    epochs: int = 15,\n    weight_decay: float = 1e-4,\n    dropout: float = 0.1,\n    embed_dim: int = 64,\n    stem_channels: int = 32,\n    num_heads: int = 4,\n    num_layers: int = 2,\n    mlp_ratio: int = 4,\n    kernel_size: int = 7,\n    stride1: int = 2,\n    stride2: int = 2,\n    use_cls_token: bool = True,\n    use_focal: bool = True,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    grad_clip_norm: float = 1.0,\n    # Quantization parameters\n    quantization_bits: int = 8,  # {8, 16, 32}\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n) -> Tuple[nn.Module, Dict[str, Any]]:\n    \"\"\"\n    Train a 5-class classifier on 2-lead, 1000-step ECG tensors using an STCT-style Conv-Transformer.\n    Inputs:\n      - X_*: tensors shaped (N, 1000, 2) or (N, 2, 1000)\n      - y_*: long tensors shaped (N,), class ids in [0..4]\n    Returns:\n      - quantized model (post-training quantization, CPU model)\n      - metrics dict with lists: train_losses, val_losses, val_acc\n    \"\"\"\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n    num_classes = 5\n\n    # Ensure d_model divisible by heads\n    if embed_dim % num_heads != 0:\n        embed_dim = (embed_dim // num_heads) * num_heads\n        embed_dim = max(embed_dim, num_heads)  # avoid zero\n\n    # Prepare datasets and loaders\n    def _to_model_shape(x: torch.Tensor) -> torch.Tensor:\n        # Accept (N, 1000, 2) or (N, 2, 1000)\n        if x.dim() != 3:\n            raise ValueError(f\"Expected 3D tensor (N, T, C) or (N, C, T), got shape {tuple(x.shape)}\")\n        if x.shape[-1] == 2:\n            # (N, T, C=2) -> (N, C, T)\n            x = x.permute(0, 2, 1)\n        elif x.shape[1] == 2:\n            # already (N, C=2, T)\n            pass\n        else:\n            raise ValueError(\"Input must have 2 channels (leads). Got shape {}\".format(tuple(x.shape)))\n        return x.contiguous().float()\n\n    X_train = _to_model_shape(X_train)\n    X_val = _to_model_shape(X_val)\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = STCTClassifier(\n        num_classes=num_classes,\n        input_channels=2,\n        embed_dim=embed_dim,\n        stem_channels=stem_channels,\n        num_layers=num_layers,\n        num_heads=num_heads,\n        mlp_ratio=mlp_ratio,\n        dropout=dropout,\n        kernel_size=kernel_size,\n        stride1=stride1,\n        stride2=stride2,\n        use_cls_token=use_cls_token,\n    ).to(device)\n\n    # Sanity-check parameter budget <= 256k\n    n_params = sum(p.numel() for p in model.parameters())\n    if n_params > 256 * 1024:\n        raise RuntimeError(f\"Model has {n_params} parameters, exceeds 256K budget. Reduce embed_dim/num_layers/mlp_ratio.\")\n\n    # Class weights to address imbalance\n    with torch.no_grad():\n        counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n        counts = torch.clamp(counts, min=1.0)\n        class_weights = (counts.sum() / (counts * num_classes)).to(device)\n\n    # Loss\n    if use_focal:\n        loss_fn = FocalLoss(alpha=class_weights, gamma=float(focal_gamma))\n    else:\n        loss_fn = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(label_smoothing))\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        # Train\n        model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = loss_fn(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_norm)\n            optimizer.step()\n            running_loss += loss.item() * xb.size(0)\n        epoch_train_loss = running_loss / len(train_loader.dataset)\n        train_losses.append(epoch_train_loss)\n\n        # Validate\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = loss_fn(logits, yb)\n                val_running_loss += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += yb.numel()\n        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        print(f\"Epoch {epoch:03d}/{epochs} - train_loss: {epoch_train_loss:.4f} | val_loss: {epoch_val_loss:.4f} | val_acc: {epoch_val_acc:.4f}\")\n\n    # Post-training quantization\n    # Strategy: dynamic quantization for Transformer (Linear layers). Convs remain in FP32.\n    # If quantization_bits == 32 or both flags False -> return FP32 model.\n    model.eval()\n    quantized_model = model\n\n    do_quantize = (quantization_bits in (8, 16)) and (quantize_weights or quantize_activations)\n    if do_quantize:\n        # Move to CPU for dynamic quantization\n        model_cpu = model.to('cpu')\n        import torch.ao.quantization as aoq\n        if quantization_bits == 8:\n            dtype = torch.qint8\n        elif quantization_bits == 16:\n            dtype = torch.float16\n        else:\n            dtype = None\n        if dtype is not None:\n            # Quantize Linear layers (covers Transformer internals and final head)\n            quantized_model = aoq.quantize_dynamic(model_cpu, {nn.Linear}, dtype=dtype, inplace=False)\n        else:\n            quantized_model = model_cpu\n    else:\n        # Keep as-is; move to CPU for portability\n        quantized_model = model.to('cpu')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'num_parameters': n_params,\n        'quantization_bits': quantization_bits,\n        'quantized_on': 'Linear (Transformer + head) via dynamic quantization' if do_quantize else 'none'\n    }\n\n    return quantized_model, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 128,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128,
        256
      ]
    },
    "epochs": {
      "default": 15,
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.001,
      "prior": "log-uniform"
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "embed_dim": {
      "default": 64,
      "type": "Integer",
      "low": 32,
      "high": 80
    },
    "stem_channels": {
      "default": 32,
      "type": "Integer",
      "low": 16,
      "high": 64
    },
    "num_heads": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        2,
        4
      ]
    },
    "num_layers": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 3
    },
    "mlp_ratio": {
      "default": 4,
      "type": "Integer",
      "low": 2,
      "high": 4
    },
    "kernel_size": {
      "default": 7,
      "type": "Categorical",
      "categories": [
        5,
        7,
        9,
        11
      ]
    },
    "stride1": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 4
    },
    "stride2": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 4
    },
    "use_cls_token": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "use_focal": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 1.0,
      "high": 4.0
    },
    "label_smoothing": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 0.1
    },
    "grad_clip_norm": {
      "default": 1.0,
      "type": "Real",
      "low": 0.0,
      "high": 2.0
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "confidence": 0.9,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758352718,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}