{
  "model_name": "CAT-Net Tiny (1D CNN + Channel Attention + Transformer)",
  "training_code": "import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\n\ndef train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a compact CAT-Net (CNN + Channel Attention + Transformer) for 5-class classification on 1000x2 ECG windows.\n\n    Args:\n        X_train, y_train, X_val, y_val: torch.Tensors\n            - X_* shape: (N, 1000, 2) or (N, 2, 1000)\n            - y_* shape: (N,), dtype long with values in [0..4]\n        device: str or torch.device\n        kwargs: hyperparameters and quantization parameters (see bo_config)\n\n    Returns:\n        quantized_model: the post-training quantized model (on CPU)\n        metrics: dict with train_losses, val_losses, val_acc, val_macro_f1 (lists per epoch)\n    \"\"\"\n    # ------------------ Device handling ------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        raise ValueError(\"GPU is required: pass device='cuda' or torch.device('cuda') and ensure CUDA is available.\")\n\n    # ------------------ Hyperparameters ------------------\n    # Architecture\n    sequence_length = 1000\n    in_channels = 2\n    num_classes = 5\n    stem_channels = int(kwargs.get('stem_channels', 16))          # after first pointwise conv\n    embed_channels = int(kwargs.get('embed_channels', 32))        # channels before patch embed conv\n    se_reduction = int(kwargs.get('se_reduction', 4))\n    d_model = int(kwargs.get('d_model', 32))\n    num_heads = int(kwargs.get('num_heads', 4))\n    num_layers = int(kwargs.get('num_layers', 1))\n    patch_size = int(kwargs.get('patch_size', 20))                # must divide 1000\n    dropout = float(kwargs.get('dropout', 0.1))\n\n    # Training\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 256))\n    epochs = int(kwargs.get('epochs', 10))\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    class_weighting = kwargs.get('class_weighting', 'balanced')   # 'none' or 'balanced'\n    label_smoothing = float(kwargs.get('label_smoothing', 0.05))\n    grad_clip = float(kwargs.get('grad_clip', 0.5))\n    scheduler_kind = kwargs.get('scheduler', 'cosine')            # 'none' or 'cosine'\n\n    # Quantization\n    quantization_bits = int(kwargs.get('quantization_bits', 8))   # {8,16,32}\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))\n    calibrate_batches = int(kwargs.get('calibrate_batches', 8))   # used if static PTQ were added\n\n    # ------------------ Validations & adjustments ------------------\n    if sequence_length % patch_size != 0:\n        raise ValueError(f\"patch_size must divide sequence length {sequence_length}, got {patch_size}\")\n    if d_model < num_heads:\n        num_heads = max(1, min(num_heads, d_model))\n    if d_model % num_heads != 0:\n        # choose the greatest divisor <= num_heads\n        divisors = [h for h in range(min(num_heads, d_model), 0, -1) if d_model % h == 0]\n        num_heads = divisors[0]\n    assert num_heads >= 1\n\n    # ------------------ Data handling ------------------\n    def _ensure_shape(x):\n        # Convert to (N, C=2, L=1000)\n        if x.dim() != 3:\n            raise ValueError('X tensors must be 3D: (N, 1000, 2) or (N, 2, 1000)')\n        if x.shape[-1] == in_channels:\n            x = x.permute(0, 2, 1).contiguous()\n        elif x.shape[1] == in_channels:\n            pass\n        else:\n            raise ValueError('Last or second dimension must be 2 channels')\n        if x.shape[-1] != sequence_length:\n            raise ValueError(f'Sequence length must be {sequence_length}')\n        return x\n\n    X_train = _ensure_shape(X_train)\n    X_val = _ensure_shape(X_val)\n    if y_train.dtype != torch.long:\n        y_train = y_train.long()\n    if y_val.dtype != torch.long:\n        y_val = y_val.long()\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=False, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=False, drop_last=False)\n\n    # ------------------ Model definition ------------------\n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, in_ch, out_ch, kernel_size, stride=1, padding=None):\n            super().__init__()\n            if padding is None:\n                # Use odd kernels to preserve length\n                padding = kernel_size // 2\n            self.depthwise = nn.Conv1d(in_ch, in_ch, kernel_size, stride=stride, padding=padding, groups=in_ch, bias=False)\n            self.pointwise = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=True)\n            self.act = nn.ReLU(inplace=True)\n        def forward(self, x):\n            x = self.depthwise(x)\n            x = self.pointwise(x)\n            x = self.act(x)\n            return x\n\n    class SEBlock(nn.Module):\n        def __init__(self, channels, reduction=4):\n            super().__init__()\n            hidden = max(1, channels // reduction)\n            self.fc1 = nn.Conv1d(channels, hidden, kernel_size=1)\n            self.fc2 = nn.Conv1d(hidden, channels, kernel_size=1)\n            self.act = nn.ReLU(inplace=True)\n            self.gate = nn.Sigmoid()\n        def forward(self, x):\n            s = x.mean(dim=-1, keepdim=True)\n            s = self.fc2(self.act(self.fc1(s)))\n            s = self.gate(s)\n            return x * s\n\n    class RelativePositionBias(nn.Module):\n        def __init__(self, num_heads, max_positions):\n            super().__init__()\n            self.num_heads = num_heads\n            self.max_positions = max_positions\n            self.bias_table = nn.Parameter(torch.zeros(num_heads, 2 * max_positions - 1))\n            nn.init.trunc_normal_(self.bias_table, std=0.02)\n        def forward(self, T):\n            if T > self.max_positions:\n                raise ValueError(f\"Token length {T} exceeds configured max_positions {self.max_positions}\")\n            # positions [0..T-1]\n            coords = torch.arange(T)\n            rel = coords[None, :] - coords[:, None]  # (T, T)\n            rel = rel + (self.max_positions - 1)     # shift to [0..2*max-2]\n            rel = rel.clamp(0, 2 * self.max_positions - 2)\n            # gather per head\n            bias = self.bias_table[:, rel]           # (H, T, T)\n            return bias\n\n    class MultiHeadSelfAttention(nn.Module):\n        def __init__(self, d_model, num_heads, dropout=0.1, max_tokens=100):\n            super().__init__()\n            self.d_model = d_model\n            self.num_heads = num_heads\n            self.d_head = d_model // num_heads\n            self.qkv = nn.Linear(d_model, 3 * d_model, bias=True)\n            self.proj = nn.Linear(d_model, d_model, bias=True)\n            self.attn_drop = nn.Dropout(dropout)\n            self.proj_drop = nn.Dropout(dropout)\n            self.rel_pos_bias = RelativePositionBias(num_heads, max_positions=max_tokens)\n        def forward(self, x):\n            B, T, C = x.shape\n            qkv = self.qkv(x)  # (B, T, 3C)\n            q, k, v = qkv.chunk(3, dim=-1)\n            # reshape to (B, H, T, Dh)\n            def reshape_heads(t):\n                return t.view(B, T, self.num_heads, self.d_head).permute(0, 2, 1, 3)\n            q = reshape_heads(q)\n            k = reshape_heads(k)\n            v = reshape_heads(v)\n            attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B,H,T,T)\n            bias = self.rel_pos_bias(T).to(attn.device)  # (H,T,T)\n            attn = attn + bias.unsqueeze(0)\n            attn = F.softmax(attn, dim=-1)\n            attn = self.attn_drop(attn)\n            out = torch.matmul(attn, v)  # (B,H,T,Dh)\n            out = out.permute(0, 2, 1, 3).contiguous().view(B, T, C)\n            out = self.proj_drop(self.proj(out))\n            return out\n\n    class TransformerBlock(nn.Module):\n        def __init__(self, d_model, num_heads, mlp_ratio=2.0, dropout=0.1, max_tokens=100):\n            super().__init__()\n            self.norm1 = nn.LayerNorm(d_model)\n            self.attn = MultiHeadSelfAttention(d_model, num_heads, dropout=dropout, max_tokens=max_tokens)\n            self.norm2 = nn.LayerNorm(d_model)\n            hidden = int(d_model * mlp_ratio)\n            self.mlp = nn.Sequential(\n                nn.Linear(d_model, hidden),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden, d_model),\n                nn.Dropout(dropout),\n            )\n        def forward(self, x):\n            x = x + self.attn(self.norm1(x))\n            x = x + self.mlp(self.norm2(x))\n            return x\n\n    class CATNetTiny(nn.Module):\n        def __init__(self, in_ch=2, stem_ch=16, emb_ch=32, d_model=32, num_heads=4, num_layers=1, patch_size=20, num_classes=5, dropout=0.1, seq_len=1000):\n            super().__init__()\n            # CNN stem\n            self.conv1 = DepthwiseSeparableConv1d(in_ch, stem_ch, kernel_size=7, stride=1)\n            self.conv2 = DepthwiseSeparableConv1d(stem_ch, emb_ch, kernel_size=5, stride=1)\n            self.se = SEBlock(emb_ch, reduction=se_reduction)\n            # Patch embedding: conv with stride=patch_size to produce tokens\n            self.patch_size = patch_size\n            self.seq_len = seq_len\n            self.tokens = seq_len // patch_size\n            self.patch_embed = nn.Conv1d(emb_ch, d_model, kernel_size=patch_size, stride=patch_size, bias=True)\n            # Transformer encoder blocks\n            max_tokens = self.tokens  # up to 100 when patch_size >= 10\n            blocks = []\n            for _ in range(num_layers):\n                blocks.append(TransformerBlock(d_model, num_heads, mlp_ratio=2.0, dropout=dropout, max_tokens=max_tokens))\n            self.blocks = nn.Sequential(*blocks)\n            self.norm = nn.LayerNorm(d_model)\n            self.dropout = nn.Dropout(dropout)\n            # Classifier\n            self.head = nn.Linear(d_model, num_classes)\n        def forward(self, x):\n            # x: (B, C=2, L=1000)\n            x = self.conv1(x)\n            x = self.conv2(x)\n            x = self.se(x)\n            # Patch embedding\n            x = self.patch_embed(x)  # (B, d_model, T)\n            x = x.transpose(1, 2)    # (B, T, d_model)\n            # Transformer\n            x = self.blocks(x)\n            x = self.norm(x)\n            x = self.dropout(x)\n            # Global mean pooling over tokens\n            x = x.mean(dim=1)\n            logits = self.head(x)\n            return logits\n\n    model = CATNetTiny(\n        in_ch=in_channels,\n        stem_ch=stem_channels,\n        emb_ch=embed_channels,\n        d_model=d_model,\n        num_heads=num_heads,\n        num_layers=num_layers,\n        patch_size=patch_size,\n        num_classes=num_classes,\n        dropout=dropout,\n        seq_len=sequence_length,\n    ).to(device)\n\n    # Loss with optional class weighting\n    if class_weighting == 'balanced':\n        with torch.no_grad():\n            counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n            inv = torch.where(counts > 0, 1.0 / counts, torch.zeros_like(counts))\n            weights = inv / inv.sum() * num_classes\n        class_weights = weights.to(device)\n    else:\n        class_weights = None\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    if scheduler_kind == 'cosine':\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, epochs))\n    else:\n        scheduler = None\n\n    # ------------------ Training loop ------------------\n    train_losses, val_losses, val_accs, val_macro_f1s = [], [], [], []\n\n    def compute_metrics(logits_list, labels_list):\n        logits = torch.cat(logits_list, dim=0)\n        labels = torch.cat(labels_list, dim=0)\n        preds = logits.argmax(dim=1)\n        acc = (preds == labels).float().mean().item()\n        # Macro F1\n        num_c = num_classes\n        cm = torch.zeros((num_c, num_c), dtype=torch.long)\n        for t, p in zip(labels, preds):\n            cm[t, p] += 1\n        f1s = []\n        for c in range(num_c):\n            tp = cm[c, c].float()\n            fp = cm[:, c].sum().float() - tp\n            fn = cm[c, :].sum().float() - tp\n            denom_p = (tp + fp).clamp(min=1.0)\n            denom_r = (tp + fn).clamp(min=1.0)\n            prec = tp / denom_p\n            rec = tp / denom_r\n            f1 = (2 * prec * rec / (prec + rec).clamp(min=1e-8)).item() if (prec + rec) > 0 else 0.0\n            f1s.append(f1)\n        macro_f1 = float(sum(f1s) / num_c)\n        return acc, macro_f1\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total_batches = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n            optimizer.step()\n            running_loss += loss.detach().item()\n            total_batches += 1\n        train_loss = running_loss / max(1, total_batches)\n\n        model.eval()\n        val_running_loss = 0.0\n        val_batches = 0\n        all_logits, all_labels = [], []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item()\n                val_batches += 1\n                all_logits.append(logits.detach().float().cpu())\n                all_labels.append(yb.detach().cpu())\n        val_loss = val_running_loss / max(1, val_batches)\n        acc, macro_f1 = compute_metrics(all_logits, all_labels)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(acc)\n        val_macro_f1s.append(macro_f1)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={acc:.4f} macro_f1={macro_f1:.4f}\")\n\n    # ------------------ Post-training quantization ------------------\n    model.eval()\n    # Move to CPU for quantization backends\n    model_cpu = model.to('cpu')\n\n    quantized_model = model_cpu\n    if quantize_weights or quantize_activations or quantization_bits in (8, 16):\n        if quantization_bits == 8:\n            # Dynamic quantization for Linear layers (safe for Transformer components)\n            from torch.ao.quantization import quantize_dynamic\n            if quantize_weights:\n                quantized_model = quantize_dynamic(\n                    model_cpu, {nn.Linear}, dtype=torch.qint8\n                )\n            else:\n                quantized_model = model_cpu  # weights unchanged\n            # Note: dynamic quantization does not quantize activations; retained for API compatibility.\n        elif quantization_bits == 16:\n            # Cast to float16 (weight-only style). Keep in eval and on CPU.\n            quantized_model = model_cpu.half()\n        else:\n            quantized_model = model_cpu  # 32-bit: no quantization\n    else:\n        quantized_model = model_cpu\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'val_macro_f1': val_macro_f1s\n    }\n\n    return quantized_model, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 256,
      "type": "Categorical",
      "categories": [
        64,
        128,
        256,
        512
      ]
    },
    "epochs": {
      "default": 10,
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "stem_channels": {
      "default": 16,
      "type": "Integer",
      "low": 16,
      "high": 32
    },
    "embed_channels": {
      "default": 32,
      "type": "Integer",
      "low": 32,
      "high": 48
    },
    "se_reduction": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        2,
        4,
        8
      ]
    },
    "d_model": {
      "default": 32,
      "type": "Integer",
      "low": 16,
      "high": 64
    },
    "num_heads": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        2,
        4
      ]
    },
    "num_layers": {
      "default": 1,
      "type": "Categorical",
      "categories": [
        1,
        2
      ]
    },
    "patch_size": {
      "default": 20,
      "type": "Categorical",
      "categories": [
        10,
        20,
        25,
        40,
        50,
        100,
        125,
        200,
        250,
        500
      ]
    },
    "class_weighting": {
      "default": "balanced",
      "type": "Categorical",
      "categories": [
        "none",
        "balanced"
      ]
    },
    "label_smoothing": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.1
    },
    "grad_clip": {
      "default": 0.5,
      "type": "Real",
      "low": 0.0,
      "high": 1.0
    },
    "scheduler": {
      "default": "cosine",
      "type": "Categorical",
      "categories": [
        "none",
        "cosine"
      ]
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": false,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "calibrate_batches": {
      "default": 8,
      "type": "Integer",
      "low": 1,
      "high": 32
    }
  },
  "confidence": 0.86,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1759168527,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}