{
  "model_name": "MR-CNN + BiGRU (BiMamba-lite) for ISRUC",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    import io\n    import time\n    import copy\n    import math\n    import torch\n    import torch.nn.functional as F\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch import nn, optim\n    \n    # ----- Robust device handling -----\n    device = torch.device(device)\n    if device.type != 'cuda':\n        raise RuntimeError('ALWAYS train on GPU. Please pass device=\"cuda\" or a CUDA device string.')\n    if not torch.cuda.is_available():\n        raise RuntimeError('CUDA is not available but GPU training is required.')\n    \n    # ----- Hyperparameters & Quantization params -----\n    epochs = int(kwargs.get('epochs', 20))\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 32))  # Safe default for 8GB VRAM and 6x6000 input\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    dropout = float(kwargs.get('dropout', 0.1))\n    hidden_size = int(kwargs.get('hidden_size', 12))\n    d_model = int(kwargs.get('d_model', 16))  # channels after fusion\n    t_pooled = int(kwargs.get('t_pooled', 256))  # temporal downsample length for sequence encoder\n    label_smoothing = float(kwargs.get('label_smoothing', 0.05))\n    use_focal_loss = bool(kwargs.get('use_focal_loss', False))\n    focal_gamma = float(kwargs.get('focal_gamma', 2.0))\n    grad_clip_norm = float(kwargs.get('grad_clip_norm', 1.0))\n    num_workers = int(kwargs.get('num_workers', 4))\n    \n    quantization_bits = int(kwargs.get('quantization_bits', 8))  # {8,16,32}\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))  # dynamic quant does not quantize activations\n    \n    # ----- Ensure CPU tensors for DataLoader (pinned-memory path) -----\n    # Large datasets should reside on CPU; DataLoader will pin and transfer to GPU efficiently.\n    if X_train.device.type != 'cpu':\n        X_train = X_train.detach().cpu()\n    if y_train.device.type != 'cpu':\n        y_train = y_train.detach().cpu()\n    if X_val.device.type != 'cpu':\n        X_val = X_val.detach().cpu()\n    if y_val.device.type != 'cpu':\n        y_val = y_val.detach().cpu()\n    \n    # ----- Dataset & DataLoaders with spawn context -----\n    mp_ctx = torch.multiprocessing.get_context('spawn')\n    train_ds = TensorDataset(X_train, y_train.long())\n    val_ds = TensorDataset(X_val, y_val.long())\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        persistent_workers=(num_workers > 0),\n        multiprocessing_context=mp_ctx,\n        drop_last=False,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n        persistent_workers=(num_workers > 0),\n        multiprocessing_context=mp_ctx,\n        drop_last=False,\n    )\n    \n    # ----- Model Definition: Multi-Resolution CNN stem + BiGRU (BiMamba-lite) -----\n    class SqueezeExcite(nn.Module):\n        def __init__(self, channels: int, reduction: int = 4):\n            super().__init__()\n            hidden = max(1, channels // reduction)\n            self.fc1 = nn.Conv1d(channels, hidden, kernel_size=1)\n            self.fc2 = nn.Conv1d(hidden, channels, kernel_size=1)\n            self.act = nn.ReLU(inplace=True)\n            self.gate = nn.Sigmoid()\n        def forward(self, x):\n            # x: (B, C, T)\n            s = x.mean(dim=-1, keepdim=True)\n            s = self.fc2(self.act(self.fc1(s)))\n            g = self.gate(s)\n            return x * g\n    \n    class DWConvPW(nn.Module):\n        def __init__(self, in_ch: int, out_ch: int, k: int, stride: int):\n            super().__init__()\n            pad = k // 2\n            self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=stride, padding=pad, groups=in_ch, bias=False)\n            self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.ReLU(inplace=True)\n        def forward(self, x):  # (B, C_in, T)\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.bn(x)\n            x = self.act(x)\n            return x\n    \n    class MRStem(nn.Module):\n        def __init__(self, in_ch=6, branch_out=8, t_out=256):\n            super().__init__()\n            # Three resolution branches capturing delta-theta/alpha/beta bands roughly\n            self.b1 = nn.Sequential(\n                DWConvPW(in_ch, branch_out, k=51, stride=2),  # ~0.5x\n                SqueezeExcite(branch_out),\n            )\n            self.b2 = nn.Sequential(\n                DWConvPW(in_ch, branch_out, k=129, stride=4),  # ~0.25x\n                SqueezeExcite(branch_out),\n            )\n            self.b3 = nn.Sequential(\n                DWConvPW(in_ch, branch_out, k=401, stride=8),  # ~0.125x\n                SqueezeExcite(branch_out),\n            )\n            self.pool = nn.AdaptiveAvgPool1d(t_out)\n            self.out_ch = branch_out * 3\n        def forward(self, x):\n            # x: (B, 6, 6000)\n            x1 = self.pool(self.b1(x))\n            x2 = self.pool(self.b2(x))\n            x3 = self.pool(self.b3(x))\n            return torch.cat([x1, x2, x3], dim=1)  # (B, 3*branch_out, t_out)\n    \n    class ChannelFusion(nn.Module):\n        def __init__(self, in_ch, out_ch):\n            super().__init__()\n            self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=5, padding=2, groups=in_ch, bias=False)\n            self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.se = SqueezeExcite(out_ch)\n            self.act = nn.ReLU(inplace=True)\n        def forward(self, x):\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.bn(x)\n            x = self.se(x)\n            x = self.act(x)\n            return x\n    \n    class BiMambaLiteEncoder(nn.Module):\n        # Bi-directional GRU used as a lightweight stand-in for a bidirectional state-space encoder\n        def __init__(self, d_model, hidden):\n            super().__init__()\n            self.gru = nn.GRU(input_size=d_model, hidden_size=hidden, num_layers=1, batch_first=True, bidirectional=True)\n        def forward(self, x):  # x: (B, C, T)\n            x = x.transpose(1, 2)  # (B, T, C)\n            out, _ = self.gru(x)   # (B, T, 2H)\n            feat = out.mean(dim=1) # (B, 2H)\n            return feat\n    \n    class SleepNet(nn.Module):\n        def __init__(self, d_model=16, hidden=12, dropout=0.1, t_out=256):\n            super().__init__()\n            self.stem = MRStem(in_ch=6, branch_out=8, t_out=t_out)\n            self.fuse = ChannelFusion(self.stem.out_ch, d_model)\n            self.enc = BiMambaLiteEncoder(d_model=d_model, hidden=hidden)\n            self.drop = nn.Dropout(dropout)\n            self.fc = nn.Linear(2 * hidden, 5)\n        def forward(self, x):  # x: (B, 6, 6000)\n            # Per-epoch per-channel z-score\n            mean = x.mean(dim=-1, keepdim=True)\n            std = x.std(dim=-1, keepdim=True).clamp_min(1e-5)\n            x = (x - mean) / std\n            x = self.stem(x)\n            x = self.fuse(x)\n            x = self.enc(x)\n            x = self.drop(x)\n            logits = self.fc(x)\n            return logits\n    \n    model = SleepNet(d_model=d_model, hidden=hidden_size, dropout=dropout, t_out=t_pooled).to(device)\n    \n    # ----- Optimizer -----\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    \n    # ----- Loss -----\n    def compute_loss(logits, targets):\n        if use_focal_loss:\n            log_probs = F.log_softmax(logits, dim=1)\n            probs = log_probs.exp()\n            num_classes = logits.size(1)\n            with torch.no_grad():\n                true_dist = torch.zeros_like(log_probs)\n                true_dist.fill_(label_smoothing / (num_classes - 1))\n                true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - label_smoothing)\n            ce = -(true_dist * log_probs).sum(dim=1)\n            p_t = probs.gather(1, targets.unsqueeze(1)).squeeze(1).clamp_(1e-6, 1.0 - 1e-6)\n            focal = (1.0 - p_t) ** focal_gamma\n            loss = (focal * ce).mean()\n            return loss\n        else:\n            return nn.CrossEntropyLoss(label_smoothing=label_smoothing)(logits, targets)\n    \n    # ----- Training/Validation Loop -----\n    train_losses, val_losses, val_accs = [], [], []\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss = 0.0\n        n_train = 0\n        t0 = time.time()\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = compute_loss(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n            batch_size_curr = xb.size(0)\n            epoch_loss += loss.detach().item() * batch_size_curr\n            n_train += batch_size_curr\n        train_loss = epoch_loss / max(1, n_train)\n        \n        # Validation\n        model.eval()\n        val_loss_accum = 0.0\n        correct = 0\n        n_val = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=True)\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = compute_loss(logits, yb)\n                val_loss_accum += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                n_val += xb.size(0)\n        val_loss = val_loss_accum / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        \n        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | time={(time.time()-t0):.1f}s\")\n    \n    # ----- Post-Training Quantization -----\n    # Note: dynamic quantization targets Linear/GRU layers (weights); activations remain float.\n    model_cpu = copy.deepcopy(model).to('cpu').eval()\n    size_bytes = None\n    try:\n        if quantize_weights and quantization_bits == 8:\n            # Dynamic weight-only int8 quantization for Linear and GRU\n            qmodel = torch.ao.quantization.quantize_dynamic(\n                model_cpu,\n                {nn.Linear, nn.GRU},\n                dtype=torch.qint8,\n            )\n        elif quantize_weights and quantization_bits == 16:\n            # Weight-only float16 conversion\n            qmodel = copy.deepcopy(model_cpu).half()\n        else:\n            # Keep as float32\n            qmodel = model_cpu.float()\n        buf = io.BytesIO()\n        torch.save(qmodel.state_dict(), buf)\n        size_bytes = buf.getbuffer().nbytes\n        if size_bytes > 256 * 1024:\n            print(f\"Warning: Quantized model size {size_bytes} bytes exceeds 256KB limit.\")\n        else:\n            print(f\"Quantized model size: {size_bytes} bytes.\")\n    except Exception as e:\n        print(f\"Quantization failed with error: {e}. Returning float32 CPU model.\")\n        qmodel = model_cpu.float()\n        buf = io.BytesIO()\n        torch.save(qmodel.state_dict(), buf)\n        size_bytes = buf.getbuffer().nbytes\n    \n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'model_size_bytes': size_bytes,\n    }\n    return qmodel, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.005,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 32,
      "type": "Categorical",
      "categories": [
        8,
        16,
        24,
        32
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 100
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "hidden_size": {
      "default": 12,
      "type": "Integer",
      "low": 8,
      "high": 32
    },
    "d_model": {
      "default": 16,
      "type": "Integer",
      "low": 8,
      "high": 32
    },
    "t_pooled": {
      "default": 256,
      "type": "Categorical",
      "categories": [
        128,
        192,
        256,
        320,
        384
      ]
    },
    "label_smoothing": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "use_focal_loss": {
      "default": false,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 0.0,
      "high": 5.0
    },
    "grad_clip_norm": {
      "default": 1.0,
      "type": "Real",
      "low": 0.0,
      "high": 5.0
    },
    "num_workers": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        4
      ]
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": false,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "confidence": 0.86,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      89283,
      6,
      6000
    ],
    "dtype": "float32",
    "feature_count": 6000,
    "sample_count": 89283,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1760345368,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}