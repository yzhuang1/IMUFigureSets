{
  "model_name": "TinyECGPatchTransformer",
  "training_code": "import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# ---- Utilities ----\ndef _ensure_channel_first(x):\n    # Expect (N, 2, T) or (N, T, 2); return (N, 2, T)\n    if x.dim() != 3:\n        raise ValueError(f\"Expected 3D tensor (N, C/T, T/C), got shape {tuple(x.shape)}\")\n    if x.shape[1] == 2:\n        return x\n    elif x.shape[2] == 2:\n        return x.permute(0, 2, 1).contiguous()\n    else:\n        raise ValueError(\"Input must have 2 ECG channels; got shape {}\".format(tuple(x.shape)))\n\n@torch.no_grad()\ndef _compute_metrics(logits_list, labels_list, num_classes=5):\n    y_pred = torch.cat(logits_list, dim=0).argmax(dim=1).cpu()\n    y_true = torch.cat(labels_list, dim=0).cpu()\n    cm = torch.zeros(num_classes, num_classes, dtype=torch.long)\n    idx = y_true * num_classes + y_pred\n    bins = torch.bincount(idx, minlength=num_classes * num_classes)\n    cm[:] = bins.view(num_classes, num_classes)\n    tp = cm.diag().to(torch.float32)\n    fp = cm.sum(dim=0).to(torch.float32) - tp\n    fn = cm.sum(dim=1).to(torch.float32) - tp\n    eps = 1e-8\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    macro_f1 = f1.mean().item()\n    acc = (tp.sum() / cm.sum().clamp_min(1)).item()\n    return {\"accuracy\": acc, \"macro_f1\": macro_f1}\n\ndef _sinusoidal_positional_encoding(L, d_model, device):\n    pe = torch.zeros(L, d_model, device=device)\n    position = torch.arange(0, L, dtype=torch.float32, device=device).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0) / d_model))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    return pe  # (L, d)\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5, weight=None, reduction='mean'):\n        super().__init__()\n        self.gamma = gamma\n        self.weight = weight\n        self.reduction = reduction\n    def forward(self, logits, targets):\n        ce = F.cross_entropy(logits, targets, weight=self.weight, reduction='none')\n        pt = torch.exp(-ce)\n        loss = ((1 - pt) ** self.gamma) * ce\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss\n\n# ---- Model ----\nclass TinyECGPatchTransformer(nn.Module):\n    def __init__(self, in_ch=2, num_classes=5, patch_len=10, d_model=96, n_heads=4, n_layers=2, dropout=0.2, ff_mult=4):\n        super().__init__()\n        self.patch_len = patch_len\n        self.embed = nn.Conv1d(in_ch, d_model, kernel_size=patch_len, stride=patch_len, bias=True)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=ff_mult * d_model,\n            dropout=dropout,\n            batch_first=True,\n            norm_first=True,\n            activation='gelu'\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):  # x: (B, C=2, T)\n        # Patch embedding\n        tok = self.embed(x)  # (B, d_model, L)\n        tok = tok.transpose(1, 2)  # (B, L, d_model)\n        B, L, D = tok.shape\n        # Add sinusoidal positional encoding\n        pe = _sinusoidal_positional_encoding(L, D, tok.device)\n        tok = tok + pe.unsqueeze(0)\n        # Transformer encoder\n        tok = self.encoder(tok)  # (B, L, D)\n        tok = self.norm(tok)\n        pooled = tok.mean(dim=1)  # mean pooling\n        out = self.head(self.dropout(pooled))\n        return out\n\n# ---- Training function ----\ndef train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    \"\"\"\n    Train a lightweight patch-based Transformer for 5-class ECG beat classification.\n\n    Args:\n        X_train, y_train, X_val, y_val: PyTorch tensors. X tensors expected shape (N, 1000, 2) or (N, 2, 1000). y long dtype in [0..4].\n        device: torch.device('cpu' or 'cuda').\n        **hyperparams: lr, batch_size, epochs, hidden_size, n_heads, n_layers, dropout, patch_len, weight_decay, use_focal, focal_gamma, grad_clip, num_workers, seed.\n\n    Returns:\n        model (nn.Module) on `device`, metrics (dict).\n    \"\"\"\n    # Defaults\n    hp = {\n        'lr': 3e-4,\n        'batch_size': 128,\n        'epochs': 15,\n        'hidden_size': 96,\n        'n_heads': 4,\n        'n_layers': 2,\n        'dropout': 0.2,\n        'patch_len': 10,\n        'weight_decay': 1e-4,\n        'use_focal': True,\n        'focal_gamma': 1.5,\n        'grad_clip': 1.0,\n        'num_workers': 0,\n        'seed': 42\n    }\n    hp.update(hyperparams or {})\n\n    torch.manual_seed(hp['seed'])\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(hp['seed'])\n        torch.backends.cudnn.benchmark = True\n\n    # Ensure correct shapes (N, C=2, T)\n    X_train = _ensure_channel_first(X_train)\n    X_val = _ensure_channel_first(X_val)\n\n    num_classes = 5\n\n    # Class weights to mitigate imbalance\n    with torch.no_grad():\n        y_cpu = y_train.detach().view(-1).to('cpu')\n        counts = torch.bincount(y_cpu, minlength=num_classes).float()\n        total = counts.sum().clamp_min(1.0)\n        class_weights = (total / (num_classes * counts.clamp_min(1.0)))  # inverse frequency\n        class_weights = class_weights / class_weights.mean()  # normalize for stability\n\n    # DataLoaders with correct pin_memory policy\n    train_pin = (X_train.device.type == 'cpu')\n    val_pin = (X_val.device.type == 'cpu')\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=hp['batch_size'],\n        shuffle=True,\n        num_workers=hp['num_workers'],\n        pin_memory=train_pin,\n        drop_last=False\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=hp['batch_size'],\n        shuffle=False,\n        num_workers=hp['num_workers'],\n        pin_memory=val_pin,\n        drop_last=False\n    )\n\n    # Build model\n    model = TinyECGPatchTransformer(\n        in_ch=2,\n        num_classes=num_classes,\n        patch_len=hp['patch_len'],\n        d_model=hp['hidden_size'],\n        n_heads=hp['n_heads'],\n        n_layers=hp['n_layers'],\n        dropout=hp['dropout'],\n        ff_mult=4,\n    ).to(device)\n\n    # Parameter count check (<256k)\n    param_count = sum(p.numel() for p in model.parameters())\n    if param_count > 256_000:\n        raise ValueError(f\"Model has {param_count} parameters, exceeds 256k limit. Reduce hidden_size/layers/heads.\")\n\n    # Loss & optimizer\n    cw = class_weights.to(device)\n    if hp['use_focal']:\n        criterion = FocalLoss(gamma=hp['focal_gamma'], weight=cw, reduction='mean')\n    else:\n        criterion = nn.CrossEntropyLoss(weight=cw)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=hp['lr'], weight_decay=hp['weight_decay'])\n\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": [], \"val_macro_f1\": []}\n    best_state = None\n    best_f1 = -1.0\n    best_epoch = -1\n\n    for epoch in range(1, hp['epochs'] + 1):\n        model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if hp['grad_clip'] and hp['grad_clip'] > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), hp['grad_clip'])\n            optimizer.step()\n            running_loss += loss.item() * xb.size(0)\n        epoch_train_loss = running_loss / len(train_ds)\n        history['train_loss'].append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        all_logits = []\n        all_labels = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=True)\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.item() * xb.size(0)\n                all_logits.append(logits.detach())\n                all_labels.append(yb.detach())\n        val_loss = val_running_loss / len(val_ds)\n        metrics = _compute_metrics(all_logits, all_labels, num_classes=num_classes)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(metrics['accuracy'])\n        history['val_macro_f1'].append(metrics['macro_f1'])\n\n        # Track best by macro-F1\n        if metrics['macro_f1'] > best_f1:\n            best_f1 = metrics['macro_f1']\n            best_epoch = epoch\n            best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    out_metrics = {\n        'best_val_macro_f1': best_f1,\n        'best_epoch': best_epoch,\n        'val_history': {\n            'loss': history['val_loss'],\n            'acc': history['val_acc'],\n            'macro_f1': history['val_macro_f1']\n        },\n        'train_history': {\n            'loss': history['train_loss']\n        },\n        'param_count': int(param_count)\n    }\n\n    return model, out_metrics\n",
  "hyperparameters": {
    "lr": 0.0003,
    "epochs": 15,
    "batch_size": 128
  },
  "reasoning": "We follow recent findings that local-window or compact Transformer encoders over short patches can efficiently capture ECG morphology and rhythm with far fewer parameters. We patch 1000x2 signals with a stride-equals-kernel Conv1d to form tokens, add sinusoidal positions, and use 2 lightweight Transformer layers (d_model≈96, 4 heads) with mean pooling. To address the severe class imbalance typical in MIT-BIH 5-class (especially S/V/F/Q), we include class-weighted loss and an optional focal term (gamma≈1.5), which helps minority classes and improves macro-F1. We prioritize macro-F1 reporting, reflecting the inter-patient evaluation emphasis in the literature. The model remains under 256k parameters, simple to train, and suitable for Bayesian hyperparameter tuning.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "batch_size",
    "epochs",
    "hidden_size",
    "dropout"
  ],
  "bo_search_space": {
    "lr": {
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "type": "Categorical",
      "categories": [
        8,
        16,
        32,
        64,
        128
      ]
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "hidden_size": {
      "type": "Integer",
      "low": 32,
      "high": 256
    },
    "dropout": {
      "type": "Real",
      "low": 0.0,
      "high": 0.7
    }
  },
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757986629,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}