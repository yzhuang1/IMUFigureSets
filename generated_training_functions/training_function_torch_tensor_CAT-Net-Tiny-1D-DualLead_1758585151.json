{
  "model_name": "CAT-Net-Tiny-1D-DualLead",
  "training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Optimizer & schedule\n    lr=1e-3,\n    batch_size=64,\n    epochs=20,\n    weight_decay=1e-4,\n    scheduler='none',\n    step_size=10,\n    gamma=0.5,\n    # Model size & architecture\n    base_channels=16,\n    cnn_depth=3,\n    kernel_size=5,\n    pool_stride=4,\n    dropout=0.1,\n    d_model=32,\n    nhead=2,\n    num_transformer_layers=1,\n    mlp_ratio=2,\n    # Loss & regularization\n    focal_gamma=2.0,\n    focal_alpha=0.25,\n    label_smoothing=0.0,\n    grad_clip=1.0,\n    augment_noise_std=0.001,\n    # Quantization options\n    quantization_bits=8,\n    quantize_weights=True,\n    quantize_activations=True,\n    # Misc\n    seed=42,\n):\n    import math\n    import io\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n    from torch.optim import AdamW\n    from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n    from torch.ao.quantization import quantize_dynamic\n\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    num_classes = 5\n\n    # Ensure tensor dtypes\n    X_train = X_train.float()\n    X_val = X_val.float()\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    # Shape normalization helper: expected (B, C=2, T=1000)\n    def ensure_ch_first(x):\n        if x.dim() != 3:\n            raise ValueError(f'Expected 3D tensor (N, T, C) or (N, C, T), got {list(x.shape)}')\n        # If last dim is 2, assume (N, T, C)\n        if x.shape[-1] == 2:\n            return x.permute(0, 2, 1).contiguous()\n        elif x.shape[1] == 2:\n            return x\n        else:\n            raise ValueError('Input must have 2 channels (leads) either as last or second dimension')\n\n    # Dataset & Sampler with class balancing\n    with torch.no_grad():\n        # Compute class weights and sample weights\n        classes, counts = torch.unique(y_train, return_counts=True)\n        freq = torch.zeros(num_classes, dtype=torch.float)\n        freq[classes] = counts.float()\n        freq = torch.clamp(freq, min=1.0)\n        class_weights = (1.0 / freq)\n        class_weights = class_weights * (num_classes / class_weights.sum())\n        sample_weights = class_weights[y_train]\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_sampler = WeightedRandomSampler(weights=sample_weights.double(), num_samples=len(sample_weights), replacement=True)\n\n    # IMPORTANT: pin_memory=False per requirement\n    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=train_sampler, shuffle=False, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Model components\n    class SEChannelAttention(nn.Module):\n        def __init__(self, channels, reduction=8):\n            super().__init__()\n            hidden = max(1, channels // reduction)\n            self.avg = nn.AdaptiveAvgPool1d(1)\n            self.fc1 = nn.Linear(channels, hidden, bias=True)\n            self.fc2 = nn.Linear(hidden, channels, bias=True)\n            self.act = nn.SiLU()\n            self.gate = nn.Sigmoid()\n        def forward(self, x):\n            # x: (B, C, T)\n            b, c, _ = x.shape\n            s = self.avg(x).view(b, c)\n            s = self.fc2(self.act(self.fc1(s)))\n            s = self.gate(s).view(b, c, 1)\n            return x * s\n\n    class ResidualBlock1D(nn.Module):\n        def __init__(self, c_in, c_out, k=5, se_reduction=8):\n            super().__init__()\n            p = k // 2\n            self.conv1 = nn.Conv1d(c_in, c_out, k, padding=p, bias=False)\n            self.bn1 = nn.BatchNorm1d(c_out)\n            self.conv2 = nn.Conv1d(c_out, c_out, k, padding=p, bias=False)\n            self.bn2 = nn.BatchNorm1d(c_out)\n            self.act = nn.SiLU()\n            self.se = SEChannelAttention(c_out, reduction=se_reduction)\n            self.down = None\n            if c_in != c_out:\n                self.down = nn.Conv1d(c_in, c_out, kernel_size=1, bias=False)\n        def forward(self, x):\n            identity = x\n            out = self.act(self.bn1(self.conv1(x)))\n            out = self.bn2(self.conv2(out))\n            out = self.se(out)\n            if self.down is not None:\n                identity = self.down(identity)\n            out = self.act(out + identity)\n            return out\n\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model, max_len=2048):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            self.register_buffer('pe', pe.unsqueeze(0), persistent=False)  # (1, max_len, d_model)\n        def forward(self, x):\n            # x: (B, T, D)\n            t = x.size(1)\n            return x + self.pe[:, :t, :]\n\n    class CATNetTiny(nn.Module):\n        def __init__(self, base_channels=16, depth=3, k=5, pool_stride=4, d_model=32, nhead=2, num_layers=1, mlp_ratio=2, dropout=0.1, num_classes=5):\n            super().__init__()\n            c1 = base_channels\n            c2 = max(base_channels, min(48, (3 * base_channels) // 2))\n            c3 = min(64, base_channels * 2)\n\n            # Stem + pooling to shrink sequence length quickly\n            p = k // 2\n            self.stem = nn.Sequential(\n                nn.Conv1d(2, c1, kernel_size=k, padding=p, bias=False),\n                nn.BatchNorm1d(c1),\n                nn.SiLU(),\n                nn.MaxPool1d(kernel_size=pool_stride, stride=pool_stride)\n            )\n\n            blocks = []\n            in_c = c1\n            for i in range(depth):\n                out_c = [c1, c2, c3][min(i, 2)]\n                blocks.append(ResidualBlock1D(in_c, out_c, k=k))\n                blocks.append(nn.MaxPool1d(kernel_size=pool_stride, stride=pool_stride))\n                in_c = out_c\n            self.cnn = nn.Sequential(*blocks)\n\n            # Project to transformer dimension\n            self.proj = nn.Conv1d(in_c, d_model, kernel_size=1, bias=True)\n\n            # Lightweight Transformer encoder\n            nhead = max(1, min(nhead, d_model))\n            # ensure divisibility\n            if d_model % nhead != 0:\n                # choose the largest head count that divides d_model\n                for h in reversed(range(1, nhead + 1)):\n                    if d_model % h == 0:\n                        nhead = h\n                        break\n            enc_layer = nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=nhead,\n                dim_feedforward=max(4, int(d_model * mlp_ratio)),\n                dropout=dropout,\n                activation='gelu',\n                batch_first=True,\n                norm_first=True,\n            )\n            self.transformer = (\n                nn.Identity() if num_layers <= 0 else nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n            )\n            self.pos = PositionalEncoding(d_model)\n            self.drop = nn.Dropout(dropout)\n\n            # Classification head\n            self.head = nn.Linear(d_model, num_classes)\n\n        def forward(self, x):\n            # x: (B, 2, 1000) or (B, 1000, 2)\n            if x.shape[-1] == 2:\n                x = x.permute(0, 2, 1).contiguous()\n            # CNN + channel attention pyramid\n            x = self.stem(x)\n            x = self.cnn(x)\n            x = self.proj(x)  # (B, D, T')\n            x = x.transpose(1, 2)  # (B, T', D)\n            x = self.pos(x)\n            x = self.transformer(x)\n            x = x.mean(dim=1)\n            x = self.drop(x)\n            logits = self.head(x)\n            return logits\n\n    # Enforce model size budget: ensure FP32 params < 256KB (very conservative)\n    # If not, shrink d_model/base_channels automatically.\n    def estimate_bytes_fp32(m):\n        return sum(p.numel() for p in m.parameters()) * 4\n\n    def build_model_with_budget():\n        nonlocal base_channels, d_model\n        # Clamp hyperparams to safe bounds\n        base_channels = int(max(8, min(32, base_channels)))\n        d_model = int(max(16, min(32, d_model)))\n        model = CATNetTiny(\n            base_channels=base_channels,\n            depth=int(max(2, min(3, cnn_depth))),\n            k=int(max(3, min(9, kernel_size))),\n            pool_stride=int(max(2, min(5, pool_stride))),\n            d_model=d_model,\n            nhead=int(max(1, min(4, nhead))),\n            num_layers=int(max(0, min(2, num_transformer_layers))),\n            mlp_ratio=int(max(2, min(4, mlp_ratio))),\n            dropout=float(max(0.0, min(0.5, dropout))),\n            num_classes=num_classes,\n        )\n        # If over budget in FP32, iteratively shrink\n        budget = 256 * 1024\n        while estimate_bytes_fp32(model) > budget and (d_model > 16 or base_channels > 8):\n            if d_model > 16:\n                d_model = max(16, d_model // 2)\n            elif base_channels > 8:\n                base_channels = max(8, base_channels // 2)\n            model = CATNetTiny(\n                base_channels=base_channels,\n                depth=int(max(2, min(3, cnn_depth))),\n                k=int(max(3, min(9, kernel_size))),\n                pool_stride=int(max(2, min(5, pool_stride))),\n                d_model=d_model,\n                nhead=int(max(1, min(4, nhead))),\n                num_layers=int(max(0, min(2, num_transformer_layers))),\n                mlp_ratio=int(max(2, min(4, mlp_ratio))),\n                dropout=float(max(0.0, min(0.5, dropout))),\n                num_classes=num_classes,\n            )\n        return model\n\n    model = build_model_with_budget().to(device)\n\n    # Log parameter count\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f'Model params: {total_params} (~{total_params*4/1024:.1f} KB in FP32)')\n\n    # Focal loss with class weights\n    class_weights = class_weights.to(device)\n    def focal_loss(logits, targets):\n        # Cross-entropy per-sample\n        ce = F.cross_entropy(logits, targets, weight=class_weights, reduction='none', label_smoothing=float(label_smoothing))\n        if focal_gamma <= 0:\n            return ce.mean()\n        with torch.no_grad():\n            pt = F.softmax(logits, dim=1).gather(1, targets.view(-1, 1)).squeeze(1)\n        alpha = float(max(0.0, min(1.0, focal_alpha))) if focal_alpha is not None else 1.0\n        mod = (1.0 - pt).clamp(min=1e-6) ** float(focal_gamma)\n        loss = alpha * mod * ce\n        return loss.mean()\n\n    # Optimizer & scheduler\n    opt = AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    if scheduler == 'step':\n        sched = StepLR(opt, step_size=int(max(1, step_size)), gamma=float(gamma))\n    elif scheduler == 'cosine':\n        sched = CosineAnnealingLR(opt, T_max=int(max(1, epochs)))\n    else:\n        sched = None\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    def run_eval():\n        model.eval()\n        total, correct = 0, 0\n        loss_sum = 0.0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = ensure_ch_first(xb).to(device)\n                yb = yb.to(device)\n                logits = model(xb)\n                loss = F.cross_entropy(logits, yb, weight=class_weights, reduction='mean', label_smoothing=float(label_smoothing))\n                loss_sum += float(loss.item()) * yb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += yb.size(0)\n        return loss_sum / max(1, total), (correct / max(1, total))\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            # Augment: small Gaussian noise on input (train only)\n            if augment_noise_std and augment_noise_std > 0:\n                noise = torch.randn_like(xb) * float(augment_noise_std)\n                xb = xb + noise\n            xb = ensure_ch_first(xb).to(device)\n            yb = yb.to(device)\n\n            opt.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = focal_loss(logits, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip))\n            opt.step()\n            running += float(loss.item()) * yb.size(0)\n            seen += yb.size(0)\n        if sched is not None:\n            sched.step()\n\n        train_loss = running / max(1, seen)\n        val_loss, val_acc = run_eval()\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        print(f'Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}')\n\n    # Post-training quantization (dynamic). Quantized model is returned on CPU.\n    model.eval()\n    model_cpu = copy.deepcopy(model).to('cpu')\n\n    # Decide quantization approach\n    def apply_quantization(m):\n        # If user explicitly disables both, return as-is\n        if (not quantize_weights) and (not quantize_activations):\n            return m\n        bits = int(quantization_bits)\n        if bits >= 32:\n            return m  # no quantization\n        # Dynamic quantization on Linear layers only. This reduces model size and keeps ops portable.\n        if bits == 8:\n            qdtype = torch.qint8\n        elif bits == 16:\n            qdtype = torch.float16\n        else:\n            qdtype = torch.qint8\n        # Note: dynamic quantization mainly affects weights (and activation path for int8). We map both toggles to dynamic quantization behavior.\n        qm = quantize_dynamic(m, {nn.Linear}, dtype=qdtype)\n        return qm\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Estimate and enforce final storage size <= 256KB via state_dict serialization\n    try:\n        buf = io.BytesIO()\n        torch.save(quantized_model.state_dict(), buf)\n        q_size = len(buf.getvalue())\n    except Exception:\n        # Fallback: conservative estimate using FP32 params if state_dict not available\n        q_size = sum(p.numel() for p in model_cpu.parameters()) * 4\n    print(f'Quantized model state_dict size: {q_size/1024:.1f} KB')\n    if q_size > 256 * 1024:\n        print('WARNING: Quantized model exceeds 256KB. Consider reducing base_channels/d_model/num_layers.')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'param_count': total_params,\n        'fp32_param_kb': total_params * 4 / 1024.0,\n        'quantized_state_dict_bytes': int(q_size),\n    }\n\n    return quantized_model, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 64,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 100
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-07,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "scheduler": {
      "default": "none",
      "type": "Categorical",
      "categories": [
        "none",
        "step",
        "cosine"
      ]
    },
    "step_size": {
      "default": 10,
      "type": "Integer",
      "low": 2,
      "high": 30
    },
    "gamma": {
      "default": 0.5,
      "type": "Real",
      "low": 0.1,
      "high": 0.99
    },
    "base_channels": {
      "default": 16,
      "type": "Integer",
      "low": 8,
      "high": 32
    },
    "cnn_depth": {
      "default": 3,
      "type": "Integer",
      "low": 2,
      "high": 3
    },
    "kernel_size": {
      "default": 5,
      "type": "Integer",
      "low": 3,
      "high": 9
    },
    "pool_stride": {
      "default": 4,
      "type": "Integer",
      "low": 2,
      "high": 5
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "d_model": {
      "default": 32,
      "type": "Integer",
      "low": 16,
      "high": 32
    },
    "nhead": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 4
    },
    "num_transformer_layers": {
      "default": 1,
      "type": "Integer",
      "low": 0,
      "high": 2
    },
    "mlp_ratio": {
      "default": 2,
      "type": "Integer",
      "low": 2,
      "high": 4
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 0.0,
      "high": 5.0
    },
    "focal_alpha": {
      "default": 0.25,
      "type": "Real",
      "low": 0.0,
      "high": 1.0
    },
    "label_smoothing": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 0.1
    },
    "grad_clip": {
      "default": 1.0,
      "type": "Real",
      "low": 0.0,
      "high": 5.0
    },
    "augment_noise_std": {
      "default": 0.001,
      "type": "Real",
      "low": 0.0,
      "high": 0.01
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "seed": {
      "default": 42,
      "type": "Integer",
      "low": 0,
      "high": 1000000
    }
  },
  "confidence": 0.86,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758585151,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}