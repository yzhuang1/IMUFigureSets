{
  "model_name": "TinyECG1D-CNN-Quantizable",
  "training_code": "import copy\\nimport math\\nfrom typing import Dict, Any\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\\n\\n\\nclass TinyECGCNN(nn.Module):\\n    def __init__(self, in_channels: int = 2, num_classes: int = 5, base_channels: int = 16, dropout: float = 0.1):\\n        super().__init__()\\n        c1 = base_channels\\n        c2 = base_channels * 2\\n        c3 = base_channels * 4\\n        # Features are laid out as Conv-ReLU-MaxPool blocks to enable module fusion for quantization\\n        self.features = nn.Sequential(\\n            nn.Conv1d(in_channels, c1, kernel_size=7, stride=2, padding=3, bias=True),\\n            nn.ReLU(inplace=True),\\n            nn.MaxPool1d(kernel_size=2, stride=2),\\n            nn.Conv1d(c1, c2, kernel_size=5, stride=1, padding=2, bias=True),\\n            nn.ReLU(inplace=True),\\n            nn.MaxPool1d(kernel_size=2, stride=2),\\n            nn.Conv1d(c2, c3, kernel_size=5, stride=1, padding=2, bias=True),\\n            nn.ReLU(inplace=True),\\n            nn.AdaptiveAvgPool1d(1),\\n        )\\n        self.classifier = nn.Sequential(\\n            nn.Dropout(p=dropout),\\n            nn.Flatten(),\\n            nn.Linear(c3, num_classes, bias=True),\\n        )\\n\\n    def forward(self, x):\\n        x = self.features(x)\\n        x = self.classifier(x)\\n        return x\\n\\n\\ndef _ensure_ch_first(x: torch.Tensor) -> torch.Tensor:\\n    # Ensure input is (N, C, L) for Conv1d\\n    if x.ndim == 3:\\n        # If (N, L, C) -> permute to (N, C, L)\\n        if x.shape[-1] <= 8 and x.shape[1] >= 8:  # likely (N, L, C) with small C (e.g., 2)\\n            x = x.permute(0, 2, 1)\\n        # If already (N, C, L), keep as-is\\n    elif x.ndim == 2:\\n        # Single sample (L, C) -> (1, C, L)\\n        if x.shape[-1] <= 8:\\n            x = x.permute(1, 0).unsqueeze(0)\\n        else:\\n            x = x.unsqueeze(0).unsqueeze(0)\\n    else:\\n        raise ValueError(f\"Unsupported input shape: {x.shape}\")\\n    return x.contiguous()\\n\\n\\ndef _count_parameters(model: nn.Module) -> int:\\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\\n\\ndef _accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:\\n    preds = torch.argmax(logits, dim=1)\\n    return (preds == targets).float().mean().item()\\n\\n\\ndef train_model(\\n    X_train: torch.Tensor,\\n    y_train: torch.Tensor,\\n    X_val: torch.Tensor,\\n    y_val: torch.Tensor,\\n    device: torch.device,\\n    **kwargs: Any,\\n):\\n    \\\"\\\"\\\"\\n    Train a compact 1D-CNN for 5-class classification on inputs shaped (1000, 2) per sample.\\n\\n    Inputs must be torch tensors. This function builds the model, trains it, logs epoch metrics,\\n    and returns a post-training-quantized model along with metrics.\\n\\n    Quantization notes:\\n    - 8-bit with activations: static quantization (prepare -> calibrate -> convert, CPU-only).\\n    - 8-bit weights-only: dynamic quantization for Linear layers.\\n    - 16-bit: dynamic quantization to float16 for Linear weights (CPU/GPU-safe); activations stay float32.\\n    - 32-bit: no quantization.\\n\\n    DataLoader pin_memory is set to False as requested.\\n    \\\"\\\"\\\"\\n    # -----------------------------\\n    # Hyperparameters (with defaults)\\n    # -----------------------------\\n    hp: Dict[str, Any] = {\\n        'lr': kwargs.get('lr', 1e-3),\\n        'batch_size': kwargs.get('batch_size', 256),\\n        'epochs': kwargs.get('epochs', 15),\\n        'weight_decay': kwargs.get('weight_decay', 1e-4),\\n        'dropout': kwargs.get('dropout', 0.1),\\n        'base_channels': kwargs.get('base_channels', 16),\\n        'clip_grad_norm': kwargs.get('clip_grad_norm', 1.0),\\n        'step_size': kwargs.get('step_size', 10),\\n        'gamma': kwargs.get('gamma', 0.9),\\n        'use_amp': kwargs.get('use_amp', True),\\n        'calib_steps': kwargs.get('calib_steps', 32),\\n        'backend': kwargs.get('backend', 'fbgemm'),\\n        # Quantization controls\\n        'quantization_bits': kwargs.get('quantization_bits', 8),\\n        'quantize_weights': kwargs.get('quantize_weights', True),\\n        'quantize_activations': kwargs.get('quantize_activations', True),\\n    }\\n\\n    # Ensure label dtype\\n    y_train = y_train.long()\\n    y_val = y_val.long()\\n\\n    # Ensure channel-first layout for Conv1d\\n    X_train = _ensure_ch_first(X_train)\\n    X_val = _ensure_ch_first(X_val)\\n\\n    # Datasets and loaders\\n    train_ds = TensorDataset(X_train, y_train)\\n    val_ds = TensorDataset(X_val, y_val)\\n    train_loader = DataLoader(train_ds, batch_size=hp['batch_size'], shuffle=True, drop_last=False, pin_memory=False, num_workers=0)\\n    val_loader = DataLoader(val_ds, batch_size=hp['batch_size'], shuffle=False, drop_last=False, pin_memory=False, num_workers=0)\\n\\n    # Model\\n    model = TinyECGCNN(in_channels=X_train.shape[1], num_classes=5, base_channels=hp['base_channels'], dropout=hp['dropout'])\\n    param_count = _count_parameters(model)\\n    assert param_count <= 256_000, f\"Model has {param_count} parameters, which exceeds the 256K limit.\"\\n\\n    model.to(device)\\n    optimizer = torch.optim.AdamW(model.parameters(), lr=hp['lr'], weight_decay=hp['weight_decay'])\\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(hp['step_size']), gamma=hp['gamma'])\\n    criterion = nn.CrossEntropyLoss()\\n\\n    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda' and hp['use_amp']))\\n\\n    train_losses, val_losses, val_accs = [], [], []\\n\\n    for epoch in range(1, int(hp['epochs']) + 1):\\n        # Training\\n        model.train()\\n        running_loss = 0.0\\n        total = 0\\n        for xb, yb in train_loader:\\n            xb = xb.to(device)\\n            yb = yb.to(device)\\n            optimizer.zero_grad(set_to_none=True)\\n            with torch.cuda.amp.autocast(enabled=(device.type == 'cuda' and hp['use_amp'])):\\n                logits = model(xb)\\n                loss = criterion(logits, yb)\\n            scaler.scale(loss).backward()\\n            if hp['clip_grad_norm'] and hp['clip_grad_norm'] > 0:\\n                scaler.unscale_(optimizer)\\n                nn.utils.clip_grad_norm_(model.parameters(), float(hp['clip_grad_norm']))\\n            scaler.step(optimizer)\\n            scaler.update()\\n            running_loss += loss.detach().item() * xb.size(0)\\n            total += xb.size(0)\\n        train_epoch_loss = running_loss / max(1, total)\\n        train_losses.append(train_epoch_loss)\\n\\n        # Validation\\n        model.eval()\\n        v_loss = 0.0\\n        v_total = 0\\n        correct = 0\\n        with torch.no_grad():\\n            for xb, yb in val_loader:\\n                xb = xb.to(device)\\n                yb = yb.to(device)\\n                logits = model(xb)\\n                loss = criterion(logits, yb)\\n                v_loss += loss.item() * xb.size(0)\\n                v_total += xb.size(0)\\n                preds = torch.argmax(logits, dim=1)\\n                correct += (preds == yb).sum().item()\\n        val_epoch_loss = v_loss / max(1, v_total)\\n        val_epoch_acc = correct / max(1, v_total)\\n        val_losses.append(val_epoch_loss)\\n        val_accs.append(val_epoch_acc)\\n\\n        scheduler.step()\\n\\n        print(f\"Epoch {epoch:03d} | train_loss={train_epoch_loss:.5f} | val_loss={val_epoch_loss:.5f} | val_acc={val_epoch_acc:.4f}\")\\n\\n    # -----------------------------\\n    # Post-Training Quantization\\n    # -----------------------------\\n    # Always create a fresh FP32 CPU copy for quantization steps\\n    fp32_cpu_model = copy.deepcopy(model).to('cpu')\\n    quantized_model = fp32_cpu_model\\n\\n    bits = int(hp['quantization_bits'])\\n    q_w = bool(hp['quantize_weights'])\\n    q_a = bool(hp['quantize_activations'])\\n\\n    if bits == 32 or (not q_w and not q_a):\\n        # No quantization requested; keep as trained model on specified device\\n        quantized_model = copy.deepcopy(model).to(device)\\n    elif bits == 8:\\n        if q_a:\\n            # Static quantization (weights + activations), CPU-only\\n            try:\\n                import torch.ao.quantization as tq\\n            except Exception:\\n                import torch.quantization as tq\\n\\n            # Set backend engine (fbgemm for x86, qnnpack for ARM)\\n            if hp['backend'] in ('fbgemm', 'qnnpack'):\\n                torch.backends.quantized.engine = hp['backend']\\n            else:\\n                torch.backends.quantized.engine = 'fbgemm'\\n\\n            quantized_model = copy.deepcopy(fp32_cpu_model)\\n            quantized_model.eval()\\n\\n            # Fuse Conv-ReLU pairs inside self.features\\n            # features indices: [0]=Conv, [1]=ReLU, [2]=Pool, [3]=Conv, [4]=ReLU, [5]=Pool, [6]=Conv, [7]=ReLU, [8]=AdaptiveAvgPool\\n            tq.fuse_modules(quantized_model.features, [['0', '1'], ['3', '4'], ['6', '7']], inplace=True)\\n\\n            # Assign qconfig and prepare\\n            qconfig = tq.get_default_qconfig(torch.backends.quantized.engine)\\n            quantized_model.qconfig = qconfig\\n            prepared = tq.prepare(quantized_model, inplace=False)\\n\\n            # Calibrate with a few CPU batches\\n            prepared.eval()\\n            calib_steps = int(hp['calib_steps'])\\n            steps_done = 0\\n            with torch.no_grad():\\n                for xb, _ in train_loader:\\n                    xb_cpu = xb.to('cpu')\\n                    prepared(xb_cpu)\\n                    steps_done += 1\\n                    if steps_done >= calib_steps:\\n                        break\\n\\n            # Convert to quantized model\\n            quantized_model = tq.convert(prepared, inplace=False)\\n            quantized_model.eval()\\n        else:\\n            # Weights-only dynamic quantization for Linear layers\\n            try:\\n                import torch.ao.quantization as tq\\n            except Exception:\\n                import torch.quantization as tq\\n            quantized_model = tq.quantize_dynamic(\\n                fp32_cpu_model,\\n                {nn.Linear},\\n                dtype=torch.qint8,\\n            )\\n            quantized_model.eval()\\n    elif bits == 16:\\n        # Use dynamic quantization to float16 for Linear layers (weights), activations stay FP32\\n        try:\\n            import torch.ao.quantization as tq\\n        except Exception:\\n            import torch.quantization as tq\\n        if q_w:\\n            quantized_model = tq.quantize_dynamic(\\n                fp32_cpu_model,\\n                {nn.Linear},\\n                dtype=torch.float16,\\n            )\\n            quantized_model.eval()\\n        else:\\n            # If only activations requested in 16-bit, fall back to returning FP32 model\\n            quantized_model = copy.deepcopy(model).to(device)\\n    else:\\n        # Fallback: unsupported bitwidth -> no quantization\\n        quantized_model = copy.deepcopy(model).to(device)\\n\\n    metrics = {\\n        'train_losses': train_losses,\\n        'val_losses': val_losses,\\n        'val_acc': val_accs,\\n    }\\n\\n    return quantized_model, metrics\\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 256,
      "type": "Categorical",
      "categories": [
        64,
        128,
        256,
        512
      ]
    },
    "epochs": {
      "default": 15,
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "base_channels": {
      "default": 16,
      "type": "Integer",
      "low": 8,
      "high": 64
    },
    "clip_grad_norm": {
      "default": 1.0,
      "type": "Real",
      "low": 0.0,
      "high": 5.0
    },
    "step_size": {
      "default": 10,
      "type": "Integer",
      "low": 5,
      "high": 20
    },
    "gamma": {
      "default": 0.9,
      "type": "Real",
      "low": 0.5,
      "high": 0.99
    },
    "use_amp": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "calib_steps": {
      "default": 32,
      "type": "Integer",
      "low": 10,
      "high": 200
    },
    "backend": {
      "default": "fbgemm",
      "type": "Categorical",
      "categories": [
        "fbgemm",
        "qnnpack"
      ]
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "confidence": 0.9,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758407349,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}