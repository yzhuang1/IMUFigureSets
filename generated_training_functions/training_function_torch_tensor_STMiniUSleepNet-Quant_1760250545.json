{
  "model_name": "STMiniUSleepNet-Quant",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    import math\n    import copy\n    import torch\n    from torch import nn, optim\n    from torch.utils.data import TensorDataset, DataLoader\n    import torch.ao.quantization as tq\n\n    # Robust device handling\n    device = torch.device(device)\n    torch.backends.cudnn.benchmark = True\n\n    # ------------ Hyperparameters ------------\n    lr = float(hyperparams.get('lr', 1e-3))\n    batch_size = int(hyperparams.get('batch_size', 32))\n    epochs = int(hyperparams.get('epochs', 15))\n    weight_decay = float(hyperparams.get('weight_decay', 1e-4))\n    dropout = float(hyperparams.get('dropout', 0.1))\n    base_channels = int(hyperparams.get('base_channels', 12))\n    temporal_kernel = int(hyperparams.get('temporal_kernel', 7))\n    spatial_segments = int(hyperparams.get('spatial_segments', 10))  # must divide 6000\n    label_smoothing = float(hyperparams.get('label_smoothing', 0.0))\n    grad_clip = float(hyperparams.get('grad_clip', 0.0))\n    num_workers = int(hyperparams.get('num_workers', 4))\n\n    # Quantization params\n    quantization_bits = int(hyperparams.get('quantization_bits', 8))  # {8,16,32}\n    quantize_weights = bool(hyperparams.get('quantize_weights', True))\n    quantize_activations = bool(hyperparams.get('quantize_activations', True))\n    calibration_samples = int(hyperparams.get('calibration_samples', 2048))\n\n    # ------------ Sanity checks ------------\n    if X_train.dim() != 3:\n        raise ValueError(f\"X_train must be a 3D tensor of shape (N, 6, 6000). Got {tuple(X_train.shape)}\")\n    if X_train.shape[1] != 6 or X_train.shape[2] != 6000:\n        raise ValueError(f\"Expected input per-sample shape (6, 6000). Got {tuple(X_train.shape[1:])}\")\n    if spatial_segments <= 0 or (6000 % spatial_segments) != 0:\n        raise ValueError(\"spatial_segments must be a positive divisor of 6000\")\n    if temporal_kernel % 2 == 0:\n        temporal_kernel += 1  # ensure odd for same padding\n\n    # ------------ DataLoaders (spawn ctx + pin_memory) ------------\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n    mp_ctx = torch.multiprocessing.get_context('spawn')\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        multiprocessing_context=mp_ctx,\n        drop_last=False,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n        multiprocessing_context=mp_ctx,\n        drop_last=False,\n    )\n\n    # ------------ Model Definition (ST-mini U-SleepNet) ------------\n    class TemporalBlock(nn.Module):\n        def __init__(self, in_ch, out_ch, ksize, stride=1):\n            super().__init__()\n            pad = ksize // 2\n            self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=ksize, stride=stride, padding=pad, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.relu = nn.ReLU(inplace=True)\n        def forward(self, x):\n            return self.relu(self.bn(self.conv(x)))\n\n    class UpBlock(nn.Module):\n        def __init__(self, in_ch, out_ch):\n            super().__init__()\n            self.deconv = nn.ConvTranspose1d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.relu = nn.ReLU(inplace=True)\n        def forward(self, x):\n            return self.relu(self.bn(self.deconv(x)))\n\n    class STMiniUSleepNet(nn.Module):\n        def __init__(self, in_channels=6, num_classes=5, base_channels=12, k=7, dropout=0.1, spatial_segments=10):\n            super().__init__()\n            C = base_channels\n            self.enc1 = TemporalBlock(in_channels, C, k)\n            self.down1 = TemporalBlock(C, 2*C, 5, stride=2)\n            self.down2 = TemporalBlock(2*C, 4*C, 5, stride=2)\n            self.bottleneck = TemporalBlock(4*C, 4*C, 3, stride=1)\n            self.up1 = UpBlock(4*C, 2*C)\n            self.up2 = UpBlock(2*C, C)\n            self.gap = nn.AdaptiveAvgPool1d(1)\n            self.temp_mlp = nn.Sequential(\n                nn.Linear(C, 16), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(16, 8)\n            )\n            self.spatial_segments = spatial_segments\n            self.adjacency = nn.Parameter(torch.eye(6))  # learnable channel graph (6x6)\n            self.spat_mlp = nn.Sequential(\n                nn.Linear(6*spatial_segments, 16), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(16, 8)\n            )\n            self.head = nn.Linear(16, num_classes)\n        def forward(self, x):\n            # Temporal U-Net branch\n            t = self.enc1(x)\n            t = self.down1(t)\n            t = self.down2(t)\n            t = self.bottleneck(t)\n            t = self.up1(t)\n            t = self.up2(t)\n            t = self.gap(t).squeeze(-1)  # (B, C)\n            tfeat = self.temp_mlp(t)     # (B, 8)\n            # Spatial graph branch over channel-time segments\n            B, C_in, T = x.shape  # C_in should be 6\n            K = self.spatial_segments\n            seg_len = T // K\n            xt = x[:, :, :seg_len*K]\n            xs = xt.view(B, C_in, K, seg_len).mean(-1)  # (B, 6, K)\n            A = torch.softmax(self.adjacency, dim=1)    # (6,6)\n            msg = torch.einsum('ij,bjk->bik', A, xs)    # (B, 6, K)\n            sfeat = self.spat_mlp(msg.reshape(B, -1))   # (B, 8)\n            fused = torch.cat([tfeat, sfeat], dim=1)     # (B, 16)\n            logits = self.head(fused)\n            return logits\n        def fuse_model(self):\n            # fuse Conv-BN-ReLU where applicable\n            try:\n                tq.fuse_modules(self, [['enc1.conv', 'enc1.bn', 'enc1.relu']], inplace=True)\n                tq.fuse_modules(self, [['down1.conv', 'down1.bn', 'down1.relu']], inplace=True)\n                tq.fuse_modules(self, [['down2.conv', 'down2.bn', 'down2.relu']], inplace=True)\n                tq.fuse_modules(self, [['bottleneck.conv', 'bottleneck.bn', 'bottleneck.relu']], inplace=True)\n            except Exception:\n                pass\n            # UpBlocks use ConvTranspose1d which is not fuse-supported; skip\n\n    model = STMiniUSleepNet(in_channels=6, num_classes=5, base_channels=base_channels, k=temporal_kernel, dropout=dropout, spatial_segments=spatial_segments)\n    model = model.to(device)\n\n    # ------------ Optimization ------------\n    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    # ------------ Training Loop ------------\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=True).float()\n            yb = yb.to(device, non_blocking=True).long()\n            optimizer.zero_grad(set_to_none=True)\n            out = model(xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0.0:\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n            optimizer.step()\n            bs = xb.size(0)\n            running_loss += loss.detach().item() * bs\n            seen += bs\n        epoch_train_loss = running_loss / max(1, seen)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_loss, correct, total = 0.0, 0, 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=True).float()\n                yb = yb.to(device, non_blocking=True).long()\n                out = model(xb)\n                loss = criterion(out, yb)\n                val_loss += loss.detach().item() * xb.size(0)\n                preds = out.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        epoch_val_loss = val_loss / max(1, total)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        print(f\"Epoch {epoch:03d}/{epochs} - train_loss: {epoch_train_loss:.4f} - val_loss: {epoch_val_loss:.4f} - val_acc: {epoch_val_acc:.4f}\")\n\n    # ------------ Post-Training Quantization ------------\n    def quantize_post_training(trained_model):\n        # Always quantize/evaluate on CPU\n        mdl = copy.deepcopy(trained_model).to('cpu')\n        mdl.eval()\n\n        # If 32-bit requested or weights quantization disabled, just return CPU model\n        if quantization_bits == 32 or not quantize_weights:\n            return mdl\n\n        if quantization_bits == 8:\n            if quantize_weights and quantize_activations:\n                # Static int8 quantization (Conv/Linear) with calibration\n                torch.backends.quantized.engine = 'fbgemm'\n                try:\n                    mdl.fuse_model()\n                except Exception:\n                    pass\n                try:\n                    default_qconfig = tq.get_default_qconfig('fbgemm')\n                    # Set global qconfig\n                    mdl.qconfig = default_qconfig\n                    # Disable quantization for ConvTranspose1d (unsupported)\n                    for name, module in mdl.named_modules():\n                        if isinstance(module, nn.ConvTranspose1d):\n                            module.qconfig = None\n                    # Also disable at container level for safety\n                    if hasattr(mdl, 'up1'):\n                        try:\n                            mdl.up1.qconfig = None\n                            if hasattr(mdl.up1, 'deconv'):\n                                mdl.up1.deconv.qconfig = None\n                        except Exception:\n                            pass\n                    if hasattr(mdl, 'up2'):\n                        try:\n                            mdl.up2.qconfig = None\n                            if hasattr(mdl.up2, 'deconv'):\n                                mdl.up2.deconv.qconfig = None\n                        except Exception:\n                            pass\n\n                    tq.prepare(mdl, inplace=True)\n\n                    # Calibration with a small CPU loader\n                    calib_bs = min(256, batch_size)\n                    calib_loader = DataLoader(\n                        TensorDataset(X_train.cpu(), y_train.cpu()),\n                        batch_size=calib_bs,\n                        shuffle=False,\n                        num_workers=0,\n                        pin_memory=False,\n                    )\n                    seen = 0\n                    with torch.no_grad():\n                        for xb, _ in calib_loader:\n                            xb = xb.float()\n                            _ = mdl(xb)\n                            seen += xb.size(0)\n                            if seen >= calibration_samples:\n                                break\n                    tq.convert(mdl, inplace=True)\n                    return mdl\n                except Exception:\n                    # Fallback to dynamic quantization (Linear-only)\n                    qdtype = torch.qint8\n                    mdl = tq.quantize_dynamic(mdl, {nn.Linear}, dtype=qdtype)\n                    return mdl\n            else:\n                # Dynamic quantization for Linear layers only\n                qdtype = torch.qint8\n                mdl = tq.quantize_dynamic(mdl, {nn.Linear}, dtype=qdtype)\n                return mdl\n\n        if quantization_bits == 16:\n            # Float16 weight cast; activations stay fp32 on CPU\n            mdl = mdl.to(dtype=torch.float16)\n            return mdl\n\n        # Fallback\n        return mdl\n\n    quantized_model = quantize_post_training(model)\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n    }\n\n    return quantized_model, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 32,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32,
        48,
        64
      ]
    },
    "epochs": {
      "default": 15,
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "base_channels": {
      "default": 12,
      "type": "Integer",
      "low": 8,
      "high": 24
    },
    "temporal_kernel": {
      "default": 7,
      "type": "Categorical",
      "categories": [
        3,
        5,
        7,
        9
      ]
    },
    "spatial_segments": {
      "default": 10,
      "type": "Categorical",
      "categories": [
        5,
        10,
        12,
        15,
        20,
        24,
        25,
        30,
        40,
        50,
        60,
        75,
        100,
        120,
        125,
        150,
        200,
        240,
        250,
        300,
        400,
        500,
        600,
        750,
        1000
      ]
    },
    "label_smoothing": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "grad_clip": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 1.0
    },
    "num_workers": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        4
      ]
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "calibration_samples": {
      "default": 2048,
      "type": "Integer",
      "low": 512,
      "high": 8192
    }
  },
  "confidence": 0.86,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      89283,
      6,
      6000
    ],
    "dtype": "float32",
    "feature_count": 6000,
    "sample_count": 89283,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1760250545,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}