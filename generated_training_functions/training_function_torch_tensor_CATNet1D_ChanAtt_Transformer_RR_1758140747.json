{
  "model_name": "CATNet1D_ChanAtt_Transformer_RR",
  "training_code": "import math\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n\ndef train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    \"\"\"\n    Train a lightweight CAT-Netâ€“style 1D CNN + Channel Attention + Transformer model for 5-class ECG classification.\n    Input tensors:\n      - X_*: torch.Tensor of shape [N, 1000, 2] or [N, 2, 1000]\n      - y_*: torch.LongTensor of shape [N]\n    Returns: (model, metrics_dict)\n    \"\"\"\n    # ------------------ Hyperparameters (defaults) ------------------\n    hp = {\n        'lr': 1e-3,\n        'batch_size': 64,\n        'epochs': 10,\n        'cnn_channels': 64,            # base channels after CNN stem\n        'kernel_size': 7,\n        'se_reduction': 8,\n        'n_transformer_layers': 2,\n        'n_heads': 4,\n        'dim_feedforward': 128,\n        'hidden_size': 96,\n        'dropout': 0.1,\n        'weight_decay': 1e-4,\n        'use_rr_features': True,\n        'rr_dim': 4,\n        'augment_prob': 0.5,\n        'gaussian_noise_scale': 0.01,  # relative to per-sample std\n        'baseline_wander_amp': 0.02,   # relative to per-sample std\n        'baseline_wander_freq_low': 0.05,\n        'baseline_wander_freq_high': 0.5,\n        'sampling_rate': 360.0,\n        'use_smote_tomek': True,\n    }\n    hp.update(hyperparams or {})\n\n    num_classes = 5\n\n    # ------------------ Utilities ------------------\n    def ensure_nct(x):\n        # Ensure shape [N, C, T] for 2 leads, 1000 samples\n        if x.dim() != 3:\n            raise ValueError('Expected X to be 3D [N, T, C] or [N, C, T]')\n        if x.shape[1] == 2 and x.shape[2] == 1000:\n            return x\n        elif x.shape[1] == 1000 and x.shape[2] == 2:\n            return x.permute(0, 2, 1)\n        else:\n            raise ValueError(f'Unsupported X shape: {tuple(x.shape)}; expected [N,1000,2] or [N,2,1000]')\n\n    def to_cpu_numpy(x):\n        return x.detach().to('cpu').numpy()\n\n    def compute_rr_features_np(X_np, fs=360.0):\n        # X_np: [N, 2, T]\n        # RR proxy features per sample (fast approximations):\n        #  1) estimated HR bpm via simple peak counting on lead 1\n        #  2) normalized peak density (peaks per second)\n        #  3) energy ratio lead1 / (lead1+lead2)\n        #  4) mean signal std across leads\n        N, C, T = X_np.shape\n        rr_feats = np.zeros((N, 4), dtype=np.float32)\n        win = max(3, int(0.15 * fs))  # smoothing window ~150ms\n        for i in range(N):\n            sig1 = X_np[i, 0]\n            sig2 = X_np[i, 1]\n            # Smooth squared envelope for simple R-peak proxy\n            env = np.convolve(sig1**2, np.ones(win, dtype=np.float32)/win, mode='same')\n            thr = 0.35 * (env.max() + 1e-8)\n            # Peak detection: local maxima above threshold\n            peaks = []\n            for t in range(1, T-1):\n                if env[t] > thr and env[t] >= env[t-1] and env[t] >= env[t+1]:\n                    peaks.append(t)\n            peak_count = len(peaks)\n            if peak_count >= 2:\n                rr_mean_samples = np.mean(np.diff(peaks))\n                rr_mean_sec = rr_mean_samples / fs\n                hr_bpm = float(60.0 / max(rr_mean_sec, 1e-3))\n            else:\n                # Fallback: estimate HR from dominant frequency of sig1\n                # Use simple FFT method\n                freqs = np.fft.rfftfreq(T, d=1.0/fs)\n                spec = np.abs(np.fft.rfft(sig1))\n                # plausible HR band: 0.5 - 3.0 Hz (30-180 bpm)\n                band = (freqs >= 0.5) & (freqs <= 3.0)\n                if band.any() and spec[band].sum() > 0:\n                    f0 = freqs[band][np.argmax(spec[band])]\n                    hr_bpm = float(60.0 * f0)\n                else:\n                    hr_bpm = 75.0\n                peak_count = max(1, int((hr_bpm/60.0) * (T/fs)))\n            peak_density = float(peak_count / (T/fs + 1e-8))  # peaks per second\n            e1 = float(np.sum(sig1**2))\n            e2 = float(np.sum(sig2**2))\n            ratio = float(e1 / (e1 + e2 + 1e-8))\n            std_mean = float(0.5 * (np.std(sig1) + np.std(sig2)))\n            rr_feats[i] = [hr_bpm, peak_density, ratio, std_mean]\n        # Normalize some features to reasonable scales\n        rr_feats[:, 0] = np.clip(rr_feats[:, 0] / 200.0, 0.0, 1.0)   # HR bpm -> [0,1] approx\n        rr_feats[:, 1] = np.clip(rr_feats[:, 1] / 3.0, 0.0, 1.0)     # peak density up to 3 Hz\n        # ratio already in [0,1], std_mean left as is (model can learn scale)\n        return rr_feats.astype(np.float32)\n\n    # ------------------ Optional SMOTE-Tomek on training set ------------------\n    X_train = ensure_nct(X_train.float())\n    X_val = ensure_nct(X_val.float())\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    used_smote = False\n    if hp['use_smote_tomek']:\n        try:\n            from imblearn.combine import SMOTETomek\n            sm = SMOTETomek(random_state=42)\n            X_np = to_cpu_numpy(X_train)\n            y_np = to_cpu_numpy(y_train).astype(np.int64)\n            X_flat = X_np.reshape(X_np.shape[0], -1)\n            X_res, y_res = sm.fit_resample(X_flat, y_np)\n            X_train = torch.from_numpy(X_res.reshape(-1, 2, 1000)).float()\n            y_train = torch.from_numpy(y_res.astype(np.int64))\n            used_smote = True\n        except Exception:\n            used_smote = False\n\n    # ------------------ RR features (computed once) ------------------\n    if hp['use_rr_features']:\n        rr_train_np = compute_rr_features_np(to_cpu_numpy(X_train), fs=float(hp['sampling_rate']))\n        rr_val_np = compute_rr_features_np(to_cpu_numpy(X_val), fs=float(hp['sampling_rate']))\n        rr_train = torch.from_numpy(rr_train_np).float()\n        rr_val = torch.from_numpy(rr_val_np).float()\n    else:\n        rr_train = torch.zeros((X_train.shape[0], hp['rr_dim']), dtype=torch.float32)\n        rr_val = torch.zeros((X_val.shape[0], hp['rr_dim']), dtype=torch.float32)\n\n    # ------------------ Dataset & Augmentations ------------------\n    class ECGDataset(Dataset):\n        def __init__(self, X, y, rr_feats, train=True):\n            self.X = X\n            self.y = y\n            self.rr = rr_feats\n            self.train = train\n            self.aug_p = float(hp['augment_prob'])\n            self.noise_scale = float(hp['gaussian_noise_scale'])\n            self.bw_amp = float(hp['baseline_wander_amp'])\n            self.f_low = float(hp['baseline_wander_freq_low'])\n            self.f_high = float(hp['baseline_wander_freq_high'])\n            self.fs = float(hp['sampling_rate'])\n\n            if self.X.shape[1] != 2 or self.X.shape[2] != 1000:\n                raise ValueError('Dataset expects X with shape [N, 2, 1000]')\n\n        def __len__(self):\n            return self.X.shape[0]\n\n        def __getitem__(self, idx):\n            x = self.X[idx].clone()\n            y = self.y[idx]\n            rr = self.rr[idx]\n            if self.train:\n                # Gaussian noise\n                if random.random() < self.aug_p:\n                    std = x.std(dim=-1, keepdim=True)\n                    noise = torch.randn_like(x) * (self.noise_scale * (std + 1e-6))\n                    x = x + noise\n                # Baseline wander (low-freq sinusoid)\n                if random.random() < self.aug_p:\n                    T = x.shape[-1]\n                    t = torch.arange(T, dtype=torch.float32) / self.fs\n                    for c in range(2):\n                        f = random.uniform(self.f_low, self.f_high)\n                        amp = self.bw_amp * (x[c].std() + 1e-6)\n                        drift = amp * torch.sin(2 * math.pi * f * t)\n                        x[c] = x[c] + drift\n            return x, y, rr\n\n    pin_mem = (X_train.device.type == 'cpu')\n    train_ds = ECGDataset(X_train.cpu(), y_train.cpu(), rr_train.cpu(), train=True)\n    val_ds = ECGDataset(X_val.cpu(), y_val.cpu(), rr_val.cpu(), train=False)\n    train_loader = DataLoader(train_ds, batch_size=int(hp['batch_size']), shuffle=True, drop_last=False, pin_memory=pin_mem)\n    val_loader = DataLoader(val_ds, batch_size=int(hp['batch_size']), shuffle=False, drop_last=False, pin_memory=pin_mem)\n\n    # ------------------ Model ------------------\n    class ChannelSE(nn.Module):\n        def __init__(self, channels, reduction=8):\n            super().__init__()\n            hidden = max(4, channels // int(reduction))\n            self.avg = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Sequential(\n                nn.Conv1d(channels, hidden, kernel_size=1),\n                nn.SiLU(),\n                nn.Conv1d(hidden, channels, kernel_size=1),\n                nn.Sigmoid(),\n            )\n        def forward(self, x):  # x: [B,C,T]\n            w = self.fc(self.avg(x))\n            return x * w\n\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model, max_len=2000, dropout=0.0):\n            super().__init__()\n            self.dropout = nn.Dropout(dropout)\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            self.register_buffer('pe', pe)  # [max_len, d_model]\n        def forward(self, x):  # x: [B,T,C]\n            T = x.size(1)\n            x = x + self.pe[:T, :].unsqueeze(0)\n            return self.dropout(x)\n\n    class ECGCatTrans(nn.Module):\n        def __init__(self, in_ch=2, base_ch=64, k=7, se_red=8, n_layers=2, n_heads=4, d_ff=128, dropout=0.1, rr_dim=4, num_classes=5):\n            super().__init__()\n            pad = k // 2\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=k, padding=pad, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.SiLU(),\n                # depthwise separable conv\n                nn.Conv1d(base_ch, base_ch, kernel_size=5, padding=2, groups=base_ch, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.SiLU(),\n                nn.Conv1d(base_ch, base_ch, kernel_size=1, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.SiLU(),\n                nn.MaxPool1d(kernel_size=2),  # T: 1000 -> 500\n            )\n            self.se = ChannelSE(base_ch, reduction=se_red)\n\n            d_model = base_ch\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=n_heads,\n                dim_feedforward=d_ff,\n                dropout=dropout,\n                batch_first=True,\n                activation='gelu'\n            )\n            self.pos = PositionalEncoding(d_model=d_model, max_len=1000, dropout=dropout)\n            self.trans = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.pool_max = nn.AdaptiveMaxPool1d(1)\n\n            cls_in = d_model * 2 + (rr_dim if rr_dim > 0 else 0)\n            self.head = nn.Sequential(\n                nn.Linear(cls_in, int(hp['hidden_size'])),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(int(hp['hidden_size']), num_classes)\n            )\n\n        def forward(self, x, rr_feats=None):  # x: [B,2,1000]\n            x = self.stem(x)     # [B,C,T]\n            x = self.se(x)\n            x_t = x.transpose(1, 2)  # [B,T,C]\n            x_t = self.pos(x_t)\n            x_t = self.trans(x_t)    # [B,T,C]\n            x_back = x_t.transpose(1, 2)  # [B,C,T]\n            feat = torch.cat([self.pool(x_back).squeeze(-1), self.pool_max(x_back).squeeze(-1)], dim=1)\n            if rr_feats is not None:\n                feat = torch.cat([feat, rr_feats], dim=1)\n            out = self.head(feat)\n            return out\n\n    model = ECGCatTrans(\n        in_ch=2,\n        base_ch=int(hp['cnn_channels']),\n        k=int(hp['kernel_size']),\n        se_red=int(hp['se_reduction']),\n        n_layers=int(hp['n_transformer_layers']),\n        n_heads=int(hp['n_heads']),\n        d_ff=int(hp['dim_feedforward']),\n        dropout=float(hp['dropout']),\n        rr_dim=int(hp['rr_dim'] if hp['use_rr_features'] else 0),\n        num_classes=num_classes,\n    ).to(device)\n\n    # ------------------ Loss, Optim ------------------\n    def compute_class_weights(y):\n        y_np = to_cpu_numpy(y).astype(np.int64)\n        counts = np.bincount(y_np, minlength=num_classes).astype(np.float32)\n        counts[counts == 0] = 1.0\n        inv = 1.0 / counts\n        w = inv / inv.sum() * num_classes\n        return torch.tensor(w, dtype=torch.float32)\n\n    if used_smote:\n        class_weights = torch.ones(num_classes, dtype=torch.float32)\n    else:\n        class_weights = compute_class_weights(y_train)\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(hp['lr']), weight_decay=float(hp['weight_decay']))\n\n    # ------------------ Metrics ------------------\n    @torch.no_grad()\n    def evaluate(loader):\n        model.eval()\n        total_loss = 0.0\n        all_preds = []\n        all_targets = []\n        for xb, yb, rb in loader:\n            xb = xb.to(device, non_blocking=pin_mem)\n            yb = yb.to(device, non_blocking=pin_mem)\n            rb = rb.to(device, non_blocking=pin_mem)\n            logits = model(xb, rb if hp['use_rr_features'] else None)\n            loss = criterion(logits, yb)\n            total_loss += loss.item() * xb.size(0)\n            preds = torch.argmax(logits, dim=1)\n            all_preds.append(preds.detach().cpu())\n            all_targets.append(yb.detach().cpu())\n        all_preds = torch.cat(all_preds).numpy()\n        all_targets = torch.cat(all_targets).numpy()\n        # Confusion matrix\n        cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n        for t, p in zip(all_targets, all_preds):\n            cm[t, p] += 1\n        # Accuracy\n        acc = float(np.trace(cm) / max(1, cm.sum()))\n        # Macro-F1\n        f1s = []\n        for k in range(num_classes):\n            tp = cm[k, k]\n            fp = cm[:, k].sum() - tp\n            fn = cm[k, :].sum() - tp\n            precision = tp / (tp + fp + 1e-8)\n            recall = tp / (tp + fn + 1e-8)\n            f1 = 2 * precision * recall / (precision + recall + 1e-8)\n            f1s.append(f1)\n        macro_f1 = float(np.mean(f1s))\n        avg_loss = total_loss / max(1, len(loader.dataset))\n        return avg_loss, acc, macro_f1\n\n    # ------------------ Training Loop ------------------\n    for epoch in range(int(hp['epochs'])):\n        model.train()\n        for xb, yb, rb in train_loader:\n            xb = xb.to(device, non_blocking=pin_mem)\n            yb = yb.to(device, non_blocking=pin_mem)\n            rb = rb.to(device, non_blocking=pin_mem)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb, rb if hp['use_rr_features'] else None)\n            loss = criterion(logits, yb)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n            optimizer.step()\n\n    train_loss, train_acc, train_f1 = evaluate(train_loader)\n    val_loss, val_acc, val_f1 = evaluate(val_loader)\n\n    metrics = {\n        'train_loss': train_loss,\n        'train_accuracy': train_acc,\n        'train_macro_f1': train_f1,\n        'val_loss': val_loss,\n        'val_accuracy': val_acc,\n        'val_macro_f1': val_f1,\n        'used_smote_tomek': used_smote\n    }\n\n    return model, metrics\n",
  "hyperparameters": {
    "lr": 0.001,
    "batch_size": 64,
    "epochs": 10,
    "cnn_channels": 64,
    "kernel_size": 7,
    "se_reduction": 8,
    "n_transformer_layers": 2,
    "n_heads": 4,
    "dim_feedforward": 128,
    "hidden_size": 96,
    "dropout": 0.1,
    "weight_decay": 0.0001,
    "use_rr_features": true,
    "augment_prob": 0.5,
    "gaussian_noise_scale": 0.01,
    "baseline_wander_amp": 0.02,
    "baseline_wander_freq_low": 0.05,
    "baseline_wander_freq_high": 0.5,
    "sampling_rate": 360.0,
    "use_smote_tomek": true
  },
  "reasoning": "Implements a compact CAT-Netâ€“style pipeline: 1D CNN stem with depthwise separable convs extracts morphology; squeeze-and-excitation (channel attention) reweights informative leads; a small Transformer encoder models longer temporal dependencies across the 1D sequence; pooled features are concatenated with lightweight RR-related context features (HR proxy, peak density, lead energy ratio, std) before classification, following Tiny-Transformer-style late fusion. Training uses inter-patient-friendly augmentations (baseline wander, Gaussian noise) and optionally SMOTE-Tomek to mitigate AAMI 5-class imbalance. The model fits under ~256K params with defaults (64 channels, 2 Transformer layers, FF=128), and code adheres to pin_memory CPU-only usage.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "batch_size",
    "epochs",
    "cnn_channels",
    "kernel_size",
    "se_reduction",
    "n_transformer_layers",
    "n_heads",
    "dim_feedforward",
    "hidden_size",
    "dropout",
    "weight_decay",
    "use_rr_features",
    "augment_prob",
    "gaussian_noise_scale",
    "baseline_wander_amp",
    "baseline_wander_freq_low",
    "baseline_wander_freq_high",
    "sampling_rate",
    "use_smote_tomek"
  ],
  "bo_search_space": {
    "lr": {
      "type": "Real",
      "low": 1e-05,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "type": "Categorical",
      "categories": [
        16,
        32,
        64,
        96,
        128
      ]
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 40
    },
    "cnn_channels": {
      "type": "Integer",
      "low": 32,
      "high": 96
    },
    "kernel_size": {
      "type": "Categorical",
      "categories": [
        5,
        7,
        9
      ]
    },
    "se_reduction": {
      "type": "Categorical",
      "categories": [
        4,
        8,
        16
      ]
    },
    "n_transformer_layers": {
      "type": "Integer",
      "low": 1,
      "high": 3
    },
    "n_heads": {
      "type": "Categorical",
      "categories": [
        2,
        4,
        8
      ]
    },
    "dim_feedforward": {
      "type": "Integer",
      "low": 64,
      "high": 256
    },
    "hidden_size": {
      "type": "Integer",
      "low": 64,
      "high": 192
    },
    "dropout": {
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "weight_decay": {
      "type": "Real",
      "low": 0.0,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "use_rr_features": {
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "augment_prob": {
      "type": "Real",
      "low": 0.0,
      "high": 1.0
    },
    "gaussian_noise_scale": {
      "type": "Real",
      "low": 0.0,
      "high": 0.05
    },
    "baseline_wander_amp": {
      "type": "Real",
      "low": 0.0,
      "high": 0.1
    },
    "baseline_wander_freq_low": {
      "type": "Real",
      "low": 0.02,
      "high": 0.2
    },
    "baseline_wander_freq_high": {
      "type": "Real",
      "low": 0.3,
      "high": 0.8
    },
    "sampling_rate": {
      "type": "Real",
      "low": 250.0,
      "high": 500.0
    },
    "use_smote_tomek": {
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758140747,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}