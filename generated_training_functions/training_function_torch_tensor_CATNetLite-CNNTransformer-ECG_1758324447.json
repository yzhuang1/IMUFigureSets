{
  "model_name": "CATNetLite-CNNTransformer-ECG",
  "training_code": "import math\\nfrom typing import Dict, Any\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.utils.data import TensorDataset, DataLoader\\n\\n# --------- Building Blocks ---------\\nclass SELayer1D(nn.Module):\\n    def __init__(self, channels: int, reduction: int = 4):\\n        super().__init__()\\n        hidden = max(1, channels // reduction)\\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\\n        self.fc = nn.Sequential(\\n            nn.Linear(channels, hidden, bias=True),\\n            nn.ReLU(inplace=True),\\n            nn.Linear(hidden, channels, bias=True),\\n            nn.Sigmoid()\\n        )\\n\\n    def forward(self, x):\\n        # x: (B, C, L)\\n        b, c, l = x.size()\\n        y = self.avg_pool(x).view(b, c)\\n        y = self.fc(y).view(b, c, 1)\\n        return x * y\\n\\nclass SEMultiScaleCNN(nn.Module):\\n    def __init__(self, in_ch: int, base_ch: int, kernel_sizes, d_model: int, stride: int = 2, se_reduction: int = 4, dropout: float = 0.1):\\n        super().__init__()\\n        self.kernel_sizes = kernel_sizes\\n        self.branches = nn.ModuleList()\\n        for k in kernel_sizes:\\n            pad = k // 2\\n            self.branches.append(nn.Sequential(\\n                nn.Conv1d(in_ch, base_ch, kernel_size=k, stride=stride, padding=pad, bias=False),\\n                nn.BatchNorm1d(base_ch),\\n                nn.ReLU(inplace=True)\\n            ))\\n        # After concat -> base_ch * len(kernels)\\n        in_cat = base_ch * len(kernel_sizes)\\n        self.conv_block1 = nn.Sequential(\\n            nn.Conv1d(in_cat, d_model, kernel_size=3, stride=2, padding=1, bias=False),\\n            nn.BatchNorm1d(d_model),\\n            nn.ReLU(inplace=True)\\n        )\\n        self.conv_block2 = nn.Sequential(\\n            nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1, bias=False),\\n            nn.BatchNorm1d(d_model),\\n            nn.ReLU(inplace=True)\\n        )\\n        self.se = SELayer1D(d_model, reduction=se_reduction)\\n        self.dropout = nn.Dropout(dropout)\\n        # Quantization stubs (used for static PTQ when enabled)\\n        try:\\n            from torch.ao.quantization import QuantStub, DeQuantStub\\n        except Exception:\\n            from torch.quantization import QuantStub, DeQuantStub\\n        self.quant = QuantStub()\\n        self.dequant = DeQuantStub()\\n\\n    def fuse_model(self):\\n        # Fuse Conv-BN-ReLU in branches and conv blocks for better quantization\\n        try:\\n            from torch.ao.quantization import fuse_modules\\n        except Exception:\\n            from torch.quantization import fuse_modules\\n        for i, _ in enumerate(self.branches):\\n            fuse_modules(self.branches[i], [[\"0\", \"1\", \"2\"]], inplace=True)\\n        fuse_modules(self.conv_block1, [[\"0\", \"1\", \"2\"]], inplace=True)\\n        fuse_modules(self.conv_block2, [[\"0\", \"1\", \"2\"]], inplace=True)\\n\\n    def forward(self, x):\\n        # x: (B, T, C) -> Conv1D expects (B, C, T)\\n        x = x.permute(0, 2, 1).contiguous()\\n        # Quantize input for static PTQ path (no-op if not prepared)\\n        x = self.quant(x)\\n        outs = [br(x) for br in self.branches]\\n        x = torch.cat(outs, dim=1)\\n        x = self.conv_block1(x)\\n        x = self.conv_block2(x)\\n        x = self.se(x)\\n        x = self.dropout(x)\\n        # Dequantize back to float for transformer (no-op if not converted)\\n        x = self.dequant(x)\\n        # (B, C, L) -> (B, L, C) for batch_first transformer\\n        x = x.transpose(1, 2).contiguous()\\n        return x\\n\\nclass CNNTransformerClassifier(nn.Module):\\n    def __init__(\\n        self,\\n        in_channels: int = 2,\\n        num_classes: int = 5,\\n        cnn_base_channels: int = 16,\\n        kernel_sizes = (5, 11, 23),\\n        stride: int = 2,\\n        transformer_d_model: int = 64,\\n        transformer_nhead: int = 4,\\n        transformer_ffn_dim: int = 128,\\n        transformer_layers: int = 2,\\n        dropout: float = 0.1,\\n        se_reduction: int = 4\\n    ):\\n        super().__init__()\\n        self.cnn = SEMultiScaleCNN(\\n            in_ch=in_channels, base_ch=cnn_base_channels, kernel_sizes=kernel_sizes,\\n            d_model=transformer_d_model, stride=stride, se_reduction=se_reduction, dropout=dropout\\n        )\\n        encoder_layer = nn.TransformerEncoderLayer(\\n            d_model=transformer_d_model, nhead=transformer_nhead,\\n            dim_feedforward=transformer_ffn_dim, dropout=dropout, batch_first=True, norm_first=True\\n        )\\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\\n        self.head = nn.Sequential(\\n            nn.LayerNorm(transformer_d_model),\\n            nn.Linear(transformer_d_model, num_classes)\\n        )\\n\\n    def forward(self, x):\\n        # x: (B, 1000, 2)\\n        x = self.cnn(x)  # (B, L, D)\\n        x = self.transformer(x)\\n        x = x.mean(dim=1)  # Global average pool over time\\n        logits = self.head(x)\\n        return logits\\n\\n# --------- Losses & Metrics ---------\\nclass FocalLoss(nn.Module):\\n    def __init__(self, gamma: float = 2.0, weight: torch.Tensor = None, reduction: str = 'mean', label_smoothing: float = 0.0):\\n        super().__init__()\\n        self.gamma = gamma\\n        self.weight = weight\\n        self.reduction = reduction\\n        self.label_smoothing = label_smoothing\\n\\n    def forward(self, logits, target):\\n        # logits: (B, C), target: (B,)\\n        log_probs = F.log_softmax(logits, dim=1)\\n        probs = log_probs.exp()\\n        # One-hot targets with label smoothing\\n        num_classes = logits.size(1)\\n        with torch.no_grad():\\n            true_dist = torch.zeros_like(logits)\\n            true_dist.fill_(self.label_smoothing / (num_classes - 1))\\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.label_smoothing)\\n        ce_loss = -(true_dist * log_probs)  # (B, C)\\n        pt = (probs * true_dist).sum(dim=1)  # (B,)\\n        focal_factor = (1 - pt).clamp(0, 1) ** self.gamma\\n        loss = ce_loss.sum(dim=1) * focal_factor\\n        if self.weight is not None:\\n            # Class weights applied to targets only\\n            wt = self.weight.gather(0, target)\\n            loss = loss * wt\\n        if self.reduction == 'mean':\\n            return loss.mean()\\n        elif self.reduction == 'sum':\\n            return loss.sum()\\n        else:\\n            return loss\\n\\ndef compute_class_weights(y: torch.Tensor, num_classes: int) -> torch.Tensor:\\n    # Inverse frequency normalization\\n    counts = torch.bincount(y.view(-1), minlength=num_classes).float()\\n    counts = torch.clamp(counts, min=1.0)\\n    inv = 1.0 / counts\\n    weights = inv * (num_classes / inv.sum())\\n    return weights\\n\\n@torch.no_grad()\\ndef accuracy(preds: torch.Tensor, targets: torch.Tensor) -> float:\\n    pred_labels = preds.argmax(dim=1)\\n    return (pred_labels == targets).float().mean().item()\\n\\n@torch.no_grad()\\ndef macro_f1(preds: torch.Tensor, targets: torch.Tensor, num_classes: int) -> float:\\n    pred_labels = preds.argmax(dim=1)\\n    f1s = []\\n    for c in range(num_classes):\\n        tp = ((pred_labels == c) & (targets == c)).sum().item()\\n        fp = ((pred_labels == c) & (targets != c)).sum().item()\\n        fn = ((pred_labels != c) & (targets == c)).sum().item()\\n        precision = tp / (tp + fp + 1e-12)\\n        recall = tp / (tp + fn + 1e-12)\\n        f1 = 2 * precision * recall / (precision + recall + 1e-12)\\n        f1s.append(f1)\\n    return float(sum(f1s) / len(f1s))\\n\\n# --------- Training / Eval Loops ---------\\ndef train_one_epoch(model, loader, optimizer, device, criterion, grad_clip: float = 0.0):\\n    model.train()\\n    running = 0.0\\n    for xb, yb in loader:\\n        xb = xb.to(device, non_blocking=True)\\n        yb = yb.to(device, non_blocking=True)\\n        logits = model(xb)\\n        loss = criterion(logits, yb)\\n        optimizer.zero_grad(set_to_none=True)\\n        loss.backward()\\n        if grad_clip and grad_clip > 0:\\n            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\\n        optimizer.step()\\n        running += loss.item() * xb.size(0)\\n    return running / len(loader.dataset)\\n\\n@torch.no_grad()\\ndef evaluate_model(model, loader, device, num_classes: int, criterion=None):\\n    model.eval()\\n    total_loss = 0.0\\n    all_logits = []\\n    all_targets = []\\n    for xb, yb in loader:\\n        xb = xb.to(device, non_blocking=True)\\n        yb = yb.to(device, non_blocking=True)\\n        logits = model(xb)\\n        if criterion is not None:\\n            total_loss += criterion(logits, yb).item() * xb.size(0)\\n        all_logits.append(logits.detach().cpu())\\n        all_targets.append(yb.detach().cpu())\\n    logits = torch.cat(all_logits, dim=0)\\n    targets = torch.cat(all_targets, dim=0)\\n    acc = accuracy(logits, targets)\\n    f1 = macro_f1(logits, targets, num_classes)\\n    avg_loss = total_loss / len(loader.dataset) if criterion is not None else float('nan')\\n    return {\"loss\": avg_loss, \"acc\": acc, \"macro_f1\": f1}\\n\\n# --------- Quantization Helpers ---------\\ndef static_quantize_cnn_submodule(model: nn.Module, calibration_loader, calibration_batches: int = 16, per_channel: bool = True):\\n    \"\"\"Apply static PTQ to the CNN submodule only (Conv/BN/ReLU), keeping Transformer in float.\\n    This quantizes both weights and activations of CNN, using observers calibrated on a few batches.\"\"\"\\n    try:\\n        from torch.ao.quantization import get_default_qconfig, QConfig, default_per_channel_weight_observer, default_observer, prepare, convert\\n    except Exception:\\n        from torch.quantization import get_default_qconfig, QConfig, default_per_channel_weight_observer, default_observer, prepare, convert\\n\\n    model.eval()\\n    model.cpu()\\n    # Fuse Conv-BN-ReLU sequences in CNN\\n    if hasattr(model.cnn, 'fuse_model'):\\n        model.cnn.fuse_model()\\n    # Configure qconfig for CNN\\n    if per_channel:\\n        act_obs = default_observer\\n        w_obs = default_per_channel_weight_observer\\n        qconfig = QConfig(activation=act_obs, weight=w_obs)\\n        model.cnn.qconfig = qconfig\\n    else:\\n        model.cnn.qconfig = get_default_qconfig('fbgemm')\\n    prepare(model.cnn, inplace=True)\\n    # Calibration\\n    model.eval()\\n    seen = 0\\n    with torch.no_grad():\\n        for xb, _ in calibration_loader:\\n            xb = xb.cpu()\\n            _ = model.cnn(xb)\\n            seen += 1\\n            if seen >= calibration_batches:\\n                break\\n    convert(model.cnn, inplace=True)\\n    return model\\n\\ndef dynamic_quantize_linear(model: nn.Module, dtype=torch.qint8):\\n    try:\\n        from torch.ao.quantization import quantize_dynamic\\n    except Exception:\\n        from torch.quantization import quantize_dynamic\\n    # Quantize all Linear layers dynamically (Transformer + classifier)\\n    qmodel = quantize_dynamic(model, {nn.Linear}, dtype=dtype)\\n    return qmodel\\n\\n# --------- Main Training Function ---------\\ndef train_model(\\n    X_train: torch.Tensor,\\n    y_train: torch.Tensor,\\n    X_val: torch.Tensor,\\n    y_val: torch.Tensor,\\n    device: torch.device,\\n    # Optimization\\n    lr: float = 3e-4,\\n    batch_size: int = 128,\\n    epochs: int = 20,\\n    weight_decay: float = 1e-5,\\n    dropout: float = 0.1,\\n    grad_clip: float = 0.5,\\n    # CNN Front-End\\n    cnn_base_channels: int = 16,\\n    kernel_sizes = (5, 11, 23),\\n    stride: int = 2,\\n    se_reduction: int = 4,\\n    # Transformer\\n    transformer_d_model: int = 64,\\n    transformer_nhead: int = 4,\\n    transformer_ffn_dim: int = 128,\\n    transformer_layers: int = 2,\\n    # Loss\\n    loss_type: str = 'focal',\\n    focal_gamma: float = 2.0,\\n    label_smoothing: float = 0.0,\\n    # Quantization hyperparams\\n    quantization_bits: int = 8,\\n    quantize_weights: bool = True,\\n    quantize_activations: bool = True,\\n    calibration_batches: int = 16,\\n    per_channel: bool = True\\n) -> Dict[str, Any]:\\n    \"\"\"Train a CNN+Transformer hybrid for 5-class classification on ECG beats and return a quantized model + metrics.\\n\\n    Inputs are torch tensors: X_* shape (N, 1000, 2), y_* shape (N,) with class indices [0..4].\\n    \"\"\"\\n    torch.backends.cudnn.benchmark = True\\n    num_classes = 5\\n\\n    # Dataloaders\\n    train_ds = TensorDataset(X_train, y_train)\\n    val_ds = TensorDataset(X_val, y_val)\\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=(device.type == 'cuda'))\\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda'))\\n\\n    # Model\\n    assert transformer_d_model % transformer_nhead == 0, \"d_model must be divisible by nhead\"\\n    model = CNNTransformerClassifier(\\n        in_channels=2, num_classes=num_classes,\\n        cnn_base_channels=cnn_base_channels, kernel_sizes=kernel_sizes, stride=stride,\\n        transformer_d_model=transformer_d_model, transformer_nhead=transformer_nhead,\\n        transformer_ffn_dim=transformer_ffn_dim, transformer_layers=transformer_layers,\\n        dropout=dropout, se_reduction=se_reduction\\n    ).to(device)\\n\\n    # Parameter count check (must be <= 256K)\\n    param_count = sum(p.numel() for p in model.parameters())\\n    if param_count > 256_000:\\n        raise ValueError(f\"Model has {param_count} parameters, which exceeds the 256K limit. Reduce hyperparameters.\")\\n\\n    # Loss\\n    class_weights = compute_class_weights(y_train.cpu(), num_classes=num_classes).to(device)\\n    if loss_type == 'focal':\\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights, reduction='mean', label_smoothing=label_smoothing)\\n    elif loss_type == 'weighted_ce':\\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)\\n    else:\\n        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\\n\\n    # Optimizer\\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\\n\\n    # Training loop\\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": [], \"val_macro_f1\": []}\\n    for epoch in range(epochs):\\n        train_loss = train_one_epoch(model, train_loader, optimizer, device, criterion, grad_clip=grad_clip)\\n        val_metrics = evaluate_model(model, val_loader, device, num_classes=num_classes, criterion=criterion)\\n        history[\"train_loss\"].append(train_loss)\\n        history[\"val_loss\"].append(val_metrics[\"loss\"])\\n        history[\"val_acc\"].append(val_metrics[\"acc\"])\\n        history[\"val_macro_f1\"].append(val_metrics[\"macro_f1\"])\\n\\n    pre_quant_metrics = {\\n        \"train_loss\": history[\"train_loss\"][-1],\\n        \"val_loss\": history[\"val_loss\"][-1],\\n        \"val_acc\": history[\"val_acc\"][-1],\\n        \"val_macro_f1\": history[\"val_macro_f1\"][-1],\\n        \"param_count\": int(param_count)\\n    }\\n\\n    # --------- Post-Training Quantization ---------\\n    # Always quantize on CPU for torch.ao quantization APIs\\n    quantized_model = model\\n\\n    if quantization_bits == 32 or (not quantize_weights and not quantize_activations):\\n        # No quantization\\n        quantized_model = model.cpu()\\n        q_device = torch.device('cpu')\\n    elif quantization_bits == 16:\\n        # FP16 weight casting (simple and portable); activations remain float32 unless autocast used\\n        quantized_model = model.cpu()\\n        quantized_model.half()\\n        q_device = torch.device('cpu')\\n    else:\\n        # INT8 path\\n        if quantize_weights and quantize_activations:\\n            # Static quantize CNN (weights + activations), dynamic quantize Linear elsewhere\\n            # 1) Static PTQ on CNN with calibration\\n            cpu_train_loader = DataLoader(train_ds, batch_size=min(256, batch_size), shuffle=False, num_workers=0)\\n            quantized_model = static_quantize_cnn_submodule(quantized_model, cpu_train_loader, calibration_batches=calibration_batches, per_channel=per_channel)\\n            # 2) Dynamic quantization on Linear layers (Transformer + Head)\\n            quantized_model = dynamic_quantize_linear(quantized_model, dtype=torch.qint8)\\n            q_device = torch.device('cpu')\\n        elif quantize_weights and not quantize_activations:\\n            # Dynamic quantization on Linear layers only\\n            quantized_model = dynamic_quantize_linear(quantized_model, dtype=torch.qint8)\\n            quantized_model.cpu()\\n            q_device = torch.device('cpu')\\n        elif (not quantize_weights) and quantize_activations:\\n            # Static activations without weight quantization is uncommon; approximate by static CNN only\\n            cpu_train_loader = DataLoader(train_ds, batch_size=min(256, batch_size), shuffle=False, num_workers=0)\\n            quantized_model = static_quantize_cnn_submodule(quantized_model, cpu_train_loader, calibration_batches=calibration_batches, per_channel=per_channel)\\n            q_device = torch.device('cpu')\\n        else:\\n            quantized_model = model.cpu()\\n            q_device = torch.device('cpu')\\n\\n    # Evaluate the quantized model on CPU\\n    q_val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\\n    q_metrics = evaluate_model(quantized_model.to(q_device), q_val_loader, q_device, num_classes=num_classes, criterion=None)\\n    final_metrics = {\\n        \"pre_quant_train_loss\": pre_quant_metrics[\"train_loss\"],\\n        \"pre_quant_val_loss\": pre_quant_metrics[\"val_loss\"],\\n        \"pre_quant_val_acc\": pre_quant_metrics[\"val_acc\"],\\n        \"pre_quant_val_macro_f1\": pre_quant_metrics[\"val_macro_f1\"],\\n        \"param_count\": pre_quant_metrics[\"param_count\"],\\n        \"post_quant_val_acc\": q_metrics[\"acc\"],\\n        \"post_quant_val_macro_f1\": q_metrics[\"macro_f1\"]\\n    }\\n\\n    return {\\n        \"model\": quantized_model,\\n        \"metrics\": final_metrics\\n    }\\n",
  "bo_config": {
    "lr": {
      "default": 0.0003,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 128,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128,
        256
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "weight_decay": {
      "default": 1e-05,
      "type": "Real",
      "low": 1e-07,
      "high": 0.001,
      "prior": "log-uniform"
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "grad_clip": {
      "default": 0.5,
      "type": "Real",
      "low": 0.0,
      "high": 1.0
    },
    "cnn_base_channels": {
      "default": 16,
      "type": "Integer",
      "low": 8,
      "high": 48
    },
    "kernel_sizes": {
      "default": [
        5,
        11,
        23
      ],
      "type": "Categorical",
      "categories": [
        [
          5,
          11,
          23
        ],
        [
          7,
          15,
          31
        ],
        [
          3,
          9,
          21
        ]
      ]
    },
    "stride": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 2
    },
    "se_reduction": {
      "default": 4,
      "type": "Integer",
      "low": 4,
      "high": 16
    },
    "transformer_d_model": {
      "default": 64,
      "type": "Integer",
      "low": 32,
      "high": 128
    },
    "transformer_nhead": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        2,
        4,
        8
      ]
    },
    "transformer_ffn_dim": {
      "default": 128,
      "type": "Integer",
      "low": 64,
      "high": 256
    },
    "transformer_layers": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 4
    },
    "loss_type": {
      "default": "focal",
      "type": "Categorical",
      "categories": [
        "focal",
        "weighted_ce",
        "ce"
      ]
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 1.0,
      "high": 3.0
    },
    "label_smoothing": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 0.1
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "calibration_batches": {
      "default": 16,
      "type": "Integer",
      "low": 4,
      "high": 64
    },
    "per_channel": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "confidence": 0.9,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758324447,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}