{
  "model_name": "Tiny1D-ViT-SE-Transformer-MITBIH",
  "training_code": "import io\nimport math\nimport copy\nfrom typing import Dict, Any, Tuple\n\ndef train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import Dataset, DataLoader\n    from torch.optim import AdamW\n    from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n\n    # -----------------------\n    # Device handling (string or torch.device)\n    # -----------------------\n    device = torch.device(device)\n    torch.backends.cudnn.benchmark = True\n\n    # -----------------------\n    # Hyperparameters with defaults\n    # -----------------------\n    hp: Dict[str, Any] = {\n        'seq_len': 1000,\n        'in_chans': 2,\n        'num_classes': 5,\n        'patch_size': 20,                    # must divide seq_len\n        'd_model': 48,\n        'n_heads': 2,\n        'num_layers': 2,\n        'mlp_ratio': 2.0,\n        'dropout': 0.1,\n        'lr': 1e-3,\n        'batch_size': 128,\n        'epochs': 20,\n        'weight_decay': 1e-4,\n        'label_smoothing': 0.05,\n        'use_focal_loss': False,\n        'focal_gamma': 2.0,\n        'grad_clip_norm': 1.0,\n        'scheduler': 'cosine',               # 'none' | 'cosine' | 'onecycle'\n        'class_weights': 'auto',             # 'none' | 'auto'\n        'seed': 42,\n        # Quantization\n        'quantization_bits': 8,              # 8 | 16 | 32\n        'quantize_weights': True,\n        'quantize_activations': True,\n    }\n    # Override with user kwargs\n    hp.update(kwargs or {})\n\n    # Validate and normalize some hp\n    assert hp['seq_len'] == 1000, 'This implementation assumes sequence length 1000.'\n    assert hp['in_chans'] == 2, 'This implementation assumes 2 input leads/channels.'\n    assert hp['num_classes'] == 5, 'This implementation assumes 5-class classification.'\n    # patch_size must divide seq_len\n    if hp['seq_len'] % hp['patch_size'] != 0:\n        valid = [1,2,4,5,8,10,20,25,40,50,100,125,200,250,500,1000]\n        # choose nearest valid (prefer 20 if possible)\n        if 20 in valid:\n            hp['patch_size'] = 20\n        else:\n            # fallback to greatest valid divisor less than seq_len/10 to avoid too many tokens\n            hp['patch_size'] = max([d for d in valid if hp['seq_len'] % d == 0 and d <= 50])\n    # ensure heads divides d_model\n    if hp['d_model'] % max(1, hp['n_heads']) != 0:\n        # adjust heads to 1 if mismatch\n        hp['n_heads'] = 1\n\n    # Set seeds\n    def set_seed(seed: int):\n        import random\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    set_seed(int(hp['seed']))\n\n    # -----------------------\n    # Dataset\n    # -----------------------\n    class ECGDataset(Dataset):\n        def __init__(self, X: torch.Tensor, y: torch.Tensor):\n            assert isinstance(X, torch.Tensor) and isinstance(y, torch.Tensor)\n            self.X = X\n            self.y = y\n            assert self.X.shape[0] == self.y.shape[0]\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            # Accept (seq, ch) or (ch, seq); normalize to (ch, seq)\n            if x.dim() == 2:\n                if x.shape[0] == hp['seq_len'] and x.shape[1] == hp['in_chans']:\n                    x = x.transpose(0,1)  # (1000,2) -> (2,1000)\n                elif x.shape[0] == hp['in_chans'] and x.shape[1] == hp['seq_len']:\n                    pass\n                else:\n                    raise ValueError(f\"Unexpected sample shape {tuple(x.shape)}, expected (2,1000) or (1000,2)\")\n            elif x.dim() == 1:\n                raise ValueError('Input must be 2D per sample, got 1D')\n            x = x.to(dtype=torch.float32)\n            y = self.y[idx].to(dtype=torch.long)\n            return x, y\n\n    train_ds = ECGDataset(X_train, y_train)\n    val_ds = ECGDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=int(hp['batch_size']), shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=int(hp['batch_size']), shuffle=False, num_workers=0, pin_memory=False)\n\n    # -----------------------\n    # Model: Tiny 1D ViT with SE channel attention\n    # -----------------------\n    class SE1D(nn.Module):\n        def __init__(self, channels: int, reduction: int = 1):\n            super().__init__()\n            hidden = max(1, channels // reduction)\n            self.avg = nn.AdaptiveAvgPool1d(1)\n            self.fc1 = nn.Linear(channels, hidden)\n            self.fc2 = nn.Linear(hidden, channels)\n        def forward(self, x):  # x: (B, C, T)\n            b, c, t = x.shape\n            s = self.avg(x).view(b, c)\n            s = F.relu(self.fc1(s))\n            s = torch.sigmoid(self.fc2(s)).view(b, c, 1)\n            return x * s\n\n    class PatchEmbed1D(nn.Module):\n        def __init__(self, in_chans, d_model, patch_size):\n            super().__init__()\n            self.proj = nn.Conv1d(in_chans, d_model, kernel_size=patch_size, stride=patch_size, bias=True)\n        def forward(self, x):  # x: (B, C, T)\n            x = self.proj(x)  # (B, d_model, N)\n            x = x.transpose(1, 2)  # (B, N, d_model)\n            return x\n\n    class MHSelfAttention(nn.Module):\n        def __init__(self, d_model, n_heads, attn_drop=0.0, proj_drop=0.0):\n            super().__init__()\n            assert d_model % n_heads == 0\n            self.d_model = d_model\n            self.n_heads = n_heads\n            self.head_dim = d_model // n_heads\n            self.scale = self.head_dim ** -0.5\n            self.qkv = nn.Linear(d_model, d_model * 3, bias=True)\n            self.attn_drop = nn.Dropout(attn_drop)\n            self.proj = nn.Linear(d_model, d_model)\n            self.proj_drop = nn.Dropout(proj_drop)\n        def forward(self, x):  # x: (B, N, d_model)\n            B, N, C = x.shape\n            qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n            q, k, v = qkv[0], qkv[1], qkv[2]  # (B, heads, N, head_dim)\n            attn = (q @ k.transpose(-2, -1)) * self.scale\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            out = attn @ v  # (B, heads, N, head_dim)\n            out = out.transpose(1, 2).reshape(B, N, C)\n            out = self.proj(out)\n            out = self.proj_drop(out)\n            return out\n\n    class TransformerBlock(nn.Module):\n        def __init__(self, d_model, n_heads, mlp_ratio=2.0, dropout=0.1):\n            super().__init__()\n            self.norm1 = nn.LayerNorm(d_model)\n            self.attn = MHSelfAttention(d_model, n_heads, attn_drop=dropout, proj_drop=dropout)\n            self.norm2 = nn.LayerNorm(d_model)\n            hidden = int(d_model * mlp_ratio)\n            self.mlp = nn.Sequential(\n                nn.Linear(d_model, hidden),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden, d_model),\n                nn.Dropout(dropout),\n            )\n        def forward(self, x):\n            x = x + self.attn(self.norm1(x))\n            x = x + self.mlp(self.norm2(x))\n            return x\n\n    class TinyViT1D(nn.Module):\n        def __init__(self, seq_len, in_chans, num_classes, patch_size, d_model, n_heads, num_layers, mlp_ratio, dropout):\n            super().__init__()\n            self.seq_len = seq_len\n            self.patch_size = patch_size\n            self.num_patches = seq_len // patch_size\n            self.se = SE1D(in_chans, reduction=1)\n            self.patch_embed = PatchEmbed1D(in_chans, d_model, patch_size)\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n            self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, d_model))\n            self.pos_drop = nn.Dropout(dropout)\n            self.blocks = nn.ModuleList([\n                TransformerBlock(d_model, n_heads, mlp_ratio, dropout) for _ in range(num_layers)\n            ])\n            self.norm = nn.LayerNorm(d_model)\n            self.head = nn.Linear(d_model, num_classes)\n            self._init_weights()\n        def _init_weights(self):\n            nn.init.trunc_normal_(self.pos_embed, std=0.02)\n            nn.init.trunc_normal_(self.cls_token, std=0.02)\n            for m in self.modules():\n                if isinstance(m, nn.Linear):\n                    nn.init.trunc_normal_(m.weight, std=0.02)\n                    if m.bias is not None:\n                        nn.init.zeros_(m.bias)\n                elif isinstance(m, nn.Conv1d):\n                    nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n                    if m.bias is not None:\n                        nn.init.zeros_(m.bias)\n        def forward(self, x):  # x: (B, C, T)\n            x = self.se(x)\n            x = self.patch_embed(x)  # (B, N, d_model)\n            B, N, C = x.shape\n            cls_tok = self.cls_token.expand(B, -1, -1)  # (B,1,C)\n            x = torch.cat([cls_tok, x], dim=1)  # (B, N+1, C)\n            x = x + self.pos_embed\n            x = self.pos_drop(x)\n            for blk in self.blocks:\n                x = blk(x)\n            x = self.norm(x)\n            cls = x[:, 0]\n            logits = self.head(cls)\n            return logits\n\n    model = TinyViT1D(\n        seq_len=int(hp['seq_len']),\n        in_chans=int(hp['in_chans']),\n        num_classes=int(hp['num_classes']),\n        patch_size=int(hp['patch_size']),\n        d_model=int(hp['d_model']),\n        n_heads=int(hp['n_heads']),\n        num_layers=int(hp['num_layers']),\n        mlp_ratio=float(hp['mlp_ratio']),\n        dropout=float(hp['dropout'])\n    ).to(device)\n\n    # -----------------------\n    # Loss (weighted CE or Focal)\n    # -----------------------\n    class_weights_tensor = None\n    if str(hp['class_weights']).lower() == 'auto':\n        with torch.no_grad():\n            classes = int(hp['num_classes'])\n            counts = torch.bincount(y_train.to(torch.long), minlength=classes).to(torch.float32)\n            counts = torch.clamp(counts, min=1.0)\n            inv_freq = 1.0 / counts\n            class_weights_tensor = (inv_freq / inv_freq.sum() * classes).to(device)\n    elif str(hp['class_weights']).lower() == 'none':\n        class_weights_tensor = None\n\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None):\n            super().__init__()\n            self.gamma = gamma\n            self.weight = weight\n        def forward(self, logits, target):\n            logp = F.log_softmax(logits, dim=-1)\n            ce = F.nll_loss(logp, target, weight=self.weight, reduction='none')\n            p = torch.exp(-ce)\n            loss = ((1 - p) ** self.gamma) * ce\n            return loss.mean()\n\n    if bool(hp['use_focal_loss']):\n        criterion = FocalLoss(gamma=float(hp['focal_gamma']), weight=class_weights_tensor)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=float(hp['label_smoothing']))\n\n    # -----------------------\n    # Optimizer and Scheduler\n    # -----------------------\n    optimizer = AdamW(model.parameters(), lr=float(hp['lr']), weight_decay=float(hp['weight_decay']))\n\n    steps_per_epoch = max(1, len(train_loader))\n    scheduler = None\n    if hp['scheduler'] == 'cosine':\n        scheduler = CosineAnnealingLR(optimizer, T_max=int(hp['epochs']))\n    elif hp['scheduler'] == 'onecycle':\n        scheduler = OneCycleLR(optimizer, max_lr=float(hp['lr']), epochs=int(hp['epochs']), steps_per_epoch=steps_per_epoch)\n\n    # -----------------------\n    # Training Loop\n    # -----------------------\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(int(hp['epochs'])):\n        model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            # Ensure tensors are on the same device as model\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if float(hp['grad_clip_norm']) > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(hp['grad_clip_norm']))\n            optimizer.step()\n\n            running_loss += loss.detach().item() * xb.size(0)\n\n            if isinstance(scheduler, OneCycleLR):\n                scheduler.step()\n\n        epoch_train_loss = running_loss / len(train_loader.dataset)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                preds = torch.argmax(logits, dim=-1)\n                correct += (preds == yb).sum().item()\n                total += yb.numel()\n        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        if isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{int(hp['epochs'])} | train_loss={epoch_train_loss:.5f} | val_loss={epoch_val_loss:.5f} | val_acc={epoch_val_acc:.4f}\")\n\n    # -----------------------\n    # Post-Training Quantization (on CPU)\n    # -----------------------\n    model_cpu = copy.deepcopy(model).to('cpu').eval()\n\n    quantization_bits = int(hp['quantization_bits'])\n    quantize_weights = bool(hp['quantize_weights'])\n    quantize_activations = bool(hp['quantize_activations'])\n\n    qmodel = model_cpu\n    # Dynamic quantization (weights + dynamic activations quant in Linear) for 8-bit\n    if quantize_weights and quantization_bits == 8:\n        try:\n            qmodel = torch.ao.quantization.quantize_dynamic(\n                model_cpu,\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n        except Exception as e:\n            print(f\"Warning: int8 dynamic quantization failed with error: {e}. Falling back to FP32 CPU model.\")\n            qmodel = model_cpu\n    elif quantize_weights and quantization_bits == 16:\n        # FP16 cast for weights and activations\n        qmodel = model_cpu.half()\n    elif quantize_weights and quantization_bits == 32:\n        qmodel = model_cpu  # No change\n    else:\n        # No weight quantization requested\n        qmodel = model_cpu\n\n    # Utility: compute model size (state_dict) in bytes\n    def get_model_size_bytes(m: nn.Module) -> int:\n        buf = io.BytesIO()\n        torch.save(m.state_dict(), buf)\n        return buf.getbuffer().nbytes\n\n    model_size_bytes = get_model_size_bytes(qmodel)\n\n    # Enforce <= 256KB by escalating to int8 dynamic quantization if needed\n    limit_bytes = 256 * 1024\n    if model_size_bytes > limit_bytes:\n        try:\n            qmodel = torch.ao.quantization.quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)\n            model_size_bytes = get_model_size_bytes(qmodel)\n            if model_size_bytes > limit_bytes:\n                # As a last resort, cast to fp16\n                qmodel = model_cpu.half()\n                model_size_bytes = get_model_size_bytes(qmodel)\n        except Exception as e:\n            print(f\"Warning: size constraint handling encountered error: {e}\")\n            # keep whatever we have\n            model_size_bytes = get_model_size_bytes(qmodel)\n\n    # Final safety check\n    if model_size_bytes > limit_bytes:\n        print(f\"Warning: Final model size {model_size_bytes} bytes exceeds 256KB limit.\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'model_size_bytes': int(model_size_bytes)\n    }\n\n    return qmodel, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 128,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128,
        256
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 60
    },
    "d_model": {
      "default": 48,
      "type": "Integer",
      "low": 32,
      "high": 96
    },
    "n_heads": {
      "default": 2,
      "type": "Categorical",
      "categories": [
        1,
        2,
        3,
        4
      ]
    },
    "num_layers": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 4
    },
    "mlp_ratio": {
      "default": 2.0,
      "type": "Real",
      "low": 1.5,
      "high": 4.0
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "patch_size": {
      "default": 20,
      "type": "Categorical",
      "categories": [
        10,
        20,
        25,
        40,
        50,
        100
      ]
    },
    "label_smoothing": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "use_focal_loss": {
      "default": false,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 1.0,
      "high": 3.0
    },
    "grad_clip_norm": {
      "default": 1.0,
      "type": "Real",
      "low": 0.0,
      "high": 2.0
    },
    "scheduler": {
      "default": "cosine",
      "type": "Categorical",
      "categories": [
        "none",
        "cosine",
        "onecycle"
      ]
    },
    "class_weights": {
      "default": "auto",
      "type": "Categorical",
      "categories": [
        "none",
        "auto"
      ]
    },
    "seed": {
      "default": 42,
      "type": "Integer",
      "low": 0,
      "high": 65535
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "confidence": 0.9,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758672582,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}