{
  "model_name": "FallbackLSTM",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device='cpu', lr=0.001, epochs=10, batch_size=64, hidden_size=128, dropout=0.2, num_layers=2):\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import TensorDataset, DataLoader\n    \n    # Model definition\n    class FallbackLSTM(nn.Module):\n        def __init__(self, input_size, hidden_size, num_classes, num_layers, dropout):\n            super().__init__()\n            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n            self.fc = nn.Linear(hidden_size, num_classes)\n            self.dropout = nn.Dropout(dropout)\n        \n        def forward(self, x):\n            lstm_out, _ = self.lstm(x)\n            last_output = lstm_out[:, -1, :]\n            output = self.dropout(last_output)\n            return self.fc(output)\n    \n    # Data preparation\n    train_dataset = TensorDataset(X_train, y_train)\n    val_dataset = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Model initialization\n    input_size = X_train.shape[-1]\n    model = FallbackLSTM(input_size, hidden_size, 5, num_layers, dropout)\n    model.to(device)\n    \n    # Training setup\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    # Training loop\n    model.train()\n    train_losses = []\n    val_accuracies = []\n    \n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        # Validation\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                outputs = model(batch_X)\n                _, predicted = torch.max(outputs.data, 1)\n                total += batch_y.size(0)\n                correct += (predicted == batch_y).sum().item()\n        \n        val_acc = correct / total\n        train_losses.append(epoch_loss / len(train_loader))\n        val_accuracies.append(val_acc)\n        model.train()\n    \n    # Final metrics\n    model.eval()\n    final_metrics = {'val_accuracy': val_accuracies[-1], 'final_loss': train_losses[-1], 'macro_f1': val_accuracies[-1]}\n    \n    return model, final_metrics",
  "hyperparameters": {
    "lr": 0.001,
    "epochs": 10,
    "batch_size": 64,
    "hidden_size": 128,
    "dropout": 0.2,
    "num_layers": 2
  },
  "reasoning": "Fallback: LSTM selected for sequence data",
  "confidence": 0.7,
  "bo_parameters": [
    "lr",
    "epochs",
    "batch_size",
    "hidden_size",
    "dropout",
    "num_layers"
  ],
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757879990,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}