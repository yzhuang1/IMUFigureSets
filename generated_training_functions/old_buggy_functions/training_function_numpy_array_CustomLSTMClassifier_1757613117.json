{
  "model_name": "CustomLSTMClassifier",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device='cpu', lr=0.001, epochs=10, batch_size=64, hidden_size=128, dropout=0.2, num_layers=2):\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import TensorDataset, DataLoader\n    \n    # Model definition\n    class CustomModel(nn.Module):\n        def __init__(self, input_size, hidden_size, num_classes, num_layers, dropout):\n            super().__init__()\n            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n            self.fc = nn.Linear(hidden_size, num_classes)\n            self.dropout = nn.Dropout(dropout)\n        \n        def forward(self, x):\n            lstm_out, _ = self.lstm(x)\n            last_output = lstm_out[:, -1, :]\n            output = self.dropout(last_output)\n            return self.fc(output)\n    \n    # Data preparation\n    train_dataset = TensorDataset(X_train, y_train)\n    val_dataset = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Model initialization\n    input_size = X_train.shape[-1]\n    model = CustomModel(input_size, hidden_size, 5, num_layers, dropout)\n    model.to(device)\n    \n    # Training setup\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    # Training loop\n    model.train()\n    train_losses = []\n    val_accuracies = []\n    \n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        # Validation\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                outputs = model(batch_X)\n                _, predicted = torch.max(outputs.data, 1)\n                total += batch_y.size(0)\n                correct += (predicted == batch_y).sum().item()\n        \n        val_acc = correct / total\n        train_losses.append(epoch_loss / len(train_loader))\n        val_accuracies.append(val_acc)\n        model.train()\n    \n    # Final metrics\n    model.eval()\n    final_metrics = {'val_accuracy': val_accuracies[-1], 'final_loss': train_losses[-1]}\n    \n    return model, final_metrics",
  "hyperparameters": {
    "lr": 0.001,
    "epochs": 10,
    "batch_size": 64,
    "hidden_size": 128,
    "dropout": 0.2,
    "num_layers": 2
  },
  "reasoning": "Given that the data is sequence data, a LSTM (Long Short-Term Memory) model is appropriate as it can capture temporal dependencies in the data. The model is defined within the training function for flexibility and to ensure that the model architecture and hyperparameters are consistent. The model consists of a LSTM layer followed by a fully connected layer. Dropout is applied after the LSTM layer to prevent overfitting. The model is trained using the Adam optimizer and cross-entropy loss, which are common choices for multi-class classification tasks. The training function includes a training loop that updates the model parameters based on the calculated gradients, and a validation loop that calculates the model's accuracy on the validation data. The function returns the trained model and the final training loss and validation accuracy.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "epochs",
    "batch_size",
    "hidden_size",
    "dropout",
    "num_layers"
  ],
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757613117,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-4",
    "version": "1.0"
  }
}