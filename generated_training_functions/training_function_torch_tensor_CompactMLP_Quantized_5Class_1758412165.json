{
  "model_name": "CompactMLP_Quantized_5Class",
  "training_code": "import copy\nimport math\nfrom typing import Dict, Any, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.ao.quantization as tq\n\n\ndef train_model(\n    X_train: torch.Tensor,\n    y_train: torch.Tensor,\n    X_val: torch.Tensor,\n    y_val: torch.Tensor,\n    device: torch.device,\n    **kwargs: Any,\n) -> Tuple[nn.Module, Dict[str, Any]]:\n    \"\"\"\n    Train a compact MLP classifier for 5 classes on time-series shaped (T, C)=(1000,2) per sample.\n\n    Notes:\n    - Although 1D CNNs are common for MIT-BIH arrhythmia, here we use a compact MLP to ensure\n      robust post-training quantization support and meet the <=256K parameter constraint.\n    - Uses static int8 PTQ when quantize_activations=True and quantization_bits==8, otherwise\n      dynamic quantization for Linear layers (8-bit or float16 weight-only).\n    - DataLoader uses pin_memory=False per requirement.\n    \"\"\"\n\n    # ------------------------\n    # Hyperparameters (with defaults)\n    # ------------------------\n    hp = {\n        'lr': kwargs.get('lr', 1e-3),\n        'batch_size': kwargs.get('batch_size', 64),\n        'epochs': kwargs.get('epochs', 20),\n        'hidden_size': kwargs.get('hidden_size', 96),\n        'hidden_size2': kwargs.get('hidden_size2', 64),\n        'dropout': kwargs.get('dropout', 0.2),\n        'weight_decay': kwargs.get('weight_decay', 1e-4),\n        'scheduler_gamma': kwargs.get('scheduler_gamma', 0.95),\n        'clip_norm': kwargs.get('clip_norm', 0.0),\n        # Quantization controls\n        'quantization_bits': kwargs.get('quantization_bits', 8),            # {8,16,32}\n        'quantize_weights': kwargs.get('quantize_weights', True),           # bool\n        'quantize_activations': kwargs.get('quantize_activations', True),   # bool\n    }\n\n    assert isinstance(X_train, torch.Tensor) and isinstance(X_val, torch.Tensor), \"X_* must be torch tensors\"\n    assert isinstance(y_train, torch.Tensor) and isinstance(y_val, torch.Tensor), \"y_* must be torch tensors\"\n\n    num_classes = 5\n    time_len = 1000\n    num_channels = 2\n    input_dim = time_len * num_channels  # 2000\n\n    # ------------------------\n    # Dataset\n    # ------------------------\n    class TensorTimeSeriesDataset(Dataset):\n        def __init__(self, X: torch.Tensor, y: torch.Tensor):\n            super().__init__()\n            assert X.dim() == 3, \"Expected X shape (N, T, C) or (N, C, T)\"\n            self.X = X.float()\n            self.y = y.long()\n\n        def __len__(self):\n            return self.X.size(0)\n\n        def __getitem__(self, idx: int):\n            x = self.X[idx]\n            # Normalize layout to (C, T) before flatten\n            if x.dim() == 2:\n                if x.shape[0] == num_channels and x.shape[1] == time_len:\n                    # already (C, T)\n                    pass\n                elif x.shape[0] == time_len and x.shape[1] == num_channels:\n                    # (T, C) -> (C, T)\n                    x = x.transpose(0, 1)\n                else:\n                    # Fallback: enforce channels-first if possible\n                    if x.shape[-1] == num_channels:\n                        x = x.transpose(0, 1)\n            x = x.reshape(-1)  # flatten to 2000\n            y = self.y[idx]\n            return x, y\n\n    train_ds = TensorTimeSeriesDataset(X_train, y_train)\n    val_ds = TensorTimeSeriesDataset(X_val, y_val)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=hp['batch_size'],\n        shuffle=True,\n        num_workers=0,\n        pin_memory=False,  # per requirement\n        drop_last=False,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=max(1, hp['batch_size'] // 2),\n        shuffle=False,\n        num_workers=0,\n        pin_memory=False,\n        drop_last=False,\n    )\n\n    # ------------------------\n    # Model\n    # ------------------------\n    class CompactMLP(nn.Module):\n        def __init__(self, in_dim: int, h1: int, h2: int, p_drop: float, n_classes: int):\n            super().__init__()\n            # Quantization stubs enable static PTQ when prepared/converted\n            self.quant = tq.QuantStub()\n            self.dequant = tq.DeQuantStub()\n            self.fc1 = nn.Linear(in_dim, h1)\n            self.relu1 = nn.ReLU(inplace=True)\n            self.drop1 = nn.Dropout(p_drop)\n            self.fc2 = nn.Linear(h1, h2)\n            self.relu2 = nn.ReLU(inplace=True)\n            self.drop2 = nn.Dropout(p_drop)\n            self.fc3 = nn.Linear(h2, n_classes)\n\n        def forward(self, x: torch.Tensor) -> torch.Tensor:\n            x = self.quant(x)\n            x = self.fc1(x)\n            x = self.relu1(x)\n            x = self.drop1(x)\n            x = self.fc2(x)\n            x = self.relu2(x)\n            x = self.drop2(x)\n            x = self.fc3(x)\n            x = self.dequant(x)\n            return x\n\n    # Enforce parameter budget <= 256K\n    def param_count_for(h1: int, h2: int) -> int:\n        # fc1: in_dim*h1 + h1; fc2: h1*h2 + h2; fc3: h2*num_classes + num_classes\n        return (input_dim * h1 + h1) + (h1 * h2 + h2) + (h2 * num_classes + num_classes)\n\n    n_params_est = param_count_for(hp['hidden_size'], hp['hidden_size2'])\n    if n_params_est > 256_000:\n        raise ValueError(f\"Parameter budget exceeded: {n_params_est} > 256000. Reduce hidden_size/hidden_size2.\")\n\n    model = CompactMLP(\n        in_dim=input_dim,\n        h1=int(hp['hidden_size']),\n        h2=int(hp['hidden_size2']),\n        p_drop=float(hp['dropout']),\n        n_classes=num_classes,\n    ).to(device)\n\n    # ------------------------\n    # Optimizer & Scheduler\n    # ------------------------\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=hp['lr'], weight_decay=hp['weight_decay'])\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=hp['scheduler_gamma'])\n\n    # ------------------------\n    # Train loop\n    # ------------------------\n    def evaluate(m: nn.Module) -> Tuple[float, float]:\n        m.eval()\n        total = 0\n        correct = 0\n        val_loss = 0.0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device)\n                yb = yb.to(device)\n                logits = m(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += yb.size(0)\n        val_loss /= max(1, total)\n        val_acc = correct / max(1, total)\n        return val_loss, val_acc\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, int(hp['epochs']) + 1):\n        model.train()\n        running_loss = 0.0\n        n_seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device)\n            yb = yb.to(device)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if hp['clip_norm'] and hp['clip_norm'] > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=hp['clip_norm'])\n            optimizer.step()\n            running_loss += loss.item() * xb.size(0)\n            n_seen += xb.size(0)\n\n        train_loss = running_loss / max(1, n_seen)\n        val_loss, val_acc = evaluate(model)\n        scheduler.step()\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch}/{hp['epochs']} - train_loss: {train_loss:.4f} - val_loss: {val_loss:.4f} - val_acc: {val_acc:.4f}\")\n\n    # ------------------------\n    # Quantization (post-training)\n    # ------------------------\n    def quantize_model_post_training(m_float: nn.Module) -> nn.Module:\n        bits = int(hp['quantization_bits'])\n        qw = bool(hp['quantize_weights'])\n        qa = bool(hp['quantize_activations'])\n\n        # Always quantize on CPU\n        m_float_cpu = copy.deepcopy(m_float).to('cpu').eval()\n\n        # No quantization case\n        if bits == 32 or (not qw and not qa):\n            return m_float_cpu\n\n        # Dynamic quantization (weights-only)\n        if qa is False:\n            # 8-bit dynamic or float16 dynamic\n            modules = {nn.Linear}\n            if bits == 8 and qw:\n                qdtype = torch.qint8\n                q_model = tq.quantize_dynamic(m_float_cpu, modules, dtype=qdtype)\n                return q_model\n            elif bits == 16 and qw:\n                # Uses float16 weights for Linear layers, activations remain fp32\n                q_model = tq.quantize_dynamic(m_float_cpu, modules, dtype=torch.float16)\n                return q_model\n            else:\n                # Unsupported combo -> return float model\n                print(\"[Quantization] Requested config not supported for dynamic quantization. Returning FP32 model.\")\n                return m_float_cpu\n\n        # Static PTQ for activations (int8 path)\n        if qa and bits == 8:\n            try:\n                torch.backends.quantized.engine = 'fbgemm'  # x86 backend\n                m_ptq = copy.deepcopy(m_float_cpu)\n                m_ptq.qconfig = tq.get_default_qconfig(torch.backends.quantized.engine)\n                tq.prepare(m_ptq, inplace=True)\n\n                # Calibration: run a few batches through the prepared model\n                calib_loader = DataLoader(\n                    train_ds,\n                    batch_size=min(256, hp['batch_size']),\n                    shuffle=False,\n                    num_workers=0,\n                    pin_memory=False,\n                    drop_last=False,\n                )\n                with torch.no_grad():\n                    for i, (xb, _) in enumerate(calib_loader):\n                        m_ptq(xb)  # CPU tensors\n                        if i >= 10:\n                            break\n                tq.convert(m_ptq, inplace=True)\n                return m_ptq\n            except Exception as e:\n                print(f\"[Quantization] Static PTQ failed ({e}). Falling back to dynamic weight-only INT8.\")\n                try:\n                    return tq.quantize_dynamic(m_float_cpu, {nn.Linear}, dtype=torch.qint8)\n                except Exception as e2:\n                    print(f\"[Quantization] Dynamic quantization fallback failed ({e2}). Returning FP32 model.\")\n                    return m_float_cpu\n\n        # If requested qa with bits==16: not supported -> fallback to weight-only float16\n        if qa and bits == 16:\n            print(\"[Quantization] INT16 activations not supported in PTQ; applying float16 dynamic weight-only quantization.\")\n            try:\n                return tq.quantize_dynamic(m_float_cpu, {nn.Linear}, dtype=torch.float16)\n            except Exception:\n                return m_float_cpu\n\n        # Default fallback\n        return m_float_cpu\n\n    quantized_model = quantize_model_post_training(model)\n\n    # Safety: assert parameter budget (same as float model)\n    total_params = sum(p.numel() for p in quantized_model.parameters())\n    if total_params > 256_000:\n        raise RuntimeError(f\"Quantized model parameter count exceeds limit: {total_params} > 256000\")\n\n    metrics = {\n        'train_losses': [float(x) for x in train_losses],\n        'val_losses': [float(x) for x in val_losses],\n        'val_acc': [float(x) for x in val_accs],\n        'num_parameters': int(total_params),\n        'final_val_acc': float(val_accs[-1]) if len(val_accs) > 0 else None,\n        'best_val_acc': float(max(val_accs)) if len(val_accs) > 0 else None,\n        'quantization_bits': int(hp['quantization_bits']),\n        'quantize_weights': bool(hp['quantize_weights']),\n        'quantize_activations': bool(hp['quantize_activations'])\n    }\n\n    return quantized_model, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 64,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 100
    },
    "hidden_size": {
      "default": 96,
      "type": "Integer",
      "low": 32,
      "high": 120
    },
    "hidden_size2": {
      "default": 64,
      "type": "Integer",
      "low": 16,
      "high": 80
    },
    "dropout": {
      "default": 0.2,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "scheduler_gamma": {
      "default": 0.95,
      "type": "Real",
      "low": 0.85,
      "high": 0.999
    },
    "clip_norm": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 5.0
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "confidence": 0.9,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758412165,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}