{
  "model_name": "TinyCAT-ECG-1D",
  "training_code": "import math\nfrom typing import Dict, Any, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\n\nclass SinusoidalPositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 2048, dropout: float = 0.0):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (B, T, C)\n        T = x.size(1)\n        x = x + self.pe[:, :T, :]\n        return self.dropout(x)\n\n\nclass TinyCATECG(nn.Module):\n    def __init__(\n        self,\n        in_channels: int = 2,\n        num_classes: int = 5,\n        hidden_size: int = 64,\n        tf_layers: int = 2,\n        n_heads: int = 4,\n        ff_mult: int = 2,  # dim_feedforward = ff_mult * hidden_size\n        dropout: float = 0.2,\n        patch_size: int = 5,\n    ):\n        super().__init__()\n        # CNN stem for local morphology (lightweight)\n        self.stem = nn.Sequential(\n            nn.Conv1d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm1d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(32, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm1d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2, bias=False),\n            nn.BatchNorm1d(128),\n            nn.ReLU(inplace=True),\n        )\n        # Patch embedding along time dimension\n        self.patch_embed = nn.Conv1d(128, hidden_size, kernel_size=patch_size, stride=patch_size, padding=0, bias=True)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_size,\n            nhead=n_heads,\n            dim_feedforward=ff_mult * hidden_size,\n            dropout=dropout,\n            batch_first=True,\n            activation='gelu',\n            norm_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=tf_layers)\n        self.posenc = SinusoidalPositionalEncoding(hidden_size, dropout=dropout)\n        self.head = nn.Sequential(\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, num_classes),\n        )\n        self.in_channels = in_channels\n        self.patch_size = patch_size\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Accept (B, C, L) or (B, L, C). Ensure channel-first for Conv1d.\n        if x.dim() != 3:\n            raise ValueError(f\"Expected input with 3 dims (B, C, L) or (B, L, C), got {x.shape}\")\n        if x.shape[1] != self.in_channels and x.shape[2] == self.in_channels:\n            x = x.transpose(1, 2)\n        elif x.shape[1] != self.in_channels:\n            raise ValueError(f\"Input has wrong channel dimension. Expected {self.in_channels} channels, got shape {x.shape}\")\n\n        x = self.stem(x)  # (B, 128, L')\n        # Ensure length is divisible by patch_size (pad right if needed)\n        L = x.shape[-1]\n        rem = L % self.patch_size\n        if rem != 0:\n            pad = self.patch_size - rem\n            x = F.pad(x, (0, pad))\n        # Patch embedding to tokens\n        x = self.patch_embed(x)  # (B, hidden, T)\n        x = x.transpose(1, 2)    # (B, T, hidden)\n        x = self.posenc(x)\n        x = self.transformer(x)  # (B, T, hidden)\n        x = x.mean(dim=1)        # global average over tokens\n        logits = self.head(x)    # (B, num_classes)\n        return logits\n\n\nclass FocalCrossEntropy(nn.Module):\n    def __init__(self, weight: torch.Tensor = None, gamma: float = 0.0, label_smoothing: float = 0.0):\n        super().__init__()\n        self.register_buffer('weight', weight if weight is not None else None)\n        self.gamma = gamma\n        self.label_smoothing = label_smoothing\n\n    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        ce = F.cross_entropy(logits, target, weight=self.weight, reduction='none', label_smoothing=self.label_smoothing)\n        if self.gamma > 0.0:\n            pt = torch.exp(-ce)  # pt = softmax probability of the true class\n            loss = ((1 - pt) ** self.gamma) * ce\n        else:\n            loss = ce\n        return loss.mean()\n\n\ndef _class_weights_from_labels(y: torch.Tensor, num_classes: int) -> torch.Tensor:\n    # y: (N,) on any device; returns weights on the same device\n    counts = torch.bincount(y.detach().to('cpu'), minlength=num_classes).float()\n    counts = counts.clamp(min=1.0)\n    inv_freq = counts.sum() / (counts * num_classes)\n    # Normalize to mean 1.0 for stability\n    inv_freq = inv_freq / inv_freq.mean()\n    return inv_freq.to(y.device)\n\n\ndef _update_confmat(confmat: torch.Tensor, preds: torch.Tensor, targets: torch.Tensor) -> None:\n    # confmat: (C, C) on CPU; preds/targets on CPU\n    K = confmat.size(0)\n    idx = targets * K + preds\n    binc = torch.bincount(idx, minlength=K*K)\n    confmat += binc.view(K, K)\n\n\ndef _f1_from_confmat(confmat: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    # confmat: (C, C) on CPU\n    tp = confmat.diag().float()\n    fp = confmat.sum(0).float() - tp\n    fn = confmat.sum(1).float() - tp\n    denom = (2*tp + fp + fn).clamp(min=1e-12)\n    f1 = (2*tp) / denom\n    precision = tp / (tp + fp).clamp(min=1e-12)\n    recall = tp / (tp + fn).clamp(min=1e-12)\n    return f1, precision, recall\n\n\ndef train_model(X_train: torch.Tensor,\n                y_train: torch.Tensor,\n                X_val: torch.Tensor,\n                y_val: torch.Tensor,\n                device: torch.device,\n                **hyperparams) -> Tuple[nn.Module, Dict[str, Any]]:\n    \"\"\"\n    Train a lightweight CNN+Transformer (TinyCAT-ECG) for 5-class classification.\n\n    Args:\n        X_train, y_train, X_val, y_val: PyTorch tensors. X can be (N, C, L) or (N, L, C).\n        device: torch.device to run on.\n        **hyperparams: lr, batch_size, epochs, hidden_size, dropout, tf_layers, n_heads, ff_mult, weight_decay, gamma, label_smoothing, num_workers, patch_size, grad_clip.\n\n    Returns:\n        model (nn.Module) and metrics dict with history and best validation scores.\n    \"\"\"\n    # Defaults\n    hp = {\n        'lr': 3e-4,\n        'batch_size': 64,\n        'epochs': 15,\n        'hidden_size': 64,\n        'dropout': 0.2,\n        'tf_layers': 2,\n        'n_heads': 4,\n        'ff_mult': 2,\n        'weight_decay': 1e-4,\n        'gamma': 0.0,  # focal gamma; 0 -> plain CE\n        'label_smoothing': 0.0,\n        'num_workers': 0,\n        'patch_size': 5,\n        'grad_clip': 0.0,\n    }\n    hp.update(hyperparams or {})\n\n    num_classes = 5\n    in_channels = X_train.shape[1] if X_train.dim() == 3 and X_train.shape[1] in (1, 2) else (X_train.shape[2] if X_train.dim() == 3 else 2)\n\n    # DataLoaders\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    pin_memory = (X_train.device.type == 'cpu')  # per requirement\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=hp['batch_size'],\n        shuffle=True,\n        num_workers=hp['num_workers'],\n        pin_memory=pin_memory,\n        drop_last=False,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=hp['batch_size'],\n        shuffle=False,\n        num_workers=hp['num_workers'],\n        pin_memory=pin_memory,\n        drop_last=False,\n    )\n\n    model = TinyCATECG(\n        in_channels=in_channels,\n        num_classes=num_classes,\n        hidden_size=hp['hidden_size'],\n        tf_layers=hp['tf_layers'],\n        n_heads=hp['n_heads'],\n        ff_mult=hp['ff_mult'],\n        dropout=hp['dropout'],\n        patch_size=hp['patch_size'],\n    ).to(device)\n\n    # Class weights for imbalance (computed from training labels)\n    class_weights = _class_weights_from_labels(y_train.detach().to(device), num_classes)\n    criterion = FocalCrossEntropy(weight=class_weights, gamma=hp['gamma'], label_smoothing=hp['label_smoothing'])\n    optimizer = torch.optim.AdamW(model.parameters(), lr=hp['lr'], weight_decay=hp['weight_decay'])\n\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'val_accuracy': [],\n        'val_macro_f1': [],\n        'val_class_f1': [],\n    }\n\n    best_macro_f1 = -1.0\n    best_state = None\n    non_blocking = bool(pin_memory and device.type != 'cpu')\n\n    for epoch in range(hp['epochs']):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=non_blocking)\n            yb = yb.to(device, non_blocking=non_blocking)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if hp['grad_clip'] and hp['grad_clip'] > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), hp['grad_clip'])\n            optimizer.step()\n\n            bsz = yb.size(0)\n            running_loss += loss.item() * bsz\n            n_train += bsz\n\n        train_loss = running_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        confmat = torch.zeros(num_classes, num_classes, dtype=torch.long)\n        with torch.inference_mode():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=non_blocking)\n                yb = yb.to(device, non_blocking=non_blocking)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.item() * yb.size(0)\n                n_val += yb.size(0)\n\n                preds = logits.argmax(dim=1).detach().to('cpu')\n                _update_confmat(confmat, preds, yb.detach().to('cpu'))\n\n        val_loss = val_running_loss / max(1, n_val)\n        f1_per_class, precision_per_class, recall_per_class = _f1_from_confmat(confmat)\n        macro_f1 = f1_per_class.mean().item()\n        accuracy = confmat.diag().sum().item() / max(1, confmat.sum().item())\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['val_accuracy'].append(accuracy)\n        history['val_macro_f1'].append(macro_f1)\n        history['val_class_f1'].append(f1_per_class.tolist())\n\n        if macro_f1 > best_macro_f1:\n            best_macro_f1 = macro_f1\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    metrics = {\n        'history': history,\n        'best_val_macro_f1': best_macro_f1,\n        'best_epoch': int(torch.tensor(history['val_macro_f1']).argmax().item()) if history['val_macro_f1'] else 0,\n        'final_val_accuracy': history['val_accuracy'][-1] if history['val_accuracy'] else None,\n        'final_val_macro_f1': history['val_macro_f1'][-1] if history['val_macro_f1'] else None,\n    }\n\n    return model, metrics\n",
  "hyperparameters": {
    "lr": 0.0003,
    "epochs": 15,
    "batch_size": 64
  },
  "reasoning": "We use a compact CNN + tiny Transformer (TinyCAT) tailored to ECG: a 1D CNN stem (local morphology) followed by patch embedding and 2 lightweight Transformer encoder layers (global rhythm context). This aligns with recent results showing CNN+attention/Transformer hybrids outperform plain CNN/LSTM on MIT-BIH, especially on minority classes, while staying efficient. We include class-weighted loss (and optional focal gamma) to mitigate class imbalance, which is a key failure mode in arrhythmia classification. Metrics report per-class and macro-F1 per AAMI guidance rather than accuracy alone. The model has under 200k parameters with d_model=64, meeting the <256k constraint. The DataLoader only enables pin_memory when tensors are on CPU, as required. The loop is simple (no schedulers/early stopping) to support Bayesian optimization over core hyperparameters.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "batch_size",
    "epochs",
    "hidden_size",
    "dropout"
  ],
  "bo_search_space": {
    "lr": {
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "type": "Categorical",
      "categories": [
        8,
        16,
        32,
        64,
        128
      ]
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "hidden_size": {
      "type": "Integer",
      "low": 32,
      "high": 128
    },
    "dropout": {
      "type": "Real",
      "low": 0.0,
      "high": 0.7
    }
  },
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757982894,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}