{
  "model_name": "Dual-Lead Tiny 1D Transformer (ECG)",
  "training_code": "import math\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.utils.data import Dataset, DataLoader\\n\\n# -----------------------------\\n# Focal Loss (multiclass, logits)\\n# -----------------------------\\nclass FocalLoss(nn.Module):\\n    def __init__(self, gamma=2.0, alpha=None, reduction='mean', label_smoothing=0.0):\\n        super().__init__()\\n        self.gamma = gamma\\n        if alpha is not None:\\n            alpha = torch.as_tensor(alpha, dtype=torch.float32)\\n        self.register_buffer('alpha', alpha if alpha is not None else None, persistent=False)\\n        self.reduction = reduction\\n        self.label_smoothing = label_smoothing\\n\\n    def forward(self, logits, target):\\n        # logits: (B, C), target: (B,)\\n        log_probs = F.log_softmax(logits, dim=-1)\\n        probs = log_probs.exp()\\n\\n        # Build smoothed one-hot\\n        num_classes = logits.size(1)\\n        with torch.no_grad():\\n            true_dist = torch.zeros_like(log_probs)\\n            true_dist.fill_(self.label_smoothing / (num_classes - 1))\\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.label_smoothing)\\n\\n        # Focal term\\n        pt = (probs * true_dist).sum(dim=-1)  # (B,) probability assigned to true class (smoothed)\\n        focal_factor = (1 - pt).pow(self.gamma)\\n\\n        # Alpha weighting per class\\n        if self.alpha is not None:\\n            alpha_weight = self.alpha.to(logits.device)\\n            at = alpha_weight.gather(0, target)  # (B,)\\n        else:\\n            at = 1.0\\n\\n        loss = -(focal_factor * at) * (true_dist * log_probs).sum(dim=-1)\\n\\n        if self.reduction == 'mean':\\n            return loss.mean()\\n        elif self.reduction == 'sum':\\n            return loss.sum()\\n        else:\\n            return loss\\n\\n# -----------------------------\\n# Dataset helpers\\n# -----------------------------\\nclass SignalDataset(Dataset):\\n    def __init__(self, X, y):\\n        self.X = X\\n        self.y = y.long()\\n\\n    def __len__(self):\\n        return self.y.shape[0]\\n\\n    def __getitem__(self, idx):\\n        return self.X[idx], self.y[idx]\\n\\nclass SignalRrDataset(Dataset):\\n    def __init__(self, X, rr, y):\\n        self.X = X\\n        self.rr = rr\\n        self.y = y.long()\\n        assert len(self.X) == len(self.rr) == len(self.y)\\n\\n    def __len__(self):\\n        return self.y.shape[0]\\n\\n    def __getitem__(self, idx):\\n        return self.X[idx], self.rr[idx], self.y[idx]\\n\\n# -----------------------------\\n# Model components\\n# -----------------------------\\nclass PatchEmbed1D(nn.Module):\\n    def __init__(self, in_ch=2, embed_dim=96, kernel_size=16, stride=8, dropout=0.0):\\n        super().__init__()\\n        self.proj = nn.Conv1d(in_ch, embed_dim, kernel_size=kernel_size, stride=stride, padding=0, bias=True)\\n        self.drop = nn.Dropout(dropout)\\n\\n    def forward(self, x):\\n        # x: (B, C, L)\\n        x = self.proj(x)  # (B, D, P)\\n        x = x.transpose(1, 2)  # (B, P, D)\\n        return self.drop(x)\\n\\nclass TransformerBlock(nn.Module):\\n    def __init__(self, dim, num_heads=4, mlp_ratio=2.0, dropout=0.1):\\n        super().__init__()\\n        self.norm1 = nn.LayerNorm(dim)\\n        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)\\n        self.drop1 = nn.Dropout(dropout)\\n        self.norm2 = nn.LayerNorm(dim)\\n        hidden = int(dim * mlp_ratio)\\n        self.mlp = nn.Sequential(\\n            nn.Linear(dim, hidden),\\n            nn.GELU(),\\n            nn.Dropout(dropout),\\n            nn.Linear(hidden, dim),\\n            nn.Dropout(dropout),\\n        )\\n\\n    def forward(self, x):\\n        # x: (B, P, D)\\n        h = self.norm1(x)\\n        attn_out, _ = self.attn(h, h, h, need_weights=False)\\n        x = x + self.drop1(attn_out)\\n        x = x + self.mlp(self.norm2(x))\\n        return x\\n\\nclass Tiny1DTransformerECG(nn.Module):\\n    def __init__(self, input_channels=2, seq_len=1000, num_classes=5, d_model=96, n_heads=4, depth=3, mlp_ratio=2.0,\\n                 patch_kernel_size=16, patch_stride=8, dropout=0.1, rr_use_aux=False, rr_embed_dim=32, rr_fuse='concat'):\\n        super().__init__()\\n        # Ensure compatibility between d_model and n_heads\\n        if d_model % n_heads != 0:\\n            new_d = int(math.ceil(d_model / n_heads) * n_heads)\\n            print(f\"[Info] Adjusting d_model from {d_model} to {new_d} to be divisible by n_heads={n_heads}.\")\\n            d_model = new_d\\n\\n        self.embed = PatchEmbed1D(in_ch=input_channels, embed_dim=d_model, kernel_size=patch_kernel_size, stride=patch_stride, dropout=dropout)\\n        # Estimate number of patches for given seq_len\\n        num_patches = max(1, (seq_len - patch_kernel_size) // patch_stride + 1)\\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, d_model))\\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\\n\\n        self.blocks = nn.ModuleList([TransformerBlock(d_model, num_heads=n_heads, mlp_ratio=mlp_ratio, dropout=dropout) for _ in range(depth)])\\n        self.norm = nn.LayerNorm(d_model)\\n\\n        # RR auxiliary branch (created lazily if rr provided)\\n        self.rr_use_aux = rr_use_aux\\n        self.rr_fuse = rr_fuse\\n        self.rr_embed_dim = rr_embed_dim\\n        self.rr_proj = None  # will be defined on first forward that provides rr\\n\\n        # Classifier will be finalized once we know fusion dimension\\n        self.classifier_in_dim = d_model  # may change if concat with rr\\n        self.classifier = None\\n        self.num_classes = num_classes\\n        self._build_classifier(self.classifier_in_dim)\\n\\n    def _build_classifier(self, in_dim):\\n        self.classifier_in_dim = in_dim\\n        self.classifier = nn.Sequential(\\n            nn.Dropout(0.1),\\n            nn.Linear(in_dim, self.num_classes)\\n        )\\n\\n    def _pos_embed(self, x):\\n        # x: (B, P, D)\\n        B, P, D = x.shape\\n        if self.pos_embed.size(1) != P:\\n            # interpolate positional embeddings if sequence of patches differs\\n            pe = self.pos_embed.transpose(1, 2)  # (1, D, P0)\\n            pe = F.interpolate(pe, size=P, mode='linear', align_corners=False)\\n            pe = pe.transpose(1, 2)  # (1, P, D)\\n        else:\\n            pe = self.pos_embed\\n        return x + pe\\n\\n    def _maybe_init_rr(self, rr):\\n        if (not self.rr_use_aux) or (rr is None):\\n            return\\n        if self.rr_proj is None:\\n            rr_in = rr.shape[-1]\\n            self.rr_proj = nn.Sequential(\\n                nn.LayerNorm(rr_in),\\n                nn.Linear(rr_in, self.rr_embed_dim),\\n                nn.GELU(),\\n                nn.Dropout(0.1)\\n            )\\n            self.rr_proj.to(rr.device)\\n            if self.rr_fuse == 'concat':\\n                self._build_classifier(self.classifier_in_dim + self.rr_embed_dim)\\n            else:\\n                # 'add' fusion: project rr to same dim as token feature\\n                self.rr_resize = nn.Linear(self.rr_embed_dim, self.classifier_in_dim)\\n                self.rr_resize.to(rr.device)\\n                self._build_classifier(self.classifier_in_dim)\\n\\n    def forward(self, x, rr=None):\\n        # x: (B, C, L)\\n        z = self.embed(x)  # (B, P, D)\\n        z = self._pos_embed(z)\\n        for blk in self.blocks:\\n            z = blk(z)\\n        z = self.norm(z)\\n        # Global average pooling (token)\\n        feat = z.mean(dim=1)  # (B, D)\\n\\n        if self.rr_use_aux and rr is not None:\\n            self._maybe_init_rr(rr)\\n            rr_feat = self.rr_proj(rr)\\n            if self.rr_fuse == 'concat':\\n                feat = torch.cat([feat, rr_feat], dim=-1)\\n            else:\\n                # add\\n                feat = feat + self.rr_resize(rr_feat)\\n\\n        logits = self.classifier(feat)\\n        return logits\\n\\n# -----------------------------\\n# Metrics\\n# -----------------------------\\n@torch.no_grad()\\ndef accuracy(preds, targets):\\n    return (preds == targets).float().mean().item()\\n\\n@torch.no_grad()\\ndef macro_f1(preds, targets, num_classes):\\n    # preds, targets: 1D tensors\\n    f1s = []\\n    for c in range(num_classes):\\n        tp = ((preds == c) & (targets == c)).sum().item()\\n        fp = ((preds == c) & (targets != c)).sum().item()\\n        fn = ((preds != c) & (targets == c)).sum().item()\\n        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\\n        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\\n        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\\n        f1s.append(f1)\\n    return float(sum(f1s) / len(f1s))\\n\\n# -----------------------------\\n# Main training API\\n# -----------------------------\\ndef train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\\n    \"\"\"\\n    Train a Dual-Lead Tiny 1D Transformer for 5-class ECG classification.\\n\\n    Expected signal shape per sample: (1000, 2) or (2, 1000).\\n    To use RR-aux branch, pass X_* as a tuple: (signal_tensor, rr_tensor).\\n    \"\"\"\\n    # Defaults for Bayesian Optimization-controlled hyperparams\\n    cfg = {\\n        'lr': 1e-3,\\n        'weight_decay': 1e-5,\\n        'batch_size': 128,\\n        'epochs': 20,\\n        'd_model': 96,\\n        'n_heads': 4,\\n        'depth': 3,\\n        'mlp_ratio': 2.0,\\n        'dropout': 0.1,\\n        'patch_kernel_size': 16,\\n        'patch_stride': 8,\\n        'loss_type': 'cross_entropy',\\n        'focal_gamma': 2.0,\\n        'use_class_weights': True,\\n        'label_smoothing': 0.0,\\n        'grad_clip_norm': 1.0,\\n        'rr_use_aux': False,\\n        'rr_embed_dim': 32,\\n        'rr_fuse': 'concat'\\n    }\\n    cfg.update(hyperparams or {})\\n\\n    num_classes = 5\\n\\n    # Unpack RR if provided\\n    train_has_rr = isinstance(X_train, (tuple, list)) and len(X_train) == 2\\n    val_has_rr = isinstance(X_val, (tuple, list)) and len(X_val) == 2\\n\\n    X_train_sig = X_train[0] if train_has_rr else X_train\\n    X_train_rr = X_train[1] if train_has_rr else None\\n    X_val_sig = X_val[0] if val_has_rr else X_val\\n    X_val_rr = X_val[1] if val_has_rr else None\\n\\n    # Helper to ensure (B, C, L) with C=2\\n    def to_ch_first(t):\\n        # Accept (B, 1000, 2) or (B, 2, 1000) or (1000,2) or (2,1000)\\n        if t.dim() == 2:\\n            if t.shape[-1] == 2:\\n                t = t.transpose(0, 1)  # (2, L)\\n            # else assume (2, L) already\\n            t = t.unsqueeze(0)\\n        elif t.dim() == 3:\\n            if t.shape[-1] == 2:\\n                t = t.transpose(1, 2)  # (B, 2, L)\\n            # else assume already (B, 2, L)\\n        else:\\n            raise ValueError('Unexpected signal tensor shape')\\n        return t.contiguous()\\n\\n    # Pre-normalize shape once for dataset tensors\\n    X_train_sig = to_ch_first(X_train_sig)\\n    X_val_sig = to_ch_first(X_val_sig)\\n\\n    # Datasets and loaders\\n    if cfg['rr_use_aux'] and (X_train_rr is not None) and (X_val_rr is not None):\\n        train_ds = SignalRrDataset(X_train_sig, X_train_rr, y_train)\\n        val_ds = SignalRrDataset(X_val_sig, X_val_rr, y_val)\\n        has_rr = True\\n    else:\\n        if cfg['rr_use_aux'] and (X_train_rr is None or X_val_rr is None):\\n            print('[Info] rr_use_aux=True but RR tensors not provided; proceeding without RR branch.')\\n        train_ds = SignalDataset(X_train_sig, y_train)\\n        val_ds = SignalDataset(X_val_sig, y_val)\\n        has_rr = False\\n\\n    train_loader = DataLoader(train_ds, batch_size=cfg['batch_size'], shuffle=True, drop_last=False)\\n    val_loader = DataLoader(val_ds, batch_size=cfg['batch_size'], shuffle=False, drop_last=False)\\n\\n    # Compute class weights if desired\\n    class_weights = None\\n    if cfg['use_class_weights']:\\n        with torch.no_grad():\\n            counts = torch.bincount(y_train.view(-1).cpu(), minlength=num_classes).float()\\n            counts[counts == 0] = 1.0\\n            inv_freq = 1.0 / counts\\n            class_weights = (inv_freq / inv_freq.sum() * num_classes).to(device)\\n\\n    # Build model\\n    seq_len = X_train_sig.shape[-1]\\n    model = Tiny1DTransformerECG(\\n        input_channels=2, seq_len=seq_len, num_classes=num_classes,\\n        d_model=cfg['d_model'], n_heads=cfg['n_heads'], depth=cfg['depth'], mlp_ratio=cfg['mlp_ratio'],\\n        patch_kernel_size=cfg['patch_kernel_size'], patch_stride=cfg['patch_stride'], dropout=cfg['dropout'],\\n        rr_use_aux=cfg['rr_use_aux'] and has_rr, rr_embed_dim=cfg['rr_embed_dim'], rr_fuse=cfg['rr_fuse']\\n    ).to(device)\\n\\n    # Criterion\\n    if cfg['loss_type'] == 'focal':\\n        alpha = class_weights if class_weights is not None else None\\n        criterion = FocalLoss(gamma=cfg['focal_gamma'], alpha=alpha, reduction='mean', label_smoothing=cfg['label_smoothing'])\\n    else:\\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=cfg['label_smoothing'])\\n\\n    # Optimizer\\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\\n\\n    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_macro_f1': []}\\n\\n    for epoch in range(int(cfg['epochs'])):\\n        model.train()\\n        total_loss = 0.0\\n        total_count = 0\\n\\n        for batch in train_loader:\\n            optimizer.zero_grad(set_to_none=True)\\n            if has_rr:\\n                x, rr, yb = batch\\n                x = x.to(device, non_blocking=True)\\n                rr = rr.to(device, non_blocking=True).float()\\n                yb = yb.to(device, non_blocking=True)\\n                logits = model(x, rr)\\n            else:\\n                x, yb = batch\\n                x = x.to(device, non_blocking=True)\\n                yb = yb.to(device, non_blocking=True)\\n                logits = model(x, None)\\n\\n            loss = criterion(logits, yb)\\n            loss.backward()\\n            if cfg['grad_clip_norm'] is not None and cfg['grad_clip_norm'] > 0:\\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(cfg['grad_clip_norm']))\\n            optimizer.step()\\n\\n            bs = yb.size(0)\\n            total_loss += loss.item() * bs\\n            total_count += bs\\n\\n        avg_train_loss = total_loss / max(1, total_count)\\n\\n        # Validation\\n        model.eval()\\n        val_loss = 0.0\\n        val_count = 0\\n        all_preds = []\\n        all_targets = []\\n        with torch.no_grad():\\n            for batch in val_loader:\\n                if has_rr:\\n                    x, rr, yb = batch\\n                    x = x.to(device, non_blocking=True)\\n                    rr = rr.to(device, non_blocking=True).float()\\n                    yb = yb.to(device, non_blocking=True)\\n                    logits = model(x, rr)\\n                else:\\n                    x, yb = batch\\n                    x = x.to(device, non_blocking=True)\\n                    yb = yb.to(device, non_blocking=True)\\n                    logits = model(x, None)\\n                loss = criterion(logits, yb)\\n                preds = logits.argmax(dim=-1)\\n                all_preds.append(preds.detach().cpu())\\n                all_targets.append(yb.detach().cpu())\\n                bs = yb.size(0)\\n                val_loss += loss.item() * bs\\n                val_count += bs\\n\\n        all_preds = torch.cat(all_preds, dim=0)\\n        all_targets = torch.cat(all_targets, dim=0)\\n        val_acc = accuracy(all_preds, all_targets)\\n        val_f1 = macro_f1(all_preds, all_targets, num_classes)\\n        avg_val_loss = val_loss / max(1, val_count)\\n\\n        history['train_loss'].append(avg_train_loss)\\n        history['val_loss'].append(avg_val_loss)\\n        history['val_acc'].append(val_acc)\\n        history['val_macro_f1'].append(val_f1)\\n\\n        # Optional: simple progress print\\n        print(f\"Epoch {epoch+1}/{cfg['epochs']} - train_loss: {avg_train_loss:.4f} - val_loss: {avg_val_loss:.4f} - val_acc: {val_acc:.4f} - val_f1: {val_f1:.4f}\")\\n\\n    num_params = sum(p.numel() for p in model.parameters())\\n    print(f\"Model parameters: {num_params}\")\\n\\n    metrics = {\\n        'history': history,\\n        'num_parameters': int(num_params)\\n    }\\n\\n    return model, metrics\\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "weight_decay": {
      "default": 1e-05,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 128,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128,
        256
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "d_model": {
      "default": 96,
      "type": "Integer",
      "low": 64,
      "high": 128
    },
    "n_heads": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        2,
        4,
        8
      ]
    },
    "depth": {
      "default": 3,
      "type": "Integer",
      "low": 2,
      "high": 4
    },
    "mlp_ratio": {
      "default": 2.0,
      "type": "Real",
      "low": 1.5,
      "high": 4.0
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "patch_kernel_size": {
      "default": 16,
      "type": "Integer",
      "low": 8,
      "high": 32
    },
    "patch_stride": {
      "default": 8,
      "type": "Integer",
      "low": 4,
      "high": 16
    },
    "loss_type": {
      "default": "cross_entropy",
      "type": "Categorical",
      "categories": [
        "cross_entropy",
        "focal"
      ]
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 1.0,
      "high": 5.0
    },
    "use_class_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "label_smoothing": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "grad_clip_norm": {
      "default": 1.0,
      "type": "Real",
      "low": 0.1,
      "high": 5.0
    },
    "rr_use_aux": {
      "default": false,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "rr_embed_dim": {
      "default": 32,
      "type": "Integer",
      "low": 16,
      "high": 64
    },
    "rr_fuse": {
      "default": "concat",
      "type": "Categorical",
      "categories": [
        "concat",
        "add"
      ]
    }
  },
  "confidence": 0.88,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758224120,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}