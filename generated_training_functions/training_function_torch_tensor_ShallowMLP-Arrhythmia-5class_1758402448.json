{
  "model_name": "ShallowMLP-Arrhythmia-5class",
  "training_code": "import copy\\nimport math\\nimport time\\nfrom typing import Dict, Any\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.utils.data import TensorDataset, DataLoader\\n\\n# Prefer torch.ao.quantization if available\\ntry:\\n    from torch.ao.quantization import quantize_dynamic as ao_quantize_dynamic\\n    AO_Q_AVAILABLE = True\\nexcept Exception:\\n    try:\\n        from torch.quantization import quantize_dynamic as ao_quantize_dynamic\\n        AO_Q_AVAILABLE = True\\n    except Exception:\\n        AO_Q_AVAILABLE = False\\n\\n\\nclass MLPClassifier(nn.Module):\\n    def __init__(self, input_dim: int, hidden_size: int, num_layers: int, dropout: float, num_classes: int = 5):\\n        super().__init__()\\n        assert num_layers >= 1\\n        layers = []\\n        prev = input_dim\\n        for i in range(num_layers):\\n            layers.append(nn.Linear(prev, hidden_size))\\n            layers.append(nn.ReLU(inplace=True))\\n            if dropout > 0.0:\\n                layers.append(nn.Dropout(p=dropout))\\n            prev = hidden_size\\n        self.features = nn.Sequential(*layers)\\n        self.classifier = nn.Linear(prev, num_classes)\\n\\n    def forward(self, x):\\n        x = self.features(x)\\n        return self.classifier(x)\\n\\n\\ndef _count_params(input_dim: int, hidden_size: int, num_layers: int, num_classes: int) -> int:\\n    # Params for first layer\\n    params = input_dim * hidden_size + hidden_size\\n    # Middle layers (if any)\\n    if num_layers > 1:\\n        params += (num_layers - 1) * (hidden_size * hidden_size + hidden_size)\\n    # Classifier\\n    params += hidden_size * num_classes + num_classes\\n    return params\\n\\n\\nclass _FP16InputWrapper(nn.Module):\\n    \"\"\"Wrap a model to cast inputs to FP16 and optionally cast outputs back to FP32.\"\"\"\\n    def __init__(self, model: nn.Module, cast_back_to_fp32: bool = True):\\n        super().__init__()\\n        self.model = model\\n        self.cast_back = cast_back_to_fp32\\n\\n    def forward(self, x):\\n        x_half = x.half()\\n        out = self.model(x_half)\\n        if self.cast_back:\\n            return out.float()\\n        return out\\n\\n\\ndef _quantize_post_training(model: nn.Module, quantization_bits: int, quantize_weights: bool, quantize_activations: bool) -> nn.Module:\\n    \"\"\"Apply post-training quantization according to provided settings.\\n    - 32 bits: returns the original FP32 model.\\n    - 16 bits: \\n        * weights-only -> dynamic quantization with dtype=torch.float16 on Linear layers.\\n        * weights+activations -> convert model to half precision and wrap inputs casting to fp16.\\n    - 8 bits: \\n        * dynamic quantization on Linear layers with dtype=torch.qint8 (standard, fast path).\\n    Notes: Some combinations (activations-only) are not supported directly in PyTorch PTQ; in such cases we return the closest supported configuration and print a notice.\\n    \"\"\"\\n    model = copy.deepcopy(model).cpu().eval()\\n\\n    if quantization_bits == 32:\\n        # No quantization\\n        return model\\n\\n    if not AO_Q_AVAILABLE:\\n        print('[Quantization] torch.ao.quantization/torch.quantization not available; returning FP32 model.')\\n        return model\\n\\n    if quantization_bits == 16:\\n        if quantize_weights and not quantize_activations:\\n            # Dynamic FP16 weights on Linear layers; activations remain FP32\\n            qmodel = ao_quantize_dynamic(model, {nn.Linear}, dtype=torch.float16)\\n            return qmodel\\n        elif quantize_weights and quantize_activations:\\n            # Convert full model to half and wrap inputs\\n            model.half()\\n            wrapped = _FP16InputWrapper(model, cast_back_to_fp32=True)\\n            return wrapped\\n        elif (not quantize_weights) and quantize_activations:\\n            # Not directly supported; fall back to FP32 with a notice.\\n            print('[Quantization] 16-bit activations-only PTQ not supported. Returning FP32 model.')\\n            return model\\n        else:\\n            # Neither weights nor activations -> no quantization\\n            return model\\n\\n    if quantization_bits == 8:\\n        try:\\n            torch.backends.quantized.engine = 'fbgemm'\\n        except Exception:\\n            pass\\n        if quantize_weights:\\n            # Standard dynamic int8 quantization for Linear layers\\n            qmodel = ao_quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\\n            # Note: dynamic quantization quantizes activations at runtime for GEMM; cannot be disabled.\\n            if not quantize_activations:\\n                print('[Quantization] int8 dynamic quantization implies runtime activation quantization; cannot disable activations. Proceeding.')\\n            return qmodel\\n        else:\\n            if quantize_activations:\\n                print('[Quantization] int8 activations-only PTQ not supported without quantized weights. Returning FP32 model.')\\n            return model\\n\\n    # Fallback\\n    print(f'[Quantization] Unsupported quantization_bits={quantization_bits}. Returning FP32 model.')\\n    return model\\n\\n\\ndef train_model(X_train: torch.Tensor,\\n                y_train: torch.Tensor,\\n                X_val: torch.Tensor,\\n                y_val: torch.Tensor,\\n                device: torch.device,\\n                **kwargs) -> tuple:\\n    \"\"\"Train a compact MLP classifier for 5 classes and return a quantized model with training metrics.\\n\\n    Inputs are tensors. Shapes:\\n      - X_*: (N, 2) float tensor\\n      - y_*: (N,) long tensor with class indices in [0..4]\\n\\n    Hyperparameters (defaults shown; can be optimized via bo_config):\\n      - lr: float = 1e-3\\n      - batch_size: int = 256\\n      - epochs: int = 20\\n      - hidden_size: int = 128\\n      - num_layers: int = 2\\n      - dropout: float = 0.1\\n      - weight_decay: float = 0.0\\n      - optimizer: str = 'Adam' in {'Adam','AdamW','SGD'}\\n      - early_stopping_patience: int = 0 (disabled if 0)\\n\\n    Quantization parameters:\\n      - quantization_bits: {8,16,32} (default: 8)\\n      - quantize_weights: bool (default: True)\\n      - quantize_activations: bool (default: True)\\n\\n    Returns: (quantized_model, metrics_dict) where metrics_dict contains\\n      - 'train_losses', 'val_losses', 'val_acc' (lists per epoch)\\n    \"\"\"\\n    torch.manual_seed(kwargs.get('seed', 42))\\n\\n    # Extract hyperparameters with defaults\\n    lr = float(kwargs.get('lr', 1e-3))\\n    batch_size = int(kwargs.get('batch_size', 256))\\n    epochs = int(kwargs.get('epochs', 20))\\n    hidden_size = int(kwargs.get('hidden_size', 128))\\n    num_layers = int(kwargs.get('num_layers', 2))\\n    dropout = float(kwargs.get('dropout', 0.1))\\n    weight_decay = float(kwargs.get('weight_decay', 0.0))\\n    optimizer_name = str(kwargs.get('optimizer', 'Adam'))\\n    early_stop_patience = int(kwargs.get('early_stopping_patience', 0))\\n\\n    # Quantization params\\n    quantization_bits = int(kwargs.get('quantization_bits', 8))\\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\\n    quantize_activations = bool(kwargs.get('quantize_activations', True))\\n\\n    num_classes = 5\\n    input_dim = int(X_train.shape[1])\\n\\n    # Enforce parameter budget <= 256k by adjusting hidden_size downward if needed\\n    PARAM_LIMIT = 256_000\\n    def params_for(H: int) -> int:\\n        return _count_params(input_dim, H, num_layers, num_classes)\\n\\n    if params_for(hidden_size) > PARAM_LIMIT:\\n        original_H = hidden_size\\n        while hidden_size > 8 and params_for(hidden_size) > PARAM_LIMIT:\\n            hidden_size = max(8, hidden_size // 2)\\n        print(f'[Model] hidden_size reduced from {original_H} to {hidden_size} to satisfy <= {PARAM_LIMIT} params.')\\n\\n    # Build model\\n    model = MLPClassifier(input_dim=input_dim, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, num_classes=num_classes)\\n    total_params = sum(p.numel() for p in model.parameters())\\n    assert total_params <= PARAM_LIMIT, f'Model has {total_params} parameters, exceeds limit {PARAM_LIMIT}.'\\n\\n    # Datasets/Dataloaders\\n    X_train = X_train.float()\\n    y_train = y_train.long()\\n    X_val = X_val.float()\\n    y_val = y_val.long()\\n\\n    train_ds = TensorDataset(X_train, y_train)\\n    val_ds = TensorDataset(X_val, y_val)\\n\\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\\n\\n    model.to(device)\\n\\n    # Optimizer\\n    if optimizer_name == 'Adam':\\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\\n    elif optimizer_name == 'AdamW':\\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\\n    elif optimizer_name == 'SGD':\\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\\n    else:\\n        raise ValueError(f'Unsupported optimizer: {optimizer_name}')\\n\\n    criterion = nn.CrossEntropyLoss()\\n\\n    train_losses, val_losses, val_accs = [], [], []\\n    best_val_loss = float('inf')\\n    best_state = copy.deepcopy(model.state_dict())\\n    epochs_no_improve = 0\\n\\n    for epoch in range(epochs):\\n        model.train()\\n        running_loss = 0.0\\n        n_train = 0\\n        for xb, yb in train_loader:\\n            xb = xb.to(device, non_blocking=False)\\n            yb = yb.to(device, non_blocking=False)\\n            optimizer.zero_grad(set_to_none=True)\\n            logits = model(xb)\\n            loss = criterion(logits, yb)\\n            loss.backward()\\n            optimizer.step()\\n            bs = yb.size(0)\\n            running_loss += loss.item() * bs\\n            n_train += bs\\n\\n        epoch_train_loss = running_loss / max(1, n_train)\\n        train_losses.append(epoch_train_loss)\\n\\n        # Validation\\n        model.eval()\\n        val_running_loss = 0.0\\n        correct = 0\\n        n_val = 0\\n        with torch.no_grad():\\n            for xb, yb in val_loader:\\n                xb = xb.to(device, non_blocking=False)\\n                yb = yb.to(device, non_blocking=False)\\n                logits = model(xb)\\n                loss = criterion(logits, yb)\\n                val_running_loss += loss.item() * yb.size(0)\\n                preds = logits.argmax(dim=1)\\n                correct += (preds == yb).sum().item()\\n                n_val += yb.size(0)\\n        epoch_val_loss = val_running_loss / max(1, n_val)\\n        epoch_val_acc = correct / max(1, n_val)\\n        val_losses.append(epoch_val_loss)\\n        val_accs.append(epoch_val_acc)\\n\\n        print(f'Epoch {epoch+1}/{epochs}: train_loss={epoch_train_loss:.6f} val_loss={epoch_val_loss:.6f} val_acc={epoch_val_acc:.4f}')\\n\\n        # Track best\\n        if epoch_val_loss < best_val_loss - 1e-8:\\n            best_val_loss = epoch_val_loss\\n            best_state = copy.deepcopy(model.state_dict())\\n            epochs_no_improve = 0\\n        else:\\n            epochs_no_improve += 1\\n\\n        # Early stopping\\n        if early_stop_patience > 0 and epochs_no_improve >= early_stop_patience:\\n            print(f'Early stopping at epoch {epoch+1} after {epochs_no_improve} epochs without improvement.')\\n            break\\n\\n    # Load best weights before quantization\\n    model.load_state_dict(best_state)\\n    model.eval()\\n    model.cpu()\\n\\n    # Post-training quantization\\n    quantized_model = _quantize_post_training(model, quantization_bits=quantization_bits,\\n                                              quantize_weights=quantize_weights,\\n                                              quantize_activations=quantize_activations)\\n\\n    metrics = {\\n        'train_losses': train_losses,\\n        'val_losses': val_losses,\\n        'val_acc': val_accs,\\n        'total_params': int(total_params)\\n    }\\n\\n    return quantized_model, metrics\\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 256,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128,
        256,
        512
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 200
    },
    "hidden_size": {
      "default": 128,
      "type": "Integer",
      "low": 16,
      "high": 512
    },
    "num_layers": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 3
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "weight_decay": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 0.01
    },
    "optimizer": {
      "default": "Adam",
      "type": "Categorical",
      "categories": [
        "Adam",
        "AdamW",
        "SGD"
      ]
    },
    "early_stopping_patience": {
      "default": 0,
      "type": "Integer",
      "low": 0,
      "high": 20
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "seed": {
      "default": 42,
      "type": "Integer",
      "low": 1,
      "high": 1000000
    }
  },
  "confidence": 0.9,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758402448,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}