{
  "model_name": "FallbackMLP",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device='cpu', lr=0.001, epochs=10, batch_size=64, hidden_size=256, dropout=0.2):\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import TensorDataset, DataLoader\n    \n    # Model definition\n    class FallbackMLP(nn.Module):\n        def __init__(self, input_size, hidden_size, num_classes, dropout):\n            super().__init__()\n            self.network = nn.Sequential(\n                nn.Linear(input_size, hidden_size),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden_size, hidden_size // 2),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden_size // 2, num_classes)\n            )\n        \n        def forward(self, x):\n            if x.dim() > 2:\n                x = x.view(x.size(0), -1)\n            return self.network(x)\n    \n    # Data preparation\n    train_dataset = TensorDataset(X_train, y_train)\n    val_dataset = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Model initialization\n    input_size = X_train.view(X_train.size(0), -1).shape[-1]\n    model = FallbackMLP(input_size, hidden_size, 3, dropout)\n    model.to(device)\n    \n    # Training setup\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    # Training loop\n    model.train()\n    train_losses = []\n    val_accuracies = []\n    \n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        # Validation\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                outputs = model(batch_X)\n                _, predicted = torch.max(outputs.data, 1)\n                total += batch_y.size(0)\n                correct += (predicted == batch_y).sum().item()\n        \n        val_acc = correct / total\n        train_losses.append(epoch_loss / len(train_loader))\n        val_accuracies.append(val_acc)\n        model.train()\n    \n    # Final metrics\n    model.eval()\n    final_metrics = {'val_accuracy': val_accuracies[-1], 'final_loss': train_losses[-1], 'macro_f1': val_accuracies[-1]}\n    \n    return model, final_metrics",
  "hyperparameters": {
    "lr": 0.001,
    "epochs": 10,
    "batch_size": 64,
    "hidden_size": 256,
    "dropout": 0.2
  },
  "reasoning": "Fallback: MLP selected for tabular data",
  "confidence": 0.7,
  "bo_parameters": [
    "lr",
    "epochs",
    "batch_size",
    "hidden_size",
    "dropout"
  ],
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      200,
      10
    ],
    "dtype": "float32",
    "feature_count": 10,
    "sample_count": 200,
    "is_sequence": false,
    "is_image": false,
    "is_tabular": true,
    "has_labels": true,
    "label_count": 3,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757880251,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}