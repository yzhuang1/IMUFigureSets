{
  "model_name": "TinyECG1D-CNN",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import TensorDataset, DataLoader\n    \n    # -------------------- Hyperparameters --------------------\n    lr = hyperparams.get('lr', 1e-3)\n    epochs = hyperparams.get('epochs', 10)\n    batch_size = hyperparams.get('batch_size', 64)\n    hidden_size = hyperparams.get('hidden_size', 128)\n    dropout = hyperparams.get('dropout', 0.2)\n    weight_decay = hyperparams.get('weight_decay', 0.0)\n    use_class_weights = hyperparams.get('use_class_weights', True)\n    num_classes = hyperparams.get('num_classes', 5)\n    \n    # -------------------- Input preparation --------------------\n    def to_channel_first(x):\n        # Expect (N, L, C) or (N, C, L). Convert to (N, C, L)\n        if x.dim() != 3:\n            raise ValueError('Expected input tensor with 3 dims: (N, L, C) or (N, C, L)')\n        # If last dim looks like channels (small), permute\n        if x.shape[-1] <= 8 and x.shape[1] > x.shape[-1]:\n            return x.permute(0, 2, 1).contiguous()\n        # If already channel-first, keep\n        if x.shape[1] <= 8:\n            return x.contiguous()\n        # Ambiguous shapes\n        raise ValueError(f'Unrecognized input shape {tuple(x.shape)}. Expected (N, L, C) or (N, C, L) with small C<=8.')\n    \n    X_train = X_train.float()\n    X_val = X_val.float()\n    X_train = to_channel_first(X_train)\n    X_val = to_channel_first(X_val)\n    in_channels = X_train.shape[1]\n    \n    # Ensure label format is class indices\n    def ensure_class_indices(y, n_classes):\n        y = y.squeeze()\n        if y.dim() > 1 and y.shape[-1] == n_classes:\n            y = y.argmax(dim=-1)\n        return y.long()\n    \n    y_train = ensure_class_indices(y_train, num_classes)\n    y_val = ensure_class_indices(y_val, num_classes)\n    \n    # -------------------- Model definition --------------------\n    class DWSeparableBlock(nn.Module):\n        def __init__(self, in_ch, out_ch, k=7, pool=True):\n            super().__init__()\n            pad = k // 2\n            self.depthwise = nn.Conv1d(in_ch, in_ch, kernel_size=k, padding=pad, groups=in_ch, bias=False)\n            self.bn1 = nn.BatchNorm1d(in_ch)\n            self.pointwise = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn2 = nn.BatchNorm1d(out_ch)\n            self.act = nn.ReLU(inplace=True)\n            self.pool = nn.MaxPool1d(2) if pool else nn.Identity()\n        def forward(self, x):\n            x = self.depthwise(x)\n            x = self.bn1(x)\n            x = self.act(x)\n            x = self.pointwise(x)\n            x = self.bn2(x)\n            x = self.act(x)\n            x = self.pool(x)\n            return x\n    \n    class TinyECG1DCNN(nn.Module):\n        def __init__(self, in_ch, hidden_size=128, dropout=0.2, num_classes=5):\n            super().__init__()\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, 32, kernel_size=7, padding=3, bias=False),\n                nn.BatchNorm1d(32),\n                nn.ReLU(inplace=True),\n                nn.MaxPool1d(2),\n            )\n            self.block1 = DWSeparableBlock(32, 64, k=7, pool=True)\n            self.block2 = DWSeparableBlock(64, 128, k=5, pool=True)\n            self.gap = nn.AdaptiveAvgPool1d(1)\n            self.head = nn.Sequential(\n                nn.Flatten(),\n                nn.Linear(128, hidden_size),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=dropout),\n                nn.Linear(hidden_size, num_classes)\n            )\n        def forward(self, x):\n            x = self.stem(x)\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.gap(x)\n            x = self.head(x)\n            return x\n    \n    model = TinyECG1DCNN(in_channels, hidden_size=hidden_size, dropout=dropout, num_classes=num_classes).to(device)\n    \n    # -------------------- Loss (optional class weighting) --------------------\n    if use_class_weights:\n        with torch.no_grad():\n            counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n            # Inverse-frequency weights, normalized to keep loss scale reasonable\n            class_weights = (counts.sum() / (counts + 1e-6)) / num_classes\n        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n    else:\n        criterion = nn.CrossEntropyLoss()\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    \n    # -------------------- DataLoaders --------------------\n    # Per requirement: only set pin_memory=True if input tensors are on CPU\n    use_pin = (X_train.device.type == 'cpu')\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=use_pin)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=use_pin)\n    \n    # -------------------- Metrics helpers --------------------\n    def update_confmat(cm, y_true, y_pred, k):\n        y_true = y_true.view(-1).cpu()\n        y_pred = y_pred.view(-1).cpu()\n        cm += torch.bincount(k * y_true + y_pred, minlength=k * k).reshape(k, k)\n        return cm\n    \n    def f1_from_confmat(cm):\n        cm = cm.float()\n        tp = torch.diag(cm)\n        fp = cm.sum(0) - tp\n        fn = cm.sum(1) - tp\n        f1 = 2 * tp / (2 * tp + fp + fn + 1e-9)\n        return f1.mean().item()\n    \n    # -------------------- Training loop --------------------\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": [], \"val_f1\": []}\n    non_block = use_pin  # only non_blocking copies if pin_memory was used\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=non_block)\n            yb = yb.to(device, non_blocking=non_block)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            optimizer.step()\n            \n            batch_size_actual = yb.size(0)\n            running_loss += loss.item() * batch_size_actual\n            total += batch_size_actual\n        train_epoch_loss = running_loss / max(total, 1)\n        history[\"train_loss\"].append(train_epoch_loss)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_total = 0\n        val_correct = 0\n        confmat = torch.zeros(num_classes, num_classes, dtype=torch.long)\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=non_block)\n                yb = yb.to(device, non_blocking=non_block)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.item() * yb.size(0)\n                val_total += yb.size(0)\n                preds = logits.argmax(dim=1)\n                val_correct += (preds == yb).sum().item()\n                confmat = update_confmat(confmat, yb, preds, num_classes)\n        val_epoch_loss = val_loss / max(val_total, 1)\n        val_acc = val_correct / max(val_total, 1)\n        val_f1 = f1_from_confmat(confmat)\n        history[\"val_loss\"].append(val_epoch_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"val_f1\"].append(val_f1)\n    \n    return model, history\n",
  "hyperparameters": {
    "lr": 0.001,
    "epochs": 10,
    "batch_size": 64
  },
  "reasoning": "A compact 1D CNN with depthwise-separable convolutions is effective and parameter-efficient for ECG waveform classification. MIT-BIH Arrhythmia is class-imbalanced; using macro-F1 for validation and optional inverse-frequency class weights can improve robustness. Global average pooling with a small MLP head keeps parameters well under 256K while preserving temporal receptive fields across the 1000-sample sequences.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "batch_size",
    "epochs",
    "hidden_size",
    "dropout"
  ],
  "bo_search_space": {
    "lr": {
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "type": "Categorical",
      "categories": [
        8,
        16,
        32,
        64,
        128
      ]
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "hidden_size": {
      "type": "Integer",
      "low": 32,
      "high": 512
    },
    "dropout": {
      "type": "Real",
      "low": 0.0,
      "high": 0.7
    }
  },
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757968838,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}