{
  "model_name": "Light1DCNNArrhythmia",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    \"\"\"\n    Train a lightweight 1D-CNN for 5-class ECG arrhythmia classification.\n\n    Expected input shapes:\n      - X tensors: [N, 2, L] (channel-first) or [N, L, 2] (channel-last). The model will auto-permute if needed.\n      - y tensors: [N] with integer class labels in [0, num_classes-1].\n\n    Notes:\n      - Uses pin_memory=True in DataLoader only if the tensors are on CPU (per requirement).\n      - Lightweight architecture (<256K params) with global average pooling.\n      - Basic training loop without schedulers/early stopping; suitable for Bayesian optimization.\n    \"\"\"\n    import math\n    from typing import Dict, Any\n    import torch\n    from torch import nn\n    from torch.utils.data import TensorDataset, DataLoader\n\n    # ----------------------------\n    # Hyperparameters (defaults)\n    # ----------------------------\n    defaults: Dict[str, Any] = {\n        'lr': 1e-3,\n        'epochs': 15,\n        'batch_size': 64,\n        'hidden_size': 128,   # channels in the final conv block\n        'dropout': 0.2,\n        'weight_decay': 1e-4,\n        'num_classes': 5,\n        'use_class_weights': True,\n        'max_grad_norm': None  # e.g., 1.0 to enable gradient clipping\n    }\n    params = {**defaults, **hyperparams}\n    lr = float(params['lr'])\n    epochs = int(params['epochs'])\n    batch_size = int(params['batch_size'])\n    hidden_size = int(params['hidden_size'])\n    dropout = float(params['dropout'])\n    weight_decay = float(params['weight_decay'])\n    num_classes = int(params['num_classes'])\n    use_class_weights = bool(params['use_class_weights'])\n    max_grad_norm = params['max_grad_norm']\n\n    # Ensure correct dtypes\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    # ----------------------------\n    # Datasets and DataLoaders\n    # ----------------------------\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    # IMPORTANT: Only use pin_memory=True if tensors are on CPU\n    pin_memory_flag = (X_train.device.type == 'cpu' and y_train.device.type == 'cpu')\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=0,\n        pin_memory=pin_memory_flag\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=pin_memory_flag\n    )\n\n    # ----------------------------\n    # Model Definition\n    # ----------------------------\n    class Light1DCNN(nn.Module):\n        def __init__(self, in_ch: int = 2, hidden: int = 128, n_classes: int = 5, p_drop: float = 0.2):\n            super().__init__()\n            self.feature = nn.Sequential(\n                nn.Conv1d(in_ch, 32, kernel_size=7, padding=3, bias=False),\n                nn.BatchNorm1d(32),\n                nn.ReLU(inplace=True),\n                nn.MaxPool1d(2),  # L -> L/2\n\n                nn.Conv1d(32, 64, kernel_size=5, padding=2, bias=False),\n                nn.BatchNorm1d(64),\n                nn.ReLU(inplace=True),\n                nn.MaxPool1d(2),  # L/2 -> L/4\n\n                nn.Conv1d(64, hidden, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm1d(hidden),\n                nn.ReLU(inplace=True),\n            )\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.head = nn.Sequential(\n                nn.Dropout(p=p_drop),\n                nn.Linear(hidden, n_classes)\n            )\n\n        def forward(self, x):\n            # Accept [B, 2, L] or [B, L, 2]\n            if x.ndim != 3:\n                raise ValueError(f\"Expected 3D input [B,C,L] or [B,L,C], got shape {tuple(x.shape)}\")\n            # If channel dim is last, permute to channel-first\n            if x.shape[1] not in (1, 2, 3) and x.shape[2] in (1, 2, 3):\n                x = x.permute(0, 2, 1).contiguous()\n            x = self.feature(x)\n            x = self.pool(x).squeeze(-1)\n            x = self.head(x)\n            return x\n\n    model = Light1DCNN(in_ch=2, hidden=hidden_size, n_classes=num_classes, p_drop=dropout).to(device)\n\n    # ----------------------------\n    # Loss, Optimizer\n    # ----------------------------\n    class_weights = None\n    if use_class_weights:\n        with torch.no_grad():\n            # Compute inverse-frequency class weights, normalized to mean=1\n            counts = torch.bincount(y_train.view(-1).cpu(), minlength=num_classes).float()\n            counts = torch.clamp(counts, min=1.0)\n            inv_freq = (counts.sum() / counts)  # larger weight for rarer classes\n            class_weights = inv_freq / inv_freq.mean()\n        class_weights = class_weights.to(device)\n\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    # ----------------------------\n    # Training Loop\n    # ----------------------------\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': []\n    }\n    best_val_acc = -1.0\n    best_epoch = -1\n\n    for epoch in range(epochs):\n        # Train\n        model.train()\n        total_loss = 0.0\n        total_correct = 0\n        total_examples = 0\n\n        for xb, yb in train_loader:\n            # Non-blocking only matters when pinned CPU memory is used\n            xb = xb.to(device, non_blocking=pin_memory_flag)\n            yb = yb.to(device, non_blocking=pin_memory_flag)\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if max_grad_norm is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n\n            with torch.no_grad():\n                total_loss += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                total_correct += (preds == yb).sum().item()\n                total_examples += xb.size(0)\n\n        train_loss = total_loss / max(total_examples, 1)\n        train_acc = total_correct / max(total_examples, 1)\n\n        # Validate\n        model.eval()\n        val_loss_sum = 0.0\n        val_correct = 0\n        val_examples = 0\n\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=pin_memory_flag)\n                yb = yb.to(device, non_blocking=pin_memory_flag)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n\n                val_loss_sum += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                val_correct += (preds == yb).sum().item()\n                val_examples += xb.size(0)\n\n        val_loss = val_loss_sum / max(val_examples, 1)\n        val_acc = val_correct / max(val_examples, 1)\n\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_epoch = epoch\n\n    metrics = {\n        'history': history,\n        'best_val_acc': float(best_val_acc),\n        'best_epoch': int(best_epoch)\n    }\n\n    return model, metrics\n",
  "hyperparameters": {
    "lr": 0.001,
    "epochs": 15,
    "batch_size": 64
  },
  "reasoning": "For ECG arrhythmia classification on MIT-BIH, compact 1D CNNs with small kernels (3â€“7) and global average pooling are widely effective, capturing local morphological features and remaining parameter-efficient. The dataset is class-imbalanced; inverse-frequency class weighting stabilizes training and often improves macro performance without complicating the loop. Two input leads (shape ~[1000, 2]) are handled via Conv1d with channel-first layout and an auto-permute safeguard. The code keeps the core loop simple for Bayesian optimization, and uses pin_memory only when the tensors are on CPU as required.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "batch_size",
    "epochs",
    "hidden_size",
    "dropout"
  ],
  "bo_search_space": {
    "lr": {
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "type": "Categorical",
      "categories": [
        8,
        16,
        32,
        64,
        128
      ]
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "hidden_size": {
      "type": "Integer",
      "low": 32,
      "high": 512
    },
    "dropout": {
      "type": "Real",
      "low": 0.0,
      "high": 0.7
    }
  },
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757967098,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}