{
  "model_name": "FT-Transformer-Tabular-HAR (quantizable, <=256KB final)",
  "training_code": "def train_model(\n    X_train: torch.Tensor,\n    y_train: torch.Tensor,\n    X_val: torch.Tensor,\n    y_val: torch.Tensor,\n    device,\n    # Architecture hyperparameters\n    d_token: int = 24,\n    n_layers: int = 2,\n    n_heads: int = 4,\n    ff_factor: int = 2,\n    dropout: float = 0.1,\n    # Optimization hyperparameters\n    lr: float = 3e-4,\n    weight_decay: float = 1e-4,\n    batch_size: int = 64,\n    epochs: int = 30,\n    patience: int = 10,\n    label_smoothing: float = 0.05,\n    max_grad_norm: float = 1.0,\n    # Quantization parameters\n    quantization_bits: int = 8,  # {8,16,32}\n    quantize_weights: bool = True,\n    quantize_activations: bool = False,  # dynamic quantization can't quantize activations here\n    size_budget_kb: int = 256,\n    # Dataloader workers\n    num_workers: int = 4,\n) -> (nn.Module, Dict[str, Any]):\n    import io\n    import copy\n    import warnings\n    import torch\n    from torch import nn, optim\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # ------------------------\n    # Helper utilities\n    # ------------------------\n    def _model_size_kb_from_state_dict(obj) -> float:\n        if isinstance(obj, nn.Module):\n            state = obj.state_dict()\n        elif isinstance(obj, dict):\n            state = obj\n        else:\n            raise TypeError(\"_model_size_kb_from_state_dict expects nn.Module or state_dict dict\")\n        buf = io.BytesIO()\n        torch.save(state, buf)\n        return buf.getbuffer().nbytes / 1024.0\n\n    def _apply_post_training_quantization(model: nn.Module, bits: int, q_weights: bool, q_acts: bool) -> nn.Module:\n        # Dynamic quantization is weight-only. Activations aren't quantized here.\n        m = copy.deepcopy(model).cpu().eval()\n        if not q_weights or bits >= 32:\n            return m.float()\n        try:\n            from torch.ao.quantization import quantize_dynamic\n        except Exception:\n            from torch.quantization import quantize_dynamic  # fallback alias\n        if bits <= 8:\n            return quantize_dynamic(m, {nn.Linear}, dtype=torch.qint8)\n        elif bits <= 16:\n            return quantize_dynamic(m, {nn.Linear}, dtype=torch.float16)\n        else:\n            return m.float()\n\n    def _estimate_quantized_size_kb_for_model(m: nn.Module, bits: int, q_weights: bool) -> float:\n        qm = _apply_post_training_quantization(m, bits, q_weights, False)\n        return _model_size_kb_from_state_dict(qm)\n\n    class ResidualFFN(nn.Module):\n        def __init__(self, d: int, ff_factor: int, dropout: float):\n            super().__init__()\n            hidden = max(1, d * max(1, ff_factor))\n            self.block = nn.Sequential(\n                nn.LayerNorm(d),\n                nn.Linear(d, hidden),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden, d),\n                nn.Dropout(dropout),\n            )\n        def forward(self, x):\n            return x + self.block(x)\n\n    class SimpleMLPClassifier(nn.Module):\n        def __init__(self, n_features: int, n_classes: int, d: int, n_layers: int, ff_factor: int, dropout: float):\n            super().__init__()\n            self.input = nn.Sequential(\n                nn.Linear(n_features, d),\n                nn.GELU(),\n                nn.Dropout(dropout),\n            )\n            blocks = []\n            for _ in range(max(0, n_layers)):\n                blocks.append(ResidualFFN(d, ff_factor, dropout))\n            self.blocks = nn.Sequential(*blocks)\n            self.norm = nn.LayerNorm(d)\n            self.head = nn.Linear(d, n_classes)\n        def forward(self, x):\n            x = self.input(x)\n            x = self.blocks(x) if len(self.blocks) > 0 else x\n            x = self.norm(x)\n            return self.head(x)\n\n    def _build_model_fitting_budget(\n        n_features: int,\n        n_classes: int,\n        d_token: int,\n        n_layers: int,\n        n_heads: int,\n        ff_factor: int,\n        dropout: float,\n        size_budget_kb: int,\n        quantization_bits: int,\n        quantize_weights: bool,\n    ) -> nn.Module:\n        # Start with given hyperparameters and shrink until we fit the budget.\n        # n_heads is unused in this MLP fallback but kept for signature compatibility.\n        min_d = 8\n        min_layers = 1\n        min_ff = 1\n        cur_d = max(min_d, int(d_token))\n        cur_layers = max(min_layers, int(n_layers))\n        cur_ff = max(min_ff, int(ff_factor))\n\n        def build(d, L, ff):\n            return SimpleMLPClassifier(n_features, n_classes, d, L, ff, dropout)\n\n        tried = set()\n        while True:\n            key = (cur_d, cur_layers, cur_ff)\n            if key in tried:\n                break\n            tried.add(key)\n            m = build(cur_d, cur_layers, cur_ff)\n            est_kb = _estimate_quantized_size_kb_for_model(m, quantization_bits, quantize_weights)\n            if est_kb <= size_budget_kb:\n                return m\n            # Shrink strategy: reduce width, then layers, then ff\n            if cur_d > min_d:\n                cur_d = max(min_d, cur_d // 2)\n                continue\n            if cur_layers > min_layers:\n                cur_layers -= 1\n                continue\n            if cur_ff > min_ff:\n                cur_ff -= 1\n                continue\n            break\n        # Fallback to a minimal logistic regression-like model if still over budget\n        fallback = nn.Sequential(nn.Linear(n_features, n_classes))\n        return fallback\n\n    # ------------------------\n    # Device checks\n    # ------------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        raise RuntimeError(\"This function requires a CUDA device for training. Pass device='cuda' or similar.\")\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA is not available but GPU training is required.\")\n\n    # Basic tensor prep\n    X_train = X_train.to(torch.float32)\n    X_val = X_val.to(torch.float32)\n    y_train = y_train.to(torch.long)\n    y_val = y_val.to(torch.long)\n\n    n_features = X_train.shape[1]\n    n_classes = int(torch.max(torch.cat([y_train, y_val])).item() + 1)\n\n    # Build a model that will fit the size budget after quantization\n    model = _build_model_fitting_budget(\n        n_features=n_features,\n        n_classes=n_classes,\n        d_token=d_token,\n        n_layers=n_layers,\n        n_heads=n_heads,\n        ff_factor=ff_factor,\n        dropout=dropout,\n        size_budget_kb=size_budget_kb,\n        quantization_bits=quantization_bits,\n        quantize_weights=quantize_weights,\n    )\n\n    # If quantization settings make it impossible to meet size budget, enforce 8-bit weights\n    q_try = _apply_post_training_quantization(model, quantization_bits, quantize_weights, quantize_activations)\n    final_est_kb = _model_size_kb_from_state_dict(q_try)\n    if final_est_kb > size_budget_kb:\n        quantization_bits = 8\n        quantize_weights = True\n\n    # Dataloaders\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(\n        train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False\n    )\n    val_loader = DataLoader(\n        val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, drop_last=False\n    )\n\n    model = model.to(device)\n\n    # Optimizer and scheduler\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n\n    # Loss with label smoothing\n    criterion = nn.CrossEntropyLoss(label_smoothing=float(label_smoothing)).to(device)\n\n    best_val_loss = float('inf')\n    best_state = None\n    epochs_no_improve = 0\n\n    metrics = {\n        'train_losses': [],\n        'val_losses': [],\n        'val_acc': [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if max_grad_norm is not None and max_grad_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n\n        train_loss = running_loss / max(1, total)\n\n        # Validate\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total_val = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=True)\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total_val += xb.size(0)\n\n        val_loss = val_running_loss / max(1, total_val)\n        val_acc = correct / max(1, total_val)\n\n        metrics['train_losses'].append(train_loss)\n        metrics['val_losses'].append(val_loss)\n        metrics['val_acc'].append(val_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n\n        # Early stopping on val_loss\n        if val_loss < best_val_loss - 1e-6:\n            best_val_loss = val_loss\n            best_state = copy.deepcopy(model.state_dict())\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f\"Early stopping at epoch {epoch}\")\n                break\n\n        scheduler.step()\n\n    # Restore best\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # Quantize after training (on CPU)\n    qmodel = _apply_post_training_quantization(model, quantization_bits, quantize_weights, quantize_activations)\n\n    # Ensure final size constraint\n    final_size_kb = _model_size_kb_from_state_dict(qmodel)\n    if final_size_kb > size_budget_kb:\n        print(\n            f\"Warning: quantized model size {final_size_kb:.2f}KB exceeds budget {size_budget_kb}KB; forcing 8-bit dynamic quantization.\"\n        )\n        qmodel = _apply_post_training_quantization(model, 8, True, False)\n        final_size_kb = _model_size_kb_from_state_dict(qmodel)\n        if final_size_kb > size_budget_kb:\n            raise RuntimeError(\n                f\"Final model size {final_size_kb:.2f}KB still exceeds budget of {size_budget_kb}KB after 8-bit quantization.\"\n            )\n\n    metrics['final_quantized_model_size_kb'] = final_size_kb\n    metrics['quantization_bits'] = quantization_bits\n    metrics['quantize_weights'] = quantize_weights\n    metrics['quantize_activations'] = quantize_activations\n\n    return qmodel, metrics",
  "bo_config": {
    "d_token": {
      "default": 24,
      "type": "Integer",
      "low": 8,
      "high": 64
    },
    "n_layers": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 4
    },
    "n_heads": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        2,
        4,
        8
      ]
    },
    "ff_factor": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 4
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "lr": {
      "default": 0.0003,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-08,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 64,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128
      ]
    },
    "epochs": {
      "default": 30,
      "type": "Integer",
      "low": 10,
      "high": 100
    },
    "patience": {
      "default": 10,
      "type": "Integer",
      "low": 3,
      "high": 20
    },
    "label_smoothing": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "max_grad_norm": {
      "default": 1.0,
      "type": "Real",
      "low": 0.0,
      "high": 5.0
    },
    "num_workers": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        4
      ]
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": false,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "size_budget_kb": {
      "default": 256,
      "type": "Integer",
      "low": 128,
      "high": 256
    }
  },
  "confidence": 0.87,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      10299,
      561
    ],
    "dtype": "float32",
    "feature_count": 561,
    "sample_count": 10299,
    "is_sequence": false,
    "is_image": false,
    "is_tabular": true,
    "has_labels": true,
    "label_count": 6,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1759541081,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}