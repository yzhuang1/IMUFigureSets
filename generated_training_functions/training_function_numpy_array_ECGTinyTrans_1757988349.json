{
  "model_name": "ECGTinyTrans",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    \"\"\"\n    Train a lightweight multi-scale CNN + tiny Transformer for 5-class ECG classification.\n    - Expects beats/segments shaped either (N, 2, 1000) or (N, 1000, 2).\n    - Returns (model, metrics_dict).\n    \"\"\"\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import TensorDataset, DataLoader\n    \n    # -------------------- Hyperparameters (with sensible defaults) --------------------\n    lr = hyperparams.get('lr', 5e-4)\n    epochs = hyperparams.get('epochs', 20)\n    batch_size = hyperparams.get('batch_size', 128)\n    d_model = hyperparams.get('hidden_size', 64)  # also used as CNN width\n    dropout = hyperparams.get('dropout', 0.1)\n    n_heads = hyperparams.get('n_heads', 4)\n    n_layers = hyperparams.get('n_layers', 2)\n    weight_decay = hyperparams.get('weight_decay', 1e-2)\n    label_smoothing = hyperparams.get('label_smoothing', 0.05)\n    class_weighting = hyperparams.get('class_weighting', True)\n    use_focal = hyperparams.get('use_focal', False)\n    focal_gamma = hyperparams.get('focal_gamma', 1.5)\n    grad_clip = hyperparams.get('grad_clip', 1.0)\n    num_workers = int(hyperparams.get('num_workers', 0))  # keep simple by default\n    \n    num_classes = 5\n    seq_len_expected = 1000\n    in_ch_expected = 2\n    device = torch.device(device)\n    \n    # -------------------- Shape handling and normalization --------------------\n    def to_channel_first(x):\n        # Ensure (N, C, L) from (N, C, L) or (N, L, C)\n        assert x.dim() == 3, \"X tensors must be 3D (N, C, L) or (N, L, C)\"\n        N, A, B = x.shape\n        if A == in_ch_expected and B == seq_len_expected:\n            return x.contiguous()\n        if A == seq_len_expected and B == in_ch_expected:\n            return x.permute(0, 2, 1).contiguous()\n        # Heuristic fallback: pick the axis that equals 2 as channels\n        if B == in_ch_expected:\n            return x.permute(0, 2, 1).contiguous()\n        elif A == in_ch_expected:\n            return x.contiguous()\n        else:\n            # If neither dim equals expected channels, assume channel-first already\n            return x.contiguous()\n    \n    X_train = X_train.float()\n    X_val = X_val.float()\n    y_train = y_train.long()\n    y_val = y_val.long()\n    \n    X_train_cf = to_channel_first(X_train)\n    X_val_cf = to_channel_first(X_val)\n    \n    # Standardize per-channel using training statistics\n    # mean/std over (N, L)\n    mean = X_train_cf.mean(dim=(0, 2), keepdim=True)\n    std = X_train_cf.std(dim=(0, 2), keepdim=True).clamp_min(1e-5)\n    X_train_cf = (X_train_cf - mean) / std\n    X_val_cf = (X_val_cf - mean) / std\n    \n    # -------------------- Model definition --------------------\n    class ECGTinyTrans(nn.Module):\n        def __init__(self, in_ch=2, d_model=64, n_heads=4, n_layers=2, dropout=0.1, num_classes=5):\n            super().__init__()\n            # CNN stem: downsample 1000 -> 500 -> 250 tokens\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, d_model // 2, kernel_size=7, stride=2, padding=3, bias=False),\n                nn.BatchNorm1d(d_model // 2),\n                nn.GELU(),\n                nn.Conv1d(d_model // 2, d_model, kernel_size=7, stride=2, padding=3, bias=False),\n                nn.BatchNorm1d(d_model),\n                nn.GELU(),\n            )\n            # Transformer encoder\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=n_heads,\n                dim_feedforward=4 * d_model,\n                dropout=dropout,\n                activation='gelu',\n                batch_first=True,\n                norm_first=True,\n            )\n            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n            self.dropout = nn.Dropout(dropout)\n            # Classifier head uses mean+max pooling\n            self.fc = nn.Linear(2 * d_model, num_classes)\n        \n        @staticmethod\n        def sinusoidal_pe(L, d_model, device):\n            pe = torch.zeros(L, d_model, device=device)\n            position = torch.arange(0, L, dtype=torch.float32, device=device).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            return pe\n        \n        def forward(self, x):\n            # x: (N, C, L)\n            x = self.stem(x)  # (N, d_model, L') with L' ~ 250\n            x = x.permute(0, 2, 1)  # (N, L', d_model)\n            L = x.size(1)\n            pe = self.sinusoidal_pe(L, x.size(-1), x.device)\n            x = x + pe.unsqueeze(0)\n            x = self.encoder(x)\n            # Global mean + max pooling\n            mean_pool = x.mean(dim=1)\n            max_pool, _ = x.max(dim=1)\n            h = torch.cat([mean_pool, max_pool], dim=-1)\n            h = self.dropout(h)\n            logits = self.fc(h)\n            return logits\n    \n    model = ECGTinyTrans(in_ch=in_ch_expected, d_model=d_model, n_heads=n_heads, n_layers=n_layers, dropout=dropout, num_classes=num_classes).to(device)\n    \n    # -------------------- Loss, optimizer --------------------\n    # Class weighting (AAMI five-class is typically imbalanced)\n    class_weights = None\n    if class_weighting:\n        with torch.no_grad():\n            counts = torch.bincount(y_train.view(-1), minlength=num_classes).float()\n            weights = 1.0 / (counts + 1e-6)\n            weights = weights * (num_classes / weights.sum())\n            class_weights = weights.to(device)\n    \n    def loss_fn(logits, targets):\n        if use_focal:\n            # Focal reweighting on top of label-smoothed CE\n            ce = F.cross_entropy(logits, targets, weight=class_weights, reduction='none', label_smoothing=label_smoothing)\n            with torch.no_grad():\n                pt = F.softmax(logits, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1).clamp_min(1e-8)\n            focal = (1.0 - pt) ** float(focal_gamma)\n            return (focal * ce).mean()\n        else:\n            return F.cross_entropy(logits, targets, weight=class_weights, label_smoothing=label_smoothing)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    use_amp = (device.type == 'cuda')\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n    \n    # -------------------- DataLoaders --------------------\n    train_ds = TensorDataset(X_train_cf, y_train)\n    val_ds = TensorDataset(X_val_cf, y_val)\n    pin_mem_flag = (X_train_cf.device.type == 'cpu')  # IMPORTANT: only pin if tensors on CPU\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_mem_flag)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_mem_flag)\n    \n    # -------------------- Training loop --------------------\n    history = {\n        'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_macro_f1': [],\n        'per_class_precision': None, 'per_class_recall': None, 'confusion_matrix': None\n    }\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0.0\n        total_samples = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=pin_mem_flag)\n            yb = yb.to(device, non_blocking=pin_mem_flag)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(xb)\n                loss = loss_fn(logits, yb)\n            scaler.scale(loss).then_unscale_(optimizer)\n            if grad_clip is not None and grad_clip > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item() * yb.size(0)\n            total_samples += yb.size(0)\n        train_epoch_loss = total_loss / max(total_samples, 1)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_samples = 0\n        all_preds = []\n        all_tgts = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=pin_mem_flag)\n                yb = yb.to(device, non_blocking=pin_mem_flag)\n                with torch.cuda.amp.autocast(enabled=use_amp):\n                    logits = model(xb)\n                    loss = loss_fn(logits, yb)\n                    probs = F.softmax(logits, dim=1)\n                    preds = probs.argmax(dim=1)\n                val_loss += loss.item() * yb.size(0)\n                val_samples += yb.size(0)\n                all_preds.append(preds.detach().cpu())\n                all_tgts.append(yb.detach().cpu())\n        \n        val_epoch_loss = val_loss / max(val_samples, 1)\n        y_true = torch.cat(all_tgts, dim=0)\n        y_pred = torch.cat(all_preds, dim=0)\n        correct = (y_true == y_pred).sum().item()\n        val_acc = correct / max(len(y_true), 1)\n        \n        # Confusion matrix and per-class metrics\n        cm = torch.zeros((num_classes, num_classes), dtype=torch.long)\n        for t, p in zip(y_true, y_pred):\n            cm[t, p] += 1\n        tp = cm.diag().to(torch.float32)\n        fp = cm.sum(dim=0).to(torch.float32) - tp\n        fn = cm.sum(dim=1).to(torch.float32) - tp\n        eps = 1e-12\n        precision_c = (tp / (tp + fp + eps)).tolist()\n        recall_c = (tp / (tp + fn + eps)).tolist()  # sensitivity\n        f1_c = [ (2 * p * r) / (p + r + eps) for p, r in zip(precision_c, recall_c) ]\n        macro_f1 = float(sum(f1_c) / num_classes)\n        \n        history['train_loss'].append(float(train_epoch_loss))\n        history['val_loss'].append(float(val_epoch_loss))\n        history['val_acc'].append(float(val_acc))\n        history['val_macro_f1'].append(float(macro_f1))\n        history['per_class_precision'] = precision_c\n        history['per_class_recall'] = recall_c\n        history['confusion_matrix'] = cm.tolist()\n    \n    return model, history\n",
  "hyperparameters": {
    "lr": 0.0005,
    "epochs": 20,
    "batch_size": 128
  },
  "reasoning": "Implements a lightweight multi-scale CNN front-end (k=7, stride 2) to capture QRS/P/T morphology, followed by a tiny Transformer (2 layers, d_model=64, 4 heads) operating on downsampled tokens (~250) to respect memory constraints at L=1000. This reflects recent findings that hybrid local+global encoders work well for MIT-BIH five-class mapping (AAMI N/S/V/F/Q) while keeping parameters under 256K. Label smoothing and optional class weighting mitigate class imbalance; we report accuracy, macro-F1, and per-class precision/recall, aligning with recommended metrics. Pin memory is used only when tensors reside on CPU, and the loop is kept simple for BO to tune core hyperparameters.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "batch_size",
    "epochs",
    "hidden_size",
    "dropout"
  ],
  "bo_search_space": {
    "lr": {
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "type": "Categorical",
      "categories": [
        8,
        16,
        32,
        64,
        128
      ]
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "hidden_size": {
      "type": "Integer",
      "low": 32,
      "high": 512
    },
    "dropout": {
      "type": "Real",
      "low": 0.0,
      "high": 0.7
    }
  },
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757988349,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}