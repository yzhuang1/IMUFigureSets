{
  "model_name": "Tiny1DTransformerRR",
  "training_code": "import math\\nimport copy\\nfrom typing import Dict, Any\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\\nimport torch.ao.quantization as tq\\n\\n\\nclass MLP(nn.Module):\\n    def __init__(self, dim: int, hidden: int, dropout: float):\\n        super().__init__()\\n        self.fc1 = nn.Linear(dim, hidden)\\n        self.act = nn.GELU()\\n        self.fc2 = nn.Linear(hidden, dim)\\n        self.drop = nn.Dropout(dropout)\\n\\n    def forward(self, x):\\n        x = self.fc1(x)\\n        x = self.act(x)\\n        x = self.drop(x)\\n        x = self.fc2(x)\\n        x = self.drop(x)\\n        return x\\n\\n\\nclass EncoderBlock(nn.Module):\\n    def __init__(self, dim: int, n_heads: int, mlp_ratio: int, dropout: float):\\n        super().__init__()\\n        self.norm1 = nn.LayerNorm(dim)\\n        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, dropout=dropout, batch_first=True)\\n        self.drop_path = nn.Dropout(dropout)\\n        self.norm2 = nn.LayerNorm(dim)\\n        self.mlp = MLP(dim, hidden=dim * mlp_ratio, dropout=dropout)\\n\\n    def forward(self, x):\\n        # Pre-LN Transformer encoder\\n        h = self.norm1(x)\\n        attn_out, _ = self.attn(h, h, h, need_weights=False)\\n        x = x + self.drop_path(attn_out)\\n        h = self.norm2(x)\\n        x = x + self.drop_path(self.mlp(h))\\n        return x\\n\\n\\nclass QuantizableHead(nn.Module):\\n    def __init__(self, in_dim: int, num_classes: int, side_dim: int = 2, hidden: int = 64, dropout: float = 0.1):\\n        super().__init__()\\n        # Quantization stubs enable static quantization of this head only\\n        self.quant = tq.QuantStub()\\n        self.dequant = tq.DeQuantStub()\\n        self.fc1 = nn.Linear(in_dim + side_dim, hidden)\\n        self.act = nn.GELU()\\n        self.drop = nn.Dropout(dropout)\\n        self.fc2 = nn.Linear(hidden, num_classes)\\n\\n    def forward(self, x, side_feats=None):\\n        if side_feats is None:\\n            side_feats = torch.zeros(x.size(0), 2, device=x.device, dtype=x.dtype)\\n        x = torch.cat([x, side_feats], dim=-1)\\n        # Quantize activations into the head if configured and prepared\\n        x = self.quant(x)\\n        x = self.fc1(x)\\n        x = self.act(x)\\n        x = self.drop(x)\\n        x = self.fc2(x)\\n        x = self.dequant(x)\\n        return x\\n\\n\\nclass Tiny1DTransformer(nn.Module):\\n    def __init__(self, seq_len: int = 1000, in_ch: int = 2, patch_size: int = 40, d_model: int = 64, n_heads: int = 4,\\n                 num_layers: int = 3, mlp_ratio: int = 2, dropout: float = 0.1, num_classes: int = 5, head_hidden: int = 64):\\n        super().__init__()\\n        assert seq_len % patch_size == 0, \"seq_len must be divisible by patch_size\"\\n        self.seq_len = seq_len\\n        self.in_ch = in_ch\\n        self.patch_size = patch_size\\n        self.num_patches = seq_len // patch_size\\n        self.d_model = d_model\\n\\n        self.patch_proj = nn.Linear(in_ch * patch_size, d_model)\\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, d_model))\\n        self.pos_drop = nn.Dropout(dropout)\\n\\n        self.blocks = nn.ModuleList([\\n            EncoderBlock(d_model, n_heads, mlp_ratio, dropout) for _ in range(num_layers)\\n        ])\\n        self.norm = nn.LayerNorm(d_model)\\n        self.head = QuantizableHead(in_dim=d_model, num_classes=num_classes, side_dim=2, hidden=head_hidden, dropout=dropout)\\n\\n        self.apply(self._init_weights)\\n\\n    @staticmethod\\n    def _init_weights(m):\\n        if isinstance(m, nn.Linear):\\n            nn.init.trunc_normal_(m.weight, std=0.02)\\n            if m.bias is not None:\\n                nn.init.zeros_(m.bias)\\n        elif isinstance(m, nn.LayerNorm):\\n            nn.init.ones_(m.weight)\\n            nn.init.zeros_(m.bias)\\n\\n    def patchify(self, x):\\n        # x: (B, C=2, L=1000) -> patches: (B, Np, C*P)\\n        B, C, L = x.shape\\n        P = self.patch_size\\n        x = x.unfold(dimension=2, size=P, step=P)  # (B, C, Np, P)\\n        x = x.permute(0, 2, 1, 3).contiguous()    # (B, Np, C, P)\\n        x = x.view(B, self.num_patches, C * P)     # (B, Np, C*P)\\n        return x\\n\\n    def forward(self, x, side_feats=None):\\n        # Expect x as (B, 2, 1000)\\n        patches = self.patchify(x)\\n        tokens = self.patch_proj(patches) + self.pos_embed\\n        x = self.pos_drop(tokens)\\n        for blk in self.blocks:\\n            x = blk(x)\\n        x = self.norm(x)\\n        x = x.mean(dim=1)  # mean pooling over tokens\\n        logits = self.head(x, side_feats=side_feats)\\n        return logits\\n\\n\\nclass FocalLoss(nn.Module):\\n    def __init__(self, gamma: float = 2.0, weight: torch.Tensor = None, reduction: str = 'mean'):\\n        super().__init__()\\n        self.gamma = gamma\\n        self.weight = weight\\n        self.reduction = reduction\\n        self.ce = nn.CrossEntropyLoss(weight=weight, reduction='none')\\n\\n    def forward(self, logits, target):\\n        ce = self.ce(logits, target)\\n        pt = torch.softmax(logits, dim=-1).gather(1, target.view(-1, 1)).squeeze(1).clamp(1e-6, 1.0)\\n        loss = ((1.0 - pt) ** self.gamma) * ce\\n        if self.reduction == 'mean':\\n            return loss.mean()\\n        elif self.reduction == 'sum':\\n            return loss.sum()\\n        else:\\n            return loss\\n\\n\\ndef _ensure_channel_first(X: torch.Tensor) -> torch.Tensor:\\n    # Converts (N, 1000, 2) or (N, 2, 1000) to (N, 2, 1000)\\n    if X.dim() != 3:\\n        raise ValueError(\"Input X must be 3D: (N, 2, 1000) or (N, 1000, 2)\")\\n    N, A, B = X.shape\\n    if A == 2 and B == 1000:\\n        return X\\n    if A == 1000 and B == 2:\\n        return X.permute(0, 2, 1).contiguous()\\n    raise ValueError(f\"Unexpected input shape: {tuple(X.shape)}. Expected (N,2,1000) or (N,1000,2)\")\\n\\n\\ndef _build_model_with_budget(seq_len: int, patch_size: int, d_model: int, n_heads: int, num_layers: int, mlp_ratio: int,\\n                             dropout: float, head_hidden: int, num_classes: int = 5, max_params: int = 256000) -> Tiny1DTransformer:\\n    # Ensure heads divide d_model\\n    if n_heads > d_model:\\n        n_heads = max(1, d_model // 2) or 1\\n    # Adjust n_heads to divide d_model\\n    while d_model % n_heads != 0 and n_heads > 1:\\n        n_heads -= 1\\n    if d_model % n_heads != 0:\\n        # fallback to 1 head\\n        n_heads = 1\\n\\n    model = Tiny1DTransformer(seq_len=seq_len, in_ch=2, patch_size=patch_size, d_model=d_model, n_heads=n_heads,\\n                              num_layers=num_layers, mlp_ratio=mlp_ratio, dropout=dropout, num_classes=num_classes,\\n                              head_hidden=head_hidden)\\n    def nparams(m):\\n        return sum(p.numel() for p in m.parameters())\\n\\n    total = nparams(model)\\n    # Try to shrink automatically if over budget\\n    while total > max_params:\\n        if d_model > 64:\\n            d_model = max(32, d_model // 2)\\n        elif num_layers > 2:\\n            num_layers -= 1\\n        elif mlp_ratio > 2:\\n            mlp_ratio -= 1\\n        elif head_hidden > 48:\\n            head_hidden = max(32, head_hidden // 2)\\n        else:\\n            break\\n        # Recompute heads divisibility\\n        if n_heads > d_model:\\n            n_heads = max(1, d_model // 2) or 1\\n        while d_model % n_heads != 0 and n_heads > 1:\\n            n_heads -= 1\\n        if d_model % n_heads != 0:\\n            n_heads = 1\\n        model = Tiny1DTransformer(seq_len=seq_len, in_ch=2, patch_size=patch_size, d_model=d_model, n_heads=n_heads,\\n                                  num_layers=num_layers, mlp_ratio=mlp_ratio, dropout=dropout, num_classes=num_classes,\\n                                  head_hidden=head_hidden)\\n        total = nparams(model)\\n    return model\\n\\n\\ndef _quantize_model_ptq(model: nn.Module, train_loader: DataLoader, quantization_bits: int, quantize_weights: bool,\\n                        quantize_activations: bool, calib_batches: int = 4) -> nn.Module:\\n    model.eval()\\n    model_cpu = copy.deepcopy(model).cpu()\\n\\n    # Optional static quantization of the classifier head activations (int8)\\n    if quantize_activations and quantization_bits == 8:\\n        try:\\n            model_cpu.head.qconfig = tq.get_default_qconfig('fbgemm')\\n            tq.prepare(model_cpu.head, inplace=True)\\n            with torch.no_grad():\\n                seen = 0\\n                for xb, yb in train_loader:\\n                    xb = xb.cpu()\\n                    _ = model_cpu(xb)\\n                    seen += 1\\n                    if seen >= calib_batches:\\n                        break\\n            tq.convert(model_cpu.head, inplace=True)\\n        except Exception as e:\\n            # Fallback gracefully if static quantization is not supported in environment\\n            pass\\n\\n    # Dynamic weight-only quantization for transformer Linear layers\\n    if quantize_weights and quantization_bits in (8, 16):\\n        dtype = torch.qint8 if quantization_bits == 8 else torch.float16\\n        # Quantize nn.Linear layers only; attention uses Linear internally\\n        model_cpu = tq.quantize_dynamic(model_cpu, {nn.Linear}, dtype=dtype, inplace=True)\\n\\n    return model_cpu\\n\\n\\ndef train_model(X_train: torch.Tensor, y_train: torch.Tensor, X_val: torch.Tensor, y_val: torch.Tensor, device: str,\\n                # Optimization\\n                lr: float = 1e-3, batch_size: int = 128, epochs: int = 20, weight_decay: float = 1e-4,\\n                optimizer: str = 'adamw', scheduler: str = 'cosine', warmup_epochs: int = 2, grad_clip: float = 0.5,\\n                amp: bool = True, seed: int = 42,\\n                # Architecture\\n                patch_size: int = 40, d_model: int = 64, n_heads: int = 4, num_layers: int = 3, mlp_ratio: int = 2,\\n                dropout: float = 0.1, head_hidden: int = 64,\\n                # Loss / sampling\\n                use_focal_loss: bool = True, focal_gamma: float = 2.0, label_smoothing: float = 0.05,\\n                class_weight_power: float = 0.5,\\n                # Quantization\\n                quantization_bits: int = 8, quantize_weights: bool = True, quantize_activations: bool = False,\\n                calib_batches: int = 4) -> tuple:\\n    \\\"\\\"\\\"\\n    Train a tiny 1D Transformer (patchified) classifier for 5 classes on inputs shaped (N, 2, 1000).\\n\\n    Args:\\n        X_train, y_train, X_val, y_val: torch.Tensors. X_* must be (N,2,1000) or (N,1000,2).\\n        device: 'cpu' or 'cuda'.\\n        Hyperparameters and quantization parameters as keyword args.\\n\\n    Returns:\\n        quantized_model (nn.Module), metrics dict with keys: train_losses, val_losses, val_acc\\n    \\\"\\\"\\\"\\n    torch.manual_seed(seed)\\n\\n    # Ensure shapes\\n    X_train = _ensure_channel_first(X_train).float()\\n    X_val = _ensure_channel_first(X_val).float()\\n    y_train = y_train.long()\\n    y_val = y_val.long()\\n\\n    num_classes = int(torch.unique(y_train).numel()) if y_train.numel() > 0 else 5\\n\\n    # Weighted sampler to mitigate class imbalance\\n    with torch.no_grad():\\n        classes, counts = torch.unique(y_train, return_counts=True)\\n        counts = counts.float()\\n        inv = 1.0 / (counts + 1e-6)\\n        inv = inv / inv.sum() * len(classes)\\n        # Temperature with class_weight_power\\n        class_weights_sampler = inv ** class_weight_power\\n        weight_per_sample = class_weights_sampler[y_train]  # map by class index order\\n    sampler = WeightedRandomSampler(weights=weight_per_sample.double(), num_samples=len(weight_per_sample), replacement=True)\\n\\n    train_ds = TensorDataset(X_train, y_train)\\n    val_ds = TensorDataset(X_val, y_val)\\n    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, shuffle=False, num_workers=0, pin_memory=False)\\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\\n\\n    # Build model under parameter budget\\n    model = _build_model_with_budget(seq_len=1000, patch_size=patch_size, d_model=d_model, n_heads=n_heads,\\n                                     num_layers=num_layers, mlp_ratio=mlp_ratio, dropout=dropout,\\n                                     head_hidden=head_hidden, num_classes=num_classes, max_params=256000)\\n\\n    model.to(device)\\n\\n    # Class weights for loss (CrossEntropy supports weights)\\n    with torch.no_grad():\\n        # Map counts to a dense vector aligned with class indices [0..num_classes-1]\\n        class_counts = torch.zeros(num_classes, dtype=torch.float32)\\n        for c, cnt in zip(classes, counts):\\n            class_counts[int(c.item())] = cnt.float()\\n        cw = (1.0 / (class_counts + 1e-6)) ** class_weight_power\\n        cw = cw / cw.mean()\\n        class_weights_ce = cw.to(device)\\n\\n    if use_focal_loss:\\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights_ce, reduction='mean')\\n    else:\\n        criterion = nn.CrossEntropyLoss(weight=class_weights_ce, label_smoothing=label_smoothing)\\n\\n    # Optimizer\\n    if optimizer.lower() == 'adamw':\\n        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\\n    else:\\n        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\\n\\n    # Scheduler (cosine with warmup)\\n    if scheduler.lower() == 'cosine':\\n        t_max = max(1, epochs - max(0, warmup_epochs))\\n        cos = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=t_max, eta_min=lr * 0.05)\\n    else:\\n        cos = None\\n\\n    scaler = torch.cuda.amp.GradScaler(enabled=(amp and (device.startswith('cuda'))))\\n\\n    def evaluate():\\n        model.eval()\\n        total = 0\\n        correct = 0\\n        val_loss = 0.0\\n        with torch.no_grad():\\n            for xb, yb in val_loader:\\n                xb = xb.to(device)\\n                yb = yb.to(device)\\n                logits = model(xb)\\n                loss = criterion(logits, yb)\\n                val_loss += loss.item() * xb.size(0)\\n                preds = logits.argmax(dim=-1)\\n                correct += (preds == yb).sum().item()\\n                total += xb.size(0)\\n        val_loss /= max(1, total)\\n        val_acc = correct / max(1, total)\\n        return val_loss, val_acc\\n\\n    train_losses, val_losses, val_accs = [], [], []\\n\\n    # Training loop\\n    for epoch in range(1, epochs + 1):\\n        model.train()\\n        epoch_loss = 0.0\\n        seen = 0\\n        # Warmup LR\\n        if scheduler.lower() == 'cosine' and warmup_epochs > 0 and epoch <= warmup_epochs:\\n            warmup_factor = epoch / float(max(1, warmup_epochs))\\n            for g in opt.param_groups:\\n                g['lr'] = lr * warmup_factor\\n\\n        for xb, yb in train_loader:\\n            xb = xb.to(device)\\n            yb = yb.to(device)\\n            opt.zero_grad(set_to_none=True)\\n            if scaler.is_enabled():\\n                with torch.cuda.amp.autocast():\\n                    logits = model(xb)\\n                    loss = criterion(logits, yb)\\n                scaler.scale(loss).backward()\\n                if grad_clip and grad_clip > 0:\\n                    scaler.unscale_(opt)\\n                    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\\n                scaler.step(opt)\\n                scaler.update()\\n            else:\\n                logits = model(xb)\\n                loss = criterion(logits, yb)\\n                loss.backward()\\n                if grad_clip and grad_clip > 0:\\n                    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\\n                opt.step()\\n            epoch_loss += loss.item() * xb.size(0)\\n            seen += xb.size(0)\\n\\n        # Step cosine scheduler after warmup\\n        if scheduler.lower() == 'cosine' and (warmup_epochs == 0 or epoch > warmup_epochs):\\n            cos.step()\\n\\n        train_loss = epoch_loss / max(1, seen)\\n        v_loss, v_acc = evaluate()\\n\\n        train_losses.append(train_loss)\\n        val_losses.append(v_loss)\\n        val_accs.append(v_acc)\\n\\n        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={v_loss:.4f} | val_acc={v_acc:.4f}\")\\n\\n    # Post-training quantization (on CPU)\\n    # Build a small CPU loader for calibration if needed\\n    calib_loader = DataLoader(train_ds, batch_size=min(batch_size, 64), shuffle=True, num_workers=0, pin_memory=False)\\n    quantized_model = _quantize_model_ptq(model, calib_loader, quantization_bits, quantize_weights, quantize_activations, calib_batches=calib_batches)\\n\\n    metrics = {\\n        'train_losses': train_losses,\\n        'val_losses': val_losses,\\n        'val_acc': val_accs\\n    }\\n\\n    return quantized_model, metrics\\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.005,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 128,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128,
        256
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 60
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "optimizer": {
      "default": "adamw",
      "type": "Categorical",
      "categories": [
        "adamw",
        "adam"
      ]
    },
    "scheduler": {
      "default": "cosine",
      "type": "Categorical",
      "categories": [
        "none",
        "cosine"
      ]
    },
    "warmup_epochs": {
      "default": 2,
      "type": "Integer",
      "low": 0,
      "high": 5
    },
    "grad_clip": {
      "default": 0.5,
      "type": "Real",
      "low": 0.0,
      "high": 1.0
    },
    "amp": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "patch_size": {
      "default": 40,
      "type": "Categorical",
      "categories": [
        20,
        25,
        40,
        50,
        100,
        125,
        200
      ]
    },
    "d_model": {
      "default": 64,
      "type": "Integer",
      "low": 32,
      "high": 128
    },
    "n_heads": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        1,
        2,
        4,
        8
      ]
    },
    "num_layers": {
      "default": 3,
      "type": "Integer",
      "low": 2,
      "high": 3
    },
    "mlp_ratio": {
      "default": 2,
      "type": "Integer",
      "low": 2,
      "high": 4
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "head_hidden": {
      "default": 64,
      "type": "Integer",
      "low": 32,
      "high": 128
    },
    "use_focal_loss": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 1.0,
      "high": 3.0
    },
    "label_smoothing": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "class_weight_power": {
      "default": 0.5,
      "type": "Real",
      "low": 0.0,
      "high": 1.0
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": false,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "calib_batches": {
      "default": 4,
      "type": "Integer",
      "low": 1,
      "high": 10
    }
  },
  "confidence": 0.9,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758572548,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}