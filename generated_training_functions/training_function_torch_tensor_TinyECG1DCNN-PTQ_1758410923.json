{
  "model_name": "TinyECG1DCNN-PTQ",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a compact 1D-CNN (<=256K params) for 5-class classification and perform post-training quantization.\n    Inputs:\n      - X_train, X_val: torch.Tensor of shape (N, 1000, 2) or (N, 2, 1000)\n      - y_train, y_val: torch.Tensor of shape (N,) with class indices [0..4]\n      - device: torch.device for training\n      - kwargs: hyperparameters and quantization parameters\n        * lr, batch_size, epochs, weight_decay, base_channels, hidden_dim, dropout, grad_clip, seed\n        * quantization_bits in {8,16,32}\n        * quantize_weights in {True, False}\n        * quantize_activations in {True, False}\n        * calibration_batches (for static int8 PTQ)\n    Returns:\n      - quantized_model: post-training quantized model (may reside on CPU for int8/dynamic; fp16 on CUDA if chosen)\n      - metrics: {train_losses, val_losses, val_acc, param_count}\n    Notes:\n      - DataLoaders use pin_memory=False to avoid CUDA tensor pinning errors as requested.\n      - Static int8 quantization uses fbgemm backend (CPU) and fuses Conv-BN-ReLU before calibration/convert.\n      - fp16: if activations+weights True and CUDA available, casts full model to half; otherwise dynamic fp16 for Linear layers.\n    \"\"\"\n    import copy\n    import random\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import TensorDataset, DataLoader\n    from collections import OrderedDict\n\n    # -------------------- Hyperparams --------------------\n    lr = float(kwargs.get(\"lr\", 1e-3))\n    batch_size = int(kwargs.get(\"batch_size\", 128))\n    epochs = int(kwargs.get(\"epochs\", 20))\n    weight_decay = float(kwargs.get(\"weight_decay\", 1e-4))\n    base_channels = int(kwargs.get(\"base_channels\", 16))\n    hidden_dim = int(kwargs.get(\"hidden_dim\", 64))\n    dropout = float(kwargs.get(\"dropout\", 0.2))\n    grad_clip = float(kwargs.get(\"grad_clip\", 0.5))\n    seed = int(kwargs.get(\"seed\", 42))\n\n    quantization_bits = int(kwargs.get(\"quantization_bits\", 8))\n    quantize_weights = bool(kwargs.get(\"quantize_weights\", True))\n    quantize_activations = bool(kwargs.get(\"quantize_activations\", True))\n    calibration_batches = int(kwargs.get(\"calibration_batches\", 32))\n\n    # -------------------- Seeding --------------------\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n    # -------------------- Shape handling --------------------\n    def ensure_channels_first(X: torch.Tensor) -> torch.Tensor:\n        if not isinstance(X, torch.Tensor):\n            raise TypeError(\"X must be a torch.Tensor\")\n        if X.dim() != 3:\n            raise ValueError(f\"Expected X to have 3 dims (N,C,L) or (N,L,C); got {tuple(X.shape)}\")\n        # If last dim is 2 (features), move to channels-first\n        if X.shape[-1] == 2 and X.shape[1] != 2:\n            X = X.permute(0, 2, 1).contiguous()\n        if X.shape[1] != 2:\n            raise ValueError(f\"Expected 2 input channels; got shape {tuple(X.shape)}\")\n        return X.float()\n\n    X_train = ensure_channels_first(X_train)\n    X_val = ensure_channels_first(X_val)\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    # -------------------- DataLoaders --------------------\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # -------------------- Model --------------------\n    class TinyECGNet(nn.Module):\n        def __init__(self, in_ch=2, base_ch=16, hidden=64, num_classes=5, drop=0.2):\n            super().__init__()\n            self.features = nn.Sequential(OrderedDict([\n                (\"conv1\", nn.Conv1d(in_ch, base_ch, kernel_size=7, stride=2, padding=3, bias=False)),\n                (\"bn1\", nn.BatchNorm1d(base_ch)),\n                (\"relu1\", nn.ReLU(inplace=True)),\n                (\"conv2\", nn.Conv1d(base_ch, base_ch * 2, kernel_size=5, stride=2, padding=2, bias=False)),\n                (\"bn2\", nn.BatchNorm1d(base_ch * 2)),\n                (\"relu2\", nn.ReLU(inplace=True)),\n                (\"conv3\", nn.Conv1d(base_ch * 2, base_ch * 4, kernel_size=5, stride=2, padding=2, bias=False)),\n                (\"bn3\", nn.BatchNorm1d(base_ch * 4)),\n                (\"relu3\", nn.ReLU(inplace=True)),\n                (\"gap\", nn.AdaptiveAvgPool1d(1)),\n            ]))\n            self.dropout = nn.Dropout(drop)\n            self.classifier = nn.Sequential(\n                nn.Linear(base_ch * 4, hidden),\n                nn.ReLU(inplace=True),\n                nn.Dropout(drop),\n                nn.Linear(hidden, num_classes),\n            )\n\n        def forward(self, x):\n            x = self.features(x)\n            x = x.flatten(1)\n            x = self.dropout(x)\n            x = self.classifier(x)\n            return x\n\n        def fuse_model(self):\n            from torch.ao.quantization import fuse_modules\n            fuse_modules(self, [\n                [\"features.conv1\", \"features.bn1\", \"features.relu1\"],\n                [\"features.conv2\", \"features.bn2\", \"features.relu2\"],\n                [\"features.conv3\", \"features.bn3\", \"features.relu3\"],\n                [\"classifier.0\", \"classifier.1\"],\n            ], inplace=True)\n\n    model = TinyECGNet(in_ch=2, base_ch=base_channels, hidden=hidden_dim, num_classes=5, drop=dropout).to(device)\n\n    total_params = sum(p.numel() for p in model.parameters())\n    if total_params > 256000:\n        raise ValueError(f\"Model has {total_params} parameters which exceeds 256000. Reduce base_channels/hidden_dim.\")\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    criterion = nn.CrossEntropyLoss()\n\n    # -------------------- Training loop --------------------\n    train_losses, val_losses, val_acc = [], [], []\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running = 0.0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip > 0.0:\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            optimizer.step()\n            running += loss.item() * xb.size(0)\n        epoch_train_loss = running / len(train_loader.dataset)\n\n        model.eval()\n        vloss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                vloss += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += yb.size(0)\n        epoch_val_loss = vloss / len(val_loader.dataset)\n        epoch_val_acc = correct / max(1, total)\n\n        train_losses.append(epoch_train_loss)\n        val_losses.append(epoch_val_loss)\n        val_acc.append(epoch_val_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={epoch_train_loss:.6f} val_loss={epoch_val_loss:.6f} val_acc={epoch_val_acc:.4f}\")\n\n    # -------------------- Post-training quantization --------------------\n    q_model = model.cpu().eval()\n\n    def static_int8_quantize(m, calib_loader, max_batches):\n        import torch\n        torch.backends.quantized.engine = \"fbgemm\"\n        qm = copy.deepcopy(m)\n        if hasattr(qm, \"fuse_model\"):\n            qm.fuse_model()\n        qm.qconfig = torch.ao.quantization.get_default_qconfig(\"fbgemm\")\n        torch.ao.quantization.prepare(qm, inplace=True)\n        seen = 0\n        with torch.no_grad():\n            for xb, yb in calib_loader:\n                xb = xb.cpu()\n                qm(xb)\n                seen += 1\n                if seen >= max_batches:\n                    break\n        torch.ao.quantization.convert(qm, inplace=True)\n        return qm\n\n    if quantization_bits == 8:\n        if quantize_activations and not quantize_weights:\n            print(\"Note: quantize_activations=True with 8-bit requires weight quantization; enabling quantize_weights.\")\n            quantize_weights = True\n        if quantize_weights and quantize_activations:\n            calib_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n            q_model = static_int8_quantize(q_model, calib_loader, calibration_batches)\n        elif quantize_weights and not quantize_activations:\n            q_model = torch.ao.quantization.quantize_dynamic(q_model, {nn.Linear}, dtype=torch.qint8)\n        else:\n            q_model = q_model  # no quantization\n    elif quantization_bits == 16:\n        if quantize_weights and quantize_activations:\n            if device.type == \"cuda\" and torch.cuda.is_available():\n                q_model = model.half().to(device).eval()\n            else:\n                q_model = torch.ao.quantization.quantize_dynamic(q_model, {nn.Linear}, dtype=torch.float16)\n        elif quantize_weights and not quantize_activations:\n            q_model = torch.ao.quantization.quantize_dynamic(q_model, {nn.Linear}, dtype=torch.float16)\n        else:\n            print(\"Note: activation-only fp16 quantization is not supported; returning float32 model.\")\n    else:\n        q_model = q_model  # 32-bit: no quantization\n\n    metrics = {\n        \"train_losses\": train_losses,\n        \"val_losses\": val_losses,\n        \"val_acc\": val_acc,\n        \"param_count\": total_params\n    }\n    return q_model, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.003,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 128,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128,
        256
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 100
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "base_channels": {
      "default": 16,
      "type": "Integer",
      "low": 8,
      "high": 32
    },
    "hidden_dim": {
      "default": 64,
      "type": "Integer",
      "low": 16,
      "high": 128
    },
    "dropout": {
      "default": 0.2,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "grad_clip": {
      "default": 0.5,
      "type": "Real",
      "low": 0.0,
      "high": 1.0
    },
    "calibration_batches": {
      "default": 32,
      "type": "Integer",
      "low": 8,
      "high": 128
    },
    "seed": {
      "default": 42,
      "type": "Integer",
      "low": 0,
      "high": 1000000
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "confidence": 0.9,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758410923,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}