{
  "model_name": "FT-Transformer (compact, per-feature tokenization)",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device,\n                num_classes=6,\n                d_head=16,\n                n_heads=4,\n                n_layers=2,\n                d_ff_multiplier=2,\n                dropout=0.3,\n                token_dropout=0.2,\n                mixup_alpha=0.2,\n                batch_size=128,\n                epochs=30,\n                lr=1e-3,\n                weight_decay=1e-4,\n                label_smoothing=0.05,\n                patience=10,\n                num_workers=4,\n                quantization_bits=8,\n                quantize_weights=True,\n                quantize_activations=True):\n    import math, copy, time\n    import torch\n    import torch.nn.functional as F\n    from torch import nn, optim\n    from torch.utils.data import TensorDataset, DataLoader\n    try:\n        from torch.ao.quantization import quantize_dynamic\n    except Exception:\n        from torch.quantization import quantize_dynamic\n\n    # Robust device handling\n    device = torch.device(device)\n    if device.type != 'cuda':\n        raise RuntimeError('This function requires a CUDA device. Pass device=\"cuda\" or a specific CUDA device string.')\n\n    torch.backends.cudnn.benchmark = True\n\n    # Ensure inputs are CPU tensors for DataLoader pin_memory\n    # This prevents: RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned\n    def _to_cpu_contig(t, dtype=None):\n        if dtype is not None:\n            t = t.to(dtype)\n        if t.device.type != 'cpu':\n            t = t.to('cpu', non_blocking=False)\n        return t.contiguous()\n\n    X_train = _to_cpu_contig(X_train, torch.float32)\n    X_val = _to_cpu_contig(X_val, torch.float32)\n    if y_train.dtype != torch.long:\n        y_train = y_train.to(torch.long)\n    if y_val.dtype != torch.long:\n        y_val = y_val.to(torch.long)\n    y_train = _to_cpu_contig(y_train)\n    y_val = _to_cpu_contig(y_val)\n\n    n_features = X_train.shape[1]\n    assert n_features == 561, f'Expected 561 features, got {n_features}'\n\n    # Ensure Transformer head divisibility: d_token = n_heads * d_head\n    d_token = n_heads * d_head\n\n    # Dataset/DataLoader with spawn context for CUDA safety\n    mp_ctx = torch.multiprocessing.get_context('spawn')\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    pin_mem_device = str(device)  # e.g., 'cuda:0'\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        pin_memory_device=pin_mem_device,\n        drop_last=False,\n        multiprocessing_context=mp_ctx,\n        persistent_workers=(num_workers > 0)\n    )\n\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n        pin_memory_device=pin_mem_device,\n        drop_last=False,\n        multiprocessing_context=mp_ctx,\n        persistent_workers=(num_workers > 0)\n    )\n\n    class FeatureDropout(nn.Module):\n        def __init__(self, p: float):\n            super().__init__()\n            self.p = float(p)\n        def forward(self, x):\n            # x: [B, L, D]\n            if not self.training or self.p <= 0.0:\n                return x\n            B, L, D = x.shape\n            keep = (torch.rand(B, L, 1, device=x.device, dtype=x.dtype) > self.p).float()\n            return x * keep\n\n    class FTTransformer(nn.Module):\n        def __init__(self, n_features, num_classes, d_token, n_heads, n_layers, d_ff_multiplier, dropout, token_dropout):\n            super().__init__()\n            self.n_features = n_features\n            self.d_token = d_token\n            self.token_dropout = FeatureDropout(token_dropout)\n\n            # Per-feature tokenization: tokens[b, i, :] = x[b, i] * W[i, :] + B[i, :]\n            self.W = nn.Parameter(torch.empty(n_features, d_token))\n            self.B = nn.Parameter(torch.empty(n_features, d_token))\n            nn.init.xavier_uniform_(self.W)\n            nn.init.zeros_(self.B)\n\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n            nn.init.normal_(self.cls_token, std=0.02)\n\n            d_ff = d_token * int(d_ff_multiplier)\n            enc_layer = nn.TransformerEncoderLayer(\n                d_model=d_token,\n                nhead=n_heads,\n                dim_feedforward=d_ff,\n                dropout=dropout,\n                activation='gelu',\n                batch_first=True,\n                norm_first=True\n            )\n            self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n            self.head = nn.Sequential(\n                nn.LayerNorm(d_token),\n                nn.Linear(d_token, num_classes)\n            )\n\n        def forward(self, x):\n            # x: [B, F]\n            B, F = x.shape\n            # Per-feature tokenization\n            tokens = x.unsqueeze(-1) * self.W.unsqueeze(0) + self.B.unsqueeze(0)  # [B, F, D]\n            # Prepend CLS\n            cls = self.cls_token.expand(B, 1, self.d_token)\n            seq = torch.cat([cls, tokens], dim=1)  # [B, 1+F, D]\n            # Feature/token dropout (do not drop CLS): mask only feature tokens\n            if self.training and self.token_dropout.p > 0:\n                cls_part = seq[:, :1]\n                feat_part = self.token_dropout(seq[:, 1:])\n                seq = torch.cat([cls_part, feat_part], dim=1)\n            # Transformer\n            h = self.encoder(seq)  # [B, 1+F, D]\n            cls_h = h[:, 0]\n            logits = self.head(cls_h)\n            return logits\n\n    def count_params(model: nn.Module):\n        return sum(p.numel() for p in model.parameters())\n\n    def estimate_state_dict_size_bytes(model: nn.Module):\n        total = 0\n        for _, v in model.state_dict().items():\n            if isinstance(v, torch.Tensor):\n                total += v.numel() * v.element_size()\n            elif isinstance(v, (bytes, bytearray)):\n                total += len(v)\n            else:\n                # Skip non-tensor entries (e.g., torch.dtype) in some quantized state_dicts\n                continue\n        return total\n\n    def cross_entropy_with_probs(logits, target_probs):\n        # logits: [B, C], target_probs: [B, C]\n        logp = F.log_softmax(logits, dim=-1)\n        return -(target_probs * logp).sum(dim=-1).mean()\n\n    @torch.no_grad()\n    def evaluate(model, data_loader, device, num_classes):\n        model.eval()\n        total_loss = 0.0\n        total_correct = 0\n        total_count = 0\n        all_preds = []\n        all_targets = []\n        for xb, yb in data_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n            logits = model(xb)\n            loss = F.cross_entropy(logits, yb)\n            total_loss += loss.item() * yb.size(0)\n            preds = logits.argmax(dim=1)\n            total_correct += (preds == yb).sum().item()\n            total_count += yb.size(0)\n            all_preds.append(preds)\n            all_targets.append(yb)\n        val_loss = total_loss / max(1, total_count)\n        preds = torch.cat(all_preds)\n        targets = torch.cat(all_targets)\n        acc = (preds == targets).float().mean().item()\n        # Macro-F1 via confusion matrix\n        C = num_classes\n        comb = targets * C + preds\n        cm = torch.bincount(comb, minlength=C*C).reshape(C, C).to(torch.float32)\n        tp = torch.diag(cm)\n        fp = cm.sum(dim=0) - tp\n        fn = cm.sum(dim=1) - tp\n        eps = 1e-9\n        precision = tp / (tp + fp + eps)\n        recall = tp / (tp + fn + eps)\n        f1 = 2 * precision * recall / (precision + recall + eps)\n        macro_f1 = f1.mean().item()\n        return val_loss, acc, macro_f1\n\n    # Build model\n    model = FTTransformer(\n        n_features=n_features,\n        num_classes=num_classes,\n        d_token=d_token,\n        n_heads=n_heads,\n        n_layers=n_layers,\n        d_ff_multiplier=d_ff_multiplier,\n        dropout=dropout,\n        token_dropout=token_dropout\n    ).to(device)\n\n    # Optimizer & scheduler\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, epochs))\n\n    train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n    best_f1 = -1.0\n    best_state = None\n    epochs_no_improve = 0\n\n    start_time = time.time()\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss = 0.0\n        nsamples = 0\n\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n\n            optimizer.zero_grad(set_to_none=True)\n\n            # MixUp (probabilistic) if enabled\n            use_mixup = mixup_alpha is not None and mixup_alpha > 0.0\n            if use_mixup:\n                lam = torch.distributions.Beta(mixup_alpha, mixup_alpha).sample().item()\n                index = torch.randperm(xb.size(0), device=xb.device)\n                x_mixed = lam * xb + (1 - lam) * xb[index]\n                # Soft targets\n                y_onehot = F.one_hot(yb, num_classes=num_classes).float()\n                y_onehot_shuf = y_onehot[index]\n                y_soft = lam * y_onehot + (1 - lam) * y_onehot_shuf\n                # Label smoothing on soft targets\n                if label_smoothing and label_smoothing > 0.0:\n                    y_soft = y_soft * (1 - label_smoothing) + label_smoothing / num_classes\n                logits = model(x_mixed)\n                loss = cross_entropy_with_probs(logits, y_soft)\n            else:\n                logits = model(xb)\n                loss = F.cross_entropy(logits, yb, label_smoothing=label_smoothing if (label_smoothing is not None) else 0.0)\n\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n            optimizer.step()\n\n            bs = yb.size(0)\n            epoch_loss += loss.detach().item() * bs\n            nsamples += bs\n\n        scheduler.step()\n        train_loss = epoch_loss / max(1, nsamples)\n\n        # Validation\n        val_loss, val_acc, val_f1 = evaluate(model, val_loader, device, num_classes)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        val_f1s.append(val_f1)\n\n        print(f'Epoch {epoch:03d}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_acc={val_acc:.4f}  val_f1={val_f1:.4f}')\n\n        # Early stopping on macro-F1\n        if val_f1 > best_f1 + 1e-6:\n            best_f1 = val_f1\n            best_state = copy.deepcopy(model.state_dict())\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f'Early stopping at epoch {epoch} (no improvement in macro-F1 for {patience} epochs).')\n                break\n\n    total_time = time.time() - start_time\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # Move to CPU for quantization export\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n\n    # Post-training quantization\n    quantized_model = model_cpu\n    if quantize_weights:\n        if quantization_bits == 8:\n            # Dynamic quantization of Linear layers to int8 (activations are dynamically quantized inside these layers)\n            quantized_model = quantize_dynamic(\n                model_cpu,\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n        elif quantization_bits == 16:\n            quantized_model = copy.deepcopy(model_cpu).half()\n        elif quantization_bits == 32:\n            quantized_model = copy.deepcopy(model_cpu)\n    else:\n        # No weight quantization; optionally cast activations-precision via 16-bit if requested\n        if quantization_bits == 16:\n            quantized_model = copy.deepcopy(model_cpu).half()\n        else:\n            quantized_model = copy.deepcopy(model_cpu)\n\n    # Size check (must be <= 256KB)\n    size_bytes = estimate_state_dict_size_bytes(quantized_model)\n    print(f'Quantized model size: {size_bytes/1024:.2f} KB')\n    if size_bytes > 262144:\n        print('Warning: Quantized model exceeds 256KB. Consider reducing d_head, n_heads, n_layers or using 8-bit quantization.')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'val_f1': val_f1s,\n        'best_val_f1': best_f1,\n        'epochs_ran': len(train_losses),\n        'train_seconds': total_time,\n        'quantized_model_size_bytes': size_bytes\n    }\n\n    return quantized_model, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 128,
      "type": "Categorical",
      "categories": [
        64,
        128,
        256
      ]
    },
    "epochs": {
      "default": 30,
      "type": "Integer",
      "low": 10,
      "high": 200
    },
    "d_head": {
      "default": 16,
      "type": "Categorical",
      "categories": [
        8,
        12,
        16
      ]
    },
    "n_heads": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        2,
        4
      ]
    },
    "n_layers": {
      "default": 2,
      "type": "Categorical",
      "categories": [
        1,
        2
      ]
    },
    "d_ff_multiplier": {
      "default": 2,
      "type": "Categorical",
      "categories": [
        2,
        3,
        4
      ]
    },
    "dropout": {
      "default": 0.3,
      "type": "Real",
      "low": 0.2,
      "high": 0.5
    },
    "token_dropout": {
      "default": 0.2,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "mixup_alpha": {
      "default": 0.2,
      "type": "Real",
      "low": 0.0,
      "high": 0.4
    },
    "label_smoothing": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "patience": {
      "default": 10,
      "type": "Integer",
      "low": 5,
      "high": 20
    },
    "num_workers": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        4
      ]
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "confidence": 0.86,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      10299,
      561
    ],
    "dtype": "float32",
    "feature_count": 561,
    "sample_count": 10299,
    "is_sequence": false,
    "is_image": false,
    "is_tabular": true,
    "has_labels": true,
    "label_count": 6,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1759589613,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}