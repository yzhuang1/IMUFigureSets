{
  "model_name": "TinyECG1DCNN",
  "training_code": "import math\nimport time\nfrom typing import Dict, Any, Tuple\n\ndef train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    \"\"\"\n    Train a lightweight 1D CNN for 5-class ECG beat classification.\n\n    Inputs (numpy arrays or PyTorch tensors):\n      - X_train: shape (N, L, C) or (N, C, L) or (N, L) where L≈1000 and C≈2 (MIT-BIH typical)\n      - y_train: shape (N,), int labels in [0..4]\n      - X_val, y_val: same conventions as train\n      - device: torch.device or string ('cpu' or 'cuda')\n\n    Returns: model, metrics_dict\n    Note: When converting tensors to numpy arrays, this code always uses .cpu().numpy() as required.\n    \"\"\"\n    import numpy as np\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\n    # -------------------- Defaults and Hyperparameters --------------------\n    hp = {\n        'lr': 1e-3,\n        'epochs': 20,\n        'batch_size': 64,\n        'hidden_size': 64,           # base channels; internally clamped to keep <256K params\n        'dropout': 0.2,\n        'weight_decay': 1e-4,\n        'label_smoothing': 0.05,\n        'early_stop_patience': 8,\n        'lr_patience': 3,\n        'lr_gamma': 0.5,\n        'balance_classes': True,\n        'grad_clip': 1.0,\n        'seed': 42,\n    }\n    hp.update(hyperparams or {})\n\n    # ---------------------------- Seeding ----------------------------\n    def set_seed(seed: int):\n        import random\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    set_seed(int(hp.get('seed', 42)))\n\n    # ------------------------- Device handling -------------------------\n    device = torch.device(device) if not isinstance(device, torch.device) else device\n\n    # ------------------------- Data preparation -------------------------\n    def to_tensor(x):\n        # Accept numpy arrays or torch tensors\n        if isinstance(x, np.ndarray):\n            t = torch.from_numpy(x)\n        elif isinstance(x, torch.Tensor):\n            t = x\n        else:\n            raise TypeError('X must be numpy array or torch.Tensor')\n        return t\n\n    def to_labels(y):\n        if isinstance(y, np.ndarray):\n            t = torch.from_numpy(y)\n        elif isinstance(y, torch.Tensor):\n            t = y\n        else:\n            raise TypeError('y must be numpy array or torch.Tensor')\n        return t.long()\n\n    X_train_t = to_tensor(X_train).float()\n    y_train_t = to_labels(y_train)\n    X_val_t = to_tensor(X_val).float()\n    y_val_t = to_labels(y_val)\n\n    # Normalize shape to (N, C, L) for Conv1d\n    def normalize_X_shape(X: torch.Tensor) -> torch.Tensor:\n        # Allowed: (N, L), (N, L, C), (N, C, L)\n        if X.ndim == 2:\n            # (N, L) -> (N, 1, L)\n            X = X.unsqueeze(1)\n        elif X.ndim == 3:\n            N, A, B = X.shape\n            # Heuristic: treat as (N, C, L) if A <= 16 (channels); else (N, L, C)\n            if A <= 16 and B >= A:\n                # likely (N, C, L)\n                pass\n            else:\n                # assume (N, L, C) -> (N, C, L)\n                X = X.permute(0, 2, 1).contiguous()\n        else:\n            raise ValueError('X must have 2 or 3 dimensions')\n        return X\n\n    X_train_t = normalize_X_shape(X_train_t)\n    X_val_t = normalize_X_shape(X_val_t)\n\n    num_classes = int(max(int(y_train_t.max().item()) + 1, 5))  # expect 5 classes\n    in_channels = int(X_train_t.shape[1])\n\n    # ------------------------- Dataset and Loader -------------------------\n    class TensorECGDataset(Dataset):\n        def __init__(self, X, y):\n            self.X = X\n            self.y = y\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            return self.X[idx], self.y[idx]\n\n    train_ds = TensorECGDataset(X_train_t, y_train_t)\n    val_ds = TensorECGDataset(X_val_t, y_val_t)\n\n    # Class weights for imbalance (MIT-BIH is highly imbalanced)\n    def compute_class_weights(y: torch.Tensor, num_classes: int) -> torch.Tensor:\n        counts = torch.bincount(y, minlength=num_classes).float()\n        counts = torch.clamp(counts, min=1.0)\n        weights = counts.sum() / (counts * num_classes)\n        return weights\n\n    class_weights = compute_class_weights(y_train_t, num_classes)\n\n    # Optional balanced sampling\n    if bool(hp.get('balance_classes', True)):\n        sample_weights = class_weights[y_train_t]\n        sampler = WeightedRandomSampler(weights=sample_weights.double(), num_samples=len(sample_weights), replacement=True)\n        shuffle = False\n    else:\n        sampler = None\n        shuffle = True\n\n    pin_memory = device.type == 'cuda'\n    train_loader = DataLoader(train_ds, batch_size=int(hp['batch_size']), sampler=sampler, shuffle=shuffle, drop_last=False, pin_memory=pin_memory)\n    val_loader = DataLoader(val_ds, batch_size=int(hp['batch_size']), shuffle=False, drop_last=False, pin_memory=pin_memory)\n\n    # ------------------------- Model Definition -------------------------\n    # Lightweight separable 1D CNN with global average pooling\n    class SeparableConv1d(nn.Module):\n        def __init__(self, in_ch, out_ch, kernel_size, stride=1, padding=None, dropout=0.0):\n            super().__init__()\n            if padding is None:\n                padding = kernel_size // 2\n            self.depthwise = nn.Conv1d(in_ch, in_ch, kernel_size, stride=stride, padding=padding, groups=in_ch, bias=False)\n            self.pointwise = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.ReLU(inplace=True)\n            self.do = nn.Dropout(p=dropout)\n        def forward(self, x):\n            x = self.depthwise(x)\n            x = self.pointwise(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.do(x)\n            return x\n\n    class TinyECG1DCNN(nn.Module):\n        def __init__(self, in_channels: int, num_classes: int, base: int = 64, dropout: float = 0.2):\n            super().__init__()\n            # Clamp base channels to keep params < 256k\n            base = int(min(max(32, base), 128))\n            self.base = base\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_channels, base, kernel_size=7, padding=3, bias=False),\n                nn.BatchNorm1d(base),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=dropout)\n            )\n            self.block1 = SeparableConv1d(base, base, kernel_size=5, dropout=dropout)\n            self.block2 = SeparableConv1d(base, base * 2, kernel_size=5, dropout=dropout)\n            self.block3 = SeparableConv1d(base * 2, base * 2, kernel_size=3, dropout=dropout)\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.head = nn.Linear(base * 2, num_classes)\n        def forward(self, x):\n            x = self.stem(x)\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.block3(x)\n            x = self.pool(x).squeeze(-1)\n            x = self.head(x)\n            return x\n\n    effective_hidden = int(min(max(32, int(hp['hidden_size'])), 128))\n    model = TinyECG1DCNN(in_channels=in_channels, num_classes=num_classes, base=effective_hidden, dropout=float(hp['dropout']))\n    model = model.to(device)\n\n    # ------------------------- Optimizer, Scheduler, Loss -------------------------\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(hp['lr']), weight_decay=float(hp['weight_decay']))\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=int(hp['lr_patience']), factor=float(hp['lr_gamma']), verbose=False)\n\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device), label_smoothing=float(hp['label_smoothing']))\n\n    # ------------------------- Metrics helpers -------------------------\n    @torch.no_grad()\n    def confusion_matrix(preds: torch.Tensor, targets: torch.Tensor, num_classes: int) -> torch.Tensor:\n        # preds, targets are 1D tensors of same length\n        k = num_classes\n        idx = targets.to(torch.int64) * k + preds.to(torch.int64)\n        cm = torch.bincount(idx, minlength=k * k).reshape(k, k)\n        return cm\n\n    @torch.no_grad()\n    def compute_metrics(all_preds: torch.Tensor, all_targets: torch.Tensor, num_classes: int) -> Dict[str, Any]:\n        cm = confusion_matrix(all_preds, all_targets, num_classes).to(torch.float32)\n        tp = cm.diag()\n        fp = cm.sum(dim=0) - tp\n        fn = cm.sum(dim=1) - tp\n        precision = tp / (tp + fp + 1e-9)\n        recall = tp / (tp + fn + 1e-9)\n        f1 = 2 * precision * recall / (precision + recall + 1e-9)\n        macro_f1 = f1.mean().item()\n        acc = (tp.sum() / (cm.sum() + 1e-9)).item()\n        return {\n            'accuracy': float(acc),\n            'macro_f1': float(macro_f1),\n            'per_class_f1': f1.cpu().numpy().tolist(),  # ensure .cpu().numpy()\n            'confusion_matrix': cm.cpu().numpy().astype(int).tolist()  # ensure .cpu().numpy()\n        }\n\n    # ------------------------- Training / Evaluation loops -------------------------\n    def train_one_epoch():\n        model.train()\n        total_loss = 0.0\n        n_batches = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if hp.get('grad_clip', None):\n                torch.nn.utils.clip_grad_norm_(model.parameters(), float(hp['grad_clip']))\n            optimizer.step()\n            total_loss += loss.item()\n            n_batches += 1\n        return total_loss / max(1, n_batches)\n\n    @torch.no_grad()\n    def evaluate():\n        model.eval()\n        total_loss = 0.0\n        n_batches = 0\n        all_preds = []\n        all_targets = []\n        for xb, yb in val_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            total_loss += loss.item()\n            n_batches += 1\n            preds = torch.argmax(logits, dim=1)\n            all_preds.append(preds.detach().cpu())\n            all_targets.append(yb.detach().cpu())\n        all_preds = torch.cat(all_preds, dim=0)\n        all_targets = torch.cat(all_targets, dim=0)\n        metrics = compute_metrics(all_preds, all_targets, num_classes)\n        return total_loss / max(1, n_batches), metrics\n\n    # ------------------------- Training loop with ES -------------------------\n    history = []\n    best_state = None\n    best_metric = -float('inf')\n    best_epoch = -1\n    no_improve = 0\n\n    t0 = time.time()\n    for epoch in range(int(hp['epochs'])):\n        train_loss = train_one_epoch()\n        val_loss, val_metrics = evaluate()\n        scheduler.step(val_loss)\n\n        record = {\n            'epoch': epoch + 1,\n            'train_loss': float(train_loss),\n            'val_loss': float(val_loss),\n            'val_accuracy': float(val_metrics['accuracy']),\n            'val_macro_f1': float(val_metrics['macro_f1']),\n            'lr': float(optimizer.param_groups[0]['lr'])\n        }\n        history.append(record)\n\n        # Early stopping on macro-F1 (robust to class imbalance)\n        score = val_metrics['macro_f1']\n        if score > best_metric:\n            best_metric = score\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}  # store on CPU\n            best_epoch = epoch + 1\n            no_improve = 0\n        else:\n            no_improve += 1\n\n        if no_improve >= int(hp['early_stop_patience']):\n            break\n\n    # Load best weights\n    if best_state is not None:\n        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n\n    # Final evaluation on validation set\n    final_val_loss, final_val_metrics = evaluate()\n    elapsed = time.time() - t0\n\n    # Count parameters and ensure budget\n    total_params = sum(p.numel() for p in model.parameters())\n\n    metrics = {\n        'best_epoch': int(best_epoch),\n        'total_params': int(total_params),\n        'history': history,\n        'final_val_loss': float(final_val_loss),\n        'final_val_accuracy': float(final_val_metrics['accuracy']),\n        'final_val_macro_f1': float(final_val_metrics['macro_f1']),\n        'final_per_class_f1': final_val_metrics['per_class_f1'],\n        'final_confusion_matrix': final_val_metrics['confusion_matrix'],\n        'class_weights': class_weights.cpu().numpy().tolist(),  # ensure .cpu().numpy()\n        'elapsed_sec': float(elapsed)\n    }\n\n    return model, metrics\n",
  "hyperparameters": {
    "lr": 0.001,
    "epochs": 20,
    "batch_size": 64,
    "hidden_size": 64,
    "dropout": 0.2
  },
  "reasoning": "MIT-BIH Arrhythmia is highly imbalanced across beat types; literature shows macro-F1 and class-weighted losses outperform accuracy-only optimization. We therefore use class weighting and optionally a WeightedRandomSampler. 1D CNNs with global average pooling are effective for beat-level ECG (compact, translation-tolerant). The proposed separable 1D CNN keeps parameters well under 256K while capturing temporal morphology over ~1000 samples and 2 channels. Label smoothing improves calibration and robustness.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "batch_size",
    "epochs",
    "hidden_size",
    "dropout"
  ],
  "bo_search_space": {
    "lr": {
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "type": "Categorical",
      "categories": [
        8,
        16,
        32,
        64,
        128
      ]
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "hidden_size": {
      "type": "Integer",
      "low": 32,
      "high": 512
    },
    "dropout": {
      "type": "Real",
      "low": 0.0,
      "high": 0.7
    }
  },
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757966037,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}