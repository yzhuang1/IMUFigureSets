{
  "model_name": "Tiny-ECG-Transformer-1D",
  "training_code": "import math\nimport random\nfrom typing import Dict, Any\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# -------------------------\n# Utility modules\n# -------------------------\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob: float = 0.0):\n        super().__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        if self.drop_prob == 0.0 or not self.training:\n            return x\n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()\n        return x.div(keep_prob) * random_tensor\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.0):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.drop = nn.Dropout(dropout)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass SelfAttention(nn.Module):\n    def __init__(self, dim, num_heads=4, attn_drop=0.0, proj_drop=0.0):\n        super().__init__()\n        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n    def forward(self, x):  # x: (B, N, C)\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, v = qkv[0], qkv[2]\n        k = qkv[1]\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=2.0, drop=0.0, attn_drop=0.0, drop_path=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = SelfAttention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = nn.LayerNorm(dim)\n        hidden_dim = int(dim * mlp_ratio)\n        self.mlp = MLP(dim, hidden_dim, dropout=drop)\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\nclass PatchEmbed1D(nn.Module):\n    def __init__(self, in_chans=2, embed_dim=96, patch_size=25, seq_len=1000):\n        super().__init__()\n        self.patch_size = patch_size\n        assert seq_len % patch_size == 0, \"seq_len must be divisible by patch_size\"\n        self.num_patches = seq_len // patch_size\n        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=True)\n    def forward(self, x):  # x: (B, C, L)\n        x = self.proj(x)  # (B, E, N)\n        x = x.transpose(1, 2)  # (B, N, E)\n        return x\n\nclass TinyECGTransformer(nn.Module):\n    def __init__(self, seq_len=1000, in_chans=2, num_classes=5, embed_dim=96, depth=3, num_heads=4, mlp_ratio=2.0, drop=0.1, attn_drop=0.0, drop_path=0.05, patch_size=25):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.patch_embed = PatchEmbed1D(in_chans, embed_dim, patch_size, seq_len)\n        num_patches = self.patch_embed.num_patches\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop)\n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, drop, attn_drop, drop_path)\n            for _ in range(depth)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.xavier_uniform_(self.head.weight)\n        nn.init.zeros_(self.head.bias)\n    def forward(self, x):  # x: (B, C, L)\n        x = self.patch_embed(x)  # (B, N, E)\n        B, N, E = x.shape\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, E)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        cls = x[:, 0]\n        return self.head(cls)\n\n# -------------------------\n# Augmentations for ECG\n# -------------------------\ndef nearest_divisor_in_range(n, low, high):\n    candidates = []\n    for p in range(low, high + 1):\n        if n % p == 0:\n            candidates.append(p)\n    if not candidates:\n        # fallback: choose closest p and pad/trim later (but we avoid here by enforcing default divisors)\n        return low\n    # choose mid-range candidate near (low+high)/2 or nearest to 25\n    target = 25\n    candidates.sort(key=lambda x: (abs(x - target), x))\n    return candidates[0]\n\ndef random_time_warp(x, max_ratio=0.05):\n    # x: (C, L), resample length L by a global scaling factor s in [1-r,1+r]\n    if max_ratio <= 0:\n        return x\n    C, L = x.shape\n    s = 1.0 + (2 * torch.rand(1).item() - 1.0) * max_ratio\n    new_L = max(8, int(round(L * s)))\n    x_ = x.unsqueeze(0)  # (1, C, L)\n    x_res = F.interpolate(x_, size=new_L, mode='linear', align_corners=False)\n    x_res = F.interpolate(x_res, size=L, mode='linear', align_corners=False)\n    return x_res.squeeze(0)\n\ndef add_baseline_wander(x, amp=0.05, freq=0.2):\n    if amp <= 0 or freq <= 0:\n        return x\n    C, L = x.shape\n    t = torch.linspace(0, 1, steps=L, device=x.device, dtype=x.dtype)\n    phase = torch.rand(1, device=x.device, dtype=x.dtype) * 2 * math.pi\n    baseline = amp * torch.sin(2 * math.pi * freq * t + phase)\n    return x + baseline.unsqueeze(0).expand(C, -1)\n\ndef add_noise(x, std=0.01):\n    if std <= 0:\n        return x\n    return x + torch.randn_like(x) * std\n\ndef random_lead_dropout(x, p=0.05):\n    if p <= 0:\n        return x\n    if random.random() < p:\n        C, _ = x.shape\n        ch = random.randrange(C)\n        x[ch, :] = 0.0\n    return x\n\nclass TensorECGDataset(Dataset):\n    def __init__(self, X: torch.Tensor, y: torch.Tensor, train: bool, aug_cfg: Dict[str, Any]):\n        super().__init__()\n        self.X = X\n        self.y = y\n        self.train = train\n        self.aug = aug_cfg\n    def __len__(self):\n        return self.X.shape[0]\n    def __getitem__(self, idx):\n        x = self.X[idx]\n        y = self.y[idx]\n        # Expect x shape (L, C); convert to (C, L)\n        if x.ndim == 2 and x.shape[0] == 1000 and x.shape[1] == 2:\n            x = x.permute(1, 0).contiguous()\n        elif x.ndim == 2 and x.shape[0] == 2:\n            pass\n        else:\n            # try to coerce\n            x = x.view(1000, 2).permute(1, 0).contiguous()\n        x = x.float()\n        if self.train:\n            # Apply augmentations in random order\n            if self.aug.get('aug_time_warp_ratio', 0.05) > 0:\n                if random.random() < 0.5:\n                    x = random_time_warp(x, self.aug.get('aug_time_warp_ratio', 0.05))\n            x = add_baseline_wander(x, self.aug.get('aug_baseline_amp', 0.05), self.aug.get('aug_baseline_freq', 0.2))\n            x = add_noise(x, self.aug.get('aug_noise_std', 0.01))\n            x = random_lead_dropout(x, self.aug.get('aug_lead_dropout_prob', 0.05))\n        return x, int(y)\n\n# -------------------------\n# Losses and metrics\n# -------------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, gamma: float = 2.0, label_smoothing: float = 0.0):\n        super().__init__()\n        self.weight = weight\n        self.gamma = gamma\n        self.label_smoothing = label_smoothing\n    def forward(self, logits, target):\n        num_classes = logits.size(-1)\n        if self.label_smoothing > 0:\n            # one-hot with smoothing\n            with torch.no_grad():\n                true_dist = torch.zeros_like(logits)\n                true_dist.fill_(self.label_smoothing / (num_classes - 1))\n                true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.label_smoothing)\n        log_probs = F.log_softmax(logits, dim=-1)\n        probs = log_probs.exp()\n        if self.weight is not None:\n            w = self.weight[target]\n        else:\n            w = 1.0\n        if self.label_smoothing > 0:\n            ce = -(true_dist * log_probs).sum(dim=-1)\n            pt = (true_dist * probs).sum(dim=-1).clamp(min=1e-6)\n        else:\n            ce = F.nll_loss(log_probs, target, reduction='none', weight=self.weight)\n            pt = probs.gather(1, target.unsqueeze(1)).squeeze(1).clamp(min=1e-6)\n        loss = (w * ((1 - pt) ** self.gamma) * ce)\n        return loss.mean()\n\ndef compute_class_weights(y: torch.Tensor, num_classes: int):\n    counts = torch.bincount(y, minlength=num_classes).float()\n    inv = 1.0 / (counts + 1e-6)\n    weights = inv / inv.sum() * num_classes\n    return weights\n\n@torch.no_grad()\ndef evaluate(model, dl, device):\n    model.eval()\n    total = 0\n    correct = 0\n    all_preds = []\n    all_targets = []\n    total_loss = 0.0\n    criterion = nn.CrossEntropyLoss(reduction='mean')\n    for xb, yb in dl:\n        xb = xb.to(device)\n        yb = yb.to(device)\n        logits = model(xb)\n        loss = criterion(logits, yb)\n        total_loss += loss.item() * yb.size(0)\n        preds = logits.argmax(dim=-1)\n        correct += (preds == yb).sum().item()\n        total += yb.size(0)\n        all_preds.append(preds.cpu())\n        all_targets.append(yb.cpu())\n    if total == 0:\n        return {\"loss\": 0.0, \"accuracy\": 0.0, \"f1_macro\": 0.0, \"confusion_matrix\": torch.zeros(5,5, dtype=torch.int64).tolist()}\n    y_true = torch.cat(all_targets)\n    y_pred = torch.cat(all_preds)\n    num_classes = int(y_true.max().item() + 1) if y_true.numel() > 0 else 5\n    # confusion matrix\n    cm = torch.zeros(num_classes, num_classes, dtype=torch.int64)\n    for t, p in zip(y_true, y_pred):\n        cm[t, p] += 1\n    # F1 per class\n    f1s = []\n    for c in range(num_classes):\n        tp = cm[c, c].item()\n        fp = cm[:, c].sum().item() - tp\n        fn = cm[c, :].sum().item() - tp\n        prec = tp / (tp + fp + 1e-9)\n        rec = tp / (tp + fn + 1e-9)\n        f1 = 2 * prec * rec / (prec + rec + 1e-9)\n        f1s.append(f1)\n    f1_macro = float(sum(f1s) / len(f1s)) if f1s else 0.0\n    return {\n        \"loss\": total_loss / total,\n        \"accuracy\": correct / total,\n        \"f1_macro\": f1_macro,\n        \"confusion_matrix\": cm.tolist(),\n    }\n\n# -------------------------\n# Quantization helpers\n# -------------------------\ndef count_parameters(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters())\n\ndef quantize_model_ptq(model: nn.Module, quantization_bits: int = 8, quantize_weights: bool = True, quantize_activations: bool = True) -> nn.Module:\n    # For Transformer-like models, dynamic quantization of Linear layers is robust and widely used.\n    # We quantize on CPU using fbgemm backend for int8.\n    model = model.cpu().eval()\n    if not quantize_weights and quantization_bits == 32:\n        return model\n    if quantization_bits == 32:\n        return model  # no quantization\n    if quantization_bits == 16:\n        # Use dynamic quantization to float16 for Linear layers\n        dtype = torch.float16\n        qmodel = torch.ao.quantization.quantize_dynamic(model, {nn.Linear}, dtype=dtype, inplace=False)\n        return qmodel\n    if quantization_bits == 8:\n        torch.backends.quantized.engine = 'fbgemm'\n        dtype = torch.qint8\n        # Dynamic quantization of Linear layers quantizes both weights and (dynamically) activations during matmul\n        qmodel = torch.ao.quantization.quantize_dynamic(model, {nn.Linear}, dtype=dtype, inplace=False)\n        return qmodel\n    # Fallback\n    return model\n\n# -------------------------\n# Main training function\n# -------------------------\ndef train_model(X_train: torch.Tensor,\n                y_train: torch.Tensor,\n                X_val: torch.Tensor,\n                y_val: torch.Tensor,\n                device: torch.device,\n                **hparams) -> Dict[str, Any]:\n    \"\"\"\n    Train a Tiny-ECG-Transformer for 5-class classification on ECG windows of shape (1000, 2).\n\n    Inputs:\n      - X_* tensors shaped (N, 1000, 2) or (N, 2, 1000), dtype float/half\n      - y_* tensors shaped (N,), dtype long ints in [0..4]\n      - device: torch.device for training\n      - hparams: hyperparameters and quantization parameters\n\n    Returns: {\"model\": quantized_model, \"metrics\": {...}}\n    \"\"\"\n    # Defaults\n    defaults = dict(\n        seed=42,\n        lr=1e-3,\n        weight_decay=1e-4,\n        batch_size=256,\n        epochs=15,\n        embed_dim=96,\n        num_heads=4,\n        depth=3,\n        mlp_ratio=2.0,\n        dropout=0.1,\n        attn_dropout=0.0,\n        drop_path=0.05,\n        patch_size=25,\n        label_smoothing=0.0,\n        use_focal=True,\n        focal_gamma=2.0,\n        max_grad_norm=1.0,\n        # Augmentations\n        aug_noise_std=0.01,\n        aug_time_warp_ratio=0.05,\n        aug_baseline_amp=0.05,\n        aug_baseline_freq=0.2,\n        aug_lead_dropout_prob=0.05,\n        # Quantization\n        quantization_bits=8,\n        quantize_weights=True,\n        quantize_activations=True,\n    )\n    cfg = {**defaults, **hparams}\n\n    # Set seeds for reproducibility\n    torch.manual_seed(int(cfg['seed']))\n    random.seed(int(cfg['seed']))\n\n    # Ensure patch_size divides 1000 (sequence length)\n    seq_len = 1000\n    ps = int(cfg['patch_size'])\n    ps = nearest_divisor_in_range(seq_len, 20, 32) if (seq_len % ps != 0 or ps < 20 or ps > 32) else ps\n    cfg['patch_size'] = ps\n\n    # Ensure num_heads divides embed_dim\n    embed_dim = int(cfg['embed_dim'])\n    num_heads = int(cfg['num_heads'])\n    if embed_dim % num_heads != 0:\n        # Adjust num_heads down to nearest divisor >= 1\n        divisors = [h for h in range(min(num_heads, embed_dim), 0, -1) if embed_dim % h == 0]\n        num_heads = divisors[0] if len(divisors) > 0 else 1\n        cfg['num_heads'] = num_heads\n\n    # Datasets and loaders\n    aug_cfg = dict(\n        aug_noise_std=float(cfg['aug_noise_std']),\n        aug_time_warp_ratio=float(cfg['aug_time_warp_ratio']),\n        aug_baseline_amp=float(cfg['aug_baseline_amp']),\n        aug_baseline_freq=float(cfg['aug_baseline_freq']),\n        aug_lead_dropout_prob=float(cfg['aug_lead_dropout_prob']),\n    )\n\n    train_ds = TensorECGDataset(X_train, y_train.long(), train=True, aug_cfg=aug_cfg)\n    val_ds = TensorECGDataset(X_val, y_val.long(), train=False, aug_cfg=aug_cfg)\n    batch_size = int(cfg['batch_size'])\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0)\n\n    # Model\n    model = TinyECGTransformer(\n        seq_len=seq_len,\n        in_chans=2,\n        num_classes=5,\n        embed_dim=embed_dim,\n        depth=int(cfg['depth']),\n        num_heads=num_heads,\n        mlp_ratio=float(cfg['mlp_ratio']),\n        drop=float(cfg['dropout']),\n        attn_drop=float(cfg['attn_dropout']),\n        drop_path=float(cfg['drop_path']),\n        patch_size=ps,\n    ).to(device)\n\n    # Parameter count constraint\n    param_count = count_parameters(model)\n    if param_count > 256_000:\n        raise ValueError(f\"Model has {param_count} parameters which exceeds the 256K limit. Reduce embed_dim/depth/heads.\")\n\n    # Loss with class weights\n    class_weights = compute_class_weights(y_train.long(), num_classes=5).to(device)\n    use_focal = bool(cfg['use_focal'])\n    label_smoothing = float(cfg['label_smoothing']) if not use_focal else float(cfg['label_smoothing'])\n    if use_focal:\n        criterion = FocalLoss(weight=class_weights, gamma=float(cfg['focal_gamma']), label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(cfg['lr']), weight_decay=float(cfg['weight_decay']))\n\n    # Training loop\n    best_state = None\n    best_val_acc = -1.0\n    epochs = int(cfg['epochs'])\n    max_grad_norm = float(cfg['max_grad_norm'])\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        n_samples = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device)\n            yb = yb.to(device)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if max_grad_norm and max_grad_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n            running_loss += loss.item() * yb.size(0)\n            n_samples += yb.size(0)\n        train_loss = running_loss / max(1, n_samples)\n        # Validation\n        val_metrics = evaluate(model, val_loader, device)\n        if val_metrics['accuracy'] > best_val_acc:\n            best_val_acc = val_metrics['accuracy']\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n        # Optional: print progress\n        # print(f\"Epoch {epoch+1}/{epochs} - train_loss: {train_loss:.4f}, val_acc: {val_metrics['accuracy']:.4f}\")\n\n    # Load best weights\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # Final FP32 eval before quantization\n    final_val_metrics_fp32 = evaluate(model, val_loader, device)\n\n    # Quantize\n    q_bits = int(cfg['quantization_bits'])\n    q_w = bool(cfg['quantize_weights'])\n    q_a = bool(cfg['quantize_activations'])\n    q_model = quantize_model_ptq(model, quantization_bits=q_bits, quantize_weights=q_w, quantize_activations=q_a)\n\n    # Evaluate quantized model on CPU\n    q_device = torch.device('cpu')\n    q_val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0)\n    q_metrics = evaluate(q_model, q_val_loader, q_device)\n\n    metrics = {\n        'param_count': int(param_count),\n        'train_val_final_fp32': final_val_metrics_fp32,\n        'val_quantized': q_metrics,\n        'config': cfg,\n    }\n\n    return {'model': q_model, 'metrics': metrics}\n",
  "bo_config": {
    "seed": {
      "default": 42,
      "type": "Integer",
      "low": 0,
      "high": 10000
    },
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.005,
      "prior": "log-uniform"
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 256,
      "type": "Categorical",
      "categories": [
        64,
        128,
        256,
        512
      ]
    },
    "epochs": {
      "default": 15,
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "embed_dim": {
      "default": 96,
      "type": "Integer",
      "low": 64,
      "high": 128
    },
    "num_heads": {
      "default": 4,
      "type": "Integer",
      "low": 2,
      "high": 8
    },
    "depth": {
      "default": 3,
      "type": "Integer",
      "low": 2,
      "high": 4
    },
    "mlp_ratio": {
      "default": 2.0,
      "type": "Real",
      "low": 1.5,
      "high": 3.0
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.3
    },
    "attn_dropout": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "drop_path": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.1
    },
    "patch_size": {
      "default": 25,
      "type": "Integer",
      "low": 20,
      "high": 32
    },
    "label_smoothing": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 0.1
    },
    "use_focal": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 1.0,
      "high": 3.0
    },
    "max_grad_norm": {
      "default": 1.0,
      "type": "Real",
      "low": 0.1,
      "high": 5.0
    },
    "aug_noise_std": {
      "default": 0.01,
      "type": "Real",
      "low": 0.0,
      "high": 0.05
    },
    "aug_time_warp_ratio": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "aug_baseline_amp": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "aug_baseline_freq": {
      "default": 0.2,
      "type": "Real",
      "low": 0.05,
      "high": 0.5
    },
    "aug_lead_dropout_prob": {
      "default": 0.05,
      "type": "Real",
      "low": 0.0,
      "high": 0.3
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    }
  },
  "confidence": 0.86,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758299765,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}