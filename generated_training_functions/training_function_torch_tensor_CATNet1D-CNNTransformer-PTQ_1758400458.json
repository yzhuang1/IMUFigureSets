{
  "model_name": "CATNet1D-CNNTransformer-PTQ",
  "training_code": "import math\nimport copy\nfrom typing import Dict, Any, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# -------------------------------\n# Depthwise-separable temporal CNN block\n# -------------------------------\nclass DWSeparableConv1DBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, kernel_size: int = 7, stride: int = 1, dilation: int = 1, dropout: float = 0.1):\n        super().__init__()\n        padding = ((kernel_size - 1) // 2) * dilation\n        self.conv_dw = nn.Conv1d(in_ch, in_ch, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_ch, dilation=dilation, bias=False)\n        self.bn_dw = nn.BatchNorm1d(in_ch)\n        self.act_dw = nn.ReLU(inplace=True)\n        self.conv_pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n        self.bn_pw = nn.BatchNorm1d(out_ch)\n        self.act_pw = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.use_res = (stride != 1) or (in_ch != out_ch)\n        if self.use_res:\n            self.res_conv = nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False)\n            self.res_bn = nn.BatchNorm1d(out_ch)\n        else:\n            self.res_conv = None\n            self.res_bn = None\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        res = x\n        out = self.conv_dw(x)\n        out = self.bn_dw(out)\n        out = self.act_dw(out)\n        out = self.conv_pw(out)\n        out = self.bn_pw(out)\n        out = self.act_pw(out)\n        out = self.dropout(out)\n        if self.use_res:\n            res = self.res_conv(res)\n            res = self.res_bn(res)\n        out = out + res\n        return F.relu(out, inplace=True)\n\n    def fuse_model(self):\n        try:\n            torch.ao.quantization.fuse_modules(self, [[\"conv_dw\", \"bn_dw\", \"act_dw\"], [\"conv_pw\", \"bn_pw\", \"act_pw\"]], inplace=True)\n            if self.use_res:\n                torch.ao.quantization.fuse_modules(self, [[\"res_conv\", \"res_bn\"]], inplace=True)\n        except Exception:\n            pass\n\n# -------------------------------\n# Sinusoidal positional encoding (no parameters)\n# -------------------------------\nclass SinePositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 5000):\n        super().__init__()\n        self.d_model = d_model\n        self.max_len = max_len\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (B, L, D)\n        B, L, D = x.shape\n        pe = torch.zeros(L, D, device=x.device)\n        position = torch.arange(0, L, dtype=torch.float32, device=x.device).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, D, 2, device=x.device).float() * (-math.log(10000.0) / D))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return x + pe.unsqueeze(0)\n\n# -------------------------------\n# CAT-Net style: 1D CNN stem + DW-Separable temporal CNNs + Transformer encoder\n# -------------------------------\nclass CATNet1D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int = 2,\n        num_classes: int = 5,\n        stem_channels: int = 16,\n        width_mult: float = 1.0,\n        dw_kernel_size: int = 7,\n        d_model: int = 64,\n        nhead: int = 4,\n        num_transformer_layers: int = 1,\n        dim_feedforward: int = 128,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n\n        # Stem\n        self.stem_conv = nn.Conv1d(in_channels, stem_channels, kernel_size=7, stride=2, padding=3, bias=False)\n        self.stem_bn = nn.BatchNorm1d(stem_channels)\n        self.stem_act = nn.ReLU(inplace=True)\n\n        # CNN blocks (downsample to reduce sequence length: 1000 -> 500 -> 250 -> 125)\n        c1 = max(8, int(32 * width_mult))\n        c2 = max(16, int(64 * width_mult))\n        c3 = d_model  # ensure last feature dim equals d_model for Transformer\n        self.block1 = DWSeparableConv1DBlock(stem_channels, c1, kernel_size=dw_kernel_size, stride=2, dilation=1, dropout=dropout)\n        self.block2 = DWSeparableConv1DBlock(c1, c2, kernel_size=dw_kernel_size, stride=2, dilation=2, dropout=dropout)\n        self.block3 = DWSeparableConv1DBlock(c2, c3, kernel_size=dw_kernel_size, stride=1, dilation=4, dropout=dropout)\n\n        # Transformer encoder (batch_first for (B, L, D))\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            activation=\"gelu\",\n            batch_first=True,\n            norm_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n        self.pos_enc = SinePositionalEncoding(d_model)\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, num_classes)\n        )\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Accept (B, L, C) or (B, C, L)\n        if x.dim() != 3:\n            raise ValueError(f\"Expected 3D tensor, got shape {tuple(x.shape)}\")\n        if x.shape[1] == self.in_channels and x.shape[2] != self.in_channels:\n            # (B, C, L)\n            x = x\n        elif x.shape[2] == self.in_channels:\n            # (B, L, C) -> (B, C, L)\n            x = x.permute(0, 2, 1).contiguous()\n        else:\n            # default assume (B, L, C)\n            x = x.permute(0, 2, 1).contiguous()\n\n        # CNN feature extractor\n        x = self.stem_conv(x)\n        x = self.stem_bn(x)\n        x = self.stem_act(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n\n        # Prepare for Transformer: (B, C, L) -> (B, L, C)\n        x = x.transpose(1, 2)\n        x = self.pos_enc(x)\n        x = self.transformer(x)\n        x = x.mean(dim=1)  # Global average over time\n        logits = self.classifier(x)\n        return logits\n\n    def fuse_model(self):\n        # Fuse CNN parts only (Transformer left in float)\n        try:\n            torch.ao.quantization.fuse_modules(self, [[\"stem_conv\", \"stem_bn\", \"stem_act\"]], inplace=True)\n        except Exception:\n            pass\n        if hasattr(self, \"block1\"):\n            self.block1.fuse_model()\n        if hasattr(self, \"block2\"):\n            self.block2.fuse_model()\n        if hasattr(self, \"block3\"):\n            self.block3.fuse_model()\n\n    def set_qconfig_for_cnn(self, qconfig):\n        # Assign qconfig to CNN parts only\n        self.qconfig = None\n        for name in [\"stem_conv\", \"stem_bn\", \"stem_act\", \"block1\", \"block2\", \"block3\"]:\n            if hasattr(self, name):\n                getattr(self, name).qconfig = qconfig\n        # Exclude Transformer & classifier from static quantization by setting None\n        if hasattr(self, \"transformer\"):\n            self.transformer.qconfig = None\n        if hasattr(self, \"classifier\"):\n            self.classifier.qconfig = None\n\n# -------------------------------\n# Losses\n# -------------------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha: torch.Tensor, gamma: float = 2.0, reduction: str = \"mean\"):\n        super().__init__()\n        self.register_buffer(\"alpha\", alpha)\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        logp = F.log_softmax(logits, dim=-1)\n        p = logp.exp()\n        targets_one_hot = F.one_hot(targets, num_classes=logits.size(-1)).float()\n        # Gather log-probabilities of targets\n        logp_t = (logp * targets_one_hot).sum(dim=-1)\n        p_t = (p * targets_one_hot).sum(dim=-1)\n        alpha_t = self.alpha[targets]\n        loss = -alpha_t * (1 - p_t) ** self.gamma * logp_t\n        if self.reduction == \"mean\":\n            return loss.mean()\n        elif self.reduction == \"sum\":\n            return loss.sum()\n        else:\n            return loss\n\n# -------------------------------\n# Utilities\n# -------------------------------\n\ndef _ensure_channel_first(x: torch.Tensor, in_channels: int = 2) -> torch.Tensor:\n    if x.dim() != 3:\n        raise ValueError(f\"Expected 3D tensor, got shape {tuple(x.shape)}\")\n    if x.shape[1] == in_channels:\n        return x  # (B, C, L)\n    elif x.shape[2] == in_channels:\n        return x.permute(0, 2, 1).contiguous()  # (B, L, C) -> (B, C, L)\n    else:\n        # Assume last dim is channels if ambiguous\n        return x.permute(0, 2, 1).contiguous()\n\n\ndef _accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:\n    preds = logits.argmax(dim=-1)\n    correct = (preds == targets).sum().item()\n    total = targets.numel()\n    return correct / max(1, total)\n\n# -------------------------------\n# Post-training quantization helpers\n# -------------------------------\n\ndef quantize_model_post_training(model: nn.Module, train_loader: DataLoader, bits: int, quantize_weights: bool, quantize_activations: bool, calibration_batches: int = 16, backend: str = \"fbgemm\") -> Tuple[nn.Module, Dict[str, Any]]:\n    info = {\"strategy\": \"none\", \"bits\": bits, \"weights\": quantize_weights, \"activations\": quantize_activations, \"backend\": backend}\n    if bits == 32 or (not quantize_weights and not quantize_activations):\n        return model, info\n\n    # Always quantize on CPU\n    model_cpu = copy.deepcopy(model).to(\"cpu\").eval()\n\n    # Prefer static PTQ for CNN when activations requested and 8-bit selected\n    if bits == 8 and quantize_activations and quantize_weights:\n        try:\n            torch.backends.quantized.engine = backend\n            qconfig = torch.ao.quantization.get_default_qconfig(backend)\n            if hasattr(model_cpu, \"fuse_model\"):\n                model_cpu.fuse_model()\n            if hasattr(model_cpu, \"set_qconfig_for_cnn\"):\n                model_cpu.set_qconfig_for_cnn(qconfig)\n            # Prepare only modules with qconfig set\n            torch.ao.quantization.prepare(model_cpu, inplace=True)\n            # Calibration pass\n            with torch.no_grad():\n                seen = 0\n                for xb, yb in train_loader:\n                    xb = xb.float()\n                    xb = _ensure_channel_first(xb, in_channels=getattr(model_cpu, \"in_channels\", 2))\n                    _ = model_cpu(xb)\n                    seen += 1\n                    if seen >= calibration_batches:\n                        break\n            torch.ao.quantization.convert(model_cpu, inplace=True)\n            info[\"strategy\"] = \"static-int8-cnn\"\n            return model_cpu, info\n        except Exception as e:\n            # Fallback to dynamic\n            info[\"static_error\"] = str(e)\n\n    # Dynamic quantization (Linear layers), supports 8-bit or fp16\n    qdtype = torch.qint8 if bits == 8 else torch.float16\n    try:\n        qmodel = torch.ao.quantization.quantize_dynamic(model_cpu, {nn.Linear}, dtype=qdtype)\n        info[\"strategy\"] = \"dynamic-linear\"\n        return qmodel, info\n    except Exception as e:\n        info[\"dynamic_error\"] = str(e)\n        # If quantization failed, return original float model on CPU\n        return model_cpu, info\n\n# -------------------------------\n# Main training entry point\n# -------------------------------\n\ndef train_model(\n    X_train: torch.Tensor,\n    y_train: torch.Tensor,\n    X_val: torch.Tensor,\n    y_val: torch.Tensor,\n    device: torch.device,\n    # Hyperparameters\n    lr: float = 1e-3,\n    batch_size: int = 128,\n    epochs: int = 20,\n    weight_decay: float = 1e-4,\n    dropout: float = 0.1,\n    stem_channels: int = 16,\n    dw_kernel_size: int = 7,\n    cnn_width_mult: float = 1.0,\n    d_model: int = 64,\n    nhead: int = 4,\n    num_transformer_layers: int = 1,\n    dim_feedforward: int = 128,\n    optimizer: str = \"adamw\",\n    scheduler: str = \"none\",\n    warmup_epochs: int = 0,\n    use_focal_loss: bool = True,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    grad_clip: float = 0.5,\n    # Quantization params\n    quantization_bits: int = 8,\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n    calibration_batches: int = 16,\n    quant_backend: str = \"fbgemm\",\n) -> Tuple[nn.Module, Dict[str, Any]]:\n    \"\"\"\n    Train a CAT-Net–style 1D CNN + Transformer model for 5-class classification and return a quantized model and training metrics.\n\n    Inputs are torch tensors. Shapes accepted for X: (N, L, 2) or (N, 2, L); labels as int64 (N,).\n\n    DataLoader uses pin_memory=False to avoid CUDA tensor pinning issues.\n    \"\"\"\n    torch.autograd.set_detect_anomaly(False)\n\n    # Ensure dtypes\n    X_train = X_train.float()\n    X_val = X_val.float()\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    num_classes = int(max(y_train.max().item(), y_val.max().item()) + 1)\n\n    # Datasets / Loaders (pin_memory=False as required)\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False)\n\n    # Build model\n    model = CATNet1D(\n        in_channels=2,\n        num_classes=num_classes,\n        stem_channels=stem_channels,\n        width_mult=cnn_width_mult,\n        dw_kernel_size=dw_kernel_size,\n        d_model=d_model,\n        nhead=nhead,\n        num_transformer_layers=num_transformer_layers,\n        dim_feedforward=dim_feedforward,\n        dropout=dropout,\n    ).to(device)\n\n    # Parameter count check (must be <= 256k)\n    param_count = sum(p.numel() for p in model.parameters())\n    if param_count > 256_000:\n        raise ValueError(f\"Model has {param_count} parameters, exceeds the 256K limit. Reduce d_model or channels.\")\n\n    # Class imbalance handling: compute alpha weights (inverse-frequency)\n    with torch.no_grad():\n        counts = torch.bincount(y_train, minlength=num_classes).float()\n        counts = torch.where(counts == 0, torch.ones_like(counts), counts)\n        inv_freq = 1.0 / counts\n        alpha = inv_freq / inv_freq.sum() * num_classes  # normalized\n    alpha = alpha.to(device)\n\n    if use_focal_loss:\n        criterion = FocalLoss(alpha=alpha, gamma=focal_gamma, reduction=\"mean\")\n    else:\n        # Cross entropy with class weights and optional label smoothing\n        criterion = nn.CrossEntropyLoss(weight=alpha, label_smoothing=float(label_smoothing))\n\n    # Optimizer\n    if optimizer.lower() == \"adamw\":\n        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optimizer.lower() == \"adam\":\n        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(\"Unsupported optimizer. Choose from ['adamw','adam']\")\n\n    # Scheduler\n    if scheduler.lower() == \"cosine\":\n        sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, epochs - warmup_epochs))\n    elif scheduler.lower() == \"onecycle\":\n        steps_per_epoch = max(1, len(train_loader))\n        sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n    else:\n        sched = None\n\n    train_losses, val_losses, val_accs = [], [], []\n    best_val_acc = 0.0\n    best_state = copy.deepcopy(model.state_dict())\n    best_epoch = 0\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        n_batches = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device)\n            yb = yb.to(device)\n            xb = _ensure_channel_first(xb, in_channels=2)\n\n            opt.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            opt.step()\n\n            running_loss += loss.item()\n            n_batches += 1\n            if scheduler.lower() == \"onecycle\" and sched is not None:\n                sched.step()\n\n        train_loss = running_loss / max(1, n_batches)\n        train_losses.append(train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        val_batches = 0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device)\n                yb = yb.to(device)\n                xb = _ensure_channel_first(xb, in_channels=2)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.item()\n                val_batches += 1\n                preds = logits.argmax(dim=-1)\n                correct += (preds == yb).sum().item()\n                total += yb.numel()\n        val_loss = val_running_loss / max(1, val_batches)\n        val_acc = (correct / max(1, total)) if total > 0 else 0.0\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        if scheduler and scheduler.lower() == \"cosine\" and sched is not None:\n            # Apply cosine step after each epoch (post-warmup)\n            if epoch >= warmup_epochs:\n                sched.step()\n\n        print(f\"Epoch {epoch+1}/{epochs} - train_loss: {train_loss:.4f} - val_loss: {val_loss:.4f} - val_acc: {val_acc:.4f}\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_epoch = epoch + 1\n            best_state = copy.deepcopy(model.state_dict())\n\n    # Load best weights\n    model.load_state_dict(best_state)\n\n    # Move to CPU for PTQ\n    model_cpu = model.to(\"cpu\").eval()\n\n    # Create a small CPU calibration loader (reuse train_loader but ensure CPU tensors)\n    calib_ds = TensorDataset(X_train.cpu(), y_train.cpu())\n    calib_loader = DataLoader(calib_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False)\n\n    quantized_model, qinfo = quantize_model_post_training(\n        model_cpu,\n        calib_loader,\n        bits=quantization_bits,\n        quantize_weights=quantize_weights,\n        quantize_activations=quantize_activations,\n        calibration_batches=calibration_batches,\n        backend=quant_backend,\n    )\n\n    metrics: Dict[str, Any] = {\n        \"train_losses\": train_losses,\n        \"val_losses\": val_losses,\n        \"val_acc\": val_accs,\n        \"best_val_acc\": best_val_acc,\n        \"best_epoch\": best_epoch,\n        \"param_count\": int(param_count),\n        \"class_distribution\": torch.bincount(y_train.cpu(), minlength=num_classes).tolist(),\n        \"quantization\": qinfo,\n    }\n\n    return quantized_model, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-05,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 128,
      "type": "Categorical",
      "categories": [
        32,
        64,
        128,
        256
      ]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 5,
      "high": 100
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-06,
      "high": 0.01,
      "prior": "log-uniform"
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "stem_channels": {
      "default": 16,
      "type": "Integer",
      "low": 8,
      "high": 32
    },
    "dw_kernel_size": {
      "default": 7,
      "type": "Integer",
      "low": 3,
      "high": 11
    },
    "cnn_width_mult": {
      "default": 1.0,
      "type": "Real",
      "low": 0.5,
      "high": 2.0
    },
    "d_model": {
      "default": 64,
      "type": "Integer",
      "low": 32,
      "high": 128
    },
    "nhead": {
      "default": 4,
      "type": "Categorical",
      "categories": [
        2,
        4,
        8
      ]
    },
    "num_transformer_layers": {
      "default": 1,
      "type": "Integer",
      "low": 1,
      "high": 3
    },
    "dim_feedforward": {
      "default": 128,
      "type": "Integer",
      "low": 64,
      "high": 256
    },
    "optimizer": {
      "default": "adamw",
      "type": "Categorical",
      "categories": [
        "adamw",
        "adam"
      ]
    },
    "scheduler": {
      "default": "none",
      "type": "Categorical",
      "categories": [
        "none",
        "cosine",
        "onecycle"
      ]
    },
    "warmup_epochs": {
      "default": 0,
      "type": "Integer",
      "low": 0,
      "high": 5
    },
    "use_focal_loss": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 1.0,
      "high": 3.0
    },
    "label_smoothing": {
      "default": 0.0,
      "type": "Real",
      "low": 0.0,
      "high": 0.2
    },
    "grad_clip": {
      "default": 0.5,
      "type": "Real",
      "low": 0.0,
      "high": 1.0
    },
    "quantization_bits": {
      "default": 8,
      "type": "Categorical",
      "categories": [
        8,
        16,
        32
      ]
    },
    "quantize_weights": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "quantize_activations": {
      "default": true,
      "type": "Categorical",
      "categories": [
        true,
        false
      ]
    },
    "calibration_batches": {
      "default": 16,
      "type": "Integer",
      "low": 4,
      "high": 64
    },
    "quant_backend": {
      "default": "fbgemm",
      "type": "Categorical",
      "categories": [
        "fbgemm",
        "qnnpack"
      ]
    }
  },
  "confidence": 0.9,
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1758400458,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}