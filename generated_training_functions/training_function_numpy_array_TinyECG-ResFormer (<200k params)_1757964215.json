{
  "model_name": "TinyECG-ResFormer (<200k params)",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    import math\n    import numpy as np\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n    from torch.optim import AdamW\n    from torch.optim.lr_scheduler import LambdaLR\n    from collections import defaultdict\n    from copy import deepcopy\n\n    # --------------------------\n    # Defaults & hyperparameters\n    # --------------------------\n    hp = {\n        'lr': 1e-3,\n        'epochs': 20,\n        'batch_size': 128,\n        'weight_decay': 1e-2,\n        'dropout': 0.2,\n        'hidden_size': 96,       # d_model\n        'num_heads': 4,\n        'num_layers': 2,\n        'gamma': 1.5,            # focal loss gamma\n        'patience': 7,\n        'seed': 42,\n        'amp': True,\n        'warmup_epochs': 2,\n        'grad_clip': 1.0,\n        'augment': True,\n        'num_classes': 5\n    }\n    hp.update(hyperparams or {})\n\n    # --------------------------\n    # Reproducibility & device\n    # --------------------------\n    torch.manual_seed(hp['seed'])\n    np.random.seed(hp['seed'])\n    if isinstance(device, str):\n        device = torch.device(device)\n    torch.backends.cudnn.benchmark = True\n\n    # --------------------------\n    # Utilities\n    # --------------------------\n    def to_ch_first(x):\n        # Expect per-sample (L, C) or (C, L); convert batch to (N, C, L)\n        if x.ndim != 3:\n            raise ValueError(f\"Expected 3D array (N, L, C) or (N, C, L), got shape {x.shape}\")\n        n, a, b = x.shape\n        if a == 2 and b != 2:\n            return x.astype(np.float32)\n        elif b == 2 and a != 2:\n            return np.transpose(x, (0, 2, 1)).astype(np.float32)\n        elif a == 2 and b == 2:\n            raise ValueError(\"Ambiguous shape: both last two dims are 2. Expected (N, 1000, 2) or (N, 2, 1000).\")\n        else:\n            # assume last dim is channels if equals 2 else error\n            raise ValueError(\"Expected channels dimension size 2.\")\n\n    class ECGDataset(Dataset):\n        def __init__(self, X, y=None, train=False, augment=False):\n            self.X = X\n            self.y = y\n            self.train = train\n            self.augment = augment and train\n            self.L = X.shape[-1]\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            if self.augment:\n                x = self._augment(x)\n            if self.y is None:\n                return torch.from_numpy(x)\n            return torch.from_numpy(x), int(self.y[idx])\n        def _augment(self, x):\n            # x: (C, L)\n            C, L = x.shape\n            xi = x.copy()\n            # Scaling (per-channel)\n            if np.random.rand() < 0.5:\n                scale = np.random.normal(1.0, 0.1, size=(C, 1)).astype(np.float32)\n                xi = xi * scale\n            # Jitter (Gaussian noise)\n            if np.random.rand() < 0.5:\n                std = np.std(xi, axis=1, keepdims=True) + 1e-6\n                noise = np.random.normal(0.0, 0.01, size=xi.shape).astype(np.float32) * std\n                xi = xi + noise\n            # Time shift (circular)\n            if np.random.rand() < 0.5:\n                max_shift = int(0.1 * L)\n                s = np.random.randint(-max_shift, max_shift + 1)\n                xi = np.roll(xi, s, axis=1)\n            # Random time mask (drop a short segment)\n            if np.random.rand() < 0.5:\n                max_w = int(0.1 * L)\n                w = np.random.randint(1, max(2, max_w))\n                start = np.random.randint(0, L - w + 1)\n                xi[:, start:start + w] = 0.0\n            return xi\n\n    # Focal Loss for multi-class\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=1.5, alpha=None, reduction='mean', eps=1e-8):\n            super().__init__()\n            self.gamma = gamma\n            self.reduction = reduction\n            self.eps = eps\n            if alpha is not None:\n                self.register_buffer('alpha', torch.tensor(alpha, dtype=torch.float32))\n            else:\n                self.alpha = None\n        def forward(self, logits, target):\n            # logits: (B, C), target: (B,)\n            log_probs = F.log_softmax(logits, dim=-1)\n            probs = log_probs.exp()\n            ce = F.nll_loss(log_probs, target, weight=self.alpha, reduction='none')\n            pt = probs.gather(1, target.unsqueeze(1)).squeeze(1).clamp(min=self.eps, max=1.0)\n            loss = ((1 - pt) ** self.gamma) * ce\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            return loss\n\n    # Squeeze-and-Excitation block (lightweight)\n    class SEBlock(nn.Module):\n        def __init__(self, c, r=8):\n            super().__init__()\n            m = max(4, c // r)\n            self.se = nn.Sequential(\n                nn.AdaptiveAvgPool1d(1),\n                nn.Conv1d(c, m, kernel_size=1),\n                nn.GELU(),\n                nn.Conv1d(m, c, kernel_size=1),\n                nn.Sigmoid()\n            )\n        def forward(self, x):\n            s = self.se(x)\n            return x * s\n\n    # Depthwise Separable Conv1d Block with Residual\n    class DSResBlock(nn.Module):\n        def __init__(self, in_c, out_c, k=7, dilation=1, stride=1, dropout=0.1):\n            super().__init__()\n            pad = (k - 1) // 2 * dilation\n            self.dw1 = nn.Conv1d(in_c, in_c, kernel_size=k, stride=stride, padding=pad, dilation=dilation, groups=in_c, bias=False)\n            self.pw1 = nn.Conv1d(in_c, out_c, kernel_size=1, bias=False)\n            self.bn1 = nn.BatchNorm1d(out_c)\n            self.dw2 = nn.Conv1d(out_c, out_c, kernel_size=k, padding=pad, dilation=dilation, groups=out_c, bias=False)\n            self.pw2 = nn.Conv1d(out_c, out_c, kernel_size=1, bias=False)\n            self.bn2 = nn.BatchNorm1d(out_c)\n            self.se = SEBlock(out_c)\n            self.act = nn.GELU()\n            self.do = nn.Dropout(dropout)\n            self.proj = None\n            if in_c != out_c or stride != 1:\n                self.proj = nn.Sequential(\n                    nn.Conv1d(in_c, out_c, kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm1d(out_c)\n                )\n        def forward(self, x):\n            identity = x\n            out = self.dw1(x)\n            out = self.pw1(out)\n            out = self.bn1(out)\n            out = self.act(out)\n            out = self.do(out)\n            out = self.dw2(out)\n            out = self.pw2(out)\n            out = self.bn2(out)\n            out = self.se(out)\n            if self.proj is not None:\n                identity = self.proj(identity)\n            out = self.act(out + identity)\n            return out\n\n    # Sinusoidal positional encoding\n    def positional_encoding(L, d):\n        pe = torch.zeros(L, d)\n        position = torch.arange(0, L, dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d, 2, dtype=torch.float32) * (-math.log(10000.0) / d))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return pe  # (L, d)\n\n    class TinyECGResFormer(nn.Module):\n        def __init__(self, in_ch=2, num_classes=5, d_model=96, num_heads=4, num_layers=2, dropout=0.2):\n            super().__init__()\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, 48, kernel_size=7, padding=3, bias=False),\n                nn.BatchNorm1d(48),\n                nn.GELU()\n            )\n            self.block1 = DSResBlock(48, 64, k=7, dilation=1, stride=2, dropout=dropout)   # L -> L/2\n            self.block2 = DSResBlock(64, 96, k=9, dilation=2, stride=2, dropout=dropout)   # L/2 -> L/4\n            self.block3 = DSResBlock(96, 96, k=17, dilation=4, stride=1, dropout=dropout)\n            self.to_model = nn.Conv1d(96, d_model, kernel_size=1)\n\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=num_heads,\n                dim_feedforward=d_model * 2,\n                dropout=dropout,\n                activation='gelu',\n                batch_first=True,\n                norm_first=True\n            )\n            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n            self.dropout = nn.Dropout(dropout)\n            self.cls_head = nn.Linear(d_model, num_classes)\n            self.pos_cache = None\n        def forward(self, x):\n            # x: (B, C, L)\n            B, C, L = x.shape\n            h = self.stem(x)\n            h = self.block1(h)\n            h = self.block2(h)\n            h = self.block3(h)\n            h = self.to_model(h)  # (B, d_model, L')\n            h = h.transpose(1, 2)  # (B, L', d_model)\n            Lp = h.size(1)\n            if (self.pos_cache is None) or (self.pos_cache.size(0) < Lp) or (self.pos_cache.size(1) != h.size(2)):\n                self.pos_cache = positional_encoding(Lp, h.size(2)).to(h.device)\n            pe = self.pos_cache[:Lp, :]\n            h = h + pe.unsqueeze(0)\n            h = self.transformer(h)\n            h = h.mean(dim=1)\n            h = self.dropout(h)\n            logits = self.cls_head(h)\n            return logits\n\n    # ---------------\n    # Data preparation\n    # ---------------\n    X_train = to_ch_first(np.asarray(X_train))\n    X_val = to_ch_first(np.asarray(X_val))\n    y_train = np.asarray(y_train).astype(np.int64)\n    y_val = np.asarray(y_val).astype(np.int64)\n\n    num_classes = hp['num_classes']\n\n    # Class weights for focal loss and sampling\n    class_counts = np.bincount(y_train, minlength=num_classes)\n    class_counts[class_counts == 0] = 1\n    inv_freq = 1.0 / class_counts\n    sample_weights = inv_freq[y_train]\n\n    train_ds = ECGDataset(X_train, y_train, train=True, augment=hp['augment'])\n    val_ds = ECGDataset(X_val, y_val, train=False, augment=False)\n\n    train_sampler = WeightedRandomSampler(weights=torch.as_tensor(sample_weights, dtype=torch.double), num_samples=len(train_ds), replacement=True)\n\n    train_loader = DataLoader(train_ds, batch_size=hp['batch_size'], sampler=train_sampler, num_workers=2, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=hp['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n\n    # ---------------\n    # Model & optimizer\n    # ---------------\n    model = TinyECGResFormer(in_ch=2, num_classes=num_classes, d_model=hp['hidden_size'], num_heads=hp['num_heads'], num_layers=hp['num_layers'], dropout=hp['dropout']).to(device)\n\n    # Parameter count check (< 256k)\n    param_count = sum(p.numel() for p in model.parameters())\n    if param_count >= 256_000:\n        raise RuntimeError(f\"Model has {param_count} parameters; exceeds the 256k limit. Reduce hidden_size/num_layers/heads.\")\n\n    optimizer = AdamW(model.parameters(), lr=hp['lr'], weight_decay=hp['weight_decay'])\n\n    # Warmup + cosine schedule (per-step)\n    steps_per_epoch = max(1, len(train_loader))\n    total_steps = steps_per_epoch * hp['epochs']\n    warmup_steps = int(hp['warmup_epochs'] * steps_per_epoch)\n\n    def lr_lambda(step):\n        if step < warmup_steps and warmup_steps > 0:\n            return float(step + 1) / float(warmup_steps)\n        if total_steps <= warmup_steps:\n            return 1.0\n        progress = (step - warmup_steps) / max(1, (total_steps - warmup_steps))\n        return 0.5 * (1 + math.cos(math.pi * progress))\n\n    scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n    # Loss: Focal with class-aware alpha\n    alpha = (len(y_train) / (num_classes * class_counts)).astype(np.float32)\n    criterion = FocalLoss(gamma=hp['gamma'], alpha=alpha)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=hp['amp'] and device.type == 'cuda')\n\n    # ---------------\n    # Metrics helpers\n    # ---------------\n    def compute_metrics(preds, targets, num_classes):\n        # Move tensors to same device as preds\n        device = preds.device\n        cm = torch.zeros((num_classes, num_classes), dtype=torch.long, device=device)\n        for t, p in zip(targets, preds):\n            cm[t, p] += 1\n        tp = cm.diag().to(torch.float32)\n        fp = cm.sum(0) - tp\n        fn = cm.sum(1) - tp\n        precision = tp / (tp + fp + 1e-9)\n        recall = tp / (tp + fn + 1e-9)\n        f1 = 2 * precision * recall / (precision + recall + 1e-9)\n        acc = tp.sum() / (cm.sum() + 1e-9)\n        macro_f1 = f1.mean()\n        return {\n            'confusion_matrix': cm.detach().cpu().numpy(),\n            'precision_per_class': precision.detach().cpu().numpy(),\n            'recall_per_class': recall.detach().cpu().numpy(),\n            'f1_per_class': f1.detach().cpu().numpy(),\n            'accuracy': float(acc.detach().cpu().item()),\n            'macro_f1': float(macro_f1.detach().cpu().item())\n        }\n\n    # ---------------\n    # Training loop\n    # ---------------\n    best_state = None\n    best_metric = -1.0\n    best_epoch = -1\n    history = defaultdict(list)\n\n    for epoch in range(hp['epochs']):\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n\n        for step, batch in enumerate(train_loader):\n            inputs, targets = batch\n            inputs = inputs.to(device, non_blocking=True)\n            targets = torch.as_tensor(targets, dtype=torch.long, device=device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=hp['amp'] and device.type == 'cuda'):\n                logits = model(inputs)\n                loss = criterion(logits, targets)\n            scaler.scale(loss).backward()\n            if hp['grad_clip'] is not None and hp['grad_clip'] > 0:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), hp['grad_clip'])\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n\n            train_loss += loss.item() * inputs.size(0)\n            preds = logits.argmax(dim=1)\n            correct += (preds == targets).sum().item()\n            total += inputs.size(0)\n\n        train_loss /= max(1, total)\n        train_acc = correct / max(1, total)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        all_preds = []\n        all_tgts = []\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs, targets = batch\n                inputs = inputs.to(device, non_blocking=True)\n                targets = torch.as_tensor(targets, dtype=torch.long, device=device)\n                logits = model(inputs)\n                loss = criterion(logits, targets)\n                val_loss += loss.item() * inputs.size(0)\n                preds = logits.argmax(dim=1)\n                all_preds.append(preds.cpu())\n                all_tgts.append(targets.cpu())\n        val_loss /= max(1, len(val_ds))\n        all_preds = torch.cat(all_preds, dim=0)\n        all_tgts = torch.cat(all_tgts, dim=0)\n        metrics = compute_metrics(all_preds, all_tgts, num_classes)\n\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(metrics['accuracy'])\n        history['val_macro_f1'].append(metrics['macro_f1'])\n\n        # Early stopping on macro-F1 (inter-class balance focus)\n        if metrics['macro_f1'] > best_metric:\n            best_metric = metrics['macro_f1']\n            best_state = deepcopy(model.state_dict())\n            best_epoch = epoch\n            best_metrics_snapshot = metrics\n\n        if epoch - best_epoch >= hp['patience']:\n            break\n\n    # Load best\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    # Final metrics on val with best model\n    model.eval()\n    with torch.no_grad():\n        all_preds = []\n        all_tgts = []\n        for batch in val_loader:\n            inputs, targets = batch\n            inputs = inputs.to(device, non_blocking=True)\n            targets = torch.as_tensor(targets, dtype=torch.long, device=device)\n            logits = model(inputs)\n            preds = logits.argmax(dim=1)\n            all_preds.append(preds.cpu())\n            all_tgts.append(targets.cpu())\n    all_preds = torch.cat(all_preds, dim=0)\n    all_tgts = torch.cat(all_tgts, dim=0)\n    final_metrics = compute_metrics(all_preds, all_tgts, num_classes)\n\n    metrics_out = {\n        'best_epoch': int(best_epoch),\n        'param_count': int(param_count),\n        'history': {k: list(map(float, v)) for k, v in history.items()},\n        'val_confusion_matrix': final_metrics['confusion_matrix'].tolist(),\n        'val_precision_per_class': final_metrics['precision_per_class'].tolist(),\n        'val_recall_per_class': final_metrics['recall_per_class'].tolist(),\n        'val_f1_per_class': final_metrics['f1_per_class'].tolist(),\n        'val_accuracy': final_metrics['accuracy'],\n        'val_macro_f1': final_metrics['macro_f1']\n    }\n\n    return model, metrics_out\n",
  "hyperparameters": {
    "lr": 0.001,
    "epochs": 20,
    "batch_size": 128,
    "dropout": 0.2,
    "hidden_size": 96
  },
  "reasoning": "Implements a compact CNN+Transformer hybrid tailored for 1D ECG windows (1000x2), aligning with recent findings that lightweight attention atop multi-scale CNN front-ends yields strong inter-patient performance. The model uses depthwise-separable residual Conv1D blocks with dilations (multi-scale morphology) and a tiny Transformer encoder (d_model=96, 2 layers, 4 heads) to keep parameters under 256k. Class imbalance is addressed via WeightedRandomSampler and class-aware focal loss, improving recall for challenging S/F classes. Time-series augmentations (jitter, scaling, shift, time-mask) further regularize training. Optimization follows AdamW with warmup+cosine schedule and gradient clipping. The training loop tracks macro-F1 and performs early stopping on it, as recommended for MIT-BIH evaluation. The function is self-contained and returns the trained model and detailed metrics.",
  "confidence": 0.9,
  "bo_parameters": [
    "lr",
    "batch_size",
    "epochs",
    "hidden_size",
    "dropout"
  ],
  "bo_search_space": {
    "lr": {
      "type": "Real",
      "low": 1e-05,
      "high": 0.1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "type": "Categorical",
      "categories": [
        8,
        16,
        32,
        64,
        128
      ]
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 50
    },
    "hidden_size": {
      "type": "Integer",
      "low": 32,
      "high": 192
    },
    "dropout": {
      "type": "Real",
      "low": 0.0,
      "high": 0.7
    }
  },
  "data_profile": {
    "data_type": "numpy_array",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  },
  "timestamp": 1757964215,
  "metadata": {
    "generated_by": "AI Code Generator",
    "api_model": "gpt-5",
    "version": "1.0"
  }
}