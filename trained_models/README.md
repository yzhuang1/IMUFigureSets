# Trained Model Checkpoints

Saved PyTorch model weights and checkpoints from successful training runs.

## Overview

This directory stores trained model weights as `.pth` files, preserving successful models for deployment, analysis, and reuse.

## File Structure

Models are saved with descriptive names including:
- Model architecture
- Dataset identifier
- Performance metrics
- Timestamp

**Examples:**
```
BiLSTM_Attention_ECG_acc87_20250102.pth
TransformerEncoder_tabular_f192_20250102.pth
QuantizedCNN_8bit_acc82_20250102.pth
```

## Saving Models

### Automatic Saving
Models are automatically saved by the training executor when they achieve good performance:

```python
# In training function generated by GPT
if val_acc > best_acc:
    best_acc = val_acc
    torch.save(model.state_dict(), f'trained_models/{model_name}_acc{int(val_acc*100)}.pth')
```

### Manual Saving
```python
import torch

# Save model state dict
torch.save(model.state_dict(), 'trained_models/my_model.pth')

# Save entire model
torch.save(model, 'trained_models/my_model_full.pth')

# Save with metadata
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'metrics': metrics,
    'hyperparams': hyperparams
}, 'trained_models/my_model_checkpoint.pth')
```

## Loading Models

### Load State Dict
```python
import torch
from _models.training_function_executor import training_executor

# Load training function to get architecture
training_data = training_executor.load_training_function('path/to/function.json')

# Create model instance
# (execute training code to instantiate model)
exec(training_data['training_code'], namespace)
model = namespace['create_model'](**hyperparams)  # If GPT defines create_model function

# Load weights
model.load_state_dict(torch.load('trained_models/model.pth'))
model.eval()
```

### Load Full Model
```python
model = torch.load('trained_models/model_full.pth')
model.eval()
```

### Load Checkpoint with Metadata
```python
checkpoint = torch.load('trained_models/checkpoint.pth')

model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
metrics = checkpoint['metrics']
```

## Quantized Models

Models quantized to 8-bit or 16-bit are significantly smaller:

```python
# Load quantized model
quantized_model = torch.load('trained_models/model_8bit.pth')

# Check size
import os
size_kb = os.path.getsize('trained_models/model_8bit.pth') / 1024
print(f"Model size: {size_kb:.1f}KB")
```

**Quantization Benefits:**
- 8-bit: ~75% size reduction, minimal accuracy loss
- 16-bit: ~50% size reduction, negligible accuracy loss
- Enables deployment on edge devices

## Model Metadata

Store additional information alongside models:

```python
import json

metadata = {
    'model_name': 'BiLSTM_Attention',
    'architecture': 'BiLSTM with attention mechanism',
    'hyperparameters': {'lr': 0.001, 'hidden_size': 128, ...},
    'metrics': {'accuracy': 0.87, 'f1_score': 0.85},
    'data_profile': {...},
    'training_date': '2025-01-02',
    'quantization': '8-bit weights',
    'storage_kb': 187.3
}

with open('trained_models/BiLSTM_Attention_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)
```

## Best Practices

1. **Consistent Naming**: Use clear, descriptive filenames
2. **Include Metrics**: Add performance indicators to filenames
3. **Save Metadata**: Store hyperparameters and configuration
4. **Version Control**: Don't commit large `.pth` files to git (use `.gitignore`)
5. **Regular Cleanup**: Remove old or underperforming models

## Deployment

### Export for Production
```python
# Export to TorchScript for deployment
scripted_model = torch.jit.script(model)
scripted_model.save('trained_models/model_scripted.pt')

# Or trace with example input
example_input = torch.randn(1, input_dim)
traced_model = torch.jit.trace(model, example_input)
traced_model.save('trained_models/model_traced.pt')
```

### ONNX Export
```python
import torch.onnx

dummy_input = torch.randn(1, input_dim)
torch.onnx.export(
    model,
    dummy_input,
    'trained_models/model.onnx',
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output']
)
```

## Storage Management

```bash
# List models by size
ls -lhS trained_models/*.pth

# Remove models older than 30 days
find trained_models -name "*.pth" -mtime +30 -delete

# Archive old models
tar -czf models_archive_$(date +%Y%m%d).tar.gz trained_models/*.pth
```

## Integration with Pipeline

Models are automatically saved during:
1. BO trials (if significant improvement)
2. Final training with best hyperparameters
3. Quantization steps

Access via pipeline results:
```python
results = train_with_iterative_selection(X, y)
model = results['model']
# Model is already the best trained version
```
