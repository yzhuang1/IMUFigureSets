LITERATURE REVIEW
=================

Query: ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-16 02:35:42
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
From 2023–2025, the most successful ECG arrhythmia classifiers on MIT-BIH have converged on three ideas: (i) hybrid backbones that mix multi‑scale 1D convolutions with attention to capture both morphologic detail and long‑range rhythm context; (ii) sequence-efficient Transformers that reduce token length via strided/patch embeddings so attention can model seconds of signal; and (iii) representation learning (self‑supervised and contrastive) to leverage unlabeled ECG and improve robustness. Recent exemplars include CAT‑Net (CNN + attention + Transformer) reporting strong five‑class results on MIT‑BIH and INCART with explicit class‑imbalance handling; multi‑branch, multi‑head‑attention TCNs that fuse multi‑scale dilated kernels with attention and focal loss; and “tiny” Transformers that keep parameters in the low‑thousands yet retain >98% accuracy for the five AAMI superclasses. Emerging work on SSL—masked autoencoders and dual‑pathway reconstruction + non‑contrastive pretraining—shows consistent downstream gains; concurrent “foundation‑style” ECG models trained with contrastive/SSL over large corpora also report top results when fine‑tuned to MIT‑BIH. At the same time, methodological papers continue to warn that beat‑wise cross‑validation can overstate generalization versus inter‑patient splits (e.g., DS1→DS2), with some methods dropping from ~99% to ~88% accuracy under record‑wise evaluation. Together, the evidence suggests hybrid CNN–(TCN/Transformer) models and SSL pretraining are the current state of the art when evaluated carefully. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))

For a PyTorch implementation tailored to your numpy_array inputs shaped (1000, 2), use a channels‑first tensor (B, C=2, L=1000), apply a short stack of 1D convolutions (kernel sizes 5–17) with residual connections to extract morphology, then either: (a) a TCN head with dilations doubling each layer until the receptive field exceeds 1000; or (b) a Transformer encoder over downsampled tokens created by a 1D conv patch embed (e.g., stride 4–8 → 125–250 tokens) plus learnable positional encodings. Train with AdamW, cosine decay, and warmup; handle class imbalance with focal loss or class‑weighted cross‑entropy and physiologically plausible augmentations (jitter, scaling, time‑warp, mixup along time) rather than naive SMOTE; and evaluate with the AAMI five‑class mapping on a strict inter‑patient split (DS1→DS2). Expect macro‑F1 in the low‑to‑mid‑90s on DS1→DS2 with robust setups, while beat‑wise k‑fold can exceed 98–99% but is optimistic. Use AMP (torch.cuda.amp.autocast), gradient clipping (0.5–1.0), and per‑class sampling to stabilize training; a 1–5M‑parameter hybrid runs comfortably on a single consumer GPU, and “tiny” variants fit edge devices. ([mdpi.com](https://www.mdpi.com/1424-8220/23/11/5237?utm_source=openai))

KEY FINDINGS:
1. Evaluation protocol matters most: inter‑patient DS1→DS2 splitting is the clinically realistic standard; beat‑wise (random) splits inflate metrics substantially. Use AAMI five‑class mapping and report macro‑F1, not just accuracy. ([arxiv.org](https://arxiv.org/html/2503.07276v1?utm_source=openai))
2. Hybrid CNN + attention backbones (e.g., CNN→MHA/Transformer or multi‑branch TCN with attention) capture morphology and rhythm across scales and are consistently strong on MIT‑BIH. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))
3. Transformers require sequence reduction: patch/strided 1D embeddings (stride 4–8) or local attention keep compute tractable while preserving context; lightweight designs can reach >98% five‑class accuracy with only ~6k params. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
4. Self‑supervised pretraining (masked autoencoders; dual‑pathway reconstruction + non‑contrastive) improves label efficiency and robustness; pretrain on large ECG corpora, then fine‑tune on MIT‑BIH. ([arxiv.org](https://arxiv.org/abs/2310.11153?utm_source=openai))
5. Class imbalance is pervasive; focal loss, per‑class sampling, and ECG‑specific augmentation (time‑warp, jitter, scaling) help minority classes more reliably than naive oversampling. ([mdpi.com](https://www.mdpi.com/1424-8220/23/11/5237?utm_source=openai))
6. Reported SOTA numbers >99% often rely on within‑record splits; under record‑wise evaluation, accuracy can fall markedly (e.g., ~88% in a 2023 study), underscoring the need for careful validation. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/37470489/?utm_source=openai))
7. For deployment, TCNs and tiny Transformers offer the best accuracy‑to‑efficiency trade‑off, enabling millijoule‑level inference latency on MCUs/ultra‑low‑power SoCs. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))

RECOMMENDED APPROACHES:
1. {'name': 'CNN → Multi‑Head Attention TCN (MB‑MHA‑TCN style)', 'description': '3–5 parallel 1D conv branches (kernels 5/9/17; dilations 1–8) feed a TCN stack; multi‑head self‑attention fuses branches; focal loss for imbalance. Typical: 1–3M params, dropout 0.1–0.3, AdamW lr 1e‑3. Strong five‑class MIT‑BIH results and robust minority‑class recognition. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/39771858/?utm_source=openai))'}
2. {'name': 'CNN + Transformer encoder (CAT‑Net style)', 'description': 'Residual 1D CNN stem → 1D patch embedding (stride 4–8) → 2–4 Transformer blocks (d_model 128–256, 4–8 heads) → MLP head. Balance classes with SMOTE‑Tomek or weighted loss; use cosine lr and warmup. Excellent accuracy and macro‑F1 on MIT‑BIH five‑class. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))'}
3. {'name': 'Tiny Transformer for edge deployment', 'description': 'Compact encoder with ~6k parameters, INT8 inference, trained with augmentation against motion artifacts. Suitable when compute is constrained; still achieves ~98–99% five‑class accuracy under common protocols. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))'}
4. {'name': 'Self‑Supervised Pretraining + Small Classifier', 'description': 'Pretrain an encoder using masked autoencoders or dual‑pathway SSL on unlabeled ECG; fine‑tune a small CNN/Transformer head for five‑class AAMI labels. Yields better generalization with limited labels/noise. ([arxiv.org](https://arxiv.org/abs/2310.11153?utm_source=openai))'}
5. {'name': 'Continuous‑segment detection with Transformer (ECG‑DETR variant)', 'description': 'For end‑to‑end rhythm analysis without beat segmentation: predict beat positions and classes jointly on 2–4 s segments; useful when your (1000,2) window spans multiple beats. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/35227968/?utm_source=openai))'}

RECENT PAPERS:
- CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (2024): No description
- Accurate Arrhythmia Classification with Multi‑Branch, Multi‑Head Attention Temporal Convolutional Networks (2025): No description
- A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (2024): No description
- Unsupervised Pre‑Training Using Masked Autoencoders for ECG Analysis (2023): No description
- NERULA: A Dual‑Pathway Self‑Supervised Learning Framework for ECG (2024): No description
- A novel transformer‑based ECG dimensionality reduction with stacked auto‑encoders (2023): No description
- Systematic Survey of Data Augmentation of ECG Signals for AI Applications (2023): No description
- A Systematic Review of ECG Arrhythmia Classification: Adherence to Standards, Fair Evaluation, and Embedded Feasibility (2025): No description
- A transformer‑based deep neural network for arrhythmia detection on continuous ECG (ECG‑DETR) (2022): No description
- An Arrhythmia Classification Model Based on Vision Transformer with Deformable Attention (2023): No description

==================================================
