# AI Literature Reviews

AI-generated literature reviews and research summaries using GPT-5 with web search to inform model architecture selection.

## Overview

This directory stores comprehensive literature reviews generated by GPT-5 to provide research-backed context for model selection. Reviews analyze recent papers and recommend architectures based on data characteristics.

## File Structure

```
literature_review_{data_type}_{timestamp}.json
```

**Examples:**
```
literature_review_numpy_array_1704067200.json
literature_review_sequence_list_1704070800.json
literature_review_tabular_1704074400.json
```

## JSON Format

```json
{
  "query": "ECG classification arrhythmia detection 2024 2025 PyTorch",
  "review_text": "Comprehensive summary of research findings...",
  "key_findings": [
    "BiLSTM with attention achieves 94% accuracy on MIT-BIH (Zhang et al. 2024)",
    "Transformer models require 2-3x more data for comparable performance",
    "Multi-scale CNN approaches show promise for real-time ECG analysis"
  ],
  "recommended_approaches": [
    "BiLSTM with attention mechanism for sequence classification"
  ],
  "recent_papers": [
    {
      "title": "Attention-Based BiLSTM for ECG Arrhythmia Detection",
      "contribution": "Achieved 94.2% accuracy with 40% fewer parameters"
    },
    {
      "title": "Temporal Convolutional Networks for Medical Time Series",
      "contribution": "Real-time inference with 95% accuracy on PhysioNet"
    }
  ],
  "confidence": 0.89,
  "timestamp": 1704067200,
  "generated_time": "2025-01-02 14:30:00",
  "data_profile": {...}
}
```

## Generation

### Automatic Generation
Literature reviews are automatically generated when enabled:

```python
from _models.ai_code_generator import generate_training_code_for_data

code_rec = generate_training_code_for_data(
    data_profile,
    input_shape=(1000, 2),
    num_classes=5,
    include_literature_review=True  # Triggers literature review
)
```

### Manual Generation
```python
from _models.literature_review import generate_literature_review_for_data

review = generate_literature_review_for_data(
    data_profile={'is_sequence': True, 'sample_count': 5000, ...},
    input_shape=(1000, 2),
    num_classes=5
)

print(f"Query: {review.query}")
print(f"Recommended: {review.recommended_approaches}")
print(f"Confidence: {review.confidence}")
```

## Configuration

### Enable/Disable
```bash
# In .env file
SKIP_LITERATURE_REVIEW=false  # Enable (default)
SKIP_LITERATURE_REVIEW=true   # Disable to save API calls
```

### Dataset Context
Reviews use dataset context from environment:

```bash
# In .env
DATASET_NAME="MIT-BIH Arrhythmia Database"
DATASET_SOURCE="PhysioNet"

# These are included in the research query for more relevant results
```

## Research Query Generation

Queries are automatically constructed based on data characteristics:

```python
# For ECG data
query = "ECG classification arrhythmia detection heart rhythm analysis 2024 2025 PyTorch"

# For general sequence data
query = "sequence classification time series machine learning 2024 2025 PyTorch"

# For tabular data
query = "tabular data classification machine learning 2024 2025 PyTorch"
```

## Integration with Code Generation

Reviews inform the GPT prompt:

```python
# In ai_code_generator.py
if literature_review:
    recommended_approach = literature_review.recommended_approaches[0]
    prompt += f"\nRecommend: {recommended_approach}"
    # GPT considers this recommendation when generating code
```

## Usage Examples

### Load Review
```python
import json

with open('literature_reviews/literature_review_numpy_array_1704067200.json') as f:
    review = json.load(f)

print(f"Research findings: {len(review['key_findings'])}")
print(f"Papers analyzed: {len(review['recent_papers'])}")
```

### Compare Reviews
```python
import os
import json

reviews_dir = 'literature_reviews'
reviews = []

for filename in os.listdir(reviews_dir):
    if filename.endswith('.json'):
        with open(os.path.join(reviews_dir, filename)) as f:
            reviews.append(json.load(f))

# Compare recommended approaches
for review in reviews:
    print(f"Query: {review['query']}")
    print(f"Recommended: {review['recommended_approaches'][0]}")
    print(f"Confidence: {review['confidence']}\n")
```

### Extract Key Insights
```python
review_data = json.load(open('literature_reviews/latest_review.json'))

print("KEY RESEARCH INSIGHTS:")
for i, finding in enumerate(review_data['key_findings'], 1):
    print(f"{i}. {finding}")

print("\nRECENT PAPERS:")
for paper in review_data['recent_papers']:
    print(f"- {paper['title']}")
    print(f"  {paper['contribution']}\n")
```

## LiteratureReview Class

```python
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class LiteratureReview:
    query: str                          # Research query used
    review_text: str                    # Comprehensive summary
    key_findings: List[str]             # Key insights (max 7)
    recommended_approaches: List[str]   # Model recommendations (max 5)
    recent_papers: List[Dict]           # Paper citations
    confidence: float                   # Review confidence (0-1)
    timestamp: int                      # Generation timestamp
```

## GPT-5 Web Search

Reviews use GPT-5's web search capabilities:

```python
# In literature_review.py
response = self.client.responses.create(
    model="gpt-5",
    tools=[{"type": "web_search"}],  # Enables web search
    input=f"Research Query: {query}\n\nTask: {research_prompt}"
)
```

**Benefits:**
- Access to current research (2024-2025)
- Real papers and citations
- Domain-specific insights
- Performance benchmarks

## Cost Considerations

Literature reviews use GPT-5 with web search, which has higher API costs:

- **When to Use**: Novel datasets, important production models
- **When to Skip**: Development, testing, known architectures
- **Configuration**: Set `SKIP_LITERATURE_REVIEW=true` to disable

## Storage Management

```bash
# List reviews by date
ls -lt literature_reviews/*.json

# Remove old reviews
find literature_reviews -name "*.json" -mtime +60 -delete

# Archive reviews
tar -czf reviews_archive_$(date +%Y%m%d).tar.gz literature_reviews/*.json
```

## Best Practices

1. **Enable for Production**: Use reviews for important model selection
2. **Disable for Testing**: Skip during development to save costs
3. **Review Quality**: Higher confidence (>0.8) indicates better research
4. **Archive Important Reviews**: Save successful reviews for future reference
5. **Context Matters**: Set DATASET_NAME and DATASET_SOURCE for better results
