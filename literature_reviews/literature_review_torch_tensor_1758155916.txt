LITERATURE REVIEW
=================

Query: ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-17 19:38:36
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "torch_tensor",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
Scope and dataset. We reviewed 2024–2025 literature on multiclass ECG arrhythmia sequence classification, emphasizing MIT-BIH (two channels at 360 Hz) and the common AAMI 5-class mapping (N/S/V/F/Q) with inter‑patient DS1/DS2 evaluation when available. MIT-BIH contains 48 half‑hour, two‑lead recordings digitized at 360 Hz with 11-bit resolution. This matches your (1000,2) tensor windows after segmentation. ([physionet.org](https://www.physionet.org/content/mitdb/1.0.0/?utm_source=openai))
State of the art (SOTA). 1) CAT-Net (CNN + attention + Transformer) reports 99.14% overall accuracy and 94.69% macro F1 on 5-class MIT-BIH, using single-lead inputs with SMOTE‑Tomek to address class imbalance; architecture captures local morphology via CNN and global dependencies via Transformer. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai)) 2) Tiny Transformer for low-power deployment attains 98.97% accuracy on 5-class MIT-BIH with only ~6k parameters, 8-bit inference, and on-device latency/energy of 4.28 ms/0.09 mJ on GAP9—useful where efficiency dominates. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai)) 3) MambaCapsule (Mamba SSM + Capsule) emphasizes transparency via signal reconstruction and reports 99.54% accuracy on AAMI-standard MIT-BIH evaluation (preprint). ([arxiv.org](https://arxiv.org/abs/2407.20893?utm_source=openai)) 4) Hierarchical Attention Network (HAN) targets interpretability with two attention scales; on MIT-BIH it achieved 98.55% accuracy while being 15.6× smaller than CAT‑Net, highlighting a strong accuracy–efficiency tradeoff. ([arxiv.org](https://arxiv.org/abs/2504.03703?utm_source=openai)) 5) A multi-branch temporal CNN with multi-head attention achieved 98.75% accuracy, precision 96.60%, sensitivity 97.21%, and F1 96.89% on 5-class MIT-BIH via five‑fold CV, underscoring the competitiveness of CNN+attention without full Transformers. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124?utm_source=openai))
Surveys/meta. 2025 Information Fusion reviews Transformers across biosignals, noting advantages for long-range dependencies and open issues (e.g., data efficiency, interpretability). A 2024 MIT‑BIH–focused survey catalogs algorithmic families and evaluation pitfalls (e.g., patient leakage), reinforcing the value of inter‑patient protocols (DS1/DS2). ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253524004755?utm_source=openai))
Methodological notes. Strongest reports often use heartbeat-centered windows and may mix patients; inter‑patient DS1/DS2 testing remains the stricter protocol for generalization. For your (1000,2) setup, two‑lead, ~3 s windows are compatible with CNN+Transformer hybrids; class imbalance mitigation (e.g., SMOTE‑Tomek or focal/class‑balanced loss) is recommended. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC6208516/?utm_source=openai))
Compute considerations. CAT‑Net offers top accuracy with moderate complexity (not fully reported) but has a lighter interpretability alternative (HAN) at ~15.6× fewer parameters with minimal accuracy loss, while the Tiny Transformer is the most efficient—~6k params with near‑SOTA accuracy and demonstrated microcontroller deployment. ([arxiv.org](https://arxiv.org/abs/2504.03703?utm_source=openai))

KEY FINDINGS:
1. Hybrid CNN+Transformer architectures currently lead 5-class MIT-BIH performance; CAT-Net reports 99.14% accuracy and 94.69% macro F1 using single-lead inputs with SMOTE‑Tomek balancing. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))
2. Ultra‑compact models can approach SOTA: a Tiny Transformer with ~6k parameters reached 98.97% accuracy on MIT-BIH and achieved 4.28 ms, 0.09 mJ inference on GAP9 with 8‑bit quantization. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
3. Interpretability–efficiency tradeoff: a Hierarchical Attention Network achieved 98.55% accuracy on MIT-BIH with 15.6× fewer parameters than CAT‑Net, suggesting attention-only hierarchies can be highly efficient with small accuracy penalties. ([arxiv.org](https://arxiv.org/abs/2504.03703?utm_source=openai))
4. Alternative attention-enhanced CNNs (multi-branch TCN + multi-head attention) achieve 98.75% accuracy (precision 96.60%, sensitivity 97.21%, F1 96.89%) on MIT-BIH, indicating full Transformers are not strictly required for high performance. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124?utm_source=openai))
5. Robust evaluation matters: inter‑patient DS1/DS2 splits and AAMI 5‑class mapping are recommended to avoid patient leakage and ensure generalization on MIT-BIH. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC6208516/?utm_source=openai))

RECOMMENDED APPROACHES:
1. CAT‑Net (CNN + multi‑head self‑attention + Transformer encoder) adapted to 2‑lead inputs: 1D multi‑scale convolutional stem over both channels → positional encoding → 1–2 lightweight Transformer encoder blocks (d_model≈128–256, 2–4 heads) → global pooling → linear 5‑way head; train with class‑balanced/focal loss or SMOTE‑Tomek and inter‑patient splits. Rationale: top published 5‑class MIT-BIH accuracy (99.14% acc, 94.69% macro F1), strong fit to (1000,2) sequences, and straightforward PyTorch implementation with available references/code. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))

RECENT PAPERS:
- CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (2024, BSPC): Hybrid CNN+Transformer with imbalance handling; 5‑class MIT‑BIH: 99.14% accuracy, 94.69% macro F1; provides a strong baseline to adapt to two‑lead, 1000‑sample windows. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))
- A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (2024, arXiv): ~6k‑param Transformer reaching 98.97% accuracy on 5‑class MIT‑BIH; 8‑bit inference on GAP9 with 4.28 ms latency and 0.09 mJ energy—best for edge constraints. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
- MambaCapsule: Towards Transparent Cardiac Disease Diagnosis with Electrocardiography Using Mamba Capsule Network (2024, arXiv): SSM (Mamba) + Capsule network with reconstruction for interpretability; reports 99.54% accuracy on AAMI‑standard MIT‑BIH. ([arxiv.org](https://arxiv.org/abs/2407.20893?utm_source=openai))
- Hierarchical Attention Network for Interpretable ECG-based Heart Disease Classification (2025, arXiv): Two‑level attention for efficiency/interpretability; 98.55% accuracy on MIT‑BIH with ~15.6× fewer parameters vs CAT‑Net. ([arxiv.org](https://arxiv.org/abs/2504.03703?utm_source=openai))
- Accurate Arrhythmia Classification with Multi‑Branch, Multi‑Head Attention Temporal CNNs (2024, Sensors): Multi‑branch dilated TCN + multi‑head attention; 5‑class MIT‑BIH: 98.75% accuracy, 96.89% F1 (5‑fold CV), showing strong CNN+attention performance. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124?utm_source=openai))
- Transformers in biosignal analysis: A review (2025, Information Fusion): Survey of Transformer applications to biosignals (including ECG), summarizing benefits, challenges, and research trends relevant to arrhythmia sequence modeling. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1566253524004755?utm_source=openai))

==================================================
