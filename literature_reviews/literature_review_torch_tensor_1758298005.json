{
  "query": "ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Scope and data. The target is multiclass sequence classification of 2‑lead ECG beats/windows with shape (1000, 2) and 5 AAMI classes (N, S, V, F, Q) on the MIT‑BIH Arrhythmia Database. MIT‑BIH provides 48 half‑hour two‑channel ambulatory ECG records at 360 Hz with standard AAMI re‑mapping commonly used for 5‑class evaluation. ([physionet.org](https://www.physionet.org/content/mitdb/1.0.0/mitdbdir/?utm_source=openai))\n\nRecent state‑of‑the‑art models (2024–2025). A 2024 Elsevier paper (CAT‑Net) combines 1D CNN feature extractors with a Transformer encoder and attention for single‑lead arrhythmia classification, explicitly addressing class imbalance via SMOTE‑Tomek; on MIT‑BIH 5‑class it reports 99.14% overall accuracy and 94.69% macro‑F1. Open notebooks accompanying the work demonstrate the full preprocessing (wavelet denoising, R‑peak segmentation) and training pipeline. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))\n\nEfficiency‑focused SOTA. A 2024 “Tiny Transformer” achieves 98.97% 5‑class accuracy on MIT‑BIH with only ~6k parameters and 8‑bit inference; it runs in 4.28 ms and ~0.09 mJ on a GAP9 microcontroller, showing strong accuracy–efficiency tradeoffs for embedded use. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))\n\nAlternative Transformer variants. A 2024 Frontiers study converts ECG to wavelet time‑frequency maps and uses a Swin Transformer; it reports 99.34% intra‑patient and 98.37% inter‑patient accuracy on MIT‑BIH, but requires 2D image pipelines and is heavier computationally than 1D models. ([frontiersin.org](https://www.frontiersin.org/journals/cardiovascular-medicine/articles/10.3389/fcvm.2024.1401143/full))\n\nHybrid temporal models. Prior (late‑2023) work fusing CNN + Transformer + Bi‑LSTM reports 99.2% accuracy and macro‑F1 on MIT‑BIH 5‑class, underscoring the effectiveness of local (convolutional) plus long‑range (attention/recurrent) modeling. ([mdpi.com](https://www.mdpi.com/2073-431X/12/3/68?utm_source=openai))\n\nComputational considerations. CAT‑Net’s complexity is moderate for modern GPUs and can be implemented efficiently in PyTorch; a 2025 comparison found a hierarchical attention network (HAN) achieving similar accuracy with ~15.6× fewer parameters than CAT‑Net, implying CAT‑Net is substantially larger than very‑compact attention baselines, though still practical on commodity hardware. Tiny‑Transformer is the most resource‑efficient (6k params, integer inference). Swin‑Transformer methods incur higher cost due to 2D transforms and windowed self‑attention. ([arxiv.org](https://arxiv.org/abs/2504.03703))\n\nMethodological notes for comparability. Results on MIT‑BIH are sensitive to split protocol (intra‑ vs. inter‑patient) and class imbalance; macro‑F1 is more informative than accuracy given rare S/F/Q classes. CAT‑Net explicitly tackles imbalance (SMOTE‑Tomek) and reports strong macro‑F1, while Tiny‑Transformer demonstrates robustness via augmentation against electrode‑motion artifacts. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))\n\nConclusion. For the given data shape (1000, 2), sequence nature, and 5‑class AAMI mapping, the most balanced option is a 1D CNN + Transformer encoder with attention pooling (CAT‑Net style), adapted to two leads. It matches data characteristics, has peer‑reviewed SOTA macro‑F1 on MIT‑BIH, and is straightforward to implement in PyTorch; for ultra‑low‑power deployments, the Tiny‑Transformer is a strong alternative with slightly lower accuracy but orders‑of‑magnitude smaller compute. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))",
  "key_findings": [
    "Hybrid 1D CNN + Transformer architectures deliver state-of-the-art MIT‑BIH 5‑class performance; CAT‑Net reports 99.14% accuracy and 94.69% macro‑F1 with SMOTE‑Tomek class balancing. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))",
    "Extreme model compression is feasible with minor accuracy loss: a ~6k‑parameter Tiny‑Transformer reaches 98.97% 5‑class accuracy and supports 8‑bit, 4.28 ms, 0.09 mJ inference on GAP9. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))",
    "Time–frequency Swin Transformers can achieve ~98–99% accuracy but require 2D spectro‑wavelet pipelines and greater compute than 1D models (99.34% intra‑patient; 98.37% inter‑patient). ([frontiersin.org](https://www.frontiersin.org/journals/cardiovascular-medicine/articles/10.3389/fcvm.2024.1401143/full))"
  ],
  "recommended_approaches": [
    "CAT‑Net style 1D CNN + Transformer encoder + attention pooling (adapted for 2 leads): Use a 1D convolutional stem over shape (B, C=2, L=1000), positional encoding, 1–2 lightweight Transformer encoder blocks (multi‑head self‑attention + MLP), and an attention or global average pooling head. Train with class‑balanced loss (e.g., weighted cross‑entropy or focal) and/or SMOTE‑Tomek as in CAT‑Net; include augmentation (jitter, scaling, time‑masking). This design matches the sequence input, is proven on MIT‑BIH 5‑class with high macro‑F1, and is readily implemented in PyTorch; prefer this for best accuracy/efficiency balance on general hardware. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))"
  ],
  "recent_papers": [
    {
      "title": "CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (2024, Biomedical Signal Processing and Control)",
      "contribution": "CNN + Transformer hybrid with class‑imbalance handling; SOTA on MIT‑BIH 5‑class (99.14% accuracy, 94.69% macro‑F1); code/notebooks available for replication. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))"
    },
    {
      "title": "A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (2024, arXiv)",
      "contribution": "Ultra‑compact (~6k params) Transformer with 8‑bit inference achieving 98.97% MIT‑BIH 5‑class accuracy; 4.28 ms and ~0.09 mJ per inference on GAP9. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))"
    },
    {
      "title": "A novel method of Swin Transformer with time‑frequency characteristics for ECG‑based arrhythmia detection (2024, Frontiers in Cardiovascular Medicine)",
      "contribution": "Wavelet time‑frequency maps + Swin Transformer; 99.34% intra‑patient and 98.37% inter‑patient accuracy on MIT‑BIH; stronger compute demands due to 2D processing. ([frontiersin.org](https://www.frontiersin.org/journals/cardiovascular-medicine/articles/10.3389/fcvm.2024.1401143/full))"
    },
    {
      "title": "A Temporal Transformer‑Based Fusion Framework for Morphological Arrhythmia Classification (2023, MDPI Computers)",
      "contribution": "CNN + Transformer + Bi‑LSTM fusion achieving 99.2% accuracy and macro‑F1 on MIT‑BIH 5‑class; demonstrates benefit of combining local and long‑range temporal modeling. ([mdpi.com](https://www.mdpi.com/2073-431X/12/3/68?utm_source=openai))"
    }
  ],
  "confidence": 0.77,
  "timestamp": 1758298005,
  "generated_time": "2025-09-19 11:06:45",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}