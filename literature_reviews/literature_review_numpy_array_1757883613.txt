LITERATURE REVIEW
=================

Query: sequence classification time series machine learning multiclass sequence classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-14 16:00:13
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
From 2023–2025, multivariate time‑series classification (MTSC) has seen three converging streams: fast convolutional feature transforms (ROCKET family), high‑capacity attention/transformer hybrids tailored for 1D signals, and representation learning via masked/contrastive pretraining. MultiRocket extends MiniROCKET with multiple poolers and first‑order differences to deliver near–state‑of‑the‑art accuracy with very low training cost when paired with a linear classifier, and it natively supports multivariate inputs; many toolkits expose practical defaults (e.g., ≈50k features) that work well out‑of‑the‑box. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-022-00844-1?utm_source=openai)) In parallel, several transformer variants have been designed specifically for MTSC: ShapeFormer combines class‑specific shapelets with a transformer encoder to fuse class‑dependent and generic features and reports top ranks on the 30 UEA datasets (KDD 2024). VSFormer augments self‑attention with value/shape priors to suppress attention to irrelevant regions and shows consistent gains across UEA MTSC. FormerTime and Swin‑style hybrids reduce attention cost via temporal reduction and multi‑scale hierarchies. ([arxiv.org](https://arxiv.org/abs/2405.14608)) A third trend is self‑supervised pretraining: Series2Vec (similarity‑based SSL) and TS‑MAE/Ti‑MAE (masked autoencoding) learn general time‑series embeddings that transfer well to classification—often particularly helpful when labeled sets are small. ([arxiv.org](https://arxiv.org/abs/2312.03998?utm_source=openai))

For a sequence classification problem with arrays shaped (1000, 2) and 5 classes, the most reliable choices balance inductive bias for local motifs with the ability to capture long‑range dependencies. In practice, MultiRocket+LogReg is a very strong, low‑maintenance baseline; InceptionTime/ResNet‑1D style CNNs remain competitive on small‑channel signals; and recent patch‑ or shapelet‑aware transformers (e.g., ShapeFormer/VSFormer) excel when you have enough data and apply patching or temporal pooling to keep O(L^2) attention manageable at L=1000. When labels are scarce, pretrain with Series2Vec or TS‑MAE, then fine‑tune a thin classifier; simple augmentations (time‑warping, jittering/scaling, window slicing) frequently add a few points on UCR/UEA‑like datasets, and recent studies report consistent gains for ROCKET and InceptionTime under augmentation. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-022-00844-1?utm_source=openai))

KEY FINDINGS:
1. Fast convolutional transforms remain state‑of‑the‑art in the accuracy‑vs‑compute trade‑off: MultiRocket features + linear classifier are hard to beat for multivariate signals with small channel counts. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-022-00844-1?utm_source=openai))
2. Transformer designs that inject domain priors (shapelets, value/shape encodings) and reduce sequence length (patching/temporal reduction) now match or exceed strong CNNs on UEA MTSC while controlling quadratic attention cost. ([arxiv.org](https://arxiv.org/abs/2405.14608))
3. Self‑supervised pretraining (Series2Vec, TS‑MAE/Ti‑MAE) improves downstream classification, especially with limited labels, by learning transferable temporal/spectral representations. ([arxiv.org](https://arxiv.org/abs/2312.03998?utm_source=openai))
4. Simple but diverse time‑domain augmentations (jitter, scaling, time‑warping, window slicing) consistently help MTSC; empirical studies show measurable gains for ROCKET and InceptionTime on UCR/UEA‑style datasets. ([arxiv.org](https://arxiv.org/abs/2406.06518?utm_source=openai))
5. For length≈1000 with C=2, 1D CNNs with multi‑scale kernels (e.g., InceptionTime) and global average pooling are reliable; transformers benefit from patch sizes 8–32 and low‑to‑moderate depth (2–4 blocks) to curb memory. ([arxiv.org](https://arxiv.org/abs/1909.04939?utm_source=openai))
6. Graph‑ or MIL‑based variants can localize sparse discriminative segments (e.g., ECG anomalies) and improve interpretability without sacrificing accuracy. ([arxiv.org](https://arxiv.org/abs/2405.03140?utm_source=openai))
7. Expected test accuracy on UEA‑like sensor tasks with similar sequence lengths typically ranges from the high‑70s to mid‑90s depending on class separability and sample size; ROCKET‑family and modern transformers often occupy the upper end when tuned and augmented. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-022-00844-1?utm_source=openai))

RECOMMENDED APPROACHES:
1. {'method': 'MultiRocket features + Logistic/Linear classifier', 'why': 'SOTA‑competitive accuracy with minimal training time; robust on small‑channel MTSC; deterministic or near‑deterministic feature map.', 'how': 'Num features: 20k–60k (common default ≈49,728); include first‑order differences; standardize per channel; L2 regularization (C≈0.1–10). Batch‑compute features in PyTorch (Conv1d) then fit a linear head.', 'refs': ['([link.springer.com](https://link.springer.com/article/10.1007/s10618-022-00844-1?utm_source=openai))']}
2. {'method': 'InceptionTime/ResNet‑1D CNN', 'why': 'Multi‑scale temporal filters with residual connections capture local motifs and are stable on length≈1000.', 'how': '3–6 residual blocks; filters 32–64; kernel sizes {10, 20, 40} (or similar); stride 1; global average pooling; dropout 0.1–0.3; Adam/AdamW lr 1e‑3→1e‑4.', 'refs': ['([arxiv.org](https://arxiv.org/abs/1909.04939?utm_source=openai))']}
3. {'method': 'Shapelet‑aware or value/shape‑aware Transformers (ShapeFormer, VSFormer)', 'why': 'Fuse class‑specific subsequences with global context; reported top‑rank accuracy on UEA MTSC.', 'how': 'Patch or downsample to 250–500 tokens (patch size 2–4 if tokenizing windows or 8–32 if using patch embeddings); d_model 64–192; 2–4 encoder blocks; 4–8 heads; dropout 0.1; AdamW lr 3e‑4, wd 0.01; add CLS token + GAP or MIL pooling.', 'refs': ['([arxiv.org](https://arxiv.org/abs/2405.14608))']}
4. {'method': 'Self‑supervised pretraining + linear probe/fine‑tune (Series2Vec, TS‑MAE/Ti‑MAE)', 'why': 'Improves data efficiency and robustness; beneficial when labeled N is limited or classes are subtle.', 'how': 'Pretrain 50–200 epochs on unlabeled/augmented series; temperature‑scaled similarity or masked reconstruction losses; then fine‑tune a shallow classifier with lower lr (e.g., 3e‑5–1e‑4) and early stopping.', 'refs': ['([arxiv.org](https://arxiv.org/abs/2312.03998?utm_source=openai))']}
5. {'method': 'Time‑aware MIL or graph‑enhanced models for sparse cues', 'why': 'Localizes brief discriminative segments and models inter‑channel relations when signals contain short events.', 'how': 'Tokenize with short windows (e.g., 32–64), MIL pooling with temporal ordering constraints; optional temporal graphs over windows; regularize with sparsity/entropy on attention.', 'refs': ['([arxiv.org](https://arxiv.org/abs/2405.03140?utm_source=openai))']}

RECENT PAPERS:
- ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification (KDD 2024): Introduces class‑specific shapelet discovery with a transformer encoder; achieves highest average rank on 30 UEA MTSC datasets.
- VSFormer: Value and Shape‑Aware Transformer with Prior‑Enhanced Self‑Attention for MTSC (2024): Adds class‑prior‑augmented positional encoding and value/shape pathways, improving attention relevance and accuracy across UEA datasets.
- FormerTime: Hierarchical Multi‑Scale Representations for MTSC (2023): Uses temporal‑reduction attention and contextual positional encoding to cut attention cost and boost MTSC accuracy.
- MultiRocket: Multiple Pooling Operators and Transformations for Fast and Effective TSC (DMKD 2022; widely adopted 2023–2025): Extends MiniROCKET with diverse pooling and first‑order differences; state‑of‑the‑art accuracy‑speed trade‑off; supports multivariate inputs.
- Series2Vec: Similarity‑based Self‑Supervised Representation Learning for Time Series Classification (2023): Learns embeddings by predicting temporal/spectral similarity; strong transfer to classification with limited labels.
- TS‑MAE / Ti‑MAE (2023–2024): Masked autoencoding tailored to time series; improves downstream classification/forecasting via reconstruction of masked segments.
- TimeMIL: Time‑aware Multiple‑Instance Learning for MTSC (2024): Weakly supervised MIL with wavelet positional tokens to localize sparse discriminative patterns; outperforms recent baselines.
- MTS2Graph: Interpretable MTSC with Temporal Evolving Graphs (Pattern Recognition 2024): Builds graphs over highly activated segments to model temporal dependencies; reports state‑of‑the‑art results with interpretability.
- InceptionTime: Finding AlexNet for TSC (2019; still a strong baseline): Multi‑scale 1D CNN ensemble that set a high bar for deep TSC; common architecture/hyperparameter reference.
- Data Augmentation for Multivariate Time Series Classification: An Experimental Study (2024): Shows consistent accuracy gains on UCR/UEA‑style datasets for ROCKET and InceptionTime with diverse augmentations.

==================================================
