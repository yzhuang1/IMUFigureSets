LITERATURE REVIEW
=================

Query: ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-15 18:25:34
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
From 2023–2025, state-of-the-art ECG arrhythmia classifiers on MIT-BIH have converged on four families: (1) lightweight Transformers and CNN–Transformer hybrids tailored to 1D signals; (2) temporal convolutional networks (TCNs) with multi-branch/dilated kernels and attention; (3) state space models (SSMs) such as S4D and Mamba for efficient long-range sequence modeling; and (4) strong 1D CNN backbones (ResNet1D/InceptionTime-style) augmented with rhythm features (e.g., RR intervals). Representative advances include a tiny Transformer achieving near–SOTA accuracy with only ~6k parameters for 5-class MIT-BIH, suitable for embedded deployment; hybrid CNN–Transformer pipelines that explicitly fuse local morphology and global context with inter-patient evaluation; SSM-based models (S4D-ECG, ECGMamba) that match Transformer accuracy while scaling linearly with sequence length; and attention-augmented TCNs that improve minority-class recognition via focal loss and tailored augmentation. Several recent works also explore multimodal/time–frequency fusion (e.g., spectrograms or Gramian Angular Fields), but for your 1D two-lead input (1000×2) the strongest trends favor efficient 1D architectures with explicit handling of RR timing and robust, inter-patient evaluation using the AAMI EC57 5-class mapping. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))

Methodologically, rigorous evaluation is essential: the field increasingly warns that intra-patient splits inflate scores; the community standard is subject-independent DS1→DS2 (De Chazal protocol) under the AAMI 5-class grouping (N/S/V/F/Q). Under strict inter-patient settings, recent hybrid CNN–Transformer and TCN variants report overall accuracy in the high 90s, but macro-F1 is typically limited by scarce S and especially F/Q classes; realistic macro-F1 in the 85–95 range is common, with SVEB (S) notably challenging. Practical pipelines segment around detected R-peaks, include relative heart-rate (RR) context, and balance classes with weighted/focal losses and mix-based augmentations. With your fixed (1000,2) windows at 360 Hz, a compact Conv1D front end for downsampling, followed by either a few Transformer/SSM blocks or a TCN stack, and a class-balanced loss is a strong, efficient choice in PyTorch. ([arxiv.org](https://arxiv.org/html/2503.07276v1?utm_source=openai))

KEY FINDINGS:
1. Prefer inter-patient DS1→DS2 evaluation on MIT-BIH with AAMI EC57 5-class mapping; intra-patient splits overestimate performance. ([arxiv.org](https://arxiv.org/html/2503.07276v1?utm_source=openai))
2. Efficient 1D models with explicit RR-interval context (adaptive beat segmentation, relative heart rate) improve S/V discrimination over morphology-only models. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0010482524011478?utm_source=openai))
3. SSM-based models (S4D/Mamba) provide Transformer-level accuracy with linear complexity, making them attractive for 1000-sample windows and on-device inference. ([link.springer.com](https://link.springer.com/article/10.1007/s13239-024-00716-3?utm_source=openai))
4. Hybrid CNN–Transformer/attention–TCN architectures excel by combining local morphology capture (Conv1D) with global temporal dependencies (attention/SSM). ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38482492/?utm_source=openai))
5. Minority classes (S, F, Q) remain the bottleneck; use class-balanced sampling, weighted cross-entropy or focal loss, and mix-based augmentations to stabilize macro-F1. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/39771858/?utm_source=openai))
6. Reported inter-patient overall accuracy commonly reaches ~97–99% on MIT-BIH, but macro-F1 can be substantially lower; scrutinize splits and metrics. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38482492/?utm_source=openai))
7. MIT-BIH specifics (two channels at 360 Hz; ~110k annotated beats) and DS1/DS2 record partitions should guide fixed-window sizing and channel handling. ([physionet.org](https://physionet.org/content/mitdb/1.0.0/))

RECOMMENDED APPROACHES:
1. {'name': 'Conv1D + Mamba (BiSSM) head', 'description': 'Depthwise-separable Conv1D stem (downsample to L≈250–500), followed by 2–4 bidirectional Mamba/SSM blocks and a global pooling classifier. Yields linear-time long-range modeling with small memory footprint; strong for 1000×2 inputs and real-time use.'}
2. {'name': 'Conv1D + Transformer encoder (lightweight)', 'description': '3–4 Conv1D blocks for feature extraction/stride downsampling, then 2–4 Transformer encoder layers (d_model 128–256, 4–8 heads) with squeeze-and-excitation; balances local morphology and global context; proven on inter-patient MIT-BIH.'}
3. {'name': 'Attention-augmented TCN (MB/Multiscale + focal loss)', 'description': 'Multi-branch dilated Conv1D stacks with residual TCN blocks and a light self-attention fusion. Use focal loss (γ=1–2) and minority-aware augmentation to boost S/F classes; competitive and compute-efficient.'}
4. {'name': 'ResNet1D/InceptionTime + RR features', 'description': 'Strong 1D CNN backbone (3–6 residual blocks, multiscale kernels) concatenated with engineered RR-interval features (relative heart rate of target beat vs neighbors) before the classifier; improves PAC/SVEB recall.'}
5. {'name': 'Hybrid 1D + time–frequency (optional)', 'description': 'Fuse a primary 1D stream with a lightweight 2D head on STFT scalograms or GAF images via late fusion; can add robustness but increases complexity—use if compute allows.'}

RECENT PAPERS:
- A Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers (2024): Demonstrates a ~6k-parameter Transformer achieving ~99% 5-class MIT-BIH accuracy with 8-bit inference for embedded devices; shows viability of compact attention models.
- Heartbeat classification combining multi-branch CNNs and Transformer (2024): Hybrid CNN–Transformer with intra- and inter-patient protocols; reports 97–99% overall accuracy with stronger inter-patient results than prior CNN-only baselines.
- ECGMamba: Towards Efficient ECG Classification with BiSSM (2024): Introduces a bidirectional Mamba/SSM backbone for ECG; achieves competitive accuracy with linear-time inference, highlighting SSMs as efficient alternatives to Transformers.
- S4D-ECG: A Shallow State-of-the-Art Model for Cardiac Abnormality Classification (2024): Applies diagonal S4 (S4D) layers to ECG; robust to noise with low complexity; shows SSMs’ practicality for ECG classification.
- Accurate Arrhythmia Classification with Multi-Branch, Multi-Head Attention TCN (2024): TCN with multi-branch kernels and attention plus focal loss; improves minority-class recognition on MIT-BIH.
- ECG classification via adaptive beat segmentation and relative heart rate with deep networks (2024): Adds adaptive beat windows and relative heart-rate context; markedly improves PAC/SVEB detection across datasets including MIT-BIH.
- CAT-Net: Convolution, Attention, and Transformer for single-lead ECG (2024): Single-lead CNN–Transformer with SMOTE-Tomek balancing; reports SOTA 5-class MIT-BIH accuracy and high macro-F1.
- A Systematic Review of ECG Arrhythmia Classification: Adherence to Standards, Fair Evaluation, and Embedded Feasibility (2025): Highlights pitfalls of intra-patient splits; recommends DS1→DS2 inter-patient protocols and standardized reporting.

==================================================
