{
  "query": "ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Problem focus: Multiclass ECG arrhythmia sequence classification on MIT‑BIH (AAMI 5 classes: N, S, V, F, Q) with 2 input leads and fixed windows (e.g., 1000 samples ≈ 2.78 s at 360 Hz). The MIT‑BIH Arrhythmia Database contains 48 two‑lead, 30‑minute records sampled at 360 Hz with expert beat annotations; it is the de‑facto benchmark for heartbeat classification. For fair evaluation, inter‑patient splits (DS1→DS2) proposed by De Chazal are widely used to avoid patient overlap. ([physionet.org](https://physionet.org/content/mitdb/1.0.0/x_mitdb/?utm_source=openai))\nRecent methods and evidence (2024–2025): (1) CAT‑Net (CNN + channel attention + Transformer encoder) achieved 99.14% overall accuracy and 94.69% macro‑F1 on 5‑class MIT‑BIH, using wavelet denoising, beat segmentation, and SMOTE‑Tomek to address class imbalance; the paper emphasizes single‑lead inputs and does not explicitly state DS1/DS2 inter‑patient evaluation. Open‑source notebooks are available for replication. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai)) (2) A tiny ViT‑style Transformer for arrhythmia classification targets edge deployment: ~6k parameters, 0.97 MOP/inference, 8‑bit quantized accuracy 98.97% on 5‑class MIT‑BIH, and 4.28 ms / 0.09 mJ per inference on GAP9; it processes ~0.5 s windows around R‑peaks and concatenates RR‑interval tokens. The authors note their evaluation is intra‑patient, so inter‑patient performance may be lower. ([ar5iv.org](https://ar5iv.org/pdf/2402.10748)) (3) A sequence‑to‑sequence CNN+RNN model rigorously evaluated on inter‑patient DS1→DS2 reports 99.53% accuracy overall; for class S it achieves PPV 92.57% and SEN 88.94%, and for class V PPV 99.50% and SEN 99.94%; model size is ≤5.5 MB—useful as a strong inter‑patient baseline. ([ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/1812.07421v2)) (4) A 2024 multimodal CNN with adaptive attention evaluated both intra‑ and inter‑patient and showed that combining MLII with V lead (dual‑lead) improves DS2 results compared with MLII alone, supporting two‑lead inputs like (1000,2). ([mdpi.com](https://www.mdpi.com/2076-3417/14/20/9307)) (5) Beyond single‑dataset training, source‑free domain adaptation on MIT‑BIH achieved 98.4% accuracy (DS1→DS2), signaling that adaptation/regularization helps generalization under inter‑patient shifts. ([mdpi.com](https://www.mdpi.com/2076-3417/13/14/8551?utm_source=openai))\nStandards and survey guidance (2024–2025): A 2025 systematic review stresses that many studies overstate results by using intra‑patient or non‑AAMI mappings; it recommends inter‑patient protocols and embedded feasibility reporting (parameters, memory, latency). A 2024 survey compiles MIT‑BIH‑based methods and highlights persistent S/V/F class imbalance and protocol inconsistencies. These inform our emphasis on DS1→DS2, class‑imbalance handling, and compute constraints. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))\nSynthesis: For windows of length 1000 with two leads, hybrid CNN‑Transformer models are a good match (CNN downsamples/mixes channels; attention captures beat‑to‑beat context). CAT‑Net’s design and empirical strength make it a strong starting point; adapt it to two‑lead inputs and train/test under DS1→DS2 with AAMI mapping. For deployment‑constrained settings, the tiny Transformer offers excellent efficiency; however, its intra‑patient training and single‑lead, shorter windows require protocol and input adaptations. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))",
  "key_findings": [
    "Hybrid CNN + attention + Transformer (CAT‑Net) attained 99.14% accuracy and 94.69% macro‑F1 on 5‑class MIT‑BIH using wavelet denoising, beat segmentation, and SMOTE‑Tomek; code notebooks are available for replication. Caveat: evaluation protocol is not explicitly DS1→DS2. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))",
    "Tiny ViT‑style model for edge devices achieved 98.97% (8‑bit) on 5‑class MIT‑BIH with ~6k parameters and 0.09 mJ per inference on GAP9; inputs are ~0.5 s (198‑sample) beat‑centered windows plus RR‑interval tokens; evaluation is intra‑patient. ([ar5iv.org](https://ar5iv.org/pdf/2402.10748))",
    "A Seq2Seq CNN+RNN inter‑patient baseline (DS1→DS2) reported 99.53% overall accuracy; S: PPV 92.57%, SEN 88.94%; V: PPV 99.50%, SEN 99.94%; model size ≤5.5 MB—demonstrating high DS2 performance with modest memory. ([ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/1812.07421v2))",
    "Dual‑lead inputs (MLII + V) with a multimodal CNN and adaptive attention improved inter‑patient DS2 results versus MLII alone, supporting the use of two channels like (1000,2). ([mdpi.com](https://www.mdpi.com/2076-3417/14/20/9307))",
    "Source‑free domain adaptation (SF‑ECG) on MIT‑BIH reports 98.4% accuracy (DS1→DS2) and 96.5% (DS2→DS1), indicating adaptation can mitigate inter‑patient distribution shifts. ([mdpi.com](https://www.mdpi.com/2076-3417/13/14/8551?utm_source=openai))"
  ],
  "recommended_approaches": [
    "CAT‑Net (CNN + channel attention + Transformer encoder), adapted for dual‑lead 1D inputs: (a) input 1000×2 windows centered on the target beat (or sliding with center‑beat labels), (b) 1D convolutions with residual/channel‑attention blocks to fuse both leads and reduce sequence length, (c) 1–2 lightweight Transformer encoder layers for long‑range temporal context, (d) classifier head for 5 AAMI classes with class‑balanced (e.g., focal) loss; train with DS1→DS2 inter‑patient protocol and SMOTE‑Tomek or class‑balanced sampling. This choice balances top reported accuracy/macrof‑F1 on MIT‑BIH, straightforward PyTorch implementation, and moderate compute; extend to two leads per evidence that MLII+V improves inter‑patient results. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))"
  ],
  "recent_papers": [
    {
      "title": "CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (Biomedical Signal Processing and Control, 2024)",
      "contribution": "Hybrid CNN+Transformer with SMOTE‑Tomek; 5‑class MIT‑BIH accuracy 99.14% and macro‑F1 94.69%; provides practical pipeline and open notebooks. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))"
    },
    {
      "title": "A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (T‑BCAS, 2024)",
      "contribution": "ViT‑style beat‑window model with RR‑interval tokens; ~6k params, 0.97 MOP/inference, 98.97% 8‑bit accuracy; 4.28 ms and 0.09 mJ on GAP9; highlights embedded feasibility (intra‑patient). ([ar5iv.org](https://ar5iv.org/pdf/2402.10748))"
    },
    {
      "title": "Inter‑ and Intra‑Patient ECG Heartbeat Classification: A Sequence‑to‑Sequence Deep Learning Approach (arXiv/IEEE, 2018–2020)",
      "contribution": "Strong inter‑patient DS1→DS2 results (ACC 99.53%; S PPV 92.57%, SEN 88.94; V PPV 99.50%, SEN 99.94) with ≤5.5 MB model, establishing a rigorous cross‑patient baseline. ([ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/1812.07421v2))"
    },
    {
      "title": "Intra‑ and Interpatient ECG Heartbeat Classification Based on Multimodal CNNs with Adaptive Attention (Applied Sciences, 2024)",
      "contribution": "Demonstrates inter‑patient evaluation and that MLII+V (dual‑lead) improves performance versus single‑lead; uses HSFC + RP image transforms with attention. ([mdpi.com](https://www.mdpi.com/2076-3417/14/20/9307))"
    },
    {
      "title": "A Systematic Review of ECG Arrhythmia Classification: Adherence to Standards, Fair Evaluation, and Embedded Feasibility (arXiv, 2025)",
      "contribution": "Survey (2017–2024) emphasizing AAMI mapping, DS1→DS2 inter‑patient evaluation, and reporting compute/embedded metrics for real‑world utility. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))"
    }
  ],
  "confidence": 0.78,
  "timestamp": 1758585036,
  "generated_time": "2025-09-22 18:50:36",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}