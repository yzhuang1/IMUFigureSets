{
  "query": "Stress Level tabular data classification multiclass classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Problem context: You have a non-sequential, tabular input (torch_tensor shape = (8,)) for 4-class stress level classification derived from the WESAD dataset. While WESAD is inherently multivariate time series (15 subjects; wrist/chest sensors; typical 3 classes), many studies aggregate windows to tabular features and report multiclass results, including a 4-class variant that adds meditation. Thus, state-of-the-art tabular learners are appropriate if you work with engineered features rather than raw sequences. ([archive.ics.uci.edu](https://archive.ics.uci.edu/ml/datasets/WESAD%2BWearable%2BStress%2Band%2BAffect%2BDetection?utm_source=openai))\nRecent tabular SOTA: The modern deep-learning landscape on tabular data has shifted toward simple but strong MLP-style models and select Transformer variants. FT-Transformer (NeurIPS 2021) established a strong deep baseline, but more recent work shows that properly tuned MLPs or parameter-efficient ensembles can match or exceed it with better efficiency. In 2024, RealMLP demonstrated competitive performance with GBDTs on 1K–500K-sample tables using robust default hyperparameters, improving the time–accuracy trade-off. In 2025, TabM (ICLR) introduced parameter-efficient ensembling (BatchEnsemble-style) for MLPs, outperforming prior deep tabular models across public benchmarks while scaling to millions of rows (13M+ reported) with practical training and inference efficiency; code is PyTorch. These findings are echoed in 2024–2025 surveys benchmarking deep models for tabular data. ([proceedings.neurips.cc](https://proceedings.neurips.cc/paper/2021/hash/9d86d83f925f2149e9edb0ac3b49229c-Abstract.html?utm_source=openai))\nDomain evidence (stress/WESAD with tabular features): Classical ML and stacking on engineered physiological features (e.g., EDA-focused) achieve very high accuracy, illustrating that tabular feature pipelines can be competitive: a feature-selection + stacking model on WESAD chest EDA reported ~99.7% accuracy; personalized 4-class WESAD models (neutral, stress, amusement, meditation) using random forests achieved 88–99% across subjects; context-aware fusion with tabular features on WESAD reported 86.3% (3-class, wrist). These support the viability of strong tabular learners when features are well designed, while also highlighting the effect of personalization. ([mdpi.com](https://www.mdpi.com/1424-8220/23/15/6664?utm_source=openai))\nEvaluation considerations: WESAD studies emphasize that subject-specific (personalized) models often outperform generalized ones; leave-one-subject-out (LOSO) evaluation is recommended to assess generalization across users. For multiclass setups (>2 classes), meta-analyses show accuracy can remain high but depends on sensing modality and label ground truth. When using tabular models, ensure stratified splits, LOSO if applicable, class-imbalance handling, and calibration. ([ai.jmir.org](https://ai.jmir.org/2024/1/e52171/?utm_source=openai))\nComputational requirements: FT-Transformer is generally slower than MLPs. RealMLP and especially TabM retain MLP-like efficiency; TabM demonstrates favorable train/infer efficiency and scalability (13M+ objects reported), with open-source PyTorch code. These properties fit large tabular datasets with few features like yours. ([github.com](https://github.com/yandex-research/rtdl-revisiting-models?utm_source=openai))\nBottom line: For an 8-feature, 4-class, non-sequence task from WESAD-style features, a modern MLP-based tabular model with parameter-efficient ensembling (TabM) offers the best balance of accuracy, scalability, and PyTorch implementability, while aligning with evidence that well-crafted tabular pipelines can reach state-of-the-art performance in this domain.",
  "key_findings": [
    "MLP-style deep tabular models are competitive or superior to many complex architectures; FT-Transformer set a strong deep baseline but is slower than MLPs. ([proceedings.neurips.cc](https://proceedings.neurips.cc/paper/2021/hash/9d86d83f925f2149e9edb0ac3b49229c-Abstract.html?utm_source=openai))",
    "RealMLP (NeurIPS 2024) shows competitive performance with GBDTs on 1K–500K-sample tables with strong default hyperparameters, offering an excellent time–accuracy trade-off. ([papers.nips.cc](https://papers.nips.cc/paper/2024/hash/2ee1c87245956e3eaa71aaba5f5753eb-Abstract-Conference.html?utm_source=openai))",
    "TabM (ICLR 2025) uses parameter-efficient ensembling to outperform prior deep tabular models on public benchmarks and scales to very large datasets (13M+ samples), with an open-source PyTorch implementation. ([proceedings.iclr.cc](https://proceedings.iclr.cc/paper_files/paper/2025/hash/c1ba41c694834aeef91ae161711d4939-Abstract-Conference.html?utm_source=openai))",
    "On WESAD-derived tabular features, classical/tabular approaches report strong results: feature-selection + stacking with chest EDA reached ~99.7% accuracy; personalized 4-class models achieved 88–99%; context-aware sensor-fusion reported 86.3% (3-class wrist), supporting the effectiveness of tabular pipelines. ([mdpi.com](https://www.mdpi.com/1424-8220/23/15/6664?utm_source=openai))",
    "Personalization and evaluation protocol matter: personalized models and LOSO evaluation often yield higher and more realistic performance; meta-analyses highlight modality and labeling as key moderators of accuracy. ([ai.jmir.org](https://ai.jmir.org/2024/1/e52171/?utm_source=openai))"
  ],
  "recommended_approaches": [
    "TabM (PyTorch MLP with parameter-efficient ensembling via BatchEnsemble): Use for the (8,) numeric tabular input. Rationale: state-of-the-art deep tabular performance with strong efficiency and scalability; straightforward PyTorch training; well-suited to small-to-moderate feature counts. Suggested recipe: standardize features; handle class imbalance (class weights or focal loss); 3–5 MLP blocks (width 128–256), BatchEnsemble rank ~4–8, dropout 0.1–0.2; cosine LR with warmup; early stopping on macro-F1; evaluate with LOSO if subject splits are available. ([proceedings.iclr.cc](https://proceedings.iclr.cc/paper_files/paper/2025/hash/c1ba41c694834aeef91ae161711d4939-Abstract-Conference.html?utm_source=openai))"
  ],
  "recent_papers": [
    {
      "title": "TabM: Advancing tabular deep learning with parameter-efficient ensembling (ICLR 2025)",
      "contribution": "Introduces TabM, an MLP + BatchEnsemble approach that outperforms prior deep tabular models on public benchmarks and scales to multimillion-row datasets; open-source PyTorch implementation available. ([proceedings.iclr.cc](https://proceedings.iclr.cc/paper_files/paper/2025/hash/c1ba41c694834aeef91ae161711d4939-Abstract-Conference.html?utm_source=openai))"
    },
    {
      "title": "Better by default: Strong pre-tuned MLPs and boosted trees on tabular data (NeurIPS 2024)",
      "contribution": "Proposes RealMLP with robust default hyperparameters; competitive with GBDTs on 1K–500K samples and favorable time–accuracy trade-off, reducing tuning burden. ([papers.nips.cc](https://papers.nips.cc/paper/2024/hash/2ee1c87245956e3eaa71aaba5f5753eb-Abstract-Conference.html?utm_source=openai))"
    },
    {
      "title": "Revisiting Deep Learning Models for Tabular Data (NeurIPS 2021)",
      "contribution": "Establishes FT-Transformer and strong MLP/ResNet baselines for tabular data; notes FT-Transformer’s strong average deep performance but slower training vs MLPs. ([proceedings.neurips.cc](https://proceedings.neurips.cc/paper/2021/hash/9d86d83f925f2149e9edb0ac3b49229c-Abstract.html?utm_source=openai))"
    },
    {
      "title": "Efficient Feature-Selection-Based Stacking Model for Stress Detection (Sensors 2023)",
      "contribution": "Tabular feature engineering (chest EDA) + stacking achieves ~99.7% accuracy on WESAD, indicating the strength of feature-based tabular pipelines for stress detection. ([mdpi.com](https://www.mdpi.com/1424-8220/23/15/6664?utm_source=openai))"
    },
    {
      "title": "Stress Detection using Context-Aware Sensor Fusion from Wearable Devices (2023)",
      "contribution": "Context-aware selective fusion using WESAD tabular features; reports 86.34% (3-class, wrist), highlighting modality and noise-context effects. ([arxiv.org](https://arxiv.org/abs/2303.08215?utm_source=openai))"
    },
    {
      "title": "Stress Detection from Multimodal Wearable Sensor Data (IOP 2020)",
      "contribution": "Personalized 4-class (neutral, stress, amusement, meditation) WESAD classification with Random Forest achieves 88–99% across subjects; evidence for the 4-class tabular setting. ([researchgate.net](https://www.researchgate.net/publication/340034752_Stress_Detection_from_Multimodal_Wearable_Sensor_Data?utm_source=openai))"
    }
  ],
  "confidence": 0.78,
  "timestamp": 1760415402,
  "generated_time": "2025-10-13 23:16:42",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      31470603,
      8
    ],
    "dtype": "float32",
    "feature_count": 8,
    "sample_count": 31470603,
    "is_sequence": false,
    "is_image": false,
    "is_tabular": true,
    "has_labels": true,
    "label_count": 4,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}