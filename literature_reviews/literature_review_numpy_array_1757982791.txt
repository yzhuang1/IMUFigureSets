LITERATURE REVIEW
=================

Query: ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-16 00:33:11
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
From 2023 to 2025, ECG arrhythmia classification on MIT-BIH has been driven by hybrids that combine localized convolutional feature extractors with global sequence modeling (Transformers/attention) and by stronger temporal CNNs with dilation and attention. Representative examples include CAT-Net, a single‑lead CNN+Transformer hybrid that explicitly tackles the AAMI 5‑class imbalance with SMOTE‑Tomek and reports new benchmarks on MIT‑BIH (99.14% accuracy; 94.69% macro‑F1), and MB‑MHA‑TCN, which fuses multi‑branch dilated temporal convolutions with multi‑head attention and focal loss to improve minority‑class recognition (98.75% accuracy; 96.89% F1). Compact models aimed at deployment also matured: a 6k‑parameter “Tiny Transformer” achieves 98.97% 5‑class accuracy on MIT‑BIH with 8‑bit inference on GAP9, highlighting efficient patching and augmentation to preserve accuracy under quantization. Alongside supervised models, self‑supervised and “foundation” ECG encoders (e.g., HuBERT‑ECG; JEPA‑style pretraining) showed that large unlabeled ECG corpora produce transferable representations that fine‑tune well on downstream datasets such as PTB‑XL and, by extension, beat‑level MIT‑BIH tasks when appropriately adapted. Interpretable directions (e.g., prototype‑based networks) and multimodal/time‑frequency variants (e.g., RP/GAF/MTF images with attention) have also gained traction, though their reported gains often depend on split protocol and class rebalancing. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))

For your problem (numpy_array of shape [1000, 2], 5 classes, sequence data from MIT‑BIH), the most reliable pipelines segment heartbeats around R‑peaks, map annotations to AAMI superclasses (N, S, V, F, Q), and evaluate with the inter‑patient DS1/DS2 split recommended by de Chazal and AAMI EC57 to avoid patient leakage. With windows of ~1000 samples at 360 Hz and two leads, 1D CNN front‑ends (kernels 7–17, dilations 1–8) feeding lightweight Transformer or TCN blocks tend to be the best balance of accuracy and efficiency; class imbalance is addressed with focal loss or cost‑sensitive weighting, and sometimes SMOTE‑Tomek on the training set. Expect strong overall accuracy (≈95–99%) under inter‑patient evaluation, with macro‑F1 varying more widely (≈85–95%) depending on minority‑class prevalence and preprocessing. Always cite and use the official PhysioNet MIT‑BIH resource and adhere to AAMI evaluation conventions (exclude pacemaker records for the 5‑class protocol). ([physionet.org](https://physionet.org/content/mitdb/1.0.0/))

KEY FINDINGS:
1. Use inter‑patient DS1→DS2 evaluation (AAMI/De Chazal) to avoid optimistic leakage; report classwise and macro‑F1, not just overall accuracy. Exclude paced‑beat records when following the 5‑class AAMI protocol. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC10569428/?utm_source=openai))
2. Hybrid CNN+attention/Transformer or attention‑augmented TCNs provide consistent gains over plain CNN/LSTM baselines on MIT‑BIH, especially for minority classes. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))
3. Compact Transformers with patch embeddings plus quantization‑aware training can retain >98% accuracy and are deployable on microcontrollers. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
4. Self‑supervised/foundation ECG encoders (e.g., HuBERT‑ECG; JEPA pretraining) improve downstream performance and data efficiency; consider pretraining on large multi‑dataset ECG and fine‑tuning for MIT‑BIH. ([medrxiv.org](https://www.medrxiv.org/content/10.1101/2024.11.14.24317328v1.full?utm_source=openai))
5. Class imbalance is the main failure mode; focal loss, reweighting, and (if used) SMOTE‑Tomek on training beats help, but must be paired with proper patient‑wise splitting. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))
6. Time‑frequency or image transforms (RP/GAF/MTF) with attention can help certain classes but add complexity; ensure fair comparisons under the same split. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38627498/?utm_source=openai))
7. Best practice preprocessing: bandpass/wavelet denoising, baseline wander removal, robust R‑peak detection, heartbeat alignment to R, and consistent resampling; verify signal specs against PhysioNet metadata. ([physionet.org](https://physionet.org/content/mitdb/1.0.0/))

RECOMMENDED APPROACHES:
1. {'name': 'CNN + lightweight Transformer (CAT‑style)', 'description': '1D CNN stem (3–5 blocks; kernels 7–17; dilations 1–4) for local morphology, then 2–4 Transformer encoder layers (d_model 128–256; 4–8 heads; dropout 0.1–0.3) over patch‑embedded sequences (patch 8–16) to capture long‑range rhythm context; class‑weighted or focal loss; optional SMOTE‑Tomek on training set. Strong macro‑F1 under AAMI 5‑class. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))'}
2. {'name': 'Multi‑branch Attention TCN (MB‑MHA‑TCN)', 'description': 'Three temporal branches with different kernels and dilations feed a TCN stack plus multi‑head attention fusion; use focal loss and targeted augmentation (jitter, scaling, time‑warp). Excels at minority classes; efficient for (1000,2) inputs. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC11679161/?utm_source=openai))'}
3. {'name': 'Tiny/Edge Transformer', 'description': 'Patch embedding + 1–3 tiny Transformer blocks (≤100k params; optionally ≤10k) with QAT or 8‑bit PTQ for deployment; retains ≥98% accuracy on MIT‑BIH 5‑class with microcontroller‑level latency/energy. Suitable if you must ship to wearables. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))'}
4. {'name': 'SSL/Foundational pretrain then fine‑tune', 'description': 'Pretrain an ECG encoder via HuBERT/JEPA/contrastive (e.g., CLOCS‑style) on large unlabeled ECG sets; fine‑tune a linear/MLP head on MIT‑BIH beats. Yields better data efficiency and robust features; particularly useful if labels are limited or noisy. ([medrxiv.org](https://www.medrxiv.org/content/10.1101/2024.11.14.24317328v1.full?utm_source=openai))'}
5. {'name': 'Multimodal time‑frequency fusion (optional)', 'description': 'Augment 1D models with RP/GAF/MTF 2D projections plus channel/frequency attention to boost difficult classes; compare carefully under identical inter‑patient split. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38627498/?utm_source=openai))'}

RECENT PAPERS:
- CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (2024): No description
- Accurate Arrhythmia Classification with Multi‑Branch, Multi‑Head Attention Temporal Convolutional Networks (2024): No description
- A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (2024): No description
- HuBERT‑ECG: a self‑supervised foundation model for broad and scalable cardiac applications (2024, medRxiv + code 2024–2025): No description
- Self‑Supervised Pre‑Training with Joint‑Embedding Predictive Architecture Boosts ECG Classification Performance (2024): No description
- HARDC: Hierarchical attention dual‑RNN with dilated CNN for arrhythmia classification (2023): No description
- ECGformer: Leveraging Transformer for ECG heartbeat arrhythmia classification (2023, CSCI + code): No description
- Multimodal ECG heartbeat classification with FCA (2024): No description
- ProtoECGNet: Case‑based interpretable deep learning with contrastive prototype learning (2025): No description
- MIT‑BIH Arrhythmia Database (PhysioNet, v1.0.0; canonical reference): No description

==================================================
