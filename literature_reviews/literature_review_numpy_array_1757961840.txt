LITERATURE REVIEW
=================

Query: ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-15 18:44:00
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
From 2023–2025, ECG arrhythmia classification has been shaped by three trends: transformer-style global context modeling, state-space models (SSMs) for long-range temporal dependencies, and self-supervised pretraining to reduce label dependence. Compact transformer variants and hybrids have reported strong 5-class AAMI results on MIT-BIH, often with small parameter counts suited for embedded inference; e.g., a 6K‑parameter tiny transformer reached ~99% accuracy on 5 AAMI classes and demonstrated efficient 8‑bit deployment, while CAT‑Net (CNN+attention+transformer) achieved state-of-the-art accuracy and macro‑F1 on MIT-BIH with targeted handling of class imbalance. Meanwhile, SSMs such as S4D and Mamba-inspired designs offer linear‑time sequence modeling with good noise robustness and hardware friendliness. In parallel, self‑supervised learning (SSL) has matured: recent frameworks like NERULA combine masked reconstruction with non‑contrastive objectives, improving downstream arrhythmia classification robustness under corruption and limited labels. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))

For MIT‑BIH (48 half‑hour, 2‑lead, 360 Hz recordings; ~110K annotated beats), rigorous evaluation remains crucial. AAMI grouping into 5 superclasses (N, S, V, F, Q) and inter‑patient protocols (e.g., de Chazal DS1→DS2 split) substantially change reported performance; realistic inter‑patient 5‑class results historically cluster near 90–95% accuracy with weaker minority‑class F/Q metrics, while many near‑ceiling results in the literature stem from less strict splits or data leakage. Recent systematic reviews note that only a subset of studies follow AAMI and inter‑patient practices, complicating SOTA claims. For your setup—numpy arrays of shape (1000,2) per segment (~2.78 s at 360 Hz) and 5 classes—modern 1D CNN+Transformer/SSM backbones, possibly SSL‑pretrained and class‑balanced, are the most reliable choices for cross‑patient generalization. ([physionet.org](https://www.physionet.org/physiobank/database/mitdb/?utm_source=openai))

KEY FINDINGS:
1. Use AAMI 5‑class mapping (N, S, V, F, Q) and inter‑patient splits (e.g., de Chazal DS1→DS2) to avoid overly optimistic results; exclude paced‑beat records if following AAMI conventions. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC5585360/?utm_source=openai))
2. Expect wide performance ranges: inter‑patient, 5‑class MIT‑BIH accuracy often ~90–95% with weaker F/Q; higher numbers frequently reflect intra‑patient settings or leakage. ([onlinelibrary.wiley.com](https://onlinelibrary.wiley.com/doi/10.1155/2021/6648432?utm_source=openai))
3. Compact transformers and hybrids (CNN+attention/Transformer) are strong on beat/window tasks and can be quantized for edge inference without major accuracy loss. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
4. State‑space models (S4D/Mamba‑style) provide efficient long‑sequence modeling with good noise robustness and low complexity—useful for 1000‑sample, 2‑lead inputs. ([link.springer.com](https://link.springer.com/article/10.1007/s13239-024-00716-3?utm_source=openai))
5. Self‑supervised pretraining (masked reconstruction + contrastive/non‑contrastive) improves robustness and label efficiency for ECG downstream tasks. ([arxiv.org](https://arxiv.org/abs/2405.19348?utm_source=openai))
6. Data augmentation matters: physiologically plausible time/magnitude warping, jitter, scaling, masking, and GAN/VAE syntheses can mitigate imbalance; prefer class‑balanced loss over naive SMOTE on sequences. ([mdpi.com](https://www.mdpi.com/1424-8220/23/11/5237?utm_source=openai))
7. MIT‑BIH specifics: 2 leads, 360 Hz sampling, ~30‑minute records; a 1000‑sample window spans ~2.78 s—suitable for window‑level models with small downsampling or tokenization. ([physionet.org](https://www.physionet.org/physiobank/database/mitdb/?utm_source=openai))

RECOMMENDED APPROACHES:
1. {'name': 'CNN + Transformer Encoder (e.g., CAT‑Net‑style)', 'why': 'Convolutions capture beat morphology; MHSA layers capture rhythm context across the 1000‑step window; works well with class‑imbalance remedies.', 'typical_setup': 'Input (B,2,1000); 3–5 Conv1D blocks (kernel 7–15, 32–128 ch, stride 1–2) → 2–4 Transformer encoder layers (d_model 64–256, 2–4 heads, dropout 0.1–0.3) → pooled MLP; AdamW (1e‑3 to 3e‑4), cosine decay, 50–150 epochs.', 'notes': 'Consider quantization‑aware training or INT8 post‑training for deployment. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))'}
2. {'name': 'State‑Space Model Backbone (S4D / Mamba‑style)', 'why': 'Linear‑time sequence modeling handles long ECG sequences efficiently; good noise robustness and small footprint.', 'typical_setup': '4–8 SSM blocks (hidden 64–128), Conv1D stem and projection, layer norm, dropout 0.1; AdamW 3e‑4; optional spectro‑temporal tokens.', 'notes': 'Useful when compute/memory are tight or for on‑device use. ([link.springer.com](https://link.springer.com/article/10.1007/s13239-024-00716-3?utm_source=openai))'}
3. {'name': 'ResNet1D / Inception‑style CNN Baseline', 'why': 'Strong, stable baselines for 1D time series; excellent for limited data and fast iteration.', 'typical_setup': '6–12 residual blocks (32–128 ch), mixed kernel sizes (3–41), global average pooling; Adam 1e‑3; early stopping; class‑balanced/focal loss.', 'notes': 'Pair with rigorous inter‑patient split and augmentation. (General baseline; see reviews.)'}
4. {'name': 'SSL‑pretrained Encoder + Lightweight Classifier', 'why': 'Improves representation quality under limited labels and noisy windows.', 'typical_setup': 'Pretrain with masking + non‑contrastive objective on unlabeled ECG; fine‑tune small MLP head on 5 classes; heavy augmentations during pretrain, lighter in finetune.', 'notes': 'Particularly helpful for minority classes and cross‑dataset transfer. ([arxiv.org](https://arxiv.org/abs/2405.19348?utm_source=openai))'}
5. {'name': 'Tiny/Embedded Transformer for deployment', 'why': 'Maintains high accuracy with very few parameters and supports INT8 inference on MCUs.', 'typical_setup': '6–20 attention heads total ≤6–50K params; 8‑bit quantization; sliding‑window or stream inference.', 'notes': 'Use for wearables/edge scenarios after training on full precision. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))'}

RECENT PAPERS:
- CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (2024): Hybrid CNN+Transformer with class‑imbalance handling (SMOTE‑Tomek); reports SOTA 5‑class MIT‑BIH performance with strong macro‑F1.
- A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (2024): 6K‑parameter transformer achieving ~99% 5‑class accuracy on MIT‑BIH with 8‑bit inference on GAP9 MCU; demonstrates viability for edge AI.
- S4D‑ECG: A shallow state‑of‑the‑art model for cardiac abnormality classification (2024): Applies diagonal state‑space sequence modeling to ECG, showing noise‑robust performance with low complexity.
- ECGMamba: Towards Efficient ECG Classification with BiSSM (2024): Introduces a bidirectional SSM for efficient ECG classification, balancing accuracy and inference efficiency.
- NERULA: Dual‑pathway self‑supervised learning for ECG (2024): Combines masked reconstruction and non‑contrastive alignment; improves ECG representations for arrhythmia classification and robustness.
- Automated inter‑patient arrhythmia classification with dual attention neural network (2023): Reports very high inter‑patient 5‑class scores on MIT‑BIH; highlights sensitivity of results to protocol details.
- Systematic review of ECG arrhythmia classification: adherence to standards, fair evaluation, and embedded feasibility (2025): Finds limited adherence to AAMI and inter‑patient protocols across 122 papers; underscores evaluation pitfalls.
- MIT‑BIH Arrhythmia Database (PhysioNet): Canonical dataset description (2 leads, 360 Hz, 48 records, ~110K annotated beats).

==================================================
