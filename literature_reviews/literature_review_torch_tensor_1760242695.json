{
  "query": "EEG classification brain signal analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Problem and data: The task is 5-class sequence classification (W, N1, N2, N3, REM) on ISRUC-Sleep with 6 EEG channels (F3–M2, C3–M2, O1–M2, F4–M1, C4–M1, O2–M1). Typical practice is 30 s epochs, often resampled to 100 Hz (yielding 6×3000 tensors), though your 6×6000 input suggests 200 Hz; both are common and supported by public tooling. ISRUC comprises three subgroups (100 single-session clinical subjects; 8 subjects with two sessions; 10 healthy controls). Labels follow AASM 5-stage scoring. Public loaders (e.g., TorchEEG) expose the six EEG channels and a W/N1/N2/N3/R mapping and commonly resample to 100 Hz. Class imbalance is notable (e.g., example splits show N1 under-represented), which impacts macro-F1 and requires imbalance-aware training. ([sleeptight.isr.uc.pt](https://sleeptight.isr.uc.pt/))\n\nRecent literature (2024–2025) and results most relevant to multi-channel EEG and ISRUC:\n- ST-USleepNet (2024, updated 2025 IJCAI-25): a raw-signal framework that builds a spatial–temporal graph from multi-channel PSG and feeds two coupled U-Nets (temporal 1D U-Net and graph U-Net) with a fusion head. Authors report SOTA across multiple datasets; official PyTorch code is available and prescribes 100 Hz resampling. This architecture directly matches 6-channel EEG and emphasizes spatial–temporal coupling. ([arxiv.org](https://arxiv.org/abs/2408.11884))\n- MSF-SleepNet (BMC Medical Informatics and Decision Making, 2025): multi-stream fusion network with interpretability. On ISRUC-S1, reports Accuracy 0.826, macro-F1 0.809, Kappa 0.774, outperforming U-Net, STGCN, MSTGCN in the same table. Note: uses multimodal inputs in some configs, but the table benchmarks reflect EEG-centric pipelines and serve as a strong ISRUC reference. ([bmcmedinformdecismak.biomedcentral.com](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-025-02995-9/tables/3))\n- Graph families on ISRUC: classic spatial–temporal GNN baselines remain strong. Sensors 2022 MGAN reported ISRUC Accuracy 0.825, MF1 0.814, Kappa 0.775 and listed very small parameter counts (~1.5e5), indicating high efficiency relative to CNN-RNN stacks. Meanwhile, ACM TIST 2024 (MSTGCN+SIDA) studied domain generalization across ISRUC-S1 age groups, with overall Accuracy ~0.785 and improved cross-domain robustness versus GraphSleepNet/MSTGCN, highlighting subject/age shift effects. ([mdpi.com](https://www.mdpi.com/1424-8220/22/23/9272/html?utm_source=openai))\n- Additional 2024–2025 directions: transformer-on-spectrogram methods (e.g., MCTSleepNet) report outperforming prior ISRUC-S3 baselines but require time–frequency preprocessing; foundational/whole-night transformers focus on very large PSG sets rather than ISRUC; diffusion-augmented and transfer-learning pipelines continue to appear. These show the trend toward transformer backbones but often at higher computational cost or with non-EEG modalities. ([proceedings.spiedigitallibrary.org](https://proceedings.spiedigitallibrary.org/conference-proceedings-of-spie/13269/132690M/MCTSleepNet-transformer-based-sleep-classification-with-multimodal-fusion/10.1117/12.3045517.short?utm_source=openai))\n\nEmpirical takeaways for ISRUC-like 6×T inputs: (i) multi-channel spatial structure helps (graph layers or spatial attention); (ii) temporal context beyond a single 30 s epoch helps, but can be captured by the temporal U-Net with sequence windows or context heads; (iii) class imbalance (especially N1) requires focal/weighted losses and balanced sampling; (iv) compute-efficient models with explicit spatial–temporal coupling (graph + CNN/UNet) can match transformer accuracy with far fewer parameters. ([bmcmedinformdecismak.biomedcentral.com](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-025-02995-9/tables/3))\n\nImplementation notes for your setting in PyTorch: With ISRUC 6 EEG channels, ST-USleepNet’s raw-signal pathway is a natural fit. If you keep 200 Hz (6×6000), set the temporal U-Net’s first-stage stride/dilations accordingly; else downsample to 100 Hz (6×3000) as in public recipes. Use a context window of ±1–2 epochs, class-weighted cross-entropy or focal loss, and per-class metrics. Consider early stopping on macro-F1 and calibration (ECE/Kappa). TorchEEG can streamline channel mapping and epochization. ([github.com](https://github.com/Majy-Yuji/ST-USleepNet))",
  "key_findings": [
    "On ISRUC-S1, MSF-SleepNet achieved Accuracy 0.826, macro-F1 0.809, and Kappa 0.774, surpassing U-Net (Acc 0.770) and graph baselines STGCN (0.786) and MSTGCN (0.804) reported in the same table. This sets a strong ISRUC reference point for 5-class staging. ([bmcmedinformdecismak.biomedcentral.com](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-025-02995-9/tables/3))",
    "Graph-based models remain highly competitive and efficient: Sensors 2022 MGAN reported ISRUC Accuracy 0.825 with only ~1.5×10^5 parameters and favorable training time per epoch, indicating excellent accuracy–efficiency trade-offs for multi-channel EEG. ([mdpi.com](https://www.mdpi.com/1424-8220/22/23/9272/html?utm_source=openai))",
    "ST-USleepNet explicitly models spatial–temporal coupling via a constructed graph and dual U-Nets (temporal 1D and graph U-Net), with official PyTorch code and 100 Hz resampling guidance. Authors report state-of-the-art results across multiple datasets, making it a strong candidate for 6-channel raw EEG without spectrogram preprocessing. ([arxiv.org](https://arxiv.org/abs/2408.11884))"
  ],
  "recommended_approaches": [
    "Adopt ST-USleepNet (temporal 1D U-Net + graph U-Net with spatial–temporal graph construction) for 6-channel raw EEG sleep staging. Justification: (i) architectural match to multi-channel 1D inputs; (ii) competitive/SOTA results reported on multiple datasets; (iii) available PyTorch code; (iv) avoids spectrogram preprocessing while capturing spatial coupling, and can be tuned for 200 Hz (6×6000) or resampled to 100 Hz for computational efficiency. Train with class-weighted/focal loss and a small context window to bolster N1 performance. ([arxiv.org](https://arxiv.org/abs/2408.11884))"
  ],
  "recent_papers": [
    {
      "title": "ST-USleepNet: A Spatial-Temporal Coupling Prominence Network for Multi-Channel Sleep Staging (IJCAI-25)",
      "contribution": "Builds a spatial–temporal graph from raw multi-channel PSG and uses dual U-Nets (temporal and graph) with fusion; authors report SOTA across several datasets; official PyTorch implementation supports raw-EEG pipelines (100 Hz). ([arxiv.org](https://arxiv.org/abs/2408.11884))"
    },
    {
      "title": "Towards interpretable sleep stage classification with a multi-stream fusion network (BMC Med Inform Decis Mak, 2025)",
      "contribution": "Provides a strong ISRUC-S1 benchmark: Acc 0.826, macro-F1 0.809, Kappa 0.774; table compares against U-Net, STGCN, MSTGCN, showing consistent gains. ([bmcmedinformdecismak.biomedcentral.com](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-025-02995-9/tables/3))"
    },
    {
      "title": "Exploring Structure Incentive Domain Adversarial Learning for Generalizable Sleep Stage Classification (ACM TIST, 2024)",
      "contribution": "Enhances MSTGCN with a domain-adversarial scheme (SIDA) to address cross-subject/age generalization; on ISRUC-S1, overall Acc ~0.785 and better robustness across age groups vs. baselines. ([dl.acm.org](https://dl.acm.org/doi/full/10.1145/3625238?utm_source=openai))"
    },
    {
      "title": "Multi-Layer Graph Attention Network (MGAN) for Sleep Stage Classification (Sensors, 2022)",
      "contribution": "On ISRUC, Acc 0.825, MF1 0.814, Kappa 0.775 with only ~1.5e5 parameters, illustrating an accuracy–efficiency sweet spot for graph-based EEG models. ([mdpi.com](https://www.mdpi.com/1424-8220/22/23/9272/html?utm_source=openai))"
    },
    {
      "title": "MCTSleepNet: Transformer-based sleep classification with multimodal fusion (SPIE AASIP 2024)",
      "contribution": "Transformer encoder on time–frequency representations for multi-channel inputs; reports outperforming prior ISRUC-S3 methods, at the cost of spectrogram preprocessing and heavier compute. ([proceedings.spiedigitallibrary.org](https://proceedings.spiedigitallibrary.org/conference-proceedings-of-spie/13269/132690M/MCTSleepNet-transformer-based-sleep-classification-with-multimodal-fusion/10.1117/12.3045517.short?utm_source=openai))"
    },
    {
      "title": "Sleep Medicine Reviews survey (2024): Research and application of deep learning-based sleep staging",
      "contribution": "Comprehensive review of datasets, modeling, validation, and clinical aspects; situates trends toward transformer and graph models and the importance of clinical validation. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1087079224000017?utm_source=openai))"
    }
  ],
  "confidence": 0.74,
  "timestamp": 1760242695,
  "generated_time": "2025-10-11 23:18:15",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      89283,
      6,
      6000
    ],
    "dtype": "float32",
    "feature_count": 6000,
    "sample_count": 89283,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}