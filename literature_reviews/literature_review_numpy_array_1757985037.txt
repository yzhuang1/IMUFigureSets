LITERATURE REVIEW
=================

Query: ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-16 01:10:37
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {},
  "previous_attempts": [
    "failed_attempt_1",
    "failed_attempt_2"
  ]
}

COMPREHENSIVE REVIEW:
Recent work (2023–2025) on ECG arrhythmia classification with MIT‑BIH shows a convergence around four families of approaches: (1) compact 1D Transformers or hybrid CNN‑Transformer encoders trained on raw beats or short multi‑beat windows; (2) enhanced 1D CNN backbones (ResNet/TCN/SE‑blocks, dilations) that mix local morphology with longer temporal context; (3) time‑frequency pipelines that map 1D signals to 2D representations (STFT/CWT/S‑transform) and classify with 2D CNNs or vision Transformers; and (4) representation learning and compression methods (self‑supervised pretraining, knowledge distillation) to improve generalization and on‑device feasibility. Examples include a 6k‑parameter Tiny Transformer that reaches ≈98.97% on 5‑class MIT‑BIH with 8‑bit inference; CNN‑Transformer variants that fuse convolutions (for morphologies) with attention (for rhythm‑level context); Swin or multi‑branch Transformers operating on wavelet/spectrogram maps; and SSL frameworks (e.g., NERULA) that combine masked reconstruction with non‑contrastive objectives. Several 2024–2025 studies also emphasize trustworthiness and lightweight deployment (e.g., EXGnet, knowledge‑distilled students) and cross‑database transfer via beat‑score maps. Reported top‑line metrics often exceed 97–99% on MIT‑BIH, but results depend strongly on evaluation protocol. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))

For your setting (numpy arrays shaped [1000, 2], 5 classes, MIT‑BIH), the most reliable pipelines use a patient‑wise (DS1→DS2) split to avoid leakage and either: (a) a compact 1D CNN‑Transformer with 2‑lead fusion, (b) a time‑frequency branch (e.g., CWT) feeding a lightweight 2D CNN/Transformer, or (c) a distilled student from a larger teacher for embedded efficiency. Typical windows of 2.5–3.0 s (≈900–1100 samples at 360 Hz) comfortably capture P‑QRS‑T morphology and inter‑beat timing; class imbalance is handled with focal loss or class‑weighted sampling; and training benefits from realistic ECG augmentations (baseline wander, motion artifact, pink noise, small time‑warps) and mixed‑precision. With strict inter‑patient evaluation under the AAMI 5‑superclass mapping (N, S, V, F, Q), recent lightweight or hybrid models generally achieve macro‑F1 ≈90–95% and overall accuracy ≈95–99% on MIT‑BIH; higher numbers are possible but often reflect differing label maps or non‑inter‑patient splits. Always cite the exact split (DS1/DS2) and the lead configuration used by MIT‑BIH (two channels at 360 Hz) when reporting results. ([frontiersin.org](https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2023.1247587/full?utm_source=openai))

KEY FINDINGS:
1. Use inter‑patient DS1→DS2 protocol (De Chazal split) to avoid patient overlap; record‑wise CV inflates performance. Explicitly report DS lists and AAMI 5‑class mapping. ([frontiersin.org](https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2023.1247587/full?utm_source=openai))
2. Compact attention helps when paired with a convolutional front‑end; pure 1D Transformers on raw signals lag, while tiny CNN‑Transformer or tiny‑Transformer models can be both accurate and efficient. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC11885558/?utm_source=openai))
3. Time‑frequency representations (CWT/STFT/S‑transform) with 2D CNN/Transformers are strong baselines when rhythm context matters or windows are not R‑peak aligned. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38434292/?utm_source=openai))
4. Self‑supervised pretraining (masked reconstruction + non‑contrastive/contrastive) improves downstream arrhythmia classification and cross‑dataset generalization. ([arxiv.org](https://arxiv.org/abs/2405.19348?utm_source=openai))
5. Knowledge distillation yields large compression (≥1000× reported) with minimal accuracy loss—useful for wearables and low‑latency inference. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/39771635/?utm_source=openai))
6. Expected ranges (AAMI 5‑class, inter‑patient, two‑lead, ≈3 s windows): overall accuracy ≈95–99%, macro‑F1 ≈90–95%, with S (SVEB) typically limiting; verify label mapping and noise handling. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
7. MIT‑BIH specifics matter: two channels at 360 Hz, heterogeneous second lead, and curated arrhythmias; document leads used (often MLII + Vx) and any resampling/denoising choices. ([physionet.org](https://www.physionet.org/physiobank/database/mitdb/?utm_source=openai))

RECOMMENDED APPROACHES:
1. {'name': 'Compact 1D Tiny Transformer (conv stem + MHSA) for 5‑class MIT‑BIH', 'why': 'Balances local morphology (conv stem) and long‑range rhythm context (attention) with very low parameter count; proven near‑SOTA on 5‑class MIT‑BIH and deployable with int8.', 'how': 'Input 1000×2; 1–2 conv blocks (k=7–15, d=1–4) → positional encoding → 2–4 Transformer encoder blocks (embed 128–256, 4–8 heads, dropout 0.1–0.2) → GAP → 5‑way head; AdamW (1e‑3→1e‑4), weight decay 1e‑4, batch 128–256, label smoothing 0.05; class weights or focal loss γ=2.', 'notes': 'Train with inter‑patient DS1/DS2; use mixed precision and gradient clipping (1.0). ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))'}
2. {'name': 'Hybrid 1D CNN‑Transformer (SE/TCN + attention)', 'why': 'Convolutions capture P‑QRS‑T morphology; attention models inter‑beat timing and rhythm episodes; robust on variable windows and two‑lead inputs.', 'how': 'Stack 3–5 residual 1D CNN blocks (k=7–9, dilation up to 8–16, 64–128 channels) with SE; add 1–3 Transformer layers (embed 128–256, 4 heads); cosine LR, warmup 5% of steps; dropout 0.2–0.3.', 'notes': 'Good trade‑off on accuracy/compute; often outperforms plain CNNs on multi‑beat inputs. ([dl.acm.org](https://dl.acm.org/doi/abs/10.1016/j.compbiomed.2022.105325?utm_source=openai))'}
3. {'name': 'Time‑frequency 2D branch (CWT/STFT/S‑transform) + lightweight 2D CNN/Vision Transformer', 'why': 'Stabilizes against window misalignment and exposes spectral cues (atrial activity, baseline wander) that aid S/V differentiation.', 'how': 'Compute CWT or STFT on each lead → stack as 2D multi‑channel image; 2D CNN (e.g., 3–4 stage ResNet‑18‑lite) or Swin‑Tiny with drop path 0.1; fuse 2‑lead features by late concat + MLP.', 'notes': 'Keep 224×224 or 256×256 resolution; augment with time‑warps and spectral masking. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38434292/?utm_source=openai))'}
4. {'name': 'Self‑supervised pretraining on unlabeled ECG + supervised fine‑tuning', 'why': 'Improves sample efficiency and cross‑dataset generalization; helpful if your labeled subset is small or imbalanced.', 'how': 'Masked reconstruction + non‑contrastive (e.g., BYOL‑style) or supervised contrastive pretext; 100–200 epochs pretraining; fine‑tune for 20–50 epochs with smaller LR (×0.1).', 'notes': 'Use realistic ECG augmentations (baseline drift, motion artifact, small scale/shift). ([arxiv.org](https://arxiv.org/abs/2405.19348?utm_source=openai))'}
5. {'name': 'Knowledge‑distilled lightweight student for deployment', 'why': 'Achieves near‑teacher accuracy at a fraction of parameters/latency for wearables or edge inference.', 'how': 'Train a larger teacher (hybrid or 2D branch) → distill logits + intermediate features (T=2–5, α=0.5–0.9) into a 1D CNN/Transformer student with ≤0.5M params.', 'notes': 'Combine with int8 quantization; validate under noise to check robustness. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/39771635/?utm_source=openai))'}

RECENT PAPERS:
- A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (2024): 6k‑param 1D Transformer reaches ≈98.97% on 5‑class MIT‑BIH with 8‑bit inference; demonstrates augmentation for motion artifacts and deployment on GAP9 MCU. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
- Enhancing ECG classification with continuous wavelet transform and multi‑branch transformer (2024): CWT feature maps with multi‑branch Transformer surpass many prior methods on ECG time‑series feature maps. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38434292/?utm_source=openai))
- A novel method of Swin Transformer with time‑frequency characteristics for ECG‑based arrhythmia detection (2024): Applies Swin on wavelet time‑frequency maps from MIT‑BIH; shows effectiveness of vision Transformers on 2D ECG representations. ([frontiersin.org](https://www.frontiersin.org/journals/cardiovascular-medicine/articles/10.3389/fcvm.2024.1401143/full?utm_source=openai))
- NERULA: A Dual‑Pathway Self‑Supervised Learning Framework for ECG (2024): Combines masked reconstruction and non‑contrastive alignment; improves ECG downstream tasks including arrhythmia classification. ([arxiv.org](https://arxiv.org/abs/2405.19348?utm_source=openai))
- Research on a Lightweight Arrhythmia Classification Model Based on Knowledge Distillation (2024/2025): KD yields ≈96% accuracy with >1000× compression for wearable single‑lead monitoring. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/39771635/?utm_source=openai))
- Local‑Global Temporal Fusion Network with Attention (2023): Fuses local and global temporal cues; episode‑level detection/classification on MIT‑BIH/AFDB with superior results. ([arxiv.org](https://arxiv.org/abs/2308.02416?utm_source=openai))
- rECGnition v2.0: Self‑Attentive Canonical Fusion of ECG and Patient Data (2025): Self‑Attentive Canonical Correlation fusion with a dual‑pathway network; ≈98% accuracy on 10‑class MIT‑BIH with low FLOPs. ([arxiv.org](https://arxiv.org/abs/2502.16255?utm_source=openai))
- EXGnet: Explainable multiresolution network for single‑lead ECG (2025): XAI‑guided multiresolution features with train‑only quantitative cues; strong accuracy with Grad‑CAM interpretability. ([arxiv.org](https://arxiv.org/abs/2506.12404?utm_source=openai))
- Cross‑Database Learning with Beat‑Score‑Map Representation (2024): Pretrain on MIT‑BIH beat types, then generate 2D BSM for rhythm classification; strong cross‑dataset F1 on SPH/PTB‑XL. ([mdpi.com](https://www.mdpi.com/2076-3417/15/10/5535?utm_source=openai))
- A lightweight hybrid CNN‑LSTM explainable model (2024): Hybrid 1D CNN‑LSTM with SHAP explanations; detects 8 arrhythmias + normal with lightweight design. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S1746809423013174?utm_source=openai))

==================================================
