LITERATURE REVIEW
=================

Query: ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-15 18:31:35
Confidence: 0.82

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
Between January 2023 and September 2025, ECG arrhythmia classification on MIT-BIH has been pushed forward by three converging lines of work: (i) efficient Transformer variants tailored to 1D signals, (ii) state space models (SSMs) such as Mamba for long sequences, and (iii) self‑supervised/foundation models pre‑trained at scale and then fine‑tuned on small labeled sets. Examples include compact or patched Transformers that reach >98% five‑class accuracy on MIT‑BIH with tiny parameter counts suitable for edge deployment, multi‑granularity/patching designs for medical time series, and CNN‑Transformer hybrids that blend local morphology with global rhythm context. SSMs like ECGMamba aim to retain Transformer‑level accuracy while reducing the O(L^2) attention cost to near‑linear time; they are attractive when your input length is around 1000 samples per beat/segment. Meanwhile, ECG foundation models (e.g., HuBERT‑ECG) and multimodal ECG‑text SSL (e.g., METS) improve generalization and label efficiency, often transferring well to MIT‑BIH after light fine‑tuning. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))

For your setting (numpy_array, shape=(1000,2), five classes, MIT‑BIH), the strongest practical choices are: a 1D CNN + Transformer hybrid (e.g., CAT‑Net) or a compact patched Transformer, both trained on beat‑centered segments with class‑aware loss; a Mamba/SSM backbone if you need linear‑time scaling; and optionally fine‑tuning a pre‑trained ECG encoder to boost minority‑class (S, F, Q) performance. Recent reports show 98–99% overall accuracy on MIT‑BIH in 5‑class setups under common preprocessing pipelines, but note that inter‑patient evaluation remains stricter than intra‑patient and typically lowers macro‑F1, especially for S and F. Use AAMI EC57 N/S/V/F/Q grouping, subject‑exclusive splits, and realistic noise augmentation (NSTDB) to avoid optimistic estimates. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))

KEY FINDINGS:
1. Data handling matters as much as the model: use the AAMI EC57 mapping to N/S/V/F/Q, subject‑exclusive (inter‑patient) splits, and report macro‑F1 per class; intra‑patient protocols can inflate metrics. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC5499251/?utm_source=openai))
2. Compact or patched Transformers and CNN‑Transformer hybrids are SOTA‑competitive on MIT‑BIH while remaining deployable; 6k‑param “tiny” Transformers have reported ~99% five‑class accuracy. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
3. State space models (e.g., ECGMamba) reduce attention overhead and scale well to sequence length ~1000, offering efficient training/inference with competitive accuracy. ([arxiv.org](https://arxiv.org/abs/2406.10098?utm_source=openai))
4. Self‑supervised/foundation ECG encoders (e.g., HuBERT‑ECG, METS) improve label efficiency and cross‑dataset transfer; fine‑tuning them on MIT‑BIH yields gains on minority classes. ([medrxiv.org](https://www.medrxiv.org/content/10.1101/2024.11.14.24317328v1.full?utm_source=openai))
5. Image transforms (STFT spectrograms/GAF) plus 2D CNN/ViT can work well when you prefer 2D tooling, but add preprocessing cost; results comparable to strong 1D baselines have been reported. ([mdpi.com](https://www.mdpi.com/2076-3417/14/21/9936?utm_source=openai))
6. Expected performance on five‑class MIT‑BIH: strong recent methods report ~98–99% accuracy; under strict inter‑patient and class‑balanced reporting, macro‑F1 may drop into the low–mid 90s, especially for S and F. ([mdpi.com](https://www.mdpi.com/1424-8220/25/17/5244))
7. Best‑practice augmentations are physiologically plausible noise (baseline wander, EMG, electrode motion from NSTDB), time warping/jitter, and amplitude scaling; avoid label‑breaking transforms. ([physionet.org](https://physionet.org/physiobank/database/nstdb/))

RECOMMENDED APPROACHES:
1. {'name': '1D CNN + Transformer hybrid (e.g., CAT‑Net)', 'why': 'Convolutions capture local morphology (P‑QRS‑T), Transformer layers model long‑range rhythm; strong results on MIT‑BIH with good minority‑class F1 using class balancing.', 'how': 'PyTorch 1D Conv stem (3–5 blocks, kernels 5–11), followed by 2–4 Transformer encoder layers (d_model 128–256, 4–8 heads) over length‑wise tokens; global average pooling + MLP.', 'hyperparams': 'LR 1e‑3→3e‑4 (AdamW), weight decay 1e‑2, batch 64 (FP16), dropout 0.2–0.4, label smoothing 0.05, class‑weighted CE or focal loss (γ=1–2).', 'notes': 'Use AAMI N/S/V/F/Q labels; segment 1000×2 windows centered on R‑peaks; SMOTE‑Tomek or weighted sampling for imbalance. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))'}
2. {'name': 'Patched/Tiny Transformer for 1D ECG', 'why': 'Maintains accuracy with very small models suitable for real‑time or embedded use on 1000‑sample sequences.', 'how': 'Split each channel into non‑overlapping patches (e.g., patch_len=10→100 tokens), linear patch embedding, 3–6 encoder layers (d_model 96–192), shared or cross‑channel mixing.', 'hyperparams': 'Batch 128 (FP16), LR 1e‑3 with cosine schedule, EMA, dropout 0.1–0.3, int8 PTQ/QAT if targeting MCUs.', 'notes': 'Demonstrated 98.97% five‑class accuracy with ~6k parameters on MIT‑BIH; add motion‑artifact augmentations. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))'}
3. {'name': 'Mamba/SSM backbone (e.g., ECGMamba)', 'why': 'Linear‑time sequence modeling that scales better than attention for L=1000 while preserving temporal dependencies.', 'how': 'Replace attention blocks with bidirectional SSM layers; optional small CNN front‑end; 4–8 SSM blocks with d_model 128–256.', 'hyperparams': 'LR 1e‑3, batch 128 (FP16), weight decay 5e‑3, dropout 0.1–0.3.', 'notes': 'Competitive accuracy with improved throughput/latency vs Transformers on ECG tasks. ([arxiv.org](https://arxiv.org/abs/2406.10098?utm_source=openai))'}
4. {'name': 'Fine‑tune a pre‑trained ECG foundation/SSL model', 'why': 'Leverages large‑scale pretraining (e.g., 9.1M ECGs or ECG‑text pairs) to improve data efficiency and cross‑dataset robustness.', 'how': 'Load a pre‑trained encoder (HuBERT‑ECG or METS‑style), freeze lower blocks initially, fine‑tune top layers + light classification head on 1000×2 inputs.', 'hyperparams': 'Small LR 1e‑4–3e‑5 for encoder, 3e‑4 for head; 5–20 epochs; strong weight decay; early stopping by macro‑F1.', 'notes': 'Reports show zero‑/few‑shot gains and strong transfer to MIT‑BIH after fine‑tuning. ([medrxiv.org](https://www.medrxiv.org/content/10.1101/2024.11.14.24317328v1.full?utm_source=openai))'}
5. {'name': '2D time–frequency route (STFT/GAF + 2D CNN/ViT)', 'why': 'Useful if your pipeline/tools are optimized for 2D; fuses complementary representations (spectrogram, HOG/LBP, etc.).', 'how': 'STFT per channel → 2D tensor or GAF; 2D CNN or lightweight ViT with late fusion.', 'hyperparams': 'n_fft 128–256, hop 32–64; CNN depth 6–10; LR 1e‑3; dropout 0.3.', 'notes': 'Recent works report competitive MIT‑BIH accuracy with feature‑fusion designs. ([mdpi.com](https://www.mdpi.com/2076-3417/14/21/9936?utm_source=openai))'}

RECENT PAPERS:
- CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (2024): CNN‑attention‑Transformer hybrid; strong 5‑class MIT‑BIH performance with SMOTE‑Tomek for imbalance; accuracy ≈99.14%. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))
- A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (2024): ≈6k‑parameter Transformer hitting 98.97% on 5‑class MIT‑BIH; demonstrates int8 deployment and robustness to motion artifacts. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
- ECGMamba: Towards Efficient ECG Classification with BiSSM (2024): Bidirectional Mamba SSM for efficient ECG classification with competitive accuracy and linear‑time scaling. ([arxiv.org](https://arxiv.org/abs/2406.10098?utm_source=openai))
- Medformer: A Multi‑Granularity Patching Transformer for Medical Time‑Series Classification (2024): Cross‑channel patching and multi‑granularity attention improve ECG/EEG classification; template for patched 1D Transformers. ([arxiv.org](https://arxiv.org/abs/2405.19363?utm_source=openai))
- HuBERT‑ECG: a self‑supervised foundation model for broad and scalable cardiac applications (medRxiv, 2024): Pre‑trained on 9.1M ECGs; fine‑tunes well across many downstream tasks and datasets, supporting label‑efficient ECG classification. ([medrxiv.org](https://www.medrxiv.org/content/10.1101/2024.11.14.24317328v1.full?utm_source=openai))
- Frozen Language Model Helps ECG Zero‑Shot Learning (METS, 2023): Multimodal ECG‑text SSL enabling zero‑shot and improved transfer; shows gains on MIT‑BIH without labeled fine‑tuning. ([arxiv.org](https://arxiv.org/abs/2303.12311?utm_source=openai))
- Local‑Global Temporal Fusion Network with Attention for Multiclass Arrhythmia Classification (2023): Combines local and global temporal features; strong results on MIT‑BIH/AFDB and cross‑dataset tests. ([arxiv.org](https://arxiv.org/abs/2308.02416?utm_source=openai))
- Masked Transformer for Electrocardiogram Classification (2023): MAE‑style pretraining adapted to ECG; practical recipe for Transformer ECG pretrain/fine‑tune. ([arxiv.org](https://arxiv.org/abs/2309.07136?utm_source=openai))
- Pruned lightweight neural networks for arrhythmia classification (Applied Soft Computing, 2024): Model compression/pruning strategy with strong MIT‑BIH benchmark (~99.24%) while keeping models small. ([dl.acm.org](https://dl.acm.org/doi/10.1016/j.asoc.2024.111340?utm_source=openai))
- MIT‑BIH Arrhythmia Database (PhysioNet; canonical description): 48 half‑hour two‑channel ECGs at 360 Hz; ~110k annotated beats; standard AAMI evaluation context. ([physionet.org](https://physionet.org/content/mitdb/1.0.0/))

==================================================
