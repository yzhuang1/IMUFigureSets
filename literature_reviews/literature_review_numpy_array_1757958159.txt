LITERATURE REVIEW
=================

Query: ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-15 17:42:39
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
From 2023–2025, ECG arrhythmia classification on MIT‑BIH has been driven by three methodological trends: (1) lightweight and hybrid attention models that mix dilated temporal convolutions with multi‑head self‑attention to capture both local morphology and long‑range rhythm, e.g., MB‑MHA‑TCN reporting 98.75% overall accuracy on five AAMI classes; (2) Transformer variants operating either on raw 1‑D beats or 2‑D time–frequency maps (e.g., Swin Transformer + wavelet scalograms) that show strong intra‑ and inter‑patient results; and (3) sequence models beyond Transformers, notably diagonal state‑space (S4D) layers that scale to long contexts with low compute and have been validated on clinical ECGs. In parallel, two practical thrusts emerged: tiny on‑device Transformers demonstrating near‑SOTA accuracy with ~6k parameters for real‑time use, and self‑supervised pretraining (contrastive or masked‑reconstruction objectives) that improves label efficiency and robustness; recent work also blends SSL with generative augmentation to bolster minority classes. Multimodal fusion using patient demographics is gaining attention for improved generalization. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/39771858/))

For your setting—two‑lead numpy arrays of shape (1000, 2), five classes (AAMI N/S/V/F/Q), MIT‑BIH signals sampled at 360 Hz with two channels—best practice is to enforce the inter‑patient DS1/DS2 split to avoid leakage, address extreme class imbalance (notably scarce F and Q), and choose architectures whose receptive field spans the ~2.8 s window while preserving P‑QRS‑T morphology. Expect strong overall accuracy (>96%) under inter‑patient evaluation when class imbalance is handled carefully, with macro‑F1 driven largely by S and F performance. Reported inter‑patient five‑class accuracies of ~97–99% are achievable with hybrid CNN–Attention/TCN or Transformer pipelines plus focal loss/oversampling; intra‑patient can reach ~99% but overestimates clinical utility. The MIT‑BIH database characteristics and the widely used DS1/DS2 protocol, including the rarity of Q beats, should guide preprocessing (R‑peak‑centered windows or rhythm windows), sampling, and metrics (macro‑F1, per‑class sensitivity). ([physionet.org](https://physionet.org/physiobank/database/mitdb/?utm_source=openai))

KEY FINDINGS:
1. Use the AAMI five‑class mapping and the De Chazal inter‑patient split (DS1 train / DS2 test). This prevents patient overlap and reflects clinical generalization; Q beats are extremely rare in DS1/DS2 and require special handling. ([github.com](https://github.com/venturit/ecg_classification?utm_source=openai))
2. Hybrid temporal models (dilated TCN + multi‑head attention) and compact Transformers consistently perform best for sequence lengths around 1000, balancing local waveform detail with long‑range rhythm context. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/39771858/))
3. Inter‑patient evaluation with five AAMI classes typically yields 97–99% overall accuracy when minority‑class strategies (class‑balanced sampling, focal loss, mixup, targeted augmentations) are used; intra‑patient results are higher but optimistic. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38911517/?utm_source=openai))
4. Self‑supervised pretraining (contrastive/masked reconstruction) on large ECG corpora improves label efficiency and robustness; fine‑tuning on MIT‑BIH boosts downstream arrhythmia classification. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/34973584/?utm_source=openai))
5. Time–frequency front‑ends (wavelet scalograms) paired with hierarchical Transformers can improve inter‑patient macro‑F1 by enriching S and F morphology, at modest compute cost. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38911517/?utm_source=openai))
6. For PyTorch: represent inputs as (B, C=2, L=1000); use torch.amp autocast + GradScaler, WeightedRandomSampler or class‑balanced loss, gradient clipping (0.5–1.0), cosine LR with warmup, and early stopping on macro‑F1; target 1–5M parameters for efficient training on a single GPU.
7. Deployment is feasible with quantized tiny Transformers or pruned CNN‑TCN models; 8‑bit inference can preserve >98% accuracy while enabling millijoule‑level energy budgets on MCUs. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))

RECOMMENDED APPROACHES:
1. {'name': 'Multi‑branch Dilated TCN + Multi‑Head Attention (MB‑MHA‑TCN)', 'why_it_works': 'Dilated 1‑D convolutions capture P‑QRS‑T morphology at multiple scales; attention fuses branches and models cross‑beat dependencies across the 1000‑step window.', 'pytorch_notes': 'Stack 3–5 residual TCN blocks with dilations {1,2,4,8,16}; add a 2–4 head attention fusion layer before classification.', 'typical_hparams': 'Channels 64–256; kernel sizes 3–7; dropout 0.1–0.3; LR 1e‑3 with cosine decay; batch 128–512; focal loss γ=1–2.', 'evidence': 'Sensors 2024 MB‑MHA‑TCN reports 98.75% five‑class accuracy on MIT‑BIH. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/39771858/))'}
2. {'name': 'CNN + Transformer Encoder (beat/rhythm window)', 'why_it_works': 'CNN stem preserves local wave morphology; Transformer layers model rhythm and inter‑beat context without losing temporal resolution.', 'pytorch_notes': 'Conv1d stem → positional encoding → 2–4 Transformer encoder blocks (hidden 128–256, 4–8 heads) → pooled classifier.', 'typical_hparams': 'Patch/stride 4–8; dropout 0.1–0.2; weight decay 1e‑4–5e‑4; LR 1e‑4–3e‑4 (AdamW).', 'evidence': 'Tiny Transformers achieve ~99% five‑class accuracy with ~6k params; Swin‑Transformer on wavelet maps achieves 98.37% inter‑patient accuracy. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))'}
3. {'name': 'State‑Space Sequence Model (S4D) backbone', 'why_it_works': 'Linear‑time long‑range modeling with stable convolutions gives strong sequence classification while keeping compute and memory low.', 'pytorch_notes': '2–4 S4D blocks (hidden 128–256) + small MLP head; optional CNN front‑end for denoising.', 'typical_hparams': 'Dropout 0.1–0.3; LR 1e‑3 → 1e‑4; batch 256–1024; label smoothing 0.05.', 'evidence': 'S4D‑ECG validated on clinical ECGs with strong noise robustness and efficient deployment characteristics. ([link.springer.com](https://link.springer.com/article/10.1007/s13239-024-00716-3?utm_source=openai))'}
4. {'name': 'Self‑Supervised Pretrain + Fine‑Tune', 'why_it_works': 'Contrastive/masked pretraining learns rhythm‑aware features that transfer to AAMI five‑class tasks, improving minority‑class recall.', 'pytorch_notes': 'Pretrain SimCLR/CPC‑style on unlabeled ECG windows (augmentations: jitter, time‑warp, scaling, lead‑dropout); fine‑tune small CNN/Transformer head.', 'typical_hparams': 'Pretrain 50–200 epochs (LR 1e‑3); fine‑tune 20–50 epochs (LR 1e‑4), strong augmentation on S/F/Q.', 'evidence': 'Self‑supervised representation learning on 12‑lead ECG improves downstream classification and robustness. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/34973584/?utm_source=openai))'}
5. {'name': 'Time–Frequency Front‑end + Hierarchical Transformer', 'why_it_works': 'Wavelet scalograms expose frequency‑localized atrial/ventricular patterns that aid S and F discrimination.', 'pytorch_notes': 'Compute CWT per lead → stack as 2‑channel image → Swin/ViT backbone (tiny/small) → classifier.', 'typical_hparams': 'Image size ~256×256; window size 8–16; heads 3–6; mixup 0.2; label smoothing 0.1.', 'evidence': 'Swin Transformer with wavelet maps reached 99.34% intra‑patient and 98.37% inter‑patient accuracy on MIT‑BIH. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38911517/?utm_source=openai))'}

RECENT PAPERS:
- Accurate Arrhythmia Classification with Multi‑Branch, Multi‑Head Attention Temporal Convolutional Networks (Sensors, 2024): Hybrid multi‑scale TCN with attention; five‑class MIT‑BIH accuracy 98.75%; uses focal loss and Bayesian hyperparameter search. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/39771858/))
- A novel method of Swin Transformer with time‑frequency characteristics for ECG‑based arrhythmia detection (Frontiers/PMC, 2024): Wavelet scalograms + Swin Transformer; reports 99.34% intra‑patient and 98.37% inter‑patient accuracy on MIT‑BIH. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38911517/?utm_source=openai))
- A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (arXiv, 2024): ~6k‑parameter Transformer, 8‑bit inference; 98.97% five‑class accuracy on MIT‑BIH; demonstrates MCU deployment. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))
- S4D‑ECG: A Shallow State‑of‑the‑Art Model for Cardiac Abnormality Classification (Cardiovascular Engineering and Technology, 2024): Applies diagonal state‑space (S4D) layers to ECG; emphasizes robustness and efficiency on clinical data. ([link.springer.com](https://link.springer.com/article/10.1007/s13239-024-00716-3?utm_source=openai))
- Heartbeat classification combining multi‑branch CNNs and Transformer (Biomed Signal Processing and Control/PMID 38482492, 2023): CNN–Transformer fusion; five‑class MIT‑BIH evaluations under intra‑ and inter‑patient protocols. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38482492/?utm_source=openai))
- NERULA: Dual‑Pathway Self‑Supervised Learning for ECG (arXiv, 2024): Masked reconstruction + non‑contrastive alignment for single‑lead ECG; improves downstream arrhythmia tasks. ([arxiv.org](https://arxiv.org/abs/2405.19348?utm_source=openai))
- ECG synthesis for cardiac arrhythmias: integrating self‑supervised learning and GANs (Computers in Biology and Medicine, 2025, in press/PMID 40460597): Generative augmentation pipeline improving arrhythmia accuracy across datasets including MIT‑BIH. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/40460597/?utm_source=openai))
- Inter‑patient split definitions and class imbalance on MIT‑BIH (supporting sources): PhysioNet MIT‑BIH spec (2 leads, 360 Hz) and DS1/DS2 record lists with rare Q class; essential for fair evaluation and class strategy. ([physionet.org](https://physionet.org/physiobank/database/mitdb/?utm_source=openai))

==================================================
