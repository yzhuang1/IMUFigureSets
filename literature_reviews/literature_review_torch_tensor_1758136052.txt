LITERATURE REVIEW
=================

Query: ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-17 14:07:32
Confidence: 0.74

DATA PROFILE:
{
  "data_type": "torch_tensor",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
Problem analysis: You have 2-lead ECG sequences shaped (time=1000, channels=2) to classify 5 arrhythmia classes on the MIT-BIH Arrhythmia Database. The accepted evaluation for heartbeat classification on MIT-BIH is the AAMI 5-class mapping (N, S, V, F, Q) with an inter-patient DS1→DS2 split to avoid patient leakage. This split is widely cited (De Chazal DS1/DS2) and should be used for fair comparison. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC7206245/?utm_source=openai)) State of the art (2024–2025): (1) Tiny-Transformer models demonstrate very high accuracy with extreme efficiency: Busia et al. (IEEE TBCAS 2024) reach 98.97% (5-class) using a ViT-like 1D model with only ~6k parameters and 8-bit inference (4.28 ms, 0.09 mJ on GAP9). Note: their comparison is intra-patient; still, it establishes a strong efficiency-accuracy point. ([ar5iv.org](https://ar5iv.org/pdf/2402.10748)) (2) Transformer or CNN–Transformer hybrids dominate recent multi-class results. A 2025 Sensors paper reports a lightweight multi-lead CNN+attention baseline (2-lead MIT-BIH, 5-class) at 99.18% accuracy and 99.18% F1 with ~308k parameters and ~5.9 ms/sample, and summarizes competing methods including STCT (CNN+Transformer) which achieves 98.96% accuracy and 99.31% F1 on 2-lead, 5-class MIT-BIH. ([mdpi.com](https://www.mdpi.com/1424-8220/25/17/5542)) (3) Inter-patient performance is systematically lower than intra-patient; e.g., an attention-TCN shows 99.84% intra-patient but 87.81% inter-patient on MIT-BIH, underscoring the need to report DS1/DS2 results. ([mdpi.com](https://www.mdpi.com/1424-8220/25/17/5542)) (4) Swin-Transformer with wavelet time–frequency maps reports 99.34% (intra) and 98.37% (inter) on MIT-BIH, reinforcing that transformer backbones with appropriate representations are effective under inter-patient protocols. ([frontiersin.org](https://www.frontiersin.org/journals/cardiovascular-medicine/articles/10.3389/fcvm.2024.1401143/full)) (5) Some works report near-perfect inter-patient scores (e.g., dual-attention networks), but a 2025 systematic review highlights that many papers deviate from standards (AAMI mapping, inter-patient splits) and lack embedded-feasibility reporting; thus results must be interpreted cautiously and reproduced with DS1/DS2. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/37116424/?utm_source=openai)) Implementation implications for your data: Your input already matches two leads and a fixed window; recent best-performing and efficient methods use a CNN front-end for local morphology and a shallow Transformer encoder for long-range temporal context, sometimes with lightweight channel attention. This design aligns with STCT-like architectures and the 2025 lightweight CNN+attention baseline that explicitly supports 2-lead inputs and reports strong F1/accuracy with modest model size and latency. ([mdpi.com](https://www.mdpi.com/1424-8220/25/17/5542)) Recommended training/eval: Use AAMI 5-class mapping, DS1→DS2 inter-patient split, class-balanced or focal loss, and augmentations (baseline wander, noise, small time-warp) to address class imbalance (S, F, Q) and deployment robustness. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC7206245/?utm_source=openai))

KEY FINDINGS:
1. Lightweight Transformers can be extremely accurate and efficient: a 6k-parameter Tiny Transformer achieved 98.97% (5-class) on MIT-BIH with 8-bit inference and 0.09 mJ/4.28 ms per inference (intra-patient). ([ar5iv.org](https://ar5iv.org/pdf/2402.10748))
2. For 2-lead, 5-class MIT-BIH, a lightweight CNN+attention model reported 99.18% accuracy and 99.18% F1 with ~307k parameters and ~5.9 ms/sample, while the STCT (CNN+Transformer) achieved 98.96% accuracy and 99.31% F1. ([mdpi.com](https://www.mdpi.com/1424-8220/25/17/5542))
3. Inter-patient results are typically lower than intra-patient: an attention-TCN dropped from 99.84% (intra) to 87.81% (inter), emphasizing the necessity of DS1/DS2 evaluation for fair benchmarking. ([mdpi.com](https://www.mdpi.com/1424-8220/25/17/5542))
4. Wavelet time–frequency + Swin Transformer attained 98.37% (inter-patient) and 99.34% (intra) on MIT-BIH, showing transformer backbones remain strong under patient-independent testing. ([frontiersin.org](https://www.frontiersin.org/journals/cardiovascular-medicine/articles/10.3389/fcvm.2024.1401143/full))
5. Standards compliance is inconsistent across the literature; a 2025 systematic review stresses AAMI mapping and inter-patient splits plus embedded feasibility when claiming SOTA. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))
6. MIT-BIH DS1/DS2 inter-patient split and AAMI 5-class mapping are the accepted protocol for heartbeat classification; use these for reproducible comparisons. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC7206245/?utm_source=openai))

RECOMMENDED APPROACHES:
1. Adopt a 1D CNN + Transformer hybrid (STCT-style) for 2-lead inputs: (a) CNN stem with 3–4 temporal conv blocks (kernel sizes 7/5/3, stride to reduce 1000→~125), optional SE/CBAM channel attention; (b) 2–3 shallow Transformer encoder blocks (d_model≈128, 4 heads) with positional encoding to capture long-range dependencies; (c) global pooling + linear head for 5 classes. Train with AAMI 5-class mapping and DS1→DS2 inter-patient split, class-balanced focal loss, and noise/baseline-wander/time-warp augmentations. This matches your (1000,2) tensor format in PyTorch ([B, 2, 1000]), is efficient (~0.3–1.0M params), and is supported by recent evidence that 2-lead CNN+attention and CNN+Transformer models achieve ≈99% accuracy/≈99% F1 on MIT-BIH, while remaining implementable and deployable. ([mdpi.com](https://www.mdpi.com/1424-8220/25/17/5542))

RECENT PAPERS:
- A Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers (IEEE TBCAS, 2024): ViT-like 1D model with ~6k params; 98.97% (5-class, intra-patient) on MIT-BIH; 8-bit int inference at 4.28 ms and 0.09 mJ on GAP9; strong template for efficient deployment. ([ar5iv.org](https://ar5iv.org/pdf/2402.10748))
- Lightweight Deep Learning Architecture for Multi-Lead ECG Arrhythmia Detection (Sensors, 2025): Multi-lead CNN+attention baseline: 2-lead MIT-BIH (5-class) 99.18% accuracy/99.18% F1 with ~307k params and ~5.9 ms/sample; also summarizes STCT (CNN+Transformer) at 99.31% F1 on the same setting. ([mdpi.com](https://www.mdpi.com/1424-8220/25/17/5542))
- A novel method of Swin Transformer with time–frequency characteristics for ECG-based arrhythmia detection (Frontiers Cardiovasc. Med., 2024): Wavelet time–frequency maps + Swin Transformer; reports 99.34% (intra) and 98.37% (inter) on MIT-BIH; demonstrates robust inter-patient transformer performance. ([frontiersin.org](https://www.frontiersin.org/journals/cardiovascular-medicine/articles/10.3389/fcvm.2024.1401143/full))
- An Enhanced Hybrid Model Combining CNN, BiLSTM, and Attention Mechanism for ECG Segment Classification (2024): CNN+CBAM+BiLSTM with SMOTE for AAMI 5-class; reports 99.20% accuracy and 98.29% mean F1 on MIT-BIH; strong beat-level hybrid baseline. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC12174755/?utm_source=openai))
- A Systematic Review of ECG Arrhythmia Classification: Adherence to Standards, Fair Evaluation, and Embedded Feasibility (2025): Reviews 2017–2024 studies; highlights frequent deviations from AAMI/DS1–DS2 and lack of embedded-feasibility reporting; provides guidance for fair, clinically relevant evaluation. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))

==================================================
