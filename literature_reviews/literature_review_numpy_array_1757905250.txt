LITERATURE REVIEW
=================

Query: sequence classification time series machine learning multiclass sequence classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-15 03:00:50
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {},
  "previous_attempts": [
    "failed_attempt_1"
  ]
}

COMPREHENSIVE REVIEW:
From 2023–2025, multivariate time-series classification (MTSC) has seen two parallel tracks of progress. First, classical but highly competitive baselines have been refreshed and re-benchmarked: MultiROCKET and its hybrids (e.g., MultiROCKET+Hydra) alongside the HIVE-COTE 2.0 meta-ensemble remain top performers across broad bake-off studies, with MultiROCKET extracting ~50k features efficiently and HIVE-COTEv2 keeping best-in-class accuracy albeit with higher compute costs. These results were reaffirmed in the 2024 “Bake off redux” review, which compared families of modern TSC methods and highlighted MultiROCKET+Hydra and HIVE-COTEv2 as state-of-the-art on extensive UCR/UEA-style evaluations. InceptionTime, a scalable 1D CNN ensemble, continues to match strong ensembles while being easier to train and deploy, with widely used default hyperparameters documented in open-source toolkits. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-024-01022-1))

Second, transformer-style and representation-learning approaches tailored to MTSC have matured. 2023–2025 work includes hierarchical and multi-scale transformers (FormerTime), shapelet-aware transformers (ShapeFormer, KDD 2024), value-and-shape–aware attention (VSFormer, 2024), timestamp-reconstruction encoders (CTNet, 2024), interpretable Swin-Transformer trees (ST-Tree, 2024), and dual-branch multiscale designs (MD-Former, 2025), many reporting top ranks on 30 UEA MTSC datasets. In parallel, self-supervised pretraining (e.g., TimeMAE, TS2Vec) continues to yield strong downstream classification on UCR/UEA, making it attractive when labels are scarce. For low-dimensional MTSC like yours (length≈1000, channels=2, classes=5), CNNs (InceptionTime/ResNet1D) and MultiROCKET remain robust first choices; transformer variants win when long-range temporal context or subtle class-specific subsequences matter, and self-supervised pretraining helps in small-N regimes. ([arxiv.org](https://arxiv.org/abs/2302.09818?utm_source=openai))

KEY FINDINGS:
1. Strong baselines still matter: MultiROCKET (+Hydra) and HIVE-COTEv2 are among the most reliable SOTA references; compare against them early to gauge headroom. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-024-01022-1))
2. InceptionTime remains a high-accuracy, training-friendly CNN for MTSC; common defaults (depth≈6, kernel_size≈40, n_filters≈32, ensemble of 5) are widely used. ([sktime.net](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.classification.deep_learning.inceptiontime.InceptionTimeClassifier.html?utm_source=openai))
3. Transformer variants that combine multi-scale features and task priors (e.g., shapelets, value-aware attention, crucial timestamp masking) lead recent MTSC gains. ([arxiv.org](https://arxiv.org/abs/2405.14608))
4. Self-supervised pretraining (TimeMAE, TS2Vec, TS-TCC/CA-TCC) improves downstream classification, especially with limited labels. ([arxiv.org](https://arxiv.org/abs/2303.00320?utm_source=openai))
5. For short-to-moderate lengths (~1000) and few channels (C=2), 1D CNNs and ROCKET-family transforms are compute-efficient and strong; transformers become more beneficial as long-range dependencies and class-specific subsequences dominate.
6. Augmentation helps: mix-based (mixup/cutmix), jitter/scale, and masked-time augmentations are consistently beneficial in physiological and general TS settings. ([arxiv.org](https://arxiv.org/abs/2309.09970?utm_source=openai))
7. Evaluation varies widely across UEA tasks; expect variability by domain and noise. Use stratified splits, strong baselines, and calibration checks before architecture escalation. ([mdpi.com](https://www.mdpi.com/2306-5729/10/5/58?utm_source=openai))

RECOMMENDED APPROACHES:
1. {'name': 'InceptionTime (CNN ensemble)', 'why': 'Excellent accuracy–efficiency tradeoff; robust to small channel counts; easy to implement and tune in PyTorch.', 'typical_hparams': 'depth=6; n_filters=32; kernel_size≈40; use_residual=True; use_bottleneck=True; batch_size=32–128; epochs 50–300 (or early stop).', 'notes': 'Use global average pooling + linear head; start with single model, add 3–5 model ensemble if needed. ([sktime.net](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.classification.deep_learning.inceptiontime.InceptionTimeClassifier.html?utm_source=openai))'}
2. {'name': 'MultiROCKET (+ Ridge/LogReg) or MultiROCKET+Hydra', 'why': 'Very fast, strong SOTA reference; transforms series to ~50k features with excellent linear separability.', 'typical_hparams': 'n_kernels≈10k; n_features_per_kernel=4 (MultiROCKET); classifier=RidgeClassifierCV (small/medium N) or SGD-LogReg (large N).', 'notes': 'Great baseline to set an accuracy floor and diagnose data issues. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-022-00844-1?utm_source=openai))'}
3. {'name': 'Transformer for MTSC (FormerTime/ShapeFormer/VSFormer/CTNet style)', 'why': 'Captures long-range dependencies and class-specific subsequences (shapelets, crucial timestamps, value-aware priors).', 'typical_hparams': 'embed dim 64–256; 2–6 encoder layers; 4–8 heads; window/patch length 16–64; dropout 0.1–0.3; AdamW lr 1e-4–5e-4.', 'notes': 'Prefer relative position encodings or hierarchical/pooling to handle L=1000 efficiently; add convolutional stem for local patterns. ([arxiv.org](https://arxiv.org/abs/2302.09818?utm_source=openai))'}
4. {'name': 'Self-supervised pretrain + linear/probe head (TimeMAE, TS2Vec, TS-TCC/CA-TCC)', 'why': 'Improves representation quality and label efficiency; strong on varied TS domains.', 'typical_hparams': 'pretrain 50–200 epochs with masking/contrastive tasks; fine-tune 20–100 epochs; lr 1e-3 (CNN) or 1e-4 (Transformer).', 'notes': 'Useful when labels are scarce or class boundaries are subtle. ([arxiv.org](https://arxiv.org/abs/2303.00320?utm_source=openai))'}

RECENT PAPERS:
- Bake off redux: a review and experimental evaluation of recent time series classification algorithms (DMKD, 2024): Comprehensive 2024 re-evaluation showing MultiROCKET+Hydra and HIVE-COTEv2 as top performers across many datasets; sets current baselines. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-024-01022-1))
- ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification (KDD 2024): Combines class-specific shapelets with transformer encoders; highest accuracy ranking on 30 UEA datasets reported. ([arxiv.org](https://arxiv.org/abs/2405.14608))
- CTNet: Multivariate time series classification with crucial timestamps guidance (ESWA, 2024): Gaussian-prior transformer encoder with data-driven masking and context-aware positional encodings; SOTA claims on UEA benchmarks. ([dl.acm.org](https://dl.acm.org/doi/10.1016/j.eswa.2024.124591?utm_source=openai))
- VSFormer: Value and Shape-Aware Transformer with Prior-Enhanced Self-Attention for MTSC (arXiv, 2024): Introduces class-specific prior-enhanced attention that integrates shape and value cues; outperforms SOTA across UEA archive. ([arxiv.org](https://arxiv.org/abs/2412.16515?utm_source=openai))
- ST-Tree with Interpretability for MTSC (arXiv, 2024): Swin-Transformer backbone coupled with interpretable neural decision tree, trading accuracy with interpretability. ([arxiv.org](https://arxiv.org/abs/2411.11620?utm_source=openai))
- MD-Former: Multiscale Dual Branch Transformer for MTSC (Sensors, 2025): Dual-branch multiscale transformer evaluated on UEA; open PyTorch implementation details and settings. ([mdpi.com](https://www.mdpi.com/1424-8220/25/5/1487))
- FormerTime: Hierarchical Multi-Scale Representations for MTSC (WWW 2023): Hierarchical transformer with temporal reduction attention and contextual positional encoding; strong results on UEA datasets. ([arxiv.org](https://arxiv.org/abs/2302.09818?utm_source=openai))
- TimeMAE (2023) and TS2Vec (2021): Self-supervised representations for time series: Masked autoencoders and hierarchical contrastive pretraining yielding strong downstream classification across UCR/UEA. ([arxiv.org](https://arxiv.org/abs/2303.00320?utm_source=openai))

==================================================
