{
  "query": "ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Scope and setup: We targeted 2024–2025 peer‑reviewed and preprint works on deep learning for multiclass ECG arrhythmia classification on MIT‑BIH, emphasizing 5‑class AAMI groupings (N, S, V, F, Q), sequence models, and PyTorch‑implementable architectures. MIT‑BIH comprises 48 two‑lead ambulatory ECG records sampled at 360 Hz; two cardiologists annotated ≈110k beats. These characteristics align with the user’s sequence input (1000 × 2). ([physionet.org](https://physionet.org/content/mitdb/1.0.0/?utm_source=openai))\nCore recent SOTA papers: (1) A hierarchical CNN+BiLSTM with attention reported 5‑class MIT‑BIH cross‑validated results of accuracy 0.997, F1 0.993 (also detailing three CNN blocks, three BiLSTM blocks, attention, Mish activations, layer norm, dropout). ([journals.sagepub.com](https://journals.sagepub.com/doi/10.1177/20552076241278942?utm_source=openai)) (2) A Tiny Transformer optimized for low‑power deployment achieved 98.97% (5‑class MIT‑BIH) with only ~6k params; 8‑bit inference on GAP9 took 4.28 ms (0.09 mJ), highlighting strong accuracy–efficiency trade‑offs. ([arxiv.org](https://arxiv.org/abs/2402.10748)) (3) CAT‑Net (Conv + attention + Transformer) reported 99.14% accuracy and 94.69% macro‑F1 on 5‑class MIT‑BIH, using SMOTE‑Tomek to mitigate imbalance—demonstrating a robust hybrid design that captures local morphology and global context. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai)) (4) A CWT + multi‑branch Transformer attained 99.38% accuracy and 98.65% F1 on MIT‑BIH, showing that time–frequency front‑ends with attention are highly competitive. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S2405844024021789?utm_source=openai)) (5) A pruning‑oriented lightweight network verified on MIT‑BIH reached 99.24% accuracy with a ~4.37 MB model and ≈48–49% reductions in params/FLOPs vs. its baseline, offering a compact alternative. ([dl.acm.org](https://dl.acm.org/doi/10.1016/j.asoc.2024.111340?utm_source=openai)) (6) rECGnition_v2.0 (DPN + depthwise separable conv + self‑attentive canonical fusion) reported 98.07% accuracy and 98.05% F1 on MIT‑BIH (10‑class), at ~82.7M FLOPs per sample, balancing accuracy with modest compute. ([arxiv.org](https://arxiv.org/abs/2502.16255?utm_source=openai))\nSupporting syntheses: A 2024 survey centered on MIT‑BIH catalogs methodological trends (including graph‑based and deep models), while a 2025 review on transformers in biosignals motivates attention mechanisms for ECG time series. ([mdpi.com](https://www.mdpi.com/2079-3197/12/2/21?utm_source=openai)) Methodological comparability remains sensitive to protocol: many high scores stem from random or cross‑validated beat splits; inter‑patient splits per AAMI guidelines provide stricter generalization (e.g., classic references and recent overviews emphasize AAMI 5 superclasses and inter‑patient evaluation). ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/15248536/?utm_source=openai))\nImplementation fit: For 1000×2 windows, hybrid Conv‑Transformer stacks are well‑matched: 1D convolutions downsample and capture wave morphology; transformer layers model long‑range rhythm context across ~2.8 s windows; class‑imbalance tactics (e.g., focal/context‑aware loss or SMOTE‑Tomek) improve minority classes (S, F, Q). Public PyTorch code exists for a closely related hybrid (ECGTransForm: multi‑scale Conv + bidirectional Transformer + context‑aware loss), with published 5‑class MIT‑BIH macro‑F1 ≈94.3% and strong per‑class metrics—making replication and adaptation straightforward for two‑lead inputs. ([github.com](https://github.com/emadeldeen24/ECGTransForm))\nConclusion: Across 2024–2025 works, hybrid Conv‑Transformer models repeatedly deliver top accuracy on 5‑class MIT‑BIH while remaining practical. Considering accuracy, reproducibility (PyTorch availability), and compute, a multi‑scale Conv front‑end plus a lightweight bidirectional Transformer (as in ECGTransForm/CAT‑style hybrids) best matches the user’s 1000×2 sequence setup and class schema.",
  "key_findings": [
    "Hybrid Conv + RNN/Attention remains highly competitive: a hierarchical CNN+BiLSTM with attention reported 5‑class MIT‑BIH accuracy 0.997 and F1 0.993 (5‑fold CV), with three CNN and three BiLSTM blocks plus attention and Mish activations. ([journals.sagepub.com](https://journals.sagepub.com/doi/10.1177/20552076241278942?utm_source=openai))",
    "Transformers can be extremely efficient without sacrificing accuracy: a Tiny Transformer achieved 98.97% on 5‑class MIT‑BIH with ~6k parameters; 8‑bit inference ran in 4.28 ms consuming 0.09 mJ on GAP9, underscoring deployment readiness. ([arxiv.org](https://arxiv.org/abs/2402.10748))",
    "Conv‑Attention‑Transformer hybrids achieve state‑of‑the‑art macro‑F1: CAT‑Net reached 99.14% accuracy and 94.69% macro‑F1 on 5‑class MIT‑BIH, aided by SMOTE‑Tomek to address class imbalance. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))",
    "Time–frequency front‑ends with multi‑branch Transformers also excel: CWT + Transformer achieved 99.38% accuracy and 98.65% F1 on MIT‑BIH, highlighting strong performance when combining spectral and temporal cues. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S2405844024021789?utm_source=openai))",
    "Model compression yields compact, accurate classifiers: a pruned lightweight network validated 99.24% accuracy on MIT‑BIH with ~4.37 MB size and ~48–49% fewer params/FLOPs vs. baseline, suggesting efficient deployment options. ([dl.acm.org](https://dl.acm.org/doi/10.1016/j.asoc.2024.111340?utm_source=openai))",
    "Open‑source PyTorch hybrid (ECGTransForm) provides a practical template: multi‑scale Conv + bidirectional Transformer + context‑aware loss obtained ≈94.3% macro‑F1 on 5‑class MIT‑BIH with strong per‑class precision/recall, facilitating reproducible implementation. ([github.com](https://github.com/emadeldeen24/ECGTransForm))"
  ],
  "recommended_approaches": [
    "Adopt a multi‑scale Conv + bidirectional Transformer architecture with a class‑imbalance‑aware loss (e.g., ECGTransForm‑style design in PyTorch): 1D convolutions (multi‑scale kernels, channel recalibration) to downsample and extract morphology from 1000×2 inputs, followed by 1–2 lightweight bidirectional Transformer encoder layers for long‑range rhythm context, and a focal/context‑aware loss to boost minority classes (S, F, Q). This matches the user’s 1000×2 two‑lead sequences, has published strong MIT‑BIH 5‑class results with available PyTorch code for rapid adaptation, and balances accuracy with moderate compute versus heavier RNN stacks or image front‑ends. ([github.com](https://github.com/emadeldeen24/ECGTransForm))"
  ],
  "recent_papers": [
    {
      "title": "Hierarchical deep learning for autonomous multi‑label arrhythmia detection and classification on real‑world wearable ECG data (2024, Digital Health)",
      "contribution": "CNN+BiLSTM with attention; on MIT‑BIH 5‑class benchmark reported accuracy 0.997, F1 0.993; detailed architecture (3 CNN + 3 BiLSTM + attention) and preprocessing choices. ([journals.sagepub.com](https://journals.sagepub.com/doi/10.1177/20552076241278942?utm_source=openai))"
    },
    {
      "title": "A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (2024, IEEE TBCAS)",
      "contribution": "~6k‑parameter Transformer reaching 98.97% (MIT‑BIH, 5 classes) with 8‑bit deployment on GAP9 (4.28 ms, 0.09 mJ), demonstrating excellent accuracy–efficiency trade‑offs. ([arxiv.org](https://arxiv.org/abs/2402.10748))"
    },
    {
      "title": "CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (2024, Biomed. Signal Process. Control)",
      "contribution": "Hybrid Conv‑Attention‑Transformer with imbalance handling (SMOTE‑Tomek); 99.14% accuracy, 94.69% macro‑F1 on 5‑class MIT‑BIH. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))"
    },
    {
      "title": "Enhancing ECG classification with continuous wavelet transform and multi‑branch transformer (2024, Heliyon/Elsevier Open Access)",
      "contribution": "CWT time–frequency front‑end plus multi‑branch Transformer; 99.38% accuracy and 98.65% F1 on MIT‑BIH, validating spectral+attention pipelines. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S2405844024021789?utm_source=openai))"
    },
    {
      "title": "Pruned lightweight neural networks for arrhythmia classification with clinical 12‑Lead ECGs (2024, Applied Soft Computing)",
      "contribution": "Lightweight/pruned models generalize to MIT‑BIH with 99.24% accuracy; ~4.37 MB model, ~48–49% param/FLOP reductions vs. baseline. ([dl.acm.org](https://dl.acm.org/doi/10.1016/j.asoc.2024.111340?utm_source=openai))"
    },
    {
      "title": "rECGnition_v2.0: Self‑Attentive Canonical Fusion with Dual‑Pathway Network (2025, arXiv)",
      "contribution": "DPN + depthwise separable conv + SACC fusion; on MIT‑BIH achieved 98.07% accuracy and 98.05% F1 (10 classes) at ~82.7M FLOPs/sample, illustrating strong accuracy with moderate compute. ([arxiv.org](https://arxiv.org/abs/2502.16255?utm_source=openai))"
    },
    {
      "title": "ECGTransForm: Empowering adaptive ECG arrhythmia classification framework with bidirectional transformer (2024, Biomed. Signal Process. Control + PyTorch code)",
      "contribution": "Multi‑scale Conv + bidirectional Transformer + context‑aware loss; public PyTorch implementation; ≈94.3% macro‑F1 on 5‑class MIT‑BIH with strong per‑class precision/recall. ([github.com](https://github.com/emadeldeen24/ECGTransForm))"
    },
    {
      "title": "Unraveling Arrhythmias with Graph‑Based Analysis: A Survey of the MIT‑BIH Database (2024, MDPI Computation)",
      "contribution": "Survey of methods on MIT‑BIH, contextualizing recent deep models and evaluation practices. ([mdpi.com](https://www.mdpi.com/2079-3197/12/2/21?utm_source=openai))"
    }
  ],
  "confidence": 0.73,
  "timestamp": 1758296819,
  "generated_time": "2025-09-19 10:46:59",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}