LITERATURE REVIEW
=================

Query: sequence classification time series machine learning multiclass sequence classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-15 00:35:05
Confidence: 0.78

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
From 2023–2025, time-series sequence classification has seen two parallel tracks: (1) continued dominance of efficient random-convolution feature transforms (the ROCKET family) and streamlined CNNs, and (2) a surge of attention-, state‑space-, and self‑supervised methods adapted specifically for multivariate time series classification (MTSC). MultiRocket extends MiniRocket with first‑order differencing and multiple pooling operators, yielding ~50k features per series and accuracy competitive with heavyweight ensembles but at a fraction of the compute, making it a strong, scalable baseline for inputs like shape (1000, 2) and 5 classes; pruning/selection variants (Detach‑ROCKET, SelF‑Rocket) further reduce feature count while maintaining or improving accuracy. In parallel, compact CNNs such as InceptionTime derivatives and the 2024 LITE architecture retain top performance with dramatically fewer parameters, remaining attractive when you need end‑to‑end PyTorch training on GPUs. ([ar5iv.org](https://ar5iv.org/pdf/2102.00457))

Emerging 2024–2025 models adapt transformers and state‑space models (SSMs) to MTSC: ShapeFormer and VSFormer inject shapelet/class‑prior information and improved encodings for stronger class discrimination across the UEA multivariate archive; Mamba‑based TSCMamba and MixMamba bring linear‑time long‑context modeling to classification; and weak‑/self‑supervised approaches (Series2Vec, TimeMAE/Ti‑MAE) improve data efficiency and transfer. Practical notes for MTSC include that concatenating channels can work as well as specialized multivariate heads in some settings, and that positional encodings and patching/tokenization matter for transformer‑style models. New large‑scale benchmarks (Monster, 2025) emphasize scalability beyond UCR/UEA. For your setting—moderate length (1000), two channels, five classes—the most reliable starting points are MultiRocket→linear classifier and an InceptionTime/LITE CNN; consider transformer/SSM or self‑supervised pretraining when you have ample compute or limited labels. ([arxiv.org](https://arxiv.org/abs/2405.14608?utm_source=openai))

KEY FINDINGS:
1. ROCKET-family transforms remain a state-of-the-art accuracy/efficiency trade-off; MultiRocket (~49,728 features default) is significantly more accurate than MiniRocket and near top ensembles while being orders faster. Use it as a strong baseline for (1000, 2) with a linear or ridge classifier. ([ar5iv.org](https://ar5iv.org/pdf/2102.00457))
2. Compact CNNs still shine: InceptionTime variants and the 2024 LITE architecture achieve near–SOTA with far fewer parameters and faster training, making them robust end-to-end choices in PyTorch. ([emergentmind.com](https://www.emergentmind.com/papers/1909.04939?utm_source=openai))
3. Transformers for MTSC benefit from task-specific inductive bias: shapelet/class-prior modules and improved positional/temporal encodings (e.g., ShapeFormer, VSFormer) yield consistent gains on UEA datasets. ([arxiv.org](https://arxiv.org/abs/2405.14608?utm_source=openai))
4. State-space models (Mamba) bring linear-time long-range modeling; early MTSC adaptations (TSCMamba, MixMamba) report gains on standard benchmarks with better scalability for long sequences. ([arxiv.org](https://arxiv.org/abs/2406.04419?utm_source=openai))
5. Self-/weakly supervised pretraining (Series2Vec, TimeMAE/Ti‑MAE, TimeMIL) is valuable when labels are scarce, typically improving downstream classification and transfer. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-024-01043-w?utm_source=openai))
6. Channel handling: simple channel concatenation can match specialized multivariate classifiers in some cases; try both concatenation and explicit multivariate models. ([arxiv.org](https://arxiv.org/abs/2308.15223?utm_source=openai))
7. Benchmarking is shifting toward larger, more realistic datasets (Monster repository), so favor methods that scale well in features and training time. ([arxiv.org](https://arxiv.org/html/2502.15122v1?utm_source=openai))

RECOMMENDED APPROACHES:
1. {'name': 'MultiRocket + linear (ridge/logistic) head', 'why': 'Top accuracy with minimal training; CPU-friendly feature extraction; robust on short-to-moderate sequences like length 1000.', 'how': 'Default ~50k features (raw + diff, 4 pool ops); RidgeClassifierCV or logistic with L2; standardize features; try 10k vs 50k features for speed/accuracy trade-off.', 'typical_hparams': 'features: 10k–50k; C (L2): 1e-3–1; class_weight: balanced if needed.', 'notes': 'For 2 channels, either concatenate channels or compute features per channel then concatenate. ([ar5iv.org](https://ar5iv.org/pdf/2102.00457))'}
2. {'name': 'InceptionTime or LITE (lightweight CNN) in PyTorch', 'why': 'Strong end-to-end baselines; stable training; good inductive bias for local-to-mid-range patterns.', 'how': 'Stack 6–12 Inception blocks with residuals; kernel sizes spanning small/medium/large (e.g., 9–39); global average pooling → linear head.', 'typical_hparams': 'channels: 2; k-sizes: [9,19,39] or [10,20,40]; filters per block: 32–64; dropout: 0.1–0.3; label smoothing: 0–0.1; AdamW lr 1e-3 with cosine decay.', 'notes': 'Prefer LITE when compute/latency is tight; it preserves accuracy with ~2–3% of InceptionTime params. ([emergentmind.com](https://www.emergentmind.com/papers/1909.04939?utm_source=openai))'}
3. {'name': 'Transformer with class-/shapelet-aware modules (e.g., ShapeFormer or VSFormer)', 'why': 'Captures long-range dependencies; class-specific priors/shapelets improve discriminability on multivariate data.', 'how': 'Tokenize time (patch 16–32) per channel; add shapelet filters or supervised priors; 4–6 encoder layers, d_model 128–256, 4–8 heads; use relative/learnable positional encodings.', 'typical_hparams': 'dropout 0.1; weight decay 1e-4–1e-3; batch 32–128 at L=1000.', 'notes': 'Use mixed precision and gradient checkpointing if VRAM is limited. ([arxiv.org](https://arxiv.org/abs/2405.14608?utm_source=openai))'}
4. {'name': 'Mamba-based classifier (TSCMamba/MixMamba)', 'why': 'Linear-time sequence modeling with strong long-range capture and good scalability for longer inputs.', 'how': 'Embed per-timestep features, stack 4–8 Mamba blocks, temporal pooling → classifier; optionally fuse time- and frequency-view features.', 'typical_hparams': 'd_model 128–256; dropout 0.1–0.2; lr 1e-3; 50–150 epochs.', 'notes': 'Promising on longer contexts; for length 1000 it’s competitive and efficient. ([arxiv.org](https://arxiv.org/abs/2406.04419?utm_source=openai))'}
5. {'name': 'Self-/weakly supervised pretraining then linear/probe fine‑tune (Series2Vec, TimeMAE/Ti‑MAE, TimeMIL)', 'why': 'Improves label efficiency and robustness; helpful if sample size is small or class imbalance exists.', 'how': 'Pretrain encoder (contrastive or masked reconstruction) on unlabeled series; freeze or fine‑tune with cross-entropy; optionally MIL pooling for sparse/localized cues.', 'typical_hparams': 'pretrain 50–200 epochs; probe lr 1e-3–1e-4; temperature 0.1 (contrastive); mask ratio 0.3–0.7.', 'notes': 'Transfer learned representations across related datasets/channels. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-024-01043-w?utm_source=openai))'}

RECENT PAPERS:
- ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification (2024): Combines class‑specific shapelets with transformer encoders to enhance class discrimination; reports top accuracy on 30 UEA MTSC datasets.
- VSFormer: Value- and Shape‑Aware Transformer with Prior‑Enhanced Self‑Attention for MTSC (2024): Injects class-specific priors into positional encoding and attention; improves on UEA benchmarks.
- TSCMamba: Mamba Meets Multi‑View Learning for Time Series Classification (2024): Fuses time- and frequency-domain views with Mamba SSM for efficient, accurate MTSC.
- MixMamba: Time Series Modeling with Adaptive Expertise (Information Fusion, 2024): Mixture‑of‑experts with Mamba blocks; competitive forecasting and classification with linear-time scaling.
- Series2Vec: Similarity‑based Self‑Supervised Representation Learning for TSC (DMKD, 2024): Self-supervised objective predicts intra‑pair similarity (temporal/spectral) without heavy augmentations; strong with limited labels.
- TimeMAE / Ti‑MAE (2023): Masked autoencoder pretraining tailored to time series; improves downstream classification and transfer.
- ECRTime (2024): Integrates retrieval with classification to mitigate inter‑class similarity/intra‑class inconsistency; surpasses strong deep baselines on UCR.
- TimeMIL (2024): Weakly supervised, time‑aware multiple‑instance learning for MTSC; improves localization of discriminative subsequences.
- Detach‑ROCKET and SelF‑Rocket (2023–2024): Feature detachment/selection on ROCKET features to prune 90%+ of features while matching or improving accuracy.
- Monster: Monash Scalable Time Series Evaluation Repository (2025): Large-scale benchmark repository emphasizing scalability for TSC beyond UCR/UEA.

==================================================
