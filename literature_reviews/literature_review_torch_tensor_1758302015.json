{
  "query": "ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Problem framing: Your data are 1D, two-lead ECG sequences shaped (T=1000, C=2) with five AAMI superclasses (N, S, V, F, Q) from the MIT-BIH Arrhythmia Database. MIT-BIH provides 48 two-channel, 30-minute recordings sampled at 360 Hz with ≈110k annotated beats; AAMI grouping and patient‑wise (inter‑patient) evaluation are standard best practices due to strong class imbalance. A 1000‑sample window corresponds to ≈2.78 s at 360 Hz, which comfortably covers a heartbeat and neighborhood context for beat‑level classification. ([physionet.org](https://physionet.org/content/mitdb/1.0.0/mitdbdir/samples/?utm_source=openai))\nMethodological signals from recent literature (2024–2025): A systematic review (2017–2024) stresses that many ECG papers over‑report results because they deviate from inter‑patient splits and AAMI compliance; it proposes E3C criteria (Embedded, Clinical, Comparative) and compares accuracy with compute/energy to encourage deployable models. This supports prioritizing studies that (a) use patient‑independent evaluation on MIT‑BIH, (b) report class‑aware metrics, and (c) quantify computational cost. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))\nStrong recent models on MIT‑BIH five‑class classification:\n- Tiny Transformer (TBCAS 2024): A very compact 1D Transformer tailored for ECG; ≈6k parameters, 8‑bit inference; 98.97% accuracy on the five AAMI classes; on a GAP9 MCU, inference is 4.28 ms using 0.09 mJ—showcasing excellent accuracy/efficiency for embedded or real‑time use. Architecture: shallow conv/embedding + positional encoding + a few lightweight self‑attention blocks + MLP head; trained with augmentation for motion‑artifact robustness. ([arxiv.org](https://arxiv.org/abs/2402.10748))\n- MB‑MHA‑TCN (Sensors 2024): A multi‑branch temporal CNN with different kernel sizes/dilations to span time scales, fused with multi‑head self‑attention; uses focal loss, K‑means undersampling + SMOTE‑Tomek to mitigate imbalance; Bayesian hyperparameter optimization. On MIT‑BIH (five classes, 5‑fold CV): 98.75% ACC, 96.89% F1, with improved minority‑class recognition. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124?utm_source=openai))\n- Multimodal frequency–image pipeline (2024): Converts beats to RP/GAF/MTF spectro‑images and applies a CNN with frequency‑channel attention (FCA). Reports 99.6% accuracy on MIT‑BIH (five classes), highlighting that image transforms with attention can be very strong but add preprocessing and memory cost compared to compact 1D models. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38627498/?utm_source=openai))\nAdditional 2024–2025 context: “CardioAttentionNet” combines BiLSTM, multi‑head attention, and depthwise separable conv for portability, reporting high external performance and category‑wise accuracies; useful as evidence that attention plus lightweight convolutions travel well to constrained devices, though task setups vary across datasets. ([frontiersin.org](https://www.frontiersin.org/journals/cardiovascular-medicine/articles/10.3389/fcvm.2024.1473482/full))\nImplementation relevance to your setting (T=1000, C=2, five classes): All above 1D sequence models naturally accept multi‑lead inputs by setting input channels = 2. For MIT‑BIH, common practice is R‑peak–centered segmentation with ±context; class imbalance remains a key challenge. The MB‑MHA‑TCN explicitly addresses imbalance (focal loss + SMOTE‑Tomek), while the Tiny Transformer reports robustness to motion artifacts and quantized inference. Given your need to balance accuracy and efficiency on potentially large datasets, the Tiny Transformer’s combination of ≈6k params, near‑SOTA accuracy, and proven embedded latency/energy is compelling; pairing it with class‑balanced or focal loss and modest augmentation aligns with the 2025 review’s E3C guidance. ([arxiv.org](https://arxiv.org/abs/2402.10748))\nEvaluation and protocol notes: Use inter‑patient splits (e.g., DS1/DS2 patient partitions) and report macro‑F1, per‑class F1/Sensitivity, plus confusion matrices to ensure minority‑class reliability; this split is widely adopted for fair comparison on MIT‑BIH. ([github.com](https://github.com/windniu/ecg_classification?utm_source=openai))",
  "key_findings": [
    "A compact 1D Tiny Transformer reached 98.97% five‑class accuracy on MIT‑BIH with only ≈6k parameters; 8‑bit inference ran in 4.28 ms using 0.09 mJ on a GAP9 MCU, indicating excellent accuracy–efficiency trade‑offs for large‑scale or embedded deployment. ([arxiv.org](https://arxiv.org/abs/2402.10748))",
    "A multi‑branch, multi‑head‑attention Temporal CNN (MB‑MHA‑TCN) achieved 98.75% ACC and 96.89% F1 on five AAMI classes, using dilated multi‑scale convolutions, attention fusion, focal loss, and SMOTE‑Tomek augmentation to boost minority‑class recognition. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124?utm_source=openai))",
    "Image‑based multimodal FCA (RP/GAF/MTF + CNN) reported 99.6% five‑class accuracy on MIT‑BIH, but requires extra preprocessing and memory compared with compact 1D sequence models, which may affect throughput on large datasets or limited hardware. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38627498/?utm_source=openai))",
    "A 2017–2024 systematic review highlights that many ECG studies lack inter‑patient evaluation and embedded feasibility reporting; adopting AAMI compliance, patient‑independent splits, and compute/energy metrics is recommended for fair, practical benchmarking. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))"
  ],
  "recommended_approaches": [
    "Use a TinyTransformer-1D with a lightweight conv/embedding stem accepting two input leads (C=2), sinusoidal positional encoding, 1–2 shallow self‑attention blocks, and an MLP classifier head; train with class‑balanced cross‑entropy or focal loss and modest augmentation (e.g., jitter, scaling). This architecture matches your (1000,2) time‑series format, is straightforward to implement in PyTorch, has demonstrated ≈99% five‑class accuracy on MIT‑BIH, and offers exceptional computational efficiency for large‑scale training or deployment (≈6k params; proven 8‑bit inference). ([arxiv.org](https://arxiv.org/abs/2402.10748))"
  ],
  "recent_papers": [
    {
      "title": "A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (IEEE TBCAS, 2024)",
      "contribution": "Compact 1D Transformer tailored for ECG with ≈6k params; 98.97% five‑class MIT‑BIH accuracy; quantized 8‑bit inference at 4.28 ms and 0.09 mJ on GAP9, demonstrating state‑of‑the‑art efficiency. ([arxiv.org](https://arxiv.org/abs/2402.10748))"
    },
    {
      "title": "Accurate Arrhythmia Classification with Multi‑Branch, Multi‑Head Attention Temporal Convolutional Networks (Sensors, 2024)",
      "contribution": "Multi‑scale dilated CNN branches fused via multi‑head attention; imbalance handling via focal loss and SMOTE‑Tomek; 98.75% ACC and 96.89% F1 on five AAMI classes. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124?utm_source=openai))"
    },
    {
      "title": "Multimodal ECG heartbeat classification with frequency‑channel attention (2024)",
      "contribution": "Transforms beats to RP/GAF/MTF spectro‑images and applies CNN+FCA; reports 99.6% five‑class accuracy on MIT‑BIH, illustrating the upside of image‑domain fusion at added preprocessing cost. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/38627498/?utm_source=openai))"
    },
    {
      "title": "A Systematic Review of ECG Arrhythmia Classification: Adherence to Standards, Fair Evaluation, and Embedded Feasibility (2025)",
      "contribution": "Synthesizes 2017–2024 studies; advocates AAMI compliance, inter‑patient splits, and embedded metrics (E3C criteria) to ensure clinically/operationally meaningful comparisons. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))"
    }
  ],
  "confidence": 0.78,
  "timestamp": 1758302015,
  "generated_time": "2025-09-19 12:13:35",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}