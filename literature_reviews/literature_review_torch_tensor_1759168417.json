{
  "query": "ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Problem framing: You need multiclass sequence classification on ECG signals from the MIT-BIH Arrhythmia Database with samples shaped (1000, 2), five classes, using PyTorch. MIT-BIH contains 48 half‑hour, two‑channel ambulatory ECG records (47 subjects) digitized at 360 Hz; annotations were made by multiple cardiologists. This matches your two‑lead, sequence‑modeling setup. ([physionet.org](https://physionet.org/content/mitdb/1.0.0/mitdbdir/samples/?utm_source=openai))\n\nWhat the recent literature (2024–2025) shows:\n- Strongest MIT‑BIH 5‑class results in peer‑reviewed literature come from hybrid CNN+attention+Transformer designs. CAT‑Net (BSPC, 2024) reports 99.14% overall accuracy and 94.69% macro‑F1 on 5‑class MIT‑BIH, using wavelet denoising, beat‑centered segmentation, and SMOTE‑Tomek for imbalance; code is public. A 2025 comparative study found a simpler Hierarchical Attention Network (HAN) achieved 98.55% on MIT‑BIH while using 15.6× fewer parameters than CAT‑Net (trade‑off: slight accuracy drop but much lower complexity and improved interpretability). ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))\n- If compute is constrained, a Tiny Transformer (2024) with only ~6k parameters reached 98.97% accuracy on MIT‑BIH 5‑class with 8‑bit quantized inference, running in 4.28 ms and ~0.09 mJ on a GAP9 microcontroller—evidence that carefully designed attention models can be extremely efficient while remaining accurate. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))\n- Technique innovations that consistently help: (a) segmentation that adapts to RR intervals and adds relative heart‑rate context markedly boosts PAC/PVC detection; a 2024 study reported sensitivities of 99.81% (Normal), 99.08% (PVC), and 97.83% (PAC) on MIT‑BIH, plus strong waveform delineation F1 scores. (b) Two‑lead/channel‑attention modules improve performance when both leads are available, achieving ~99.18% accuracy/F1 on MIT‑BIH with a lightweight CNN+channel‑attention architecture. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0010482524011478?utm_source=openai))\n- For feature fusion and interpretability at moderate cost, rECGnition v2.0 (2025) combines dual‑pathway CNNs with a Self‑Attentive Canonical Correlation module, reporting 98.07% accuracy and 98.05% F1 on 10‑class MIT‑BIH at ~82.7M FLOPs per sample (useful as a computational reference point). ([arxiv.org](https://arxiv.org/abs/2502.16255?utm_source=openai))\n- Evaluation protocol matters: ANSI/AAMI EC57 recommends class groupings and metrics beyond accuracy; many high MIT‑BIH scores arise from record‑wise splits that leak subject identity. Studies comparing intra‑ vs inter‑patient schemes show notable drops under inter‑patient splits—so use patient‑wise splits and report macro‑F1, per‑class sensitivity/PPV. ([mdpi.com](https://www.mdpi.com/1424-8220/17/11/2445?utm_source=openai))\n- Methodological trend (survey 2025): transformers help capture long‑range rhythm dependencies; practical wins often come from hybrid CNN–Transformer backbones, patching/tokenization of long waveforms, and efficiency tricks (sparse attention, depthwise 1D convs, pruning/quantization). ([link.springer.com](https://link.springer.com/article/10.1007/s10462-025-11259-x?utm_source=openai))\n\nComputational/implementation notes for your data shape (1000, 2):\n- All cited architectures operate on 1D sequences and extend naturally from 1 to 2 leads by setting input channels=2 in 1D CNN blocks and using channel‑attention if desired. CAT‑Net and Tiny Transformer both accept fixed‑length windows; 1000 samples (~2.8 s at 360 Hz) is typical for beat‑centered or short multi‑beat windows. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))\n- If training on 5 AAMI classes (N/S/V/F/Q), adopt patient‑wise splits (e.g., DS1/DS2 protocol) and report macro‑F1 alongside accuracy. Use class‑balanced loss or SMOTE‑Tomek as in CAT‑Net to stabilize minority classes. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))\n\nBottom line: For highest accuracy with proven MIT‑BIH results and available PyTorch code, a two‑lead adaptation of CAT‑Net is the most compelling. If you must optimize for very low compute, the Tiny Transformer reaches ~99% with orders‑of‑magnitude fewer parameters, but CAT‑Net remains the accuracy leader on 5‑class MIT‑BIH in 2024 peer‑reviewed results. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))",
  "key_findings": [
    "Hybrid CNN–attention–Transformer backbones achieve the strongest peer‑reviewed MIT‑BIH 5‑class results: CAT‑Net reports 99.14% accuracy and 94.69% macro‑F1 with wavelet denoising, beat segmentation, and SMOTE‑Tomek; open‑source PyTorch code is available. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))",
    "A simpler Hierarchical Attention Network attains 98.55% test accuracy on MIT‑BIH with 15.6× fewer parameters than CAT‑Net, offering a favorable accuracy–complexity trade‑off and clearer attention maps for interpretability. ([arxiv.org](https://arxiv.org/abs/2504.03703))",
    "For resource‑constrained deployment, a Tiny Transformer with ~6k parameters achieves 98.97% MIT‑BIH 5‑class accuracy with 8‑bit inference, 4.28 ms latency and ~0.09 mJ per inference on GAP9, demonstrating excellent efficiency. ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))",
    "Adaptive beat segmentation plus relative heart‑rate context significantly improves minority‑class detection (PAC/PVC): reported sensitivities on MIT‑BIH were 99.81% (Normal), 99.08% (PVC), 97.83% (PAC), and strong delineation F1 scores. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0010482524011478?utm_source=openai))",
    "Two‑lead models with channel attention leverage both MIT‑BIH leads effectively, reaching ~99.18% accuracy/F1; channel attention is a low‑cost addition for multi‑lead inputs like your (1000, 2) tensors. ([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/40942969/?utm_source=openai))",
    "Use patient‑wise (inter‑patient) splits per AAMI EC57 guidance; record‑wise splits inflate numbers. Inter‑ vs intra‑patient studies show large performance deltas, so report macro‑F1 and per‑class metrics. ([mdpi.com](https://www.mdpi.com/1424-8220/17/11/2445?utm_source=openai))"
  ],
  "recommended_approaches": [
    "CAT‑Net (CNN + channel attention + Transformer encoder), adapted for two leads: 1D Conv stem (depthwise‑separable convs, channels=2), squeeze‑and‑excitation/channel‑attention, 1–2 Transformer encoder blocks (multi‑head self‑attention with relative position bias) over patch‑wise tokens from the 1000×2 window, followed by global pooling and a 5‑way classifier. Train with wavelet denoising + beat‑centered or short multi‑beat windows, patient‑wise splits, class‑balanced loss (or SMOTE‑Tomek on the training set), and report macro‑F1. Justification: state‑of‑the‑art MIT‑BIH 5‑class accuracy (99.14%, macro‑F1 94.69%) with public PyTorch implementation; architecture natively fits 1D sequences and scales to two leads with channel attention, balancing accuracy and efficiency. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))"
  ],
  "recent_papers": [
    {
      "title": "CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (BSPC, 2024)",
      "contribution": "Hybrid CNN+attention+Transformer; 5‑class MIT‑BIH accuracy 99.14% and macro‑F1 94.69%; uses wavelet denoising, beat segmentation, SMOTE‑Tomek; code available. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))"
    },
    {
      "title": "Hierarchical Attention Network for Interpretable ECG‑based Heart Disease Classification (arXiv, 2025)",
      "contribution": "HAN adapted to ECG; 98.55% on MIT‑BIH with 15.6× fewer parameters than CAT‑Net; better interpretability via attention maps. ([arxiv.org](https://arxiv.org/abs/2504.03703))"
    },
    {
      "title": "A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (arXiv, 2024)",
      "contribution": "~6k‑param Transformer reaches 98.97% MIT‑BIH 5‑class accuracy with 8‑bit inference; 4.28 ms and ~0.09 mJ per inference on GAP9 (strong efficiency evidence). ([arxiv.org](https://arxiv.org/abs/2402.10748?utm_source=openai))"
    },
    {
      "title": "ECG classification via adaptive beat segmentation and relative heart‑rate integration (Computers in Biology and Medicine, 2024)",
      "contribution": "Shows segmentation and relative HR features markedly improve PAC/PVC: sensitivities 99.81% (Normal), 99.08% (PVC), 97.83% (PAC) on MIT‑BIH; also reports high delineation F1 scores. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0010482524011478?utm_source=openai))"
    }
  ],
  "confidence": 0.72,
  "timestamp": 1759168417,
  "generated_time": "2025-09-29 12:53:37",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}