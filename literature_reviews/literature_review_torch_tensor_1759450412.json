{
  "query": "ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Problem framing: You have 1D sequential ECG signals (1000 time steps × 2 leads) from MIT‑BIH, targeting 5-class heartbeat-level arrhythmia recognition (commonly the AAMI N, S, V, F, Q superclasses). State-of-the-art work in 2024–2025 on MIT‑BIH increasingly uses hybrid CNN+Transformer or compact Transformer variants, with careful handling of class imbalance and (ideally) inter‑patient evaluation (DS1/DS2) to avoid patient leakage. ([physionet.org](https://www.physionet.org/physiobank/database/html/mitdbdir/tables.htm?utm_source=openai))\n\nRecent SOTA models: (1) CAT‑Net (Biomedical Signal Processing and Control, 2024) fuses 1D CNNs with a Transformer encoder and channel attention. On MIT‑BIH 5‑class classification it reports 99.14% overall accuracy and 94.69% macro‑F1; it also evaluates class‑balancing schemes and adopts SMOTE‑Tomek to improve minority‑class performance. Architecture is single‑lead but easily extended to multi‑lead via channel-wise convolutions. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))\n\n(2) A Tiny Transformer (IEEE TBCAS, 2024) emphasizes efficiency: ~6k parameters with 8‑bit inference achieving 98.97% accuracy on 5 MIT‑BIH arrhythmia classes; on GAP9 MCU it reaches 4.28 ms inference and ~0.09 mJ energy, making it attractive for low‑power or large‑scale deployment. While reported on five “most common” classes (not explicitly stated as AAMI N/S/V/F/Q), it demonstrates that compact self‑attention with small token counts can match or exceed heavier CNNs. ([arxiv.org](https://arxiv.org/abs/2402.10748))\n\n(3) ECGTransForm (BSPC, 2024) provides a PyTorch reference with a bidirectional Transformer, multi‑scale CNN front‑end, and a context‑aware loss to mitigate class imbalance; results are reported on MIT‑BIH and PTB and the repository includes end‑to‑end training code and preprocessing, easing adoption. ([github.com](https://github.com/emadeldeen24/ECGTransForm))\n\nEvaluation and standards: Multiple reviews stress that many MIT‑BIH studies mix patients across train/test, inflating metrics; inter‑patient DS1/DS2 splits are recommended for fair benchmarking against AAMI classes. When comparing reported numbers (e.g., >98–99% accuracy), verify whether authors used inter‑patient protocols and macro‑F1 (more informative under imbalance). A 2025 systematic review also highlights embedded/clinical feasibility (latency, energy, memory) as key comparison axes—areas where compact Transformers are strong. ([arxiv.org](https://arxiv.org/html/2404.15367v2?utm_source=openai))\n\nFit to your data: Your 1000×2 input can be handled by either (a) CNN+Transformer hybrids like CAT‑Net by adding a 2‑channel conv stem and keeping 1000‑sample windows, or (b) a Tiny‑Transformer with a shallow 1D conv patch embedding that projects the 2 channels into token embeddings (e.g., patch size 10 → ~100 tokens), followed by 2–4 lightweight MHSA blocks and a classifier. Both are straightforward in PyTorch and compatible with AAMI 5‑class targets; the Tiny‑Transformer offers a better accuracy–efficiency–deployment trade‑off with strong MIT‑BIH results and explicit compute metrics. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))",
  "key_findings": [
    "Hybrid CNN+Transformer models are currently top-performing on MIT‑BIH; CAT‑Net reports 99.14% accuracy and 94.69% macro‑F1 on 5‑class MIT‑BIH with SMOTE‑Tomek to boost minority classes. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))",
    "Ultra‑compact Transformers can reach near‑SOTA accuracy with orders‑of‑magnitude fewer parameters; the Tiny Transformer uses ~6k params, achieves 98.97% accuracy on 5 MIT‑BIH classes, and runs in 4.28 ms (~0.09 mJ) on a GAP9 MCU with 8‑bit inference. ([arxiv.org](https://arxiv.org/abs/2402.10748))",
    "Inter‑patient DS1/DS2 splitting and AAMI EC57 class mapping are essential for fair evaluation; many papers otherwise overestimate performance—macro‑F1 and sensitivity per AAMI class should be reported. ([arxiv.org](https://arxiv.org/html/2404.15367v2?utm_source=openai))",
    "Open PyTorch implementations exist for Transformer‑based ECG (e.g., ECGTransForm with bidirectional Transformer + multi‑scale CNN and a context‑aware loss), facilitating reproducible training and adaptation to two‑lead inputs. ([github.com](https://github.com/emadeldeen24/ECGTransForm))"
  ],
  "recommended_approaches": [
    "Adopt a compact 1D Tiny‑Transformer (Busia et al., IEEE TBCAS 2024) adapted for (1000,2): use a 1D conv patch embedding (e.g., kernel/stride=10) to produce ~100 tokens per lead, concatenate channel embeddings, 2–4 lightweight Transformer encoder blocks (small d_model, few heads), global pooling, and a 5‑way classifier. Train with inter‑patient DS1/DS2, class‑balanced or focal loss, and data augmentation (noise, scaling). This model matches your data shape, has proven MIT‑BIH performance, is easy to implement in PyTorch, and offers excellent accuracy–efficiency with optional 8‑bit post‑training quantization. ([arxiv.org](https://arxiv.org/abs/2402.10748))"
  ],
  "recent_papers": [
    {
      "title": "CAT‑Net: Convolution, attention, and transformer based network for single‑lead ECG arrhythmia classification (BSPC, 2024)",
      "contribution": "CNN + Transformer encoder with channel attention; reports 99.14% accuracy and 94.69% macro‑F1 on 5‑class MIT‑BIH; evaluates SMOTE‑Tomek for imbalance. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S1746809424002696?utm_source=openai))"
    },
    {
      "title": "A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (IEEE TBCAS, 2024)",
      "contribution": "~6k‑parameter Transformer reaches 98.97% MIT‑BIH 5‑class accuracy; demonstrates 8‑bit inference with 4.28 ms latency and ~0.09 mJ on GAP9; strong efficiency/SOTA trade‑off. ([arxiv.org](https://arxiv.org/abs/2402.10748))"
    },
    {
      "title": "ECGTransForm: Empowering adaptive ECG arrhythmia classification framework with bidirectional transformer (BSPC, 2024) + PyTorch code",
      "contribution": "Bidirectional Transformer + multi‑scale CNN front‑end and context‑aware loss; open-source PyTorch repo for MIT‑BIH/PTB end‑to‑end training. ([github.com](https://github.com/emadeldeen24/ECGTransForm))"
    },
    {
      "title": "Accurate Arrhythmia Classification with Multi‑Branch, Multi‑Head Attention Temporal Convolutional Networks (Sensors, 2024)",
      "contribution": "Attention‑augmented TCN variant addressing class imbalance; recent open‑access benchmark on ECG arrhythmia. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124))"
    },
    {
      "title": "A Systematic Review of ECG Arrhythmia Classification: Adherence to Standards, Fair Evaluation, and Embedded Feasibility (arXiv, 2025)",
      "contribution": "Survey (2017–2024) emphasizing inter‑patient evaluation, AAMI compliance, and embedded metrics (latency/energy/memory) for clinically viable ECG models. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))"
    }
  ],
  "confidence": 0.78,
  "timestamp": 1759450412,
  "generated_time": "2025-10-02 19:13:32",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}