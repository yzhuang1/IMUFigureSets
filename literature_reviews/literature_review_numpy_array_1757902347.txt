LITERATURE REVIEW
=================

Query: sequence classification time series machine learning multiclass sequence classification 2024 2025 PyTorch implementation state-of-the-art methods
Generated: 2025-09-15 02:12:27
Confidence: 0.72

DATA PROFILE:
{
  "data_type": "numpy_array",
  "shape": [
    62352,
    1000,
    2
  ],
  "dtype": "float32",
  "feature_count": 2,
  "sample_count": 62352,
  "is_sequence": true,
  "is_image": false,
  "is_tabular": false,
  "has_labels": true,
  "label_count": 5,
  "sequence_lengths": null,
  "channels": null,
  "height": null,
  "width": null,
  "metadata": {}
}

COMPREHENSIVE REVIEW:
From 2023–2025, multivariate time‑series classification (MTSC) has seen two parallel trends: (a) light, strong convolutional baselines refined for efficiency, and (b) new sequence models that scale to long contexts without quadratic attention. On the convolutional side, InceptionTime has inspired compact successors such as LITE/LITEMV, which retain accuracy with orders‑of‑magnitude fewer parameters, and continue to be competitive on UCR/UEA while training faster and greener. In parallel, ROCKET‑family transforms (MiniRocket, MultiRocket) remain a state‑of‑practice baseline because they deliver near–state‑of‑the‑art accuracy with tiny training cost and simple linear heads; MultiRocket in particular is competitive with heavy ensembles on UEA/UEC MTSC while being far more efficient. ([arxiv.org](https://arxiv.org/abs/2409.02869?utm_source=openai))

On the sequence‑model front, selective state‑space models (Mamba) are a notable breakthrough: recent MTSC works use Mamba blocks to capture long‑range dependencies with near‑linear complexity and to fuse time/frequency views, showing consistent gains over prior deep baselines. Transformer variants tailored to MTSC also advanced: hierarchical/patching designs (e.g., FormerTime), shapelet‑aware transformers (ShapeFormer), and value‑plus‑shape priors (VSFormer) improve robustness and class specificity on UEA datasets. Importantly, large comparative studies emphasize keeping strong classics in the loop: meta‑ensembles like HIVE‑COTE 2.0 still set accuracy bars (albeit at high compute), and simple tabular models can match or beat fancy TSC on a non‑trivial share of datasets—use them as sanity checks. For your setting (numpy arrays of shape [L=1000, C=2], 5 classes), efficient CNNs, MultiRocket, and lightweight SSM/Transformer designs that patch or downsample sequences are the most practical high‑accuracy choices in PyTorch. ([arxiv.org](https://arxiv.org/abs/2406.04419?utm_source=openai))

KEY FINDINGS:
1. Strong, efficient baselines first: MultiRocket + linear classifier and compact Inception‑style CNNs remain hard to beat for MTSC accuracy‑per‑compute; they should anchor any study and provide robust yardsticks. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-022-00844-1?utm_source=openai))
2. Mamba/SSM models are emerging SoTA‑caliber for long sequences thanks to near‑linear scaling; MTSC variants (e.g., TSCMamba, SiMBA‑style hybrids) report consistent gains over prior deep baselines while staying memory‑friendly. ([arxiv.org](https://arxiv.org/abs/2406.04419?utm_source=openai))
3. Transformer designs that reduce sequence length (patching, hierarchical pooling) or inject class‑specific priors (shapelets, value+shape cues) improve classification fidelity on UEA MTSC. ([arxiv.org](https://arxiv.org/abs/2302.09818?utm_source=openai))
4. Heavy ensembles (HIVE‑COTE 2.0) still top accuracy on average but are computationally expensive; use them selectively as upper bounds or for small datasets. ([link.springer.com](https://link.springer.com/article/10.1007/s10994-021-06057-9?utm_source=openai))
5. Sanity checks matter: across UCR/UEA, simple tabular models can outperform ROCKET‑family on ~28% of multivariate datasets—always include strong tabular baselines. ([arxiv.org](https://arxiv.org/abs/2308.07886?utm_source=openai))
6. For regularization on MTSC, time‑series–specific augmentations (jitter, scaling, window‑warp, mixup/cutmix) significantly improve CNN/RNN/Transformer classifiers, especially with limited data. ([arxiv.org](https://arxiv.org/abs/2201.11739?utm_source=openai))
7. Representative accuracy: on UEA multivariate suites, means around 74–76% are common even for top ensembles; individual datasets vary widely, so expect 75–95% on well‑separable 5‑class problems with adequate data, and lower otherwise. ([ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2307.13679?utm_source=openai))

RECOMMENDED APPROACHES:
1. {'name': 'MultiRocket features + Ridge/Logistic head', 'why': 'Fast to train, strong across many MTSC datasets, minimal tuning; great baseline and often competitive with the best.', 'typical_setup': '10k–50k features; include first‑difference stream; ridge C in [0.1, 100]; per‑channel standardization; optionally class weights.', 'notes': 'Use tsai’s MiniRocket/MiniRocketPlus PyTorch modules for easy integration. ([link.springer.com](https://link.springer.com/article/10.1007/s10618-022-00844-1?utm_source=openai))'}
2. {'name': 'Inception‑style CNN (InceptionTime / LITE[LITEMV])', 'why': 'Excellent accuracy‑efficiency tradeoff; handles long sequences with dilated multi‑scale kernels; LITE reduces params by ~97% with similar accuracy.', 'typical_setup': 'Depth 3–6 blocks; nf 16–64; kernel sizes [10–80] with dilation; dropout 0–0.2; AdamW lr 1e‑3; bs 64–256; 50–150 epochs.', 'notes': 'PyTorch implementations available in tsai; try LITE/LITEMV for small GPUs or low latency. ([williamsdoug.github.io](https://williamsdoug.github.io/tsai/models.InceptionTime.html?utm_source=openai))'}
3. {'name': 'Mamba/SSM‑based classifier (e.g., TSCMamba, SiMBA‑style)', 'why': 'Near‑linear time/space with strong long‑range modeling; good fit for L=1000; recent MTSC papers report solid gains.', 'typical_setup': 'd_model 64–256; SSM state dim 16–64; 4–8 layers; residual + layernorm; AdamW 1e‑3; bs 128–512; optional multi‑view fusion (time + CWT).', 'notes': 'Stabilize with weight decay 0.01, grad‑clip 1.0; consider channel‑order robustness regularizers if channels are exchangeable. ([arxiv.org](https://arxiv.org/abs/2406.04419?utm_source=openai))'}
4. {'name': 'Transformer with sequence reduction (FormerTime / ShapeFormer‑style)', 'why': 'Patching/hierarchical attention reduces O(L^2) cost; shapelet‑aware modules capture class‑specific motifs that CNNs may miss.', 'typical_setup': 'Patch length 16–64, stride = patch; d_model 64–256; 2–6 layers; 4–8 heads; dropout 0.1–0.3; label smoothing 0.05.', 'notes': 'Use class‑specific shapelets or prior‑enhanced attention when classes differ by subtle subsequences or values. ([arxiv.org](https://arxiv.org/abs/2302.09818?utm_source=openai))'}
5. {'name': 'Time–Frequency fusion (CWT/STFT + 2D‑CNN) hybrid', 'why': 'For C=2 and L=1000, joint time/frequency cues can be discriminative; recent MTSC works show strong results with dual‑mode CNNs.', 'typical_setup': 'Per‑channel CWT to scalograms (e.g., 224×224), stack as 2–4 channels; ResNet‑18/34 with small lr 3e‑4; heavy augmentation.', 'notes': 'Adds preprocessing cost, but small models often suffice; good when frequency signatures drive class differences. ([dl.acm.org](https://dl.acm.org/doi/10.1016/j.knosys.2024.112015?utm_source=openai))'}

RECENT PAPERS:
- TSCMamba: Mamba Meets Multi‑View Learning for Time Series Classification (2024): Combines time‑ and frequency‑domain views with Mamba SSM; reports ~6.45% avg accuracy gain over strong MTSC baselines on 10 datasets.
- SiMBA: Simplified Mamba‑Based Architecture for Vision and Multivariate Time Series (2024): Hybrid EinFFT channel modeling + Mamba sequence blocks; strong efficiency and accuracy across image/time‑series benchmarks.
- Look Into the LITE in Deep Learning for Time Series Classification (2024): LITE/LITEMV: Inception‑style with depthwise separable convs; ~2.34% of InceptionTime params with comparable accuracy and 2–3× faster training.
- ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification (2024): Integrates class‑specific shapelet discovery with transformer encoders; top accuracy ranking on 30 UEA MTSC datasets.
- VSFormer: Value‑ and Shape‑Aware Transformer with Prior‑Enhanced Self‑Attention for MTSC (2024): Combines value cues and shape patterns with class‑specific prior‑enhanced attention; superior results on all 30 UEA datasets reported.
- FormerTime: Hierarchical Multi‑Scale Representations for MTSC (2023): Patching/temporal‑reduction attention and hierarchical features to improve efficiency and accuracy on UEA MTSC.
- HIVE‑COTE 2.0: A New Meta‑Ensemble for Time Series Classification (2021; still a key reference): Significantly more accurate than prior SoTA across 26 UEA MTSC datasets; sets an upper‑bound but at high computational cost.
- MultiRocket: Multiple Pooling Operators and Transformations for Fast and Effective TSC (2022): Competitive with HIVE‑COTE 2.0 while being orders faster; practical baseline for MTSC.
- Back to Basics: A Sanity Check on Modern TSC Algorithms (2023): Shows tabular models can outperform ROCKET on ~28% of MTSC datasets; motivates strong baselines and data diagnostics.
- Robust Augmentation for Multivariate Time Series Classification (2022): Demonstrates that cutout/cutmix/mixup/window‑warp significantly boost deep TSC models on UEA MTSC.

==================================================
