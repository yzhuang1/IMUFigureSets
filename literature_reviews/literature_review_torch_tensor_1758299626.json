{
  "query": "ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Problem framing and dataset alignment: Your task is multiclass sequence classification on ECG time series from the MIT-BIH Arrhythmia Database, with 5 classes, input tensors shaped (1000, 2). MIT-BIH provides two synchronized ECG leads (typically MLII and a precordial lead such as V1/V5) at 360 Hz, which fits your two-channel setting; 1000 samples correspond to ~2.78 s windows, a common scale for rhythm/beat-context modeling. The standard AAMI 5-class mapping (N, S, V, F, Q) is widely used for benchmarking. ([physionet.org](https://www.physionet.org/physiobank/database/mitdb/?utm_source=openai))\n\nRecent state-of-the-art (2024–2025) methods on MIT-BIH five-class tasks: (1) Tiny ECG Transformer: An extremely compact ViT-style 1D transformer achieves 99.05% (32-bit) and 98.97% (8‑bit) accuracy on five-class MIT-BIH, with only ~6.6k parameters and demonstrated embedded deployment (4.28 ms, 0.09 mJ on GAP9). The authors also report robust performance (98.36%) under electrode-motion noise using augmentation, and provide detailed per-class precision/sensitivity analyses. This is among the most computationally efficient, near-SOTA solutions reported recently. ([arxiv.org](https://arxiv.org/html/2402.10748)) (2) MB‑MHA‑TCN (Sensors 2024): A multi-branch temporal CNN with dilations plus multi-head self‑attention and focal loss reports 98.75% accuracy, precision 96.60%, sensitivity 97.21%, and F1 96.89% on MIT‑BIH five-class (five‑fold CV), explicitly addressing minority-class performance via augmentation and focal loss. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124?utm_source=openai)) (3) rECGnition v2.0 (2025): A dual‑pathway CNN with depthwise separable convs and a Self‑Attentive Canonical Correlation (SACC) fusion layer (ECG + patient data) achieves 98.07% accuracy and F1 98.05% on MIT‑BIH (10 classes) with 82.7M FLOPs per sample; also strong AAMI results on INCART/EDB, highlighting efficiency and interpretability. ([arxiv.org](https://arxiv.org/abs/2502.16255?utm_source=openai))\n\nGeneralization and evaluation rigor: A 2025 systematic review emphasizes that many ECG studies report high intra‑patient CV scores but do not use inter‑patient splits, which inflates headline metrics; it urges adherence to AAMI mapping and inter‑patient evaluation and considers embedded feasibility (E3C criteria). Where inter‑patient evaluation is used, performance is typically lower; for example, a 2024 visibility‑graph + GCN method reports 95.81% accuracy under a subject‑oriented (inter‑patient) protocol on MIT‑BIH. These findings should guide your validation design. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))\n\nMethodological patterns across SOTA: Successful models for five‑class MIT‑BIH typically (a) capture multi‑scale temporal features (dilated 1D CNN/TCN or short 1D patches), (b) add global dependency modeling (self‑attention), (c) mitigate class imbalance (focal/weighted losses and augmentation), and (d) constrain compute (tiny transformers, depthwise separable convs) to enable deployment. The Tiny Transformer shows that careful patching, small depth/width, and quantization can preserve accuracy while minimizing parameters/energy; MB‑MHA‑TCN shows multi‑branch temporal receptive fields plus attention improve minority‑class F1; rECGnition shows feature fusion and separable convs reduce FLOPs while boosting interpretability. ([arxiv.org](https://arxiv.org/html/2402.10748))\n\nImplications for your data shape (1000×2) and task: For multi‑second two‑lead inputs, both dilated TCNs with attention and lightweight transformers using 1D patch embeddings adapt naturally: patchify along time (e.g., 20–32‑sample stride) with 2‑channel linear/conv embeddings, then apply 2–4 encoder blocks, a global pooling token, and a 5‑way head. Train with AAMI mapping and an inter‑patient split; use class‑weighted or focal loss and ECG‑specific augmentations (time‑warp, baseline‑wander, Gaussian noise, random lead‑dropout) to improve minority‑class sensitivity and robustness. The Tiny Transformer variant provides the best accuracy–efficiency trade‑off while remaining straightforward to implement in PyTorch and deployable on modest hardware. ([arxiv.org](https://arxiv.org/html/2402.10748))",
  "key_findings": [
    "A tiny ViT-style 1D Transformer achieved 99.05% (32-bit) and 98.97% (8-bit) five-class accuracy on MIT-BIH with ~6.6k parameters, and ran at 4.28 ms/inference and 0.09 mJ on GAP9, demonstrating excellent accuracy–efficiency for embedded use. ([arxiv.org](https://arxiv.org/html/2402.10748))",
    "MB-MHA-TCN (multi-branch CNN + multi-head attention + dilated TCN) reported 98.75% accuracy, precision 96.60%, sensitivity 97.21%, and F1 96.89% on MIT-BIH five-class with five-fold CV, aided by focal loss and tailored augmentation. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124?utm_source=openai))",
    "rECGnition v2.0 (dual-path CNN with Self-Attentive Canonical Correlation fusion) reached 98.07% accuracy and 98.05% F1 on MIT-BIH (10 classes) at 82.7M FLOPs/sample, illustrating strong performance with efficient depthwise separable convolutions and interpretability. ([arxiv.org](https://arxiv.org/abs/2502.16255?utm_source=openai))",
    "Inter-patient evaluation is crucial: a 2025 systematic review shows many studies overestimate performance by not following inter-patient splits; a 2024 graph-convolution approach under a subject-oriented protocol obtained 95.81% accuracy on MIT-BIH, reflecting more realistic generalization. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))",
    "MIT-BIH provides two synchronized ECG channels at 360 Hz; the 1000×2 input corresponds to ~2.78 s windows, which suits multi-scale temporal modeling (dilated TCNs or 1D patch transformers) for five-class AAMI tasks. ([physionet.org](https://www.physionet.org/physiobank/database/mitdb/?utm_source=openai))"
  ],
  "recommended_approaches": [
    "Tiny-ECG-Transformer (Busia et al., 2025) adapted for (1000, 2): 1D patch embedding over time (kernel/stride 20–32), 2–4 lightweight Transformer encoder blocks (dim 64–128, 4–8 heads), class token + pooled head for 5 classes, trained with inter-patient split under AAMI mapping and class-weighted/focal loss plus ECG augmentations (time-warp, baseline-wander, noise, occasional lead dropout). Rationale: near-SOTA five-class accuracy (≈99%), extremely low parameter count (~6.6k) and proven quantized deployment while remaining easy to implement in PyTorch and scalable to two leads and multi-second windows. ([arxiv.org](https://arxiv.org/html/2402.10748))"
  ],
  "recent_papers": [
    {
      "title": "A Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers (IEEE TBioCAS, 2025; arXiv 2024)",
      "contribution": "Compact ViT-style 1D transformer with ~6.6k params; 99.05% (FP32) / 98.97% (INT8) five-class MIT-BIH; 4.28 ms, 0.09 mJ on GAP9; strong noise-robustness via augmentation. ([arxiv.org](https://arxiv.org/html/2402.10748))"
    },
    {
      "title": "Accurate Arrhythmia Classification with Multi-Branch, Multi-Head Attention Temporal Convolutional Networks (Sensors, 2024)",
      "contribution": "Three CNN branches (multi-kernel, dilations) + MHA + TCN; focal loss and augmentation for class imbalance; 98.75% accuracy, precision 96.60%, sensitivity 97.21%, F1 96.89% on MIT-BIH five-class (5-fold CV). ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124?utm_source=openai))"
    },
    {
      "title": "rECGnition v2.0: Self-Attentive Canonical Fusion of ECG and Patient Data (arXiv, 2025)",
      "contribution": "Dual-pathway CNN with depthwise separable convs and SACC fusion; 98.07% accuracy and 98.05% F1 on MIT-BIH (10 classes) with 82.7M FLOPs/sample; highlights interpretability and efficiency. ([arxiv.org](https://arxiv.org/abs/2502.16255?utm_source=openai))"
    },
    {
      "title": "Leveraging Visibility Graphs for Enhanced Arrhythmia Classification with Graph Convolutional Networks (arXiv, 2024)",
      "contribution": "Subject-oriented (inter-patient) evaluation on MIT-BIH; 95.81% accuracy; demonstrates typical drop versus intra-patient CV and value of graph features. ([arxiv.org](https://arxiv.org/html/2404.15367v1?utm_source=openai))"
    },
    {
      "title": "A Systematic Review of ECG Arrhythmia Classification: Adherence to Standards, Fair Evaluation, and Embedded Feasibility (arXiv, 2025)",
      "contribution": "Survey (2017–2024) critiquing lack of inter-patient splits; proposes E3C criteria (Embedded, Clinical, Comparative) and catalogs methods meeting AAMI and deployment constraints. ([arxiv.org](https://arxiv.org/abs/2503.07276?utm_source=openai))"
    }
  ],
  "confidence": 0.78,
  "timestamp": 1758299626,
  "generated_time": "2025-09-19 11:33:46",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}