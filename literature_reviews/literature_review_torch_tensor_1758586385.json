{
  "query": "ECG classification arrhythmia detection heart rhythm analysis sequence classification time series machine learning multiclass sequence classification large dataset classification 2024 2025 PyTorch implementation state-of-the-art methods",
  "review_text": "Problem fit: You have 1D sequential ECG data (torch_tensor) with shape (1000, 2), 5-way classification from the MIT-BIH Arrhythmia Database. MIT-BIH provides two-channel, 360 Hz annotated ECG from 48 half-hour records, and is the de facto benchmark for beat-type classification and AAMI five-class grouping (N/S/V/F/Q). Any rigorous evaluation should use inter-patient splitting (e.g., DS1/DS2) to avoid subject leakage. ([physionet.org](https://physionet.org/content/mitdb/1.0.0/))\nRecent state of the art (2024–2025): (1) A compact Transformer: Busia et al. (IEEE TBCAS 2024) introduce a 6k-parameter Tiny Transformer reaching 98.97% accuracy on 5-class MIT-BIH; with 8-bit inference it still attains 98.36% and runs in 4.28 ms on GAP9 (0.09 mJ), illustrating an excellent accuracy–efficiency trade-off for edge deployment. Class mapping is the 5 most common arrhythmias (not explicitly the AAMI five superclasses). ([arxiv.org](https://arxiv.org/abs/2402.10748)) (2) MB‑MHA‑TCN (Sensors 2024) combines multi-branch temporal convolutions with multi-head attention, addresses severe class imbalance with SMOTE/Tomek and focal loss, and reports five-fold cross‑validation results on MIT‑BIH (overall accuracy 98.75%, F1 96.89%) using the AAMI 5-category standard. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124)) (3) rECGnition_v2.0 (arXiv 2025) fuses ECG with patient metadata via a self‑attentive canonical correlation module in a dual‑pathway CNN; on MIT‑BIH it reports 98.07% accuracy and F1 98.05% with only 82.7M FLOPs per sample, highlighting strong performance with explicit compute reporting. ([arxiv.org](https://arxiv.org/abs/2502.16255)) (4) Adaptive beat segmentation + relative heart‑rate context (Computers in Biology & Medicine 2024) significantly improves detection of PACs (a common failure mode) and reports high class‑wise sensitivities on MIT‑BIH, suggesting that segmentation policy and rhythm context can matter as much as backbone choice. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0010482524011478?utm_source=openai)) (5) A 2025 survey in Artificial Intelligence Review synthesizes evidence that Transformer-style temporal modeling is increasingly competitive for ECG, but notes compute and interpretability concerns; it advocates attention with efficient convolutional stems and explicit imbalance handling, aligning with (2). ([link.springer.com](https://link.springer.com/article/10.1007/s10462-025-11259-x?utm_source=openai))\nImplementation notes: For PyTorch, TCN blocks (Conv1d with dilation + residuals) and nn.MultiheadAttention handle (1000, 2) inputs via a Conv1d stem that maps 2 channels→d_model before attention; focal loss or class-weighted cross‑entropy mitigates N/S/V/F/Q imbalance. Use inter‑patient DS1/DS2 splitting for MIT‑BIH to ensure generalization. ([github.com](https://github.com/windniu/ecg_classification?utm_source=openai))",
  "key_findings": [
    "Multi-branch temporal convolutions plus multi-head attention (MB‑MHA‑TCN) with focal loss and SMOTE/Tomek achieve OA 98.75% and F1 96.89% on AAMI 5-class MIT‑BIH (5-fold CV), indicating strong performance under the standard five-category protocol. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124))",
    "A 6k‑parameter Tiny Transformer attains 98.97% (5 classes) on MIT‑BIH and sustains 98.36% after 8‑bit quantization with 4.28 ms inference on GAP9, demonstrating state‑of‑the‑art efficiency for edge while keeping accuracy high. ([arxiv.org](https://arxiv.org/abs/2402.10748))",
    "Self‑attentive canonical fusion (rECGnition_v2.0) reports 98.07% accuracy and 98.05% F1 on MIT‑BIH with 82.7M FLOPs/sample, showing that lightweight attention with feature fusion can be accurate and compute‑efficient. ([arxiv.org](https://arxiv.org/abs/2502.16255))",
    "Segmentation strategy and rhythm context (adaptive RR‑based windows + relative heart rate) materially boost PAC sensitivity on MIT‑BIH, addressing a common minority‑class failure mode in 5‑class tasks. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0010482524011478?utm_source=openai))",
    "Inter‑patient evaluation (e.g., DS1/DS2) is necessary to avoid over‑optimistic results due to subject overlap; it is a widely used benchmark split for MIT‑BIH. ([github.com](https://github.com/windniu/ecg_classification?utm_source=openai))"
  ],
  "recommended_approaches": [
    "Adopt a PyTorch MB‑MHA‑TCN: a Conv1d stem (in_channels=2) → multi‑branch dilated TCN blocks (e.g., kernel sizes [7, 15, 31] with residuals) → projection to d_model (e.g., 128) → 1–2 MultiheadAttention encoder blocks (4–8 heads) with positional encoding → global pooling → linear head for 5 classes; train with focal loss or class‑weighted CE, SMOTE/Tomek only on training set, and inter‑patient DS1/DS2 split. This matches your (1000, 2) input, aligns with AAMI 5‑class MIT‑BIH practice, has published OA≈98.75%/F1≈96.9%, and is lighter than full Transformers while capturing both local morphology and long‑range rhythm context. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124))"
  ],
  "recent_papers": [
    {
      "title": "A Tiny Transformer for Low‑Power Arrhythmia Classification on Microcontrollers (IEEE TBCAS, 2024)",
      "contribution": "6k‑param Transformer achieves 98.97% (5‑class MIT‑BIH); 8‑bit inference 98.36% with 4.28 ms / 0.09 mJ on GAP9—excellent accuracy–efficiency for deployment. ([arxiv.org](https://arxiv.org/abs/2402.10748))"
    },
    {
      "title": "Accurate Arrhythmia Classification with Multi‑Branch, Multi‑Head Attention Temporal Convolutional Networks (Sensors, 2024)",
      "contribution": "MB‑MHA‑TCN with focal loss and class‑imbalance sampling achieves OA 98.75% and F1 96.89% on AAMI 5‑class MIT‑BIH (5‑fold CV); architecture captures multi‑scale and long‑range patterns efficiently. ([mdpi.com](https://www.mdpi.com/1424-8220/24/24/8124))"
    },
    {
      "title": "rECGnition_v2.0: Self‑Attentive Canonical Fusion of ECG and Patient Data (arXiv, 2025)",
      "contribution": "Dual‑pathway CNN with self‑attentive fusion; 98.07% accuracy / 98.05% F1 on MIT‑BIH with 82.7M FLOPs/sample; strong performance with compute reporting. ([arxiv.org](https://arxiv.org/abs/2502.16255))"
    },
    {
      "title": "ECG classification via adaptive beat segmentation and relative heart‑rate context (Computers in Biology & Medicine, 2024)",
      "contribution": "Introduces RR‑adaptive segmentation and relative HR features; improves PAC sensitivity and overall beat classification on MIT‑BIH; highlights preprocessing and context as key levers. ([sciencedirect.com](https://www.sciencedirect.com/science/article/abs/pii/S0010482524011478?utm_source=openai))"
    },
    {
      "title": "Survey: Transformers and LLMs for ECG diagnosis (Artificial Intelligence Review, 2025)",
      "contribution": "Comprehensive review: attention models capture long‑range dependencies but need efficiency and interpretability; supports hybrid Conv‑Attention designs like MB‑MHA‑TCN. ([link.springer.com](https://link.springer.com/article/10.1007/s10462-025-11259-x?utm_source=openai))"
    }
  ],
  "confidence": 0.73,
  "timestamp": 1758586385,
  "generated_time": "2025-09-22 19:13:05",
  "data_profile": {
    "data_type": "torch_tensor",
    "shape": [
      62352,
      1000,
      2
    ],
    "dtype": "float32",
    "feature_count": 2,
    "sample_count": 62352,
    "is_sequence": true,
    "is_image": false,
    "is_tabular": false,
    "has_labels": true,
    "label_count": 5,
    "sequence_lengths": null,
    "channels": null,
    "height": null,
    "width": null,
    "metadata": {}
  }
}