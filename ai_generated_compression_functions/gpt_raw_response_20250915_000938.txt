```json
{
  "compression_strategy": {
    "method": "Combines quantization, pruning, and low-rank factorization to reach the desired size and performance.",
    "estimated_size_kb": 245,
    "estimated_compression_ratio": 2.32,
    "techniques": ["Quantization", "Pruning", "Low-rank Factorization"]
  },
  "compression_function": {
    "function_name": "compress_model",
    "code": "import torch\\nimport torch.nn.utils.prune as prune\\nimport torch.quantization\\ndef compress_model(model_path, save_path):\\n    model = torch.load(model_path)\\n    model.eval()\\n    quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.LSTM, torch.nn.Linear}, dtype=torch.qint8)\\n    parameters_to_prune = [(model.lstm, 'weight_ih_l0'), (model.lstm, 'weight_hh_l0')]\\n    for param in parameters_to_prune:\\n        prune.l1_unstructured(param[0], name=param[1], amount=0.3)\\n    prune.remove(model.lstm, 'weight_ih_l0')\\n    prune.remove(model.lstm, 'weight_hh_l0')\\n    torch.save(quantized_model, save_path)\\n    return quantized_model",
    "imports": ["torch", "torch.nn.utils.prune", "torch.quantization"],
    "description": "This function loads a PyTorch LSTM model, applies quantization, pruning, and removes pruned elements permanently. It saves the compressed model to a new file."
  },
  "usage_instructions": {
    "how_to_run": "Call compress_model('path_to_your_model.pth', 'path_to_save_compressed_model.pth')",
    "expected_output": "A quantized and pruned PyTorch model will be saved to the specified path. The model's size should be less than 256KB.",
    "validation_steps": ["Verify the size of the output file is < 256KB", "Load the compressed model and evaluate its performance to ensure < 5% accuracy degradation", "Check that pruned weights are removed from the model"]
  }
}
```