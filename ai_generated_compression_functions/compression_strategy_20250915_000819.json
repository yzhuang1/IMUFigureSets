{
  "model_info": {
    "file_info": {
      "path": "trained_models/best_model_FallbackLSTM_20250914_231643.pth",
      "size_bytes": 581377,
      "size_kb": 567.7509765625,
      "target_size_kb": 256,
      "compression_ratio_needed": 2.2177772521972656
    },
    "architecture": {
      "total_parameters": 144233,
      "layer_analysis": {
        "LSTM_input_hidden": {
          "tensors": 3,
          "params": 57120,
          "shapes": [
            [
              336,
              2
            ],
            [
              336,
              84
            ],
            [
              336,
              84
            ]
          ],
          "names": [
            "lstm.weight_ih_l0",
            "lstm.weight_ih_l1",
            "lstm.weight_ih_l2"
          ],
          "percentage": 39.602587479980315
        },
        "LSTM_hidden_hidden": {
          "tensors": 3,
          "params": 84672,
          "shapes": [
            [
              336,
              84
            ],
            [
              336,
              84
            ],
            [
              336,
              84
            ]
          ],
          "names": [
            "lstm.weight_hh_l0",
            "lstm.weight_hh_l1",
            "lstm.weight_hh_l2"
          ],
          "percentage": 58.705012029147284
        },
        "LSTM_bias": {
          "tensors": 6,
          "params": 2016,
          "shapes": [
            [
              336
            ],
            [
              336
            ],
            [
              336
            ],
            [
              336
            ],
            [
              336
            ],
            [
              336
            ]
          ],
          "names": [
            "lstm.bias_ih_l0",
            "lstm.bias_hh_l0",
            "lstm.bias_ih_l1",
            "lstm.bias_hh_l1",
            "lstm.bias_ih_l2",
            "lstm.bias_hh_l2"
          ],
          "percentage": 1.397738381646364
        },
        "Linear": {
          "tensors": 2,
          "params": 425,
          "shapes": [
            [
              5,
              84
            ],
            [
              5
            ]
          ],
          "names": [
            "fc.weight",
            "fc.bias"
          ],
          "percentage": 0.29466210922604397
        }
      },
      "dtype_analysis": {
        "torch.float32": {
          "tensors": 14,
          "params": 144233
        }
      },
      "model_metadata": {
        "model_name": "FallbackLSTM",
        "hyperparameters": {
          "lr": 0.003918194347141745,
          "epochs": 15,
          "batch_size": 128,
          "hidden_size": 84,
          "dropout": 0.16240745617697463,
          "num_layers": 3
        },
        "final_metrics": {
          "acc": 0.9273514553764735,
          "macro_f1": 0.6787301582536586
        },
        "timestamp": "20250914_231643"
      }
    },
    "sample_weights": {
      "lstm.weight_ih_l0": {
        "shape": [
          336,
          2
        ],
        "dtype": "torch.float32",
        "sample_values": [
          0.3852119445800781,
          0.5378137230873108,
          -0.12588322162628174,
          0.4365813136100769,
          0.16023722290992737
        ],
        "min_value": -1.4516714811325073,
        "max_value": 1.9184547662734985,
        "mean_value": 0.04317547753453255,
        "std_value": 0.4040932059288025
      },
      "lstm.weight_hh_l0": {
        "shape": [
          336,
          84
        ],
        "dtype": "torch.float32",
        "sample_values": [
          -0.20540232956409454,
          0.15263211727142334,
          0.3850644826889038,
          0.076286680996418,
          -0.29667750000953674
        ],
        "min_value": -1.6184029579162598,
        "max_value": 1.9276429414749146,
        "mean_value": -0.0019205632852390409,
        "std_value": 0.25557443499565125
      },
      "lstm.bias_ih_l0": {
        "shape": [
          336
        ],
        "dtype": "torch.float32",
        "sample_values": [
          -0.15410037338733673,
          -0.25481656193733215,
          0.13038280606269836,
          0.13881106674671173,
          -0.2292858213186264
        ],
        "min_value": -0.7837626934051514,
        "max_value": 0.579433262348175,
        "mean_value": -0.03526212275028229,
        "std_value": 0.21477362513542175
      }
    }
  },
  "compression_strategy": {
    "compression_strategy": {
      "method": "Combining quantization, pruning, and low-rank factorization to compress an LSTM-based model",
      "estimated_size_kb": 245,
      "estimated_compression_ratio": 2.32,
      "techniques": [
        "Quantization",
        "Pruning",
        "Low-rank Factorization"
      ]
    },
    "compression_function": {
      "function_name": "compress_model",
      "code": "import torch\\nimport torch.nn.utils.prune as prune\\nimport torch.quantization\\n\\ndef compress_model(model_path, output_path):\\n    model = torch.load(model_path)\\n    model.eval()\\n    \\n    # Quantization\\n    model_quantized = torch.quantization.quantize_dynamic(model, {torch.nn.LSTM, torch.nn.Linear}, dtype=torch.qint8)\\n    \\n    # Pruning\\n    parameters_to_prune = [(model.lstm, 'weight_ih_l0'), (model.lstm, 'weight_hh_l0')]\\n    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.3)\\n    \\n    # Low-rank Factorization on Linear layers\\n    for name, module in model.named_modules():\\n        if isinstance(module, torch.nn.Linear):\\n            ranks = min(module.weight.data.size()) // 2\\n            decomposed = torch.nn.utils.parametrizations.low_rank(module, 'weight', rank=ranks)\\n            setattr(model, name, decomposed)\\n    \\n    # Save compressed model\\n    torch.save(model_quantized.state_dict(), output_path)\\n    \\n    return output_path",
      "imports": [
        "torch",
        "torch.nn.utils.prune",
        "torch.quantization"
      ],
      "description": "This function compresses an LSTM model by applying quantization (to reduce model size by reducing the bit-width of parameters), pruning (to remove insignificant weights), and low-rank factorization (to decompose weights of linear layers into lower rank matrices). The function loads a model from a given file path, applies these compression techniques, and saves the compressed model state dict to a specified output path."
    },
    "usage_instructions": {
      "how_to_run": "Call the compress_model function with arguments being the path to the original model file and the path where the compressed model should be saved.",
      "expected_output": "The function will save the compressed model to the specified output path and return this path.",
      "validation_steps": [
        "Load the original and compressed models",
        "Compare the sizes of the original and compressed model files",
        "Validate the performance of the compressed model on a test/validation dataset to verify its accuracy",
        "Ensure the size of the compressed model is less than 256 KB"
      ]
    }
  },
  "gpt_response": "```json\n{\n  \"compression_strategy\": {\n    \"method\": \"Combining quantization, pruning, and low-rank factorization to compress an LSTM-based model\",\n    \"estimated_size_kb\": 245,\n    \"estimated_compression_ratio\": 2.32,\n    \"techniques\": [\"Quantization\", \"Pruning\", \"Low-rank Factorization\"]\n  },\n  \"compression_function\": {\n    \"function_name\": \"compress_model\",\n    \"code\": \"import torch\\\\nimport torch.nn.utils.prune as prune\\\\nimport torch.quantization\\\\n\\\\ndef compress_model(model_path, output_path):\\\\n    model = torch.load(model_path)\\\\n    model.eval()\\\\n    \\\\n    # Quantization\\\\n    model_quantized = torch.quantization.quantize_dynamic(model, {torch.nn.LSTM, torch.nn.Linear}, dtype=torch.qint8)\\\\n    \\\\n    # Pruning\\\\n    parameters_to_prune = [(model.lstm, 'weight_ih_l0'), (model.lstm, 'weight_hh_l0')]\\\\n    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.3)\\\\n    \\\\n    # Low-rank Factorization on Linear layers\\\\n    for name, module in model.named_modules():\\\\n        if isinstance(module, torch.nn.Linear):\\\\n            ranks = min(module.weight.data.size()) // 2\\\\n            decomposed = torch.nn.utils.parametrizations.low_rank(module, 'weight', rank=ranks)\\\\n            setattr(model, name, decomposed)\\\\n    \\\\n    # Save compressed model\\\\n    torch.save(model_quantized.state_dict(), output_path)\\\\n    \\\\n    return output_path\",\n    \"imports\": [\"torch\", \"torch.nn.utils.prune\", \"torch.quantization\"],\n    \"description\": \"This function compresses an LSTM model by applying quantization (to reduce model size by reducing the bit-width of parameters), pruning (to remove insignificant weights), and low-rank factorization (to decompose weights of linear layers into lower rank matrices). The function loads a model from a given file path, applies these compression techniques, and saves the compressed model state dict to a specified output path.\"\n  },\n  \"usage_instructions\": {\n    \"how_to_run\": \"Call the compress_model function with arguments being the path to the original model file and the path where the compressed model should be saved.\",\n    \"expected_output\": \"The function will save the compressed model to the specified output path and return this path.\",\n    \"validation_steps\": [\n      \"Load the original and compressed models\",\n      \"Compare the sizes of the original and compressed model files\",\n      \"Validate the performance of the compressed model on a test/validation dataset to verify its accuracy\",\n      \"Ensure the size of the compressed model is less than 256 KB\"\n    ]\n  }\n}\n```",
  "timestamp": "20250915_000819"
}