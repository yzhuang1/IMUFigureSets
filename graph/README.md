# Graph Pipeline

This sub-pipeline starts from the Bayesian Optimization (BO) process, bypassing the AI code generation step. It takes an existing training function JSON file and runs:

**BO → Training → Evaluation**

All outputs are saved to `graph/new/` with the same folder structure as the original pipeline.

## Usage

### Command Line

```bash
python graph/graph_pipeline.py --training_function <path_to_json> [--data_folder <folder_name>]
```

**Arguments:**
- `--training_function`: Path to training function JSON file (required)
- `--data_folder`: Data folder name (optional, defaults to config.data_folder)

**Example:**

```bash
python graph/graph_pipeline.py --training_function generated_training_functions/training_function_CNN_1728567890.json
```

### Python API

```python
from graph.graph_pipeline import run_graph_pipeline
import numpy as np

# Your data
X = np.random.randn(1000, 128)
y = np.random.randint(0, 2, 1000)

# Run pipeline
results = run_graph_pipeline(
    training_function_path="generated_training_functions/training_function_CNN_1728567890.json",
    X=X,
    y=y,
    device="cuda",  # or "cpu"
    epochs=8
)

print(f"Model: {results['model_name']}")
print(f"Final metrics: {results['final_metrics']}")
print(f"Best BO params: {results['optimized_hyperparameters']}")
```

## Pipeline Flow

1. **Load Training Function**: Loads the training function from JSON file
2. **Data Splitting**: Creates consistent train/val/test splits (same as original pipeline)
3. **Bayesian Optimization**: Optimizes hyperparameters using scikit-optimize
4. **Final Training**: Trains model with best hyperparameters
5. **Save Results**: Saves everything to `graph/new/` folders

## Output Structure

All outputs are saved under `graph/new/` with the original folder structure:

```
graph/new/
├── trained_models/
│   └── <timestamp>_<model_name>/
│       ├── model.pt          # Trained model state dict
│       ├── X_test.pt          # Test features
│       └── y_test.pt          # Test labels
└── charts/
    ├── <timestamp>_BO_<model_name>/
    │   ├── accuracy_progression.png
    │   ├── hyperparameter_analysis.png
    │   ├── convergence_analysis.png
    │   ├── bo_summary.txt
    │   ├── bo_raw_data.json
    │   └── bo_raw_data.npz
    └── pipeline_summary_<timestamp>.json
```

## Input: Training Function JSON

The training function JSON should have this structure (same as generated by AI code generator):

```json
{
  "model_name": "CNN",
  "training_code": "def train_model(X_train, y_train, X_val, y_val, device='cpu', **hparams):\n    ...",
  "bo_config": {
    "lr": {
      "type": "Real",
      "low": 1e-5,
      "high": 1e-1,
      "prior": "log-uniform",
      "default": 0.001
    },
    "epochs": {
      "type": "Integer",
      "low": 5,
      "high": 20,
      "default": 10
    }
  },
  "confidence": 0.9,
  "data_profile": {...},
  "timestamp": 1728567890
}
```

## Differences from Original Pipeline

| Feature | Original Pipeline | Graph Pipeline |
|---------|------------------|----------------|
| **Start Point** | Data → AI Code Generation | Training Function JSON |
| **AI Code Generation** | ✅ Yes | ❌ No (uses existing JSON) |
| **Bayesian Optimization** | ✅ Yes | ✅ Yes |
| **Training & Evaluation** | ✅ Yes | ✅ Yes |
| **Output Location** | `trained_models/`, `charts/` | `graph/new/trained_models/`, `graph/new/charts/` |
| **Error Handling** | Fail-fast | Continues with remaining trials |

## Use Cases

1. **Re-run BO**: Re-optimize hyperparameters for an existing training function
2. **Different Data**: Test an existing training function on new data
3. **Comparison**: Compare BO results across different runs
4. **Debugging**: Isolate BO process from code generation
5. **Model Development**: Iterate on hyperparameters without regenerating code

## Configuration

Uses the same configuration as the main pipeline:
- `config.max_bo_trials`: Number of BO trials (default: 40)
- `config.bo_sample_num`: BO subset size for efficiency
- Device selection: Auto-detects CUDA availability

## Example Workflow

1. Run main pipeline to generate training function:
   ```bash
   python main.py
   # This creates: generated_training_functions/training_function_CNN_1728567890.json
   ```

2. Run graph pipeline with the generated function:
   ```bash
   python graph/graph_pipeline.py \
     --training_function generated_training_functions/training_function_CNN_1728567890.json
   ```

3. Results are saved to `graph/new/` folders

4. Compare results:
   - Original: `charts/pipeline_summary_*.json`
   - Graph: `graph/new/charts/pipeline_summary_*.json`
