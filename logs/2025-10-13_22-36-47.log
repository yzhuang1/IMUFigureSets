2025-10-13 22:36:48,003 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-13 22:36:48,111 - INFO - __main__ - Logging system initialized successfully
2025-10-13 22:36:48,111 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-10-13 22:36:48,111 - INFO - __main__ - Starting real data processing from data/dataset6/ directory
2025-10-13 22:36:48,112 - INFO - __main__ - Found 4 data files: ['wearable_sensor_data.csv', 'X.npy', 'y.npy', 'dataset6_metadata.json']
2025-10-13 22:36:48,112 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-10-13 22:36:48,112 - INFO - __main__ - Attempting to load: X.npy
2025-10-13 22:36:48,112 - INFO - __main__ - Successfully loaded NPY data: X(500, 4), y(500,)
2025-10-13 22:36:48,112 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (500, 4), device: cuda
2025-10-13 22:36:48,112 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-10-13 22:36:48,113 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-10-13 22:36:48,113 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-10-13 22:36:48,113 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-13 22:36:48,113 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (500, 4), 'dtype': 'float32', 'feature_count': 4, 'sample_count': 500, 'is_sequence': False, 'is_image': False, 'is_tabular': True, 'has_labels': True, 'label_count': 3, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-10-13 22:36:48,113 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-10-13 22:36:48,113 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-10-13 22:36:48,113 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-10-13 22:36:48,113 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-10-13 22:36:48,113 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-10-13 22:36:48,113 - INFO - data_splitting - Input data shape: X=(500, 4), y=(500,)
2025-10-13 22:36:48,113 - INFO - data_splitting - Class distribution: [358  91  51]
2025-10-13 22:36:48,114 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.4657933042212518), np.int64(1): np.float64(1.839080459770115), np.int64(2): np.float64(3.2323232323232323)}
2025-10-13 22:36:48,115 - INFO - class_balancing - Class imbalance analysis:
2025-10-13 22:36:48,115 - INFO - class_balancing -   Strategy: moderate_imbalance
2025-10-13 22:36:48,115 - INFO - class_balancing -   Imbalance ratio: 6.94
2025-10-13 22:36:48,115 - INFO - class_balancing -   Recommendations: Use weighted loss function, Consider weighted sampling
2025-10-13 22:36:48,115 - INFO - data_splitting - Final splits - Train: 320, Val: 80, Test: 100
2025-10-13 22:36:48,115 - INFO - data_splitting - Train class distribution: [229  58  33]
2025-10-13 22:36:48,115 - INFO - data_splitting - Val class distribution: [57 15  8]
2025-10-13 22:36:48,115 - INFO - data_splitting - Test class distribution: [72 18 10]
2025-10-13 22:36:48,115 - INFO - data_splitting - Recommended balancing strategy: moderate_imbalance
2025-10-13 22:36:48,115 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 4]), std shape: torch.Size([1, 4])
2025-10-13 22:36:48,115 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-10-13 22:36:48,115 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-10-13 22:36:48,115 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-10-13 22:36:48,115 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-10-13 22:36:48,115 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-10-13 22:36:48,115 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-10-13 22:39:08,138 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-13 22:39:08,162 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-13 22:39:08,162 - INFO - _models.ai_code_generator - Prompt length: 5272 characters
2025-10-13 22:39:08,162 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-13 22:39:08,162 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-13 22:39:08,162 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-13 22:40:39,174 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-13 22:40:39,175 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-13 22:40:39,175 - INFO - _models.ai_code_generator - AI generated training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:40:39,175 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-10-13 22:40:39,175 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-10-13 22:40:39,175 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:40:39,175 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'hidden_size', 'dropout', 'weight_decay', 'label_smoothing', 'grad_clip_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-10-13 22:40:39,175 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-10-13 22:40:39,175 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-10-13 22:40:39,176 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_TinyTabularMLP-PE-Classifier_1760413239.json
2025-10-13 22:40:39,176 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_TinyTabularMLP-PE-Classifier_1760413239.json
2025-10-13 22:40:39,176 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-10-13 22:40:39,176 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:40:39,176 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-10-13 22:40:39,176 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-13 22:40:39,177 - INFO - package_installer - Extracted imports from code: {'torch', 'contextlib'}
2025-10-13 22:40:39,178 - INFO - package_installer - Available packages: {'torch', 'contextlib'}
2025-10-13 22:40:39,178 - INFO - package_installer - Missing packages: set()
2025-10-13 22:40:39,178 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-13 22:40:39,178 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-13 22:40:39,178 - INFO - data_splitting - Using all 320 training samples for BO
2025-10-13 22:40:39,178 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 320 samples (using bo_sample_num=100000000000000)
2025-10-13 22:40:39,178 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'hidden_size', 'dropout', 'weight_decay', 'label_smoothing', 'grad_clip_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-10-13 22:40:39,178 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-13 22:40:39,178 - INFO - data_splitting - Using all 320 training samples for BO
2025-10-13 22:40:39,178 - INFO - _models.training_function_executor - Using BO subset for optimization: 320 samples (bo_sample_num=100000000000000)
2025-10-13 22:40:39,178 - INFO - _models.training_function_executor - BO splits - Train: 256, Val: 64
2025-10-13 22:40:39,213 - INFO - bo.run_bo - Converted GPT search space: 11 parameters
2025-10-13 22:40:39,213 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-13 22:40:39,213 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-13 22:40:39,213 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-13 22:40:39,214 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:40:39,214 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 1 (NaN monitoring active)
2025-10-13 22:40:39,214 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:40:39,214 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:40:39,214 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 81, 'hidden_size': 52, 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'label_smoothing': 0.011616722433639894, 'grad_clip_norm': 4.3977926559872085, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:40:39,214 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 81, 'hidden_size': 52, 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'label_smoothing': 0.011616722433639894, 'grad_clip_norm': 4.3977926559872085, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:40:44,815 - INFO - _models.training_function_executor - Epoch 001/081 | train_loss=1.9625 | val_loss=1.0790 | val_acc=0.5156
2025-10-13 22:40:44,823 - INFO - _models.training_function_executor - Epoch 002/081 | train_loss=1.8988 | val_loss=1.1120 | val_acc=0.7188
2025-10-13 22:40:44,830 - INFO - _models.training_function_executor - Epoch 003/081 | train_loss=1.3094 | val_loss=0.8187 | val_acc=0.6875
2025-10-13 22:40:44,836 - INFO - _models.training_function_executor - Epoch 004/081 | train_loss=1.1976 | val_loss=0.7190 | val_acc=0.7188
2025-10-13 22:40:44,843 - INFO - _models.training_function_executor - Epoch 005/081 | train_loss=1.1644 | val_loss=0.7125 | val_acc=0.7188
2025-10-13 22:40:44,849 - INFO - _models.training_function_executor - Epoch 006/081 | train_loss=1.0074 | val_loss=0.6801 | val_acc=0.7188
2025-10-13 22:40:44,856 - INFO - _models.training_function_executor - Epoch 007/081 | train_loss=0.8797 | val_loss=0.6722 | val_acc=0.7188
2025-10-13 22:40:44,862 - INFO - _models.training_function_executor - Epoch 008/081 | train_loss=0.9570 | val_loss=0.6704 | val_acc=0.7188
2025-10-13 22:40:44,868 - INFO - _models.training_function_executor - Epoch 009/081 | train_loss=1.0208 | val_loss=0.8001 | val_acc=0.7188
2025-10-13 22:40:44,874 - INFO - _models.training_function_executor - Epoch 010/081 | train_loss=0.9059 | val_loss=0.6934 | val_acc=0.6719
2025-10-13 22:40:44,880 - INFO - _models.training_function_executor - Epoch 011/081 | train_loss=0.7855 | val_loss=0.6633 | val_acc=0.7188
2025-10-13 22:40:44,886 - INFO - _models.training_function_executor - Epoch 012/081 | train_loss=0.7962 | val_loss=0.6462 | val_acc=0.7188
2025-10-13 22:40:44,892 - INFO - _models.training_function_executor - Epoch 013/081 | train_loss=0.7682 | val_loss=0.6598 | val_acc=0.7188
2025-10-13 22:40:44,898 - INFO - _models.training_function_executor - Epoch 014/081 | train_loss=0.7723 | val_loss=0.6405 | val_acc=0.7188
2025-10-13 22:40:44,904 - INFO - _models.training_function_executor - Epoch 015/081 | train_loss=0.7695 | val_loss=0.6655 | val_acc=0.6406
2025-10-13 22:40:44,910 - INFO - _models.training_function_executor - Epoch 016/081 | train_loss=0.7664 | val_loss=0.6304 | val_acc=0.7188
2025-10-13 22:40:44,917 - INFO - _models.training_function_executor - Epoch 017/081 | train_loss=0.7442 | val_loss=0.6354 | val_acc=0.7188
2025-10-13 22:40:44,923 - INFO - _models.training_function_executor - Epoch 018/081 | train_loss=0.7202 | val_loss=0.6316 | val_acc=0.7188
2025-10-13 22:40:44,929 - INFO - _models.training_function_executor - Epoch 019/081 | train_loss=0.7497 | val_loss=0.6304 | val_acc=0.7188
2025-10-13 22:40:44,935 - INFO - _models.training_function_executor - Epoch 020/081 | train_loss=0.7292 | val_loss=0.6246 | val_acc=0.7188
2025-10-13 22:40:44,940 - INFO - _models.training_function_executor - Epoch 021/081 | train_loss=0.7234 | val_loss=0.6326 | val_acc=0.7188
2025-10-13 22:40:44,946 - INFO - _models.training_function_executor - Epoch 022/081 | train_loss=0.7231 | val_loss=0.6368 | val_acc=0.7188
2025-10-13 22:40:44,953 - INFO - _models.training_function_executor - Epoch 023/081 | train_loss=0.6967 | val_loss=0.6351 | val_acc=0.7188
2025-10-13 22:40:44,958 - INFO - _models.training_function_executor - Epoch 024/081 | train_loss=0.6956 | val_loss=0.6234 | val_acc=0.7188
2025-10-13 22:40:44,965 - INFO - _models.training_function_executor - Epoch 025/081 | train_loss=0.7020 | val_loss=0.6194 | val_acc=0.7188
2025-10-13 22:40:44,971 - INFO - _models.training_function_executor - Epoch 026/081 | train_loss=0.7338 | val_loss=0.6375 | val_acc=0.6875
2025-10-13 22:40:44,977 - INFO - _models.training_function_executor - Epoch 027/081 | train_loss=0.7166 | val_loss=0.6271 | val_acc=0.7188
2025-10-13 22:40:44,983 - INFO - _models.training_function_executor - Epoch 028/081 | train_loss=0.6663 | val_loss=0.6235 | val_acc=0.7188
2025-10-13 22:40:44,989 - INFO - _models.training_function_executor - Epoch 029/081 | train_loss=0.6996 | val_loss=0.6156 | val_acc=0.7188
2025-10-13 22:40:44,995 - INFO - _models.training_function_executor - Epoch 030/081 | train_loss=0.6791 | val_loss=0.6211 | val_acc=0.7188
2025-10-13 22:40:45,001 - INFO - _models.training_function_executor - Epoch 031/081 | train_loss=0.7051 | val_loss=0.6199 | val_acc=0.7188
2025-10-13 22:40:45,008 - INFO - _models.training_function_executor - Epoch 032/081 | train_loss=0.7166 | val_loss=0.6110 | val_acc=0.7188
2025-10-13 22:40:45,014 - INFO - _models.training_function_executor - Epoch 033/081 | train_loss=0.6961 | val_loss=0.6110 | val_acc=0.7188
2025-10-13 22:40:45,020 - INFO - _models.training_function_executor - Epoch 034/081 | train_loss=0.6698 | val_loss=0.6141 | val_acc=0.7188
2025-10-13 22:40:45,026 - INFO - _models.training_function_executor - Epoch 035/081 | train_loss=0.7071 | val_loss=0.6148 | val_acc=0.7188
2025-10-13 22:40:45,032 - INFO - _models.training_function_executor - Epoch 036/081 | train_loss=0.6874 | val_loss=0.6148 | val_acc=0.7188
2025-10-13 22:40:45,038 - INFO - _models.training_function_executor - Epoch 037/081 | train_loss=0.6792 | val_loss=0.6130 | val_acc=0.7188
2025-10-13 22:40:45,044 - INFO - _models.training_function_executor - Epoch 038/081 | train_loss=0.7057 | val_loss=0.6144 | val_acc=0.7188
2025-10-13 22:40:45,050 - INFO - _models.training_function_executor - Epoch 039/081 | train_loss=0.6980 | val_loss=0.6250 | val_acc=0.7188
2025-10-13 22:40:45,056 - INFO - _models.training_function_executor - Epoch 040/081 | train_loss=0.7041 | val_loss=0.6297 | val_acc=0.7188
2025-10-13 22:40:45,062 - INFO - _models.training_function_executor - Epoch 041/081 | train_loss=0.6906 | val_loss=0.6308 | val_acc=0.7188
2025-10-13 22:40:45,067 - INFO - _models.training_function_executor - Epoch 042/081 | train_loss=0.6890 | val_loss=0.6184 | val_acc=0.7188
2025-10-13 22:40:45,073 - INFO - _models.training_function_executor - Epoch 043/081 | train_loss=0.6609 | val_loss=0.6133 | val_acc=0.7188
2025-10-13 22:40:45,079 - INFO - _models.training_function_executor - Epoch 044/081 | train_loss=0.7216 | val_loss=0.6101 | val_acc=0.7188
2025-10-13 22:40:45,085 - INFO - _models.training_function_executor - Epoch 045/081 | train_loss=0.6874 | val_loss=0.6146 | val_acc=0.7188
2025-10-13 22:40:45,091 - INFO - _models.training_function_executor - Epoch 046/081 | train_loss=0.6665 | val_loss=0.6152 | val_acc=0.7188
2025-10-13 22:40:45,096 - INFO - _models.training_function_executor - Epoch 047/081 | train_loss=0.6896 | val_loss=0.6156 | val_acc=0.7188
2025-10-13 22:40:45,102 - INFO - _models.training_function_executor - Epoch 048/081 | train_loss=0.6718 | val_loss=0.6117 | val_acc=0.7188
2025-10-13 22:40:45,108 - INFO - _models.training_function_executor - Epoch 049/081 | train_loss=0.7121 | val_loss=0.6135 | val_acc=0.7188
2025-10-13 22:40:45,114 - INFO - _models.training_function_executor - Epoch 050/081 | train_loss=0.6572 | val_loss=0.6160 | val_acc=0.7188
2025-10-13 22:40:45,119 - INFO - _models.training_function_executor - Epoch 051/081 | train_loss=0.6681 | val_loss=0.6147 | val_acc=0.7188
2025-10-13 22:40:45,125 - INFO - _models.training_function_executor - Epoch 052/081 | train_loss=0.6766 | val_loss=0.6129 | val_acc=0.7188
2025-10-13 22:40:45,131 - INFO - _models.training_function_executor - Epoch 053/081 | train_loss=0.6685 | val_loss=0.6115 | val_acc=0.7188
2025-10-13 22:40:45,137 - INFO - _models.training_function_executor - Epoch 054/081 | train_loss=0.6671 | val_loss=0.6136 | val_acc=0.7188
2025-10-13 22:40:45,143 - INFO - _models.training_function_executor - Epoch 055/081 | train_loss=0.6665 | val_loss=0.6095 | val_acc=0.7188
2025-10-13 22:40:45,149 - INFO - _models.training_function_executor - Epoch 056/081 | train_loss=0.6785 | val_loss=0.6061 | val_acc=0.7188
2025-10-13 22:40:45,154 - INFO - _models.training_function_executor - Epoch 057/081 | train_loss=0.6572 | val_loss=0.6039 | val_acc=0.7188
2025-10-13 22:40:45,160 - INFO - _models.training_function_executor - Epoch 058/081 | train_loss=0.6628 | val_loss=0.6068 | val_acc=0.7188
2025-10-13 22:40:45,166 - INFO - _models.training_function_executor - Epoch 059/081 | train_loss=0.6673 | val_loss=0.6123 | val_acc=0.7188
2025-10-13 22:40:45,173 - INFO - _models.training_function_executor - Epoch 060/081 | train_loss=0.6516 | val_loss=0.6158 | val_acc=0.7188
2025-10-13 22:40:45,179 - INFO - _models.training_function_executor - Epoch 061/081 | train_loss=0.6465 | val_loss=0.6209 | val_acc=0.7188
2025-10-13 22:40:45,185 - INFO - _models.training_function_executor - Epoch 062/081 | train_loss=0.6725 | val_loss=0.6214 | val_acc=0.7188
2025-10-13 22:40:45,192 - INFO - _models.training_function_executor - Epoch 063/081 | train_loss=0.6716 | val_loss=0.6190 | val_acc=0.7188
2025-10-13 22:40:45,199 - INFO - _models.training_function_executor - Epoch 064/081 | train_loss=0.6729 | val_loss=0.6171 | val_acc=0.7188
2025-10-13 22:40:45,205 - INFO - _models.training_function_executor - Epoch 065/081 | train_loss=0.7087 | val_loss=0.6167 | val_acc=0.7188
2025-10-13 22:40:45,212 - INFO - _models.training_function_executor - Epoch 066/081 | train_loss=0.7094 | val_loss=0.6180 | val_acc=0.7188
2025-10-13 22:40:45,218 - INFO - _models.training_function_executor - Epoch 067/081 | train_loss=0.6744 | val_loss=0.6193 | val_acc=0.7188
2025-10-13 22:40:45,224 - INFO - _models.training_function_executor - Epoch 068/081 | train_loss=0.6704 | val_loss=0.6205 | val_acc=0.7188
2025-10-13 22:40:45,230 - INFO - _models.training_function_executor - Epoch 069/081 | train_loss=0.6731 | val_loss=0.6212 | val_acc=0.7188
2025-10-13 22:40:45,236 - INFO - _models.training_function_executor - Epoch 070/081 | train_loss=0.6577 | val_loss=0.6214 | val_acc=0.7188
2025-10-13 22:40:45,242 - INFO - _models.training_function_executor - Epoch 071/081 | train_loss=0.6796 | val_loss=0.6221 | val_acc=0.7188
2025-10-13 22:40:45,248 - INFO - _models.training_function_executor - Epoch 072/081 | train_loss=0.6944 | val_loss=0.6220 | val_acc=0.7188
2025-10-13 22:40:45,254 - INFO - _models.training_function_executor - Epoch 073/081 | train_loss=0.6690 | val_loss=0.6215 | val_acc=0.7188
2025-10-13 22:40:45,260 - INFO - _models.training_function_executor - Epoch 074/081 | train_loss=0.6745 | val_loss=0.6209 | val_acc=0.7188
2025-10-13 22:40:45,267 - INFO - _models.training_function_executor - Epoch 075/081 | train_loss=0.6735 | val_loss=0.6204 | val_acc=0.7188
2025-10-13 22:40:45,273 - INFO - _models.training_function_executor - Epoch 076/081 | train_loss=0.6846 | val_loss=0.6202 | val_acc=0.7188
2025-10-13 22:40:45,279 - INFO - _models.training_function_executor - Epoch 077/081 | train_loss=0.6660 | val_loss=0.6200 | val_acc=0.7188
2025-10-13 22:40:45,285 - INFO - _models.training_function_executor - Epoch 078/081 | train_loss=0.6676 | val_loss=0.6198 | val_acc=0.7188
2025-10-13 22:40:45,291 - INFO - _models.training_function_executor - Epoch 079/081 | train_loss=0.6846 | val_loss=0.6197 | val_acc=0.7188
2025-10-13 22:40:45,297 - INFO - _models.training_function_executor - Epoch 080/081 | train_loss=0.6704 | val_loss=0.6196 | val_acc=0.7188
2025-10-13 22:40:45,303 - INFO - _models.training_function_executor - Epoch 081/081 | train_loss=0.6601 | val_loss=0.6196 | val_acc=0.7188
2025-10-13 22:40:46,125 - INFO - _models.training_function_executor - Model: 3,175 parameters, 13.6KB storage
2025-10-13 22:40:46,126 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.962492048740387, 1.8988377749919891, 1.3093544244766235, 1.1976338624954224, 1.1643504947423935, 1.0074155777692795, 0.8797273486852646, 0.9570057094097137, 1.0207880586385727, 0.9058821052312851, 0.7854973375797272, 0.7962213903665543, 0.7682208865880966, 0.7723087519407272, 0.7695136517286301, 0.7664286196231842, 0.7441600412130356, 0.7201637923717499, 0.7497354596853256, 0.7291852682828903, 0.723356306552887, 0.7231020331382751, 0.6967177093029022, 0.6956460475921631, 0.7019909024238586, 0.7337862104177475, 0.7165886461734772, 0.6663040965795517, 0.6996105909347534, 0.6790989190340042, 0.7051156461238861, 0.7165631949901581, 0.6961276233196259, 0.6697738468647003, 0.707095667719841, 0.6873615235090256, 0.679244339466095, 0.7056604102253914, 0.6980372220277786, 0.7041127681732178, 0.690551221370697, 0.689047172665596, 0.6609126478433609, 0.7216367423534393, 0.6874262392520905, 0.6665320992469788, 0.6895888447761536, 0.6717739254236221, 0.7120603621006012, 0.6571997404098511, 0.6680847257375717, 0.6765840798616409, 0.6685227453708649, 0.6671256124973297, 0.6664503663778305, 0.6785064041614532, 0.6572121828794479, 0.6627548784017563, 0.6673186272382736, 0.6516293436288834, 0.6465381234884262, 0.6725341975688934, 0.6715725213289261, 0.6728561818599701, 0.708726555109024, 0.7093830555677414, 0.674395814538002, 0.670417845249176, 0.6730965673923492, 0.6577225625514984, 0.6795994192361832, 0.69439697265625, 0.6690456718206406, 0.6744556874036789, 0.6734907031059265, 0.6845851689577103, 0.6659866720438004, 0.6676152497529984, 0.6845889389514923, 0.6704275757074356, 0.6601271331310272], 'val_losses': [1.0790177583694458, 1.1119803190231323, 0.8186547756195068, 0.7189748883247375, 0.7124735116958618, 0.6801120638847351, 0.6722356081008911, 0.6704074740409851, 0.800136387348175, 0.693375289440155, 0.6633076071739197, 0.6462326645851135, 0.6598379015922546, 0.6404525637626648, 0.6655306220054626, 0.630439281463623, 0.6353744268417358, 0.6315895318984985, 0.6303671598434448, 0.6246132850646973, 0.6326162219047546, 0.6368328928947449, 0.6351486444473267, 0.6234471797943115, 0.6194222569465637, 0.637475848197937, 0.6270878314971924, 0.6235406994819641, 0.6155720353126526, 0.6211379170417786, 0.6199385523796082, 0.610958456993103, 0.6109734177589417, 0.6141307950019836, 0.614758312702179, 0.6148058772087097, 0.6130156517028809, 0.6144282221794128, 0.6250262260437012, 0.6296869516372681, 0.6307955384254456, 0.6183961033821106, 0.6133070588111877, 0.6101280450820923, 0.6145794987678528, 0.6151771545410156, 0.6156001687049866, 0.6117021441459656, 0.613470196723938, 0.615974485874176, 0.614740788936615, 0.6129244565963745, 0.6115127801895142, 0.6135751008987427, 0.609531819820404, 0.6061002016067505, 0.6038930416107178, 0.6067627668380737, 0.6122848987579346, 0.6158301830291748, 0.6209229230880737, 0.6213676929473877, 0.6190488338470459, 0.6171352863311768, 0.6167135238647461, 0.6180413961410522, 0.6192528009414673, 0.6205348968505859, 0.6211898922920227, 0.62144935131073, 0.6221340894699097, 0.6220383048057556, 0.6215023398399353, 0.6208993196487427, 0.6204103231430054, 0.6202423572540283, 0.6199761629104614, 0.6198143362998962, 0.6196800470352173, 0.6195799708366394, 0.6195797324180603], 'val_acc': [0.515625, 0.71875, 0.6875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.671875, 0.71875, 0.71875, 0.71875, 0.71875, 0.640625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.6875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 81, 'hidden_size': 52, 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'label_smoothing': 0.011616722433639894, 'grad_clip_norm': 4.3977926559872085, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 3175, 'model_storage_size_kb': 13.642578125000002, 'model_size_validation': 'PASS'}
2025-10-13 22:40:46,126 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:40:46,126 - INFO - _models.training_function_executor - Model: 3,175 parameters, 13.6KB (PASS 256KB limit)
2025-10-13 22:40:46,126 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 6.912s
2025-10-13 22:40:46,127 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:40:46,127 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-13 22:40:46,127 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': np.int64(81), 'hidden_size': np.int64(52), 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'label_smoothing': 0.011616722433639894, 'grad_clip_norm': 4.3977926559872085, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, value=0.7188
2025-10-13 22:40:46,127 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': np.int64(81), 'hidden_size': np.int64(52), 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'label_smoothing': 0.011616722433639894, 'grad_clip_norm': 4.3977926559872085, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True} -> 0.7188
2025-10-13 22:40:46,127 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-10-13 22:40:46,127 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:40:46,127 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 2 (NaN monitoring active)
2025-10-13 22:40:46,127 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:40:46,127 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:40:46,127 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.008123245085588694, 'batch_size': 256, 'epochs': 47, 'hidden_size': 161, 'dropout': 0.09091248360355032, 'weight_decay': 5.415244119402536e-06, 'label_smoothing': 0.06084844859190756, 'grad_clip_norm': 2.8614039423450706, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:40:46,128 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.008123245085588694, 'batch_size': 256, 'epochs': 47, 'hidden_size': 161, 'dropout': 0.09091248360355032, 'weight_decay': 5.415244119402536e-06, 'label_smoothing': 0.06084844859190756, 'grad_clip_norm': 2.8614039423450706, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:40:50,230 - INFO - _models.training_function_executor - Epoch 001/047 | train_loss=3.4555 | val_loss=2.7390 | val_acc=0.7188
2025-10-13 22:40:50,236 - INFO - _models.training_function_executor - Epoch 002/047 | train_loss=3.6032 | val_loss=2.7390 | val_acc=0.7188
2025-10-13 22:40:50,240 - INFO - _models.training_function_executor - Epoch 003/047 | train_loss=3.4636 | val_loss=2.7390 | val_acc=0.7188
2025-10-13 22:40:50,244 - INFO - _models.training_function_executor - Epoch 004/047 | train_loss=3.7883 | val_loss=41.8443 | val_acc=0.0938
2025-10-13 22:40:50,247 - INFO - _models.training_function_executor - Epoch 005/047 | train_loss=41.9159 | val_loss=41.8443 | val_acc=0.0938
2025-10-13 22:40:50,250 - INFO - _models.training_function_executor - Epoch 006/047 | train_loss=41.5969 | val_loss=41.8443 | val_acc=0.0938
2025-10-13 22:40:50,253 - INFO - _models.training_function_executor - Epoch 007/047 | train_loss=42.0569 | val_loss=41.8443 | val_acc=0.0938
2025-10-13 22:40:50,257 - INFO - _models.training_function_executor - Epoch 008/047 | train_loss=41.7758 | val_loss=29.8509 | val_acc=0.7188
2025-10-13 22:40:50,260 - INFO - _models.training_function_executor - Epoch 009/047 | train_loss=28.8538 | val_loss=23.2887 | val_acc=0.7188
2025-10-13 22:40:50,263 - INFO - _models.training_function_executor - Epoch 010/047 | train_loss=22.3899 | val_loss=14.6452 | val_acc=0.4844
2025-10-13 22:40:50,266 - INFO - _models.training_function_executor - Epoch 011/047 | train_loss=14.7998 | val_loss=8.2186 | val_acc=0.5938
2025-10-13 22:40:50,269 - INFO - _models.training_function_executor - Epoch 012/047 | train_loss=8.5683 | val_loss=4.8444 | val_acc=0.7188
2025-10-13 22:40:50,272 - INFO - _models.training_function_executor - Epoch 013/047 | train_loss=4.8395 | val_loss=1.1799 | val_acc=0.7188
2025-10-13 22:40:50,276 - INFO - _models.training_function_executor - Epoch 014/047 | train_loss=2.0015 | val_loss=3.1788 | val_acc=0.1875
2025-10-13 22:40:50,279 - INFO - _models.training_function_executor - Epoch 015/047 | train_loss=3.9388 | val_loss=2.4466 | val_acc=0.2656
2025-10-13 22:40:50,282 - INFO - _models.training_function_executor - Epoch 016/047 | train_loss=3.1246 | val_loss=1.1166 | val_acc=0.7188
2025-10-13 22:40:50,285 - INFO - _models.training_function_executor - Epoch 017/047 | train_loss=1.6315 | val_loss=1.6291 | val_acc=0.7188
2025-10-13 22:40:50,289 - INFO - _models.training_function_executor - Epoch 018/047 | train_loss=1.7176 | val_loss=1.5976 | val_acc=0.7188
2025-10-13 22:40:50,292 - INFO - _models.training_function_executor - Epoch 019/047 | train_loss=1.8294 | val_loss=1.4602 | val_acc=0.6406
2025-10-13 22:40:50,295 - INFO - _models.training_function_executor - Epoch 020/047 | train_loss=1.7959 | val_loss=1.0480 | val_acc=0.7188
2025-10-13 22:40:50,298 - INFO - _models.training_function_executor - Epoch 021/047 | train_loss=1.2784 | val_loss=0.7486 | val_acc=0.7188
2025-10-13 22:40:50,301 - INFO - _models.training_function_executor - Epoch 022/047 | train_loss=0.9928 | val_loss=0.8422 | val_acc=0.6562
2025-10-13 22:40:50,304 - INFO - _models.training_function_executor - Epoch 023/047 | train_loss=1.1053 | val_loss=0.7997 | val_acc=0.6562
2025-10-13 22:40:50,307 - INFO - _models.training_function_executor - Epoch 024/047 | train_loss=1.0585 | val_loss=0.7964 | val_acc=0.7188
2025-10-13 22:40:50,311 - INFO - _models.training_function_executor - Epoch 025/047 | train_loss=0.9937 | val_loss=0.7391 | val_acc=0.7188
2025-10-13 22:40:50,314 - INFO - _models.training_function_executor - Epoch 026/047 | train_loss=0.9919 | val_loss=0.7227 | val_acc=0.7188
2025-10-13 22:40:50,317 - INFO - _models.training_function_executor - Epoch 027/047 | train_loss=0.9101 | val_loss=0.7057 | val_acc=0.7188
2025-10-13 22:40:50,320 - INFO - _models.training_function_executor - Epoch 028/047 | train_loss=0.8701 | val_loss=0.7227 | val_acc=0.7188
2025-10-13 22:40:50,323 - INFO - _models.training_function_executor - Epoch 029/047 | train_loss=0.8248 | val_loss=0.7119 | val_acc=0.7188
2025-10-13 22:40:50,326 - INFO - _models.training_function_executor - Epoch 030/047 | train_loss=0.8457 | val_loss=0.6958 | val_acc=0.7188
2025-10-13 22:40:50,330 - INFO - _models.training_function_executor - Epoch 031/047 | train_loss=0.8182 | val_loss=0.7106 | val_acc=0.6562
2025-10-13 22:40:50,333 - INFO - _models.training_function_executor - Epoch 032/047 | train_loss=0.7725 | val_loss=0.7041 | val_acc=0.7188
2025-10-13 22:40:50,336 - INFO - _models.training_function_executor - Epoch 033/047 | train_loss=0.8336 | val_loss=0.7001 | val_acc=0.7188
2025-10-13 22:40:50,340 - INFO - _models.training_function_executor - Epoch 034/047 | train_loss=0.7981 | val_loss=0.7006 | val_acc=0.7188
2025-10-13 22:40:50,343 - INFO - _models.training_function_executor - Epoch 035/047 | train_loss=0.7891 | val_loss=0.6976 | val_acc=0.7188
2025-10-13 22:40:50,346 - INFO - _models.training_function_executor - Epoch 036/047 | train_loss=0.7779 | val_loss=0.6994 | val_acc=0.7188
2025-10-13 22:40:50,349 - INFO - _models.training_function_executor - Epoch 037/047 | train_loss=0.7860 | val_loss=0.7039 | val_acc=0.7188
2025-10-13 22:40:50,352 - INFO - _models.training_function_executor - Epoch 038/047 | train_loss=0.7901 | val_loss=0.7055 | val_acc=0.7188
2025-10-13 22:40:50,355 - INFO - _models.training_function_executor - Epoch 039/047 | train_loss=0.7895 | val_loss=0.7042 | val_acc=0.7188
2025-10-13 22:40:50,359 - INFO - _models.training_function_executor - Epoch 040/047 | train_loss=0.7714 | val_loss=0.7019 | val_acc=0.7188
2025-10-13 22:40:50,362 - INFO - _models.training_function_executor - Epoch 041/047 | train_loss=0.7616 | val_loss=0.6988 | val_acc=0.7188
2025-10-13 22:40:50,365 - INFO - _models.training_function_executor - Epoch 042/047 | train_loss=0.7791 | val_loss=0.6962 | val_acc=0.7188
2025-10-13 22:40:50,368 - INFO - _models.training_function_executor - Epoch 043/047 | train_loss=0.7608 | val_loss=0.6943 | val_acc=0.7188
2025-10-13 22:40:50,371 - INFO - _models.training_function_executor - Epoch 044/047 | train_loss=0.7655 | val_loss=0.6932 | val_acc=0.7188
2025-10-13 22:40:50,375 - INFO - _models.training_function_executor - Epoch 045/047 | train_loss=0.7639 | val_loss=0.6928 | val_acc=0.7188
2025-10-13 22:40:50,378 - INFO - _models.training_function_executor - Epoch 046/047 | train_loss=0.7402 | val_loss=0.6926 | val_acc=0.7188
2025-10-13 22:40:50,381 - INFO - _models.training_function_executor - Epoch 047/047 | train_loss=0.7802 | val_loss=0.6925 | val_acc=0.7188
2025-10-13 22:40:51,205 - INFO - _models.training_function_executor - Model: 27,373 parameters, 58.8KB storage
2025-10-13 22:40:51,205 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [3.455543041229248, 3.603215217590332, 3.4635813236236572, 3.7883033752441406, 41.91590118408203, 41.596920013427734, 42.056907653808594, 41.77580261230469, 28.853790283203125, 22.38992691040039, 14.799837112426758, 8.568304061889648, 4.83949613571167, 2.001467704772949, 3.938770294189453, 3.124553918838501, 1.6314607858657837, 1.71757173538208, 1.829408884048462, 1.7958533763885498, 1.2784122228622437, 0.9927804470062256, 1.105315923690796, 1.0585386753082275, 0.9936829805374146, 0.9919242858886719, 0.9101484417915344, 0.8700879812240601, 0.824815034866333, 0.845670759677887, 0.8182083368301392, 0.7724875807762146, 0.8335766196250916, 0.79813551902771, 0.7891092896461487, 0.7778514623641968, 0.7860286235809326, 0.7901135087013245, 0.7894802689552307, 0.7713541984558105, 0.7615692019462585, 0.7790716886520386, 0.7607899904251099, 0.7655287384986877, 0.7639057636260986, 0.7401600480079651, 0.7801787853240967], 'val_losses': [2.7390310764312744, 2.7390310764312744, 2.7390310764312744, 41.84428024291992, 41.84428024291992, 41.84428024291992, 41.84428024291992, 29.850934982299805, 23.28868865966797, 14.645225524902344, 8.21856689453125, 4.844379425048828, 1.1799143552780151, 3.178770065307617, 2.4466359615325928, 1.1165698766708374, 1.6291316747665405, 1.5975980758666992, 1.4602155685424805, 1.0479899644851685, 0.748626172542572, 0.8422422409057617, 0.7996549010276794, 0.796422004699707, 0.7390934824943542, 0.7227284908294678, 0.705715537071228, 0.7226516604423523, 0.7119076251983643, 0.6958245038986206, 0.7105638384819031, 0.7041484117507935, 0.7000851631164551, 0.7005770802497864, 0.6976340413093567, 0.6993667483329773, 0.703872561454773, 0.7055120468139648, 0.7042328119277954, 0.7019129991531372, 0.6987690329551697, 0.6961697340011597, 0.6942814588546753, 0.6932293772697449, 0.6927888989448547, 0.692553699016571, 0.6924871802330017], 'val_acc': [0.71875, 0.71875, 0.71875, 0.09375, 0.09375, 0.09375, 0.09375, 0.71875, 0.71875, 0.484375, 0.59375, 0.71875, 0.71875, 0.1875, 0.265625, 0.71875, 0.71875, 0.71875, 0.640625, 0.71875, 0.71875, 0.65625, 0.65625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.65625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.008123245085588694, 'batch_size': 256, 'epochs': 47, 'hidden_size': 161, 'dropout': 0.09091248360355032, 'weight_decay': 5.415244119402536e-06, 'label_smoothing': 0.06084844859190756, 'grad_clip_norm': 2.8614039423450706, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 27373, 'model_storage_size_kb': 58.809179687500006, 'model_size_validation': 'PASS'}
2025-10-13 22:40:51,206 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:40:51,206 - INFO - _models.training_function_executor - Model: 27,373 parameters, 58.8KB (PASS 256KB limit)
2025-10-13 22:40:51,206 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.078s
2025-10-13 22:40:51,206 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:40:51,206 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-13 22:40:51,207 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 0.008123245085588694, 'batch_size': 256, 'epochs': np.int64(47), 'hidden_size': np.int64(161), 'dropout': 0.09091248360355032, 'weight_decay': 5.415244119402536e-06, 'label_smoothing': 0.06084844859190756, 'grad_clip_norm': 2.8614039423450706, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, value=0.7188
2025-10-13 22:40:51,207 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 0.008123245085588694, 'batch_size': 256, 'epochs': np.int64(47), 'hidden_size': np.int64(161), 'dropout': 0.09091248360355032, 'weight_decay': 5.415244119402536e-06, 'label_smoothing': 0.06084844859190756, 'grad_clip_norm': 2.8614039423450706, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False} -> 0.7188
2025-10-13 22:40:51,207 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-10-13 22:40:51,207 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-13 22:40:51,207 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 3 (NaN monitoring active)
2025-10-13 22:40:51,207 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:40:51,207 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:40:51,207 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.6210878782654418e-05, 'batch_size': 96, 'epochs': 24, 'hidden_size': 82, 'dropout': 0.19123099563358142, 'weight_decay': 0.008568869785189018, 'label_smoothing': 0.09335257864959601, 'grad_clip_norm': 4.369731830313443, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:40:51,208 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.6210878782654418e-05, 'batch_size': 96, 'epochs': 24, 'hidden_size': 82, 'dropout': 0.19123099563358142, 'weight_decay': 0.008568869785189018, 'label_smoothing': 0.09335257864959601, 'grad_clip_norm': 4.369731830313443, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:40:54,796 - INFO - _models.training_function_executor - Epoch 001/024 | train_loss=9.7769 | val_loss=8.0536 | val_acc=0.1875
2025-10-13 22:40:54,804 - INFO - _models.training_function_executor - Epoch 002/024 | train_loss=9.7773 | val_loss=7.9762 | val_acc=0.1875
2025-10-13 22:40:54,810 - INFO - _models.training_function_executor - Epoch 003/024 | train_loss=9.4951 | val_loss=7.7509 | val_acc=0.1875
2025-10-13 22:40:54,816 - INFO - _models.training_function_executor - Epoch 004/024 | train_loss=9.4589 | val_loss=7.5250 | val_acc=0.1875
2025-10-13 22:40:54,822 - INFO - _models.training_function_executor - Epoch 005/024 | train_loss=9.2452 | val_loss=7.3106 | val_acc=0.1875
2025-10-13 22:40:54,828 - INFO - _models.training_function_executor - Epoch 006/024 | train_loss=9.4633 | val_loss=7.1030 | val_acc=0.1875
2025-10-13 22:40:54,833 - INFO - _models.training_function_executor - Epoch 007/024 | train_loss=9.1911 | val_loss=6.9071 | val_acc=0.1875
2025-10-13 22:40:54,839 - INFO - _models.training_function_executor - Epoch 008/024 | train_loss=9.3145 | val_loss=6.7242 | val_acc=0.1875
2025-10-13 22:40:54,845 - INFO - _models.training_function_executor - Epoch 009/024 | train_loss=9.1285 | val_loss=6.5533 | val_acc=0.1875
2025-10-13 22:40:54,850 - INFO - _models.training_function_executor - Epoch 010/024 | train_loss=8.8986 | val_loss=6.3932 | val_acc=0.1875
2025-10-13 22:40:54,855 - INFO - _models.training_function_executor - Epoch 011/024 | train_loss=8.4293 | val_loss=6.2516 | val_acc=0.1875
2025-10-13 22:40:54,861 - INFO - _models.training_function_executor - Epoch 012/024 | train_loss=8.6100 | val_loss=6.1244 | val_acc=0.1875
2025-10-13 22:40:54,866 - INFO - _models.training_function_executor - Epoch 013/024 | train_loss=7.9663 | val_loss=6.0077 | val_acc=0.1875
2025-10-13 22:40:54,871 - INFO - _models.training_function_executor - Epoch 014/024 | train_loss=8.1883 | val_loss=5.9101 | val_acc=0.1875
2025-10-13 22:40:54,877 - INFO - _models.training_function_executor - Epoch 015/024 | train_loss=7.4801 | val_loss=5.8266 | val_acc=0.1875
2025-10-13 22:40:54,882 - INFO - _models.training_function_executor - Epoch 016/024 | train_loss=7.7145 | val_loss=5.7558 | val_acc=0.1875
2025-10-13 22:40:54,887 - INFO - _models.training_function_executor - Epoch 017/024 | train_loss=8.1967 | val_loss=5.6986 | val_acc=0.1875
2025-10-13 22:40:54,892 - INFO - _models.training_function_executor - Epoch 018/024 | train_loss=7.5855 | val_loss=5.6547 | val_acc=0.1875
2025-10-13 22:40:54,897 - INFO - _models.training_function_executor - Epoch 019/024 | train_loss=7.5170 | val_loss=5.6237 | val_acc=0.1875
2025-10-13 22:40:54,902 - INFO - _models.training_function_executor - Epoch 020/024 | train_loss=7.6003 | val_loss=5.6019 | val_acc=0.1875
2025-10-13 22:40:54,907 - INFO - _models.training_function_executor - Epoch 021/024 | train_loss=7.2816 | val_loss=5.5888 | val_acc=0.1875
2025-10-13 22:40:54,912 - INFO - _models.training_function_executor - Epoch 022/024 | train_loss=8.1880 | val_loss=5.5785 | val_acc=0.1875
2025-10-13 22:40:54,916 - INFO - _models.training_function_executor - Epoch 023/024 | train_loss=7.2023 | val_loss=5.5741 | val_acc=0.1875
2025-10-13 22:40:54,921 - INFO - _models.training_function_executor - Epoch 024/024 | train_loss=7.9964 | val_loss=5.5732 | val_acc=0.1875
2025-10-13 22:40:55,756 - INFO - _models.training_function_executor - Model: 7,465 parameters, 32.1KB storage
2025-10-13 22:40:55,756 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [9.776934027671814, 9.777323961257935, 9.495090246200562, 9.458894848823547, 9.245202898979187, 9.463286757469177, 9.19109857082367, 9.31454336643219, 9.128538608551025, 8.898604154586792, 8.42927873134613, 8.609967470169067, 7.966349542140961, 8.188305139541626, 7.480058968067169, 7.714466750621796, 8.196667730808258, 7.58553546667099, 7.517009496688843, 7.600256443023682, 7.281633257865906, 8.18803060054779, 7.202278912067413, 7.996422111988068], 'val_losses': [8.053634643554688, 7.976184844970703, 7.750943183898926, 7.5249738693237305, 7.310576438903809, 7.102969169616699, 6.907139301300049, 6.724163055419922, 6.5533318519592285, 6.393194198608398, 6.2515764236450195, 6.124357223510742, 6.007730484008789, 5.910149574279785, 5.826643943786621, 5.755819797515869, 5.698635101318359, 5.654676914215088, 5.623672962188721, 5.601878643035889, 5.58879280090332, 5.578532695770264, 5.574080944061279, 5.57322359085083], 'val_acc': [0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.6210878782654418e-05, 'batch_size': 96, 'epochs': 24, 'hidden_size': 82, 'dropout': 0.19123099563358142, 'weight_decay': 0.008568869785189018, 'label_smoothing': 0.09335257864959601, 'grad_clip_norm': 4.369731830313443, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 7465, 'model_storage_size_kb': 32.076171875, 'model_size_validation': 'PASS'}
2025-10-13 22:40:55,756 - INFO - _models.training_function_executor - BO Objective: base=0.1875, size_penalty=0.0000, final=0.1875
2025-10-13 22:40:55,756 - INFO - _models.training_function_executor - Model: 7,465 parameters, 32.1KB (PASS 256KB limit)
2025-10-13 22:40:55,756 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.549s
2025-10-13 22:40:55,820 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1875
2025-10-13 22:40:55,820 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.063s
2025-10-13 22:40:55,820 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 2.6210878782654418e-05, 'batch_size': 96, 'epochs': np.int64(24), 'hidden_size': np.int64(82), 'dropout': 0.19123099563358142, 'weight_decay': 0.008568869785189018, 'label_smoothing': 0.09335257864959601, 'grad_clip_norm': 4.369731830313443, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, value=0.1875
2025-10-13 22:40:55,820 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 2.6210878782654418e-05, 'batch_size': 96, 'epochs': np.int64(24), 'hidden_size': np.int64(82), 'dropout': 0.19123099563358142, 'weight_decay': 0.008568869785189018, 'label_smoothing': 0.09335257864959601, 'grad_clip_norm': 4.369731830313443, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True} -> 0.1875
2025-10-13 22:40:55,820 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-10-13 22:40:55,820 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:40:55,820 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 4 (NaN monitoring active)
2025-10-13 22:40:55,820 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:40:55,820 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:40:55,820 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006708188643346291, 'batch_size': 256, 'epochs': 28, 'hidden_size': 190, 'dropout': 0.11760496295172493, 'weight_decay': 3.800147420834506e-05, 'label_smoothing': 0.051245344297846315, 'grad_clip_norm': 4.0716144360993765, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:40:55,821 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006708188643346291, 'batch_size': 256, 'epochs': 28, 'hidden_size': 190, 'dropout': 0.11760496295172493, 'weight_decay': 3.800147420834506e-05, 'label_smoothing': 0.051245344297846315, 'grad_clip_norm': 4.0716144360993765, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:40:59,516 - INFO - _models.training_function_executor - Epoch 001/028 | train_loss=12.9958 | val_loss=11.7048 | val_acc=0.1875
2025-10-13 22:40:59,521 - INFO - _models.training_function_executor - Epoch 002/028 | train_loss=12.3211 | val_loss=11.7048 | val_acc=0.1875
2025-10-13 22:40:59,525 - INFO - _models.training_function_executor - Epoch 003/028 | train_loss=12.6566 | val_loss=11.7048 | val_acc=0.1875
2025-10-13 22:40:59,529 - INFO - _models.training_function_executor - Epoch 004/028 | train_loss=12.8771 | val_loss=11.7048 | val_acc=0.1875
2025-10-13 22:40:59,532 - INFO - _models.training_function_executor - Epoch 005/028 | train_loss=12.6014 | val_loss=11.7048 | val_acc=0.1875
2025-10-13 22:40:59,536 - INFO - _models.training_function_executor - Epoch 006/028 | train_loss=12.4884 | val_loss=23.3360 | val_acc=0.7188
2025-10-13 22:40:59,539 - INFO - _models.training_function_executor - Epoch 007/028 | train_loss=22.8214 | val_loss=15.6192 | val_acc=0.7188
2025-10-13 22:40:59,543 - INFO - _models.training_function_executor - Epoch 008/028 | train_loss=15.6146 | val_loss=14.3560 | val_acc=0.0938
2025-10-13 22:40:59,546 - INFO - _models.training_function_executor - Epoch 009/028 | train_loss=14.2079 | val_loss=14.3560 | val_acc=0.0938
2025-10-13 22:40:59,549 - INFO - _models.training_function_executor - Epoch 010/028 | train_loss=14.5277 | val_loss=8.4120 | val_acc=0.0938
2025-10-13 22:40:59,553 - INFO - _models.training_function_executor - Epoch 011/028 | train_loss=8.8625 | val_loss=5.1023 | val_acc=0.7188
2025-10-13 22:40:59,556 - INFO - _models.training_function_executor - Epoch 012/028 | train_loss=5.1422 | val_loss=3.6522 | val_acc=0.7188
2025-10-13 22:40:59,559 - INFO - _models.training_function_executor - Epoch 013/028 | train_loss=3.7231 | val_loss=1.7590 | val_acc=0.7188
2025-10-13 22:40:59,563 - INFO - _models.training_function_executor - Epoch 014/028 | train_loss=2.0830 | val_loss=1.7533 | val_acc=0.2656
2025-10-13 22:40:59,566 - INFO - _models.training_function_executor - Epoch 015/028 | train_loss=1.9679 | val_loss=1.8885 | val_acc=0.1875
2025-10-13 22:40:59,569 - INFO - _models.training_function_executor - Epoch 016/028 | train_loss=2.0005 | val_loss=1.5230 | val_acc=0.1875
2025-10-13 22:40:59,573 - INFO - _models.training_function_executor - Epoch 017/028 | train_loss=1.9420 | val_loss=1.2424 | val_acc=0.2969
2025-10-13 22:40:59,576 - INFO - _models.training_function_executor - Epoch 018/028 | train_loss=1.5972 | val_loss=0.8300 | val_acc=0.7188
2025-10-13 22:40:59,579 - INFO - _models.training_function_executor - Epoch 019/028 | train_loss=1.1825 | val_loss=0.8391 | val_acc=0.7188
2025-10-13 22:40:59,583 - INFO - _models.training_function_executor - Epoch 020/028 | train_loss=1.0481 | val_loss=0.9270 | val_acc=0.7188
2025-10-13 22:40:59,586 - INFO - _models.training_function_executor - Epoch 021/028 | train_loss=1.0544 | val_loss=0.9256 | val_acc=0.7188
2025-10-13 22:40:59,589 - INFO - _models.training_function_executor - Epoch 022/028 | train_loss=1.0455 | val_loss=0.8754 | val_acc=0.7188
2025-10-13 22:40:59,593 - INFO - _models.training_function_executor - Epoch 023/028 | train_loss=0.9990 | val_loss=0.8231 | val_acc=0.7188
2025-10-13 22:40:59,596 - INFO - _models.training_function_executor - Epoch 024/028 | train_loss=0.8924 | val_loss=0.7899 | val_acc=0.7188
2025-10-13 22:40:59,599 - INFO - _models.training_function_executor - Epoch 025/028 | train_loss=0.9043 | val_loss=0.7735 | val_acc=0.7188
2025-10-13 22:40:59,602 - INFO - _models.training_function_executor - Epoch 026/028 | train_loss=0.9725 | val_loss=0.7667 | val_acc=0.7188
2025-10-13 22:40:59,606 - INFO - _models.training_function_executor - Epoch 027/028 | train_loss=0.9410 | val_loss=0.7637 | val_acc=0.7188
2025-10-13 22:40:59,609 - INFO - _models.training_function_executor - Epoch 028/028 | train_loss=0.9160 | val_loss=0.7633 | val_acc=0.7188
2025-10-13 22:41:00,435 - INFO - _models.training_function_executor - Model: 37,813 parameters, 162.5KB storage
2025-10-13 22:41:00,435 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [12.995781898498535, 12.321062088012695, 12.656627655029297, 12.877095222473145, 12.60144329071045, 12.488350868225098, 22.821388244628906, 15.614582061767578, 14.207911491394043, 14.527734756469727, 8.862541198730469, 5.142214298248291, 3.7230684757232666, 2.0830280780792236, 1.9679012298583984, 2.000455617904663, 1.94198477268219, 1.5971580743789673, 1.1825401782989502, 1.0481053590774536, 1.0543795824050903, 1.045459270477295, 0.9990394711494446, 0.8924244046211243, 0.9043036699295044, 0.9724705219268799, 0.9409624338150024, 0.9159610271453857], 'val_losses': [11.704761505126953, 11.704761505126953, 11.704761505126953, 11.704761505126953, 11.704761505126953, 23.335952758789062, 15.619209289550781, 14.356019020080566, 14.356019020080566, 8.412019729614258, 5.10228967666626, 3.6521685123443604, 1.758953332901001, 1.7533143758773804, 1.888539433479309, 1.5230357646942139, 1.2424103021621704, 0.8300091028213501, 0.8390829563140869, 0.9270099401473999, 0.9256298542022705, 0.8753647208213806, 0.8230572938919067, 0.7899202108383179, 0.7735242247581482, 0.766658365726471, 0.7637229561805725, 0.7633451223373413], 'val_acc': [0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.71875, 0.71875, 0.09375, 0.09375, 0.09375, 0.71875, 0.71875, 0.71875, 0.265625, 0.1875, 0.1875, 0.296875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006708188643346291, 'batch_size': 256, 'epochs': 28, 'hidden_size': 190, 'dropout': 0.11760496295172493, 'weight_decay': 3.800147420834506e-05, 'label_smoothing': 0.051245344297846315, 'grad_clip_norm': 4.0716144360993765, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 37813, 'model_storage_size_kb': 162.477734375, 'model_size_validation': 'PASS'}
2025-10-13 22:41:00,435 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:41:00,435 - INFO - _models.training_function_executor - Model: 37,813 parameters, 162.5KB (PASS 256KB limit)
2025-10-13 22:41:00,435 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.615s
2025-10-13 22:41:00,496 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:41:00,496 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.060s
2025-10-13 22:41:00,496 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 0.006708188643346291, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'hidden_size': np.int64(190), 'dropout': 0.11760496295172493, 'weight_decay': 3.800147420834506e-05, 'label_smoothing': 0.051245344297846315, 'grad_clip_norm': 4.0716144360993765, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:41:00,496 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 0.006708188643346291, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'hidden_size': np.int64(190), 'dropout': 0.11760496295172493, 'weight_decay': 3.800147420834506e-05, 'label_smoothing': 0.051245344297846315, 'grad_clip_norm': 4.0716144360993765, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:41:00,496 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-10-13 22:41:00,496 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:00,496 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 5 (NaN monitoring active)
2025-10-13 22:41:00,496 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:00,496 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:00,496 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004336357109516722, 'batch_size': 256, 'epochs': 143, 'hidden_size': 164, 'dropout': 0.3428277251587937, 'weight_decay': 0.00015887871249660882, 'label_smoothing': 0.10866460470182046, 'grad_clip_norm': 1.709999679568698, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:00,497 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004336357109516722, 'batch_size': 256, 'epochs': 143, 'hidden_size': 164, 'dropout': 0.3428277251587937, 'weight_decay': 0.00015887871249660882, 'label_smoothing': 0.10866460470182046, 'grad_clip_norm': 1.709999679568698, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:04,406 - INFO - _models.training_function_executor - Epoch 001/143 | train_loss=12.3844 | val_loss=11.3868 | val_acc=0.0938
2025-10-13 22:41:04,411 - INFO - _models.training_function_executor - Epoch 002/143 | train_loss=12.2438 | val_loss=11.3868 | val_acc=0.0938
2025-10-13 22:41:04,415 - INFO - _models.training_function_executor - Epoch 003/143 | train_loss=12.5749 | val_loss=11.3868 | val_acc=0.0938
2025-10-13 22:41:04,419 - INFO - _models.training_function_executor - Epoch 004/143 | train_loss=12.6497 | val_loss=11.3868 | val_acc=0.0938
2025-10-13 22:41:04,423 - INFO - _models.training_function_executor - Epoch 005/143 | train_loss=12.7937 | val_loss=11.3868 | val_acc=0.0938
2025-10-13 22:41:04,427 - INFO - _models.training_function_executor - Epoch 006/143 | train_loss=12.4484 | val_loss=11.3868 | val_acc=0.0938
2025-10-13 22:41:04,430 - INFO - _models.training_function_executor - Epoch 007/143 | train_loss=12.2361 | val_loss=13.7812 | val_acc=0.7188
2025-10-13 22:41:04,433 - INFO - _models.training_function_executor - Epoch 008/143 | train_loss=14.5424 | val_loss=8.5435 | val_acc=0.7188
2025-10-13 22:41:04,436 - INFO - _models.training_function_executor - Epoch 009/143 | train_loss=10.5633 | val_loss=7.4282 | val_acc=0.1875
2025-10-13 22:41:04,440 - INFO - _models.training_function_executor - Epoch 010/143 | train_loss=7.8299 | val_loss=4.6672 | val_acc=0.1875
2025-10-13 22:41:04,443 - INFO - _models.training_function_executor - Epoch 011/143 | train_loss=5.8050 | val_loss=1.3174 | val_acc=0.7188
2025-10-13 22:41:04,446 - INFO - _models.training_function_executor - Epoch 012/143 | train_loss=3.4036 | val_loss=1.9200 | val_acc=0.7188
2025-10-13 22:41:04,449 - INFO - _models.training_function_executor - Epoch 013/143 | train_loss=3.1027 | val_loss=2.0921 | val_acc=0.6250
2025-10-13 22:41:04,453 - INFO - _models.training_function_executor - Epoch 014/143 | train_loss=3.1374 | val_loss=1.5533 | val_acc=0.7188
2025-10-13 22:41:04,456 - INFO - _models.training_function_executor - Epoch 015/143 | train_loss=2.6164 | val_loss=0.9618 | val_acc=0.7188
2025-10-13 22:41:04,460 - INFO - _models.training_function_executor - Epoch 016/143 | train_loss=1.5794 | val_loss=1.0833 | val_acc=0.3906
2025-10-13 22:41:04,463 - INFO - _models.training_function_executor - Epoch 017/143 | train_loss=1.3701 | val_loss=1.2043 | val_acc=0.1875
2025-10-13 22:41:04,466 - INFO - _models.training_function_executor - Epoch 018/143 | train_loss=1.3515 | val_loss=1.0674 | val_acc=0.2500
2025-10-13 22:41:04,470 - INFO - _models.training_function_executor - Epoch 019/143 | train_loss=1.2535 | val_loss=0.9804 | val_acc=0.6875
2025-10-13 22:41:04,473 - INFO - _models.training_function_executor - Epoch 020/143 | train_loss=1.0495 | val_loss=0.9478 | val_acc=0.7188
2025-10-13 22:41:04,476 - INFO - _models.training_function_executor - Epoch 021/143 | train_loss=1.0012 | val_loss=0.9913 | val_acc=0.7188
2025-10-13 22:41:04,479 - INFO - _models.training_function_executor - Epoch 022/143 | train_loss=0.9144 | val_loss=0.9492 | val_acc=0.7188
2025-10-13 22:41:04,483 - INFO - _models.training_function_executor - Epoch 023/143 | train_loss=0.9000 | val_loss=0.8615 | val_acc=0.7188
2025-10-13 22:41:04,486 - INFO - _models.training_function_executor - Epoch 024/143 | train_loss=0.9183 | val_loss=0.8297 | val_acc=0.7188
2025-10-13 22:41:04,489 - INFO - _models.training_function_executor - Epoch 025/143 | train_loss=0.9161 | val_loss=0.8229 | val_acc=0.7188
2025-10-13 22:41:04,492 - INFO - _models.training_function_executor - Epoch 026/143 | train_loss=0.9298 | val_loss=0.8508 | val_acc=0.7188
2025-10-13 22:41:04,495 - INFO - _models.training_function_executor - Epoch 027/143 | train_loss=0.8800 | val_loss=0.8642 | val_acc=0.7188
2025-10-13 22:41:04,498 - INFO - _models.training_function_executor - Epoch 028/143 | train_loss=0.8862 | val_loss=0.8373 | val_acc=0.7188
2025-10-13 22:41:04,502 - INFO - _models.training_function_executor - Epoch 029/143 | train_loss=0.8773 | val_loss=0.8146 | val_acc=0.7188
2025-10-13 22:41:04,505 - INFO - _models.training_function_executor - Epoch 030/143 | train_loss=0.8902 | val_loss=0.7959 | val_acc=0.7188
2025-10-13 22:41:04,508 - INFO - _models.training_function_executor - Epoch 031/143 | train_loss=0.8476 | val_loss=0.7677 | val_acc=0.7188
2025-10-13 22:41:04,512 - INFO - _models.training_function_executor - Epoch 032/143 | train_loss=0.8851 | val_loss=0.7676 | val_acc=0.7188
2025-10-13 22:41:04,515 - INFO - _models.training_function_executor - Epoch 033/143 | train_loss=0.8880 | val_loss=0.7850 | val_acc=0.7188
2025-10-13 22:41:04,518 - INFO - _models.training_function_executor - Epoch 034/143 | train_loss=0.8731 | val_loss=0.8191 | val_acc=0.7188
2025-10-13 22:41:04,521 - INFO - _models.training_function_executor - Epoch 035/143 | train_loss=0.8508 | val_loss=0.8201 | val_acc=0.7188
2025-10-13 22:41:04,525 - INFO - _models.training_function_executor - Epoch 036/143 | train_loss=0.8580 | val_loss=0.7912 | val_acc=0.7188
2025-10-13 22:41:04,528 - INFO - _models.training_function_executor - Epoch 037/143 | train_loss=0.8436 | val_loss=0.7662 | val_acc=0.7188
2025-10-13 22:41:04,531 - INFO - _models.training_function_executor - Epoch 038/143 | train_loss=0.8083 | val_loss=0.7494 | val_acc=0.7188
2025-10-13 22:41:04,534 - INFO - _models.training_function_executor - Epoch 039/143 | train_loss=0.8110 | val_loss=0.7525 | val_acc=0.7188
2025-10-13 22:41:04,538 - INFO - _models.training_function_executor - Epoch 040/143 | train_loss=0.8233 | val_loss=0.7673 | val_acc=0.7188
2025-10-13 22:41:04,541 - INFO - _models.training_function_executor - Epoch 041/143 | train_loss=0.8326 | val_loss=0.7893 | val_acc=0.7188
2025-10-13 22:41:04,544 - INFO - _models.training_function_executor - Epoch 042/143 | train_loss=0.8351 | val_loss=0.7912 | val_acc=0.7188
2025-10-13 22:41:04,547 - INFO - _models.training_function_executor - Epoch 043/143 | train_loss=0.8472 | val_loss=0.7771 | val_acc=0.7188
2025-10-13 22:41:04,551 - INFO - _models.training_function_executor - Epoch 044/143 | train_loss=0.8444 | val_loss=0.7629 | val_acc=0.7188
2025-10-13 22:41:04,554 - INFO - _models.training_function_executor - Epoch 045/143 | train_loss=0.8321 | val_loss=0.7514 | val_acc=0.7188
2025-10-13 22:41:04,557 - INFO - _models.training_function_executor - Epoch 046/143 | train_loss=0.8676 | val_loss=0.7530 | val_acc=0.7188
2025-10-13 22:41:04,560 - INFO - _models.training_function_executor - Epoch 047/143 | train_loss=0.8247 | val_loss=0.7552 | val_acc=0.7188
2025-10-13 22:41:04,563 - INFO - _models.training_function_executor - Epoch 048/143 | train_loss=0.8174 | val_loss=0.7549 | val_acc=0.7188
2025-10-13 22:41:04,566 - INFO - _models.training_function_executor - Epoch 049/143 | train_loss=0.8105 | val_loss=0.7527 | val_acc=0.7188
2025-10-13 22:41:04,570 - INFO - _models.training_function_executor - Epoch 050/143 | train_loss=0.8088 | val_loss=0.7470 | val_acc=0.7188
2025-10-13 22:41:04,573 - INFO - _models.training_function_executor - Epoch 051/143 | train_loss=0.8177 | val_loss=0.7441 | val_acc=0.7188
2025-10-13 22:41:04,576 - INFO - _models.training_function_executor - Epoch 052/143 | train_loss=0.8158 | val_loss=0.7401 | val_acc=0.7188
2025-10-13 22:41:04,579 - INFO - _models.training_function_executor - Epoch 053/143 | train_loss=0.7948 | val_loss=0.7384 | val_acc=0.7188
2025-10-13 22:41:04,582 - INFO - _models.training_function_executor - Epoch 054/143 | train_loss=0.8133 | val_loss=0.7381 | val_acc=0.7188
2025-10-13 22:41:04,585 - INFO - _models.training_function_executor - Epoch 055/143 | train_loss=0.8289 | val_loss=0.7440 | val_acc=0.7188
2025-10-13 22:41:04,588 - INFO - _models.training_function_executor - Epoch 056/143 | train_loss=0.8187 | val_loss=0.7494 | val_acc=0.7188
2025-10-13 22:41:04,591 - INFO - _models.training_function_executor - Epoch 057/143 | train_loss=0.8104 | val_loss=0.7508 | val_acc=0.7188
2025-10-13 22:41:04,595 - INFO - _models.training_function_executor - Epoch 058/143 | train_loss=0.8033 | val_loss=0.7459 | val_acc=0.7188
2025-10-13 22:41:04,598 - INFO - _models.training_function_executor - Epoch 059/143 | train_loss=0.7988 | val_loss=0.7372 | val_acc=0.7188
2025-10-13 22:41:04,601 - INFO - _models.training_function_executor - Epoch 060/143 | train_loss=0.8074 | val_loss=0.7336 | val_acc=0.7188
2025-10-13 22:41:04,604 - INFO - _models.training_function_executor - Epoch 061/143 | train_loss=0.8226 | val_loss=0.7336 | val_acc=0.7188
2025-10-13 22:41:04,607 - INFO - _models.training_function_executor - Epoch 062/143 | train_loss=0.7974 | val_loss=0.7338 | val_acc=0.7188
2025-10-13 22:41:04,610 - INFO - _models.training_function_executor - Epoch 063/143 | train_loss=0.8146 | val_loss=0.7366 | val_acc=0.7188
2025-10-13 22:41:04,613 - INFO - _models.training_function_executor - Epoch 064/143 | train_loss=0.7951 | val_loss=0.7375 | val_acc=0.7188
2025-10-13 22:41:04,616 - INFO - _models.training_function_executor - Epoch 065/143 | train_loss=0.8131 | val_loss=0.7378 | val_acc=0.7188
2025-10-13 22:41:04,620 - INFO - _models.training_function_executor - Epoch 066/143 | train_loss=0.7965 | val_loss=0.7350 | val_acc=0.7188
2025-10-13 22:41:04,622 - INFO - _models.training_function_executor - Epoch 067/143 | train_loss=0.8118 | val_loss=0.7322 | val_acc=0.7188
2025-10-13 22:41:04,625 - INFO - _models.training_function_executor - Epoch 068/143 | train_loss=0.7961 | val_loss=0.7316 | val_acc=0.7188
2025-10-13 22:41:04,628 - INFO - _models.training_function_executor - Epoch 069/143 | train_loss=0.8069 | val_loss=0.7329 | val_acc=0.7188
2025-10-13 22:41:04,632 - INFO - _models.training_function_executor - Epoch 070/143 | train_loss=0.7837 | val_loss=0.7340 | val_acc=0.7188
2025-10-13 22:41:04,635 - INFO - _models.training_function_executor - Epoch 071/143 | train_loss=0.7944 | val_loss=0.7361 | val_acc=0.7188
2025-10-13 22:41:04,638 - INFO - _models.training_function_executor - Epoch 072/143 | train_loss=0.7996 | val_loss=0.7377 | val_acc=0.7188
2025-10-13 22:41:04,642 - INFO - _models.training_function_executor - Epoch 073/143 | train_loss=0.8026 | val_loss=0.7396 | val_acc=0.7188
2025-10-13 22:41:04,645 - INFO - _models.training_function_executor - Epoch 074/143 | train_loss=0.8023 | val_loss=0.7417 | val_acc=0.7188
2025-10-13 22:41:04,648 - INFO - _models.training_function_executor - Epoch 075/143 | train_loss=0.8016 | val_loss=0.7428 | val_acc=0.7188
2025-10-13 22:41:04,651 - INFO - _models.training_function_executor - Epoch 076/143 | train_loss=0.7877 | val_loss=0.7426 | val_acc=0.7188
2025-10-13 22:41:04,654 - INFO - _models.training_function_executor - Epoch 077/143 | train_loss=0.7955 | val_loss=0.7414 | val_acc=0.7188
2025-10-13 22:41:04,657 - INFO - _models.training_function_executor - Epoch 078/143 | train_loss=0.7756 | val_loss=0.7374 | val_acc=0.7188
2025-10-13 22:41:04,660 - INFO - _models.training_function_executor - Epoch 079/143 | train_loss=0.8046 | val_loss=0.7346 | val_acc=0.7188
2025-10-13 22:41:04,663 - INFO - _models.training_function_executor - Epoch 080/143 | train_loss=0.8111 | val_loss=0.7335 | val_acc=0.7188
2025-10-13 22:41:04,667 - INFO - _models.training_function_executor - Epoch 081/143 | train_loss=0.8219 | val_loss=0.7345 | val_acc=0.7188
2025-10-13 22:41:04,670 - INFO - _models.training_function_executor - Epoch 082/143 | train_loss=0.7862 | val_loss=0.7365 | val_acc=0.7188
2025-10-13 22:41:04,673 - INFO - _models.training_function_executor - Epoch 083/143 | train_loss=0.7899 | val_loss=0.7368 | val_acc=0.7188
2025-10-13 22:41:04,676 - INFO - _models.training_function_executor - Epoch 084/143 | train_loss=0.7909 | val_loss=0.7367 | val_acc=0.7188
2025-10-13 22:41:04,679 - INFO - _models.training_function_executor - Epoch 085/143 | train_loss=0.7904 | val_loss=0.7383 | val_acc=0.7188
2025-10-13 22:41:04,682 - INFO - _models.training_function_executor - Epoch 086/143 | train_loss=0.8029 | val_loss=0.7396 | val_acc=0.7188
2025-10-13 22:41:04,685 - INFO - _models.training_function_executor - Epoch 087/143 | train_loss=0.7884 | val_loss=0.7411 | val_acc=0.7188
2025-10-13 22:41:04,688 - INFO - _models.training_function_executor - Epoch 088/143 | train_loss=0.7885 | val_loss=0.7406 | val_acc=0.7188
2025-10-13 22:41:04,691 - INFO - _models.training_function_executor - Epoch 089/143 | train_loss=0.8043 | val_loss=0.7400 | val_acc=0.7188
2025-10-13 22:41:04,694 - INFO - _models.training_function_executor - Epoch 090/143 | train_loss=0.7814 | val_loss=0.7381 | val_acc=0.7188
2025-10-13 22:41:04,697 - INFO - _models.training_function_executor - Epoch 091/143 | train_loss=0.7862 | val_loss=0.7370 | val_acc=0.7188
2025-10-13 22:41:04,701 - INFO - _models.training_function_executor - Epoch 092/143 | train_loss=0.7888 | val_loss=0.7357 | val_acc=0.7188
2025-10-13 22:41:04,704 - INFO - _models.training_function_executor - Epoch 093/143 | train_loss=0.7967 | val_loss=0.7349 | val_acc=0.7188
2025-10-13 22:41:04,707 - INFO - _models.training_function_executor - Epoch 094/143 | train_loss=0.7980 | val_loss=0.7343 | val_acc=0.7188
2025-10-13 22:41:04,710 - INFO - _models.training_function_executor - Epoch 095/143 | train_loss=0.7833 | val_loss=0.7342 | val_acc=0.7188
2025-10-13 22:41:04,713 - INFO - _models.training_function_executor - Epoch 096/143 | train_loss=0.7995 | val_loss=0.7345 | val_acc=0.7188
2025-10-13 22:41:04,716 - INFO - _models.training_function_executor - Epoch 097/143 | train_loss=0.7915 | val_loss=0.7347 | val_acc=0.7188
2025-10-13 22:41:04,719 - INFO - _models.training_function_executor - Epoch 098/143 | train_loss=0.8048 | val_loss=0.7352 | val_acc=0.7188
2025-10-13 22:41:04,722 - INFO - _models.training_function_executor - Epoch 099/143 | train_loss=0.7842 | val_loss=0.7349 | val_acc=0.7188
2025-10-13 22:41:04,725 - INFO - _models.training_function_executor - Epoch 100/143 | train_loss=0.7850 | val_loss=0.7344 | val_acc=0.7188
2025-10-13 22:41:04,728 - INFO - _models.training_function_executor - Epoch 101/143 | train_loss=0.7900 | val_loss=0.7335 | val_acc=0.7188
2025-10-13 22:41:04,731 - INFO - _models.training_function_executor - Epoch 102/143 | train_loss=0.7962 | val_loss=0.7327 | val_acc=0.7188
2025-10-13 22:41:04,735 - INFO - _models.training_function_executor - Epoch 103/143 | train_loss=0.7787 | val_loss=0.7320 | val_acc=0.7188
2025-10-13 22:41:04,738 - INFO - _models.training_function_executor - Epoch 104/143 | train_loss=0.7645 | val_loss=0.7310 | val_acc=0.7188
2025-10-13 22:41:04,741 - INFO - _models.training_function_executor - Epoch 105/143 | train_loss=0.7911 | val_loss=0.7305 | val_acc=0.7188
2025-10-13 22:41:04,744 - INFO - _models.training_function_executor - Epoch 106/143 | train_loss=0.7950 | val_loss=0.7304 | val_acc=0.7188
2025-10-13 22:41:04,747 - INFO - _models.training_function_executor - Epoch 107/143 | train_loss=0.7852 | val_loss=0.7307 | val_acc=0.7188
2025-10-13 22:41:04,750 - INFO - _models.training_function_executor - Epoch 108/143 | train_loss=0.8026 | val_loss=0.7314 | val_acc=0.7188
2025-10-13 22:41:04,754 - INFO - _models.training_function_executor - Epoch 109/143 | train_loss=0.7856 | val_loss=0.7323 | val_acc=0.7188
2025-10-13 22:41:04,757 - INFO - _models.training_function_executor - Epoch 110/143 | train_loss=0.7988 | val_loss=0.7334 | val_acc=0.7188
2025-10-13 22:41:04,760 - INFO - _models.training_function_executor - Epoch 111/143 | train_loss=0.7869 | val_loss=0.7345 | val_acc=0.7188
2025-10-13 22:41:04,763 - INFO - _models.training_function_executor - Epoch 112/143 | train_loss=0.7844 | val_loss=0.7352 | val_acc=0.7188
2025-10-13 22:41:04,766 - INFO - _models.training_function_executor - Epoch 113/143 | train_loss=0.7746 | val_loss=0.7357 | val_acc=0.7188
2025-10-13 22:41:04,769 - INFO - _models.training_function_executor - Epoch 114/143 | train_loss=0.7864 | val_loss=0.7357 | val_acc=0.7188
2025-10-13 22:41:04,772 - INFO - _models.training_function_executor - Epoch 115/143 | train_loss=0.8000 | val_loss=0.7357 | val_acc=0.7188
2025-10-13 22:41:04,775 - INFO - _models.training_function_executor - Epoch 116/143 | train_loss=0.7909 | val_loss=0.7356 | val_acc=0.7188
2025-10-13 22:41:04,778 - INFO - _models.training_function_executor - Epoch 117/143 | train_loss=0.7834 | val_loss=0.7353 | val_acc=0.7188
2025-10-13 22:41:04,781 - INFO - _models.training_function_executor - Epoch 118/143 | train_loss=0.7908 | val_loss=0.7348 | val_acc=0.7188
2025-10-13 22:41:04,784 - INFO - _models.training_function_executor - Epoch 119/143 | train_loss=0.8019 | val_loss=0.7344 | val_acc=0.7188
2025-10-13 22:41:04,788 - INFO - _models.training_function_executor - Epoch 120/143 | train_loss=0.7931 | val_loss=0.7340 | val_acc=0.7188
2025-10-13 22:41:04,791 - INFO - _models.training_function_executor - Epoch 121/143 | train_loss=0.7948 | val_loss=0.7337 | val_acc=0.7188
2025-10-13 22:41:04,794 - INFO - _models.training_function_executor - Epoch 122/143 | train_loss=0.7877 | val_loss=0.7336 | val_acc=0.7188
2025-10-13 22:41:04,797 - INFO - _models.training_function_executor - Epoch 123/143 | train_loss=0.7852 | val_loss=0.7333 | val_acc=0.7188
2025-10-13 22:41:04,800 - INFO - _models.training_function_executor - Epoch 124/143 | train_loss=0.7907 | val_loss=0.7332 | val_acc=0.7188
2025-10-13 22:41:04,803 - INFO - _models.training_function_executor - Epoch 125/143 | train_loss=0.7882 | val_loss=0.7331 | val_acc=0.7188
2025-10-13 22:41:04,807 - INFO - _models.training_function_executor - Epoch 126/143 | train_loss=0.8029 | val_loss=0.7332 | val_acc=0.7188
2025-10-13 22:41:04,810 - INFO - _models.training_function_executor - Epoch 127/143 | train_loss=0.7987 | val_loss=0.7332 | val_acc=0.7188
2025-10-13 22:41:04,813 - INFO - _models.training_function_executor - Epoch 128/143 | train_loss=0.7889 | val_loss=0.7334 | val_acc=0.7188
2025-10-13 22:41:04,816 - INFO - _models.training_function_executor - Epoch 129/143 | train_loss=0.7878 | val_loss=0.7335 | val_acc=0.7188
2025-10-13 22:41:04,819 - INFO - _models.training_function_executor - Epoch 130/143 | train_loss=0.7810 | val_loss=0.7336 | val_acc=0.7188
2025-10-13 22:41:04,823 - INFO - _models.training_function_executor - Epoch 131/143 | train_loss=0.7819 | val_loss=0.7336 | val_acc=0.7188
2025-10-13 22:41:04,826 - INFO - _models.training_function_executor - Epoch 132/143 | train_loss=0.7960 | val_loss=0.7336 | val_acc=0.7188
2025-10-13 22:41:04,829 - INFO - _models.training_function_executor - Epoch 133/143 | train_loss=0.7892 | val_loss=0.7337 | val_acc=0.7188
2025-10-13 22:41:04,832 - INFO - _models.training_function_executor - Epoch 134/143 | train_loss=0.7898 | val_loss=0.7337 | val_acc=0.7188
2025-10-13 22:41:04,835 - INFO - _models.training_function_executor - Epoch 135/143 | train_loss=0.7960 | val_loss=0.7338 | val_acc=0.7188
2025-10-13 22:41:04,838 - INFO - _models.training_function_executor - Epoch 136/143 | train_loss=0.7920 | val_loss=0.7338 | val_acc=0.7188
2025-10-13 22:41:04,841 - INFO - _models.training_function_executor - Epoch 137/143 | train_loss=0.7944 | val_loss=0.7338 | val_acc=0.7188
2025-10-13 22:41:04,844 - INFO - _models.training_function_executor - Epoch 138/143 | train_loss=0.7879 | val_loss=0.7338 | val_acc=0.7188
2025-10-13 22:41:04,847 - INFO - _models.training_function_executor - Epoch 139/143 | train_loss=0.7876 | val_loss=0.7338 | val_acc=0.7188
2025-10-13 22:41:04,851 - INFO - _models.training_function_executor - Epoch 140/143 | train_loss=0.8014 | val_loss=0.7338 | val_acc=0.7188
2025-10-13 22:41:04,854 - INFO - _models.training_function_executor - Epoch 141/143 | train_loss=0.7912 | val_loss=0.7338 | val_acc=0.7188
2025-10-13 22:41:04,857 - INFO - _models.training_function_executor - Epoch 142/143 | train_loss=0.7885 | val_loss=0.7338 | val_acc=0.7188
2025-10-13 22:41:04,860 - INFO - _models.training_function_executor - Epoch 143/143 | train_loss=0.7935 | val_loss=0.7339 | val_acc=0.7188
2025-10-13 22:41:05,707 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-10-13 22:41:05,707 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [12.384387969970703, 12.243824005126953, 12.574868202209473, 12.649744033813477, 12.793745994567871, 12.448390007019043, 12.236127853393555, 14.542426109313965, 10.563297271728516, 7.829860687255859, 5.8050360679626465, 3.4036240577697754, 3.1026744842529297, 3.137354612350464, 2.6163735389709473, 1.5793708562850952, 1.37007737159729, 1.3514702320098877, 1.2535306215286255, 1.0495249032974243, 1.001242756843567, 0.9144213795661926, 0.9000434875488281, 0.9182887077331543, 0.9160943627357483, 0.9298185706138611, 0.8799616098403931, 0.8861973881721497, 0.8773072957992554, 0.8901573419570923, 0.8475805521011353, 0.8850652575492859, 0.8880113363265991, 0.8731378316879272, 0.8508135080337524, 0.8579809665679932, 0.8436204791069031, 0.8082611560821533, 0.8110196590423584, 0.8232855200767517, 0.8326201438903809, 0.8351379632949829, 0.8471980094909668, 0.8444488644599915, 0.8321003913879395, 0.8675631880760193, 0.824672281742096, 0.8173903226852417, 0.8104692697525024, 0.808828592300415, 0.8177300095558167, 0.8158314824104309, 0.794783353805542, 0.8132736086845398, 0.828948974609375, 0.8187131881713867, 0.8103687167167664, 0.8032839298248291, 0.7988290786743164, 0.8074095249176025, 0.8225884437561035, 0.7974244356155396, 0.8146113753318787, 0.7951335310935974, 0.8131314516067505, 0.7964524626731873, 0.8118002414703369, 0.7961041927337646, 0.806884229183197, 0.7836996912956238, 0.7944326400756836, 0.7996030449867249, 0.8026072978973389, 0.8023069500923157, 0.8016143441200256, 0.7876837253570557, 0.7954536080360413, 0.7755515575408936, 0.8046132326126099, 0.8111258149147034, 0.8218744993209839, 0.7861825823783875, 0.7898678779602051, 0.7908881306648254, 0.7904202938079834, 0.8028566837310791, 0.7883628010749817, 0.788490891456604, 0.8043381571769714, 0.7813588380813599, 0.7862486243247986, 0.7888247966766357, 0.7966552972793579, 0.7980190515518188, 0.783340334892273, 0.7994763851165771, 0.7914829254150391, 0.804756760597229, 0.7841557264328003, 0.7850440740585327, 0.7900018095970154, 0.7961523532867432, 0.7786848545074463, 0.7645329833030701, 0.7910606265068054, 0.7949755191802979, 0.7851552963256836, 0.8025813698768616, 0.7856084704399109, 0.798785924911499, 0.7868576049804688, 0.7843984365463257, 0.7746423482894897, 0.7863701581954956, 0.7999618053436279, 0.7909477949142456, 0.783356249332428, 0.7908399701118469, 0.8018649816513062, 0.793065071105957, 0.7948145270347595, 0.7877310514450073, 0.7852115035057068, 0.7906681299209595, 0.7882336378097534, 0.8028877377510071, 0.7986712455749512, 0.7889063358306885, 0.7878461480140686, 0.7810428142547607, 0.7818809747695923, 0.7959917783737183, 0.7892441153526306, 0.7897619009017944, 0.7959921956062317, 0.7919505834579468, 0.7943683862686157, 0.7879475355148315, 0.7876153588294983, 0.8014248013496399, 0.7912036776542664, 0.7885323762893677, 0.7934959530830383], 'val_losses': [11.386821746826172, 11.386821746826172, 11.386821746826172, 11.386821746826172, 11.386821746826172, 11.386821746826172, 13.781168937683105, 8.543456077575684, 7.428191661834717, 4.667218208312988, 1.3173710107803345, 1.9199626445770264, 2.09212327003479, 1.5533374547958374, 0.96183180809021, 1.0833061933517456, 1.2042728662490845, 1.0673539638519287, 0.9803839921951294, 0.94778972864151, 0.9913223385810852, 0.9491888880729675, 0.8614591360092163, 0.8296757340431213, 0.8228844404220581, 0.850818395614624, 0.8642024397850037, 0.8372836112976074, 0.8145695924758911, 0.7959490418434143, 0.7676746845245361, 0.7675686478614807, 0.7849622964859009, 0.8191312551498413, 0.8201283812522888, 0.7911702990531921, 0.7662343382835388, 0.749377429485321, 0.7525326609611511, 0.7672560214996338, 0.7893224358558655, 0.7911773920059204, 0.777060866355896, 0.7628917694091797, 0.751352071762085, 0.752954363822937, 0.7552182674407959, 0.7548928260803223, 0.7527166604995728, 0.7470380067825317, 0.7440879344940186, 0.7400707006454468, 0.7383768558502197, 0.7381157279014587, 0.7439822554588318, 0.7494215369224548, 0.7507582902908325, 0.7459284067153931, 0.737167239189148, 0.7336469888687134, 0.7335976362228394, 0.7337946891784668, 0.7365678548812866, 0.7375379204750061, 0.7377665042877197, 0.7350208163261414, 0.7322139739990234, 0.7316041588783264, 0.7328720092773438, 0.7340114116668701, 0.7361141443252563, 0.7376961708068848, 0.739616870880127, 0.7416812181472778, 0.7428032755851746, 0.7426450252532959, 0.7414442300796509, 0.737420916557312, 0.7345920205116272, 0.7334705591201782, 0.7345029711723328, 0.7364848256111145, 0.7368433475494385, 0.7366846203804016, 0.7382675409317017, 0.7395616173744202, 0.7410565614700317, 0.7405540943145752, 0.7399826049804688, 0.7381483912467957, 0.7369502782821655, 0.73567795753479, 0.7348632216453552, 0.7343289256095886, 0.7341680526733398, 0.7345003485679626, 0.7347023487091064, 0.7351966500282288, 0.7348730564117432, 0.7343899607658386, 0.7335224747657776, 0.732700765132904, 0.7319607138633728, 0.7310370206832886, 0.730481743812561, 0.7304302453994751, 0.7307078838348389, 0.7314335703849792, 0.732342541217804, 0.733417272567749, 0.7344603538513184, 0.7352468371391296, 0.7357268929481506, 0.7357468605041504, 0.735710620880127, 0.7356045246124268, 0.7352765202522278, 0.7348006367683411, 0.7344156503677368, 0.7339630722999573, 0.7336703538894653, 0.7335643768310547, 0.7333487272262573, 0.7332199215888977, 0.7331066727638245, 0.7332029342651367, 0.733216404914856, 0.7333939671516418, 0.7334713935852051, 0.7335510849952698, 0.7336370348930359, 0.733648955821991, 0.7336902618408203, 0.7337408065795898, 0.7337656021118164, 0.7337608337402344, 0.7338281869888306, 0.7338268160820007, 0.7338365316390991, 0.7338342070579529, 0.7338308691978455, 0.7338495254516602, 0.7338501811027527], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.71875, 0.71875, 0.1875, 0.1875, 0.71875, 0.71875, 0.625, 0.71875, 0.71875, 0.390625, 0.1875, 0.25, 0.6875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004336357109516722, 'batch_size': 256, 'epochs': 143, 'hidden_size': 164, 'dropout': 0.3428277251587937, 'weight_decay': 0.00015887871249660882, 'label_smoothing': 0.10866460470182046, 'grad_clip_norm': 1.709999679568698, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-10-13 22:41:05,707 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:41:05,707 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-10-13 22:41:05,707 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.211s
2025-10-13 22:41:05,768 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:41:05,768 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.060s
2025-10-13 22:41:05,768 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 0.004336357109516722, 'batch_size': np.int64(256), 'epochs': np.int64(143), 'hidden_size': np.int64(164), 'dropout': 0.3428277251587937, 'weight_decay': 0.00015887871249660882, 'label_smoothing': 0.10866460470182046, 'grad_clip_norm': 1.709999679568698, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:41:05,768 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 0.004336357109516722, 'batch_size': np.int64(256), 'epochs': np.int64(143), 'hidden_size': np.int64(164), 'dropout': 0.3428277251587937, 'weight_decay': 0.00015887871249660882, 'label_smoothing': 0.10866460470182046, 'grad_clip_norm': 1.709999679568698, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:41:05,768 - INFO - bo.run_bo - üîçBO Trial 6: Using RF surrogate + Expected Improvement
2025-10-13 22:41:05,768 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:05,768 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 6 (NaN monitoring active)
2025-10-13 22:41:05,768 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:05,768 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:05,768 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 7.45394345874408e-05, 'batch_size': 128, 'epochs': 122, 'hidden_size': 44, 'dropout': 0.12028753847708981, 'weight_decay': 3.4264472104742706e-05, 'label_smoothing': 0.0383355238711737, 'grad_clip_norm': 3.6275255350598052, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:41:05,769 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 7.45394345874408e-05, 'batch_size': 128, 'epochs': 122, 'hidden_size': 44, 'dropout': 0.12028753847708981, 'weight_decay': 3.4264472104742706e-05, 'label_smoothing': 0.0383355238711737, 'grad_clip_norm': 3.6275255350598052, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:41:09,437 - INFO - _models.training_function_executor - Epoch 001/122 | train_loss=11.1383 | val_loss=9.3020 | val_acc=0.1875
2025-10-13 22:41:09,444 - INFO - _models.training_function_executor - Epoch 002/122 | train_loss=10.5884 | val_loss=9.3020 | val_acc=0.1875
2025-10-13 22:41:09,448 - INFO - _models.training_function_executor - Epoch 003/122 | train_loss=10.7727 | val_loss=9.1836 | val_acc=0.1875
2025-10-13 22:41:09,453 - INFO - _models.training_function_executor - Epoch 004/122 | train_loss=10.8000 | val_loss=8.9591 | val_acc=0.1875
2025-10-13 22:41:09,457 - INFO - _models.training_function_executor - Epoch 005/122 | train_loss=10.6220 | val_loss=8.7354 | val_acc=0.1875
2025-10-13 22:41:09,462 - INFO - _models.training_function_executor - Epoch 006/122 | train_loss=10.2863 | val_loss=8.5153 | val_acc=0.1875
2025-10-13 22:41:09,466 - INFO - _models.training_function_executor - Epoch 007/122 | train_loss=9.5743 | val_loss=8.2976 | val_acc=0.1875
2025-10-13 22:41:09,470 - INFO - _models.training_function_executor - Epoch 008/122 | train_loss=9.9323 | val_loss=8.0731 | val_acc=0.1875
2025-10-13 22:41:09,474 - INFO - _models.training_function_executor - Epoch 009/122 | train_loss=9.2596 | val_loss=7.8636 | val_acc=0.1875
2025-10-13 22:41:09,479 - INFO - _models.training_function_executor - Epoch 010/122 | train_loss=8.9019 | val_loss=7.6509 | val_acc=0.1875
2025-10-13 22:41:09,483 - INFO - _models.training_function_executor - Epoch 011/122 | train_loss=9.0116 | val_loss=7.4413 | val_acc=0.1875
2025-10-13 22:41:09,488 - INFO - _models.training_function_executor - Epoch 012/122 | train_loss=8.6248 | val_loss=7.2344 | val_acc=0.1875
2025-10-13 22:41:09,492 - INFO - _models.training_function_executor - Epoch 013/122 | train_loss=8.4411 | val_loss=7.0231 | val_acc=0.1875
2025-10-13 22:41:09,496 - INFO - _models.training_function_executor - Epoch 014/122 | train_loss=8.5899 | val_loss=6.8201 | val_acc=0.1875
2025-10-13 22:41:09,500 - INFO - _models.training_function_executor - Epoch 015/122 | train_loss=8.3080 | val_loss=6.6187 | val_acc=0.1875
2025-10-13 22:41:09,504 - INFO - _models.training_function_executor - Epoch 016/122 | train_loss=8.1176 | val_loss=6.4191 | val_acc=0.1875
2025-10-13 22:41:09,508 - INFO - _models.training_function_executor - Epoch 017/122 | train_loss=7.8164 | val_loss=6.2248 | val_acc=0.1875
2025-10-13 22:41:09,512 - INFO - _models.training_function_executor - Epoch 018/122 | train_loss=7.4147 | val_loss=6.0335 | val_acc=0.1875
2025-10-13 22:41:09,516 - INFO - _models.training_function_executor - Epoch 019/122 | train_loss=7.9449 | val_loss=5.8420 | val_acc=0.1875
2025-10-13 22:41:09,520 - INFO - _models.training_function_executor - Epoch 020/122 | train_loss=7.1651 | val_loss=5.6540 | val_acc=0.1875
2025-10-13 22:41:09,524 - INFO - _models.training_function_executor - Epoch 021/122 | train_loss=6.8579 | val_loss=5.4678 | val_acc=0.1875
2025-10-13 22:41:09,528 - INFO - _models.training_function_executor - Epoch 022/122 | train_loss=6.5167 | val_loss=5.2785 | val_acc=0.1875
2025-10-13 22:41:09,532 - INFO - _models.training_function_executor - Epoch 023/122 | train_loss=6.6828 | val_loss=5.1039 | val_acc=0.1875
2025-10-13 22:41:09,536 - INFO - _models.training_function_executor - Epoch 024/122 | train_loss=6.0577 | val_loss=4.9246 | val_acc=0.1875
2025-10-13 22:41:09,540 - INFO - _models.training_function_executor - Epoch 025/122 | train_loss=6.4085 | val_loss=4.7485 | val_acc=0.1875
2025-10-13 22:41:09,544 - INFO - _models.training_function_executor - Epoch 026/122 | train_loss=5.8564 | val_loss=4.5767 | val_acc=0.1875
2025-10-13 22:41:09,548 - INFO - _models.training_function_executor - Epoch 027/122 | train_loss=6.3312 | val_loss=4.4039 | val_acc=0.1875
2025-10-13 22:41:09,552 - INFO - _models.training_function_executor - Epoch 028/122 | train_loss=5.7164 | val_loss=4.2389 | val_acc=0.1875
2025-10-13 22:41:09,555 - INFO - _models.training_function_executor - Epoch 029/122 | train_loss=5.4018 | val_loss=4.0734 | val_acc=0.1875
2025-10-13 22:41:09,559 - INFO - _models.training_function_executor - Epoch 030/122 | train_loss=5.7918 | val_loss=3.9135 | val_acc=0.1875
2025-10-13 22:41:09,563 - INFO - _models.training_function_executor - Epoch 031/122 | train_loss=5.5113 | val_loss=3.7502 | val_acc=0.1875
2025-10-13 22:41:09,568 - INFO - _models.training_function_executor - Epoch 032/122 | train_loss=5.5860 | val_loss=3.5926 | val_acc=0.1875
2025-10-13 22:41:09,572 - INFO - _models.training_function_executor - Epoch 033/122 | train_loss=5.0503 | val_loss=3.4401 | val_acc=0.1875
2025-10-13 22:41:09,576 - INFO - _models.training_function_executor - Epoch 034/122 | train_loss=4.3758 | val_loss=3.2924 | val_acc=0.1875
2025-10-13 22:41:09,580 - INFO - _models.training_function_executor - Epoch 035/122 | train_loss=4.5756 | val_loss=3.1441 | val_acc=0.1875
2025-10-13 22:41:09,584 - INFO - _models.training_function_executor - Epoch 036/122 | train_loss=4.7109 | val_loss=3.0034 | val_acc=0.1875
2025-10-13 22:41:09,588 - INFO - _models.training_function_executor - Epoch 037/122 | train_loss=4.1673 | val_loss=2.8604 | val_acc=0.1875
2025-10-13 22:41:09,593 - INFO - _models.training_function_executor - Epoch 038/122 | train_loss=4.0564 | val_loss=2.7251 | val_acc=0.1875
2025-10-13 22:41:09,597 - INFO - _models.training_function_executor - Epoch 039/122 | train_loss=4.1417 | val_loss=2.5938 | val_acc=0.1875
2025-10-13 22:41:09,601 - INFO - _models.training_function_executor - Epoch 040/122 | train_loss=3.8922 | val_loss=2.4674 | val_acc=0.1875
2025-10-13 22:41:09,605 - INFO - _models.training_function_executor - Epoch 041/122 | train_loss=3.6971 | val_loss=2.3458 | val_acc=0.1875
2025-10-13 22:41:09,609 - INFO - _models.training_function_executor - Epoch 042/122 | train_loss=3.8381 | val_loss=2.2232 | val_acc=0.1875
2025-10-13 22:41:09,613 - INFO - _models.training_function_executor - Epoch 043/122 | train_loss=3.3913 | val_loss=2.1060 | val_acc=0.1875
2025-10-13 22:41:09,617 - INFO - _models.training_function_executor - Epoch 044/122 | train_loss=3.7161 | val_loss=1.9964 | val_acc=0.1875
2025-10-13 22:41:09,621 - INFO - _models.training_function_executor - Epoch 045/122 | train_loss=3.8182 | val_loss=1.8864 | val_acc=0.2031
2025-10-13 22:41:09,625 - INFO - _models.training_function_executor - Epoch 046/122 | train_loss=3.6755 | val_loss=1.7848 | val_acc=0.2188
2025-10-13 22:41:09,629 - INFO - _models.training_function_executor - Epoch 047/122 | train_loss=3.5171 | val_loss=1.6838 | val_acc=0.2344
2025-10-13 22:41:09,633 - INFO - _models.training_function_executor - Epoch 048/122 | train_loss=3.4081 | val_loss=1.5875 | val_acc=0.2969
2025-10-13 22:41:09,638 - INFO - _models.training_function_executor - Epoch 049/122 | train_loss=3.0129 | val_loss=1.5002 | val_acc=0.3594
2025-10-13 22:41:09,642 - INFO - _models.training_function_executor - Epoch 050/122 | train_loss=3.0081 | val_loss=1.4131 | val_acc=0.3594
2025-10-13 22:41:09,646 - INFO - _models.training_function_executor - Epoch 051/122 | train_loss=2.9087 | val_loss=1.3398 | val_acc=0.3750
2025-10-13 22:41:09,650 - INFO - _models.training_function_executor - Epoch 052/122 | train_loss=2.9817 | val_loss=1.2616 | val_acc=0.4062
2025-10-13 22:41:09,654 - INFO - _models.training_function_executor - Epoch 053/122 | train_loss=2.9365 | val_loss=1.1905 | val_acc=0.5000
2025-10-13 22:41:09,658 - INFO - _models.training_function_executor - Epoch 054/122 | train_loss=2.9173 | val_loss=1.1263 | val_acc=0.5469
2025-10-13 22:41:09,662 - INFO - _models.training_function_executor - Epoch 055/122 | train_loss=2.8493 | val_loss=1.0677 | val_acc=0.5625
2025-10-13 22:41:09,666 - INFO - _models.training_function_executor - Epoch 056/122 | train_loss=2.9189 | val_loss=1.0128 | val_acc=0.5781
2025-10-13 22:41:09,670 - INFO - _models.training_function_executor - Epoch 057/122 | train_loss=2.6717 | val_loss=0.9641 | val_acc=0.5938
2025-10-13 22:41:09,674 - INFO - _models.training_function_executor - Epoch 058/122 | train_loss=2.3711 | val_loss=0.9221 | val_acc=0.5938
2025-10-13 22:41:09,678 - INFO - _models.training_function_executor - Epoch 059/122 | train_loss=2.2800 | val_loss=0.8852 | val_acc=0.6250
2025-10-13 22:41:09,682 - INFO - _models.training_function_executor - Epoch 060/122 | train_loss=2.8854 | val_loss=0.8522 | val_acc=0.6719
2025-10-13 22:41:09,686 - INFO - _models.training_function_executor - Epoch 061/122 | train_loss=2.4404 | val_loss=0.8241 | val_acc=0.6562
2025-10-13 22:41:09,690 - INFO - _models.training_function_executor - Epoch 062/122 | train_loss=2.3748 | val_loss=0.8013 | val_acc=0.6562
2025-10-13 22:41:09,694 - INFO - _models.training_function_executor - Epoch 063/122 | train_loss=2.2813 | val_loss=0.7822 | val_acc=0.6406
2025-10-13 22:41:09,699 - INFO - _models.training_function_executor - Epoch 064/122 | train_loss=2.3729 | val_loss=0.7657 | val_acc=0.6875
2025-10-13 22:41:09,703 - INFO - _models.training_function_executor - Epoch 065/122 | train_loss=2.7003 | val_loss=0.7532 | val_acc=0.6562
2025-10-13 22:41:09,707 - INFO - _models.training_function_executor - Epoch 066/122 | train_loss=2.2982 | val_loss=0.7444 | val_acc=0.6875
2025-10-13 22:41:09,711 - INFO - _models.training_function_executor - Epoch 067/122 | train_loss=2.0683 | val_loss=0.7398 | val_acc=0.7031
2025-10-13 22:41:09,715 - INFO - _models.training_function_executor - Epoch 068/122 | train_loss=2.3605 | val_loss=0.7350 | val_acc=0.7188
2025-10-13 22:41:09,719 - INFO - _models.training_function_executor - Epoch 069/122 | train_loss=2.2002 | val_loss=0.7348 | val_acc=0.7188
2025-10-13 22:41:09,723 - INFO - _models.training_function_executor - Epoch 070/122 | train_loss=2.3470 | val_loss=0.7364 | val_acc=0.7188
2025-10-13 22:41:09,727 - INFO - _models.training_function_executor - Epoch 071/122 | train_loss=2.0758 | val_loss=0.7386 | val_acc=0.7188
2025-10-13 22:41:09,730 - INFO - _models.training_function_executor - Epoch 072/122 | train_loss=2.3441 | val_loss=0.7437 | val_acc=0.7188
2025-10-13 22:41:09,734 - INFO - _models.training_function_executor - Epoch 073/122 | train_loss=1.9359 | val_loss=0.7483 | val_acc=0.7188
2025-10-13 22:41:09,738 - INFO - _models.training_function_executor - Epoch 074/122 | train_loss=2.1282 | val_loss=0.7554 | val_acc=0.7188
2025-10-13 22:41:09,742 - INFO - _models.training_function_executor - Epoch 075/122 | train_loss=2.1941 | val_loss=0.7613 | val_acc=0.7188
2025-10-13 22:41:09,746 - INFO - _models.training_function_executor - Epoch 076/122 | train_loss=2.2729 | val_loss=0.7678 | val_acc=0.7188
2025-10-13 22:41:09,751 - INFO - _models.training_function_executor - Epoch 077/122 | train_loss=2.2381 | val_loss=0.7759 | val_acc=0.7188
2025-10-13 22:41:09,754 - INFO - _models.training_function_executor - Epoch 078/122 | train_loss=2.1828 | val_loss=0.7823 | val_acc=0.7188
2025-10-13 22:41:09,758 - INFO - _models.training_function_executor - Epoch 079/122 | train_loss=2.3552 | val_loss=0.7899 | val_acc=0.7188
2025-10-13 22:41:09,762 - INFO - _models.training_function_executor - Epoch 080/122 | train_loss=2.3230 | val_loss=0.7978 | val_acc=0.7188
2025-10-13 22:41:09,766 - INFO - _models.training_function_executor - Epoch 081/122 | train_loss=1.8265 | val_loss=0.8062 | val_acc=0.7188
2025-10-13 22:41:09,770 - INFO - _models.training_function_executor - Epoch 082/122 | train_loss=2.0621 | val_loss=0.8143 | val_acc=0.7188
2025-10-13 22:41:09,774 - INFO - _models.training_function_executor - Epoch 083/122 | train_loss=2.0379 | val_loss=0.8214 | val_acc=0.7188
2025-10-13 22:41:09,778 - INFO - _models.training_function_executor - Epoch 084/122 | train_loss=2.4124 | val_loss=0.8273 | val_acc=0.7188
2025-10-13 22:41:09,782 - INFO - _models.training_function_executor - Epoch 085/122 | train_loss=2.2800 | val_loss=0.8312 | val_acc=0.7188
2025-10-13 22:41:09,786 - INFO - _models.training_function_executor - Epoch 086/122 | train_loss=2.1220 | val_loss=0.8367 | val_acc=0.7188
2025-10-13 22:41:09,791 - INFO - _models.training_function_executor - Epoch 087/122 | train_loss=1.8235 | val_loss=0.8407 | val_acc=0.7188
2025-10-13 22:41:09,795 - INFO - _models.training_function_executor - Epoch 088/122 | train_loss=2.2913 | val_loss=0.8441 | val_acc=0.7188
2025-10-13 22:41:09,799 - INFO - _models.training_function_executor - Epoch 089/122 | train_loss=2.2588 | val_loss=0.8468 | val_acc=0.7188
2025-10-13 22:41:09,803 - INFO - _models.training_function_executor - Epoch 090/122 | train_loss=2.1899 | val_loss=0.8499 | val_acc=0.7188
2025-10-13 22:41:09,807 - INFO - _models.training_function_executor - Epoch 091/122 | train_loss=2.2459 | val_loss=0.8537 | val_acc=0.7188
2025-10-13 22:41:09,811 - INFO - _models.training_function_executor - Epoch 092/122 | train_loss=1.8195 | val_loss=0.8559 | val_acc=0.7188
2025-10-13 22:41:09,815 - INFO - _models.training_function_executor - Epoch 093/122 | train_loss=2.3497 | val_loss=0.8585 | val_acc=0.7188
2025-10-13 22:41:09,819 - INFO - _models.training_function_executor - Epoch 094/122 | train_loss=2.1268 | val_loss=0.8594 | val_acc=0.7188
2025-10-13 22:41:09,824 - INFO - _models.training_function_executor - Epoch 095/122 | train_loss=2.0028 | val_loss=0.8613 | val_acc=0.7188
2025-10-13 22:41:09,827 - INFO - _models.training_function_executor - Epoch 096/122 | train_loss=2.0186 | val_loss=0.8621 | val_acc=0.7188
2025-10-13 22:41:09,831 - INFO - _models.training_function_executor - Epoch 097/122 | train_loss=2.2622 | val_loss=0.8621 | val_acc=0.7188
2025-10-13 22:41:09,835 - INFO - _models.training_function_executor - Epoch 098/122 | train_loss=2.4274 | val_loss=0.8627 | val_acc=0.7188
2025-10-13 22:41:09,840 - INFO - _models.training_function_executor - Epoch 099/122 | train_loss=2.2321 | val_loss=0.8630 | val_acc=0.7188
2025-10-13 22:41:09,844 - INFO - _models.training_function_executor - Epoch 100/122 | train_loss=2.2078 | val_loss=0.8630 | val_acc=0.7188
2025-10-13 22:41:09,848 - INFO - _models.training_function_executor - Epoch 101/122 | train_loss=2.2533 | val_loss=0.8636 | val_acc=0.7188
2025-10-13 22:41:09,852 - INFO - _models.training_function_executor - Epoch 102/122 | train_loss=2.6360 | val_loss=0.8647 | val_acc=0.7188
2025-10-13 22:41:09,856 - INFO - _models.training_function_executor - Epoch 103/122 | train_loss=2.0303 | val_loss=0.8640 | val_acc=0.7188
2025-10-13 22:41:09,860 - INFO - _models.training_function_executor - Epoch 104/122 | train_loss=2.0584 | val_loss=0.8645 | val_acc=0.7188
2025-10-13 22:41:09,864 - INFO - _models.training_function_executor - Epoch 105/122 | train_loss=2.0119 | val_loss=0.8656 | val_acc=0.7188
2025-10-13 22:41:09,868 - INFO - _models.training_function_executor - Epoch 106/122 | train_loss=2.3753 | val_loss=0.8656 | val_acc=0.7188
2025-10-13 22:41:09,873 - INFO - _models.training_function_executor - Epoch 107/122 | train_loss=2.1022 | val_loss=0.8665 | val_acc=0.7188
2025-10-13 22:41:09,877 - INFO - _models.training_function_executor - Epoch 108/122 | train_loss=1.9327 | val_loss=0.8671 | val_acc=0.7188
2025-10-13 22:41:09,881 - INFO - _models.training_function_executor - Epoch 109/122 | train_loss=2.1912 | val_loss=0.8684 | val_acc=0.7188
2025-10-13 22:41:09,885 - INFO - _models.training_function_executor - Epoch 110/122 | train_loss=1.9919 | val_loss=0.8697 | val_acc=0.7188
2025-10-13 22:41:09,889 - INFO - _models.training_function_executor - Epoch 111/122 | train_loss=2.0341 | val_loss=0.8698 | val_acc=0.7188
2025-10-13 22:41:09,893 - INFO - _models.training_function_executor - Epoch 112/122 | train_loss=2.1534 | val_loss=0.8703 | val_acc=0.7188
2025-10-13 22:41:09,897 - INFO - _models.training_function_executor - Epoch 113/122 | train_loss=2.0570 | val_loss=0.8708 | val_acc=0.7188
2025-10-13 22:41:09,902 - INFO - _models.training_function_executor - Epoch 114/122 | train_loss=2.1662 | val_loss=0.8709 | val_acc=0.7188
2025-10-13 22:41:09,906 - INFO - _models.training_function_executor - Epoch 115/122 | train_loss=1.6937 | val_loss=0.8707 | val_acc=0.7188
2025-10-13 22:41:09,910 - INFO - _models.training_function_executor - Epoch 116/122 | train_loss=2.0330 | val_loss=0.8710 | val_acc=0.7188
2025-10-13 22:41:09,914 - INFO - _models.training_function_executor - Epoch 117/122 | train_loss=2.1377 | val_loss=0.8708 | val_acc=0.7188
2025-10-13 22:41:09,918 - INFO - _models.training_function_executor - Epoch 118/122 | train_loss=2.1940 | val_loss=0.8707 | val_acc=0.7188
2025-10-13 22:41:09,922 - INFO - _models.training_function_executor - Epoch 119/122 | train_loss=1.8654 | val_loss=0.8710 | val_acc=0.7188
2025-10-13 22:41:09,926 - INFO - _models.training_function_executor - Epoch 120/122 | train_loss=1.9888 | val_loss=0.8713 | val_acc=0.7188
2025-10-13 22:41:09,930 - INFO - _models.training_function_executor - Epoch 121/122 | train_loss=2.2185 | val_loss=0.8713 | val_acc=0.7188
2025-10-13 22:41:09,935 - INFO - _models.training_function_executor - Epoch 122/122 | train_loss=1.9302 | val_loss=0.8713 | val_acc=0.7188
2025-10-13 22:41:10,771 - INFO - _models.training_function_executor - Model: 2,335 parameters, 10.0KB storage
2025-10-13 22:41:10,771 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [11.138299465179443, 10.58839225769043, 10.772745609283447, 10.800002098083496, 10.622045040130615, 10.286260604858398, 9.57429027557373, 9.93228006362915, 9.259560585021973, 8.901883125305176, 9.011645793914795, 8.624829292297363, 8.44112777709961, 8.589949607849121, 8.307983875274658, 8.117609977722168, 7.816421270370483, 7.4147255420684814, 7.944861888885498, 7.1651291847229, 6.857948541641235, 6.516702175140381, 6.682847261428833, 6.05765700340271, 6.408530235290527, 5.856371164321899, 6.331223964691162, 5.716361999511719, 5.401782989501953, 5.79176926612854, 5.511284351348877, 5.585973501205444, 5.050283432006836, 4.3757712841033936, 4.5755860805511475, 4.710931062698364, 4.167332410812378, 4.05639123916626, 4.141712427139282, 3.8922497034072876, 3.6971250772476196, 3.838071584701538, 3.391273021697998, 3.7160929441452026, 3.818196177482605, 3.675487518310547, 3.517066717147827, 3.4081308841705322, 3.012911558151245, 3.0081006288528442, 2.90865957736969, 2.9816960096359253, 2.936519742012024, 2.917316436767578, 2.849333167076111, 2.9188637733459473, 2.6716729402542114, 2.3710559606552124, 2.27995502948761, 2.8853886127471924, 2.440351724624634, 2.3748369216918945, 2.281283974647522, 2.372926354408264, 2.7003203630447388, 2.298200249671936, 2.0682798624038696, 2.3605350255966187, 2.2002142667770386, 2.3470033407211304, 2.075793743133545, 2.34408438205719, 1.935931921005249, 2.128246307373047, 2.1940905451774597, 2.27289617061615, 2.238092541694641, 2.182757019996643, 2.3551632165908813, 2.3229845762252808, 1.8265146017074585, 2.0621477365493774, 2.037876307964325, 2.4124248027801514, 2.2800207138061523, 2.122004985809326, 1.8234590888023376, 2.2912940979003906, 2.2587555646896362, 2.189871311187744, 2.2458603382110596, 1.8194946646690369, 2.349739670753479, 2.126848518848419, 2.0027530193328857, 2.018620252609253, 2.262231707572937, 2.4273929595947266, 2.2321165800094604, 2.207789659500122, 2.253342032432556, 2.6359641551971436, 2.0302600264549255, 2.058428704738617, 2.011869966983795, 2.3752875328063965, 2.1021732687950134, 1.9327411651611328, 2.1912277936935425, 1.9918619394302368, 2.034107208251953, 2.153416156768799, 2.057024598121643, 2.1661532521247864, 1.6936870217323303, 2.033020317554474, 2.1377151012420654, 2.193965196609497, 1.8654391169548035, 1.988799273967743, 2.218485116958618, 1.930208146572113], 'val_losses': [9.301994323730469, 9.301994323730469, 9.183557510375977, 8.959084510803223, 8.735363006591797, 8.515268325805664, 8.297597885131836, 8.073101043701172, 7.863598346710205, 7.650914192199707, 7.441277503967285, 7.234360694885254, 7.023080825805664, 6.820089340209961, 6.618672847747803, 6.419100284576416, 6.224771499633789, 6.033526420593262, 5.842041492462158, 5.653979301452637, 5.467803955078125, 5.278491020202637, 5.103949069976807, 4.924649715423584, 4.748549461364746, 4.576666831970215, 4.403886795043945, 4.238901615142822, 4.0734477043151855, 3.913459062576294, 3.750194549560547, 3.5925631523132324, 3.440077304840088, 3.292384386062622, 3.1441268920898438, 3.003410816192627, 2.8603506088256836, 2.7250773906707764, 2.593834161758423, 2.467373847961426, 2.345797300338745, 2.2231621742248535, 2.1059839725494385, 1.9964148998260498, 1.8864083290100098, 1.7848173379898071, 1.6837621927261353, 1.5875232219696045, 1.500207781791687, 1.4130762815475464, 1.3398144245147705, 1.2615875005722046, 1.190529227256775, 1.1262829303741455, 1.0676521062850952, 1.012800693511963, 0.9641315340995789, 0.9220957159996033, 0.8852204084396362, 0.8522310853004456, 0.8240693807601929, 0.8012574315071106, 0.7822328805923462, 0.7657079100608826, 0.7532057762145996, 0.7444082498550415, 0.7398499846458435, 0.7350313067436218, 0.7348325252532959, 0.7364340424537659, 0.7386258840560913, 0.7437236905097961, 0.7482827305793762, 0.7553775310516357, 0.761330246925354, 0.7678284645080566, 0.7759450674057007, 0.7822957634925842, 0.789917528629303, 0.7977911233901978, 0.8062138557434082, 0.8143457770347595, 0.8213774561882019, 0.8272891640663147, 0.831231415271759, 0.8366854190826416, 0.8406574130058289, 0.8440647125244141, 0.8468291163444519, 0.849886953830719, 0.8537003397941589, 0.8558878898620605, 0.8585321307182312, 0.859389066696167, 0.8612841963768005, 0.8621265888214111, 0.8620561361312866, 0.8627148270606995, 0.862998366355896, 0.8630069494247437, 0.8636236786842346, 0.8646824359893799, 0.8639505505561829, 0.8645187616348267, 0.8656018972396851, 0.8655657172203064, 0.8664880990982056, 0.8670795559883118, 0.8684099912643433, 0.8696566224098206, 0.8698383569717407, 0.8703102469444275, 0.8707769513130188, 0.870919406414032, 0.8706741333007812, 0.8710145950317383, 0.8708396553993225, 0.8706967234611511, 0.8709837198257446, 0.8712629079818726, 0.8712714314460754, 0.8712714314460754], 'val_acc': [0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.203125, 0.21875, 0.234375, 0.296875, 0.359375, 0.359375, 0.375, 0.40625, 0.5, 0.546875, 0.5625, 0.578125, 0.59375, 0.59375, 0.625, 0.671875, 0.65625, 0.65625, 0.640625, 0.6875, 0.65625, 0.6875, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 7.45394345874408e-05, 'batch_size': 128, 'epochs': 122, 'hidden_size': 44, 'dropout': 0.12028753847708981, 'weight_decay': 3.4264472104742706e-05, 'label_smoothing': 0.0383355238711737, 'grad_clip_norm': 3.6275255350598052, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 2335, 'model_storage_size_kb': 10.033203125, 'model_size_validation': 'PASS'}
2025-10-13 22:41:10,771 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:41:10,771 - INFO - _models.training_function_executor - Model: 2,335 parameters, 10.0KB (PASS 256KB limit)
2025-10-13 22:41:10,771 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.003s
2025-10-13 22:41:10,831 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:41:10,831 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.060s
2025-10-13 22:41:10,831 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 7.45394345874408e-05, 'batch_size': np.int64(128), 'epochs': np.int64(122), 'hidden_size': np.int64(44), 'dropout': 0.12028753847708981, 'weight_decay': 3.4264472104742706e-05, 'label_smoothing': 0.0383355238711737, 'grad_clip_norm': 3.6275255350598052, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:41:10,831 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 7.45394345874408e-05, 'batch_size': np.int64(128), 'epochs': np.int64(122), 'hidden_size': np.int64(44), 'dropout': 0.12028753847708981, 'weight_decay': 3.4264472104742706e-05, 'label_smoothing': 0.0383355238711737, 'grad_clip_norm': 3.6275255350598052, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:41:10,831 - INFO - bo.run_bo - üîçBO Trial 7: Using RF surrogate + Expected Improvement
2025-10-13 22:41:10,831 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:10,831 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 7 (NaN monitoring active)
2025-10-13 22:41:10,831 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:10,832 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:10,832 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015294663694452992, 'batch_size': 96, 'epochs': 169, 'hidden_size': 34, 'dropout': 0.17816498822834012, 'weight_decay': 0.0020173827714239185, 'label_smoothing': 0.10245068677860654, 'grad_clip_norm': 2.0768324953734245, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:41:10,832 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015294663694452992, 'batch_size': 96, 'epochs': 169, 'hidden_size': 34, 'dropout': 0.17816498822834012, 'weight_decay': 0.0020173827714239185, 'label_smoothing': 0.10245068677860654, 'grad_clip_norm': 2.0768324953734245, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:41:14,386 - INFO - _models.training_function_executor - Epoch 001/169 | train_loss=13.6273 | val_loss=12.4729 | val_acc=0.0938
2025-10-13 22:41:14,394 - INFO - _models.training_function_executor - Epoch 002/169 | train_loss=13.0958 | val_loss=9.5190 | val_acc=0.0938
2025-10-13 22:41:14,400 - INFO - _models.training_function_executor - Epoch 003/169 | train_loss=10.2717 | val_loss=3.6965 | val_acc=0.0938
2025-10-13 22:41:14,405 - INFO - _models.training_function_executor - Epoch 004/169 | train_loss=6.0340 | val_loss=2.1436 | val_acc=0.7188
2025-10-13 22:41:14,411 - INFO - _models.training_function_executor - Epoch 005/169 | train_loss=4.1389 | val_loss=2.4241 | val_acc=0.7188
2025-10-13 22:41:14,416 - INFO - _models.training_function_executor - Epoch 006/169 | train_loss=3.2457 | val_loss=1.9071 | val_acc=0.7188
2025-10-13 22:41:14,421 - INFO - _models.training_function_executor - Epoch 007/169 | train_loss=3.5607 | val_loss=1.5823 | val_acc=0.7188
2025-10-13 22:41:14,426 - INFO - _models.training_function_executor - Epoch 008/169 | train_loss=3.1118 | val_loss=1.2792 | val_acc=0.7188
2025-10-13 22:41:14,432 - INFO - _models.training_function_executor - Epoch 009/169 | train_loss=2.7974 | val_loss=1.2211 | val_acc=0.7188
2025-10-13 22:41:14,437 - INFO - _models.training_function_executor - Epoch 010/169 | train_loss=2.4523 | val_loss=1.0553 | val_acc=0.7188
2025-10-13 22:41:14,442 - INFO - _models.training_function_executor - Epoch 011/169 | train_loss=2.1417 | val_loss=0.8802 | val_acc=0.7188
2025-10-13 22:41:14,448 - INFO - _models.training_function_executor - Epoch 012/169 | train_loss=2.0129 | val_loss=0.8666 | val_acc=0.7188
2025-10-13 22:41:14,453 - INFO - _models.training_function_executor - Epoch 013/169 | train_loss=1.5692 | val_loss=0.8970 | val_acc=0.6562
2025-10-13 22:41:14,458 - INFO - _models.training_function_executor - Epoch 014/169 | train_loss=1.5602 | val_loss=0.8668 | val_acc=0.6562
2025-10-13 22:41:14,464 - INFO - _models.training_function_executor - Epoch 015/169 | train_loss=1.4519 | val_loss=0.8301 | val_acc=0.7188
2025-10-13 22:41:14,469 - INFO - _models.training_function_executor - Epoch 016/169 | train_loss=1.2659 | val_loss=0.8104 | val_acc=0.7188
2025-10-13 22:41:14,474 - INFO - _models.training_function_executor - Epoch 017/169 | train_loss=1.2621 | val_loss=0.8144 | val_acc=0.7188
2025-10-13 22:41:14,479 - INFO - _models.training_function_executor - Epoch 018/169 | train_loss=1.0911 | val_loss=0.8240 | val_acc=0.7188
2025-10-13 22:41:14,484 - INFO - _models.training_function_executor - Epoch 019/169 | train_loss=1.0142 | val_loss=0.8337 | val_acc=0.7188
2025-10-13 22:41:14,490 - INFO - _models.training_function_executor - Epoch 020/169 | train_loss=1.0133 | val_loss=0.8372 | val_acc=0.7188
2025-10-13 22:41:14,495 - INFO - _models.training_function_executor - Epoch 021/169 | train_loss=1.0018 | val_loss=0.8509 | val_acc=0.7188
2025-10-13 22:41:14,500 - INFO - _models.training_function_executor - Epoch 022/169 | train_loss=0.9447 | val_loss=0.8506 | val_acc=0.7188
2025-10-13 22:41:14,505 - INFO - _models.training_function_executor - Epoch 023/169 | train_loss=0.9134 | val_loss=0.8447 | val_acc=0.7188
2025-10-13 22:41:14,510 - INFO - _models.training_function_executor - Epoch 024/169 | train_loss=0.9280 | val_loss=0.8517 | val_acc=0.7188
2025-10-13 22:41:14,515 - INFO - _models.training_function_executor - Epoch 025/169 | train_loss=0.9167 | val_loss=0.8459 | val_acc=0.7188
2025-10-13 22:41:14,520 - INFO - _models.training_function_executor - Epoch 026/169 | train_loss=0.9017 | val_loss=0.8475 | val_acc=0.7188
2025-10-13 22:41:14,525 - INFO - _models.training_function_executor - Epoch 027/169 | train_loss=0.8746 | val_loss=0.8439 | val_acc=0.7188
2025-10-13 22:41:14,530 - INFO - _models.training_function_executor - Epoch 028/169 | train_loss=0.8702 | val_loss=0.8314 | val_acc=0.7188
2025-10-13 22:41:14,536 - INFO - _models.training_function_executor - Epoch 029/169 | train_loss=0.8693 | val_loss=0.8099 | val_acc=0.7188
2025-10-13 22:41:14,540 - INFO - _models.training_function_executor - Epoch 030/169 | train_loss=0.8623 | val_loss=0.7876 | val_acc=0.7188
2025-10-13 22:41:14,546 - INFO - _models.training_function_executor - Epoch 031/169 | train_loss=0.8731 | val_loss=0.7815 | val_acc=0.7188
2025-10-13 22:41:14,551 - INFO - _models.training_function_executor - Epoch 032/169 | train_loss=0.8643 | val_loss=0.7918 | val_acc=0.7188
2025-10-13 22:41:14,556 - INFO - _models.training_function_executor - Epoch 033/169 | train_loss=0.8814 | val_loss=0.7962 | val_acc=0.7188
2025-10-13 22:41:14,562 - INFO - _models.training_function_executor - Epoch 034/169 | train_loss=0.8642 | val_loss=0.8029 | val_acc=0.7188
2025-10-13 22:41:14,567 - INFO - _models.training_function_executor - Epoch 035/169 | train_loss=0.8170 | val_loss=0.7987 | val_acc=0.7188
2025-10-13 22:41:14,572 - INFO - _models.training_function_executor - Epoch 036/169 | train_loss=0.8218 | val_loss=0.7822 | val_acc=0.7188
2025-10-13 22:41:14,577 - INFO - _models.training_function_executor - Epoch 037/169 | train_loss=0.8313 | val_loss=0.7747 | val_acc=0.7188
2025-10-13 22:41:14,582 - INFO - _models.training_function_executor - Epoch 038/169 | train_loss=0.8309 | val_loss=0.7712 | val_acc=0.7188
2025-10-13 22:41:14,588 - INFO - _models.training_function_executor - Epoch 039/169 | train_loss=0.8271 | val_loss=0.7701 | val_acc=0.7188
2025-10-13 22:41:14,593 - INFO - _models.training_function_executor - Epoch 040/169 | train_loss=0.8109 | val_loss=0.7692 | val_acc=0.7188
2025-10-13 22:41:14,598 - INFO - _models.training_function_executor - Epoch 041/169 | train_loss=0.8168 | val_loss=0.7662 | val_acc=0.7188
2025-10-13 22:41:14,603 - INFO - _models.training_function_executor - Epoch 042/169 | train_loss=0.8084 | val_loss=0.7663 | val_acc=0.7188
2025-10-13 22:41:14,608 - INFO - _models.training_function_executor - Epoch 043/169 | train_loss=0.8233 | val_loss=0.7600 | val_acc=0.7188
2025-10-13 22:41:14,613 - INFO - _models.training_function_executor - Epoch 044/169 | train_loss=0.7954 | val_loss=0.7492 | val_acc=0.7188
2025-10-13 22:41:14,618 - INFO - _models.training_function_executor - Epoch 045/169 | train_loss=0.8344 | val_loss=0.7447 | val_acc=0.7188
2025-10-13 22:41:14,623 - INFO - _models.training_function_executor - Epoch 046/169 | train_loss=0.8318 | val_loss=0.7540 | val_acc=0.7188
2025-10-13 22:41:14,628 - INFO - _models.training_function_executor - Epoch 047/169 | train_loss=0.8188 | val_loss=0.7558 | val_acc=0.7188
2025-10-13 22:41:14,633 - INFO - _models.training_function_executor - Epoch 048/169 | train_loss=0.8434 | val_loss=0.7621 | val_acc=0.7188
2025-10-13 22:41:14,638 - INFO - _models.training_function_executor - Epoch 049/169 | train_loss=0.7962 | val_loss=0.7595 | val_acc=0.7188
2025-10-13 22:41:14,643 - INFO - _models.training_function_executor - Epoch 050/169 | train_loss=0.8103 | val_loss=0.7541 | val_acc=0.7188
2025-10-13 22:41:14,648 - INFO - _models.training_function_executor - Epoch 051/169 | train_loss=0.8042 | val_loss=0.7595 | val_acc=0.7188
2025-10-13 22:41:14,653 - INFO - _models.training_function_executor - Epoch 052/169 | train_loss=0.7850 | val_loss=0.7627 | val_acc=0.7188
2025-10-13 22:41:14,659 - INFO - _models.training_function_executor - Epoch 053/169 | train_loss=0.8011 | val_loss=0.7639 | val_acc=0.7188
2025-10-13 22:41:14,664 - INFO - _models.training_function_executor - Epoch 054/169 | train_loss=0.8000 | val_loss=0.7677 | val_acc=0.7188
2025-10-13 22:41:14,669 - INFO - _models.training_function_executor - Epoch 055/169 | train_loss=0.7804 | val_loss=0.7634 | val_acc=0.7188
2025-10-13 22:41:14,674 - INFO - _models.training_function_executor - Epoch 056/169 | train_loss=0.8121 | val_loss=0.7573 | val_acc=0.7188
2025-10-13 22:41:14,679 - INFO - _models.training_function_executor - Epoch 057/169 | train_loss=0.8250 | val_loss=0.7604 | val_acc=0.7188
2025-10-13 22:41:14,684 - INFO - _models.training_function_executor - Epoch 058/169 | train_loss=0.8177 | val_loss=0.7614 | val_acc=0.7188
2025-10-13 22:41:14,689 - INFO - _models.training_function_executor - Epoch 059/169 | train_loss=0.8339 | val_loss=0.7605 | val_acc=0.7188
2025-10-13 22:41:14,694 - INFO - _models.training_function_executor - Epoch 060/169 | train_loss=0.8112 | val_loss=0.7558 | val_acc=0.7188
2025-10-13 22:41:14,699 - INFO - _models.training_function_executor - Epoch 061/169 | train_loss=0.7920 | val_loss=0.7490 | val_acc=0.7188
2025-10-13 22:41:14,704 - INFO - _models.training_function_executor - Epoch 062/169 | train_loss=0.7964 | val_loss=0.7443 | val_acc=0.7188
2025-10-13 22:41:14,709 - INFO - _models.training_function_executor - Epoch 063/169 | train_loss=0.8042 | val_loss=0.7393 | val_acc=0.7188
2025-10-13 22:41:14,714 - INFO - _models.training_function_executor - Epoch 064/169 | train_loss=0.8008 | val_loss=0.7392 | val_acc=0.7188
2025-10-13 22:41:14,720 - INFO - _models.training_function_executor - Epoch 065/169 | train_loss=0.7899 | val_loss=0.7409 | val_acc=0.7188
2025-10-13 22:41:14,725 - INFO - _models.training_function_executor - Epoch 066/169 | train_loss=0.7907 | val_loss=0.7453 | val_acc=0.7188
2025-10-13 22:41:14,730 - INFO - _models.training_function_executor - Epoch 067/169 | train_loss=0.7888 | val_loss=0.7435 | val_acc=0.7188
2025-10-13 22:41:14,735 - INFO - _models.training_function_executor - Epoch 068/169 | train_loss=0.7995 | val_loss=0.7392 | val_acc=0.7188
2025-10-13 22:41:14,739 - INFO - _models.training_function_executor - Epoch 069/169 | train_loss=0.8129 | val_loss=0.7391 | val_acc=0.7188
2025-10-13 22:41:14,744 - INFO - _models.training_function_executor - Epoch 070/169 | train_loss=0.7916 | val_loss=0.7416 | val_acc=0.7188
2025-10-13 22:41:14,749 - INFO - _models.training_function_executor - Epoch 071/169 | train_loss=0.8047 | val_loss=0.7448 | val_acc=0.7188
2025-10-13 22:41:14,754 - INFO - _models.training_function_executor - Epoch 072/169 | train_loss=0.8077 | val_loss=0.7482 | val_acc=0.7188
2025-10-13 22:41:14,759 - INFO - _models.training_function_executor - Epoch 073/169 | train_loss=0.8064 | val_loss=0.7473 | val_acc=0.7188
2025-10-13 22:41:14,764 - INFO - _models.training_function_executor - Epoch 074/169 | train_loss=0.8122 | val_loss=0.7473 | val_acc=0.7188
2025-10-13 22:41:14,769 - INFO - _models.training_function_executor - Epoch 075/169 | train_loss=0.8072 | val_loss=0.7460 | val_acc=0.7188
2025-10-13 22:41:14,773 - INFO - _models.training_function_executor - Epoch 076/169 | train_loss=0.8022 | val_loss=0.7431 | val_acc=0.7188
2025-10-13 22:41:14,779 - INFO - _models.training_function_executor - Epoch 077/169 | train_loss=0.8005 | val_loss=0.7405 | val_acc=0.7188
2025-10-13 22:41:14,784 - INFO - _models.training_function_executor - Epoch 078/169 | train_loss=0.7989 | val_loss=0.7383 | val_acc=0.7188
2025-10-13 22:41:14,789 - INFO - _models.training_function_executor - Epoch 079/169 | train_loss=0.8019 | val_loss=0.7373 | val_acc=0.7188
2025-10-13 22:41:14,794 - INFO - _models.training_function_executor - Epoch 080/169 | train_loss=0.7975 | val_loss=0.7384 | val_acc=0.7188
2025-10-13 22:41:14,798 - INFO - _models.training_function_executor - Epoch 081/169 | train_loss=0.7829 | val_loss=0.7414 | val_acc=0.7188
2025-10-13 22:41:14,803 - INFO - _models.training_function_executor - Epoch 082/169 | train_loss=0.7770 | val_loss=0.7424 | val_acc=0.7188
2025-10-13 22:41:14,809 - INFO - _models.training_function_executor - Epoch 083/169 | train_loss=0.7764 | val_loss=0.7429 | val_acc=0.7188
2025-10-13 22:41:14,813 - INFO - _models.training_function_executor - Epoch 084/169 | train_loss=0.8058 | val_loss=0.7419 | val_acc=0.7188
2025-10-13 22:41:14,818 - INFO - _models.training_function_executor - Epoch 085/169 | train_loss=0.7939 | val_loss=0.7391 | val_acc=0.7188
2025-10-13 22:41:14,823 - INFO - _models.training_function_executor - Epoch 086/169 | train_loss=0.7899 | val_loss=0.7367 | val_acc=0.7188
2025-10-13 22:41:14,828 - INFO - _models.training_function_executor - Epoch 087/169 | train_loss=0.7731 | val_loss=0.7329 | val_acc=0.7188
2025-10-13 22:41:14,833 - INFO - _models.training_function_executor - Epoch 088/169 | train_loss=0.7982 | val_loss=0.7304 | val_acc=0.7188
2025-10-13 22:41:14,838 - INFO - _models.training_function_executor - Epoch 089/169 | train_loss=0.7773 | val_loss=0.7309 | val_acc=0.7188
2025-10-13 22:41:14,843 - INFO - _models.training_function_executor - Epoch 090/169 | train_loss=0.7846 | val_loss=0.7330 | val_acc=0.7188
2025-10-13 22:41:14,848 - INFO - _models.training_function_executor - Epoch 091/169 | train_loss=0.8026 | val_loss=0.7357 | val_acc=0.7188
2025-10-13 22:41:14,853 - INFO - _models.training_function_executor - Epoch 092/169 | train_loss=0.7921 | val_loss=0.7375 | val_acc=0.7188
2025-10-13 22:41:14,858 - INFO - _models.training_function_executor - Epoch 093/169 | train_loss=0.7960 | val_loss=0.7396 | val_acc=0.7188
2025-10-13 22:41:14,863 - INFO - _models.training_function_executor - Epoch 094/169 | train_loss=0.7811 | val_loss=0.7423 | val_acc=0.7188
2025-10-13 22:41:14,868 - INFO - _models.training_function_executor - Epoch 095/169 | train_loss=0.7927 | val_loss=0.7436 | val_acc=0.7188
2025-10-13 22:41:14,873 - INFO - _models.training_function_executor - Epoch 096/169 | train_loss=0.7842 | val_loss=0.7421 | val_acc=0.7188
2025-10-13 22:41:14,878 - INFO - _models.training_function_executor - Epoch 097/169 | train_loss=0.7922 | val_loss=0.7399 | val_acc=0.7188
2025-10-13 22:41:14,883 - INFO - _models.training_function_executor - Epoch 098/169 | train_loss=0.8025 | val_loss=0.7373 | val_acc=0.7188
2025-10-13 22:41:14,888 - INFO - _models.training_function_executor - Epoch 099/169 | train_loss=0.7859 | val_loss=0.7353 | val_acc=0.7188
2025-10-13 22:41:14,893 - INFO - _models.training_function_executor - Epoch 100/169 | train_loss=0.7887 | val_loss=0.7354 | val_acc=0.7188
2025-10-13 22:41:14,898 - INFO - _models.training_function_executor - Epoch 101/169 | train_loss=0.7815 | val_loss=0.7354 | val_acc=0.7188
2025-10-13 22:41:14,903 - INFO - _models.training_function_executor - Epoch 102/169 | train_loss=0.7906 | val_loss=0.7363 | val_acc=0.7188
2025-10-13 22:41:14,908 - INFO - _models.training_function_executor - Epoch 103/169 | train_loss=0.7881 | val_loss=0.7358 | val_acc=0.7188
2025-10-13 22:41:14,913 - INFO - _models.training_function_executor - Epoch 104/169 | train_loss=0.7839 | val_loss=0.7346 | val_acc=0.7188
2025-10-13 22:41:14,918 - INFO - _models.training_function_executor - Epoch 105/169 | train_loss=0.7952 | val_loss=0.7349 | val_acc=0.7188
2025-10-13 22:41:14,923 - INFO - _models.training_function_executor - Epoch 106/169 | train_loss=0.7907 | val_loss=0.7361 | val_acc=0.7188
2025-10-13 22:41:14,928 - INFO - _models.training_function_executor - Epoch 107/169 | train_loss=0.7841 | val_loss=0.7378 | val_acc=0.7188
2025-10-13 22:41:14,933 - INFO - _models.training_function_executor - Epoch 108/169 | train_loss=0.7686 | val_loss=0.7381 | val_acc=0.7188
2025-10-13 22:41:14,938 - INFO - _models.training_function_executor - Epoch 109/169 | train_loss=0.7895 | val_loss=0.7375 | val_acc=0.7188
2025-10-13 22:41:14,943 - INFO - _models.training_function_executor - Epoch 110/169 | train_loss=0.7762 | val_loss=0.7367 | val_acc=0.7188
2025-10-13 22:41:14,948 - INFO - _models.training_function_executor - Epoch 111/169 | train_loss=0.7729 | val_loss=0.7356 | val_acc=0.7188
2025-10-13 22:41:14,954 - INFO - _models.training_function_executor - Epoch 112/169 | train_loss=0.7834 | val_loss=0.7345 | val_acc=0.7188
2025-10-13 22:41:14,959 - INFO - _models.training_function_executor - Epoch 113/169 | train_loss=0.7912 | val_loss=0.7339 | val_acc=0.7188
2025-10-13 22:41:14,964 - INFO - _models.training_function_executor - Epoch 114/169 | train_loss=0.7875 | val_loss=0.7335 | val_acc=0.7188
2025-10-13 22:41:14,969 - INFO - _models.training_function_executor - Epoch 115/169 | train_loss=0.7897 | val_loss=0.7331 | val_acc=0.7188
2025-10-13 22:41:14,974 - INFO - _models.training_function_executor - Epoch 116/169 | train_loss=0.7832 | val_loss=0.7328 | val_acc=0.7188
2025-10-13 22:41:14,979 - INFO - _models.training_function_executor - Epoch 117/169 | train_loss=0.7863 | val_loss=0.7328 | val_acc=0.7188
2025-10-13 22:41:14,984 - INFO - _models.training_function_executor - Epoch 118/169 | train_loss=0.7774 | val_loss=0.7331 | val_acc=0.7188
2025-10-13 22:41:14,989 - INFO - _models.training_function_executor - Epoch 119/169 | train_loss=0.7782 | val_loss=0.7329 | val_acc=0.7188
2025-10-13 22:41:14,994 - INFO - _models.training_function_executor - Epoch 120/169 | train_loss=0.7763 | val_loss=0.7330 | val_acc=0.7188
2025-10-13 22:41:14,999 - INFO - _models.training_function_executor - Epoch 121/169 | train_loss=0.7886 | val_loss=0.7326 | val_acc=0.7188
2025-10-13 22:41:15,004 - INFO - _models.training_function_executor - Epoch 122/169 | train_loss=0.7788 | val_loss=0.7323 | val_acc=0.7188
2025-10-13 22:41:15,009 - INFO - _models.training_function_executor - Epoch 123/169 | train_loss=0.7868 | val_loss=0.7319 | val_acc=0.7188
2025-10-13 22:41:15,014 - INFO - _models.training_function_executor - Epoch 124/169 | train_loss=0.7797 | val_loss=0.7325 | val_acc=0.7188
2025-10-13 22:41:15,019 - INFO - _models.training_function_executor - Epoch 125/169 | train_loss=0.7733 | val_loss=0.7333 | val_acc=0.7188
2025-10-13 22:41:15,024 - INFO - _models.training_function_executor - Epoch 126/169 | train_loss=0.7736 | val_loss=0.7334 | val_acc=0.7188
2025-10-13 22:41:15,029 - INFO - _models.training_function_executor - Epoch 127/169 | train_loss=0.7729 | val_loss=0.7332 | val_acc=0.7188
2025-10-13 22:41:15,034 - INFO - _models.training_function_executor - Epoch 128/169 | train_loss=0.7976 | val_loss=0.7334 | val_acc=0.7188
2025-10-13 22:41:15,039 - INFO - _models.training_function_executor - Epoch 129/169 | train_loss=0.7850 | val_loss=0.7336 | val_acc=0.7188
2025-10-13 22:41:15,044 - INFO - _models.training_function_executor - Epoch 130/169 | train_loss=0.7873 | val_loss=0.7339 | val_acc=0.7188
2025-10-13 22:41:15,049 - INFO - _models.training_function_executor - Epoch 131/169 | train_loss=0.7745 | val_loss=0.7342 | val_acc=0.7188
2025-10-13 22:41:15,054 - INFO - _models.training_function_executor - Epoch 132/169 | train_loss=0.7894 | val_loss=0.7342 | val_acc=0.7188
2025-10-13 22:41:15,059 - INFO - _models.training_function_executor - Epoch 133/169 | train_loss=0.7900 | val_loss=0.7344 | val_acc=0.7188
2025-10-13 22:41:15,064 - INFO - _models.training_function_executor - Epoch 134/169 | train_loss=0.7857 | val_loss=0.7347 | val_acc=0.7188
2025-10-13 22:41:15,069 - INFO - _models.training_function_executor - Epoch 135/169 | train_loss=0.7789 | val_loss=0.7351 | val_acc=0.7188
2025-10-13 22:41:15,074 - INFO - _models.training_function_executor - Epoch 136/169 | train_loss=0.7739 | val_loss=0.7352 | val_acc=0.7188
2025-10-13 22:41:15,079 - INFO - _models.training_function_executor - Epoch 137/169 | train_loss=0.7753 | val_loss=0.7347 | val_acc=0.7188
2025-10-13 22:41:15,085 - INFO - _models.training_function_executor - Epoch 138/169 | train_loss=0.7711 | val_loss=0.7341 | val_acc=0.7188
2025-10-13 22:41:15,090 - INFO - _models.training_function_executor - Epoch 139/169 | train_loss=0.7733 | val_loss=0.7330 | val_acc=0.7188
2025-10-13 22:41:15,095 - INFO - _models.training_function_executor - Epoch 140/169 | train_loss=0.7807 | val_loss=0.7321 | val_acc=0.7188
2025-10-13 22:41:15,101 - INFO - _models.training_function_executor - Epoch 141/169 | train_loss=0.7917 | val_loss=0.7317 | val_acc=0.7188
2025-10-13 22:41:15,107 - INFO - _models.training_function_executor - Epoch 142/169 | train_loss=0.7674 | val_loss=0.7316 | val_acc=0.7188
2025-10-13 22:41:15,112 - INFO - _models.training_function_executor - Epoch 143/169 | train_loss=0.7761 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:15,117 - INFO - _models.training_function_executor - Epoch 144/169 | train_loss=0.7737 | val_loss=0.7313 | val_acc=0.7188
2025-10-13 22:41:15,122 - INFO - _models.training_function_executor - Epoch 145/169 | train_loss=0.7774 | val_loss=0.7312 | val_acc=0.7188
2025-10-13 22:41:15,127 - INFO - _models.training_function_executor - Epoch 146/169 | train_loss=0.7958 | val_loss=0.7310 | val_acc=0.7188
2025-10-13 22:41:15,132 - INFO - _models.training_function_executor - Epoch 147/169 | train_loss=0.7858 | val_loss=0.7308 | val_acc=0.7188
2025-10-13 22:41:15,137 - INFO - _models.training_function_executor - Epoch 148/169 | train_loss=0.7808 | val_loss=0.7311 | val_acc=0.7188
2025-10-13 22:41:15,142 - INFO - _models.training_function_executor - Epoch 149/169 | train_loss=0.7750 | val_loss=0.7311 | val_acc=0.7188
2025-10-13 22:41:15,147 - INFO - _models.training_function_executor - Epoch 150/169 | train_loss=0.7879 | val_loss=0.7311 | val_acc=0.7188
2025-10-13 22:41:15,152 - INFO - _models.training_function_executor - Epoch 151/169 | train_loss=0.7816 | val_loss=0.7311 | val_acc=0.7188
2025-10-13 22:41:15,157 - INFO - _models.training_function_executor - Epoch 152/169 | train_loss=0.7814 | val_loss=0.7312 | val_acc=0.7188
2025-10-13 22:41:15,162 - INFO - _models.training_function_executor - Epoch 153/169 | train_loss=0.7873 | val_loss=0.7314 | val_acc=0.7188
2025-10-13 22:41:15,167 - INFO - _models.training_function_executor - Epoch 154/169 | train_loss=0.7729 | val_loss=0.7313 | val_acc=0.7188
2025-10-13 22:41:15,172 - INFO - _models.training_function_executor - Epoch 155/169 | train_loss=0.7682 | val_loss=0.7312 | val_acc=0.7188
2025-10-13 22:41:15,177 - INFO - _models.training_function_executor - Epoch 156/169 | train_loss=0.7874 | val_loss=0.7313 | val_acc=0.7188
2025-10-13 22:41:15,182 - INFO - _models.training_function_executor - Epoch 157/169 | train_loss=0.7853 | val_loss=0.7313 | val_acc=0.7188
2025-10-13 22:41:15,188 - INFO - _models.training_function_executor - Epoch 158/169 | train_loss=0.7698 | val_loss=0.7314 | val_acc=0.7188
2025-10-13 22:41:15,194 - INFO - _models.training_function_executor - Epoch 159/169 | train_loss=0.7777 | val_loss=0.7314 | val_acc=0.7188
2025-10-13 22:41:15,199 - INFO - _models.training_function_executor - Epoch 160/169 | train_loss=0.7832 | val_loss=0.7316 | val_acc=0.7188
2025-10-13 22:41:15,205 - INFO - _models.training_function_executor - Epoch 161/169 | train_loss=0.7770 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:15,210 - INFO - _models.training_function_executor - Epoch 162/169 | train_loss=0.7721 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:15,215 - INFO - _models.training_function_executor - Epoch 163/169 | train_loss=0.7877 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:15,220 - INFO - _models.training_function_executor - Epoch 164/169 | train_loss=0.7867 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:15,225 - INFO - _models.training_function_executor - Epoch 165/169 | train_loss=0.7826 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:15,230 - INFO - _models.training_function_executor - Epoch 166/169 | train_loss=0.7826 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:15,235 - INFO - _models.training_function_executor - Epoch 167/169 | train_loss=0.7803 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:15,240 - INFO - _models.training_function_executor - Epoch 168/169 | train_loss=0.7884 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:15,245 - INFO - _models.training_function_executor - Epoch 169/169 | train_loss=0.7658 | val_loss=0.7315 | val_acc=0.7188
2025-10-13 22:41:16,059 - INFO - _models.training_function_executor - Model: 1,465 parameters, 3.1KB storage
2025-10-13 22:41:16,059 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [13.627308130264282, 13.095777869224548, 10.271668076515198, 6.034015595912933, 4.138884425163269, 3.2457125186920166, 3.560650110244751, 3.111846387386322, 2.797415465116501, 2.4523196518421173, 2.1417329907417297, 2.0129083395004272, 1.569238469004631, 1.5601681917905807, 1.4519302248954773, 1.265850305557251, 1.2620915621519089, 1.091091588139534, 1.0141678974032402, 1.0133479982614517, 1.001792624592781, 0.9446513801813126, 0.913394920527935, 0.9279917031526566, 0.9167201295495033, 0.9016987234354019, 0.8745672628283501, 0.8702067956328392, 0.8693469986319542, 0.8623399585485458, 0.8730896487832069, 0.8642736375331879, 0.8813600763678551, 0.8642449229955673, 0.8170369938015938, 0.8217710927128792, 0.8313156291842461, 0.8309480920433998, 0.8271218612790108, 0.810888335108757, 0.8167830854654312, 0.8083969205617905, 0.8232884779572487, 0.7953768074512482, 0.834417074918747, 0.831776462495327, 0.8188499212265015, 0.8433826714754105, 0.7961666285991669, 0.8103444576263428, 0.8042353093624115, 0.7850191965699196, 0.8010612428188324, 0.7999649569392204, 0.7803925275802612, 0.8120797201991081, 0.8250000700354576, 0.8176815137267113, 0.8338618203997612, 0.8112163320183754, 0.7919542565941811, 0.7963558286428452, 0.8042254447937012, 0.8008416965603828, 0.7899459004402161, 0.7906590327620506, 0.7887627333402634, 0.7994691133499146, 0.8128675818443298, 0.7916211485862732, 0.8047432452440262, 0.8076627179980278, 0.8063999190926552, 0.8121549040079117, 0.8071694746613503, 0.802164189517498, 0.8004986867308617, 0.798892430961132, 0.8018796890974045, 0.7975090071558952, 0.782889649271965, 0.7770286425948143, 0.7763771042227745, 0.8057918697595596, 0.7939093932509422, 0.789901502430439, 0.77309450507164, 0.7981923744082451, 0.7772596180438995, 0.7846243157982826, 0.802602007985115, 0.7920577451586723, 0.7959983050823212, 0.7810650989413261, 0.7926545664668083, 0.7841994911432266, 0.7921552658081055, 0.8025187253952026, 0.7858559861779213, 0.7886990159749985, 0.7815012782812119, 0.7906154319643974, 0.7881121933460236, 0.7838506251573563, 0.7951592728495598, 0.7907125279307365, 0.7840951085090637, 0.7685835361480713, 0.7894979789853096, 0.7761754766106606, 0.7728573307394981, 0.7834271341562271, 0.7911534830927849, 0.7874696925282478, 0.7896911650896072, 0.7831754982471466, 0.786289669573307, 0.7773864418268204, 0.7782371565699577, 0.7763051763176918, 0.7886037155985832, 0.7787783592939377, 0.7867856472730637, 0.7796620354056358, 0.7733445763587952, 0.7735641375184059, 0.7729006856679916, 0.7975916564464569, 0.7849532216787338, 0.7873436361551285, 0.7745133116841316, 0.7894379794597626, 0.7899542450904846, 0.7857459113001823, 0.7788683846592903, 0.7739368677139282, 0.7753294110298157, 0.7711206153035164, 0.7733276411890984, 0.7806780263781548, 0.7917317003011703, 0.7673701196908951, 0.7761031836271286, 0.7737148180603981, 0.7773907408118248, 0.7958071082830429, 0.7857733145356178, 0.7808302119374275, 0.7749689966440201, 0.7878649532794952, 0.7815979272127151, 0.7813803255558014, 0.7873378470540047, 0.7728796303272247, 0.768246240913868, 0.7874181494116783, 0.7852944359183311, 0.7697730511426926, 0.7776612788438797, 0.7831708267331123, 0.7769908979535103, 0.7721127644181252, 0.7877082526683807, 0.78667401522398, 0.78255395591259, 0.7826155424118042, 0.7802749946713448, 0.7883802354335785, 0.7657598704099655], 'val_losses': [12.472850799560547, 9.51898193359375, 3.696542263031006, 2.1436245441436768, 2.424123525619507, 1.907069444656372, 1.5823192596435547, 1.2791721820831299, 1.2210910320281982, 1.055268406867981, 0.8802371025085449, 0.866603434085846, 0.8969526886940002, 0.8667975664138794, 0.8300515413284302, 0.8104245662689209, 0.8143519759178162, 0.8239666819572449, 0.8337014317512512, 0.837242603302002, 0.8508565425872803, 0.850612461566925, 0.844666063785553, 0.8516786098480225, 0.8459383845329285, 0.8475354313850403, 0.8439111113548279, 0.8313502073287964, 0.809931755065918, 0.7876138687133789, 0.7814725637435913, 0.7917998433113098, 0.7962177395820618, 0.8029435873031616, 0.7986703515052795, 0.7821853756904602, 0.7747494578361511, 0.7712091207504272, 0.7701460123062134, 0.7692282795906067, 0.766157865524292, 0.7662838101387024, 0.7599563598632812, 0.7492045164108276, 0.744672954082489, 0.7540488243103027, 0.7557790279388428, 0.7621166706085205, 0.759532630443573, 0.7541334629058838, 0.759527862071991, 0.7627087831497192, 0.763887345790863, 0.7676626443862915, 0.7634407877922058, 0.7572622895240784, 0.7604106664657593, 0.7613844275474548, 0.7605481147766113, 0.7558441162109375, 0.749046802520752, 0.7443133592605591, 0.7393119931221008, 0.7392354011535645, 0.7408838272094727, 0.745306134223938, 0.7434542179107666, 0.7392185926437378, 0.739078164100647, 0.7416288256645203, 0.7448302507400513, 0.7481788992881775, 0.7472944259643555, 0.7473220825195312, 0.7459564208984375, 0.7430540919303894, 0.7405428290367126, 0.7382833957672119, 0.7373417615890503, 0.7383574843406677, 0.7413673400878906, 0.7424114942550659, 0.7428779602050781, 0.7418949604034424, 0.7390878796577454, 0.7367426753044128, 0.732871949672699, 0.7303836345672607, 0.7309227585792542, 0.7330169677734375, 0.7357361316680908, 0.737464189529419, 0.7396376729011536, 0.7422622442245483, 0.743644118309021, 0.74211186170578, 0.7399297952651978, 0.7372830510139465, 0.7353483438491821, 0.7354351282119751, 0.735350489616394, 0.736319899559021, 0.7358036637306213, 0.7346192002296448, 0.7348902225494385, 0.7361127138137817, 0.7377876043319702, 0.7381066083908081, 0.7375332117080688, 0.7367408275604248, 0.7356141209602356, 0.7344692945480347, 0.7338659763336182, 0.7335076928138733, 0.7331415414810181, 0.7328399419784546, 0.7327893376350403, 0.7330880761146545, 0.7328923344612122, 0.7329981327056885, 0.7326162457466125, 0.7322922945022583, 0.7318850755691528, 0.7325404286384583, 0.7333123683929443, 0.7333911657333374, 0.7331518530845642, 0.7334041595458984, 0.7335845232009888, 0.7339116334915161, 0.7341517210006714, 0.7342479825019836, 0.7344251871109009, 0.7347003221511841, 0.7351312041282654, 0.7351775169372559, 0.734748125076294, 0.7341006398200989, 0.7330307960510254, 0.7321006059646606, 0.7316594123840332, 0.7316197752952576, 0.7314847111701965, 0.7313189506530762, 0.7311635613441467, 0.7310400009155273, 0.7308416962623596, 0.7311218976974487, 0.7310808897018433, 0.7311460971832275, 0.7310526371002197, 0.7311902642250061, 0.7313501834869385, 0.7313038110733032, 0.7312420606613159, 0.7312681674957275, 0.7312867045402527, 0.7314250469207764, 0.7314449548721313, 0.7315527200698853, 0.7315373420715332, 0.731495201587677, 0.7315294742584229, 0.7315020561218262, 0.7314859628677368, 0.7314711809158325, 0.7314786911010742, 0.7315045595169067, 0.7315045595169067], 'val_acc': [0.09375, 0.09375, 0.09375, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.65625, 0.65625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0015294663694452992, 'batch_size': 96, 'epochs': 169, 'hidden_size': 34, 'dropout': 0.17816498822834012, 'weight_decay': 0.0020173827714239185, 'label_smoothing': 0.10245068677860654, 'grad_clip_norm': 2.0768324953734245, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 1465, 'model_storage_size_kb': 3.1474609375000004, 'model_size_validation': 'PASS'}
2025-10-13 22:41:16,059 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:41:16,059 - INFO - _models.training_function_executor - Model: 1,465 parameters, 3.1KB (PASS 256KB limit)
2025-10-13 22:41:16,059 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.228s
2025-10-13 22:41:16,121 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:41:16,121 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.061s
2025-10-13 22:41:16,121 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.0015294663694452992, 'batch_size': np.int64(96), 'epochs': np.int64(169), 'hidden_size': np.int64(34), 'dropout': 0.17816498822834012, 'weight_decay': 0.0020173827714239185, 'label_smoothing': 0.10245068677860654, 'grad_clip_norm': 2.0768324953734245, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:41:16,121 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.0015294663694452992, 'batch_size': np.int64(96), 'epochs': np.int64(169), 'hidden_size': np.int64(34), 'dropout': 0.17816498822834012, 'weight_decay': 0.0020173827714239185, 'label_smoothing': 0.10245068677860654, 'grad_clip_norm': 2.0768324953734245, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:41:16,121 - INFO - bo.run_bo - üîçBO Trial 8: Using RF surrogate + Expected Improvement
2025-10-13 22:41:16,121 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:16,121 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 8 (NaN monitoring active)
2025-10-13 22:41:16,121 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:16,121 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:16,121 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002347001246134893, 'batch_size': 128, 'epochs': 44, 'hidden_size': 186, 'dropout': 0.308965826052733, 'weight_decay': 0.0004666005440084391, 'label_smoothing': 0.15967028829112617, 'grad_clip_norm': 3.9447596773451266, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:16,122 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002347001246134893, 'batch_size': 128, 'epochs': 44, 'hidden_size': 186, 'dropout': 0.308965826052733, 'weight_decay': 0.0004666005440084391, 'label_smoothing': 0.15967028829112617, 'grad_clip_norm': 3.9447596773451266, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:19,863 - INFO - _models.training_function_executor - Epoch 001/044 | train_loss=5.4480 | val_loss=3.5396 | val_acc=0.0938
2025-10-13 22:41:19,869 - INFO - _models.training_function_executor - Epoch 002/044 | train_loss=5.7763 | val_loss=3.5396 | val_acc=0.0938
2025-10-13 22:41:19,874 - INFO - _models.training_function_executor - Epoch 003/044 | train_loss=7.5915 | val_loss=8.8020 | val_acc=0.7188
2025-10-13 22:41:19,878 - INFO - _models.training_function_executor - Epoch 004/044 | train_loss=7.8731 | val_loss=3.3283 | val_acc=0.6719
2025-10-13 22:41:19,882 - INFO - _models.training_function_executor - Epoch 005/044 | train_loss=5.7807 | val_loss=2.6760 | val_acc=0.7188
2025-10-13 22:41:19,886 - INFO - _models.training_function_executor - Epoch 006/044 | train_loss=3.5239 | val_loss=2.3905 | val_acc=0.2656
2025-10-13 22:41:19,890 - INFO - _models.training_function_executor - Epoch 007/044 | train_loss=3.2371 | val_loss=0.8352 | val_acc=0.7188
2025-10-13 22:41:19,894 - INFO - _models.training_function_executor - Epoch 008/044 | train_loss=2.1953 | val_loss=1.2118 | val_acc=0.7188
2025-10-13 22:41:19,898 - INFO - _models.training_function_executor - Epoch 009/044 | train_loss=2.1300 | val_loss=0.9051 | val_acc=0.7188
2025-10-13 22:41:19,902 - INFO - _models.training_function_executor - Epoch 010/044 | train_loss=1.6384 | val_loss=0.8627 | val_acc=0.7188
2025-10-13 22:41:19,907 - INFO - _models.training_function_executor - Epoch 011/044 | train_loss=1.3607 | val_loss=1.0563 | val_acc=0.4688
2025-10-13 22:41:19,911 - INFO - _models.training_function_executor - Epoch 012/044 | train_loss=1.2012 | val_loss=0.9834 | val_acc=0.7188
2025-10-13 22:41:19,915 - INFO - _models.training_function_executor - Epoch 013/044 | train_loss=1.0781 | val_loss=0.9377 | val_acc=0.7188
2025-10-13 22:41:19,919 - INFO - _models.training_function_executor - Epoch 014/044 | train_loss=1.0604 | val_loss=0.9336 | val_acc=0.7188
2025-10-13 22:41:19,923 - INFO - _models.training_function_executor - Epoch 015/044 | train_loss=0.9670 | val_loss=0.9132 | val_acc=0.7188
2025-10-13 22:41:19,927 - INFO - _models.training_function_executor - Epoch 016/044 | train_loss=0.9512 | val_loss=0.9225 | val_acc=0.7188
2025-10-13 22:41:19,931 - INFO - _models.training_function_executor - Epoch 017/044 | train_loss=0.9116 | val_loss=0.9038 | val_acc=0.7188
2025-10-13 22:41:19,935 - INFO - _models.training_function_executor - Epoch 018/044 | train_loss=0.8874 | val_loss=0.8695 | val_acc=0.7188
2025-10-13 22:41:19,939 - INFO - _models.training_function_executor - Epoch 019/044 | train_loss=0.9225 | val_loss=0.8726 | val_acc=0.7188
2025-10-13 22:41:19,943 - INFO - _models.training_function_executor - Epoch 020/044 | train_loss=0.9019 | val_loss=0.8841 | val_acc=0.7188
2025-10-13 22:41:19,947 - INFO - _models.training_function_executor - Epoch 021/044 | train_loss=0.9027 | val_loss=0.8922 | val_acc=0.7188
2025-10-13 22:41:19,951 - INFO - _models.training_function_executor - Epoch 022/044 | train_loss=0.9086 | val_loss=0.8807 | val_acc=0.7188
2025-10-13 22:41:19,955 - INFO - _models.training_function_executor - Epoch 023/044 | train_loss=0.8893 | val_loss=0.8583 | val_acc=0.7188
2025-10-13 22:41:19,959 - INFO - _models.training_function_executor - Epoch 024/044 | train_loss=0.8701 | val_loss=0.8293 | val_acc=0.7188
2025-10-13 22:41:19,963 - INFO - _models.training_function_executor - Epoch 025/044 | train_loss=0.8794 | val_loss=0.8182 | val_acc=0.7188
2025-10-13 22:41:19,967 - INFO - _models.training_function_executor - Epoch 026/044 | train_loss=0.8812 | val_loss=0.8264 | val_acc=0.7188
2025-10-13 22:41:19,971 - INFO - _models.training_function_executor - Epoch 027/044 | train_loss=0.8534 | val_loss=0.8391 | val_acc=0.7188
2025-10-13 22:41:19,975 - INFO - _models.training_function_executor - Epoch 028/044 | train_loss=0.8537 | val_loss=0.8379 | val_acc=0.7188
2025-10-13 22:41:19,979 - INFO - _models.training_function_executor - Epoch 029/044 | train_loss=0.8544 | val_loss=0.8327 | val_acc=0.7188
2025-10-13 22:41:19,983 - INFO - _models.training_function_executor - Epoch 030/044 | train_loss=0.8606 | val_loss=0.8316 | val_acc=0.7188
2025-10-13 22:41:19,987 - INFO - _models.training_function_executor - Epoch 031/044 | train_loss=0.8473 | val_loss=0.8285 | val_acc=0.7188
2025-10-13 22:41:19,992 - INFO - _models.training_function_executor - Epoch 032/044 | train_loss=0.8501 | val_loss=0.8242 | val_acc=0.7188
2025-10-13 22:41:19,996 - INFO - _models.training_function_executor - Epoch 033/044 | train_loss=0.8551 | val_loss=0.8219 | val_acc=0.7188
2025-10-13 22:41:20,000 - INFO - _models.training_function_executor - Epoch 034/044 | train_loss=0.8685 | val_loss=0.8198 | val_acc=0.7188
2025-10-13 22:41:20,004 - INFO - _models.training_function_executor - Epoch 035/044 | train_loss=0.8501 | val_loss=0.8197 | val_acc=0.7188
2025-10-13 22:41:20,009 - INFO - _models.training_function_executor - Epoch 036/044 | train_loss=0.8795 | val_loss=0.8202 | val_acc=0.7188
2025-10-13 22:41:20,013 - INFO - _models.training_function_executor - Epoch 037/044 | train_loss=0.8523 | val_loss=0.8208 | val_acc=0.7188
2025-10-13 22:41:20,017 - INFO - _models.training_function_executor - Epoch 038/044 | train_loss=0.8677 | val_loss=0.8217 | val_acc=0.7188
2025-10-13 22:41:20,021 - INFO - _models.training_function_executor - Epoch 039/044 | train_loss=0.8430 | val_loss=0.8218 | val_acc=0.7188
2025-10-13 22:41:20,026 - INFO - _models.training_function_executor - Epoch 040/044 | train_loss=0.8493 | val_loss=0.8218 | val_acc=0.7188
2025-10-13 22:41:20,030 - INFO - _models.training_function_executor - Epoch 041/044 | train_loss=0.8585 | val_loss=0.8221 | val_acc=0.7188
2025-10-13 22:41:20,034 - INFO - _models.training_function_executor - Epoch 042/044 | train_loss=0.8683 | val_loss=0.8222 | val_acc=0.7188
2025-10-13 22:41:20,039 - INFO - _models.training_function_executor - Epoch 043/044 | train_loss=0.8388 | val_loss=0.8222 | val_acc=0.7188
2025-10-13 22:41:20,043 - INFO - _models.training_function_executor - Epoch 044/044 | train_loss=0.8674 | val_loss=0.8223 | val_acc=0.7188
2025-10-13 22:41:20,887 - INFO - _models.training_function_executor - Model: 36,273 parameters, 155.9KB storage
2025-10-13 22:41:20,887 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [5.448045492172241, 5.776302814483643, 7.5915069580078125, 7.873145818710327, 5.780686855316162, 3.5238531827926636, 3.237108588218689, 2.195284366607666, 2.1300345063209534, 1.6383508443832397, 1.360679805278778, 1.201234757900238, 1.078138828277588, 1.0603810548782349, 0.9669948816299438, 0.9511616230010986, 0.9115897715091705, 0.8874217569828033, 0.9225047528743744, 0.9018771350383759, 0.9027393162250519, 0.9086032509803772, 0.8893447816371918, 0.8701297640800476, 0.8793801963329315, 0.8811961114406586, 0.853414922952652, 0.8537264168262482, 0.854432225227356, 0.860590010881424, 0.8473167717456818, 0.8500933349132538, 0.8550857901573181, 0.868519127368927, 0.8500945866107941, 0.8795127272605896, 0.8522732555866241, 0.8676902651786804, 0.8429938554763794, 0.8492788374423981, 0.8585492372512817, 0.8682829737663269, 0.8388343751430511, 0.867392897605896], 'val_losses': [3.539579153060913, 3.539579153060913, 8.802014350891113, 3.328279495239258, 2.6760127544403076, 2.3905484676361084, 0.8351948261260986, 1.2117819786071777, 0.9051303863525391, 0.8627327680587769, 1.0562715530395508, 0.9833712577819824, 0.9376978278160095, 0.9335899949073792, 0.9131960868835449, 0.9225426912307739, 0.9037656784057617, 0.8695292472839355, 0.8726317882537842, 0.8840726613998413, 0.8921581506729126, 0.8807358741760254, 0.8583394885063171, 0.8292598128318787, 0.8182194828987122, 0.8263933658599854, 0.839149534702301, 0.8379167318344116, 0.8326935768127441, 0.8316081166267395, 0.8284934163093567, 0.8241684436798096, 0.8218562602996826, 0.8197821378707886, 0.8197020888328552, 0.8202382326126099, 0.8208169341087341, 0.8217010498046875, 0.8218483924865723, 0.8218432664871216, 0.8221197128295898, 0.8221921920776367, 0.8222218751907349, 0.822269082069397], 'val_acc': [0.09375, 0.09375, 0.71875, 0.671875, 0.71875, 0.265625, 0.71875, 0.71875, 0.71875, 0.71875, 0.46875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002347001246134893, 'batch_size': 128, 'epochs': 44, 'hidden_size': 186, 'dropout': 0.308965826052733, 'weight_decay': 0.0004666005440084391, 'label_smoothing': 0.15967028829112617, 'grad_clip_norm': 3.9447596773451266, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 36273, 'model_storage_size_kb': 155.860546875, 'model_size_validation': 'PASS'}
2025-10-13 22:41:20,887 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:41:20,887 - INFO - _models.training_function_executor - Model: 36,273 parameters, 155.9KB (PASS 256KB limit)
2025-10-13 22:41:20,887 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.766s
2025-10-13 22:41:20,951 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:41:20,951 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.063s
2025-10-13 22:41:20,951 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 0.002347001246134893, 'batch_size': np.int64(128), 'epochs': np.int64(44), 'hidden_size': np.int64(186), 'dropout': 0.308965826052733, 'weight_decay': 0.0004666005440084391, 'label_smoothing': 0.15967028829112617, 'grad_clip_norm': 3.9447596773451266, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:41:20,951 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 0.002347001246134893, 'batch_size': np.int64(128), 'epochs': np.int64(44), 'hidden_size': np.int64(186), 'dropout': 0.308965826052733, 'weight_decay': 0.0004666005440084391, 'label_smoothing': 0.15967028829112617, 'grad_clip_norm': 3.9447596773451266, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:41:20,951 - INFO - bo.run_bo - üîçBO Trial 9: Using RF surrogate + Expected Improvement
2025-10-13 22:41:20,951 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:20,951 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 9 (NaN monitoring active)
2025-10-13 22:41:20,951 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:20,951 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:20,951 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00044337152861535507, 'batch_size': 96, 'epochs': 25, 'hidden_size': 91, 'dropout': 0.4394527600093718, 'weight_decay': 2.995519690939991e-05, 'label_smoothing': 0.027195657512135048, 'grad_clip_norm': 4.47076571066717, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:41:20,952 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00044337152861535507, 'batch_size': 96, 'epochs': 25, 'hidden_size': 91, 'dropout': 0.4394527600093718, 'weight_decay': 2.995519690939991e-05, 'label_smoothing': 0.027195657512135048, 'grad_clip_norm': 4.47076571066717, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:41:24,904 - INFO - _models.training_function_executor - Epoch 001/025 | train_loss=9.3308 | val_loss=2.2126 | val_acc=0.1875
2025-10-13 22:41:24,912 - INFO - _models.training_function_executor - Epoch 002/025 | train_loss=8.2308 | val_loss=1.2312 | val_acc=0.5000
2025-10-13 22:41:24,918 - INFO - _models.training_function_executor - Epoch 003/025 | train_loss=7.7721 | val_loss=2.5461 | val_acc=0.7188
2025-10-13 22:41:24,924 - INFO - _models.training_function_executor - Epoch 004/025 | train_loss=4.7962 | val_loss=3.9940 | val_acc=0.7188
2025-10-13 22:41:24,930 - INFO - _models.training_function_executor - Epoch 005/025 | train_loss=5.4222 | val_loss=4.8321 | val_acc=0.7188
2025-10-13 22:41:24,935 - INFO - _models.training_function_executor - Epoch 006/025 | train_loss=5.9214 | val_loss=4.9846 | val_acc=0.7188
2025-10-13 22:41:24,941 - INFO - _models.training_function_executor - Epoch 007/025 | train_loss=5.6302 | val_loss=4.7215 | val_acc=0.7188
2025-10-13 22:41:24,946 - INFO - _models.training_function_executor - Epoch 008/025 | train_loss=4.8038 | val_loss=4.3470 | val_acc=0.7188
2025-10-13 22:41:24,952 - INFO - _models.training_function_executor - Epoch 009/025 | train_loss=5.2616 | val_loss=3.9081 | val_acc=0.7188
2025-10-13 22:41:24,957 - INFO - _models.training_function_executor - Epoch 010/025 | train_loss=4.7972 | val_loss=3.4350 | val_acc=0.7188
2025-10-13 22:41:24,962 - INFO - _models.training_function_executor - Epoch 011/025 | train_loss=4.5551 | val_loss=3.0892 | val_acc=0.7188
2025-10-13 22:41:24,968 - INFO - _models.training_function_executor - Epoch 012/025 | train_loss=4.4516 | val_loss=2.8749 | val_acc=0.7188
2025-10-13 22:41:24,973 - INFO - _models.training_function_executor - Epoch 013/025 | train_loss=3.8093 | val_loss=2.7360 | val_acc=0.7188
2025-10-13 22:41:24,979 - INFO - _models.training_function_executor - Epoch 014/025 | train_loss=4.4388 | val_loss=2.6956 | val_acc=0.7188
2025-10-13 22:41:24,984 - INFO - _models.training_function_executor - Epoch 015/025 | train_loss=4.5187 | val_loss=2.7050 | val_acc=0.7188
2025-10-13 22:41:24,989 - INFO - _models.training_function_executor - Epoch 016/025 | train_loss=3.8667 | val_loss=2.6958 | val_acc=0.7188
2025-10-13 22:41:24,994 - INFO - _models.training_function_executor - Epoch 017/025 | train_loss=3.6980 | val_loss=2.6910 | val_acc=0.7188
2025-10-13 22:41:24,999 - INFO - _models.training_function_executor - Epoch 018/025 | train_loss=3.8241 | val_loss=2.6589 | val_acc=0.7188
2025-10-13 22:41:25,004 - INFO - _models.training_function_executor - Epoch 019/025 | train_loss=3.7227 | val_loss=2.6252 | val_acc=0.7188
2025-10-13 22:41:25,010 - INFO - _models.training_function_executor - Epoch 020/025 | train_loss=3.5059 | val_loss=2.6024 | val_acc=0.7188
2025-10-13 22:41:25,016 - INFO - _models.training_function_executor - Epoch 021/025 | train_loss=3.9195 | val_loss=2.5777 | val_acc=0.7188
2025-10-13 22:41:25,021 - INFO - _models.training_function_executor - Epoch 022/025 | train_loss=3.6675 | val_loss=2.5582 | val_acc=0.7188
2025-10-13 22:41:25,027 - INFO - _models.training_function_executor - Epoch 023/025 | train_loss=3.8407 | val_loss=2.5471 | val_acc=0.7188
2025-10-13 22:41:25,032 - INFO - _models.training_function_executor - Epoch 024/025 | train_loss=3.6508 | val_loss=2.5436 | val_acc=0.7188
2025-10-13 22:41:25,037 - INFO - _models.training_function_executor - Epoch 025/025 | train_loss=3.8206 | val_loss=2.5429 | val_acc=0.7188
2025-10-13 22:41:25,918 - INFO - _models.training_function_executor - Model: 9,103 parameters, 39.1KB storage
2025-10-13 22:41:25,918 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [9.330799221992493, 8.23080575466156, 7.772108435630798, 4.796236097812653, 5.422150731086731, 5.9214407205581665, 5.630196630954742, 4.803770005702972, 5.261615753173828, 4.7971804440021515, 4.555061638355255, 4.451618194580078, 3.809322953224182, 4.438791930675507, 4.518718242645264, 3.8667476773262024, 3.6979878544807434, 3.8240621984004974, 3.7226828932762146, 3.505917251110077, 3.919540286064148, 3.6674538254737854, 3.8407169580459595, 3.6507880985736847, 3.8205981850624084], 'val_losses': [2.212554931640625, 1.2311575412750244, 2.546147346496582, 3.993975877761841, 4.832124710083008, 4.984607696533203, 4.72145938873291, 4.347030162811279, 3.9080896377563477, 3.434974193572998, 3.0892224311828613, 2.8748908042907715, 2.7359654903411865, 2.695551872253418, 2.705003499984741, 2.6958038806915283, 2.6909966468811035, 2.658932685852051, 2.6252105236053467, 2.6024115085601807, 2.5776753425598145, 2.5582165718078613, 2.547110080718994, 2.5435752868652344, 2.5428855419158936], 'val_acc': [0.1875, 0.5, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00044337152861535507, 'batch_size': 96, 'epochs': 25, 'hidden_size': 91, 'dropout': 0.4394527600093718, 'weight_decay': 2.995519690939991e-05, 'label_smoothing': 0.027195657512135048, 'grad_clip_norm': 4.47076571066717, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 9103, 'model_storage_size_kb': 39.114453125000004, 'model_size_validation': 'PASS'}
2025-10-13 22:41:25,918 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:41:25,918 - INFO - _models.training_function_executor - Model: 9,103 parameters, 39.1KB (PASS 256KB limit)
2025-10-13 22:41:25,918 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.967s
2025-10-13 22:41:25,982 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:41:25,982 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.064s
2025-10-13 22:41:25,983 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 0.00044337152861535507, 'batch_size': np.int64(96), 'epochs': np.int64(25), 'hidden_size': np.int64(91), 'dropout': 0.4394527600093718, 'weight_decay': 2.995519690939991e-05, 'label_smoothing': 0.027195657512135048, 'grad_clip_norm': 4.47076571066717, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:41:25,983 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 0.00044337152861535507, 'batch_size': np.int64(96), 'epochs': np.int64(25), 'hidden_size': np.int64(91), 'dropout': 0.4394527600093718, 'weight_decay': 2.995519690939991e-05, 'label_smoothing': 0.027195657512135048, 'grad_clip_norm': 4.47076571066717, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:41:25,983 - INFO - bo.run_bo - üîçBO Trial 10: Using RF surrogate + Expected Improvement
2025-10-13 22:41:25,983 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:25,983 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 10 (NaN monitoring active)
2025-10-13 22:41:25,983 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:25,983 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:25,983 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.1275001669000045e-05, 'batch_size': 256, 'epochs': 28, 'hidden_size': 71, 'dropout': 0.14884664103561565, 'weight_decay': 0.006471720841701994, 'label_smoothing': 0.12768152403762514, 'grad_clip_norm': 4.056721638776345, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:41:25,984 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.1275001669000045e-05, 'batch_size': 256, 'epochs': 28, 'hidden_size': 71, 'dropout': 0.14884664103561565, 'weight_decay': 0.006471720841701994, 'label_smoothing': 0.12768152403762514, 'grad_clip_norm': 4.056721638776345, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:41:29,711 - INFO - _models.training_function_executor - Epoch 001/028 | train_loss=2.6472 | val_loss=1.5796 | val_acc=0.7188
2025-10-13 22:41:29,717 - INFO - _models.training_function_executor - Epoch 002/028 | train_loss=2.7630 | val_loss=1.5796 | val_acc=0.7188
2025-10-13 22:41:29,721 - INFO - _models.training_function_executor - Epoch 003/028 | train_loss=2.4986 | val_loss=1.5796 | val_acc=0.7188
2025-10-13 22:41:29,725 - INFO - _models.training_function_executor - Epoch 004/028 | train_loss=2.5895 | val_loss=1.5771 | val_acc=0.7188
2025-10-13 22:41:29,728 - INFO - _models.training_function_executor - Epoch 005/028 | train_loss=2.5878 | val_loss=1.5749 | val_acc=0.7188
2025-10-13 22:41:29,732 - INFO - _models.training_function_executor - Epoch 006/028 | train_loss=2.8200 | val_loss=1.5734 | val_acc=0.7188
2025-10-13 22:41:29,735 - INFO - _models.training_function_executor - Epoch 007/028 | train_loss=2.5495 | val_loss=1.5724 | val_acc=0.7188
2025-10-13 22:41:29,738 - INFO - _models.training_function_executor - Epoch 008/028 | train_loss=2.6890 | val_loss=1.5702 | val_acc=0.7188
2025-10-13 22:41:29,742 - INFO - _models.training_function_executor - Epoch 009/028 | train_loss=2.5711 | val_loss=1.5688 | val_acc=0.7188
2025-10-13 22:41:29,745 - INFO - _models.training_function_executor - Epoch 010/028 | train_loss=2.6872 | val_loss=1.5687 | val_acc=0.7188
2025-10-13 22:41:29,749 - INFO - _models.training_function_executor - Epoch 011/028 | train_loss=2.7352 | val_loss=1.5681 | val_acc=0.7188
2025-10-13 22:41:29,752 - INFO - _models.training_function_executor - Epoch 012/028 | train_loss=2.3380 | val_loss=1.5680 | val_acc=0.7188
2025-10-13 22:41:29,755 - INFO - _models.training_function_executor - Epoch 013/028 | train_loss=2.2899 | val_loss=1.5658 | val_acc=0.7188
2025-10-13 22:41:29,759 - INFO - _models.training_function_executor - Epoch 014/028 | train_loss=2.7925 | val_loss=1.5644 | val_acc=0.7188
2025-10-13 22:41:29,762 - INFO - _models.training_function_executor - Epoch 015/028 | train_loss=2.6567 | val_loss=1.5637 | val_acc=0.7188
2025-10-13 22:41:29,765 - INFO - _models.training_function_executor - Epoch 016/028 | train_loss=2.6385 | val_loss=1.5645 | val_acc=0.7188
2025-10-13 22:41:29,768 - INFO - _models.training_function_executor - Epoch 017/028 | train_loss=2.9249 | val_loss=1.5635 | val_acc=0.7188
2025-10-13 22:41:29,772 - INFO - _models.training_function_executor - Epoch 018/028 | train_loss=2.8006 | val_loss=1.5637 | val_acc=0.7188
2025-10-13 22:41:29,775 - INFO - _models.training_function_executor - Epoch 019/028 | train_loss=2.8231 | val_loss=1.5634 | val_acc=0.7188
2025-10-13 22:41:29,778 - INFO - _models.training_function_executor - Epoch 020/028 | train_loss=2.6284 | val_loss=1.5630 | val_acc=0.7188
2025-10-13 22:41:29,781 - INFO - _models.training_function_executor - Epoch 021/028 | train_loss=2.5795 | val_loss=1.5632 | val_acc=0.7188
2025-10-13 22:41:29,785 - INFO - _models.training_function_executor - Epoch 022/028 | train_loss=2.6154 | val_loss=1.5624 | val_acc=0.7188
2025-10-13 22:41:29,788 - INFO - _models.training_function_executor - Epoch 023/028 | train_loss=2.4771 | val_loss=1.5623 | val_acc=0.7188
2025-10-13 22:41:29,791 - INFO - _models.training_function_executor - Epoch 024/028 | train_loss=2.6692 | val_loss=1.5623 | val_acc=0.7188
2025-10-13 22:41:29,795 - INFO - _models.training_function_executor - Epoch 025/028 | train_loss=2.8332 | val_loss=1.5622 | val_acc=0.7188
2025-10-13 22:41:29,798 - INFO - _models.training_function_executor - Epoch 026/028 | train_loss=2.6085 | val_loss=1.5622 | val_acc=0.7188
2025-10-13 22:41:29,801 - INFO - _models.training_function_executor - Epoch 027/028 | train_loss=2.7633 | val_loss=1.5624 | val_acc=0.7188
2025-10-13 22:41:29,804 - INFO - _models.training_function_executor - Epoch 028/028 | train_loss=2.5713 | val_loss=1.5624 | val_acc=0.7188
2025-10-13 22:41:30,656 - INFO - _models.training_function_executor - Model: 5,683 parameters, 12.2KB storage
2025-10-13 22:41:30,656 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.647191047668457, 2.7629776000976562, 2.4986023902893066, 2.5895185470581055, 2.5877864360809326, 2.820033073425293, 2.549520254135132, 2.6889522075653076, 2.5710790157318115, 2.6872196197509766, 2.735191583633423, 2.3380370140075684, 2.28987455368042, 2.7924914360046387, 2.6566553115844727, 2.6384716033935547, 2.92488431930542, 2.8005993366241455, 2.823063373565674, 2.628373861312866, 2.5794694423675537, 2.6153910160064697, 2.477132797241211, 2.6692023277282715, 2.833217144012451, 2.6084542274475098, 2.763317584991455, 2.5713205337524414], 'val_losses': [1.57958984375, 1.57958984375, 1.57958984375, 1.57706880569458, 1.5749194622039795, 1.5734113454818726, 1.5724091529846191, 1.5702018737792969, 1.56882905960083, 1.5687228441238403, 1.5681288242340088, 1.5680270195007324, 1.5657771825790405, 1.5643746852874756, 1.5637152194976807, 1.5644978284835815, 1.5634709596633911, 1.5637397766113281, 1.5634207725524902, 1.5630016326904297, 1.5631872415542603, 1.5623724460601807, 1.562337040901184, 1.5622856616973877, 1.5622155666351318, 1.562207818031311, 1.5623615980148315, 1.5624157190322876], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.1275001669000045e-05, 'batch_size': 256, 'epochs': 28, 'hidden_size': 71, 'dropout': 0.14884664103561565, 'weight_decay': 0.006471720841701994, 'label_smoothing': 0.12768152403762514, 'grad_clip_norm': 4.056721638776345, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 5683, 'model_storage_size_kb': 12.2095703125, 'model_size_validation': 'PASS'}
2025-10-13 22:41:30,656 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:41:30,656 - INFO - _models.training_function_executor - Model: 5,683 parameters, 12.2KB (PASS 256KB limit)
2025-10-13 22:41:30,656 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.674s
2025-10-13 22:41:30,721 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:41:30,721 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.064s
2025-10-13 22:41:30,721 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 1.1275001669000045e-05, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'hidden_size': np.int64(71), 'dropout': 0.14884664103561565, 'weight_decay': 0.006471720841701994, 'label_smoothing': 0.12768152403762514, 'grad_clip_norm': 4.056721638776345, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:41:30,721 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 1.1275001669000045e-05, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'hidden_size': np.int64(71), 'dropout': 0.14884664103561565, 'weight_decay': 0.006471720841701994, 'label_smoothing': 0.12768152403762514, 'grad_clip_norm': 4.056721638776345, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:41:30,722 - INFO - bo.run_bo - üîçBO Trial 11: Using RF surrogate + Expected Improvement
2025-10-13 22:41:30,722 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:30,722 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 11 (NaN monitoring active)
2025-10-13 22:41:30,722 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:30,722 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:30,722 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.1030490773087426e-05, 'batch_size': 128, 'epochs': 114, 'hidden_size': 77, 'dropout': 0.34045991673767184, 'weight_decay': 2.7349891716209504e-06, 'label_smoothing': 0.04264628661878732, 'grad_clip_norm': 4.816841241704172, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:41:30,723 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.1030490773087426e-05, 'batch_size': 128, 'epochs': 114, 'hidden_size': 77, 'dropout': 0.34045991673767184, 'weight_decay': 2.7349891716209504e-06, 'label_smoothing': 0.04264628661878732, 'grad_clip_norm': 4.816841241704172, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:41:34,570 - INFO - _models.training_function_executor - Epoch 001/114 | train_loss=7.0315 | val_loss=3.2450 | val_acc=0.0938
2025-10-13 22:41:34,577 - INFO - _models.training_function_executor - Epoch 002/114 | train_loss=7.3841 | val_loss=3.2450 | val_acc=0.0938
2025-10-13 22:41:34,582 - INFO - _models.training_function_executor - Epoch 003/114 | train_loss=6.5791 | val_loss=3.1970 | val_acc=0.0938
2025-10-13 22:41:34,586 - INFO - _models.training_function_executor - Epoch 004/114 | train_loss=7.4114 | val_loss=3.1154 | val_acc=0.0938
2025-10-13 22:41:34,591 - INFO - _models.training_function_executor - Epoch 005/114 | train_loss=6.8993 | val_loss=3.0351 | val_acc=0.0938
2025-10-13 22:41:34,596 - INFO - _models.training_function_executor - Epoch 006/114 | train_loss=6.9304 | val_loss=2.9560 | val_acc=0.0938
2025-10-13 22:41:34,600 - INFO - _models.training_function_executor - Epoch 007/114 | train_loss=7.0194 | val_loss=2.8874 | val_acc=0.0781
2025-10-13 22:41:34,605 - INFO - _models.training_function_executor - Epoch 008/114 | train_loss=6.1161 | val_loss=2.8191 | val_acc=0.1250
2025-10-13 22:41:34,610 - INFO - _models.training_function_executor - Epoch 009/114 | train_loss=6.4184 | val_loss=2.7536 | val_acc=0.1562
2025-10-13 22:41:34,615 - INFO - _models.training_function_executor - Epoch 010/114 | train_loss=6.3472 | val_loss=2.6971 | val_acc=0.1562
2025-10-13 22:41:34,620 - INFO - _models.training_function_executor - Epoch 011/114 | train_loss=6.5720 | val_loss=2.6427 | val_acc=0.1562
2025-10-13 22:41:34,624 - INFO - _models.training_function_executor - Epoch 012/114 | train_loss=6.6834 | val_loss=2.5937 | val_acc=0.2031
2025-10-13 22:41:34,629 - INFO - _models.training_function_executor - Epoch 013/114 | train_loss=5.9927 | val_loss=2.5501 | val_acc=0.1875
2025-10-13 22:41:34,633 - INFO - _models.training_function_executor - Epoch 014/114 | train_loss=6.1180 | val_loss=2.5079 | val_acc=0.1875
2025-10-13 22:41:34,638 - INFO - _models.training_function_executor - Epoch 015/114 | train_loss=6.4613 | val_loss=2.4715 | val_acc=0.1875
2025-10-13 22:41:34,643 - INFO - _models.training_function_executor - Epoch 016/114 | train_loss=6.3627 | val_loss=2.4410 | val_acc=0.2031
2025-10-13 22:41:34,647 - INFO - _models.training_function_executor - Epoch 017/114 | train_loss=6.5334 | val_loss=2.4113 | val_acc=0.2031
2025-10-13 22:41:34,651 - INFO - _models.training_function_executor - Epoch 018/114 | train_loss=6.3600 | val_loss=2.3855 | val_acc=0.1875
2025-10-13 22:41:34,656 - INFO - _models.training_function_executor - Epoch 019/114 | train_loss=6.2444 | val_loss=2.3651 | val_acc=0.2031
2025-10-13 22:41:34,660 - INFO - _models.training_function_executor - Epoch 020/114 | train_loss=5.8980 | val_loss=2.3457 | val_acc=0.2344
2025-10-13 22:41:34,664 - INFO - _models.training_function_executor - Epoch 021/114 | train_loss=6.3036 | val_loss=2.3288 | val_acc=0.2500
2025-10-13 22:41:34,668 - INFO - _models.training_function_executor - Epoch 022/114 | train_loss=5.4575 | val_loss=2.3147 | val_acc=0.2656
2025-10-13 22:41:34,673 - INFO - _models.training_function_executor - Epoch 023/114 | train_loss=5.9117 | val_loss=2.3035 | val_acc=0.2812
2025-10-13 22:41:34,677 - INFO - _models.training_function_executor - Epoch 024/114 | train_loss=6.1163 | val_loss=2.2947 | val_acc=0.2812
2025-10-13 22:41:34,682 - INFO - _models.training_function_executor - Epoch 025/114 | train_loss=6.0746 | val_loss=2.2869 | val_acc=0.2969
2025-10-13 22:41:34,686 - INFO - _models.training_function_executor - Epoch 026/114 | train_loss=6.1854 | val_loss=2.2822 | val_acc=0.3125
2025-10-13 22:41:34,691 - INFO - _models.training_function_executor - Epoch 027/114 | train_loss=5.4315 | val_loss=2.2781 | val_acc=0.3125
2025-10-13 22:41:34,695 - INFO - _models.training_function_executor - Epoch 028/114 | train_loss=6.1845 | val_loss=2.2776 | val_acc=0.3125
2025-10-13 22:41:34,699 - INFO - _models.training_function_executor - Epoch 029/114 | train_loss=5.4418 | val_loss=2.2767 | val_acc=0.3281
2025-10-13 22:41:34,704 - INFO - _models.training_function_executor - Epoch 030/114 | train_loss=4.8863 | val_loss=2.2774 | val_acc=0.3281
2025-10-13 22:41:34,708 - INFO - _models.training_function_executor - Epoch 031/114 | train_loss=6.0409 | val_loss=2.2787 | val_acc=0.3281
2025-10-13 22:41:34,713 - INFO - _models.training_function_executor - Epoch 032/114 | train_loss=5.7211 | val_loss=2.2805 | val_acc=0.3438
2025-10-13 22:41:34,717 - INFO - _models.training_function_executor - Epoch 033/114 | train_loss=6.1401 | val_loss=2.2843 | val_acc=0.3438
2025-10-13 22:41:34,721 - INFO - _models.training_function_executor - Epoch 034/114 | train_loss=5.7820 | val_loss=2.2889 | val_acc=0.3438
2025-10-13 22:41:34,726 - INFO - _models.training_function_executor - Epoch 035/114 | train_loss=5.4519 | val_loss=2.2951 | val_acc=0.3750
2025-10-13 22:41:34,730 - INFO - _models.training_function_executor - Epoch 036/114 | train_loss=5.5582 | val_loss=2.3005 | val_acc=0.3750
2025-10-13 22:41:34,734 - INFO - _models.training_function_executor - Epoch 037/114 | train_loss=6.0183 | val_loss=2.3062 | val_acc=0.3750
2025-10-13 22:41:34,738 - INFO - _models.training_function_executor - Epoch 038/114 | train_loss=4.8146 | val_loss=2.3132 | val_acc=0.3906
2025-10-13 22:41:34,743 - INFO - _models.training_function_executor - Epoch 039/114 | train_loss=5.2377 | val_loss=2.3195 | val_acc=0.3906
2025-10-13 22:41:34,747 - INFO - _models.training_function_executor - Epoch 040/114 | train_loss=4.9320 | val_loss=2.3264 | val_acc=0.4219
2025-10-13 22:41:34,751 - INFO - _models.training_function_executor - Epoch 041/114 | train_loss=5.2735 | val_loss=2.3325 | val_acc=0.4531
2025-10-13 22:41:34,756 - INFO - _models.training_function_executor - Epoch 042/114 | train_loss=4.9521 | val_loss=2.3391 | val_acc=0.4844
2025-10-13 22:41:34,760 - INFO - _models.training_function_executor - Epoch 043/114 | train_loss=4.7092 | val_loss=2.3451 | val_acc=0.4844
2025-10-13 22:41:34,764 - INFO - _models.training_function_executor - Epoch 044/114 | train_loss=4.7506 | val_loss=2.3524 | val_acc=0.4844
2025-10-13 22:41:34,768 - INFO - _models.training_function_executor - Epoch 045/114 | train_loss=5.1547 | val_loss=2.3589 | val_acc=0.4844
2025-10-13 22:41:34,772 - INFO - _models.training_function_executor - Epoch 046/114 | train_loss=4.7437 | val_loss=2.3669 | val_acc=0.4844
2025-10-13 22:41:34,777 - INFO - _models.training_function_executor - Epoch 047/114 | train_loss=5.2626 | val_loss=2.3735 | val_acc=0.4844
2025-10-13 22:41:34,781 - INFO - _models.training_function_executor - Epoch 048/114 | train_loss=5.5191 | val_loss=2.3809 | val_acc=0.4844
2025-10-13 22:41:34,786 - INFO - _models.training_function_executor - Epoch 049/114 | train_loss=4.7793 | val_loss=2.3885 | val_acc=0.4844
2025-10-13 22:41:34,790 - INFO - _models.training_function_executor - Epoch 050/114 | train_loss=5.1736 | val_loss=2.3965 | val_acc=0.5000
2025-10-13 22:41:34,794 - INFO - _models.training_function_executor - Epoch 051/114 | train_loss=5.4450 | val_loss=2.4040 | val_acc=0.5000
2025-10-13 22:41:34,798 - INFO - _models.training_function_executor - Epoch 052/114 | train_loss=5.0825 | val_loss=2.4086 | val_acc=0.5000
2025-10-13 22:41:34,802 - INFO - _models.training_function_executor - Epoch 053/114 | train_loss=4.8836 | val_loss=2.4159 | val_acc=0.5000
2025-10-13 22:41:34,807 - INFO - _models.training_function_executor - Epoch 054/114 | train_loss=4.8163 | val_loss=2.4211 | val_acc=0.5000
2025-10-13 22:41:34,811 - INFO - _models.training_function_executor - Epoch 055/114 | train_loss=4.7188 | val_loss=2.4279 | val_acc=0.5000
2025-10-13 22:41:34,815 - INFO - _models.training_function_executor - Epoch 056/114 | train_loss=5.5966 | val_loss=2.4361 | val_acc=0.5000
2025-10-13 22:41:34,819 - INFO - _models.training_function_executor - Epoch 057/114 | train_loss=5.0618 | val_loss=2.4426 | val_acc=0.5000
2025-10-13 22:41:34,823 - INFO - _models.training_function_executor - Epoch 058/114 | train_loss=5.0733 | val_loss=2.4507 | val_acc=0.5000
2025-10-13 22:41:34,828 - INFO - _models.training_function_executor - Epoch 059/114 | train_loss=5.3174 | val_loss=2.4572 | val_acc=0.5000
2025-10-13 22:41:34,832 - INFO - _models.training_function_executor - Epoch 060/114 | train_loss=5.6363 | val_loss=2.4649 | val_acc=0.5000
2025-10-13 22:41:34,836 - INFO - _models.training_function_executor - Epoch 061/114 | train_loss=4.7417 | val_loss=2.4724 | val_acc=0.5156
2025-10-13 22:41:34,840 - INFO - _models.training_function_executor - Epoch 062/114 | train_loss=4.7695 | val_loss=2.4781 | val_acc=0.5312
2025-10-13 22:41:34,844 - INFO - _models.training_function_executor - Epoch 063/114 | train_loss=4.8959 | val_loss=2.4842 | val_acc=0.5312
2025-10-13 22:41:34,849 - INFO - _models.training_function_executor - Epoch 064/114 | train_loss=5.0619 | val_loss=2.4898 | val_acc=0.5312
2025-10-13 22:41:34,853 - INFO - _models.training_function_executor - Epoch 065/114 | train_loss=5.4863 | val_loss=2.4963 | val_acc=0.5469
2025-10-13 22:41:34,857 - INFO - _models.training_function_executor - Epoch 066/114 | train_loss=4.8854 | val_loss=2.5007 | val_acc=0.5469
2025-10-13 22:41:34,862 - INFO - _models.training_function_executor - Epoch 067/114 | train_loss=5.0378 | val_loss=2.5055 | val_acc=0.5469
2025-10-13 22:41:34,866 - INFO - _models.training_function_executor - Epoch 068/114 | train_loss=5.0551 | val_loss=2.5095 | val_acc=0.5469
2025-10-13 22:41:34,870 - INFO - _models.training_function_executor - Epoch 069/114 | train_loss=5.0968 | val_loss=2.5135 | val_acc=0.5469
2025-10-13 22:41:34,875 - INFO - _models.training_function_executor - Epoch 070/114 | train_loss=4.6519 | val_loss=2.5179 | val_acc=0.5469
2025-10-13 22:41:34,879 - INFO - _models.training_function_executor - Epoch 071/114 | train_loss=5.2642 | val_loss=2.5212 | val_acc=0.5469
2025-10-13 22:41:34,883 - INFO - _models.training_function_executor - Epoch 072/114 | train_loss=4.8317 | val_loss=2.5256 | val_acc=0.5469
2025-10-13 22:41:34,888 - INFO - _models.training_function_executor - Epoch 073/114 | train_loss=4.7795 | val_loss=2.5283 | val_acc=0.5469
2025-10-13 22:41:34,892 - INFO - _models.training_function_executor - Epoch 074/114 | train_loss=5.2065 | val_loss=2.5317 | val_acc=0.5469
2025-10-13 22:41:34,896 - INFO - _models.training_function_executor - Epoch 075/114 | train_loss=4.3244 | val_loss=2.5337 | val_acc=0.5469
2025-10-13 22:41:34,900 - INFO - _models.training_function_executor - Epoch 076/114 | train_loss=4.6865 | val_loss=2.5369 | val_acc=0.5625
2025-10-13 22:41:34,904 - INFO - _models.training_function_executor - Epoch 077/114 | train_loss=4.2728 | val_loss=2.5385 | val_acc=0.5625
2025-10-13 22:41:34,909 - INFO - _models.training_function_executor - Epoch 078/114 | train_loss=5.3513 | val_loss=2.5400 | val_acc=0.5625
2025-10-13 22:41:34,913 - INFO - _models.training_function_executor - Epoch 079/114 | train_loss=5.4936 | val_loss=2.5423 | val_acc=0.5625
2025-10-13 22:41:34,917 - INFO - _models.training_function_executor - Epoch 080/114 | train_loss=4.8777 | val_loss=2.5435 | val_acc=0.5625
2025-10-13 22:41:34,921 - INFO - _models.training_function_executor - Epoch 081/114 | train_loss=5.0033 | val_loss=2.5437 | val_acc=0.5625
2025-10-13 22:41:34,926 - INFO - _models.training_function_executor - Epoch 082/114 | train_loss=5.0516 | val_loss=2.5444 | val_acc=0.5625
2025-10-13 22:41:34,930 - INFO - _models.training_function_executor - Epoch 083/114 | train_loss=5.1157 | val_loss=2.5458 | val_acc=0.5625
2025-10-13 22:41:34,934 - INFO - _models.training_function_executor - Epoch 084/114 | train_loss=5.0027 | val_loss=2.5463 | val_acc=0.5625
2025-10-13 22:41:34,938 - INFO - _models.training_function_executor - Epoch 085/114 | train_loss=4.7577 | val_loss=2.5466 | val_acc=0.5625
2025-10-13 22:41:34,942 - INFO - _models.training_function_executor - Epoch 086/114 | train_loss=4.6657 | val_loss=2.5480 | val_acc=0.5625
2025-10-13 22:41:34,947 - INFO - _models.training_function_executor - Epoch 087/114 | train_loss=5.1212 | val_loss=2.5496 | val_acc=0.5625
2025-10-13 22:41:34,951 - INFO - _models.training_function_executor - Epoch 088/114 | train_loss=4.6239 | val_loss=2.5506 | val_acc=0.5625
2025-10-13 22:41:34,955 - INFO - _models.training_function_executor - Epoch 089/114 | train_loss=4.3557 | val_loss=2.5514 | val_acc=0.5625
2025-10-13 22:41:34,959 - INFO - _models.training_function_executor - Epoch 090/114 | train_loss=5.4028 | val_loss=2.5521 | val_acc=0.5625
2025-10-13 22:41:34,964 - INFO - _models.training_function_executor - Epoch 091/114 | train_loss=4.5808 | val_loss=2.5528 | val_acc=0.5625
2025-10-13 22:41:34,968 - INFO - _models.training_function_executor - Epoch 092/114 | train_loss=5.1340 | val_loss=2.5537 | val_acc=0.5625
2025-10-13 22:41:34,972 - INFO - _models.training_function_executor - Epoch 093/114 | train_loss=4.5410 | val_loss=2.5543 | val_acc=0.5625
2025-10-13 22:41:34,977 - INFO - _models.training_function_executor - Epoch 094/114 | train_loss=5.1378 | val_loss=2.5550 | val_acc=0.5625
2025-10-13 22:41:34,981 - INFO - _models.training_function_executor - Epoch 095/114 | train_loss=4.9369 | val_loss=2.5555 | val_acc=0.5625
2025-10-13 22:41:34,985 - INFO - _models.training_function_executor - Epoch 096/114 | train_loss=5.0415 | val_loss=2.5565 | val_acc=0.5625
2025-10-13 22:41:34,988 - INFO - _models.training_function_executor - Epoch 097/114 | train_loss=4.5518 | val_loss=2.5569 | val_acc=0.5625
2025-10-13 22:41:34,993 - INFO - _models.training_function_executor - Epoch 098/114 | train_loss=5.2524 | val_loss=2.5573 | val_acc=0.5625
2025-10-13 22:41:34,997 - INFO - _models.training_function_executor - Epoch 099/114 | train_loss=4.4881 | val_loss=2.5576 | val_acc=0.5625
2025-10-13 22:41:35,000 - INFO - _models.training_function_executor - Epoch 100/114 | train_loss=5.0351 | val_loss=2.5579 | val_acc=0.5625
2025-10-13 22:41:35,004 - INFO - _models.training_function_executor - Epoch 101/114 | train_loss=4.9904 | val_loss=2.5585 | val_acc=0.5625
2025-10-13 22:41:35,009 - INFO - _models.training_function_executor - Epoch 102/114 | train_loss=4.9125 | val_loss=2.5585 | val_acc=0.5625
2025-10-13 22:41:35,013 - INFO - _models.training_function_executor - Epoch 103/114 | train_loss=4.4886 | val_loss=2.5592 | val_acc=0.5625
2025-10-13 22:41:35,017 - INFO - _models.training_function_executor - Epoch 104/114 | train_loss=5.1592 | val_loss=2.5590 | val_acc=0.5625
2025-10-13 22:41:35,021 - INFO - _models.training_function_executor - Epoch 105/114 | train_loss=5.9650 | val_loss=2.5589 | val_acc=0.5625
2025-10-13 22:41:35,025 - INFO - _models.training_function_executor - Epoch 106/114 | train_loss=5.0979 | val_loss=2.5589 | val_acc=0.5625
2025-10-13 22:41:35,030 - INFO - _models.training_function_executor - Epoch 107/114 | train_loss=4.7537 | val_loss=2.5588 | val_acc=0.5625
2025-10-13 22:41:35,034 - INFO - _models.training_function_executor - Epoch 108/114 | train_loss=4.7185 | val_loss=2.5589 | val_acc=0.5625
2025-10-13 22:41:35,038 - INFO - _models.training_function_executor - Epoch 109/114 | train_loss=4.9610 | val_loss=2.5590 | val_acc=0.5625
2025-10-13 22:41:35,042 - INFO - _models.training_function_executor - Epoch 110/114 | train_loss=4.5216 | val_loss=2.5589 | val_acc=0.5625
2025-10-13 22:41:35,046 - INFO - _models.training_function_executor - Epoch 111/114 | train_loss=4.7974 | val_loss=2.5589 | val_acc=0.5625
2025-10-13 22:41:35,050 - INFO - _models.training_function_executor - Epoch 112/114 | train_loss=4.6963 | val_loss=2.5590 | val_acc=0.5625
2025-10-13 22:41:35,055 - INFO - _models.training_function_executor - Epoch 113/114 | train_loss=5.2993 | val_loss=2.5590 | val_acc=0.5625
2025-10-13 22:41:35,059 - INFO - _models.training_function_executor - Epoch 114/114 | train_loss=4.5447 | val_loss=2.5590 | val_acc=0.5625
2025-10-13 22:41:35,958 - INFO - _models.training_function_executor - Model: 6,625 parameters, 28.5KB storage
2025-10-13 22:41:35,958 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [7.031470060348511, 7.384092330932617, 6.5791027545928955, 7.411431312561035, 6.899258613586426, 6.930351734161377, 7.019370079040527, 6.116098880767822, 6.418389081954956, 6.3472442626953125, 6.572005987167358, 6.683387041091919, 5.9926793575286865, 6.118028402328491, 6.461251258850098, 6.362691879272461, 6.533429145812988, 6.360025405883789, 6.244359254837036, 5.897984981536865, 6.303569316864014, 5.457504034042358, 5.911679983139038, 6.116260766983032, 6.0745627880096436, 6.1854164600372314, 5.43146014213562, 6.184477806091309, 5.441791296005249, 4.886288642883301, 6.0408775806427, 5.721142292022705, 6.140136241912842, 5.782017230987549, 5.451938629150391, 5.5582053661346436, 6.018306493759155, 4.81463098526001, 5.23766565322876, 4.932039976119995, 5.2735326290130615, 4.9520909786224365, 4.709211349487305, 4.750625371932983, 5.154709815979004, 4.743698358535767, 5.2625648975372314, 5.519060373306274, 4.779253959655762, 5.173590660095215, 5.444979190826416, 5.082485675811768, 4.883589506149292, 4.816318035125732, 4.718817710876465, 5.596568822860718, 5.061833620071411, 5.073282718658447, 5.317445755004883, 5.636339902877808, 4.741687059402466, 4.769486665725708, 4.895910263061523, 5.061888694763184, 5.48627495765686, 4.8853607177734375, 5.037816762924194, 5.055068254470825, 5.096813678741455, 4.65191650390625, 5.2642271518707275, 4.831748008728027, 4.779454469680786, 5.206548690795898, 4.324421763420105, 4.686450481414795, 4.272844552993774, 5.351302623748779, 5.493602514266968, 4.877698183059692, 5.0033278465271, 5.051632881164551, 5.115706443786621, 5.002664566040039, 4.7577431201934814, 4.665717601776123, 5.121169805526733, 4.623904705047607, 4.3557164669036865, 5.402804851531982, 4.580783128738403, 5.134039640426636, 4.541027784347534, 5.137828826904297, 4.93693995475769, 5.041532039642334, 4.551775693893433, 5.252374172210693, 4.4881064891815186, 5.035138845443726, 4.990447998046875, 4.912547826766968, 4.488633871078491, 5.159239768981934, 5.965019941329956, 5.0979297161102295, 4.753685235977173, 4.718508720397949, 4.9609668254852295, 4.521611928939819, 4.797417163848877, 4.696284532546997, 5.29930305480957, 4.5446672439575195], 'val_losses': [3.2449536323547363, 3.2449536323547363, 3.197045087814331, 3.1153793334960938, 3.035078525543213, 2.955970048904419, 2.887406349182129, 2.8191306591033936, 2.7536087036132812, 2.6970620155334473, 2.642705202102661, 2.593735694885254, 2.550058364868164, 2.5078582763671875, 2.4715211391448975, 2.441033124923706, 2.4113385677337646, 2.385528087615967, 2.3651137351989746, 2.3456554412841797, 2.3288135528564453, 2.3146989345550537, 2.3035173416137695, 2.2947232723236084, 2.2869067192077637, 2.282191276550293, 2.2780580520629883, 2.27756929397583, 2.276714324951172, 2.2774341106414795, 2.2787399291992188, 2.280526876449585, 2.2843387126922607, 2.2889060974121094, 2.295050621032715, 2.300508499145508, 2.306201457977295, 2.3131561279296875, 2.319514036178589, 2.3263516426086426, 2.3324670791625977, 2.3391246795654297, 2.3450920581817627, 2.352358341217041, 2.358919382095337, 2.36690354347229, 2.3735148906707764, 2.380911350250244, 2.3885252475738525, 2.3964521884918213, 2.4040162563323975, 2.4086337089538574, 2.415881633758545, 2.421076536178589, 2.427931547164917, 2.436055898666382, 2.4426190853118896, 2.4507107734680176, 2.4572136402130127, 2.464925765991211, 2.4724361896514893, 2.478147506713867, 2.484172821044922, 2.4898343086242676, 2.4963221549987793, 2.5006661415100098, 2.5054612159729004, 2.509467363357544, 2.5134952068328857, 2.5179336071014404, 2.5211715698242188, 2.525606155395508, 2.5282959938049316, 2.5316827297210693, 2.533693790435791, 2.536893844604492, 2.5384600162506104, 2.5399789810180664, 2.5422985553741455, 2.5434913635253906, 2.5436782836914062, 2.544365167617798, 2.54575514793396, 2.5463483333587646, 2.5466296672821045, 2.5479657649993896, 2.549604892730713, 2.5506045818328857, 2.551422119140625, 2.55208158493042, 2.5528018474578857, 2.553675413131714, 2.5543386936187744, 2.554950475692749, 2.5555171966552734, 2.5565035343170166, 2.556936740875244, 2.557349920272827, 2.5575625896453857, 2.557918071746826, 2.55845046043396, 2.558464288711548, 2.559199810028076, 2.5589656829833984, 2.5589332580566406, 2.5589382648468018, 2.558835983276367, 2.5588841438293457, 2.559023857116699, 2.558872938156128, 2.5588772296905518, 2.558988094329834, 2.558988094329834, 2.558988094329834], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.078125, 0.125, 0.15625, 0.15625, 0.15625, 0.203125, 0.1875, 0.1875, 0.1875, 0.203125, 0.203125, 0.1875, 0.203125, 0.234375, 0.25, 0.265625, 0.28125, 0.28125, 0.296875, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.328125, 0.34375, 0.34375, 0.34375, 0.375, 0.375, 0.375, 0.390625, 0.390625, 0.421875, 0.453125, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.515625, 0.53125, 0.53125, 0.53125, 0.546875, 0.546875, 0.546875, 0.546875, 0.546875, 0.546875, 0.546875, 0.546875, 0.546875, 0.546875, 0.546875, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.1030490773087426e-05, 'batch_size': 128, 'epochs': 114, 'hidden_size': 77, 'dropout': 0.34045991673767184, 'weight_decay': 2.7349891716209504e-06, 'label_smoothing': 0.04264628661878732, 'grad_clip_norm': 4.816841241704172, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 6625, 'model_storage_size_kb': 28.466796875000004, 'model_size_validation': 'PASS'}
2025-10-13 22:41:35,958 - INFO - _models.training_function_executor - BO Objective: base=0.5625, size_penalty=0.0000, final=0.5625
2025-10-13 22:41:35,958 - INFO - _models.training_function_executor - Model: 6,625 parameters, 28.5KB (PASS 256KB limit)
2025-10-13 22:41:35,958 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.236s
2025-10-13 22:41:36,026 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5625
2025-10-13 22:41:36,026 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.068s
2025-10-13 22:41:36,026 - INFO - bo.run_bo - Recorded observation #11: hparams={'lr': 2.1030490773087426e-05, 'batch_size': np.int64(128), 'epochs': np.int64(114), 'hidden_size': np.int64(77), 'dropout': 0.34045991673767184, 'weight_decay': 2.7349891716209504e-06, 'label_smoothing': 0.04264628661878732, 'grad_clip_norm': 4.816841241704172, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.5625
2025-10-13 22:41:36,026 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 11: {'lr': 2.1030490773087426e-05, 'batch_size': np.int64(128), 'epochs': np.int64(114), 'hidden_size': np.int64(77), 'dropout': 0.34045991673767184, 'weight_decay': 2.7349891716209504e-06, 'label_smoothing': 0.04264628661878732, 'grad_clip_norm': 4.816841241704172, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.5625
2025-10-13 22:41:36,026 - INFO - bo.run_bo - üîçBO Trial 12: Using RF surrogate + Expected Improvement
2025-10-13 22:41:36,026 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:36,026 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 12 (NaN monitoring active)
2025-10-13 22:41:36,027 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:36,027 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:36,027 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002460293818043768, 'batch_size': 256, 'epochs': 21, 'hidden_size': 73, 'dropout': 0.20346080722090681, 'weight_decay': 0.00907617255457941, 'label_smoothing': 0.06760282288144302, 'grad_clip_norm': 3.7222054116398646, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:41:36,027 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002460293818043768, 'batch_size': 256, 'epochs': 21, 'hidden_size': 73, 'dropout': 0.20346080722090681, 'weight_decay': 0.00907617255457941, 'label_smoothing': 0.06760282288144302, 'grad_clip_norm': 3.7222054116398646, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:41:39,837 - INFO - _models.training_function_executor - Epoch 001/021 | train_loss=14.2363 | val_loss=12.4324 | val_acc=0.0938
2025-10-13 22:41:39,843 - INFO - _models.training_function_executor - Epoch 002/021 | train_loss=15.1892 | val_loss=12.4324 | val_acc=0.0938
2025-10-13 22:41:39,847 - INFO - _models.training_function_executor - Epoch 003/021 | train_loss=14.4812 | val_loss=12.4324 | val_acc=0.0938
2025-10-13 22:41:39,851 - INFO - _models.training_function_executor - Epoch 004/021 | train_loss=14.2951 | val_loss=12.4324 | val_acc=0.0938
2025-10-13 22:41:39,854 - INFO - _models.training_function_executor - Epoch 005/021 | train_loss=14.2573 | val_loss=12.4324 | val_acc=0.0938
2025-10-13 22:41:39,858 - INFO - _models.training_function_executor - Epoch 006/021 | train_loss=14.4644 | val_loss=6.6255 | val_acc=0.0938
2025-10-13 22:41:39,861 - INFO - _models.training_function_executor - Epoch 007/021 | train_loss=8.0553 | val_loss=2.5039 | val_acc=0.2344
2025-10-13 22:41:39,864 - INFO - _models.training_function_executor - Epoch 008/021 | train_loss=4.9811 | val_loss=2.2267 | val_acc=0.5625
2025-10-13 22:41:39,868 - INFO - _models.training_function_executor - Epoch 009/021 | train_loss=3.7826 | val_loss=3.0632 | val_acc=0.7188
2025-10-13 22:41:39,871 - INFO - _models.training_function_executor - Epoch 010/021 | train_loss=4.1758 | val_loss=3.2135 | val_acc=0.7188
2025-10-13 22:41:39,874 - INFO - _models.training_function_executor - Epoch 011/021 | train_loss=4.0783 | val_loss=2.9927 | val_acc=0.7188
2025-10-13 22:41:39,878 - INFO - _models.training_function_executor - Epoch 012/021 | train_loss=3.6570 | val_loss=2.6537 | val_acc=0.6719
2025-10-13 22:41:39,881 - INFO - _models.training_function_executor - Epoch 013/021 | train_loss=4.1258 | val_loss=2.3509 | val_acc=0.5781
2025-10-13 22:41:39,884 - INFO - _models.training_function_executor - Epoch 014/021 | train_loss=3.2991 | val_loss=2.1374 | val_acc=0.5000
2025-10-13 22:41:39,887 - INFO - _models.training_function_executor - Epoch 015/021 | train_loss=3.4896 | val_loss=2.0160 | val_acc=0.4531
2025-10-13 22:41:39,890 - INFO - _models.training_function_executor - Epoch 016/021 | train_loss=3.4158 | val_loss=1.9328 | val_acc=0.4375
2025-10-13 22:41:39,894 - INFO - _models.training_function_executor - Epoch 017/021 | train_loss=3.5102 | val_loss=1.8698 | val_acc=0.4531
2025-10-13 22:41:39,897 - INFO - _models.training_function_executor - Epoch 018/021 | train_loss=2.9128 | val_loss=1.8239 | val_acc=0.4531
2025-10-13 22:41:39,901 - INFO - _models.training_function_executor - Epoch 019/021 | train_loss=2.9764 | val_loss=1.7957 | val_acc=0.4688
2025-10-13 22:41:39,904 - INFO - _models.training_function_executor - Epoch 020/021 | train_loss=3.3228 | val_loss=1.7816 | val_acc=0.4531
2025-10-13 22:41:39,907 - INFO - _models.training_function_executor - Epoch 021/021 | train_loss=2.9499 | val_loss=1.7782 | val_acc=0.4531
2025-10-13 22:41:40,788 - INFO - _models.training_function_executor - Model: 5,989 parameters, 25.7KB storage
2025-10-13 22:41:40,788 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [14.236345291137695, 15.18921947479248, 14.481213569641113, 14.295055389404297, 14.257328987121582, 14.464350700378418, 8.05531120300293, 4.9810614585876465, 3.782649517059326, 4.175767421722412, 4.07828426361084, 3.6569900512695312, 4.1257643699646, 3.299145460128784, 3.489626169204712, 3.415841817855835, 3.5102226734161377, 2.912782907485962, 2.9764397144317627, 3.322824478149414, 2.949939489364624], 'val_losses': [12.432357788085938, 12.432357788085938, 12.432357788085938, 12.432357788085938, 12.432357788085938, 6.625499725341797, 2.50386905670166, 2.226684093475342, 3.063201427459717, 3.213458776473999, 2.992738962173462, 2.653709650039673, 2.3509366512298584, 2.1374406814575195, 2.0159859657287598, 1.9328402280807495, 1.8698451519012451, 1.8238639831542969, 1.7957422733306885, 1.7816065549850464, 1.7782156467437744], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.234375, 0.5625, 0.71875, 0.71875, 0.71875, 0.671875, 0.578125, 0.5, 0.453125, 0.4375, 0.453125, 0.453125, 0.46875, 0.453125, 0.453125], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002460293818043768, 'batch_size': 256, 'epochs': 21, 'hidden_size': 73, 'dropout': 0.20346080722090681, 'weight_decay': 0.00907617255457941, 'label_smoothing': 0.06760282288144302, 'grad_clip_norm': 3.7222054116398646, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 5989, 'model_storage_size_kb': 25.733984375000002, 'model_size_validation': 'PASS'}
2025-10-13 22:41:40,788 - INFO - _models.training_function_executor - BO Objective: base=0.4531, size_penalty=0.0000, final=0.4531
2025-10-13 22:41:40,788 - INFO - _models.training_function_executor - Model: 5,989 parameters, 25.7KB (PASS 256KB limit)
2025-10-13 22:41:40,788 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.762s
2025-10-13 22:41:40,857 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.4531
2025-10-13 22:41:40,857 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.068s
2025-10-13 22:41:40,857 - INFO - bo.run_bo - Recorded observation #12: hparams={'lr': 0.002460293818043768, 'batch_size': np.int64(256), 'epochs': np.int64(21), 'hidden_size': np.int64(73), 'dropout': 0.20346080722090681, 'weight_decay': 0.00907617255457941, 'label_smoothing': 0.06760282288144302, 'grad_clip_norm': 3.7222054116398646, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.4531
2025-10-13 22:41:40,857 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 12: {'lr': 0.002460293818043768, 'batch_size': np.int64(256), 'epochs': np.int64(21), 'hidden_size': np.int64(73), 'dropout': 0.20346080722090681, 'weight_decay': 0.00907617255457941, 'label_smoothing': 0.06760282288144302, 'grad_clip_norm': 3.7222054116398646, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.4531
2025-10-13 22:41:40,857 - INFO - bo.run_bo - üîçBO Trial 13: Using RF surrogate + Expected Improvement
2025-10-13 22:41:40,857 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:40,857 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 13 (NaN monitoring active)
2025-10-13 22:41:40,857 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:40,857 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:40,857 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.2185196759042526e-05, 'batch_size': 64, 'epochs': 21, 'hidden_size': 105, 'dropout': 0.1003570220412032, 'weight_decay': 9.165785890989771e-05, 'label_smoothing': 0.036869646876711376, 'grad_clip_norm': 2.5219034923908294, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:40,858 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.2185196759042526e-05, 'batch_size': 64, 'epochs': 21, 'hidden_size': 105, 'dropout': 0.1003570220412032, 'weight_decay': 9.165785890989771e-05, 'label_smoothing': 0.036869646876711376, 'grad_clip_norm': 2.5219034923908294, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:44,874 - INFO - _models.training_function_executor - Epoch 001/021 | train_loss=9.9624 | val_loss=9.1639 | val_acc=0.1875
2025-10-13 22:41:44,884 - INFO - _models.training_function_executor - Epoch 002/021 | train_loss=9.6427 | val_loss=8.8834 | val_acc=0.1875
2025-10-13 22:41:44,891 - INFO - _models.training_function_executor - Epoch 003/021 | train_loss=9.0313 | val_loss=8.3453 | val_acc=0.1875
2025-10-13 22:41:44,898 - INFO - _models.training_function_executor - Epoch 004/021 | train_loss=8.5618 | val_loss=7.8140 | val_acc=0.1875
2025-10-13 22:41:44,905 - INFO - _models.training_function_executor - Epoch 005/021 | train_loss=8.3306 | val_loss=7.3125 | val_acc=0.1875
2025-10-13 22:41:44,911 - INFO - _models.training_function_executor - Epoch 006/021 | train_loss=7.8692 | val_loss=6.8422 | val_acc=0.1875
2025-10-13 22:41:44,918 - INFO - _models.training_function_executor - Epoch 007/021 | train_loss=7.3414 | val_loss=6.3892 | val_acc=0.1875
2025-10-13 22:41:44,925 - INFO - _models.training_function_executor - Epoch 008/021 | train_loss=7.3003 | val_loss=5.9808 | val_acc=0.1875
2025-10-13 22:41:44,932 - INFO - _models.training_function_executor - Epoch 009/021 | train_loss=6.3094 | val_loss=5.6092 | val_acc=0.1875
2025-10-13 22:41:44,938 - INFO - _models.training_function_executor - Epoch 010/021 | train_loss=5.8174 | val_loss=5.2866 | val_acc=0.1875
2025-10-13 22:41:44,945 - INFO - _models.training_function_executor - Epoch 011/021 | train_loss=6.1629 | val_loss=5.0044 | val_acc=0.1875
2025-10-13 22:41:44,953 - INFO - _models.training_function_executor - Epoch 012/021 | train_loss=5.3285 | val_loss=4.7543 | val_acc=0.1875
2025-10-13 22:41:44,960 - INFO - _models.training_function_executor - Epoch 013/021 | train_loss=5.3907 | val_loss=4.5484 | val_acc=0.1875
2025-10-13 22:41:44,967 - INFO - _models.training_function_executor - Epoch 014/021 | train_loss=5.2252 | val_loss=4.3855 | val_acc=0.1875
2025-10-13 22:41:44,974 - INFO - _models.training_function_executor - Epoch 015/021 | train_loss=4.8716 | val_loss=4.2570 | val_acc=0.1875
2025-10-13 22:41:44,981 - INFO - _models.training_function_executor - Epoch 016/021 | train_loss=4.8764 | val_loss=4.1564 | val_acc=0.1875
2025-10-13 22:41:44,987 - INFO - _models.training_function_executor - Epoch 017/021 | train_loss=4.8703 | val_loss=4.0873 | val_acc=0.1875
2025-10-13 22:41:44,993 - INFO - _models.training_function_executor - Epoch 018/021 | train_loss=4.7296 | val_loss=4.0445 | val_acc=0.1875
2025-10-13 22:41:44,999 - INFO - _models.training_function_executor - Epoch 019/021 | train_loss=4.9475 | val_loss=4.0215 | val_acc=0.1875
2025-10-13 22:41:45,005 - INFO - _models.training_function_executor - Epoch 020/021 | train_loss=4.5022 | val_loss=4.0101 | val_acc=0.1875
2025-10-13 22:41:45,011 - INFO - _models.training_function_executor - Epoch 021/021 | train_loss=4.7636 | val_loss=4.0073 | val_acc=0.1875
2025-10-13 22:41:45,907 - INFO - _models.training_function_executor - Model: 11,973 parameters, 25.7KB storage
2025-10-13 22:41:45,907 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [9.962380170822144, 9.642741918563843, 9.031298398971558, 8.561826586723328, 8.33064317703247, 7.869231224060059, 7.341415286064148, 7.3003339767456055, 6.309380650520325, 5.817434549331665, 6.162906527519226, 5.328543543815613, 5.390717029571533, 5.225209355354309, 4.871587455272675, 4.876403093338013, 4.870344519615173, 4.729639887809753, 4.9475181102752686, 4.5021693110466, 4.7636436223983765], 'val_losses': [9.163944244384766, 8.883442878723145, 8.345281600952148, 7.8139753341674805, 7.312543869018555, 6.842189788818359, 6.389199256896973, 5.98078727722168, 5.6092209815979, 5.286571502685547, 5.004430294036865, 4.7542948722839355, 4.548447132110596, 4.38546895980835, 4.2569966316223145, 4.1564202308654785, 4.0872650146484375, 4.044544696807861, 4.021496295928955, 4.01007080078125, 4.007312774658203], 'val_acc': [0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.2185196759042526e-05, 'batch_size': 64, 'epochs': 21, 'hidden_size': 105, 'dropout': 0.1003570220412032, 'weight_decay': 9.165785890989771e-05, 'label_smoothing': 0.036869646876711376, 'grad_clip_norm': 2.5219034923908294, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 11973, 'model_storage_size_kb': 25.723242187500002, 'model_size_validation': 'PASS'}
2025-10-13 22:41:45,907 - INFO - _models.training_function_executor - BO Objective: base=0.1875, size_penalty=0.0000, final=0.1875
2025-10-13 22:41:45,907 - INFO - _models.training_function_executor - Model: 11,973 parameters, 25.7KB (PASS 256KB limit)
2025-10-13 22:41:45,907 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.050s
2025-10-13 22:41:45,976 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1875
2025-10-13 22:41:45,976 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.068s
2025-10-13 22:41:45,976 - INFO - bo.run_bo - Recorded observation #13: hparams={'lr': 2.2185196759042526e-05, 'batch_size': np.int64(64), 'epochs': np.int64(21), 'hidden_size': np.int64(105), 'dropout': 0.1003570220412032, 'weight_decay': 9.165785890989771e-05, 'label_smoothing': 0.036869646876711376, 'grad_clip_norm': 2.5219034923908294, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.1875
2025-10-13 22:41:45,976 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 13: {'lr': 2.2185196759042526e-05, 'batch_size': np.int64(64), 'epochs': np.int64(21), 'hidden_size': np.int64(105), 'dropout': 0.1003570220412032, 'weight_decay': 9.165785890989771e-05, 'label_smoothing': 0.036869646876711376, 'grad_clip_norm': 2.5219034923908294, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.1875
2025-10-13 22:41:45,976 - INFO - bo.run_bo - üîçBO Trial 14: Using RF surrogate + Expected Improvement
2025-10-13 22:41:45,976 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:45,976 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 14 (NaN monitoring active)
2025-10-13 22:41:45,976 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:45,976 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:45,976 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.2875053928420108e-05, 'batch_size': 64, 'epochs': 53, 'hidden_size': 170, 'dropout': 0.009034464839901392, 'weight_decay': 5.432564073497845e-05, 'label_smoothing': 0.13973306860919554, 'grad_clip_norm': 3.7216447812656637, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:45,977 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.2875053928420108e-05, 'batch_size': 64, 'epochs': 53, 'hidden_size': 170, 'dropout': 0.009034464839901392, 'weight_decay': 5.432564073497845e-05, 'label_smoothing': 0.13973306860919554, 'grad_clip_norm': 3.7216447812656637, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:49,768 - INFO - _models.training_function_executor - Epoch 001/053 | train_loss=6.5027 | val_loss=6.4449 | val_acc=0.7188
2025-10-13 22:41:49,777 - INFO - _models.training_function_executor - Epoch 002/053 | train_loss=6.4112 | val_loss=6.2394 | val_acc=0.7188
2025-10-13 22:41:49,784 - INFO - _models.training_function_executor - Epoch 003/053 | train_loss=6.1667 | val_loss=5.9668 | val_acc=0.7188
2025-10-13 22:41:49,790 - INFO - _models.training_function_executor - Epoch 004/053 | train_loss=5.8371 | val_loss=5.6941 | val_acc=0.7188
2025-10-13 22:41:49,797 - INFO - _models.training_function_executor - Epoch 005/053 | train_loss=5.4971 | val_loss=5.4255 | val_acc=0.7188
2025-10-13 22:41:49,803 - INFO - _models.training_function_executor - Epoch 006/053 | train_loss=5.3629 | val_loss=5.1588 | val_acc=0.7188
2025-10-13 22:41:49,810 - INFO - _models.training_function_executor - Epoch 007/053 | train_loss=5.0008 | val_loss=4.8908 | val_acc=0.7188
2025-10-13 22:41:49,816 - INFO - _models.training_function_executor - Epoch 008/053 | train_loss=4.7815 | val_loss=4.6270 | val_acc=0.7188
2025-10-13 22:41:49,823 - INFO - _models.training_function_executor - Epoch 009/053 | train_loss=4.4883 | val_loss=4.3643 | val_acc=0.7188
2025-10-13 22:41:49,829 - INFO - _models.training_function_executor - Epoch 010/053 | train_loss=4.1973 | val_loss=4.1084 | val_acc=0.7188
2025-10-13 22:41:49,835 - INFO - _models.training_function_executor - Epoch 011/053 | train_loss=4.0554 | val_loss=3.8550 | val_acc=0.7188
2025-10-13 22:41:49,841 - INFO - _models.training_function_executor - Epoch 012/053 | train_loss=3.6743 | val_loss=3.6020 | val_acc=0.7188
2025-10-13 22:41:49,847 - INFO - _models.training_function_executor - Epoch 013/053 | train_loss=3.3915 | val_loss=3.3520 | val_acc=0.7188
2025-10-13 22:41:49,854 - INFO - _models.training_function_executor - Epoch 014/053 | train_loss=3.2736 | val_loss=3.1139 | val_acc=0.7188
2025-10-13 22:41:49,861 - INFO - _models.training_function_executor - Epoch 015/053 | train_loss=2.9875 | val_loss=2.8802 | val_acc=0.7188
2025-10-13 22:41:49,868 - INFO - _models.training_function_executor - Epoch 016/053 | train_loss=2.7283 | val_loss=2.6530 | val_acc=0.7188
2025-10-13 22:41:49,874 - INFO - _models.training_function_executor - Epoch 017/053 | train_loss=2.5940 | val_loss=2.4343 | val_acc=0.7188
2025-10-13 22:41:49,881 - INFO - _models.training_function_executor - Epoch 018/053 | train_loss=2.3507 | val_loss=2.2255 | val_acc=0.7188
2025-10-13 22:41:49,888 - INFO - _models.training_function_executor - Epoch 019/053 | train_loss=2.1871 | val_loss=2.0248 | val_acc=0.7188
2025-10-13 22:41:49,894 - INFO - _models.training_function_executor - Epoch 020/053 | train_loss=2.0302 | val_loss=1.8385 | val_acc=0.7188
2025-10-13 22:41:49,900 - INFO - _models.training_function_executor - Epoch 021/053 | train_loss=1.7400 | val_loss=1.6635 | val_acc=0.7188
2025-10-13 22:41:49,907 - INFO - _models.training_function_executor - Epoch 022/053 | train_loss=1.6239 | val_loss=1.5072 | val_acc=0.7188
2025-10-13 22:41:49,913 - INFO - _models.training_function_executor - Epoch 023/053 | train_loss=1.5214 | val_loss=1.3686 | val_acc=0.7188
2025-10-13 22:41:49,919 - INFO - _models.training_function_executor - Epoch 024/053 | train_loss=1.3566 | val_loss=1.2501 | val_acc=0.7188
2025-10-13 22:41:49,925 - INFO - _models.training_function_executor - Epoch 025/053 | train_loss=1.3471 | val_loss=1.1578 | val_acc=0.7188
2025-10-13 22:41:49,932 - INFO - _models.training_function_executor - Epoch 026/053 | train_loss=1.2464 | val_loss=1.0920 | val_acc=0.7188
2025-10-13 22:41:49,938 - INFO - _models.training_function_executor - Epoch 027/053 | train_loss=1.1982 | val_loss=1.0525 | val_acc=0.7188
2025-10-13 22:41:49,944 - INFO - _models.training_function_executor - Epoch 028/053 | train_loss=1.2489 | val_loss=1.0262 | val_acc=0.7031
2025-10-13 22:41:49,950 - INFO - _models.training_function_executor - Epoch 029/053 | train_loss=1.2808 | val_loss=1.0108 | val_acc=0.7031
2025-10-13 22:41:49,957 - INFO - _models.training_function_executor - Epoch 030/053 | train_loss=1.1396 | val_loss=1.0008 | val_acc=0.6875
2025-10-13 22:41:49,964 - INFO - _models.training_function_executor - Epoch 031/053 | train_loss=1.1165 | val_loss=0.9933 | val_acc=0.6875
2025-10-13 22:41:49,971 - INFO - _models.training_function_executor - Epoch 032/053 | train_loss=1.2102 | val_loss=0.9871 | val_acc=0.7031
2025-10-13 22:41:49,978 - INFO - _models.training_function_executor - Epoch 033/053 | train_loss=1.1788 | val_loss=0.9829 | val_acc=0.7031
2025-10-13 22:41:49,985 - INFO - _models.training_function_executor - Epoch 034/053 | train_loss=1.2086 | val_loss=0.9774 | val_acc=0.7031
2025-10-13 22:41:49,991 - INFO - _models.training_function_executor - Epoch 035/053 | train_loss=1.1140 | val_loss=0.9730 | val_acc=0.7188
2025-10-13 22:41:49,998 - INFO - _models.training_function_executor - Epoch 036/053 | train_loss=1.1616 | val_loss=0.9691 | val_acc=0.7188
2025-10-13 22:41:50,005 - INFO - _models.training_function_executor - Epoch 037/053 | train_loss=1.1439 | val_loss=0.9664 | val_acc=0.7188
2025-10-13 22:41:50,011 - INFO - _models.training_function_executor - Epoch 038/053 | train_loss=1.1358 | val_loss=0.9638 | val_acc=0.7188
2025-10-13 22:41:50,018 - INFO - _models.training_function_executor - Epoch 039/053 | train_loss=1.1597 | val_loss=0.9617 | val_acc=0.7188
2025-10-13 22:41:50,024 - INFO - _models.training_function_executor - Epoch 040/053 | train_loss=1.1291 | val_loss=0.9578 | val_acc=0.7188
2025-10-13 22:41:50,031 - INFO - _models.training_function_executor - Epoch 041/053 | train_loss=1.1734 | val_loss=0.9551 | val_acc=0.7188
2025-10-13 22:41:50,037 - INFO - _models.training_function_executor - Epoch 042/053 | train_loss=1.2014 | val_loss=0.9535 | val_acc=0.7188
2025-10-13 22:41:50,044 - INFO - _models.training_function_executor - Epoch 043/053 | train_loss=1.0974 | val_loss=0.9523 | val_acc=0.7188
2025-10-13 22:41:50,051 - INFO - _models.training_function_executor - Epoch 044/053 | train_loss=1.0921 | val_loss=0.9512 | val_acc=0.7188
2025-10-13 22:41:50,057 - INFO - _models.training_function_executor - Epoch 045/053 | train_loss=1.1655 | val_loss=0.9504 | val_acc=0.7188
2025-10-13 22:41:50,064 - INFO - _models.training_function_executor - Epoch 046/053 | train_loss=1.1168 | val_loss=0.9493 | val_acc=0.7188
2025-10-13 22:41:50,070 - INFO - _models.training_function_executor - Epoch 047/053 | train_loss=1.0593 | val_loss=0.9485 | val_acc=0.7188
2025-10-13 22:41:50,076 - INFO - _models.training_function_executor - Epoch 048/053 | train_loss=1.1991 | val_loss=0.9477 | val_acc=0.7188
2025-10-13 22:41:50,083 - INFO - _models.training_function_executor - Epoch 049/053 | train_loss=1.1316 | val_loss=0.9475 | val_acc=0.7188
2025-10-13 22:41:50,090 - INFO - _models.training_function_executor - Epoch 050/053 | train_loss=1.1895 | val_loss=0.9472 | val_acc=0.7188
2025-10-13 22:41:50,096 - INFO - _models.training_function_executor - Epoch 051/053 | train_loss=1.1503 | val_loss=0.9471 | val_acc=0.7188
2025-10-13 22:41:50,102 - INFO - _models.training_function_executor - Epoch 052/053 | train_loss=1.0810 | val_loss=0.9470 | val_acc=0.7188
2025-10-13 22:41:50,109 - INFO - _models.training_function_executor - Epoch 053/053 | train_loss=1.1553 | val_loss=0.9469 | val_acc=0.7188
2025-10-13 22:41:51,025 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-10-13 22:41:51,026 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [6.502691388130188, 6.411246180534363, 6.1667234897613525, 5.837081432342529, 5.497101783752441, 5.362864971160889, 5.000823736190796, 4.78151273727417, 4.488314986228943, 4.197258114814758, 4.055447220802307, 3.674250543117523, 3.391514301300049, 3.2736434936523438, 2.987466871738434, 2.7282880544662476, 2.5940052270889282, 2.350703477859497, 2.1870673298835754, 2.0302367210388184, 1.7400427758693695, 1.6238903105258942, 1.521419107913971, 1.3565663695335388, 1.3470633327960968, 1.2463918030261993, 1.1981973052024841, 1.2489132583141327, 1.280828446149826, 1.1396479308605194, 1.1165048778057098, 1.2101767361164093, 1.178771734237671, 1.208615317940712, 1.1139942109584808, 1.1615608036518097, 1.143910974264145, 1.1358490586280823, 1.159700334072113, 1.1291116327047348, 1.173350065946579, 1.201412171125412, 1.0974444895982742, 1.0921436697244644, 1.165539175271988, 1.1168476939201355, 1.059294655919075, 1.1990535259246826, 1.1315983533859253, 1.1894951462745667, 1.150261789560318, 1.0809865593910217, 1.1553340554237366], 'val_losses': [6.444943428039551, 6.239418983459473, 5.966803550720215, 5.694104194641113, 5.425474166870117, 5.15877103805542, 4.890827655792236, 4.6269659996032715, 4.364287853240967, 4.108429908752441, 3.855041027069092, 3.6020054817199707, 3.352031707763672, 3.113940954208374, 2.880204200744629, 2.6529693603515625, 2.434269905090332, 2.225475311279297, 2.0248379707336426, 1.8384521007537842, 1.6635414361953735, 1.5071659088134766, 1.36862313747406, 1.250125765800476, 1.1578153371810913, 1.092029094696045, 1.0525476932525635, 1.0261719226837158, 1.010751485824585, 1.0007927417755127, 0.993261456489563, 0.9870920181274414, 0.9829219579696655, 0.9773806929588318, 0.9729576110839844, 0.9691112041473389, 0.9664372205734253, 0.9638261198997498, 0.9616502523422241, 0.957783579826355, 0.9551264047622681, 0.9534738659858704, 0.952258288860321, 0.9512156248092651, 0.9503574967384338, 0.9492691159248352, 0.9484752416610718, 0.9477443695068359, 0.9475257396697998, 0.9471971392631531, 0.9470825791358948, 0.9469747543334961, 0.9469197392463684], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.703125, 0.703125, 0.6875, 0.6875, 0.703125, 0.703125, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.2875053928420108e-05, 'batch_size': 64, 'epochs': 53, 'hidden_size': 170, 'dropout': 0.009034464839901392, 'weight_decay': 5.432564073497845e-05, 'label_smoothing': 0.13973306860919554, 'grad_clip_norm': 3.7216447812656637, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-10-13 22:41:51,026 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:41:51,026 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-10-13 22:41:51,026 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.049s
2025-10-13 22:41:51,096 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:41:51,096 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.070s
2025-10-13 22:41:51,096 - INFO - bo.run_bo - Recorded observation #14: hparams={'lr': 1.2875053928420108e-05, 'batch_size': np.int64(64), 'epochs': np.int64(53), 'hidden_size': np.int64(170), 'dropout': 0.009034464839901392, 'weight_decay': 5.432564073497845e-05, 'label_smoothing': 0.13973306860919554, 'grad_clip_norm': 3.7216447812656637, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:41:51,096 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 14: {'lr': 1.2875053928420108e-05, 'batch_size': np.int64(64), 'epochs': np.int64(53), 'hidden_size': np.int64(170), 'dropout': 0.009034464839901392, 'weight_decay': 5.432564073497845e-05, 'label_smoothing': 0.13973306860919554, 'grad_clip_norm': 3.7216447812656637, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:41:51,096 - INFO - bo.run_bo - üîçBO Trial 15: Using RF surrogate + Expected Improvement
2025-10-13 22:41:51,096 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:51,096 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 15 (NaN monitoring active)
2025-10-13 22:41:51,096 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:51,097 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:51,097 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002139811569735564, 'batch_size': 64, 'epochs': 32, 'hidden_size': 97, 'dropout': 0.0490244594427965, 'weight_decay': 5.286339771351423e-06, 'label_smoothing': 0.014708588985398289, 'grad_clip_norm': 3.7225622511310577, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:51,097 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002139811569735564, 'batch_size': 64, 'epochs': 32, 'hidden_size': 97, 'dropout': 0.0490244594427965, 'weight_decay': 5.286339771351423e-06, 'label_smoothing': 0.014708588985398289, 'grad_clip_norm': 3.7225622511310577, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:41:54,832 - INFO - _models.training_function_executor - Epoch 001/032 | train_loss=3.8075 | val_loss=3.0734 | val_acc=0.1250
2025-10-13 22:41:54,841 - INFO - _models.training_function_executor - Epoch 002/032 | train_loss=2.8414 | val_loss=1.7526 | val_acc=0.4531
2025-10-13 22:41:54,848 - INFO - _models.training_function_executor - Epoch 003/032 | train_loss=1.7386 | val_loss=0.9216 | val_acc=0.7188
2025-10-13 22:41:54,854 - INFO - _models.training_function_executor - Epoch 004/032 | train_loss=1.1926 | val_loss=0.6948 | val_acc=0.7188
2025-10-13 22:41:54,861 - INFO - _models.training_function_executor - Epoch 005/032 | train_loss=1.0831 | val_loss=0.6660 | val_acc=0.6562
2025-10-13 22:41:54,867 - INFO - _models.training_function_executor - Epoch 006/032 | train_loss=1.1272 | val_loss=0.7276 | val_acc=0.7188
2025-10-13 22:41:54,873 - INFO - _models.training_function_executor - Epoch 007/032 | train_loss=1.0584 | val_loss=0.6927 | val_acc=0.7188
2025-10-13 22:41:54,879 - INFO - _models.training_function_executor - Epoch 008/032 | train_loss=0.8370 | val_loss=0.7228 | val_acc=0.6875
2025-10-13 22:41:54,886 - INFO - _models.training_function_executor - Epoch 009/032 | train_loss=0.8667 | val_loss=0.6750 | val_acc=0.7188
2025-10-13 22:41:54,893 - INFO - _models.training_function_executor - Epoch 010/032 | train_loss=0.8479 | val_loss=0.7190 | val_acc=0.7188
2025-10-13 22:41:54,899 - INFO - _models.training_function_executor - Epoch 011/032 | train_loss=0.8020 | val_loss=0.6541 | val_acc=0.6719
2025-10-13 22:41:54,906 - INFO - _models.training_function_executor - Epoch 012/032 | train_loss=0.8012 | val_loss=0.7364 | val_acc=0.7188
2025-10-13 22:41:54,913 - INFO - _models.training_function_executor - Epoch 013/032 | train_loss=0.7640 | val_loss=0.6562 | val_acc=0.6562
2025-10-13 22:41:54,920 - INFO - _models.training_function_executor - Epoch 014/032 | train_loss=0.7973 | val_loss=0.6785 | val_acc=0.7188
2025-10-13 22:41:54,926 - INFO - _models.training_function_executor - Epoch 015/032 | train_loss=0.8109 | val_loss=0.6878 | val_acc=0.7188
2025-10-13 22:41:54,938 - INFO - _models.training_function_executor - Epoch 016/032 | train_loss=0.7157 | val_loss=0.6298 | val_acc=0.7188
2025-10-13 22:41:54,951 - INFO - _models.training_function_executor - Epoch 017/032 | train_loss=0.6948 | val_loss=0.6428 | val_acc=0.7188
2025-10-13 22:41:54,963 - INFO - _models.training_function_executor - Epoch 018/032 | train_loss=0.7119 | val_loss=0.6392 | val_acc=0.7188
2025-10-13 22:41:54,974 - INFO - _models.training_function_executor - Epoch 019/032 | train_loss=0.7261 | val_loss=0.6390 | val_acc=0.7031
2025-10-13 22:41:54,981 - INFO - _models.training_function_executor - Epoch 020/032 | train_loss=0.7207 | val_loss=0.6291 | val_acc=0.7188
2025-10-13 22:41:54,988 - INFO - _models.training_function_executor - Epoch 021/032 | train_loss=0.7080 | val_loss=0.6385 | val_acc=0.7188
2025-10-13 22:41:54,994 - INFO - _models.training_function_executor - Epoch 022/032 | train_loss=0.6995 | val_loss=0.6297 | val_acc=0.7188
2025-10-13 22:41:55,000 - INFO - _models.training_function_executor - Epoch 023/032 | train_loss=0.7045 | val_loss=0.6188 | val_acc=0.7188
2025-10-13 22:41:55,006 - INFO - _models.training_function_executor - Epoch 024/032 | train_loss=0.7258 | val_loss=0.6172 | val_acc=0.7188
2025-10-13 22:41:55,012 - INFO - _models.training_function_executor - Epoch 025/032 | train_loss=0.6955 | val_loss=0.6224 | val_acc=0.7188
2025-10-13 22:41:55,018 - INFO - _models.training_function_executor - Epoch 026/032 | train_loss=0.6766 | val_loss=0.6236 | val_acc=0.7188
2025-10-13 22:41:55,025 - INFO - _models.training_function_executor - Epoch 027/032 | train_loss=0.7178 | val_loss=0.6228 | val_acc=0.7188
2025-10-13 22:41:55,031 - INFO - _models.training_function_executor - Epoch 028/032 | train_loss=0.7178 | val_loss=0.6232 | val_acc=0.7188
2025-10-13 22:41:55,037 - INFO - _models.training_function_executor - Epoch 029/032 | train_loss=0.6873 | val_loss=0.6228 | val_acc=0.7188
2025-10-13 22:41:55,043 - INFO - _models.training_function_executor - Epoch 030/032 | train_loss=0.6909 | val_loss=0.6213 | val_acc=0.7188
2025-10-13 22:41:55,049 - INFO - _models.training_function_executor - Epoch 031/032 | train_loss=0.6648 | val_loss=0.6204 | val_acc=0.7188
2025-10-13 22:41:55,055 - INFO - _models.training_function_executor - Epoch 032/032 | train_loss=0.6878 | val_loss=0.6203 | val_acc=0.7188
2025-10-13 22:41:55,988 - INFO - _models.training_function_executor - Model: 10,285 parameters, 44.2KB storage
2025-10-13 22:41:55,988 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [3.807480037212372, 2.8414023518562317, 1.7385528981685638, 1.1925727128982544, 1.0830740928649902, 1.127219796180725, 1.058402732014656, 0.8369702696800232, 0.8667474091053009, 0.847915381193161, 0.8019560724496841, 0.8011889606714249, 0.7639902234077454, 0.797316387295723, 0.8108652532100677, 0.715692326426506, 0.6947500556707382, 0.7119337022304535, 0.7261273711919785, 0.7206782102584839, 0.7080072313547134, 0.6995228081941605, 0.704494372010231, 0.7258096933364868, 0.6955388933420181, 0.6766483634710312, 0.7178331911563873, 0.7177543044090271, 0.6872539520263672, 0.6908583343029022, 0.6647545695304871, 0.6877898275852203], 'val_losses': [3.0733633041381836, 1.752589464187622, 0.9215649366378784, 0.6948119401931763, 0.6660058498382568, 0.7276173233985901, 0.6926646828651428, 0.7228399515151978, 0.6750430464744568, 0.7190250158309937, 0.6540530920028687, 0.7363976240158081, 0.6562216877937317, 0.6784957051277161, 0.6877828240394592, 0.6298381090164185, 0.6427730917930603, 0.6391681432723999, 0.6390396952629089, 0.6290864944458008, 0.6385171413421631, 0.629730224609375, 0.6188135743141174, 0.6171716451644897, 0.6224148273468018, 0.6236026287078857, 0.6228387951850891, 0.6231675744056702, 0.6228309273719788, 0.6212644577026367, 0.6204244494438171, 0.6202774047851562], 'val_acc': [0.125, 0.453125, 0.71875, 0.71875, 0.65625, 0.71875, 0.71875, 0.6875, 0.71875, 0.71875, 0.671875, 0.71875, 0.65625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002139811569735564, 'batch_size': 64, 'epochs': 32, 'hidden_size': 97, 'dropout': 0.0490244594427965, 'weight_decay': 5.286339771351423e-06, 'label_smoothing': 0.014708588985398289, 'grad_clip_norm': 3.7225622511310577, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 10285, 'model_storage_size_kb': 44.19335937500001, 'model_size_validation': 'PASS'}
2025-10-13 22:41:55,988 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:41:55,988 - INFO - _models.training_function_executor - Model: 10,285 parameters, 44.2KB (PASS 256KB limit)
2025-10-13 22:41:55,988 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.892s
2025-10-13 22:41:56,061 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:41:56,061 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.072s
2025-10-13 22:41:56,061 - INFO - bo.run_bo - Recorded observation #15: hparams={'lr': 0.002139811569735564, 'batch_size': np.int64(64), 'epochs': np.int64(32), 'hidden_size': np.int64(97), 'dropout': 0.0490244594427965, 'weight_decay': 5.286339771351423e-06, 'label_smoothing': 0.014708588985398289, 'grad_clip_norm': 3.7225622511310577, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:41:56,061 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 15: {'lr': 0.002139811569735564, 'batch_size': np.int64(64), 'epochs': np.int64(32), 'hidden_size': np.int64(97), 'dropout': 0.0490244594427965, 'weight_decay': 5.286339771351423e-06, 'label_smoothing': 0.014708588985398289, 'grad_clip_norm': 3.7225622511310577, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:41:56,061 - INFO - bo.run_bo - üîçBO Trial 16: Using RF surrogate + Expected Improvement
2025-10-13 22:41:56,061 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:41:56,061 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 16 (NaN monitoring active)
2025-10-13 22:41:56,061 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:41:56,061 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:41:56,061 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005297391595122908, 'batch_size': 256, 'epochs': 27, 'hidden_size': 152, 'dropout': 0.09731128070585662, 'weight_decay': 0.00027265688653456025, 'label_smoothing': 0.10190044267516778, 'grad_clip_norm': 0.8075497174116617, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:41:56,062 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005297391595122908, 'batch_size': 256, 'epochs': 27, 'hidden_size': 152, 'dropout': 0.09731128070585662, 'weight_decay': 0.00027265688653456025, 'label_smoothing': 0.10190044267516778, 'grad_clip_norm': 0.8075497174116617, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:41:59,850 - INFO - _models.training_function_executor - Epoch 001/027 | train_loss=2.4681 | val_loss=1.5692 | val_acc=0.3438
2025-10-13 22:41:59,855 - INFO - _models.training_function_executor - Epoch 002/027 | train_loss=2.6823 | val_loss=1.5692 | val_acc=0.3438
2025-10-13 22:41:59,859 - INFO - _models.training_function_executor - Epoch 003/027 | train_loss=2.5166 | val_loss=1.5692 | val_acc=0.3438
2025-10-13 22:41:59,863 - INFO - _models.training_function_executor - Epoch 004/027 | train_loss=2.4272 | val_loss=1.5692 | val_acc=0.3438
2025-10-13 22:41:59,868 - INFO - _models.training_function_executor - Epoch 005/027 | train_loss=2.5981 | val_loss=15.0702 | val_acc=0.7188
2025-10-13 22:41:59,871 - INFO - _models.training_function_executor - Epoch 006/027 | train_loss=14.6623 | val_loss=15.0702 | val_acc=0.7188
2025-10-13 22:41:59,875 - INFO - _models.training_function_executor - Epoch 007/027 | train_loss=14.9884 | val_loss=15.5812 | val_acc=0.0938
2025-10-13 22:41:59,879 - INFO - _models.training_function_executor - Epoch 008/027 | train_loss=15.1228 | val_loss=15.5812 | val_acc=0.0938
2025-10-13 22:41:59,883 - INFO - _models.training_function_executor - Epoch 009/027 | train_loss=14.8544 | val_loss=9.9028 | val_acc=0.5469
2025-10-13 22:41:59,886 - INFO - _models.training_function_executor - Epoch 010/027 | train_loss=10.4075 | val_loss=9.5352 | val_acc=0.7188
2025-10-13 22:41:59,889 - INFO - _models.training_function_executor - Epoch 011/027 | train_loss=9.5493 | val_loss=7.5217 | val_acc=0.7188
2025-10-13 22:41:59,892 - INFO - _models.training_function_executor - Epoch 012/027 | train_loss=7.5237 | val_loss=4.8163 | val_acc=0.7188
2025-10-13 22:41:59,896 - INFO - _models.training_function_executor - Epoch 013/027 | train_loss=4.8763 | val_loss=2.1144 | val_acc=0.7188
2025-10-13 22:41:59,899 - INFO - _models.training_function_executor - Epoch 014/027 | train_loss=2.3882 | val_loss=1.0978 | val_acc=0.5156
2025-10-13 22:41:59,902 - INFO - _models.training_function_executor - Epoch 015/027 | train_loss=1.9454 | val_loss=2.4266 | val_acc=0.2969
2025-10-13 22:41:59,905 - INFO - _models.training_function_executor - Epoch 016/027 | train_loss=3.1542 | val_loss=2.8635 | val_acc=0.2188
2025-10-13 22:41:59,909 - INFO - _models.training_function_executor - Epoch 017/027 | train_loss=3.5814 | val_loss=2.5514 | val_acc=0.2656
2025-10-13 22:41:59,912 - INFO - _models.training_function_executor - Epoch 018/027 | train_loss=3.2234 | val_loss=1.9581 | val_acc=0.3281
2025-10-13 22:41:59,915 - INFO - _models.training_function_executor - Epoch 019/027 | train_loss=2.6073 | val_loss=1.3991 | val_acc=0.5000
2025-10-13 22:41:59,919 - INFO - _models.training_function_executor - Epoch 020/027 | train_loss=2.1558 | val_loss=0.9870 | val_acc=0.6875
2025-10-13 22:41:59,922 - INFO - _models.training_function_executor - Epoch 021/027 | train_loss=1.6466 | val_loss=0.7819 | val_acc=0.6719
2025-10-13 22:41:59,925 - INFO - _models.training_function_executor - Epoch 022/027 | train_loss=1.2200 | val_loss=0.7554 | val_acc=0.7188
2025-10-13 22:41:59,929 - INFO - _models.training_function_executor - Epoch 023/027 | train_loss=1.2500 | val_loss=0.8092 | val_acc=0.7188
2025-10-13 22:41:59,934 - INFO - _models.training_function_executor - Epoch 024/027 | train_loss=1.2519 | val_loss=0.8504 | val_acc=0.7188
2025-10-13 22:41:59,937 - INFO - _models.training_function_executor - Epoch 025/027 | train_loss=1.2006 | val_loss=0.8647 | val_acc=0.7188
2025-10-13 22:41:59,941 - INFO - _models.training_function_executor - Epoch 026/027 | train_loss=1.2922 | val_loss=0.8667 | val_acc=0.7188
2025-10-13 22:41:59,945 - INFO - _models.training_function_executor - Epoch 027/027 | train_loss=1.3235 | val_loss=0.8664 | val_acc=0.7188
2025-10-13 22:42:00,866 - INFO - _models.training_function_executor - Model: 24,475 parameters, 105.2KB storage
2025-10-13 22:42:00,867 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.468088388442993, 2.682342767715454, 2.516618490219116, 2.42716383934021, 2.5980570316314697, 14.662267684936523, 14.988422393798828, 15.122773170471191, 14.854425430297852, 10.407475471496582, 9.549349784851074, 7.523711204528809, 4.876306533813477, 2.3881795406341553, 1.945388913154602, 3.1541907787323, 3.5814201831817627, 3.223374128341675, 2.6072638034820557, 2.1558377742767334, 1.6465942859649658, 1.2199678421020508, 1.2500481605529785, 1.2519360780715942, 1.200612187385559, 1.2922476530075073, 1.3235359191894531], 'val_losses': [1.569246530532837, 1.569246530532837, 1.569246530532837, 1.569246530532837, 15.070150375366211, 15.070150375366211, 15.581180572509766, 15.581180572509766, 9.902765274047852, 9.535196304321289, 7.521710395812988, 4.816263198852539, 2.114353895187378, 1.097771406173706, 2.42655086517334, 2.8634605407714844, 2.551400899887085, 1.9581140279769897, 1.3990952968597412, 0.9870098233222961, 0.7818853259086609, 0.7553993463516235, 0.8091638088226318, 0.8504413962364197, 0.8647083044052124, 0.8666886687278748, 0.8663797378540039], 'val_acc': [0.34375, 0.34375, 0.34375, 0.34375, 0.71875, 0.71875, 0.09375, 0.09375, 0.546875, 0.71875, 0.71875, 0.71875, 0.71875, 0.515625, 0.296875, 0.21875, 0.265625, 0.328125, 0.5, 0.6875, 0.671875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005297391595122908, 'batch_size': 256, 'epochs': 27, 'hidden_size': 152, 'dropout': 0.09731128070585662, 'weight_decay': 0.00027265688653456025, 'label_smoothing': 0.10190044267516778, 'grad_clip_norm': 0.8075497174116617, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 24475, 'model_storage_size_kb': 105.16601562500001, 'model_size_validation': 'PASS'}
2025-10-13 22:42:00,867 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:42:00,867 - INFO - _models.training_function_executor - Model: 24,475 parameters, 105.2KB (PASS 256KB limit)
2025-10-13 22:42:00,867 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.806s
2025-10-13 22:42:00,937 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:42:00,937 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.070s
2025-10-13 22:42:00,937 - INFO - bo.run_bo - Recorded observation #16: hparams={'lr': 0.005297391595122908, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'hidden_size': np.int64(152), 'dropout': 0.09731128070585662, 'weight_decay': 0.00027265688653456025, 'label_smoothing': 0.10190044267516778, 'grad_clip_norm': 0.8075497174116617, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:42:00,937 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 16: {'lr': 0.005297391595122908, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'hidden_size': np.int64(152), 'dropout': 0.09731128070585662, 'weight_decay': 0.00027265688653456025, 'label_smoothing': 0.10190044267516778, 'grad_clip_norm': 0.8075497174116617, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:42:00,937 - INFO - bo.run_bo - üîçBO Trial 17: Using RF surrogate + Expected Improvement
2025-10-13 22:42:00,937 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:00,937 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 17 (NaN monitoring active)
2025-10-13 22:42:00,937 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:00,937 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:00,937 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.1557295318677796e-05, 'batch_size': 128, 'epochs': 57, 'hidden_size': 191, 'dropout': 0.03865796674415096, 'weight_decay': 0.005891910678651642, 'label_smoothing': 0.022499020723097268, 'grad_clip_norm': 2.2559500907160204, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:00,938 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.1557295318677796e-05, 'batch_size': 128, 'epochs': 57, 'hidden_size': 191, 'dropout': 0.03865796674415096, 'weight_decay': 0.005891910678651642, 'label_smoothing': 0.022499020723097268, 'grad_clip_norm': 2.2559500907160204, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:04,977 - INFO - _models.training_function_executor - Epoch 001/057 | train_loss=7.1144 | val_loss=6.4539 | val_acc=0.1875
2025-10-13 22:42:04,983 - INFO - _models.training_function_executor - Epoch 002/057 | train_loss=7.0157 | val_loss=6.4539 | val_acc=0.1875
2025-10-13 22:42:04,988 - INFO - _models.training_function_executor - Epoch 003/057 | train_loss=7.2071 | val_loss=6.4539 | val_acc=0.1875
2025-10-13 22:42:04,993 - INFO - _models.training_function_executor - Epoch 004/057 | train_loss=7.0005 | val_loss=6.1675 | val_acc=0.1875
2025-10-13 22:42:04,997 - INFO - _models.training_function_executor - Epoch 005/057 | train_loss=6.3361 | val_loss=5.8797 | val_acc=0.1875
2025-10-13 22:42:05,001 - INFO - _models.training_function_executor - Epoch 006/057 | train_loss=6.5538 | val_loss=5.5987 | val_acc=0.1875
2025-10-13 22:42:05,005 - INFO - _models.training_function_executor - Epoch 007/057 | train_loss=5.9138 | val_loss=5.3199 | val_acc=0.1875
2025-10-13 22:42:05,010 - INFO - _models.training_function_executor - Epoch 008/057 | train_loss=5.8591 | val_loss=5.0444 | val_acc=0.1875
2025-10-13 22:42:05,014 - INFO - _models.training_function_executor - Epoch 009/057 | train_loss=5.5086 | val_loss=4.7762 | val_acc=0.1875
2025-10-13 22:42:05,019 - INFO - _models.training_function_executor - Epoch 010/057 | train_loss=5.3648 | val_loss=4.5150 | val_acc=0.1875
2025-10-13 22:42:05,023 - INFO - _models.training_function_executor - Epoch 011/057 | train_loss=4.7533 | val_loss=4.2587 | val_acc=0.1875
2025-10-13 22:42:05,028 - INFO - _models.training_function_executor - Epoch 012/057 | train_loss=4.6681 | val_loss=4.0093 | val_acc=0.1875
2025-10-13 22:42:05,032 - INFO - _models.training_function_executor - Epoch 013/057 | train_loss=4.6000 | val_loss=3.7672 | val_acc=0.1875
2025-10-13 22:42:05,036 - INFO - _models.training_function_executor - Epoch 014/057 | train_loss=4.2900 | val_loss=3.5370 | val_acc=0.1875
2025-10-13 22:42:05,041 - INFO - _models.training_function_executor - Epoch 015/057 | train_loss=4.1733 | val_loss=3.3130 | val_acc=0.2031
2025-10-13 22:42:05,046 - INFO - _models.training_function_executor - Epoch 016/057 | train_loss=3.6525 | val_loss=3.0994 | val_acc=0.2656
2025-10-13 22:42:05,050 - INFO - _models.training_function_executor - Epoch 017/057 | train_loss=3.7339 | val_loss=2.8966 | val_acc=0.2969
2025-10-13 22:42:05,054 - INFO - _models.training_function_executor - Epoch 018/057 | train_loss=3.3684 | val_loss=2.7033 | val_acc=0.3281
2025-10-13 22:42:05,059 - INFO - _models.training_function_executor - Epoch 019/057 | train_loss=3.4098 | val_loss=2.5236 | val_acc=0.3594
2025-10-13 22:42:05,064 - INFO - _models.training_function_executor - Epoch 020/057 | train_loss=3.1483 | val_loss=2.3567 | val_acc=0.3906
2025-10-13 22:42:05,070 - INFO - _models.training_function_executor - Epoch 021/057 | train_loss=2.9108 | val_loss=2.2013 | val_acc=0.4375
2025-10-13 22:42:05,074 - INFO - _models.training_function_executor - Epoch 022/057 | train_loss=2.6324 | val_loss=2.0609 | val_acc=0.4844
2025-10-13 22:42:05,079 - INFO - _models.training_function_executor - Epoch 023/057 | train_loss=2.5291 | val_loss=1.9331 | val_acc=0.5312
2025-10-13 22:42:05,083 - INFO - _models.training_function_executor - Epoch 024/057 | train_loss=2.5438 | val_loss=1.8164 | val_acc=0.5781
2025-10-13 22:42:05,087 - INFO - _models.training_function_executor - Epoch 025/057 | train_loss=2.2500 | val_loss=1.7134 | val_acc=0.5938
2025-10-13 22:42:05,092 - INFO - _models.training_function_executor - Epoch 026/057 | train_loss=2.4887 | val_loss=1.6259 | val_acc=0.6250
2025-10-13 22:42:05,096 - INFO - _models.training_function_executor - Epoch 027/057 | train_loss=2.2672 | val_loss=1.5482 | val_acc=0.6875
2025-10-13 22:42:05,100 - INFO - _models.training_function_executor - Epoch 028/057 | train_loss=2.4160 | val_loss=1.4827 | val_acc=0.6875
2025-10-13 22:42:05,104 - INFO - _models.training_function_executor - Epoch 029/057 | train_loss=2.1287 | val_loss=1.4267 | val_acc=0.6719
2025-10-13 22:42:05,109 - INFO - _models.training_function_executor - Epoch 030/057 | train_loss=2.0643 | val_loss=1.3788 | val_acc=0.6562
2025-10-13 22:42:05,113 - INFO - _models.training_function_executor - Epoch 031/057 | train_loss=2.0214 | val_loss=1.3408 | val_acc=0.6406
2025-10-13 22:42:05,117 - INFO - _models.training_function_executor - Epoch 032/057 | train_loss=2.1228 | val_loss=1.3099 | val_acc=0.6719
2025-10-13 22:42:05,122 - INFO - _models.training_function_executor - Epoch 033/057 | train_loss=2.0337 | val_loss=1.2851 | val_acc=0.6719
2025-10-13 22:42:05,126 - INFO - _models.training_function_executor - Epoch 034/057 | train_loss=1.8809 | val_loss=1.2651 | val_acc=0.6719
2025-10-13 22:42:05,130 - INFO - _models.training_function_executor - Epoch 035/057 | train_loss=1.9189 | val_loss=1.2485 | val_acc=0.7188
2025-10-13 22:42:05,135 - INFO - _models.training_function_executor - Epoch 036/057 | train_loss=1.8252 | val_loss=1.2355 | val_acc=0.7344
2025-10-13 22:42:05,139 - INFO - _models.training_function_executor - Epoch 037/057 | train_loss=1.7645 | val_loss=1.2254 | val_acc=0.7344
2025-10-13 22:42:05,143 - INFO - _models.training_function_executor - Epoch 038/057 | train_loss=1.7981 | val_loss=1.2163 | val_acc=0.7344
2025-10-13 22:42:05,147 - INFO - _models.training_function_executor - Epoch 039/057 | train_loss=1.8182 | val_loss=1.2094 | val_acc=0.7188
2025-10-13 22:42:05,151 - INFO - _models.training_function_executor - Epoch 040/057 | train_loss=1.9281 | val_loss=1.2031 | val_acc=0.7188
2025-10-13 22:42:05,155 - INFO - _models.training_function_executor - Epoch 041/057 | train_loss=1.6948 | val_loss=1.1977 | val_acc=0.7031
2025-10-13 22:42:05,159 - INFO - _models.training_function_executor - Epoch 042/057 | train_loss=1.7533 | val_loss=1.1931 | val_acc=0.7188
2025-10-13 22:42:05,163 - INFO - _models.training_function_executor - Epoch 043/057 | train_loss=1.6797 | val_loss=1.1886 | val_acc=0.7188
2025-10-13 22:42:05,167 - INFO - _models.training_function_executor - Epoch 044/057 | train_loss=1.8187 | val_loss=1.1854 | val_acc=0.7188
2025-10-13 22:42:05,171 - INFO - _models.training_function_executor - Epoch 045/057 | train_loss=1.5324 | val_loss=1.1827 | val_acc=0.7188
2025-10-13 22:42:05,176 - INFO - _models.training_function_executor - Epoch 046/057 | train_loss=1.7277 | val_loss=1.1800 | val_acc=0.7188
2025-10-13 22:42:05,180 - INFO - _models.training_function_executor - Epoch 047/057 | train_loss=1.6567 | val_loss=1.1782 | val_acc=0.7188
2025-10-13 22:42:05,184 - INFO - _models.training_function_executor - Epoch 048/057 | train_loss=1.7453 | val_loss=1.1762 | val_acc=0.7188
2025-10-13 22:42:05,188 - INFO - _models.training_function_executor - Epoch 049/057 | train_loss=1.7033 | val_loss=1.1746 | val_acc=0.7188
2025-10-13 22:42:05,193 - INFO - _models.training_function_executor - Epoch 050/057 | train_loss=1.7567 | val_loss=1.1731 | val_acc=0.7188
2025-10-13 22:42:05,198 - INFO - _models.training_function_executor - Epoch 051/057 | train_loss=1.8271 | val_loss=1.1721 | val_acc=0.7188
2025-10-13 22:42:05,202 - INFO - _models.training_function_executor - Epoch 052/057 | train_loss=1.6989 | val_loss=1.1709 | val_acc=0.7188
2025-10-13 22:42:05,207 - INFO - _models.training_function_executor - Epoch 053/057 | train_loss=1.7311 | val_loss=1.1706 | val_acc=0.7188
2025-10-13 22:42:05,211 - INFO - _models.training_function_executor - Epoch 054/057 | train_loss=1.7725 | val_loss=1.1701 | val_acc=0.7188
2025-10-13 22:42:05,215 - INFO - _models.training_function_executor - Epoch 055/057 | train_loss=1.7658 | val_loss=1.1701 | val_acc=0.7188
2025-10-13 22:42:05,219 - INFO - _models.training_function_executor - Epoch 056/057 | train_loss=1.6828 | val_loss=1.1701 | val_acc=0.7188
2025-10-13 22:42:05,224 - INFO - _models.training_function_executor - Epoch 057/057 | train_loss=1.7966 | val_loss=1.1701 | val_acc=0.7188
2025-10-13 22:42:06,105 - INFO - _models.training_function_executor - Model: 38,203 parameters, 82.1KB storage
2025-10-13 22:42:06,105 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [7.114407062530518, 7.015690088272095, 7.207055568695068, 7.000527620315552, 6.336077451705933, 6.553773880004883, 5.913848638534546, 5.859121799468994, 5.508551359176636, 5.364847421646118, 4.753292798995972, 4.668111324310303, 4.600038051605225, 4.290049076080322, 4.173293948173523, 3.6525052785873413, 3.733889102935791, 3.368360757827759, 3.4097522497177124, 3.1482901573181152, 2.910802483558655, 2.6324164867401123, 2.529092311859131, 2.54376220703125, 2.2500287294387817, 2.488730549812317, 2.267239212989807, 2.416016936302185, 2.128710985183716, 2.064344882965088, 2.021440625190735, 2.122765362262726, 2.033678114414215, 1.8808680772781372, 1.9188899993896484, 1.825150191783905, 1.764548420906067, 1.798083245754242, 1.8181689381599426, 1.9281396865844727, 1.6947816610336304, 1.7533124089241028, 1.6796780228614807, 1.8186918497085571, 1.532380759716034, 1.7277106642723083, 1.656688153743744, 1.7452560067176819, 1.7033292651176453, 1.75674569606781, 1.8271048069000244, 1.6989280581474304, 1.731129229068756, 1.772544264793396, 1.765817105770111, 1.6828497648239136, 1.7966078519821167], 'val_losses': [6.453932762145996, 6.453932762145996, 6.453932762145996, 6.167450904846191, 5.8796610832214355, 5.5987091064453125, 5.31987190246582, 5.0443549156188965, 4.7762250900268555, 4.514957904815674, 4.258702754974365, 4.009334564208984, 3.7671728134155273, 3.5369908809661865, 3.312953472137451, 3.099426507949829, 2.8965671062469482, 2.703324794769287, 2.523632287979126, 2.356686592102051, 2.2013323307037354, 2.060894250869751, 1.9330724477767944, 1.8164082765579224, 1.7133698463439941, 1.6258546113967896, 1.5481802225112915, 1.4826897382736206, 1.4266986846923828, 1.3788378238677979, 1.3408445119857788, 1.3099024295806885, 1.2851406335830688, 1.2651137113571167, 1.2485488653182983, 1.2355271577835083, 1.2254247665405273, 1.2162797451019287, 1.209357500076294, 1.2031476497650146, 1.1977185010910034, 1.193111538887024, 1.1885714530944824, 1.1853711605072021, 1.1827281713485718, 1.1800012588500977, 1.1781842708587646, 1.1761759519577026, 1.174553632736206, 1.1731268167495728, 1.17206609249115, 1.1708755493164062, 1.1706267595291138, 1.170135498046875, 1.170130968093872, 1.1701242923736572, 1.1700533628463745], 'val_acc': [0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.203125, 0.265625, 0.296875, 0.328125, 0.359375, 0.390625, 0.4375, 0.484375, 0.53125, 0.578125, 0.59375, 0.625, 0.6875, 0.6875, 0.671875, 0.65625, 0.640625, 0.671875, 0.671875, 0.671875, 0.71875, 0.734375, 0.734375, 0.734375, 0.71875, 0.71875, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.1557295318677796e-05, 'batch_size': 128, 'epochs': 57, 'hidden_size': 191, 'dropout': 0.03865796674415096, 'weight_decay': 0.005891910678651642, 'label_smoothing': 0.022499020723097268, 'grad_clip_norm': 2.2559500907160204, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 38203, 'model_storage_size_kb': 82.0767578125, 'model_size_validation': 'PASS'}
2025-10-13 22:42:06,105 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:42:06,105 - INFO - _models.training_function_executor - Model: 38,203 parameters, 82.1KB (PASS 256KB limit)
2025-10-13 22:42:06,105 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.168s
2025-10-13 22:42:06,175 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:42:06,176 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.069s
2025-10-13 22:42:06,176 - INFO - bo.run_bo - Recorded observation #17: hparams={'lr': 1.1557295318677796e-05, 'batch_size': np.int64(128), 'epochs': np.int64(57), 'hidden_size': np.int64(191), 'dropout': 0.03865796674415096, 'weight_decay': 0.005891910678651642, 'label_smoothing': 0.022499020723097268, 'grad_clip_norm': 2.2559500907160204, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:42:06,176 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 17: {'lr': 1.1557295318677796e-05, 'batch_size': np.int64(128), 'epochs': np.int64(57), 'hidden_size': np.int64(191), 'dropout': 0.03865796674415096, 'weight_decay': 0.005891910678651642, 'label_smoothing': 0.022499020723097268, 'grad_clip_norm': 2.2559500907160204, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:42:06,176 - INFO - bo.run_bo - üîçBO Trial 18: Using RF surrogate + Expected Improvement
2025-10-13 22:42:06,176 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:06,176 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 18 (NaN monitoring active)
2025-10-13 22:42:06,176 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:06,176 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:06,176 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.2455898774772873e-05, 'batch_size': 96, 'epochs': 27, 'hidden_size': 153, 'dropout': 0.09741737148594906, 'weight_decay': 1.2008360944957143e-05, 'label_smoothing': 0.009698879478081481, 'grad_clip_norm': 2.9891750917051008, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:06,177 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.2455898774772873e-05, 'batch_size': 96, 'epochs': 27, 'hidden_size': 153, 'dropout': 0.09741737148594906, 'weight_decay': 1.2008360944957143e-05, 'label_smoothing': 0.009698879478081481, 'grad_clip_norm': 2.9891750917051008, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:09,808 - INFO - _models.training_function_executor - Epoch 001/027 | train_loss=8.0714 | val_loss=7.1218 | val_acc=0.0781
2025-10-13 22:42:09,815 - INFO - _models.training_function_executor - Epoch 002/027 | train_loss=7.9187 | val_loss=7.0499 | val_acc=0.0781
2025-10-13 22:42:09,821 - INFO - _models.training_function_executor - Epoch 003/027 | train_loss=7.8340 | val_loss=6.8367 | val_acc=0.0781
2025-10-13 22:42:09,826 - INFO - _models.training_function_executor - Epoch 004/027 | train_loss=7.6676 | val_loss=6.6219 | val_acc=0.0781
2025-10-13 22:42:09,832 - INFO - _models.training_function_executor - Epoch 005/027 | train_loss=7.4658 | val_loss=6.4157 | val_acc=0.0781
2025-10-13 22:42:09,837 - INFO - _models.training_function_executor - Epoch 006/027 | train_loss=7.0440 | val_loss=6.2201 | val_acc=0.0938
2025-10-13 22:42:09,842 - INFO - _models.training_function_executor - Epoch 007/027 | train_loss=6.9256 | val_loss=6.0259 | val_acc=0.0938
2025-10-13 22:42:09,847 - INFO - _models.training_function_executor - Epoch 008/027 | train_loss=6.7947 | val_loss=5.8443 | val_acc=0.0938
2025-10-13 22:42:09,852 - INFO - _models.training_function_executor - Epoch 009/027 | train_loss=6.6496 | val_loss=5.6680 | val_acc=0.0938
2025-10-13 22:42:09,857 - INFO - _models.training_function_executor - Epoch 010/027 | train_loss=6.5072 | val_loss=5.5055 | val_acc=0.0938
2025-10-13 22:42:09,862 - INFO - _models.training_function_executor - Epoch 011/027 | train_loss=6.5348 | val_loss=5.3525 | val_acc=0.0938
2025-10-13 22:42:09,867 - INFO - _models.training_function_executor - Epoch 012/027 | train_loss=6.5969 | val_loss=5.2159 | val_acc=0.0938
2025-10-13 22:42:09,872 - INFO - _models.training_function_executor - Epoch 013/027 | train_loss=6.0634 | val_loss=5.0908 | val_acc=0.0938
2025-10-13 22:42:09,877 - INFO - _models.training_function_executor - Epoch 014/027 | train_loss=5.6928 | val_loss=4.9742 | val_acc=0.0938
2025-10-13 22:42:09,882 - INFO - _models.training_function_executor - Epoch 015/027 | train_loss=5.8377 | val_loss=4.8761 | val_acc=0.0938
2025-10-13 22:42:09,888 - INFO - _models.training_function_executor - Epoch 016/027 | train_loss=5.5040 | val_loss=4.7875 | val_acc=0.0938
2025-10-13 22:42:09,893 - INFO - _models.training_function_executor - Epoch 017/027 | train_loss=5.0620 | val_loss=4.7125 | val_acc=0.0938
2025-10-13 22:42:09,899 - INFO - _models.training_function_executor - Epoch 018/027 | train_loss=5.2259 | val_loss=4.6470 | val_acc=0.0938
2025-10-13 22:42:09,905 - INFO - _models.training_function_executor - Epoch 019/027 | train_loss=5.2509 | val_loss=4.5923 | val_acc=0.0938
2025-10-13 22:42:09,910 - INFO - _models.training_function_executor - Epoch 020/027 | train_loss=4.9107 | val_loss=4.5512 | val_acc=0.0938
2025-10-13 22:42:09,915 - INFO - _models.training_function_executor - Epoch 021/027 | train_loss=5.3204 | val_loss=4.5160 | val_acc=0.0938
2025-10-13 22:42:09,920 - INFO - _models.training_function_executor - Epoch 022/027 | train_loss=4.9482 | val_loss=4.4915 | val_acc=0.0938
2025-10-13 22:42:09,925 - INFO - _models.training_function_executor - Epoch 023/027 | train_loss=5.2847 | val_loss=4.4750 | val_acc=0.0938
2025-10-13 22:42:09,930 - INFO - _models.training_function_executor - Epoch 024/027 | train_loss=5.1975 | val_loss=4.4641 | val_acc=0.0938
2025-10-13 22:42:09,935 - INFO - _models.training_function_executor - Epoch 025/027 | train_loss=5.6184 | val_loss=4.4568 | val_acc=0.0938
2025-10-13 22:42:09,940 - INFO - _models.training_function_executor - Epoch 026/027 | train_loss=5.2993 | val_loss=4.4536 | val_acc=0.0938
2025-10-13 22:42:09,945 - INFO - _models.training_function_executor - Epoch 027/027 | train_loss=5.2434 | val_loss=4.4529 | val_acc=0.0938
2025-10-13 22:42:10,789 - INFO - _models.training_function_executor - Model: 24,789 parameters, 53.3KB storage
2025-10-13 22:42:10,789 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [8.071410536766052, 7.918737888336182, 7.834013044834137, 7.667612314224243, 7.465774357318878, 7.043997406959534, 6.925613462924957, 6.794738411903381, 6.649571001529694, 6.507183492183685, 6.534818589687347, 6.596919059753418, 6.0633732080459595, 5.692761421203613, 5.837704122066498, 5.503980457782745, 5.061958014965057, 5.225912928581238, 5.250905930995941, 4.910673558712006, 5.320366263389587, 4.948191523551941, 5.284726321697235, 5.1974963545799255, 5.618425369262695, 5.2992594838142395, 5.243435740470886], 'val_losses': [7.1217851638793945, 7.049949645996094, 6.8366875648498535, 6.621888637542725, 6.4156622886657715, 6.2200608253479, 6.025903224945068, 5.844272136688232, 5.668039321899414, 5.50553560256958, 5.3524556159973145, 5.215914726257324, 5.090845584869385, 4.974175930023193, 4.876066207885742, 4.787537574768066, 4.712527751922607, 4.647027492523193, 4.592304706573486, 4.5511627197265625, 4.516049385070801, 4.491523265838623, 4.475027084350586, 4.464052200317383, 4.456817150115967, 4.453622817993164, 4.452932834625244], 'val_acc': [0.078125, 0.078125, 0.078125, 0.078125, 0.078125, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.2455898774772873e-05, 'batch_size': 96, 'epochs': 27, 'hidden_size': 153, 'dropout': 0.09741737148594906, 'weight_decay': 1.2008360944957143e-05, 'label_smoothing': 0.009698879478081481, 'grad_clip_norm': 2.9891750917051008, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 24789, 'model_storage_size_kb': 53.2576171875, 'model_size_validation': 'PASS'}
2025-10-13 22:42:10,789 - INFO - _models.training_function_executor - BO Objective: base=0.0938, size_penalty=0.0000, final=0.0938
2025-10-13 22:42:10,789 - INFO - _models.training_function_executor - Model: 24,789 parameters, 53.3KB (PASS 256KB limit)
2025-10-13 22:42:10,789 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.613s
2025-10-13 22:42:10,860 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0938
2025-10-13 22:42:10,860 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.070s
2025-10-13 22:42:10,860 - INFO - bo.run_bo - Recorded observation #18: hparams={'lr': 1.2455898774772873e-05, 'batch_size': np.int64(96), 'epochs': np.int64(27), 'hidden_size': np.int64(153), 'dropout': 0.09741737148594906, 'weight_decay': 1.2008360944957143e-05, 'label_smoothing': 0.009698879478081481, 'grad_clip_norm': 2.9891750917051008, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.0938
2025-10-13 22:42:10,860 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 18: {'lr': 1.2455898774772873e-05, 'batch_size': np.int64(96), 'epochs': np.int64(27), 'hidden_size': np.int64(153), 'dropout': 0.09741737148594906, 'weight_decay': 1.2008360944957143e-05, 'label_smoothing': 0.009698879478081481, 'grad_clip_norm': 2.9891750917051008, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.0938
2025-10-13 22:42:10,860 - INFO - bo.run_bo - üîçBO Trial 19: Using RF surrogate + Expected Improvement
2025-10-13 22:42:10,860 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:10,860 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 19 (NaN monitoring active)
2025-10-13 22:42:10,860 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:10,860 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:10,860 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 4.1367488108947045e-05, 'batch_size': 96, 'epochs': 48, 'hidden_size': 186, 'dropout': 0.10023581903811099, 'weight_decay': 3.469313100080408e-05, 'label_smoothing': 0.029122471696255196, 'grad_clip_norm': 3.864046630124746, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:10,861 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 4.1367488108947045e-05, 'batch_size': 96, 'epochs': 48, 'hidden_size': 186, 'dropout': 0.10023581903811099, 'weight_decay': 3.469313100080408e-05, 'label_smoothing': 0.029122471696255196, 'grad_clip_norm': 3.864046630124746, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:14,180 - INFO - _models.training_function_executor - Epoch 001/048 | train_loss=4.1150 | val_loss=3.3998 | val_acc=0.7188
2025-10-13 22:42:14,188 - INFO - _models.training_function_executor - Epoch 002/048 | train_loss=3.9443 | val_loss=3.3555 | val_acc=0.7188
2025-10-13 22:42:14,195 - INFO - _models.training_function_executor - Epoch 003/048 | train_loss=3.6759 | val_loss=3.2437 | val_acc=0.7188
2025-10-13 22:42:14,200 - INFO - _models.training_function_executor - Epoch 004/048 | train_loss=3.2750 | val_loss=3.1098 | val_acc=0.7188
2025-10-13 22:42:14,205 - INFO - _models.training_function_executor - Epoch 005/048 | train_loss=3.2561 | val_loss=2.8638 | val_acc=0.7188
2025-10-13 22:42:14,211 - INFO - _models.training_function_executor - Epoch 006/048 | train_loss=3.1452 | val_loss=2.6038 | val_acc=0.7188
2025-10-13 22:42:14,217 - INFO - _models.training_function_executor - Epoch 007/048 | train_loss=2.9629 | val_loss=2.3232 | val_acc=0.7188
2025-10-13 22:42:14,222 - INFO - _models.training_function_executor - Epoch 008/048 | train_loss=2.6883 | val_loss=2.0456 | val_acc=0.7188
2025-10-13 22:42:14,227 - INFO - _models.training_function_executor - Epoch 009/048 | train_loss=2.2345 | val_loss=1.8035 | val_acc=0.7188
2025-10-13 22:42:14,232 - INFO - _models.training_function_executor - Epoch 010/048 | train_loss=2.2004 | val_loss=1.5803 | val_acc=0.7188
2025-10-13 22:42:14,237 - INFO - _models.training_function_executor - Epoch 011/048 | train_loss=1.9900 | val_loss=1.3759 | val_acc=0.7188
2025-10-13 22:42:14,243 - INFO - _models.training_function_executor - Epoch 012/048 | train_loss=1.8309 | val_loss=1.2081 | val_acc=0.7188
2025-10-13 22:42:14,248 - INFO - _models.training_function_executor - Epoch 013/048 | train_loss=1.7041 | val_loss=1.1357 | val_acc=0.7188
2025-10-13 22:42:14,253 - INFO - _models.training_function_executor - Epoch 014/048 | train_loss=1.7584 | val_loss=1.1074 | val_acc=0.7188
2025-10-13 22:42:14,259 - INFO - _models.training_function_executor - Epoch 015/048 | train_loss=1.8373 | val_loss=1.1049 | val_acc=0.7188
2025-10-13 22:42:14,265 - INFO - _models.training_function_executor - Epoch 016/048 | train_loss=1.7550 | val_loss=1.1039 | val_acc=0.7188
2025-10-13 22:42:14,270 - INFO - _models.training_function_executor - Epoch 017/048 | train_loss=1.7045 | val_loss=1.1007 | val_acc=0.7188
2025-10-13 22:42:14,276 - INFO - _models.training_function_executor - Epoch 018/048 | train_loss=1.8135 | val_loss=1.0924 | val_acc=0.7188
2025-10-13 22:42:14,281 - INFO - _models.training_function_executor - Epoch 019/048 | train_loss=1.7392 | val_loss=1.1100 | val_acc=0.7188
2025-10-13 22:42:14,286 - INFO - _models.training_function_executor - Epoch 020/048 | train_loss=1.7324 | val_loss=1.0969 | val_acc=0.7188
2025-10-13 22:42:14,292 - INFO - _models.training_function_executor - Epoch 021/048 | train_loss=1.6426 | val_loss=1.1278 | val_acc=0.7188
2025-10-13 22:42:14,297 - INFO - _models.training_function_executor - Epoch 022/048 | train_loss=1.7030 | val_loss=1.1134 | val_acc=0.7188
2025-10-13 22:42:14,302 - INFO - _models.training_function_executor - Epoch 023/048 | train_loss=1.7549 | val_loss=1.0528 | val_acc=0.7188
2025-10-13 22:42:14,307 - INFO - _models.training_function_executor - Epoch 024/048 | train_loss=1.8543 | val_loss=1.0060 | val_acc=0.7188
2025-10-13 22:42:14,312 - INFO - _models.training_function_executor - Epoch 025/048 | train_loss=1.6056 | val_loss=0.9757 | val_acc=0.7188
2025-10-13 22:42:14,317 - INFO - _models.training_function_executor - Epoch 026/048 | train_loss=1.6266 | val_loss=0.9502 | val_acc=0.7188
2025-10-13 22:42:14,322 - INFO - _models.training_function_executor - Epoch 027/048 | train_loss=1.5499 | val_loss=0.9441 | val_acc=0.7188
2025-10-13 22:42:14,327 - INFO - _models.training_function_executor - Epoch 028/048 | train_loss=1.8235 | val_loss=0.9511 | val_acc=0.7188
2025-10-13 22:42:14,332 - INFO - _models.training_function_executor - Epoch 029/048 | train_loss=1.5776 | val_loss=0.9545 | val_acc=0.7188
2025-10-13 22:42:14,337 - INFO - _models.training_function_executor - Epoch 030/048 | train_loss=1.7622 | val_loss=0.9777 | val_acc=0.7188
2025-10-13 22:42:14,342 - INFO - _models.training_function_executor - Epoch 031/048 | train_loss=1.6276 | val_loss=0.9860 | val_acc=0.7188
2025-10-13 22:42:14,347 - INFO - _models.training_function_executor - Epoch 032/048 | train_loss=1.6149 | val_loss=0.9967 | val_acc=0.7188
2025-10-13 22:42:14,352 - INFO - _models.training_function_executor - Epoch 033/048 | train_loss=1.4946 | val_loss=1.0112 | val_acc=0.7188
2025-10-13 22:42:14,357 - INFO - _models.training_function_executor - Epoch 034/048 | train_loss=1.6238 | val_loss=1.0203 | val_acc=0.7188
2025-10-13 22:42:14,362 - INFO - _models.training_function_executor - Epoch 035/048 | train_loss=1.6129 | val_loss=1.0278 | val_acc=0.7188
2025-10-13 22:42:14,368 - INFO - _models.training_function_executor - Epoch 036/048 | train_loss=1.5633 | val_loss=1.0327 | val_acc=0.7188
2025-10-13 22:42:14,373 - INFO - _models.training_function_executor - Epoch 037/048 | train_loss=1.6629 | val_loss=1.0257 | val_acc=0.7188
2025-10-13 22:42:14,378 - INFO - _models.training_function_executor - Epoch 038/048 | train_loss=1.5485 | val_loss=1.0177 | val_acc=0.7188
2025-10-13 22:42:14,383 - INFO - _models.training_function_executor - Epoch 039/048 | train_loss=1.6703 | val_loss=1.0114 | val_acc=0.7188
2025-10-13 22:42:14,388 - INFO - _models.training_function_executor - Epoch 040/048 | train_loss=1.5716 | val_loss=1.0037 | val_acc=0.7188
2025-10-13 22:42:14,393 - INFO - _models.training_function_executor - Epoch 041/048 | train_loss=1.6684 | val_loss=0.9960 | val_acc=0.7188
2025-10-13 22:42:14,399 - INFO - _models.training_function_executor - Epoch 042/048 | train_loss=1.5261 | val_loss=0.9883 | val_acc=0.7188
2025-10-13 22:42:14,404 - INFO - _models.training_function_executor - Epoch 043/048 | train_loss=1.5663 | val_loss=0.9828 | val_acc=0.7188
2025-10-13 22:42:14,409 - INFO - _models.training_function_executor - Epoch 044/048 | train_loss=1.6378 | val_loss=0.9788 | val_acc=0.7188
2025-10-13 22:42:14,414 - INFO - _models.training_function_executor - Epoch 045/048 | train_loss=1.3572 | val_loss=0.9762 | val_acc=0.7188
2025-10-13 22:42:14,419 - INFO - _models.training_function_executor - Epoch 046/048 | train_loss=1.7423 | val_loss=0.9751 | val_acc=0.7188
2025-10-13 22:42:14,424 - INFO - _models.training_function_executor - Epoch 047/048 | train_loss=1.5766 | val_loss=0.9746 | val_acc=0.7188
2025-10-13 22:42:14,429 - INFO - _models.training_function_executor - Epoch 048/048 | train_loss=1.6302 | val_loss=0.9745 | val_acc=0.7188
2025-10-13 22:42:15,298 - INFO - _models.training_function_executor - Model: 36,273 parameters, 77.9KB storage
2025-10-13 22:42:15,298 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [4.114982336759567, 3.944322347640991, 3.675931394100189, 3.274993598461151, 3.256099045276642, 3.1452488005161285, 2.9628677368164062, 2.6883276402950287, 2.234471082687378, 2.2004202604293823, 1.9900076687335968, 1.830850288271904, 1.7040594965219498, 1.7584271728992462, 1.8372769057750702, 1.7550498247146606, 1.704457402229309, 1.8135168254375458, 1.739221766591072, 1.7324388772249222, 1.6425978243350983, 1.7030054777860641, 1.7548817992210388, 1.8543494194746017, 1.605628490447998, 1.6265756487846375, 1.549938216805458, 1.8234901279211044, 1.577555164694786, 1.7622426599264145, 1.627574399113655, 1.6148795932531357, 1.494629442691803, 1.623767375946045, 1.6129380017518997, 1.563324674963951, 1.662861704826355, 1.5484725832939148, 1.6702684611082077, 1.5716457664966583, 1.6684110313653946, 1.5261405855417252, 1.566277414560318, 1.63778954744339, 1.3571815490722656, 1.7422700822353363, 1.576595738530159, 1.6302080601453781], 'val_losses': [3.3998379707336426, 3.3554930686950684, 3.24373197555542, 3.1098124980926514, 2.8638041019439697, 2.6038143634796143, 2.3232064247131348, 2.0456395149230957, 1.8035297393798828, 1.5803319215774536, 1.3758538961410522, 1.2081055641174316, 1.135677456855774, 1.107358694076538, 1.1048803329467773, 1.103907823562622, 1.1007037162780762, 1.0923912525177002, 1.1099573373794556, 1.0969374179840088, 1.1278204917907715, 1.1134284734725952, 1.0528417825698853, 1.0059733390808105, 0.9757186770439148, 0.9502449631690979, 0.9440791010856628, 0.9511067867279053, 0.954535961151123, 0.9777308702468872, 0.9859526753425598, 0.9967343807220459, 1.0112054347991943, 1.0203455686569214, 1.0278126001358032, 1.0327492952346802, 1.0257076025009155, 1.0176631212234497, 1.0113964080810547, 1.0037133693695068, 0.9959949851036072, 0.9883100986480713, 0.9827650189399719, 0.9788405895233154, 0.9761819839477539, 0.9750984907150269, 0.9746009707450867, 0.9745131134986877], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 4.1367488108947045e-05, 'batch_size': 96, 'epochs': 48, 'hidden_size': 186, 'dropout': 0.10023581903811099, 'weight_decay': 3.469313100080408e-05, 'label_smoothing': 0.029122471696255196, 'grad_clip_norm': 3.864046630124746, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 36273, 'model_storage_size_kb': 77.9302734375, 'model_size_validation': 'PASS'}
2025-10-13 22:42:15,298 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:42:15,298 - INFO - _models.training_function_executor - Model: 36,273 parameters, 77.9KB (PASS 256KB limit)
2025-10-13 22:42:15,298 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.438s
2025-10-13 22:42:15,368 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:42:15,368 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.069s
2025-10-13 22:42:15,368 - INFO - bo.run_bo - Recorded observation #19: hparams={'lr': 4.1367488108947045e-05, 'batch_size': np.int64(96), 'epochs': np.int64(48), 'hidden_size': np.int64(186), 'dropout': 0.10023581903811099, 'weight_decay': 3.469313100080408e-05, 'label_smoothing': 0.029122471696255196, 'grad_clip_norm': 3.864046630124746, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:42:15,368 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 19: {'lr': 4.1367488108947045e-05, 'batch_size': np.int64(96), 'epochs': np.int64(48), 'hidden_size': np.int64(186), 'dropout': 0.10023581903811099, 'weight_decay': 3.469313100080408e-05, 'label_smoothing': 0.029122471696255196, 'grad_clip_norm': 3.864046630124746, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:42:15,368 - INFO - bo.run_bo - üîçBO Trial 20: Using RF surrogate + Expected Improvement
2025-10-13 22:42:15,369 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:15,369 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 20 (NaN monitoring active)
2025-10-13 22:42:15,369 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:15,369 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:15,369 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.702824066923046e-05, 'batch_size': 96, 'epochs': 190, 'hidden_size': 177, 'dropout': 0.05667832243016542, 'weight_decay': 5.815352586746574e-05, 'label_smoothing': 0.0009692157535190706, 'grad_clip_norm': 1.944668216037016, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:42:15,369 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.702824066923046e-05, 'batch_size': 96, 'epochs': 190, 'hidden_size': 177, 'dropout': 0.05667832243016542, 'weight_decay': 5.815352586746574e-05, 'label_smoothing': 0.0009692157535190706, 'grad_clip_norm': 1.944668216037016, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:42:19,063 - INFO - _models.training_function_executor - Epoch 001/190 | train_loss=6.4098 | val_loss=5.3141 | val_acc=0.0938
2025-10-13 22:42:19,070 - INFO - _models.training_function_executor - Epoch 002/190 | train_loss=6.1932 | val_loss=5.0999 | val_acc=0.0938
2025-10-13 22:42:19,076 - INFO - _models.training_function_executor - Epoch 003/190 | train_loss=5.3926 | val_loss=4.6879 | val_acc=0.0938
2025-10-13 22:42:19,082 - INFO - _models.training_function_executor - Epoch 004/190 | train_loss=5.2802 | val_loss=4.0629 | val_acc=0.0938
2025-10-13 22:42:19,087 - INFO - _models.training_function_executor - Epoch 005/190 | train_loss=4.6173 | val_loss=3.4588 | val_acc=0.0938
2025-10-13 22:42:19,092 - INFO - _models.training_function_executor - Epoch 006/190 | train_loss=3.9899 | val_loss=2.8789 | val_acc=0.0938
2025-10-13 22:42:19,098 - INFO - _models.training_function_executor - Epoch 007/190 | train_loss=3.9727 | val_loss=2.3432 | val_acc=0.0938
2025-10-13 22:42:19,103 - INFO - _models.training_function_executor - Epoch 008/190 | train_loss=3.5140 | val_loss=1.9055 | val_acc=0.0938
2025-10-13 22:42:19,108 - INFO - _models.training_function_executor - Epoch 009/190 | train_loss=3.2397 | val_loss=1.6220 | val_acc=0.2031
2025-10-13 22:42:19,113 - INFO - _models.training_function_executor - Epoch 010/190 | train_loss=2.7464 | val_loss=1.4979 | val_acc=0.3281
2025-10-13 22:42:19,118 - INFO - _models.training_function_executor - Epoch 011/190 | train_loss=2.4603 | val_loss=1.4909 | val_acc=0.4062
2025-10-13 22:42:19,124 - INFO - _models.training_function_executor - Epoch 012/190 | train_loss=2.2381 | val_loss=1.5519 | val_acc=0.4531
2025-10-13 22:42:19,129 - INFO - _models.training_function_executor - Epoch 013/190 | train_loss=2.1180 | val_loss=1.6357 | val_acc=0.5781
2025-10-13 22:42:19,134 - INFO - _models.training_function_executor - Epoch 014/190 | train_loss=2.3466 | val_loss=1.7069 | val_acc=0.5781
2025-10-13 22:42:19,139 - INFO - _models.training_function_executor - Epoch 015/190 | train_loss=2.0952 | val_loss=1.7493 | val_acc=0.6406
2025-10-13 22:42:19,146 - INFO - _models.training_function_executor - Epoch 016/190 | train_loss=2.0998 | val_loss=1.7391 | val_acc=0.6562
2025-10-13 22:42:19,152 - INFO - _models.training_function_executor - Epoch 017/190 | train_loss=1.9802 | val_loss=1.6789 | val_acc=0.6406
2025-10-13 22:42:19,158 - INFO - _models.training_function_executor - Epoch 018/190 | train_loss=2.1285 | val_loss=1.6036 | val_acc=0.6406
2025-10-13 22:42:19,164 - INFO - _models.training_function_executor - Epoch 019/190 | train_loss=1.9408 | val_loss=1.5229 | val_acc=0.6406
2025-10-13 22:42:19,169 - INFO - _models.training_function_executor - Epoch 020/190 | train_loss=2.0971 | val_loss=1.4611 | val_acc=0.6719
2025-10-13 22:42:19,175 - INFO - _models.training_function_executor - Epoch 021/190 | train_loss=1.9987 | val_loss=1.4279 | val_acc=0.7031
2025-10-13 22:42:19,181 - INFO - _models.training_function_executor - Epoch 022/190 | train_loss=2.0443 | val_loss=1.3885 | val_acc=0.7188
2025-10-13 22:42:19,186 - INFO - _models.training_function_executor - Epoch 023/190 | train_loss=1.9194 | val_loss=1.3511 | val_acc=0.7188
2025-10-13 22:42:19,191 - INFO - _models.training_function_executor - Epoch 024/190 | train_loss=2.0072 | val_loss=1.3133 | val_acc=0.7188
2025-10-13 22:42:19,197 - INFO - _models.training_function_executor - Epoch 025/190 | train_loss=2.3516 | val_loss=1.2522 | val_acc=0.7188
2025-10-13 22:42:19,202 - INFO - _models.training_function_executor - Epoch 026/190 | train_loss=1.8064 | val_loss=1.1934 | val_acc=0.7188
2025-10-13 22:42:19,207 - INFO - _models.training_function_executor - Epoch 027/190 | train_loss=1.6968 | val_loss=1.1467 | val_acc=0.7188
2025-10-13 22:42:19,213 - INFO - _models.training_function_executor - Epoch 028/190 | train_loss=1.6683 | val_loss=1.1131 | val_acc=0.7188
2025-10-13 22:42:19,218 - INFO - _models.training_function_executor - Epoch 029/190 | train_loss=1.7428 | val_loss=1.0898 | val_acc=0.7188
2025-10-13 22:42:19,223 - INFO - _models.training_function_executor - Epoch 030/190 | train_loss=1.9016 | val_loss=1.0624 | val_acc=0.7188
2025-10-13 22:42:19,229 - INFO - _models.training_function_executor - Epoch 031/190 | train_loss=1.6932 | val_loss=1.0368 | val_acc=0.7188
2025-10-13 22:42:19,235 - INFO - _models.training_function_executor - Epoch 032/190 | train_loss=1.6673 | val_loss=1.0385 | val_acc=0.7188
2025-10-13 22:42:19,240 - INFO - _models.training_function_executor - Epoch 033/190 | train_loss=1.5886 | val_loss=1.0560 | val_acc=0.7188
2025-10-13 22:42:19,245 - INFO - _models.training_function_executor - Epoch 034/190 | train_loss=1.9687 | val_loss=1.0746 | val_acc=0.7188
2025-10-13 22:42:19,250 - INFO - _models.training_function_executor - Epoch 035/190 | train_loss=1.8417 | val_loss=1.0700 | val_acc=0.7188
2025-10-13 22:42:19,255 - INFO - _models.training_function_executor - Epoch 036/190 | train_loss=1.6512 | val_loss=1.0706 | val_acc=0.7188
2025-10-13 22:42:19,261 - INFO - _models.training_function_executor - Epoch 037/190 | train_loss=2.0232 | val_loss=1.0549 | val_acc=0.7188
2025-10-13 22:42:19,266 - INFO - _models.training_function_executor - Epoch 038/190 | train_loss=1.8083 | val_loss=1.0274 | val_acc=0.7188
2025-10-13 22:42:19,271 - INFO - _models.training_function_executor - Epoch 039/190 | train_loss=1.8541 | val_loss=1.0204 | val_acc=0.7188
2025-10-13 22:42:19,276 - INFO - _models.training_function_executor - Epoch 040/190 | train_loss=1.7122 | val_loss=1.0108 | val_acc=0.7188
2025-10-13 22:42:19,281 - INFO - _models.training_function_executor - Epoch 041/190 | train_loss=1.7771 | val_loss=0.9927 | val_acc=0.7188
2025-10-13 22:42:19,286 - INFO - _models.training_function_executor - Epoch 042/190 | train_loss=1.6800 | val_loss=0.9709 | val_acc=0.7188
2025-10-13 22:42:19,291 - INFO - _models.training_function_executor - Epoch 043/190 | train_loss=1.7873 | val_loss=0.9487 | val_acc=0.7188
2025-10-13 22:42:19,296 - INFO - _models.training_function_executor - Epoch 044/190 | train_loss=1.4849 | val_loss=0.9342 | val_acc=0.7188
2025-10-13 22:42:19,301 - INFO - _models.training_function_executor - Epoch 045/190 | train_loss=1.6454 | val_loss=0.9174 | val_acc=0.7188
2025-10-13 22:42:19,306 - INFO - _models.training_function_executor - Epoch 046/190 | train_loss=1.7403 | val_loss=0.9009 | val_acc=0.7188
2025-10-13 22:42:19,311 - INFO - _models.training_function_executor - Epoch 047/190 | train_loss=1.7889 | val_loss=0.8819 | val_acc=0.7188
2025-10-13 22:42:19,316 - INFO - _models.training_function_executor - Epoch 048/190 | train_loss=1.7235 | val_loss=0.8625 | val_acc=0.7188
2025-10-13 22:42:19,321 - INFO - _models.training_function_executor - Epoch 049/190 | train_loss=1.7376 | val_loss=0.8582 | val_acc=0.7188
2025-10-13 22:42:19,326 - INFO - _models.training_function_executor - Epoch 050/190 | train_loss=1.6881 | val_loss=0.8608 | val_acc=0.7188
2025-10-13 22:42:19,331 - INFO - _models.training_function_executor - Epoch 051/190 | train_loss=1.9007 | val_loss=0.8702 | val_acc=0.7188
2025-10-13 22:42:19,336 - INFO - _models.training_function_executor - Epoch 052/190 | train_loss=1.6468 | val_loss=0.8624 | val_acc=0.7188
2025-10-13 22:42:19,341 - INFO - _models.training_function_executor - Epoch 053/190 | train_loss=1.7368 | val_loss=0.8510 | val_acc=0.7188
2025-10-13 22:42:19,346 - INFO - _models.training_function_executor - Epoch 054/190 | train_loss=1.5746 | val_loss=0.8499 | val_acc=0.7188
2025-10-13 22:42:19,351 - INFO - _models.training_function_executor - Epoch 055/190 | train_loss=1.6599 | val_loss=0.8417 | val_acc=0.7188
2025-10-13 22:42:19,356 - INFO - _models.training_function_executor - Epoch 056/190 | train_loss=1.3291 | val_loss=0.8249 | val_acc=0.7188
2025-10-13 22:42:19,362 - INFO - _models.training_function_executor - Epoch 057/190 | train_loss=1.7429 | val_loss=0.8062 | val_acc=0.7188
2025-10-13 22:42:19,367 - INFO - _models.training_function_executor - Epoch 058/190 | train_loss=1.4805 | val_loss=0.7995 | val_acc=0.7188
2025-10-13 22:42:19,372 - INFO - _models.training_function_executor - Epoch 059/190 | train_loss=1.6263 | val_loss=0.7876 | val_acc=0.7188
2025-10-13 22:42:19,377 - INFO - _models.training_function_executor - Epoch 060/190 | train_loss=1.5041 | val_loss=0.7608 | val_acc=0.7188
2025-10-13 22:42:19,382 - INFO - _models.training_function_executor - Epoch 061/190 | train_loss=1.5250 | val_loss=0.7514 | val_acc=0.7188
2025-10-13 22:42:19,387 - INFO - _models.training_function_executor - Epoch 062/190 | train_loss=1.3819 | val_loss=0.7593 | val_acc=0.7188
2025-10-13 22:42:19,392 - INFO - _models.training_function_executor - Epoch 063/190 | train_loss=1.7033 | val_loss=0.7659 | val_acc=0.7188
2025-10-13 22:42:19,398 - INFO - _models.training_function_executor - Epoch 064/190 | train_loss=1.6963 | val_loss=0.7752 | val_acc=0.7188
2025-10-13 22:42:19,403 - INFO - _models.training_function_executor - Epoch 065/190 | train_loss=1.6484 | val_loss=0.7891 | val_acc=0.7188
2025-10-13 22:42:19,408 - INFO - _models.training_function_executor - Epoch 066/190 | train_loss=1.4266 | val_loss=0.7910 | val_acc=0.7188
2025-10-13 22:42:19,413 - INFO - _models.training_function_executor - Epoch 067/190 | train_loss=1.5591 | val_loss=0.7895 | val_acc=0.7188
2025-10-13 22:42:19,418 - INFO - _models.training_function_executor - Epoch 068/190 | train_loss=1.5645 | val_loss=0.7837 | val_acc=0.7188
2025-10-13 22:42:19,423 - INFO - _models.training_function_executor - Epoch 069/190 | train_loss=1.6871 | val_loss=0.7752 | val_acc=0.7188
2025-10-13 22:42:19,429 - INFO - _models.training_function_executor - Epoch 070/190 | train_loss=1.4965 | val_loss=0.7689 | val_acc=0.7188
2025-10-13 22:42:19,434 - INFO - _models.training_function_executor - Epoch 071/190 | train_loss=1.6212 | val_loss=0.7734 | val_acc=0.7188
2025-10-13 22:42:19,439 - INFO - _models.training_function_executor - Epoch 072/190 | train_loss=1.8347 | val_loss=0.7809 | val_acc=0.7188
2025-10-13 22:42:19,444 - INFO - _models.training_function_executor - Epoch 073/190 | train_loss=1.5083 | val_loss=0.7917 | val_acc=0.7188
2025-10-13 22:42:19,449 - INFO - _models.training_function_executor - Epoch 074/190 | train_loss=1.6683 | val_loss=0.7918 | val_acc=0.7188
2025-10-13 22:42:19,454 - INFO - _models.training_function_executor - Epoch 075/190 | train_loss=1.7120 | val_loss=0.7983 | val_acc=0.7188
2025-10-13 22:42:19,459 - INFO - _models.training_function_executor - Epoch 076/190 | train_loss=1.5937 | val_loss=0.7866 | val_acc=0.7188
2025-10-13 22:42:19,464 - INFO - _models.training_function_executor - Epoch 077/190 | train_loss=1.5338 | val_loss=0.7762 | val_acc=0.7188
2025-10-13 22:42:19,469 - INFO - _models.training_function_executor - Epoch 078/190 | train_loss=1.4111 | val_loss=0.7656 | val_acc=0.7188
2025-10-13 22:42:19,475 - INFO - _models.training_function_executor - Epoch 079/190 | train_loss=1.5427 | val_loss=0.7644 | val_acc=0.7188
2025-10-13 22:42:19,480 - INFO - _models.training_function_executor - Epoch 080/190 | train_loss=1.7571 | val_loss=0.7627 | val_acc=0.7188
2025-10-13 22:42:19,485 - INFO - _models.training_function_executor - Epoch 081/190 | train_loss=1.5520 | val_loss=0.7634 | val_acc=0.7188
2025-10-13 22:42:19,490 - INFO - _models.training_function_executor - Epoch 082/190 | train_loss=1.5900 | val_loss=0.7622 | val_acc=0.7188
2025-10-13 22:42:19,495 - INFO - _models.training_function_executor - Epoch 083/190 | train_loss=1.6447 | val_loss=0.7725 | val_acc=0.7188
2025-10-13 22:42:19,500 - INFO - _models.training_function_executor - Epoch 084/190 | train_loss=1.5342 | val_loss=0.7770 | val_acc=0.7188
2025-10-13 22:42:19,505 - INFO - _models.training_function_executor - Epoch 085/190 | train_loss=1.5106 | val_loss=0.7826 | val_acc=0.7188
2025-10-13 22:42:19,511 - INFO - _models.training_function_executor - Epoch 086/190 | train_loss=1.6560 | val_loss=0.7865 | val_acc=0.7188
2025-10-13 22:42:19,517 - INFO - _models.training_function_executor - Epoch 087/190 | train_loss=1.5788 | val_loss=0.7864 | val_acc=0.7188
2025-10-13 22:42:19,522 - INFO - _models.training_function_executor - Epoch 088/190 | train_loss=1.5631 | val_loss=0.7887 | val_acc=0.7188
2025-10-13 22:42:19,527 - INFO - _models.training_function_executor - Epoch 089/190 | train_loss=1.5265 | val_loss=0.7752 | val_acc=0.7188
2025-10-13 22:42:19,532 - INFO - _models.training_function_executor - Epoch 090/190 | train_loss=1.5361 | val_loss=0.7667 | val_acc=0.7188
2025-10-13 22:42:19,537 - INFO - _models.training_function_executor - Epoch 091/190 | train_loss=1.3981 | val_loss=0.7546 | val_acc=0.7188
2025-10-13 22:42:19,543 - INFO - _models.training_function_executor - Epoch 092/190 | train_loss=1.5810 | val_loss=0.7518 | val_acc=0.7188
2025-10-13 22:42:19,548 - INFO - _models.training_function_executor - Epoch 093/190 | train_loss=1.8416 | val_loss=0.7589 | val_acc=0.7188
2025-10-13 22:42:19,554 - INFO - _models.training_function_executor - Epoch 094/190 | train_loss=1.7441 | val_loss=0.7620 | val_acc=0.7188
2025-10-13 22:42:19,559 - INFO - _models.training_function_executor - Epoch 095/190 | train_loss=1.4421 | val_loss=0.7581 | val_acc=0.7188
2025-10-13 22:42:19,565 - INFO - _models.training_function_executor - Epoch 096/190 | train_loss=1.3917 | val_loss=0.7469 | val_acc=0.7188
2025-10-13 22:42:19,570 - INFO - _models.training_function_executor - Epoch 097/190 | train_loss=1.4448 | val_loss=0.7348 | val_acc=0.7188
2025-10-13 22:42:19,576 - INFO - _models.training_function_executor - Epoch 098/190 | train_loss=1.6780 | val_loss=0.7303 | val_acc=0.7188
2025-10-13 22:42:19,581 - INFO - _models.training_function_executor - Epoch 099/190 | train_loss=1.3077 | val_loss=0.7290 | val_acc=0.7188
2025-10-13 22:42:19,587 - INFO - _models.training_function_executor - Epoch 100/190 | train_loss=1.4767 | val_loss=0.7325 | val_acc=0.7188
2025-10-13 22:42:19,592 - INFO - _models.training_function_executor - Epoch 101/190 | train_loss=1.5509 | val_loss=0.7337 | val_acc=0.7188
2025-10-13 22:42:19,598 - INFO - _models.training_function_executor - Epoch 102/190 | train_loss=1.5320 | val_loss=0.7332 | val_acc=0.7188
2025-10-13 22:42:19,604 - INFO - _models.training_function_executor - Epoch 103/190 | train_loss=1.6749 | val_loss=0.7340 | val_acc=0.7188
2025-10-13 22:42:19,609 - INFO - _models.training_function_executor - Epoch 104/190 | train_loss=1.2907 | val_loss=0.7407 | val_acc=0.7188
2025-10-13 22:42:19,614 - INFO - _models.training_function_executor - Epoch 105/190 | train_loss=1.5156 | val_loss=0.7434 | val_acc=0.7188
2025-10-13 22:42:19,620 - INFO - _models.training_function_executor - Epoch 106/190 | train_loss=1.4764 | val_loss=0.7448 | val_acc=0.7188
2025-10-13 22:42:19,625 - INFO - _models.training_function_executor - Epoch 107/190 | train_loss=1.6575 | val_loss=0.7427 | val_acc=0.7188
2025-10-13 22:42:19,631 - INFO - _models.training_function_executor - Epoch 108/190 | train_loss=1.3631 | val_loss=0.7420 | val_acc=0.7188
2025-10-13 22:42:19,636 - INFO - _models.training_function_executor - Epoch 109/190 | train_loss=1.5265 | val_loss=0.7412 | val_acc=0.7188
2025-10-13 22:42:19,641 - INFO - _models.training_function_executor - Epoch 110/190 | train_loss=1.5988 | val_loss=0.7367 | val_acc=0.7188
2025-10-13 22:42:19,648 - INFO - _models.training_function_executor - Epoch 111/190 | train_loss=1.5915 | val_loss=0.7391 | val_acc=0.7188
2025-10-13 22:42:19,653 - INFO - _models.training_function_executor - Epoch 112/190 | train_loss=1.3728 | val_loss=0.7368 | val_acc=0.7188
2025-10-13 22:42:19,658 - INFO - _models.training_function_executor - Epoch 113/190 | train_loss=1.6066 | val_loss=0.7421 | val_acc=0.7188
2025-10-13 22:42:19,664 - INFO - _models.training_function_executor - Epoch 114/190 | train_loss=1.3588 | val_loss=0.7478 | val_acc=0.7188
2025-10-13 22:42:19,670 - INFO - _models.training_function_executor - Epoch 115/190 | train_loss=1.4823 | val_loss=0.7462 | val_acc=0.7188
2025-10-13 22:42:19,675 - INFO - _models.training_function_executor - Epoch 116/190 | train_loss=1.5152 | val_loss=0.7429 | val_acc=0.7188
2025-10-13 22:42:19,681 - INFO - _models.training_function_executor - Epoch 117/190 | train_loss=1.5897 | val_loss=0.7389 | val_acc=0.7188
2025-10-13 22:42:19,686 - INFO - _models.training_function_executor - Epoch 118/190 | train_loss=1.4069 | val_loss=0.7366 | val_acc=0.7188
2025-10-13 22:42:19,691 - INFO - _models.training_function_executor - Epoch 119/190 | train_loss=1.3768 | val_loss=0.7344 | val_acc=0.7188
2025-10-13 22:42:19,696 - INFO - _models.training_function_executor - Epoch 120/190 | train_loss=1.5313 | val_loss=0.7360 | val_acc=0.7188
2025-10-13 22:42:19,701 - INFO - _models.training_function_executor - Epoch 121/190 | train_loss=1.4641 | val_loss=0.7381 | val_acc=0.7188
2025-10-13 22:42:19,706 - INFO - _models.training_function_executor - Epoch 122/190 | train_loss=1.5530 | val_loss=0.7400 | val_acc=0.7188
2025-10-13 22:42:19,712 - INFO - _models.training_function_executor - Epoch 123/190 | train_loss=1.4529 | val_loss=0.7419 | val_acc=0.7188
2025-10-13 22:42:19,717 - INFO - _models.training_function_executor - Epoch 124/190 | train_loss=1.3148 | val_loss=0.7438 | val_acc=0.7188
2025-10-13 22:42:19,722 - INFO - _models.training_function_executor - Epoch 125/190 | train_loss=1.4511 | val_loss=0.7453 | val_acc=0.7188
2025-10-13 22:42:19,727 - INFO - _models.training_function_executor - Epoch 126/190 | train_loss=1.5137 | val_loss=0.7449 | val_acc=0.7188
2025-10-13 22:42:19,732 - INFO - _models.training_function_executor - Epoch 127/190 | train_loss=1.2986 | val_loss=0.7453 | val_acc=0.7188
2025-10-13 22:42:19,737 - INFO - _models.training_function_executor - Epoch 128/190 | train_loss=1.6145 | val_loss=0.7468 | val_acc=0.7188
2025-10-13 22:42:19,742 - INFO - _models.training_function_executor - Epoch 129/190 | train_loss=1.7415 | val_loss=0.7456 | val_acc=0.7188
2025-10-13 22:42:19,747 - INFO - _models.training_function_executor - Epoch 130/190 | train_loss=1.6482 | val_loss=0.7437 | val_acc=0.7188
2025-10-13 22:42:19,752 - INFO - _models.training_function_executor - Epoch 131/190 | train_loss=1.4954 | val_loss=0.7428 | val_acc=0.7188
2025-10-13 22:42:19,757 - INFO - _models.training_function_executor - Epoch 132/190 | train_loss=1.4601 | val_loss=0.7370 | val_acc=0.7188
2025-10-13 22:42:19,762 - INFO - _models.training_function_executor - Epoch 133/190 | train_loss=1.4653 | val_loss=0.7325 | val_acc=0.7188
2025-10-13 22:42:19,767 - INFO - _models.training_function_executor - Epoch 134/190 | train_loss=1.3752 | val_loss=0.7259 | val_acc=0.7188
2025-10-13 22:42:19,772 - INFO - _models.training_function_executor - Epoch 135/190 | train_loss=1.3584 | val_loss=0.7233 | val_acc=0.7188
2025-10-13 22:42:19,777 - INFO - _models.training_function_executor - Epoch 136/190 | train_loss=1.2574 | val_loss=0.7207 | val_acc=0.7188
2025-10-13 22:42:19,782 - INFO - _models.training_function_executor - Epoch 137/190 | train_loss=1.4934 | val_loss=0.7182 | val_acc=0.7188
2025-10-13 22:42:19,787 - INFO - _models.training_function_executor - Epoch 138/190 | train_loss=1.5266 | val_loss=0.7175 | val_acc=0.7188
2025-10-13 22:42:19,792 - INFO - _models.training_function_executor - Epoch 139/190 | train_loss=1.4019 | val_loss=0.7183 | val_acc=0.7188
2025-10-13 22:42:19,797 - INFO - _models.training_function_executor - Epoch 140/190 | train_loss=1.3136 | val_loss=0.7198 | val_acc=0.7188
2025-10-13 22:42:19,802 - INFO - _models.training_function_executor - Epoch 141/190 | train_loss=1.3975 | val_loss=0.7186 | val_acc=0.7188
2025-10-13 22:42:19,807 - INFO - _models.training_function_executor - Epoch 142/190 | train_loss=1.3507 | val_loss=0.7195 | val_acc=0.7188
2025-10-13 22:42:19,812 - INFO - _models.training_function_executor - Epoch 143/190 | train_loss=1.5009 | val_loss=0.7207 | val_acc=0.7188
2025-10-13 22:42:19,817 - INFO - _models.training_function_executor - Epoch 144/190 | train_loss=1.2752 | val_loss=0.7235 | val_acc=0.7188
2025-10-13 22:42:19,822 - INFO - _models.training_function_executor - Epoch 145/190 | train_loss=1.3216 | val_loss=0.7259 | val_acc=0.7188
2025-10-13 22:42:19,828 - INFO - _models.training_function_executor - Epoch 146/190 | train_loss=1.3782 | val_loss=0.7267 | val_acc=0.7188
2025-10-13 22:42:19,833 - INFO - _models.training_function_executor - Epoch 147/190 | train_loss=1.5012 | val_loss=0.7263 | val_acc=0.7188
2025-10-13 22:42:19,838 - INFO - _models.training_function_executor - Epoch 148/190 | train_loss=1.5024 | val_loss=0.7255 | val_acc=0.7188
2025-10-13 22:42:19,843 - INFO - _models.training_function_executor - Epoch 149/190 | train_loss=1.3508 | val_loss=0.7255 | val_acc=0.7188
2025-10-13 22:42:19,849 - INFO - _models.training_function_executor - Epoch 150/190 | train_loss=1.6019 | val_loss=0.7254 | val_acc=0.7188
2025-10-13 22:42:19,855 - INFO - _models.training_function_executor - Epoch 151/190 | train_loss=1.3169 | val_loss=0.7251 | val_acc=0.7188
2025-10-13 22:42:19,861 - INFO - _models.training_function_executor - Epoch 152/190 | train_loss=1.3148 | val_loss=0.7241 | val_acc=0.7188
2025-10-13 22:42:19,866 - INFO - _models.training_function_executor - Epoch 153/190 | train_loss=1.4705 | val_loss=0.7242 | val_acc=0.7188
2025-10-13 22:42:19,872 - INFO - _models.training_function_executor - Epoch 154/190 | train_loss=1.4410 | val_loss=0.7250 | val_acc=0.7188
2025-10-13 22:42:19,877 - INFO - _models.training_function_executor - Epoch 155/190 | train_loss=1.3360 | val_loss=0.7245 | val_acc=0.7188
2025-10-13 22:42:19,884 - INFO - _models.training_function_executor - Epoch 156/190 | train_loss=1.4631 | val_loss=0.7249 | val_acc=0.7188
2025-10-13 22:42:19,889 - INFO - _models.training_function_executor - Epoch 157/190 | train_loss=1.4737 | val_loss=0.7253 | val_acc=0.7188
2025-10-13 22:42:19,895 - INFO - _models.training_function_executor - Epoch 158/190 | train_loss=1.4866 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:42:19,901 - INFO - _models.training_function_executor - Epoch 159/190 | train_loss=1.4551 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:42:19,907 - INFO - _models.training_function_executor - Epoch 160/190 | train_loss=1.4091 | val_loss=0.7253 | val_acc=0.7188
2025-10-13 22:42:19,912 - INFO - _models.training_function_executor - Epoch 161/190 | train_loss=1.4157 | val_loss=0.7254 | val_acc=0.7188
2025-10-13 22:42:19,918 - INFO - _models.training_function_executor - Epoch 162/190 | train_loss=1.3378 | val_loss=0.7250 | val_acc=0.7188
2025-10-13 22:42:19,923 - INFO - _models.training_function_executor - Epoch 163/190 | train_loss=1.6469 | val_loss=0.7252 | val_acc=0.7188
2025-10-13 22:42:19,929 - INFO - _models.training_function_executor - Epoch 164/190 | train_loss=1.2728 | val_loss=0.7253 | val_acc=0.7188
2025-10-13 22:42:19,934 - INFO - _models.training_function_executor - Epoch 165/190 | train_loss=1.3465 | val_loss=0.7254 | val_acc=0.7188
2025-10-13 22:42:19,939 - INFO - _models.training_function_executor - Epoch 166/190 | train_loss=1.6837 | val_loss=0.7248 | val_acc=0.7188
2025-10-13 22:42:19,945 - INFO - _models.training_function_executor - Epoch 167/190 | train_loss=1.5438 | val_loss=0.7245 | val_acc=0.7188
2025-10-13 22:42:19,951 - INFO - _models.training_function_executor - Epoch 168/190 | train_loss=1.3973 | val_loss=0.7247 | val_acc=0.7188
2025-10-13 22:42:19,956 - INFO - _models.training_function_executor - Epoch 169/190 | train_loss=1.2501 | val_loss=0.7247 | val_acc=0.7188
2025-10-13 22:42:19,961 - INFO - _models.training_function_executor - Epoch 170/190 | train_loss=1.6181 | val_loss=0.7247 | val_acc=0.7188
2025-10-13 22:42:19,967 - INFO - _models.training_function_executor - Epoch 171/190 | train_loss=1.2921 | val_loss=0.7248 | val_acc=0.7188
2025-10-13 22:42:19,972 - INFO - _models.training_function_executor - Epoch 172/190 | train_loss=1.5457 | val_loss=0.7251 | val_acc=0.7188
2025-10-13 22:42:19,977 - INFO - _models.training_function_executor - Epoch 173/190 | train_loss=1.3139 | val_loss=0.7254 | val_acc=0.7188
2025-10-13 22:42:19,982 - INFO - _models.training_function_executor - Epoch 174/190 | train_loss=1.4143 | val_loss=0.7256 | val_acc=0.7188
2025-10-13 22:42:19,987 - INFO - _models.training_function_executor - Epoch 175/190 | train_loss=1.5557 | val_loss=0.7256 | val_acc=0.7188
2025-10-13 22:42:19,993 - INFO - _models.training_function_executor - Epoch 176/190 | train_loss=1.4320 | val_loss=0.7259 | val_acc=0.7188
2025-10-13 22:42:19,998 - INFO - _models.training_function_executor - Epoch 177/190 | train_loss=1.4112 | val_loss=0.7259 | val_acc=0.7188
2025-10-13 22:42:20,003 - INFO - _models.training_function_executor - Epoch 178/190 | train_loss=1.4532 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:42:20,008 - INFO - _models.training_function_executor - Epoch 179/190 | train_loss=1.5621 | val_loss=0.7258 | val_acc=0.7188
2025-10-13 22:42:20,013 - INFO - _models.training_function_executor - Epoch 180/190 | train_loss=1.3495 | val_loss=0.7259 | val_acc=0.7188
2025-10-13 22:42:20,018 - INFO - _models.training_function_executor - Epoch 181/190 | train_loss=1.4279 | val_loss=0.7258 | val_acc=0.7188
2025-10-13 22:42:20,023 - INFO - _models.training_function_executor - Epoch 182/190 | train_loss=1.4268 | val_loss=0.7258 | val_acc=0.7188
2025-10-13 22:42:20,028 - INFO - _models.training_function_executor - Epoch 183/190 | train_loss=1.4746 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:42:20,034 - INFO - _models.training_function_executor - Epoch 184/190 | train_loss=1.5838 | val_loss=0.7256 | val_acc=0.7188
2025-10-13 22:42:20,039 - INFO - _models.training_function_executor - Epoch 185/190 | train_loss=1.5199 | val_loss=0.7256 | val_acc=0.7188
2025-10-13 22:42:20,044 - INFO - _models.training_function_executor - Epoch 186/190 | train_loss=1.3491 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:42:20,049 - INFO - _models.training_function_executor - Epoch 187/190 | train_loss=1.3996 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:42:20,054 - INFO - _models.training_function_executor - Epoch 188/190 | train_loss=1.4726 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:42:20,059 - INFO - _models.training_function_executor - Epoch 189/190 | train_loss=1.5274 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:42:20,065 - INFO - _models.training_function_executor - Epoch 190/190 | train_loss=1.3844 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:42:20,959 - INFO - _models.training_function_executor - Model: 32,925 parameters, 141.5KB storage
2025-10-13 22:42:20,960 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [6.409789741039276, 6.193154513835907, 5.392625689506531, 5.280211627483368, 4.617309212684631, 3.989907056093216, 3.9726953506469727, 3.5139754116535187, 3.239731878042221, 2.7464457750320435, 2.4602906107902527, 2.2381124794483185, 2.1180374920368195, 2.346636950969696, 2.095220774412155, 2.0997558534145355, 1.9801704436540604, 2.128498747944832, 1.9408018589019775, 2.097108080983162, 1.9987390637397766, 2.0443025082349777, 1.9193840324878693, 2.0071573108434677, 2.3515648245811462, 1.8063600957393646, 1.696812927722931, 1.668296530842781, 1.7428259700536728, 1.9016233384609222, 1.6931783556938171, 1.6673438549041748, 1.5886182188987732, 1.968689888715744, 1.841701328754425, 1.6511747390031815, 2.0232296884059906, 1.808340772986412, 1.8540616780519485, 1.7121684849262238, 1.7770592868328094, 1.6800214499235153, 1.7872544527053833, 1.484859973192215, 1.6453528851270676, 1.7402694076299667, 1.7889145016670227, 1.7235050052404404, 1.7376305758953094, 1.6880779564380646, 1.9007463455200195, 1.6468311548233032, 1.7367530912160873, 1.5746382772922516, 1.6599029302597046, 1.329075276851654, 1.7428878247737885, 1.4805221259593964, 1.626296117901802, 1.5040783286094666, 1.5249920040369034, 1.3819371610879898, 1.703318104147911, 1.6963309198617935, 1.6483990997076035, 1.4266167134046555, 1.5591241866350174, 1.5644741356372833, 1.6871147453784943, 1.4965035915374756, 1.6212027966976166, 1.8347016274929047, 1.5082686841487885, 1.668310672044754, 1.7120451629161835, 1.5936800837516785, 1.5338355600833893, 1.4110823273658752, 1.5427146553993225, 1.757134273648262, 1.552033081650734, 1.5899651646614075, 1.6446712911128998, 1.5341718345880508, 1.5106207728385925, 1.6560042053461075, 1.578802078962326, 1.5631479173898697, 1.5265154242515564, 1.5361157655715942, 1.3980737924575806, 1.5809609293937683, 1.8416247814893723, 1.7441471368074417, 1.4420908838510513, 1.3917050063610077, 1.4448456317186356, 1.6779815405607224, 1.3077244311571121, 1.4766647964715958, 1.5508804321289062, 1.5320070683956146, 1.6748948097229004, 1.290723368525505, 1.5155813843011856, 1.4764150828123093, 1.6575228571891785, 1.36310213804245, 1.5264623314142227, 1.598763331770897, 1.5914957523345947, 1.3728467375040054, 1.6066446155309677, 1.3588489294052124, 1.4822978079319, 1.515174761414528, 1.5897435396909714, 1.406933307647705, 1.3768281787633896, 1.5313409119844437, 1.4640906155109406, 1.5530299544334412, 1.4529154002666473, 1.3147977590560913, 1.4510889947414398, 1.5137111246585846, 1.2985931485891342, 1.614466667175293, 1.7415047138929367, 1.6481585502624512, 1.4953569769859314, 1.4601079374551773, 1.4652862399816513, 1.3752457648515701, 1.3583976328372955, 1.2574287503957748, 1.4934173226356506, 1.5266266465187073, 1.401884451508522, 1.3135968446731567, 1.3974909037351608, 1.3507498055696487, 1.5008941143751144, 1.275169923901558, 1.321552649140358, 1.3781604617834091, 1.5012128353118896, 1.5023971945047379, 1.3507719188928604, 1.6019303500652313, 1.316853478550911, 1.3148250430822372, 1.4705430418252945, 1.441035881638527, 1.3359789699316025, 1.4631403088569641, 1.4736751466989517, 1.4866120219230652, 1.4550731629133224, 1.4090547412633896, 1.4156582057476044, 1.337785229086876, 1.6469338089227676, 1.2727543860673904, 1.346492052078247, 1.6837474554777145, 1.543779045343399, 1.397298440337181, 1.2500737011432648, 1.6181258708238602, 1.2921078205108643, 1.5457063764333725, 1.313897356390953, 1.4143135398626328, 1.5557328462600708, 1.4320107698440552, 1.411210983991623, 1.4532173722982407, 1.562119498848915, 1.349514126777649, 1.4279239773750305, 1.4268345832824707, 1.474608764052391, 1.5838346928358078, 1.5198513865470886, 1.349101185798645, 1.3996251374483109, 1.4726342856884003, 1.5274453312158585, 1.384386122226715], 'val_losses': [5.314050674438477, 5.099887847900391, 4.687917232513428, 4.062906742095947, 3.4587833881378174, 2.878934860229492, 2.3431925773620605, 1.9055099487304688, 1.6220269203186035, 1.4979382753372192, 1.4909487962722778, 1.551932454109192, 1.635680913925171, 1.7069380283355713, 1.7492568492889404, 1.7391191720962524, 1.6789073944091797, 1.603602647781372, 1.5228650569915771, 1.4610644578933716, 1.4278570413589478, 1.3884657621383667, 1.3511419296264648, 1.3132883310317993, 1.2522157430648804, 1.1933865547180176, 1.1466974020004272, 1.1131324768066406, 1.0898245573043823, 1.0624181032180786, 1.0367823839187622, 1.03850257396698, 1.0560061931610107, 1.0745552778244019, 1.0700331926345825, 1.0706404447555542, 1.0549124479293823, 1.027437686920166, 1.0203570127487183, 1.0107977390289307, 0.9927443265914917, 0.9708896279335022, 0.9486704468727112, 0.9342041015625, 0.9174476265907288, 0.9009303450584412, 0.881941556930542, 0.8624647855758667, 0.8582491278648376, 0.8608147501945496, 0.8702165484428406, 0.8623905181884766, 0.8509854078292847, 0.8499113917350769, 0.8416728377342224, 0.824938952922821, 0.8062100410461426, 0.7995288968086243, 0.7875840663909912, 0.7607922554016113, 0.7514280676841736, 0.7593356966972351, 0.7658851742744446, 0.7751941680908203, 0.7891135811805725, 0.7909913063049316, 0.7894611358642578, 0.7837355732917786, 0.7751992344856262, 0.7689447999000549, 0.7733655571937561, 0.780864417552948, 0.7917000651359558, 0.7917863130569458, 0.7982661724090576, 0.7866484522819519, 0.776211678981781, 0.7655941843986511, 0.7644485831260681, 0.7627357840538025, 0.7634478211402893, 0.7622424364089966, 0.7724596858024597, 0.7770211100578308, 0.7826278805732727, 0.7864823937416077, 0.7864080667495728, 0.7887449264526367, 0.7751923203468323, 0.7667069435119629, 0.7545543909072876, 0.7518176436424255, 0.7588503956794739, 0.7619835138320923, 0.7581210732460022, 0.7469146251678467, 0.7348026037216187, 0.7302550077438354, 0.7290209531784058, 0.7324970960617065, 0.7337435483932495, 0.733218252658844, 0.7339754700660706, 0.7407099008560181, 0.7434250712394714, 0.7448315024375916, 0.7426671981811523, 0.7420394420623779, 0.7412173748016357, 0.7366586327552795, 0.7390974164009094, 0.736759603023529, 0.7421252727508545, 0.7477940917015076, 0.7462418079376221, 0.7429336309432983, 0.7388994693756104, 0.7365722060203552, 0.7344368100166321, 0.7359631061553955, 0.7380813956260681, 0.7399950623512268, 0.7418791651725769, 0.7437636256217957, 0.7453079223632812, 0.7449436187744141, 0.7453439831733704, 0.7467891573905945, 0.7455565333366394, 0.7436539530754089, 0.7427979111671448, 0.7369545102119446, 0.7325147986412048, 0.7258812785148621, 0.7233196496963501, 0.7206990718841553, 0.7181675434112549, 0.717455267906189, 0.71826171875, 0.7197945713996887, 0.7186459302902222, 0.7195374965667725, 0.7207170724868774, 0.7234524488449097, 0.7258833050727844, 0.7267377376556396, 0.7262725234031677, 0.7255166172981262, 0.7255496978759766, 0.7254281640052795, 0.7251191735267639, 0.7240565419197083, 0.7242341041564941, 0.7250140309333801, 0.7245046496391296, 0.724866509437561, 0.7252992987632751, 0.7257179617881775, 0.7257485389709473, 0.725292444229126, 0.7253905534744263, 0.725024402141571, 0.7252073884010315, 0.7252925634384155, 0.7253638505935669, 0.7248126864433289, 0.7245084047317505, 0.7247048020362854, 0.7247205972671509, 0.7246547341346741, 0.7247580885887146, 0.7251238822937012, 0.7253586053848267, 0.7256004214286804, 0.7256418466567993, 0.7259045243263245, 0.7258952856063843, 0.7257435321807861, 0.7257713675498962, 0.7258621454238892, 0.7258220314979553, 0.7258350253105164, 0.7256835699081421, 0.7256457209587097, 0.7256381511688232, 0.7256829738616943, 0.7257243990898132, 0.7257071733474731, 0.7257124781608582, 0.7257124781608582], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.203125, 0.328125, 0.40625, 0.453125, 0.578125, 0.578125, 0.640625, 0.65625, 0.640625, 0.640625, 0.640625, 0.671875, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.702824066923046e-05, 'batch_size': 96, 'epochs': 190, 'hidden_size': 177, 'dropout': 0.05667832243016542, 'weight_decay': 5.815352586746574e-05, 'label_smoothing': 0.0009692157535190706, 'grad_clip_norm': 1.944668216037016, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 32925, 'model_storage_size_kb': 141.474609375, 'model_size_validation': 'PASS'}
2025-10-13 22:42:20,960 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:42:20,960 - INFO - _models.training_function_executor - Model: 32,925 parameters, 141.5KB (PASS 256KB limit)
2025-10-13 22:42:20,960 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.591s
2025-10-13 22:42:21,030 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:42:21,030 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.069s
2025-10-13 22:42:21,030 - INFO - bo.run_bo - Recorded observation #20: hparams={'lr': 1.702824066923046e-05, 'batch_size': np.int64(96), 'epochs': np.int64(190), 'hidden_size': np.int64(177), 'dropout': 0.05667832243016542, 'weight_decay': 5.815352586746574e-05, 'label_smoothing': 0.0009692157535190706, 'grad_clip_norm': 1.944668216037016, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:42:21,030 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 20: {'lr': 1.702824066923046e-05, 'batch_size': np.int64(96), 'epochs': np.int64(190), 'hidden_size': np.int64(177), 'dropout': 0.05667832243016542, 'weight_decay': 5.815352586746574e-05, 'label_smoothing': 0.0009692157535190706, 'grad_clip_norm': 1.944668216037016, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:42:21,030 - INFO - bo.run_bo - üîçBO Trial 21: Using RF surrogate + Expected Improvement
2025-10-13 22:42:21,030 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:21,030 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 21 (NaN monitoring active)
2025-10-13 22:42:21,030 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:21,030 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:21,030 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.4462495934684917e-05, 'batch_size': 96, 'epochs': 68, 'hidden_size': 107, 'dropout': 0.043645269808585914, 'weight_decay': 6.61495300124737e-05, 'label_smoothing': 0.009486248143193878, 'grad_clip_norm': 2.596654724203868, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:21,031 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.4462495934684917e-05, 'batch_size': 96, 'epochs': 68, 'hidden_size': 107, 'dropout': 0.043645269808585914, 'weight_decay': 6.61495300124737e-05, 'label_smoothing': 0.009486248143193878, 'grad_clip_norm': 2.596654724203868, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:24,789 - INFO - _models.training_function_executor - Epoch 001/068 | train_loss=4.9320 | val_loss=4.7857 | val_acc=0.7188
2025-10-13 22:42:24,797 - INFO - _models.training_function_executor - Epoch 002/068 | train_loss=5.0447 | val_loss=4.7337 | val_acc=0.7188
2025-10-13 22:42:24,804 - INFO - _models.training_function_executor - Epoch 003/068 | train_loss=4.8348 | val_loss=4.6547 | val_acc=0.7188
2025-10-13 22:42:24,809 - INFO - _models.training_function_executor - Epoch 004/068 | train_loss=4.6468 | val_loss=4.5772 | val_acc=0.7188
2025-10-13 22:42:24,815 - INFO - _models.training_function_executor - Epoch 005/068 | train_loss=4.5567 | val_loss=4.4981 | val_acc=0.7188
2025-10-13 22:42:24,820 - INFO - _models.training_function_executor - Epoch 006/068 | train_loss=4.6410 | val_loss=4.4226 | val_acc=0.7188
2025-10-13 22:42:24,825 - INFO - _models.training_function_executor - Epoch 007/068 | train_loss=4.3703 | val_loss=4.3459 | val_acc=0.7188
2025-10-13 22:42:24,831 - INFO - _models.training_function_executor - Epoch 008/068 | train_loss=4.3177 | val_loss=4.2664 | val_acc=0.7188
2025-10-13 22:42:24,836 - INFO - _models.training_function_executor - Epoch 009/068 | train_loss=4.3903 | val_loss=4.1909 | val_acc=0.7188
2025-10-13 22:42:24,841 - INFO - _models.training_function_executor - Epoch 010/068 | train_loss=4.2071 | val_loss=4.1165 | val_acc=0.7188
2025-10-13 22:42:24,847 - INFO - _models.training_function_executor - Epoch 011/068 | train_loss=4.2340 | val_loss=4.0406 | val_acc=0.7188
2025-10-13 22:42:24,853 - INFO - _models.training_function_executor - Epoch 012/068 | train_loss=4.2544 | val_loss=3.9686 | val_acc=0.7188
2025-10-13 22:42:24,858 - INFO - _models.training_function_executor - Epoch 013/068 | train_loss=4.0332 | val_loss=3.8975 | val_acc=0.7188
2025-10-13 22:42:24,865 - INFO - _models.training_function_executor - Epoch 014/068 | train_loss=3.8731 | val_loss=3.8236 | val_acc=0.7188
2025-10-13 22:42:24,871 - INFO - _models.training_function_executor - Epoch 015/068 | train_loss=3.8231 | val_loss=3.7548 | val_acc=0.7188
2025-10-13 22:42:24,877 - INFO - _models.training_function_executor - Epoch 016/068 | train_loss=3.8016 | val_loss=3.6868 | val_acc=0.7188
2025-10-13 22:42:24,883 - INFO - _models.training_function_executor - Epoch 017/068 | train_loss=3.9578 | val_loss=3.6177 | val_acc=0.7188
2025-10-13 22:42:24,889 - INFO - _models.training_function_executor - Epoch 018/068 | train_loss=3.8416 | val_loss=3.5538 | val_acc=0.7188
2025-10-13 22:42:24,894 - INFO - _models.training_function_executor - Epoch 019/068 | train_loss=3.7078 | val_loss=3.4904 | val_acc=0.7188
2025-10-13 22:42:24,900 - INFO - _models.training_function_executor - Epoch 020/068 | train_loss=3.6941 | val_loss=3.4269 | val_acc=0.7188
2025-10-13 22:42:24,906 - INFO - _models.training_function_executor - Epoch 021/068 | train_loss=3.6455 | val_loss=3.3666 | val_acc=0.7188
2025-10-13 22:42:24,911 - INFO - _models.training_function_executor - Epoch 022/068 | train_loss=3.4008 | val_loss=3.3029 | val_acc=0.7188
2025-10-13 22:42:24,916 - INFO - _models.training_function_executor - Epoch 023/068 | train_loss=3.3316 | val_loss=3.2467 | val_acc=0.7188
2025-10-13 22:42:24,921 - INFO - _models.training_function_executor - Epoch 024/068 | train_loss=3.4245 | val_loss=3.1888 | val_acc=0.7188
2025-10-13 22:42:24,927 - INFO - _models.training_function_executor - Epoch 025/068 | train_loss=3.3801 | val_loss=3.1356 | val_acc=0.7188
2025-10-13 22:42:24,932 - INFO - _models.training_function_executor - Epoch 026/068 | train_loss=3.2682 | val_loss=3.0813 | val_acc=0.7188
2025-10-13 22:42:24,937 - INFO - _models.training_function_executor - Epoch 027/068 | train_loss=3.2784 | val_loss=3.0315 | val_acc=0.7188
2025-10-13 22:42:24,942 - INFO - _models.training_function_executor - Epoch 028/068 | train_loss=3.2068 | val_loss=2.9802 | val_acc=0.7188
2025-10-13 22:42:24,946 - INFO - _models.training_function_executor - Epoch 029/068 | train_loss=3.1606 | val_loss=2.9337 | val_acc=0.7188
2025-10-13 22:42:24,951 - INFO - _models.training_function_executor - Epoch 030/068 | train_loss=3.1923 | val_loss=2.8863 | val_acc=0.7188
2025-10-13 22:42:24,956 - INFO - _models.training_function_executor - Epoch 031/068 | train_loss=3.1197 | val_loss=2.8428 | val_acc=0.7188
2025-10-13 22:42:24,961 - INFO - _models.training_function_executor - Epoch 032/068 | train_loss=2.9613 | val_loss=2.7998 | val_acc=0.7188
2025-10-13 22:42:24,966 - INFO - _models.training_function_executor - Epoch 033/068 | train_loss=2.9715 | val_loss=2.7620 | val_acc=0.7188
2025-10-13 22:42:24,971 - INFO - _models.training_function_executor - Epoch 034/068 | train_loss=2.7577 | val_loss=2.7254 | val_acc=0.7188
2025-10-13 22:42:24,976 - INFO - _models.training_function_executor - Epoch 035/068 | train_loss=2.9747 | val_loss=2.6882 | val_acc=0.7188
2025-10-13 22:42:24,981 - INFO - _models.training_function_executor - Epoch 036/068 | train_loss=2.9065 | val_loss=2.6550 | val_acc=0.7188
2025-10-13 22:42:24,987 - INFO - _models.training_function_executor - Epoch 037/068 | train_loss=2.9734 | val_loss=2.6236 | val_acc=0.7188
2025-10-13 22:42:24,992 - INFO - _models.training_function_executor - Epoch 038/068 | train_loss=2.7736 | val_loss=2.5917 | val_acc=0.7188
2025-10-13 22:42:24,997 - INFO - _models.training_function_executor - Epoch 039/068 | train_loss=2.6841 | val_loss=2.5635 | val_acc=0.7188
2025-10-13 22:42:25,001 - INFO - _models.training_function_executor - Epoch 040/068 | train_loss=2.8034 | val_loss=2.5370 | val_acc=0.7188
2025-10-13 22:42:25,006 - INFO - _models.training_function_executor - Epoch 041/068 | train_loss=2.7510 | val_loss=2.5113 | val_acc=0.7188
2025-10-13 22:42:25,011 - INFO - _models.training_function_executor - Epoch 042/068 | train_loss=2.6492 | val_loss=2.4860 | val_acc=0.7188
2025-10-13 22:42:25,016 - INFO - _models.training_function_executor - Epoch 043/068 | train_loss=2.6036 | val_loss=2.4655 | val_acc=0.7188
2025-10-13 22:42:25,021 - INFO - _models.training_function_executor - Epoch 044/068 | train_loss=2.5612 | val_loss=2.4456 | val_acc=0.7188
2025-10-13 22:42:25,025 - INFO - _models.training_function_executor - Epoch 045/068 | train_loss=2.6627 | val_loss=2.4262 | val_acc=0.7188
2025-10-13 22:42:25,030 - INFO - _models.training_function_executor - Epoch 046/068 | train_loss=2.5264 | val_loss=2.4072 | val_acc=0.7188
2025-10-13 22:42:25,035 - INFO - _models.training_function_executor - Epoch 047/068 | train_loss=2.6942 | val_loss=2.3924 | val_acc=0.7188
2025-10-13 22:42:25,040 - INFO - _models.training_function_executor - Epoch 048/068 | train_loss=2.6331 | val_loss=2.3783 | val_acc=0.7188
2025-10-13 22:42:25,045 - INFO - _models.training_function_executor - Epoch 049/068 | train_loss=2.6720 | val_loss=2.3661 | val_acc=0.7188
2025-10-13 22:42:25,050 - INFO - _models.training_function_executor - Epoch 050/068 | train_loss=2.4773 | val_loss=2.3541 | val_acc=0.7188
2025-10-13 22:42:25,055 - INFO - _models.training_function_executor - Epoch 051/068 | train_loss=2.6856 | val_loss=2.3424 | val_acc=0.7188
2025-10-13 22:42:25,060 - INFO - _models.training_function_executor - Epoch 052/068 | train_loss=2.5930 | val_loss=2.3326 | val_acc=0.7188
2025-10-13 22:42:25,065 - INFO - _models.training_function_executor - Epoch 053/068 | train_loss=2.4924 | val_loss=2.3240 | val_acc=0.7188
2025-10-13 22:42:25,070 - INFO - _models.training_function_executor - Epoch 054/068 | train_loss=2.5300 | val_loss=2.3161 | val_acc=0.7188
2025-10-13 22:42:25,075 - INFO - _models.training_function_executor - Epoch 055/068 | train_loss=2.5784 | val_loss=2.3096 | val_acc=0.7188
2025-10-13 22:42:25,080 - INFO - _models.training_function_executor - Epoch 056/068 | train_loss=2.5098 | val_loss=2.3035 | val_acc=0.7188
2025-10-13 22:42:25,085 - INFO - _models.training_function_executor - Epoch 057/068 | train_loss=2.5773 | val_loss=2.2996 | val_acc=0.7188
2025-10-13 22:42:25,091 - INFO - _models.training_function_executor - Epoch 058/068 | train_loss=2.4004 | val_loss=2.2953 | val_acc=0.7188
2025-10-13 22:42:25,095 - INFO - _models.training_function_executor - Epoch 059/068 | train_loss=2.4963 | val_loss=2.2936 | val_acc=0.7188
2025-10-13 22:42:25,101 - INFO - _models.training_function_executor - Epoch 060/068 | train_loss=2.3759 | val_loss=2.2903 | val_acc=0.7188
2025-10-13 22:42:25,106 - INFO - _models.training_function_executor - Epoch 061/068 | train_loss=2.4310 | val_loss=2.2877 | val_acc=0.7188
2025-10-13 22:42:25,111 - INFO - _models.training_function_executor - Epoch 062/068 | train_loss=2.5440 | val_loss=2.2853 | val_acc=0.7188
2025-10-13 22:42:25,116 - INFO - _models.training_function_executor - Epoch 063/068 | train_loss=2.5621 | val_loss=2.2845 | val_acc=0.7188
2025-10-13 22:42:25,121 - INFO - _models.training_function_executor - Epoch 064/068 | train_loss=2.4685 | val_loss=2.2840 | val_acc=0.7188
2025-10-13 22:42:25,126 - INFO - _models.training_function_executor - Epoch 065/068 | train_loss=2.5698 | val_loss=2.2833 | val_acc=0.7188
2025-10-13 22:42:25,132 - INFO - _models.training_function_executor - Epoch 066/068 | train_loss=2.6021 | val_loss=2.2832 | val_acc=0.7188
2025-10-13 22:42:25,137 - INFO - _models.training_function_executor - Epoch 067/068 | train_loss=2.4550 | val_loss=2.2830 | val_acc=0.7188
2025-10-13 22:42:25,142 - INFO - _models.training_function_executor - Epoch 068/068 | train_loss=2.6215 | val_loss=2.2831 | val_acc=0.7188
2025-10-13 22:42:26,017 - INFO - _models.training_function_executor - Model: 12,415 parameters, 26.7KB storage
2025-10-13 22:42:26,017 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [4.932009339332581, 5.04474800825119, 4.83481764793396, 4.646786630153656, 4.556740999221802, 4.641049027442932, 4.370295882225037, 4.317736387252808, 4.390312999486923, 4.207099199295044, 4.233976811170578, 4.254446059465408, 4.0331796407699585, 3.8730686008930206, 3.823136866092682, 3.801577478647232, 3.957762658596039, 3.8415834307670593, 3.7078442573547363, 3.694137752056122, 3.645533472299576, 3.4007848501205444, 3.3316429555416107, 3.424534410238266, 3.380059152841568, 3.268227994441986, 3.278359532356262, 3.2067841291427612, 3.160580277442932, 3.1922596991062164, 3.119655430316925, 2.9613057076931, 2.971454083919525, 2.7577213644981384, 2.974697843194008, 2.9065120816230774, 2.9734378457069397, 2.7736414074897766, 2.6840823590755463, 2.8033508956432343, 2.75098192691803, 2.649237275123596, 2.60357466340065, 2.5611974000930786, 2.6627448201179504, 2.5263792276382446, 2.694152355194092, 2.6331416964530945, 2.671957314014435, 2.4772902131080627, 2.685621052980423, 2.593042343854904, 2.492416590452194, 2.529985398054123, 2.578392505645752, 2.5097816586494446, 2.577261060476303, 2.4003635942935944, 2.496258467435837, 2.375858813524246, 2.4309614300727844, 2.5440181493759155, 2.5621349811553955, 2.4684920608997345, 2.5698082000017166, 2.602124661207199, 2.4549971520900726, 2.621497690677643], 'val_losses': [4.785740375518799, 4.7336859703063965, 4.654722690582275, 4.5772385597229, 4.498053073883057, 4.422595024108887, 4.345932960510254, 4.266362190246582, 4.190871238708496, 4.116458892822266, 4.0406270027160645, 3.9685750007629395, 3.897515296936035, 3.8235726356506348, 3.7548348903656006, 3.6867873668670654, 3.6177046298980713, 3.5537753105163574, 3.490363597869873, 3.4269425868988037, 3.3666155338287354, 3.302891492843628, 3.246655225753784, 3.1888175010681152, 3.1356420516967773, 3.0813112258911133, 3.031454086303711, 2.980238676071167, 2.9336941242218018, 2.8862991333007812, 2.8428080081939697, 2.799832344055176, 2.7619948387145996, 2.725390911102295, 2.6881837844848633, 2.655043840408325, 2.6236088275909424, 2.591733694076538, 2.563479423522949, 2.536973237991333, 2.5113096237182617, 2.4859955310821533, 2.4655144214630127, 2.445561647415161, 2.4261860847473145, 2.407205581665039, 2.392371892929077, 2.3783295154571533, 2.3661091327667236, 2.354105234146118, 2.3424205780029297, 2.3325533866882324, 2.3239777088165283, 2.3160860538482666, 2.3096237182617188, 2.3035290241241455, 2.2996368408203125, 2.295304298400879, 2.2936031818389893, 2.290250062942505, 2.287729501724243, 2.285348415374756, 2.28448224067688, 2.284036636352539, 2.2833080291748047, 2.2832441329956055, 2.2830052375793457, 2.2830655574798584], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.4462495934684917e-05, 'batch_size': 96, 'epochs': 68, 'hidden_size': 107, 'dropout': 0.043645269808585914, 'weight_decay': 6.61495300124737e-05, 'label_smoothing': 0.009486248143193878, 'grad_clip_norm': 2.596654724203868, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 12415, 'model_storage_size_kb': 26.672851562500004, 'model_size_validation': 'PASS'}
2025-10-13 22:42:26,017 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:42:26,017 - INFO - _models.training_function_executor - Model: 12,415 parameters, 26.7KB (PASS 256KB limit)
2025-10-13 22:42:26,017 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.986s
2025-10-13 22:42:26,437 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:42:26,437 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.419s
2025-10-13 22:42:26,437 - INFO - bo.run_bo - Recorded observation #21: hparams={'lr': 1.4462495934684917e-05, 'batch_size': np.int64(96), 'epochs': np.int64(68), 'hidden_size': np.int64(107), 'dropout': 0.043645269808585914, 'weight_decay': 6.61495300124737e-05, 'label_smoothing': 0.009486248143193878, 'grad_clip_norm': 2.596654724203868, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:42:26,437 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 21: {'lr': 1.4462495934684917e-05, 'batch_size': np.int64(96), 'epochs': np.int64(68), 'hidden_size': np.int64(107), 'dropout': 0.043645269808585914, 'weight_decay': 6.61495300124737e-05, 'label_smoothing': 0.009486248143193878, 'grad_clip_norm': 2.596654724203868, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:42:26,437 - INFO - bo.run_bo - üîçBO Trial 22: Using RF surrogate + Expected Improvement
2025-10-13 22:42:26,437 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:26,437 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 22 (NaN monitoring active)
2025-10-13 22:42:26,437 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:26,437 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:26,437 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.1826230036826168e-05, 'batch_size': 96, 'epochs': 29, 'hidden_size': 165, 'dropout': 0.09085982151453505, 'weight_decay': 0.000245338883934438, 'label_smoothing': 0.19480250416505557, 'grad_clip_norm': 1.711319049504417, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:26,438 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.1826230036826168e-05, 'batch_size': 96, 'epochs': 29, 'hidden_size': 165, 'dropout': 0.09085982151453505, 'weight_decay': 0.000245338883934438, 'label_smoothing': 0.19480250416505557, 'grad_clip_norm': 1.711319049504417, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:30,181 - INFO - _models.training_function_executor - Epoch 001/029 | train_loss=6.1325 | val_loss=5.9816 | val_acc=0.0938
2025-10-13 22:42:30,188 - INFO - _models.training_function_executor - Epoch 002/029 | train_loss=5.8078 | val_loss=5.8775 | val_acc=0.0938
2025-10-13 22:42:30,196 - INFO - _models.training_function_executor - Epoch 003/029 | train_loss=6.3092 | val_loss=5.5603 | val_acc=0.0938
2025-10-13 22:42:30,202 - INFO - _models.training_function_executor - Epoch 004/029 | train_loss=5.5819 | val_loss=5.2457 | val_acc=0.0938
2025-10-13 22:42:30,208 - INFO - _models.training_function_executor - Epoch 005/029 | train_loss=5.3825 | val_loss=4.9443 | val_acc=0.0938
2025-10-13 22:42:30,213 - INFO - _models.training_function_executor - Epoch 006/029 | train_loss=5.5930 | val_loss=4.6480 | val_acc=0.0938
2025-10-13 22:42:30,218 - INFO - _models.training_function_executor - Epoch 007/029 | train_loss=4.9940 | val_loss=4.3653 | val_acc=0.0938
2025-10-13 22:42:30,224 - INFO - _models.training_function_executor - Epoch 008/029 | train_loss=4.7173 | val_loss=4.0995 | val_acc=0.0938
2025-10-13 22:42:30,229 - INFO - _models.training_function_executor - Epoch 009/029 | train_loss=4.9107 | val_loss=3.8460 | val_acc=0.0938
2025-10-13 22:42:30,235 - INFO - _models.training_function_executor - Epoch 010/029 | train_loss=4.3491 | val_loss=3.6058 | val_acc=0.0938
2025-10-13 22:42:30,240 - INFO - _models.training_function_executor - Epoch 011/029 | train_loss=4.1283 | val_loss=3.3887 | val_acc=0.0938
2025-10-13 22:42:30,246 - INFO - _models.training_function_executor - Epoch 012/029 | train_loss=4.0160 | val_loss=3.1866 | val_acc=0.0938
2025-10-13 22:42:30,251 - INFO - _models.training_function_executor - Epoch 013/029 | train_loss=3.5277 | val_loss=3.0016 | val_acc=0.0938
2025-10-13 22:42:30,256 - INFO - _models.training_function_executor - Epoch 014/029 | train_loss=3.7176 | val_loss=2.8360 | val_acc=0.1094
2025-10-13 22:42:30,262 - INFO - _models.training_function_executor - Epoch 015/029 | train_loss=3.5941 | val_loss=2.6850 | val_acc=0.1250
2025-10-13 22:42:30,268 - INFO - _models.training_function_executor - Epoch 016/029 | train_loss=3.6318 | val_loss=2.5544 | val_acc=0.1719
2025-10-13 22:42:30,274 - INFO - _models.training_function_executor - Epoch 017/029 | train_loss=3.3211 | val_loss=2.4422 | val_acc=0.2031
2025-10-13 22:42:30,279 - INFO - _models.training_function_executor - Epoch 018/029 | train_loss=3.1096 | val_loss=2.3449 | val_acc=0.2031
2025-10-13 22:42:30,285 - INFO - _models.training_function_executor - Epoch 019/029 | train_loss=3.4873 | val_loss=2.2608 | val_acc=0.2344
2025-10-13 22:42:30,290 - INFO - _models.training_function_executor - Epoch 020/029 | train_loss=3.4588 | val_loss=2.1915 | val_acc=0.2344
2025-10-13 22:42:30,295 - INFO - _models.training_function_executor - Epoch 021/029 | train_loss=3.2336 | val_loss=2.1388 | val_acc=0.2500
2025-10-13 22:42:30,300 - INFO - _models.training_function_executor - Epoch 022/029 | train_loss=2.9269 | val_loss=2.0950 | val_acc=0.2500
2025-10-13 22:42:30,306 - INFO - _models.training_function_executor - Epoch 023/029 | train_loss=3.2078 | val_loss=2.0605 | val_acc=0.2500
2025-10-13 22:42:30,311 - INFO - _models.training_function_executor - Epoch 024/029 | train_loss=3.3678 | val_loss=2.0357 | val_acc=0.2656
2025-10-13 22:42:30,316 - INFO - _models.training_function_executor - Epoch 025/029 | train_loss=2.7550 | val_loss=2.0176 | val_acc=0.2656
2025-10-13 22:42:30,321 - INFO - _models.training_function_executor - Epoch 026/029 | train_loss=3.0719 | val_loss=2.0075 | val_acc=0.2656
2025-10-13 22:42:30,326 - INFO - _models.training_function_executor - Epoch 027/029 | train_loss=3.5612 | val_loss=2.0010 | val_acc=0.2656
2025-10-13 22:42:30,332 - INFO - _models.training_function_executor - Epoch 028/029 | train_loss=3.0939 | val_loss=1.9978 | val_acc=0.2812
2025-10-13 22:42:30,337 - INFO - _models.training_function_executor - Epoch 029/029 | train_loss=2.9097 | val_loss=1.9968 | val_acc=0.2812
2025-10-13 22:42:31,276 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-10-13 22:42:31,276 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [6.1325132846832275, 5.807770073413849, 6.309162259101868, 5.581944584846497, 5.382466077804565, 5.593013048171997, 4.9940385222435, 4.717260956764221, 4.910734951496124, 4.349141299724579, 4.128340065479279, 4.016030997037888, 3.5276697278022766, 3.7175795435905457, 3.594132363796234, 3.6318196654319763, 3.3210741877555847, 3.109635144472122, 3.4873478412628174, 3.458805948495865, 3.233572393655777, 2.926890552043915, 3.2078394889831543, 3.36779922246933, 2.7549740970134735, 3.071933627128601, 3.5612185895442963, 3.0939285159111023, 2.9096958339214325], 'val_losses': [5.981574058532715, 5.877453804016113, 5.560329914093018, 5.245700359344482, 4.944323539733887, 4.648044586181641, 4.3653483390808105, 4.099524974822998, 3.8459854125976562, 3.60581636428833, 3.3887081146240234, 3.1866073608398438, 3.001621723175049, 2.8360085487365723, 2.6849899291992188, 2.5543556213378906, 2.442193031311035, 2.344902992248535, 2.260751724243164, 2.1915102005004883, 2.1388156414031982, 2.094972848892212, 2.060452461242676, 2.035651922225952, 2.0175881385803223, 2.0074658393859863, 2.000998020172119, 1.9977827072143555, 1.9967964887619019], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.109375, 0.125, 0.171875, 0.203125, 0.203125, 0.234375, 0.234375, 0.25, 0.25, 0.25, 0.265625, 0.265625, 0.265625, 0.265625, 0.28125, 0.28125], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.1826230036826168e-05, 'batch_size': 96, 'epochs': 29, 'hidden_size': 165, 'dropout': 0.09085982151453505, 'weight_decay': 0.000245338883934438, 'label_smoothing': 0.19480250416505557, 'grad_clip_norm': 1.711319049504417, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-10-13 22:42:31,276 - INFO - _models.training_function_executor - BO Objective: base=0.2812, size_penalty=0.0000, final=0.2812
2025-10-13 22:42:31,276 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-10-13 22:42:31,276 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.839s
2025-10-13 22:42:31,349 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2812
2025-10-13 22:42:31,349 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.072s
2025-10-13 22:42:31,349 - INFO - bo.run_bo - Recorded observation #22: hparams={'lr': 1.1826230036826168e-05, 'batch_size': np.int64(96), 'epochs': np.int64(29), 'hidden_size': np.int64(165), 'dropout': 0.09085982151453505, 'weight_decay': 0.000245338883934438, 'label_smoothing': 0.19480250416505557, 'grad_clip_norm': 1.711319049504417, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.2812
2025-10-13 22:42:31,349 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 22: {'lr': 1.1826230036826168e-05, 'batch_size': np.int64(96), 'epochs': np.int64(29), 'hidden_size': np.int64(165), 'dropout': 0.09085982151453505, 'weight_decay': 0.000245338883934438, 'label_smoothing': 0.19480250416505557, 'grad_clip_norm': 1.711319049504417, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.2812
2025-10-13 22:42:31,349 - INFO - bo.run_bo - üîçBO Trial 23: Using RF surrogate + Expected Improvement
2025-10-13 22:42:31,349 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:31,349 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 23 (NaN monitoring active)
2025-10-13 22:42:31,349 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:31,349 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:31,349 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0005837439545554e-05, 'batch_size': 96, 'epochs': 59, 'hidden_size': 60, 'dropout': 0.15792739133747127, 'weight_decay': 2.126316452291405e-05, 'label_smoothing': 0.011504213784301245, 'grad_clip_norm': 1.6528632020844085, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:31,350 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0005837439545554e-05, 'batch_size': 96, 'epochs': 59, 'hidden_size': 60, 'dropout': 0.15792739133747127, 'weight_decay': 2.126316452291405e-05, 'label_smoothing': 0.011504213784301245, 'grad_clip_norm': 1.6528632020844085, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:35,028 - INFO - _models.training_function_executor - Epoch 001/059 | train_loss=4.0185 | val_loss=3.7623 | val_acc=0.7188
2025-10-13 22:42:35,035 - INFO - _models.training_function_executor - Epoch 002/059 | train_loss=3.8935 | val_loss=3.7478 | val_acc=0.7188
2025-10-13 22:42:35,041 - INFO - _models.training_function_executor - Epoch 003/059 | train_loss=3.7620 | val_loss=3.7261 | val_acc=0.7188
2025-10-13 22:42:35,047 - INFO - _models.training_function_executor - Epoch 004/059 | train_loss=3.9896 | val_loss=3.7061 | val_acc=0.7188
2025-10-13 22:42:35,053 - INFO - _models.training_function_executor - Epoch 005/059 | train_loss=3.5365 | val_loss=3.6796 | val_acc=0.7188
2025-10-13 22:42:35,058 - INFO - _models.training_function_executor - Epoch 006/059 | train_loss=4.0565 | val_loss=3.6603 | val_acc=0.7188
2025-10-13 22:42:35,064 - INFO - _models.training_function_executor - Epoch 007/059 | train_loss=3.4275 | val_loss=3.6377 | val_acc=0.7188
2025-10-13 22:42:35,070 - INFO - _models.training_function_executor - Epoch 008/059 | train_loss=3.7710 | val_loss=3.6177 | val_acc=0.7188
2025-10-13 22:42:35,075 - INFO - _models.training_function_executor - Epoch 009/059 | train_loss=3.6267 | val_loss=3.5956 | val_acc=0.7188
2025-10-13 22:42:35,081 - INFO - _models.training_function_executor - Epoch 010/059 | train_loss=3.8055 | val_loss=3.5760 | val_acc=0.7188
2025-10-13 22:42:35,086 - INFO - _models.training_function_executor - Epoch 011/059 | train_loss=3.7930 | val_loss=3.5550 | val_acc=0.7188
2025-10-13 22:42:35,092 - INFO - _models.training_function_executor - Epoch 012/059 | train_loss=3.6886 | val_loss=3.5336 | val_acc=0.7188
2025-10-13 22:42:35,097 - INFO - _models.training_function_executor - Epoch 013/059 | train_loss=3.8777 | val_loss=3.5143 | val_acc=0.7188
2025-10-13 22:42:35,102 - INFO - _models.training_function_executor - Epoch 014/059 | train_loss=3.4094 | val_loss=3.4941 | val_acc=0.7188
2025-10-13 22:42:35,107 - INFO - _models.training_function_executor - Epoch 015/059 | train_loss=3.7720 | val_loss=3.4764 | val_acc=0.7188
2025-10-13 22:42:35,113 - INFO - _models.training_function_executor - Epoch 016/059 | train_loss=3.6165 | val_loss=3.4582 | val_acc=0.7188
2025-10-13 22:42:35,120 - INFO - _models.training_function_executor - Epoch 017/059 | train_loss=4.0678 | val_loss=3.4408 | val_acc=0.7188
2025-10-13 22:42:35,125 - INFO - _models.training_function_executor - Epoch 018/059 | train_loss=3.6089 | val_loss=3.4235 | val_acc=0.7188
2025-10-13 22:42:35,131 - INFO - _models.training_function_executor - Epoch 019/059 | train_loss=3.7990 | val_loss=3.4055 | val_acc=0.7188
2025-10-13 22:42:35,136 - INFO - _models.training_function_executor - Epoch 020/059 | train_loss=3.7989 | val_loss=3.3905 | val_acc=0.7188
2025-10-13 22:42:35,141 - INFO - _models.training_function_executor - Epoch 021/059 | train_loss=3.2429 | val_loss=3.3740 | val_acc=0.7188
2025-10-13 22:42:35,146 - INFO - _models.training_function_executor - Epoch 022/059 | train_loss=3.1583 | val_loss=3.3572 | val_acc=0.7188
2025-10-13 22:42:35,152 - INFO - _models.training_function_executor - Epoch 023/059 | train_loss=3.4911 | val_loss=3.3428 | val_acc=0.7188
2025-10-13 22:42:35,158 - INFO - _models.training_function_executor - Epoch 024/059 | train_loss=3.1759 | val_loss=3.3291 | val_acc=0.7188
2025-10-13 22:42:35,163 - INFO - _models.training_function_executor - Epoch 025/059 | train_loss=3.6886 | val_loss=3.3153 | val_acc=0.7188
2025-10-13 22:42:35,169 - INFO - _models.training_function_executor - Epoch 026/059 | train_loss=3.7005 | val_loss=3.3021 | val_acc=0.7188
2025-10-13 22:42:35,175 - INFO - _models.training_function_executor - Epoch 027/059 | train_loss=3.5755 | val_loss=3.2889 | val_acc=0.7188
2025-10-13 22:42:35,181 - INFO - _models.training_function_executor - Epoch 028/059 | train_loss=3.4695 | val_loss=3.2767 | val_acc=0.7188
2025-10-13 22:42:35,187 - INFO - _models.training_function_executor - Epoch 029/059 | train_loss=3.6744 | val_loss=3.2669 | val_acc=0.7188
2025-10-13 22:42:35,193 - INFO - _models.training_function_executor - Epoch 030/059 | train_loss=3.5981 | val_loss=3.2563 | val_acc=0.7188
2025-10-13 22:42:35,200 - INFO - _models.training_function_executor - Epoch 031/059 | train_loss=3.3047 | val_loss=3.2466 | val_acc=0.7188
2025-10-13 22:42:35,206 - INFO - _models.training_function_executor - Epoch 032/059 | train_loss=3.6383 | val_loss=3.2357 | val_acc=0.7188
2025-10-13 22:42:35,212 - INFO - _models.training_function_executor - Epoch 033/059 | train_loss=3.6265 | val_loss=3.2282 | val_acc=0.7188
2025-10-13 22:42:35,218 - INFO - _models.training_function_executor - Epoch 034/059 | train_loss=3.4380 | val_loss=3.2201 | val_acc=0.7188
2025-10-13 22:42:35,224 - INFO - _models.training_function_executor - Epoch 035/059 | train_loss=3.3156 | val_loss=3.2120 | val_acc=0.7188
2025-10-13 22:42:35,229 - INFO - _models.training_function_executor - Epoch 036/059 | train_loss=3.6524 | val_loss=3.2044 | val_acc=0.7188
2025-10-13 22:42:35,235 - INFO - _models.training_function_executor - Epoch 037/059 | train_loss=3.3827 | val_loss=3.1982 | val_acc=0.7188
2025-10-13 22:42:35,240 - INFO - _models.training_function_executor - Epoch 038/059 | train_loss=3.4068 | val_loss=3.1905 | val_acc=0.7188
2025-10-13 22:42:35,247 - INFO - _models.training_function_executor - Epoch 039/059 | train_loss=3.6632 | val_loss=3.1850 | val_acc=0.7188
2025-10-13 22:42:35,252 - INFO - _models.training_function_executor - Epoch 040/059 | train_loss=3.2723 | val_loss=3.1797 | val_acc=0.7188
2025-10-13 22:42:35,257 - INFO - _models.training_function_executor - Epoch 041/059 | train_loss=3.2245 | val_loss=3.1748 | val_acc=0.7188
2025-10-13 22:42:35,262 - INFO - _models.training_function_executor - Epoch 042/059 | train_loss=3.3031 | val_loss=3.1704 | val_acc=0.7188
2025-10-13 22:42:35,267 - INFO - _models.training_function_executor - Epoch 043/059 | train_loss=3.4837 | val_loss=3.1662 | val_acc=0.7188
2025-10-13 22:42:35,272 - INFO - _models.training_function_executor - Epoch 044/059 | train_loss=3.4750 | val_loss=3.1636 | val_acc=0.7188
2025-10-13 22:42:35,277 - INFO - _models.training_function_executor - Epoch 045/059 | train_loss=3.2514 | val_loss=3.1605 | val_acc=0.7188
2025-10-13 22:42:35,282 - INFO - _models.training_function_executor - Epoch 046/059 | train_loss=3.7521 | val_loss=3.1576 | val_acc=0.7188
2025-10-13 22:42:35,287 - INFO - _models.training_function_executor - Epoch 047/059 | train_loss=3.7365 | val_loss=3.1550 | val_acc=0.7188
2025-10-13 22:42:35,292 - INFO - _models.training_function_executor - Epoch 048/059 | train_loss=3.5131 | val_loss=3.1529 | val_acc=0.7188
2025-10-13 22:42:35,297 - INFO - _models.training_function_executor - Epoch 049/059 | train_loss=3.5234 | val_loss=3.1510 | val_acc=0.7188
2025-10-13 22:42:35,302 - INFO - _models.training_function_executor - Epoch 050/059 | train_loss=3.3740 | val_loss=3.1490 | val_acc=0.7188
2025-10-13 22:42:35,306 - INFO - _models.training_function_executor - Epoch 051/059 | train_loss=3.4041 | val_loss=3.1482 | val_acc=0.7188
2025-10-13 22:42:35,311 - INFO - _models.training_function_executor - Epoch 052/059 | train_loss=3.4334 | val_loss=3.1470 | val_acc=0.7188
2025-10-13 22:42:35,316 - INFO - _models.training_function_executor - Epoch 053/059 | train_loss=3.4103 | val_loss=3.1461 | val_acc=0.7188
2025-10-13 22:42:35,321 - INFO - _models.training_function_executor - Epoch 054/059 | train_loss=3.6633 | val_loss=3.1459 | val_acc=0.7188
2025-10-13 22:42:35,326 - INFO - _models.training_function_executor - Epoch 055/059 | train_loss=3.3947 | val_loss=3.1458 | val_acc=0.7188
2025-10-13 22:42:35,331 - INFO - _models.training_function_executor - Epoch 056/059 | train_loss=3.1248 | val_loss=3.1458 | val_acc=0.7188
2025-10-13 22:42:35,336 - INFO - _models.training_function_executor - Epoch 057/059 | train_loss=3.1500 | val_loss=3.1457 | val_acc=0.7188
2025-10-13 22:42:35,340 - INFO - _models.training_function_executor - Epoch 058/059 | train_loss=3.1170 | val_loss=3.1455 | val_acc=0.7188
2025-10-13 22:42:35,345 - INFO - _models.training_function_executor - Epoch 059/059 | train_loss=3.2225 | val_loss=3.1455 | val_acc=0.7188
2025-10-13 22:42:36,228 - INFO - _models.training_function_executor - Model: 4,143 parameters, 8.9KB storage
2025-10-13 22:42:36,228 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [4.018536627292633, 3.893482983112335, 3.7619699239730835, 3.9895880818367004, 3.5365112125873566, 4.056473016738892, 3.42752006649971, 3.7709901034832, 3.626733660697937, 3.8054893612861633, 3.793022572994232, 3.6886011362075806, 3.877725660800934, 3.4093722701072693, 3.772036910057068, 3.6165228486061096, 4.067842453718185, 3.60886013507843, 3.79899999499321, 3.7989454567432404, 3.242918610572815, 3.1582656800746918, 3.4911170303821564, 3.1759192645549774, 3.6885657012462616, 3.700538158416748, 3.575522243976593, 3.4695001542568207, 3.6743674874305725, 3.5981357097625732, 3.3047419488430023, 3.6382645964622498, 3.6264681816101074, 3.438029021024704, 3.3155952095985413, 3.652411460876465, 3.382680743932724, 3.4068472385406494, 3.6631644666194916, 3.2722527980804443, 3.224530816078186, 3.3031271398067474, 3.483669102191925, 3.475033074617386, 3.2513615489006042, 3.7520779371261597, 3.7364812791347504, 3.513108789920807, 3.523368537425995, 3.373982757329941, 3.4041362404823303, 3.4333759248256683, 3.410303682088852, 3.6632931232452393, 3.394713580608368, 3.1248322129249573, 3.1500005424022675, 3.1169522404670715, 3.222483456134796], 'val_losses': [3.7623331546783447, 3.747817039489746, 3.726091146469116, 3.7061498165130615, 3.679612874984741, 3.6602985858917236, 3.637746810913086, 3.6176562309265137, 3.5955591201782227, 3.576021194458008, 3.5549633502960205, 3.533550977706909, 3.5142812728881836, 3.494091272354126, 3.4764277935028076, 3.458218812942505, 3.4407870769500732, 3.4234824180603027, 3.405460834503174, 3.3904919624328613, 3.3739943504333496, 3.3572208881378174, 3.3428120613098145, 3.3291211128234863, 3.3153235912323, 3.3020968437194824, 3.288933753967285, 3.276681900024414, 3.266887903213501, 3.2562780380249023, 3.246605634689331, 3.2357099056243896, 3.22818922996521, 3.2201268672943115, 3.2120108604431152, 3.2044172286987305, 3.198181390762329, 3.190488576889038, 3.1850380897521973, 3.1797068119049072, 3.174816608428955, 3.1703951358795166, 3.166226863861084, 3.1635549068450928, 3.160515308380127, 3.1575613021850586, 3.155033588409424, 3.152926445007324, 3.1509852409362793, 3.1490437984466553, 3.148188591003418, 3.1470398902893066, 3.146139621734619, 3.1458961963653564, 3.145775318145752, 3.1457724571228027, 3.1457138061523438, 3.145474910736084, 3.145474910736084], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0005837439545554e-05, 'batch_size': 96, 'epochs': 59, 'hidden_size': 60, 'dropout': 0.15792739133747127, 'weight_decay': 2.126316452291405e-05, 'label_smoothing': 0.011504213784301245, 'grad_clip_norm': 1.6528632020844085, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 4143, 'model_storage_size_kb': 8.9009765625, 'model_size_validation': 'PASS'}
2025-10-13 22:42:36,228 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:42:36,228 - INFO - _models.training_function_executor - Model: 4,143 parameters, 8.9KB (PASS 256KB limit)
2025-10-13 22:42:36,228 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.879s
2025-10-13 22:42:36,301 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:42:36,301 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.073s
2025-10-13 22:42:36,301 - INFO - bo.run_bo - Recorded observation #23: hparams={'lr': 1.0005837439545554e-05, 'batch_size': np.int64(96), 'epochs': np.int64(59), 'hidden_size': np.int64(60), 'dropout': 0.15792739133747127, 'weight_decay': 2.126316452291405e-05, 'label_smoothing': 0.011504213784301245, 'grad_clip_norm': 1.6528632020844085, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:42:36,301 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 23: {'lr': 1.0005837439545554e-05, 'batch_size': np.int64(96), 'epochs': np.int64(59), 'hidden_size': np.int64(60), 'dropout': 0.15792739133747127, 'weight_decay': 2.126316452291405e-05, 'label_smoothing': 0.011504213784301245, 'grad_clip_norm': 1.6528632020844085, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:42:36,302 - INFO - bo.run_bo - üîçBO Trial 24: Using RF surrogate + Expected Improvement
2025-10-13 22:42:36,302 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:36,302 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 24 (NaN monitoring active)
2025-10-13 22:42:36,302 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:36,302 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:36,302 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 5.437872154118726e-05, 'batch_size': 64, 'epochs': 28, 'hidden_size': 145, 'dropout': 0.11292026759493132, 'weight_decay': 2.4101483630335058e-05, 'label_smoothing': 0.07520786392143729, 'grad_clip_norm': 2.4802641243031642, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:36,303 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 5.437872154118726e-05, 'batch_size': 64, 'epochs': 28, 'hidden_size': 145, 'dropout': 0.11292026759493132, 'weight_decay': 2.4101483630335058e-05, 'label_smoothing': 0.07520786392143729, 'grad_clip_norm': 2.4802641243031642, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:40,000 - INFO - _models.training_function_executor - Epoch 001/028 | train_loss=5.0610 | val_loss=3.3687 | val_acc=0.1875
2025-10-13 22:42:40,008 - INFO - _models.training_function_executor - Epoch 002/028 | train_loss=5.3956 | val_loss=2.4335 | val_acc=0.2031
2025-10-13 22:42:40,015 - INFO - _models.training_function_executor - Epoch 003/028 | train_loss=3.7951 | val_loss=1.3714 | val_acc=0.2500
2025-10-13 22:42:40,022 - INFO - _models.training_function_executor - Epoch 004/028 | train_loss=2.8642 | val_loss=0.8200 | val_acc=0.6719
2025-10-13 22:42:40,028 - INFO - _models.training_function_executor - Epoch 005/028 | train_loss=2.2838 | val_loss=1.0423 | val_acc=0.7188
2025-10-13 22:42:40,034 - INFO - _models.training_function_executor - Epoch 006/028 | train_loss=2.1154 | val_loss=1.4352 | val_acc=0.7188
2025-10-13 22:42:40,041 - INFO - _models.training_function_executor - Epoch 007/028 | train_loss=2.2885 | val_loss=1.6145 | val_acc=0.7188
2025-10-13 22:42:40,047 - INFO - _models.training_function_executor - Epoch 008/028 | train_loss=2.0517 | val_loss=1.5744 | val_acc=0.7188
2025-10-13 22:42:40,054 - INFO - _models.training_function_executor - Epoch 009/028 | train_loss=2.1509 | val_loss=1.3923 | val_acc=0.7188
2025-10-13 22:42:40,060 - INFO - _models.training_function_executor - Epoch 010/028 | train_loss=2.1427 | val_loss=1.2455 | val_acc=0.7188
2025-10-13 22:42:40,067 - INFO - _models.training_function_executor - Epoch 011/028 | train_loss=2.0253 | val_loss=1.1533 | val_acc=0.7188
2025-10-13 22:42:40,074 - INFO - _models.training_function_executor - Epoch 012/028 | train_loss=2.1695 | val_loss=1.0858 | val_acc=0.7188
2025-10-13 22:42:40,080 - INFO - _models.training_function_executor - Epoch 013/028 | train_loss=2.0791 | val_loss=1.0931 | val_acc=0.7188
2025-10-13 22:42:40,087 - INFO - _models.training_function_executor - Epoch 014/028 | train_loss=2.1711 | val_loss=1.1642 | val_acc=0.7188
2025-10-13 22:42:40,094 - INFO - _models.training_function_executor - Epoch 015/028 | train_loss=2.0639 | val_loss=1.1997 | val_acc=0.7188
2025-10-13 22:42:40,101 - INFO - _models.training_function_executor - Epoch 016/028 | train_loss=2.2250 | val_loss=1.2394 | val_acc=0.7188
2025-10-13 22:42:40,108 - INFO - _models.training_function_executor - Epoch 017/028 | train_loss=1.8510 | val_loss=1.2752 | val_acc=0.7188
2025-10-13 22:42:40,115 - INFO - _models.training_function_executor - Epoch 018/028 | train_loss=2.0823 | val_loss=1.2658 | val_acc=0.7188
2025-10-13 22:42:40,122 - INFO - _models.training_function_executor - Epoch 019/028 | train_loss=1.9044 | val_loss=1.2620 | val_acc=0.7188
2025-10-13 22:42:40,129 - INFO - _models.training_function_executor - Epoch 020/028 | train_loss=1.9180 | val_loss=1.2465 | val_acc=0.7188
2025-10-13 22:42:40,136 - INFO - _models.training_function_executor - Epoch 021/028 | train_loss=1.9845 | val_loss=1.2409 | val_acc=0.7188
2025-10-13 22:42:40,143 - INFO - _models.training_function_executor - Epoch 022/028 | train_loss=1.7771 | val_loss=1.2251 | val_acc=0.7188
2025-10-13 22:42:40,150 - INFO - _models.training_function_executor - Epoch 023/028 | train_loss=1.7680 | val_loss=1.2162 | val_acc=0.7188
2025-10-13 22:42:40,157 - INFO - _models.training_function_executor - Epoch 024/028 | train_loss=1.9698 | val_loss=1.2090 | val_acc=0.7188
2025-10-13 22:42:40,163 - INFO - _models.training_function_executor - Epoch 025/028 | train_loss=2.0832 | val_loss=1.2023 | val_acc=0.7188
2025-10-13 22:42:40,170 - INFO - _models.training_function_executor - Epoch 026/028 | train_loss=1.9659 | val_loss=1.2006 | val_acc=0.7188
2025-10-13 22:42:40,176 - INFO - _models.training_function_executor - Epoch 027/028 | train_loss=1.9300 | val_loss=1.1990 | val_acc=0.7188
2025-10-13 22:42:40,183 - INFO - _models.training_function_executor - Epoch 028/028 | train_loss=1.9647 | val_loss=1.1988 | val_acc=0.7188
2025-10-13 22:42:41,080 - INFO - _models.training_function_executor - Model: 22,333 parameters, 48.0KB storage
2025-10-13 22:42:41,080 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [5.060964584350586, 5.395553708076477, 3.7950984835624695, 2.8642072081565857, 2.2837867736816406, 2.1153714656829834, 2.2884912192821503, 2.05167019367218, 2.1509280800819397, 2.1426839232444763, 2.0253326296806335, 2.1694526076316833, 2.0790958404541016, 2.171088397502899, 2.0638576447963715, 2.2249951362609863, 1.8509590923786163, 2.082324296236038, 1.9043831527233124, 1.9179881513118744, 1.984548181295395, 1.7771113514900208, 1.7679804265499115, 1.9698423743247986, 2.083220422267914, 1.9658516347408295, 1.9300480782985687, 1.964733898639679], 'val_losses': [3.368725299835205, 2.4334959983825684, 1.37143075466156, 0.8200313448905945, 1.042264699935913, 1.4352004528045654, 1.6145299673080444, 1.57437002658844, 1.39232337474823, 1.2454967498779297, 1.1532588005065918, 1.0858426094055176, 1.0930875539779663, 1.1642138957977295, 1.1997182369232178, 1.2393591403961182, 1.275205373764038, 1.2658195495605469, 1.2619867324829102, 1.2465310096740723, 1.2409417629241943, 1.2250723838806152, 1.2162069082260132, 1.2090482711791992, 1.2023245096206665, 1.2006080150604248, 1.1989792585372925, 1.1987714767456055], 'val_acc': [0.1875, 0.203125, 0.25, 0.671875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 5.437872154118726e-05, 'batch_size': 64, 'epochs': 28, 'hidden_size': 145, 'dropout': 0.11292026759493132, 'weight_decay': 2.4101483630335058e-05, 'label_smoothing': 0.07520786392143729, 'grad_clip_norm': 2.4802641243031642, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 22333, 'model_storage_size_kb': 47.981054687500006, 'model_size_validation': 'PASS'}
2025-10-13 22:42:41,080 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:42:41,080 - INFO - _models.training_function_executor - Model: 22,333 parameters, 48.0KB (PASS 256KB limit)
2025-10-13 22:42:41,080 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.779s
2025-10-13 22:42:41,155 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:42:41,155 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.074s
2025-10-13 22:42:41,155 - INFO - bo.run_bo - Recorded observation #24: hparams={'lr': 5.437872154118726e-05, 'batch_size': np.int64(64), 'epochs': np.int64(28), 'hidden_size': np.int64(145), 'dropout': 0.11292026759493132, 'weight_decay': 2.4101483630335058e-05, 'label_smoothing': 0.07520786392143729, 'grad_clip_norm': 2.4802641243031642, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:42:41,155 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 24: {'lr': 5.437872154118726e-05, 'batch_size': np.int64(64), 'epochs': np.int64(28), 'hidden_size': np.int64(145), 'dropout': 0.11292026759493132, 'weight_decay': 2.4101483630335058e-05, 'label_smoothing': 0.07520786392143729, 'grad_clip_norm': 2.4802641243031642, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:42:41,155 - INFO - bo.run_bo - üîçBO Trial 25: Using RF surrogate + Expected Improvement
2025-10-13 22:42:41,155 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:41,155 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 25 (NaN monitoring active)
2025-10-13 22:42:41,156 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:41,156 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:41,156 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.1211343656202492e-05, 'batch_size': 64, 'epochs': 35, 'hidden_size': 167, 'dropout': 0.11453339110141972, 'weight_decay': 0.002745728191524506, 'label_smoothing': 0.022187505566948356, 'grad_clip_norm': 1.5691024710601627, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:41,157 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.1211343656202492e-05, 'batch_size': 64, 'epochs': 35, 'hidden_size': 167, 'dropout': 0.11453339110141972, 'weight_decay': 0.002745728191524506, 'label_smoothing': 0.022187505566948356, 'grad_clip_norm': 1.5691024710601627, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:42:44,900 - INFO - _models.training_function_executor - Epoch 001/035 | train_loss=15.3358 | val_loss=14.1218 | val_acc=0.0938
2025-10-13 22:42:44,908 - INFO - _models.training_function_executor - Epoch 002/035 | train_loss=15.2055 | val_loss=13.8201 | val_acc=0.0938
2025-10-13 22:42:44,915 - INFO - _models.training_function_executor - Epoch 003/035 | train_loss=15.2966 | val_loss=13.2108 | val_acc=0.0938
2025-10-13 22:42:44,922 - INFO - _models.training_function_executor - Epoch 004/035 | train_loss=14.0601 | val_loss=12.6201 | val_acc=0.0938
2025-10-13 22:42:44,928 - INFO - _models.training_function_executor - Epoch 005/035 | train_loss=13.4986 | val_loss=12.0289 | val_acc=0.0938
2025-10-13 22:42:44,935 - INFO - _models.training_function_executor - Epoch 006/035 | train_loss=13.2376 | val_loss=11.4443 | val_acc=0.0938
2025-10-13 22:42:44,942 - INFO - _models.training_function_executor - Epoch 007/035 | train_loss=12.5466 | val_loss=10.8910 | val_acc=0.0938
2025-10-13 22:42:44,948 - INFO - _models.training_function_executor - Epoch 008/035 | train_loss=11.4973 | val_loss=10.3448 | val_acc=0.0938
2025-10-13 22:42:44,954 - INFO - _models.training_function_executor - Epoch 009/035 | train_loss=10.7857 | val_loss=9.8162 | val_acc=0.0938
2025-10-13 22:42:44,961 - INFO - _models.training_function_executor - Epoch 010/035 | train_loss=10.2091 | val_loss=9.3175 | val_acc=0.0938
2025-10-13 22:42:44,968 - INFO - _models.training_function_executor - Epoch 011/035 | train_loss=9.8296 | val_loss=8.8396 | val_acc=0.0938
2025-10-13 22:42:44,974 - INFO - _models.training_function_executor - Epoch 012/035 | train_loss=10.2986 | val_loss=8.3719 | val_acc=0.0938
2025-10-13 22:42:44,981 - INFO - _models.training_function_executor - Epoch 013/035 | train_loss=9.1069 | val_loss=7.9386 | val_acc=0.0938
2025-10-13 22:42:44,989 - INFO - _models.training_function_executor - Epoch 014/035 | train_loss=8.9128 | val_loss=7.5207 | val_acc=0.0938
2025-10-13 22:42:44,997 - INFO - _models.training_function_executor - Epoch 015/035 | train_loss=8.3907 | val_loss=7.1449 | val_acc=0.0938
2025-10-13 22:42:45,003 - INFO - _models.training_function_executor - Epoch 016/035 | train_loss=8.3821 | val_loss=6.7916 | val_acc=0.0938
2025-10-13 22:42:45,010 - INFO - _models.training_function_executor - Epoch 017/035 | train_loss=7.9209 | val_loss=6.4619 | val_acc=0.0938
2025-10-13 22:42:45,017 - INFO - _models.training_function_executor - Epoch 018/035 | train_loss=7.5495 | val_loss=6.1671 | val_acc=0.0938
2025-10-13 22:42:45,023 - INFO - _models.training_function_executor - Epoch 019/035 | train_loss=7.6076 | val_loss=5.8828 | val_acc=0.0938
2025-10-13 22:42:45,031 - INFO - _models.training_function_executor - Epoch 020/035 | train_loss=7.4219 | val_loss=5.6382 | val_acc=0.0938
2025-10-13 22:42:45,038 - INFO - _models.training_function_executor - Epoch 021/035 | train_loss=6.8715 | val_loss=5.4218 | val_acc=0.0938
2025-10-13 22:42:45,045 - INFO - _models.training_function_executor - Epoch 022/035 | train_loss=6.8861 | val_loss=5.2286 | val_acc=0.0938
2025-10-13 22:42:45,051 - INFO - _models.training_function_executor - Epoch 023/035 | train_loss=6.9235 | val_loss=5.0560 | val_acc=0.0938
2025-10-13 22:42:45,057 - INFO - _models.training_function_executor - Epoch 024/035 | train_loss=6.6102 | val_loss=4.9173 | val_acc=0.0938
2025-10-13 22:42:45,065 - INFO - _models.training_function_executor - Epoch 025/035 | train_loss=6.6223 | val_loss=4.8023 | val_acc=0.0938
2025-10-13 22:42:45,071 - INFO - _models.training_function_executor - Epoch 026/035 | train_loss=6.4634 | val_loss=4.7067 | val_acc=0.0938
2025-10-13 22:42:45,078 - INFO - _models.training_function_executor - Epoch 027/035 | train_loss=6.3565 | val_loss=4.6228 | val_acc=0.0938
2025-10-13 22:42:45,084 - INFO - _models.training_function_executor - Epoch 028/035 | train_loss=6.3361 | val_loss=4.5590 | val_acc=0.0938
2025-10-13 22:42:45,091 - INFO - _models.training_function_executor - Epoch 029/035 | train_loss=6.5517 | val_loss=4.5008 | val_acc=0.0938
2025-10-13 22:42:45,097 - INFO - _models.training_function_executor - Epoch 030/035 | train_loss=6.2362 | val_loss=4.4638 | val_acc=0.0938
2025-10-13 22:42:45,103 - INFO - _models.training_function_executor - Epoch 031/035 | train_loss=5.5891 | val_loss=4.4372 | val_acc=0.0938
2025-10-13 22:42:45,110 - INFO - _models.training_function_executor - Epoch 032/035 | train_loss=6.1604 | val_loss=4.4224 | val_acc=0.0938
2025-10-13 22:42:45,117 - INFO - _models.training_function_executor - Epoch 033/035 | train_loss=6.2227 | val_loss=4.4125 | val_acc=0.0938
2025-10-13 22:42:45,123 - INFO - _models.training_function_executor - Epoch 034/035 | train_loss=6.2567 | val_loss=4.4059 | val_acc=0.0938
2025-10-13 22:42:45,129 - INFO - _models.training_function_executor - Epoch 035/035 | train_loss=6.1106 | val_loss=4.4040 | val_acc=0.0938
2025-10-13 22:42:45,994 - INFO - _models.training_function_executor - Model: 29,395 parameters, 63.2KB storage
2025-10-13 22:42:45,995 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [15.335813283920288, 15.205461740493774, 15.296568870544434, 14.060112476348877, 13.498559474945068, 13.237642049789429, 12.546594142913818, 11.497313737869263, 10.785674095153809, 10.209138870239258, 9.829634189605713, 10.298578262329102, 9.10693073272705, 8.912801027297974, 8.39070475101471, 8.382108688354492, 7.920874714851379, 7.549459218978882, 7.607607841491699, 7.421880125999451, 6.871525764465332, 6.8860520124435425, 6.923451662063599, 6.610239863395691, 6.622316241264343, 6.463439583778381, 6.356476187705994, 6.336094617843628, 6.5516557693481445, 6.236215829849243, 5.589084029197693, 6.160390734672546, 6.222673654556274, 6.2567185163497925, 6.1105934381484985], 'val_losses': [14.121753692626953, 13.820066452026367, 13.210821151733398, 12.620070457458496, 12.02890396118164, 11.444348335266113, 10.891010284423828, 10.344812393188477, 9.81617546081543, 9.31750202178955, 8.839600563049316, 8.371919631958008, 7.938599586486816, 7.520657062530518, 7.144928455352783, 6.791574478149414, 6.461933135986328, 6.167078495025635, 5.882809162139893, 5.638176918029785, 5.421814918518066, 5.228634834289551, 5.056018352508545, 4.917342185974121, 4.802340507507324, 4.7066969871521, 4.6227874755859375, 4.558951377868652, 4.500831604003906, 4.463829040527344, 4.4371795654296875, 4.422356605529785, 4.412548542022705, 4.405921459197998, 4.404043197631836], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.1211343656202492e-05, 'batch_size': 64, 'epochs': 35, 'hidden_size': 167, 'dropout': 0.11453339110141972, 'weight_decay': 0.002745728191524506, 'label_smoothing': 0.022187505566948356, 'grad_clip_norm': 1.5691024710601627, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 29395, 'model_storage_size_kb': 63.15332031250001, 'model_size_validation': 'PASS'}
2025-10-13 22:42:45,995 - INFO - _models.training_function_executor - BO Objective: base=0.0938, size_penalty=0.0000, final=0.0938
2025-10-13 22:42:45,995 - INFO - _models.training_function_executor - Model: 29,395 parameters, 63.2KB (PASS 256KB limit)
2025-10-13 22:42:45,995 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.839s
2025-10-13 22:42:46,069 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0938
2025-10-13 22:42:46,069 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.074s
2025-10-13 22:42:46,069 - INFO - bo.run_bo - Recorded observation #25: hparams={'lr': 1.1211343656202492e-05, 'batch_size': np.int64(64), 'epochs': np.int64(35), 'hidden_size': np.int64(167), 'dropout': 0.11453339110141972, 'weight_decay': 0.002745728191524506, 'label_smoothing': 0.022187505566948356, 'grad_clip_norm': 1.5691024710601627, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.0938
2025-10-13 22:42:46,069 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 25: {'lr': 1.1211343656202492e-05, 'batch_size': np.int64(64), 'epochs': np.int64(35), 'hidden_size': np.int64(167), 'dropout': 0.11453339110141972, 'weight_decay': 0.002745728191524506, 'label_smoothing': 0.022187505566948356, 'grad_clip_norm': 1.5691024710601627, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.0938
2025-10-13 22:42:46,069 - INFO - bo.run_bo - üîçBO Trial 26: Using RF surrogate + Expected Improvement
2025-10-13 22:42:46,069 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:46,069 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 26 (NaN monitoring active)
2025-10-13 22:42:46,069 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:46,069 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:46,069 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0638726500700914e-05, 'batch_size': 64, 'epochs': 73, 'hidden_size': 150, 'dropout': 0.08036155551911735, 'weight_decay': 2.6827824623452492e-05, 'label_smoothing': 0.010573210824939207, 'grad_clip_norm': 2.5498099076076395, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:46,070 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0638726500700914e-05, 'batch_size': 64, 'epochs': 73, 'hidden_size': 150, 'dropout': 0.08036155551911735, 'weight_decay': 2.6827824623452492e-05, 'label_smoothing': 0.010573210824939207, 'grad_clip_norm': 2.5498099076076395, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:50,192 - INFO - _models.training_function_executor - Epoch 001/073 | train_loss=3.3289 | val_loss=2.2838 | val_acc=0.7188
2025-10-13 22:42:50,202 - INFO - _models.training_function_executor - Epoch 002/073 | train_loss=3.7650 | val_loss=2.2455 | val_acc=0.7188
2025-10-13 22:42:50,209 - INFO - _models.training_function_executor - Epoch 003/073 | train_loss=3.1132 | val_loss=2.1889 | val_acc=0.7188
2025-10-13 22:42:50,216 - INFO - _models.training_function_executor - Epoch 004/073 | train_loss=2.9465 | val_loss=2.1353 | val_acc=0.7188
2025-10-13 22:42:50,223 - INFO - _models.training_function_executor - Epoch 005/073 | train_loss=3.3484 | val_loss=2.0644 | val_acc=0.7188
2025-10-13 22:42:50,229 - INFO - _models.training_function_executor - Epoch 006/073 | train_loss=2.5783 | val_loss=1.9914 | val_acc=0.7188
2025-10-13 22:42:50,235 - INFO - _models.training_function_executor - Epoch 007/073 | train_loss=2.8243 | val_loss=1.9203 | val_acc=0.7188
2025-10-13 22:42:50,241 - INFO - _models.training_function_executor - Epoch 008/073 | train_loss=2.7140 | val_loss=1.8260 | val_acc=0.7188
2025-10-13 22:42:50,248 - INFO - _models.training_function_executor - Epoch 009/073 | train_loss=2.5167 | val_loss=1.7429 | val_acc=0.7188
2025-10-13 22:42:50,255 - INFO - _models.training_function_executor - Epoch 010/073 | train_loss=2.7526 | val_loss=1.6561 | val_acc=0.7188
2025-10-13 22:42:50,262 - INFO - _models.training_function_executor - Epoch 011/073 | train_loss=2.7191 | val_loss=1.5661 | val_acc=0.7188
2025-10-13 22:42:50,269 - INFO - _models.training_function_executor - Epoch 012/073 | train_loss=2.2967 | val_loss=1.4831 | val_acc=0.7188
2025-10-13 22:42:50,276 - INFO - _models.training_function_executor - Epoch 013/073 | train_loss=2.4565 | val_loss=1.4084 | val_acc=0.7188
2025-10-13 22:42:50,283 - INFO - _models.training_function_executor - Epoch 014/073 | train_loss=2.1231 | val_loss=1.3442 | val_acc=0.7188
2025-10-13 22:42:50,290 - INFO - _models.training_function_executor - Epoch 015/073 | train_loss=2.2263 | val_loss=1.2683 | val_acc=0.7188
2025-10-13 22:42:50,297 - INFO - _models.training_function_executor - Epoch 016/073 | train_loss=2.1946 | val_loss=1.1897 | val_acc=0.7188
2025-10-13 22:42:50,303 - INFO - _models.training_function_executor - Epoch 017/073 | train_loss=2.1929 | val_loss=1.1220 | val_acc=0.7188
2025-10-13 22:42:50,309 - INFO - _models.training_function_executor - Epoch 018/073 | train_loss=2.2854 | val_loss=1.0699 | val_acc=0.7188
2025-10-13 22:42:50,315 - INFO - _models.training_function_executor - Epoch 019/073 | train_loss=2.1854 | val_loss=1.0298 | val_acc=0.7188
2025-10-13 22:42:50,321 - INFO - _models.training_function_executor - Epoch 020/073 | train_loss=2.2415 | val_loss=1.0018 | val_acc=0.7188
2025-10-13 22:42:50,328 - INFO - _models.training_function_executor - Epoch 021/073 | train_loss=2.1556 | val_loss=0.9775 | val_acc=0.7188
2025-10-13 22:42:50,334 - INFO - _models.training_function_executor - Epoch 022/073 | train_loss=2.1883 | val_loss=0.9696 | val_acc=0.7188
2025-10-13 22:42:50,340 - INFO - _models.training_function_executor - Epoch 023/073 | train_loss=2.2827 | val_loss=0.9752 | val_acc=0.7188
2025-10-13 22:42:50,346 - INFO - _models.training_function_executor - Epoch 024/073 | train_loss=2.0058 | val_loss=0.9624 | val_acc=0.7188
2025-10-13 22:42:50,352 - INFO - _models.training_function_executor - Epoch 025/073 | train_loss=2.0656 | val_loss=0.9498 | val_acc=0.7188
2025-10-13 22:42:50,359 - INFO - _models.training_function_executor - Epoch 026/073 | train_loss=1.8798 | val_loss=0.9527 | val_acc=0.7188
2025-10-13 22:42:50,366 - INFO - _models.training_function_executor - Epoch 027/073 | train_loss=2.0574 | val_loss=0.9592 | val_acc=0.7188
2025-10-13 22:42:50,372 - INFO - _models.training_function_executor - Epoch 028/073 | train_loss=1.9119 | val_loss=0.9558 | val_acc=0.7188
2025-10-13 22:42:50,379 - INFO - _models.training_function_executor - Epoch 029/073 | train_loss=1.7338 | val_loss=0.9645 | val_acc=0.7188
2025-10-13 22:42:50,386 - INFO - _models.training_function_executor - Epoch 030/073 | train_loss=2.1061 | val_loss=0.9778 | val_acc=0.7188
2025-10-13 22:42:50,393 - INFO - _models.training_function_executor - Epoch 031/073 | train_loss=2.1649 | val_loss=0.9937 | val_acc=0.7188
2025-10-13 22:42:50,401 - INFO - _models.training_function_executor - Epoch 032/073 | train_loss=2.0007 | val_loss=1.0005 | val_acc=0.7188
2025-10-13 22:42:50,407 - INFO - _models.training_function_executor - Epoch 033/073 | train_loss=1.9607 | val_loss=0.9902 | val_acc=0.7188
2025-10-13 22:42:50,415 - INFO - _models.training_function_executor - Epoch 034/073 | train_loss=1.8457 | val_loss=0.9729 | val_acc=0.7188
2025-10-13 22:42:50,421 - INFO - _models.training_function_executor - Epoch 035/073 | train_loss=1.9791 | val_loss=0.9535 | val_acc=0.7188
2025-10-13 22:42:50,429 - INFO - _models.training_function_executor - Epoch 036/073 | train_loss=2.0368 | val_loss=0.9271 | val_acc=0.7188
2025-10-13 22:42:50,436 - INFO - _models.training_function_executor - Epoch 037/073 | train_loss=2.0005 | val_loss=0.9188 | val_acc=0.7188
2025-10-13 22:42:50,442 - INFO - _models.training_function_executor - Epoch 038/073 | train_loss=1.9315 | val_loss=0.9167 | val_acc=0.7188
2025-10-13 22:42:50,450 - INFO - _models.training_function_executor - Epoch 039/073 | train_loss=1.8717 | val_loss=0.9214 | val_acc=0.7188
2025-10-13 22:42:50,457 - INFO - _models.training_function_executor - Epoch 040/073 | train_loss=2.2444 | val_loss=0.9305 | val_acc=0.7188
2025-10-13 22:42:50,463 - INFO - _models.training_function_executor - Epoch 041/073 | train_loss=2.0838 | val_loss=0.9415 | val_acc=0.7188
2025-10-13 22:42:50,469 - INFO - _models.training_function_executor - Epoch 042/073 | train_loss=2.1691 | val_loss=0.9497 | val_acc=0.7188
2025-10-13 22:42:50,476 - INFO - _models.training_function_executor - Epoch 043/073 | train_loss=1.9619 | val_loss=0.9501 | val_acc=0.7188
2025-10-13 22:42:50,482 - INFO - _models.training_function_executor - Epoch 044/073 | train_loss=1.9366 | val_loss=0.9500 | val_acc=0.7188
2025-10-13 22:42:50,488 - INFO - _models.training_function_executor - Epoch 045/073 | train_loss=1.7940 | val_loss=0.9442 | val_acc=0.7188
2025-10-13 22:42:50,495 - INFO - _models.training_function_executor - Epoch 046/073 | train_loss=1.8789 | val_loss=0.9394 | val_acc=0.7188
2025-10-13 22:42:50,501 - INFO - _models.training_function_executor - Epoch 047/073 | train_loss=2.2887 | val_loss=0.9375 | val_acc=0.7188
2025-10-13 22:42:50,507 - INFO - _models.training_function_executor - Epoch 048/073 | train_loss=1.9955 | val_loss=0.9334 | val_acc=0.7188
2025-10-13 22:42:50,513 - INFO - _models.training_function_executor - Epoch 049/073 | train_loss=2.0449 | val_loss=0.9322 | val_acc=0.7188
2025-10-13 22:42:50,519 - INFO - _models.training_function_executor - Epoch 050/073 | train_loss=1.8994 | val_loss=0.9312 | val_acc=0.7188
2025-10-13 22:42:50,527 - INFO - _models.training_function_executor - Epoch 051/073 | train_loss=1.7551 | val_loss=0.9303 | val_acc=0.7188
2025-10-13 22:42:50,533 - INFO - _models.training_function_executor - Epoch 052/073 | train_loss=2.0081 | val_loss=0.9337 | val_acc=0.7188
2025-10-13 22:42:50,539 - INFO - _models.training_function_executor - Epoch 053/073 | train_loss=2.0347 | val_loss=0.9349 | val_acc=0.7188
2025-10-13 22:42:50,545 - INFO - _models.training_function_executor - Epoch 054/073 | train_loss=1.8858 | val_loss=0.9390 | val_acc=0.7188
2025-10-13 22:42:50,551 - INFO - _models.training_function_executor - Epoch 055/073 | train_loss=1.8643 | val_loss=0.9426 | val_acc=0.7188
2025-10-13 22:42:50,557 - INFO - _models.training_function_executor - Epoch 056/073 | train_loss=1.7554 | val_loss=0.9449 | val_acc=0.7188
2025-10-13 22:42:50,564 - INFO - _models.training_function_executor - Epoch 057/073 | train_loss=2.1837 | val_loss=0.9482 | val_acc=0.7188
2025-10-13 22:42:50,570 - INFO - _models.training_function_executor - Epoch 058/073 | train_loss=1.7217 | val_loss=0.9502 | val_acc=0.7188
2025-10-13 22:42:50,576 - INFO - _models.training_function_executor - Epoch 059/073 | train_loss=1.9362 | val_loss=0.9528 | val_acc=0.7188
2025-10-13 22:42:50,583 - INFO - _models.training_function_executor - Epoch 060/073 | train_loss=2.4646 | val_loss=0.9548 | val_acc=0.7188
2025-10-13 22:42:50,589 - INFO - _models.training_function_executor - Epoch 061/073 | train_loss=1.8017 | val_loss=0.9568 | val_acc=0.7188
2025-10-13 22:42:50,595 - INFO - _models.training_function_executor - Epoch 062/073 | train_loss=2.2130 | val_loss=0.9572 | val_acc=0.7188
2025-10-13 22:42:50,601 - INFO - _models.training_function_executor - Epoch 063/073 | train_loss=1.8306 | val_loss=0.9581 | val_acc=0.7188
2025-10-13 22:42:50,608 - INFO - _models.training_function_executor - Epoch 064/073 | train_loss=1.7389 | val_loss=0.9585 | val_acc=0.7188
2025-10-13 22:42:50,614 - INFO - _models.training_function_executor - Epoch 065/073 | train_loss=1.9479 | val_loss=0.9590 | val_acc=0.7188
2025-10-13 22:42:50,620 - INFO - _models.training_function_executor - Epoch 066/073 | train_loss=2.0946 | val_loss=0.9594 | val_acc=0.7188
2025-10-13 22:42:50,627 - INFO - _models.training_function_executor - Epoch 067/073 | train_loss=1.7430 | val_loss=0.9595 | val_acc=0.7188
2025-10-13 22:42:50,634 - INFO - _models.training_function_executor - Epoch 068/073 | train_loss=1.8629 | val_loss=0.9599 | val_acc=0.7188
2025-10-13 22:42:50,640 - INFO - _models.training_function_executor - Epoch 069/073 | train_loss=2.1305 | val_loss=0.9597 | val_acc=0.7188
2025-10-13 22:42:50,647 - INFO - _models.training_function_executor - Epoch 070/073 | train_loss=2.0802 | val_loss=0.9598 | val_acc=0.7188
2025-10-13 22:42:50,654 - INFO - _models.training_function_executor - Epoch 071/073 | train_loss=1.9463 | val_loss=0.9599 | val_acc=0.7188
2025-10-13 22:42:50,660 - INFO - _models.training_function_executor - Epoch 072/073 | train_loss=1.8792 | val_loss=0.9599 | val_acc=0.7188
2025-10-13 22:42:50,667 - INFO - _models.training_function_executor - Epoch 073/073 | train_loss=2.4882 | val_loss=0.9598 | val_acc=0.7188
2025-10-13 22:42:51,575 - INFO - _models.training_function_executor - Model: 23,853 parameters, 51.2KB storage
2025-10-13 22:42:51,575 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [3.3288851976394653, 3.765019953250885, 3.113229811191559, 2.9465436339378357, 3.348421633243561, 2.578300714492798, 2.824349969625473, 2.71396267414093, 2.516736477613449, 2.7525673508644104, 2.719104826450348, 2.296745091676712, 2.4564683735370636, 2.1231346428394318, 2.226255238056183, 2.194603145122528, 2.1928822100162506, 2.2854014933109283, 2.1854205429553986, 2.2414933145046234, 2.155608505010605, 2.188292056322098, 2.2827207446098328, 2.0058033764362335, 2.065597504377365, 1.8797832131385803, 2.057427704334259, 1.9119251072406769, 1.7337926030158997, 2.1061295568943024, 2.1649190187454224, 2.0006835758686066, 1.9606925249099731, 1.8457136452198029, 1.979142189025879, 2.0367545783519745, 2.000470846891403, 1.9315264523029327, 1.8716789186000824, 2.2444193363189697, 2.083822578191757, 2.169097900390625, 1.9618763029575348, 1.9365979433059692, 1.7939850389957428, 1.8789413273334503, 2.2887173891067505, 1.9955109655857086, 2.0449107885360718, 1.899400532245636, 1.755068063735962, 2.008129119873047, 2.0347433984279633, 1.8858021199703217, 1.864278644323349, 1.7554317116737366, 2.1837096214294434, 1.7217130959033966, 1.936154156923294, 2.4646271467208862, 1.8017255365848541, 2.2130323946475983, 1.8306144177913666, 1.7388730943202972, 1.9478985965251923, 2.094604879617691, 1.74296373128891, 1.862851470708847, 2.130512833595276, 2.080204129219055, 1.946290284395218, 1.8792183101177216, 2.4881592392921448], 'val_losses': [2.2838213443756104, 2.245499849319458, 2.188939094543457, 2.135314702987671, 2.0644307136535645, 1.991396427154541, 1.9202616214752197, 1.8260263204574585, 1.742902398109436, 1.6560677289962769, 1.5661017894744873, 1.4830739498138428, 1.4084136486053467, 1.3442320823669434, 1.268338918685913, 1.1897382736206055, 1.121982455253601, 1.0698938369750977, 1.0297905206680298, 1.0018333196640015, 0.9775087237358093, 0.9695641994476318, 0.9751505255699158, 0.9624239206314087, 0.9498011469841003, 0.9526771903038025, 0.9591691493988037, 0.9558077454566956, 0.9645435810089111, 0.9778019785881042, 0.9936755299568176, 1.000469446182251, 0.9902214407920837, 0.9729105830192566, 0.953548014163971, 0.9270815253257751, 0.9188248515129089, 0.9167172312736511, 0.9213845133781433, 0.930546760559082, 0.9415399432182312, 0.9496526122093201, 0.9501476287841797, 0.9500207304954529, 0.9442132711410522, 0.9393892884254456, 0.9374873638153076, 0.9333610534667969, 0.932196855545044, 0.9312461614608765, 0.9302834272384644, 0.9336788058280945, 0.9348568320274353, 0.9390361309051514, 0.9425860643386841, 0.9449431300163269, 0.9481563568115234, 0.9502183794975281, 0.9527837038040161, 0.9548413157463074, 0.9568423628807068, 0.957224428653717, 0.9580813050270081, 0.9584600925445557, 0.9590314626693726, 0.9593981504440308, 0.9594554901123047, 0.9598922729492188, 0.9597192406654358, 0.9597504138946533, 0.9598789215087891, 0.9598673582077026, 0.9598181247711182], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0638726500700914e-05, 'batch_size': 64, 'epochs': 73, 'hidden_size': 150, 'dropout': 0.08036155551911735, 'weight_decay': 2.6827824623452492e-05, 'label_smoothing': 0.010573210824939207, 'grad_clip_norm': 2.5498099076076395, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 23853, 'model_storage_size_kb': 51.246679687500006, 'model_size_validation': 'PASS'}
2025-10-13 22:42:51,575 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:42:51,575 - INFO - _models.training_function_executor - Model: 23,853 parameters, 51.2KB (PASS 256KB limit)
2025-10-13 22:42:51,575 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.506s
2025-10-13 22:42:51,650 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:42:51,650 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.074s
2025-10-13 22:42:51,650 - INFO - bo.run_bo - Recorded observation #26: hparams={'lr': 1.0638726500700914e-05, 'batch_size': np.int64(64), 'epochs': np.int64(73), 'hidden_size': np.int64(150), 'dropout': 0.08036155551911735, 'weight_decay': 2.6827824623452492e-05, 'label_smoothing': 0.010573210824939207, 'grad_clip_norm': 2.5498099076076395, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:42:51,650 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 26: {'lr': 1.0638726500700914e-05, 'batch_size': np.int64(64), 'epochs': np.int64(73), 'hidden_size': np.int64(150), 'dropout': 0.08036155551911735, 'weight_decay': 2.6827824623452492e-05, 'label_smoothing': 0.010573210824939207, 'grad_clip_norm': 2.5498099076076395, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:42:51,650 - INFO - bo.run_bo - üîçBO Trial 27: Using RF surrogate + Expected Improvement
2025-10-13 22:42:51,650 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:51,650 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 27 (NaN monitoring active)
2025-10-13 22:42:51,650 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:51,650 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:51,650 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0038621865654490752, 'batch_size': 64, 'epochs': 41, 'hidden_size': 177, 'dropout': 0.0715850013618682, 'weight_decay': 0.008825186613900601, 'label_smoothing': 0.03191895184449625, 'grad_clip_norm': 0.9729230117425376, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:51,651 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0038621865654490752, 'batch_size': 64, 'epochs': 41, 'hidden_size': 177, 'dropout': 0.0715850013618682, 'weight_decay': 0.008825186613900601, 'label_smoothing': 0.03191895184449625, 'grad_clip_norm': 0.9729230117425376, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:42:55,028 - INFO - _models.training_function_executor - Epoch 001/041 | train_loss=2.2868 | val_loss=1.7999 | val_acc=0.5469
2025-10-13 22:42:55,037 - INFO - _models.training_function_executor - Epoch 002/041 | train_loss=7.4672 | val_loss=6.0844 | val_acc=0.7031
2025-10-13 22:42:55,044 - INFO - _models.training_function_executor - Epoch 003/041 | train_loss=6.7764 | val_loss=2.7936 | val_acc=0.5781
2025-10-13 22:42:55,051 - INFO - _models.training_function_executor - Epoch 004/041 | train_loss=2.3333 | val_loss=1.6479 | val_acc=0.7188
2025-10-13 22:42:55,058 - INFO - _models.training_function_executor - Epoch 005/041 | train_loss=1.4911 | val_loss=1.1811 | val_acc=0.5781
2025-10-13 22:42:55,064 - INFO - _models.training_function_executor - Epoch 006/041 | train_loss=1.6433 | val_loss=0.7445 | val_acc=0.7188
2025-10-13 22:42:55,070 - INFO - _models.training_function_executor - Epoch 007/041 | train_loss=1.3716 | val_loss=1.2351 | val_acc=0.7188
2025-10-13 22:42:55,077 - INFO - _models.training_function_executor - Epoch 008/041 | train_loss=1.0479 | val_loss=0.7350 | val_acc=0.6562
2025-10-13 22:42:55,083 - INFO - _models.training_function_executor - Epoch 009/041 | train_loss=0.8533 | val_loss=0.7230 | val_acc=0.7188
2025-10-13 22:42:55,089 - INFO - _models.training_function_executor - Epoch 010/041 | train_loss=0.7545 | val_loss=0.6834 | val_acc=0.7188
2025-10-13 22:42:55,095 - INFO - _models.training_function_executor - Epoch 011/041 | train_loss=0.7352 | val_loss=0.6471 | val_acc=0.7188
2025-10-13 22:42:55,102 - INFO - _models.training_function_executor - Epoch 012/041 | train_loss=0.7377 | val_loss=0.6681 | val_acc=0.7188
2025-10-13 22:42:55,108 - INFO - _models.training_function_executor - Epoch 013/041 | train_loss=0.7031 | val_loss=0.6475 | val_acc=0.7188
2025-10-13 22:42:55,116 - INFO - _models.training_function_executor - Epoch 014/041 | train_loss=0.7933 | val_loss=0.6505 | val_acc=0.7188
2025-10-13 22:42:55,123 - INFO - _models.training_function_executor - Epoch 015/041 | train_loss=0.7244 | val_loss=0.6487 | val_acc=0.7188
2025-10-13 22:42:55,130 - INFO - _models.training_function_executor - Epoch 016/041 | train_loss=0.7279 | val_loss=0.6493 | val_acc=0.7188
2025-10-13 22:42:55,136 - INFO - _models.training_function_executor - Epoch 017/041 | train_loss=0.7174 | val_loss=0.6738 | val_acc=0.7188
2025-10-13 22:42:55,143 - INFO - _models.training_function_executor - Epoch 018/041 | train_loss=0.7163 | val_loss=0.6534 | val_acc=0.7188
2025-10-13 22:42:55,149 - INFO - _models.training_function_executor - Epoch 019/041 | train_loss=0.7077 | val_loss=0.6431 | val_acc=0.7188
2025-10-13 22:42:55,155 - INFO - _models.training_function_executor - Epoch 020/041 | train_loss=0.7147 | val_loss=0.6503 | val_acc=0.7188
2025-10-13 22:42:55,161 - INFO - _models.training_function_executor - Epoch 021/041 | train_loss=0.7034 | val_loss=0.6536 | val_acc=0.7188
2025-10-13 22:42:55,167 - INFO - _models.training_function_executor - Epoch 022/041 | train_loss=0.7176 | val_loss=0.6513 | val_acc=0.7188
2025-10-13 22:42:55,173 - INFO - _models.training_function_executor - Epoch 023/041 | train_loss=0.7112 | val_loss=0.6509 | val_acc=0.7188
2025-10-13 22:42:55,178 - INFO - _models.training_function_executor - Epoch 024/041 | train_loss=0.7028 | val_loss=0.6456 | val_acc=0.7188
2025-10-13 22:42:55,184 - INFO - _models.training_function_executor - Epoch 025/041 | train_loss=0.6991 | val_loss=0.6442 | val_acc=0.7188
2025-10-13 22:42:55,191 - INFO - _models.training_function_executor - Epoch 026/041 | train_loss=0.6945 | val_loss=0.6504 | val_acc=0.7188
2025-10-13 22:42:55,197 - INFO - _models.training_function_executor - Epoch 027/041 | train_loss=0.6918 | val_loss=0.6489 | val_acc=0.7188
2025-10-13 22:42:55,204 - INFO - _models.training_function_executor - Epoch 028/041 | train_loss=0.6852 | val_loss=0.6530 | val_acc=0.7188
2025-10-13 22:42:55,210 - INFO - _models.training_function_executor - Epoch 029/041 | train_loss=0.7084 | val_loss=0.6543 | val_acc=0.7188
2025-10-13 22:42:55,216 - INFO - _models.training_function_executor - Epoch 030/041 | train_loss=0.7043 | val_loss=0.6509 | val_acc=0.7188
2025-10-13 22:42:55,222 - INFO - _models.training_function_executor - Epoch 031/041 | train_loss=0.6854 | val_loss=0.6489 | val_acc=0.7188
2025-10-13 22:42:55,229 - INFO - _models.training_function_executor - Epoch 032/041 | train_loss=0.6868 | val_loss=0.6485 | val_acc=0.7188
2025-10-13 22:42:55,235 - INFO - _models.training_function_executor - Epoch 033/041 | train_loss=0.6809 | val_loss=0.6506 | val_acc=0.7188
2025-10-13 22:42:55,241 - INFO - _models.training_function_executor - Epoch 034/041 | train_loss=0.6871 | val_loss=0.6487 | val_acc=0.7188
2025-10-13 22:42:55,247 - INFO - _models.training_function_executor - Epoch 035/041 | train_loss=0.6990 | val_loss=0.6485 | val_acc=0.7188
2025-10-13 22:42:55,253 - INFO - _models.training_function_executor - Epoch 036/041 | train_loss=0.6903 | val_loss=0.6496 | val_acc=0.7188
2025-10-13 22:42:55,259 - INFO - _models.training_function_executor - Epoch 037/041 | train_loss=0.6965 | val_loss=0.6488 | val_acc=0.7188
2025-10-13 22:42:55,265 - INFO - _models.training_function_executor - Epoch 038/041 | train_loss=0.6958 | val_loss=0.6489 | val_acc=0.7188
2025-10-13 22:42:55,271 - INFO - _models.training_function_executor - Epoch 039/041 | train_loss=0.6912 | val_loss=0.6489 | val_acc=0.7188
2025-10-13 22:42:55,276 - INFO - _models.training_function_executor - Epoch 040/041 | train_loss=0.6888 | val_loss=0.6491 | val_acc=0.7188
2025-10-13 22:42:55,282 - INFO - _models.training_function_executor - Epoch 041/041 | train_loss=0.6965 | val_loss=0.6491 | val_acc=0.7188
2025-10-13 22:42:56,146 - INFO - _models.training_function_executor - Model: 32,925 parameters, 70.7KB storage
2025-10-13 22:42:56,146 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.286812663078308, 7.4671531319618225, 6.776444435119629, 2.333279848098755, 1.4911499917507172, 1.6432677507400513, 1.3715896755456924, 1.047920435667038, 0.8532684594392776, 0.7544737756252289, 0.7352192401885986, 0.7377128005027771, 0.7030951231718063, 0.7932806462049484, 0.7243653684854507, 0.7278843224048615, 0.717409074306488, 0.7163051664829254, 0.7076747566461563, 0.7147190123796463, 0.7034430801868439, 0.7176048755645752, 0.7111720591783524, 0.7027657181024551, 0.6991071105003357, 0.6944669187068939, 0.6917738914489746, 0.685212180018425, 0.7083980590105057, 0.7042912989854813, 0.6854015439748764, 0.6867515593767166, 0.6809147298336029, 0.687140479683876, 0.6989896893501282, 0.6903039067983627, 0.6965222060680389, 0.6958485543727875, 0.6912175714969635, 0.6887840926647186, 0.6965446621179581], 'val_losses': [1.7999273538589478, 6.084414482116699, 2.7935853004455566, 1.6479161977767944, 1.1811401844024658, 0.7444589734077454, 1.235130786895752, 0.7350466251373291, 0.7230042219161987, 0.6833729147911072, 0.6471058130264282, 0.66811203956604, 0.6474518179893494, 0.650485098361969, 0.6486696004867554, 0.6492908000946045, 0.6738317012786865, 0.6534173488616943, 0.6430563926696777, 0.6502782106399536, 0.6536499857902527, 0.6513470411300659, 0.6508936882019043, 0.6456015706062317, 0.6442136764526367, 0.6504152417182922, 0.6489264965057373, 0.6529762148857117, 0.6543003916740417, 0.6509004831314087, 0.6488595008850098, 0.6484646797180176, 0.6505904793739319, 0.6486830115318298, 0.6485348343849182, 0.6495728492736816, 0.6487544178962708, 0.6488560438156128, 0.6489100456237793, 0.6490709185600281, 0.6491227149963379], 'val_acc': [0.546875, 0.703125, 0.578125, 0.71875, 0.578125, 0.71875, 0.71875, 0.65625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0038621865654490752, 'batch_size': 64, 'epochs': 41, 'hidden_size': 177, 'dropout': 0.0715850013618682, 'weight_decay': 0.008825186613900601, 'label_smoothing': 0.03191895184449625, 'grad_clip_norm': 0.9729230117425376, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 32925, 'model_storage_size_kb': 70.7373046875, 'model_size_validation': 'PASS'}
2025-10-13 22:42:56,146 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:42:56,146 - INFO - _models.training_function_executor - Model: 32,925 parameters, 70.7KB (PASS 256KB limit)
2025-10-13 22:42:56,146 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.496s
2025-10-13 22:42:56,219 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:42:56,219 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.072s
2025-10-13 22:42:56,219 - INFO - bo.run_bo - Recorded observation #27: hparams={'lr': 0.0038621865654490752, 'batch_size': np.int64(64), 'epochs': np.int64(41), 'hidden_size': np.int64(177), 'dropout': 0.0715850013618682, 'weight_decay': 0.008825186613900601, 'label_smoothing': 0.03191895184449625, 'grad_clip_norm': 0.9729230117425376, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:42:56,219 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 27: {'lr': 0.0038621865654490752, 'batch_size': np.int64(64), 'epochs': np.int64(41), 'hidden_size': np.int64(177), 'dropout': 0.0715850013618682, 'weight_decay': 0.008825186613900601, 'label_smoothing': 0.03191895184449625, 'grad_clip_norm': 0.9729230117425376, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:42:56,220 - INFO - bo.run_bo - üîçBO Trial 28: Using RF surrogate + Expected Improvement
2025-10-13 22:42:56,220 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:42:56,220 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 28 (NaN monitoring active)
2025-10-13 22:42:56,220 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:42:56,220 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:42:56,220 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.082448236769665e-05, 'batch_size': 256, 'epochs': 54, 'hidden_size': 122, 'dropout': 0.1376481086773302, 'weight_decay': 0.006087177517266088, 'label_smoothing': 0.08175535372464002, 'grad_clip_norm': 2.429849466520067, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:42:56,221 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.082448236769665e-05, 'batch_size': 256, 'epochs': 54, 'hidden_size': 122, 'dropout': 0.1376481086773302, 'weight_decay': 0.006087177517266088, 'label_smoothing': 0.08175535372464002, 'grad_clip_norm': 2.429849466520067, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:42:59,968 - INFO - _models.training_function_executor - Epoch 001/054 | train_loss=6.8186 | val_loss=6.6875 | val_acc=0.0938
2025-10-13 22:42:59,974 - INFO - _models.training_function_executor - Epoch 002/054 | train_loss=6.6138 | val_loss=6.6875 | val_acc=0.0938
2025-10-13 22:42:59,978 - INFO - _models.training_function_executor - Epoch 003/054 | train_loss=6.1410 | val_loss=6.6875 | val_acc=0.0938
2025-10-13 22:42:59,982 - INFO - _models.training_function_executor - Epoch 004/054 | train_loss=6.0635 | val_loss=6.6875 | val_acc=0.0938
2025-10-13 22:42:59,985 - INFO - _models.training_function_executor - Epoch 005/054 | train_loss=6.1639 | val_loss=6.6875 | val_acc=0.0938
2025-10-13 22:42:59,988 - INFO - _models.training_function_executor - Epoch 006/054 | train_loss=5.9655 | val_loss=6.6875 | val_acc=0.0938
2025-10-13 22:42:59,991 - INFO - _models.training_function_executor - Epoch 007/054 | train_loss=6.6733 | val_loss=6.5862 | val_acc=0.0938
2025-10-13 22:42:59,995 - INFO - _models.training_function_executor - Epoch 008/054 | train_loss=6.6068 | val_loss=6.4889 | val_acc=0.0938
2025-10-13 22:42:59,998 - INFO - _models.training_function_executor - Epoch 009/054 | train_loss=6.5664 | val_loss=6.3964 | val_acc=0.0938
2025-10-13 22:43:00,001 - INFO - _models.training_function_executor - Epoch 010/054 | train_loss=6.4464 | val_loss=6.3057 | val_acc=0.0938
2025-10-13 22:43:00,004 - INFO - _models.training_function_executor - Epoch 011/054 | train_loss=6.3123 | val_loss=6.2145 | val_acc=0.0938
2025-10-13 22:43:00,008 - INFO - _models.training_function_executor - Epoch 012/054 | train_loss=6.1380 | val_loss=6.1225 | val_acc=0.0938
2025-10-13 22:43:00,011 - INFO - _models.training_function_executor - Epoch 013/054 | train_loss=5.9452 | val_loss=6.0322 | val_acc=0.0938
2025-10-13 22:43:00,014 - INFO - _models.training_function_executor - Epoch 014/054 | train_loss=5.8940 | val_loss=5.9491 | val_acc=0.0938
2025-10-13 22:43:00,017 - INFO - _models.training_function_executor - Epoch 015/054 | train_loss=6.1127 | val_loss=5.8647 | val_acc=0.0938
2025-10-13 22:43:00,021 - INFO - _models.training_function_executor - Epoch 016/054 | train_loss=5.8180 | val_loss=5.7829 | val_acc=0.0938
2025-10-13 22:43:00,024 - INFO - _models.training_function_executor - Epoch 017/054 | train_loss=6.1732 | val_loss=5.7031 | val_acc=0.0938
2025-10-13 22:43:00,027 - INFO - _models.training_function_executor - Epoch 018/054 | train_loss=6.4237 | val_loss=5.6224 | val_acc=0.0938
2025-10-13 22:43:00,030 - INFO - _models.training_function_executor - Epoch 019/054 | train_loss=6.3159 | val_loss=5.5494 | val_acc=0.0938
2025-10-13 22:43:00,033 - INFO - _models.training_function_executor - Epoch 020/054 | train_loss=5.4142 | val_loss=5.4760 | val_acc=0.0938
2025-10-13 22:43:00,036 - INFO - _models.training_function_executor - Epoch 021/054 | train_loss=6.2326 | val_loss=5.4064 | val_acc=0.0938
2025-10-13 22:43:00,040 - INFO - _models.training_function_executor - Epoch 022/054 | train_loss=5.4280 | val_loss=5.3396 | val_acc=0.0938
2025-10-13 22:43:00,043 - INFO - _models.training_function_executor - Epoch 023/054 | train_loss=5.8676 | val_loss=5.2734 | val_acc=0.0938
2025-10-13 22:43:00,046 - INFO - _models.training_function_executor - Epoch 024/054 | train_loss=5.6652 | val_loss=5.2137 | val_acc=0.0938
2025-10-13 22:43:00,050 - INFO - _models.training_function_executor - Epoch 025/054 | train_loss=5.8125 | val_loss=5.1569 | val_acc=0.0938
2025-10-13 22:43:00,055 - INFO - _models.training_function_executor - Epoch 026/054 | train_loss=6.1914 | val_loss=5.1013 | val_acc=0.0938
2025-10-13 22:43:00,058 - INFO - _models.training_function_executor - Epoch 027/054 | train_loss=5.0850 | val_loss=5.0486 | val_acc=0.0938
2025-10-13 22:43:00,062 - INFO - _models.training_function_executor - Epoch 028/054 | train_loss=5.5855 | val_loss=5.0016 | val_acc=0.0938
2025-10-13 22:43:00,066 - INFO - _models.training_function_executor - Epoch 029/054 | train_loss=5.1997 | val_loss=4.9556 | val_acc=0.0938
2025-10-13 22:43:00,069 - INFO - _models.training_function_executor - Epoch 030/054 | train_loss=5.0335 | val_loss=4.9070 | val_acc=0.0938
2025-10-13 22:43:00,073 - INFO - _models.training_function_executor - Epoch 031/054 | train_loss=5.3224 | val_loss=4.8699 | val_acc=0.0938
2025-10-13 22:43:00,076 - INFO - _models.training_function_executor - Epoch 032/054 | train_loss=5.5544 | val_loss=4.8309 | val_acc=0.0938
2025-10-13 22:43:00,080 - INFO - _models.training_function_executor - Epoch 033/054 | train_loss=5.2520 | val_loss=4.7932 | val_acc=0.0938
2025-10-13 22:43:00,083 - INFO - _models.training_function_executor - Epoch 034/054 | train_loss=4.3415 | val_loss=4.7659 | val_acc=0.0938
2025-10-13 22:43:00,086 - INFO - _models.training_function_executor - Epoch 035/054 | train_loss=5.0896 | val_loss=4.7344 | val_acc=0.0938
2025-10-13 22:43:00,089 - INFO - _models.training_function_executor - Epoch 036/054 | train_loss=5.3977 | val_loss=4.7069 | val_acc=0.0938
2025-10-13 22:43:00,092 - INFO - _models.training_function_executor - Epoch 037/054 | train_loss=5.3367 | val_loss=4.6833 | val_acc=0.0938
2025-10-13 22:43:00,095 - INFO - _models.training_function_executor - Epoch 038/054 | train_loss=5.5738 | val_loss=4.6608 | val_acc=0.0938
2025-10-13 22:43:00,098 - INFO - _models.training_function_executor - Epoch 039/054 | train_loss=4.7431 | val_loss=4.6398 | val_acc=0.0938
2025-10-13 22:43:00,102 - INFO - _models.training_function_executor - Epoch 040/054 | train_loss=5.1985 | val_loss=4.6219 | val_acc=0.0938
2025-10-13 22:43:00,105 - INFO - _models.training_function_executor - Epoch 041/054 | train_loss=4.9432 | val_loss=4.6101 | val_acc=0.0938
2025-10-13 22:43:00,108 - INFO - _models.training_function_executor - Epoch 042/054 | train_loss=4.8860 | val_loss=4.5976 | val_acc=0.0938
2025-10-13 22:43:00,112 - INFO - _models.training_function_executor - Epoch 043/054 | train_loss=5.4793 | val_loss=4.5860 | val_acc=0.0938
2025-10-13 22:43:00,115 - INFO - _models.training_function_executor - Epoch 044/054 | train_loss=5.1733 | val_loss=4.5759 | val_acc=0.0938
2025-10-13 22:43:00,118 - INFO - _models.training_function_executor - Epoch 045/054 | train_loss=5.0806 | val_loss=4.5677 | val_acc=0.0938
2025-10-13 22:43:00,121 - INFO - _models.training_function_executor - Epoch 046/054 | train_loss=5.2530 | val_loss=4.5609 | val_acc=0.0938
2025-10-13 22:43:00,124 - INFO - _models.training_function_executor - Epoch 047/054 | train_loss=4.9700 | val_loss=4.5549 | val_acc=0.0938
2025-10-13 22:43:00,128 - INFO - _models.training_function_executor - Epoch 048/054 | train_loss=5.1786 | val_loss=4.5507 | val_acc=0.0938
2025-10-13 22:43:00,131 - INFO - _models.training_function_executor - Epoch 049/054 | train_loss=5.2237 | val_loss=4.5480 | val_acc=0.0938
2025-10-13 22:43:00,134 - INFO - _models.training_function_executor - Epoch 050/054 | train_loss=5.0889 | val_loss=4.5460 | val_acc=0.0938
2025-10-13 22:43:00,137 - INFO - _models.training_function_executor - Epoch 051/054 | train_loss=5.3305 | val_loss=4.5440 | val_acc=0.0938
2025-10-13 22:43:00,140 - INFO - _models.training_function_executor - Epoch 052/054 | train_loss=4.7379 | val_loss=4.5429 | val_acc=0.0938
2025-10-13 22:43:00,144 - INFO - _models.training_function_executor - Epoch 053/054 | train_loss=5.0678 | val_loss=4.5423 | val_acc=0.0938
2025-10-13 22:43:00,147 - INFO - _models.training_function_executor - Epoch 054/054 | train_loss=4.9853 | val_loss=4.5423 | val_acc=0.0938
2025-10-13 22:43:01,003 - INFO - _models.training_function_executor - Model: 15,985 parameters, 68.7KB storage
2025-10-13 22:43:01,003 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [6.818563938140869, 6.613771915435791, 6.141012191772461, 6.063486099243164, 6.163894176483154, 5.965518951416016, 6.67334508895874, 6.606844425201416, 6.566409587860107, 6.446390151977539, 6.312305927276611, 6.138044357299805, 5.945180892944336, 5.89402961730957, 6.112687110900879, 5.818026542663574, 6.173155307769775, 6.423716068267822, 6.315900802612305, 5.4142069816589355, 6.232614517211914, 5.427971839904785, 5.867644786834717, 5.66515588760376, 5.812506198883057, 6.191434383392334, 5.084983825683594, 5.585494041442871, 5.1996588706970215, 5.033530235290527, 5.322403907775879, 5.554365158081055, 5.252039909362793, 4.341518402099609, 5.089603900909424, 5.397656440734863, 5.336741924285889, 5.573821544647217, 4.74310827255249, 5.198492527008057, 4.9431867599487305, 4.885986328125, 5.479287147521973, 5.1732892990112305, 5.080580234527588, 5.2530364990234375, 4.9700493812561035, 5.178617000579834, 5.22368860244751, 5.088910102844238, 5.33049201965332, 4.737934589385986, 5.067802429199219, 4.9853129386901855], 'val_losses': [6.6874680519104, 6.6874680519104, 6.6874680519104, 6.6874680519104, 6.6874680519104, 6.6874680519104, 6.586230754852295, 6.488882541656494, 6.3964152336120605, 6.305685997009277, 6.214467525482178, 6.122517108917236, 6.032169818878174, 5.949123859405518, 5.864736557006836, 5.782919406890869, 5.703103065490723, 5.622409820556641, 5.549428462982178, 5.476009368896484, 5.406439781188965, 5.339591979980469, 5.273394584655762, 5.213723659515381, 5.1568922996521, 5.101261615753174, 5.048640727996826, 5.001644134521484, 4.955624580383301, 4.906992435455322, 4.869876384735107, 4.83094596862793, 4.793249607086182, 4.765902519226074, 4.7344441413879395, 4.706921577453613, 4.683320045471191, 4.660750389099121, 4.6398162841796875, 4.621948719024658, 4.610079765319824, 4.597575664520264, 4.586025714874268, 4.575917720794678, 4.567691802978516, 4.560940265655518, 4.55493688583374, 4.55068826675415, 4.548037528991699, 4.545957565307617, 4.544010162353516, 4.542896270751953, 4.542273998260498, 4.542273998260498], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.082448236769665e-05, 'batch_size': 256, 'epochs': 54, 'hidden_size': 122, 'dropout': 0.1376481086773302, 'weight_decay': 0.006087177517266088, 'label_smoothing': 0.08175535372464002, 'grad_clip_norm': 2.429849466520067, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 15985, 'model_storage_size_kb': 68.685546875, 'model_size_validation': 'PASS'}
2025-10-13 22:43:01,003 - INFO - _models.training_function_executor - BO Objective: base=0.0938, size_penalty=0.0000, final=0.0938
2025-10-13 22:43:01,003 - INFO - _models.training_function_executor - Model: 15,985 parameters, 68.7KB (PASS 256KB limit)
2025-10-13 22:43:01,003 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.783s
2025-10-13 22:43:01,079 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0938
2025-10-13 22:43:01,079 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.076s
2025-10-13 22:43:01,079 - INFO - bo.run_bo - Recorded observation #28: hparams={'lr': 1.082448236769665e-05, 'batch_size': np.int64(256), 'epochs': np.int64(54), 'hidden_size': np.int64(122), 'dropout': 0.1376481086773302, 'weight_decay': 0.006087177517266088, 'label_smoothing': 0.08175535372464002, 'grad_clip_norm': 2.429849466520067, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.0938
2025-10-13 22:43:01,079 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 28: {'lr': 1.082448236769665e-05, 'batch_size': np.int64(256), 'epochs': np.int64(54), 'hidden_size': np.int64(122), 'dropout': 0.1376481086773302, 'weight_decay': 0.006087177517266088, 'label_smoothing': 0.08175535372464002, 'grad_clip_norm': 2.429849466520067, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.0938
2025-10-13 22:43:01,079 - INFO - bo.run_bo - üîçBO Trial 29: Using RF surrogate + Expected Improvement
2025-10-13 22:43:01,079 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:01,079 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 29 (NaN monitoring active)
2025-10-13 22:43:01,079 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:01,080 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:01,080 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0644086132742091e-05, 'batch_size': 192, 'epochs': 63, 'hidden_size': 192, 'dropout': 0.1069163275336649, 'weight_decay': 2.4965493991712193e-05, 'label_smoothing': 0.01711238616469995, 'grad_clip_norm': 3.679365018533577, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:43:01,080 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0644086132742091e-05, 'batch_size': 192, 'epochs': 63, 'hidden_size': 192, 'dropout': 0.1069163275336649, 'weight_decay': 2.4965493991712193e-05, 'label_smoothing': 0.01711238616469995, 'grad_clip_norm': 3.679365018533577, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:43:04,664 - INFO - _models.training_function_executor - Epoch 001/063 | train_loss=2.8882 | val_loss=1.4299 | val_acc=0.1562
2025-10-13 22:43:04,670 - INFO - _models.training_function_executor - Epoch 002/063 | train_loss=3.0055 | val_loss=1.4299 | val_acc=0.1562
2025-10-13 22:43:04,675 - INFO - _models.training_function_executor - Epoch 003/063 | train_loss=3.1784 | val_loss=1.3240 | val_acc=0.1719
2025-10-13 22:43:04,680 - INFO - _models.training_function_executor - Epoch 004/063 | train_loss=3.1383 | val_loss=1.2389 | val_acc=0.2656
2025-10-13 22:43:04,685 - INFO - _models.training_function_executor - Epoch 005/063 | train_loss=3.1464 | val_loss=1.1743 | val_acc=0.3438
2025-10-13 22:43:04,689 - INFO - _models.training_function_executor - Epoch 006/063 | train_loss=2.7498 | val_loss=1.1249 | val_acc=0.3906
2025-10-13 22:43:04,693 - INFO - _models.training_function_executor - Epoch 007/063 | train_loss=2.6120 | val_loss=1.1077 | val_acc=0.4062
2025-10-13 22:43:04,697 - INFO - _models.training_function_executor - Epoch 008/063 | train_loss=2.6826 | val_loss=1.0858 | val_acc=0.4688
2025-10-13 22:43:04,702 - INFO - _models.training_function_executor - Epoch 009/063 | train_loss=2.3796 | val_loss=1.0758 | val_acc=0.5469
2025-10-13 22:43:04,706 - INFO - _models.training_function_executor - Epoch 010/063 | train_loss=2.5477 | val_loss=1.0782 | val_acc=0.5625
2025-10-13 22:43:04,710 - INFO - _models.training_function_executor - Epoch 011/063 | train_loss=2.6194 | val_loss=1.0927 | val_acc=0.6094
2025-10-13 22:43:04,715 - INFO - _models.training_function_executor - Epoch 012/063 | train_loss=2.2699 | val_loss=1.1161 | val_acc=0.6406
2025-10-13 22:43:04,719 - INFO - _models.training_function_executor - Epoch 013/063 | train_loss=2.6491 | val_loss=1.1458 | val_acc=0.6719
2025-10-13 22:43:04,723 - INFO - _models.training_function_executor - Epoch 014/063 | train_loss=2.1670 | val_loss=1.1811 | val_acc=0.6875
2025-10-13 22:43:04,728 - INFO - _models.training_function_executor - Epoch 015/063 | train_loss=2.1128 | val_loss=1.2176 | val_acc=0.7031
2025-10-13 22:43:04,732 - INFO - _models.training_function_executor - Epoch 016/063 | train_loss=2.1547 | val_loss=1.2535 | val_acc=0.7188
2025-10-13 22:43:04,737 - INFO - _models.training_function_executor - Epoch 017/063 | train_loss=2.2650 | val_loss=1.2916 | val_acc=0.7188
2025-10-13 22:43:04,741 - INFO - _models.training_function_executor - Epoch 018/063 | train_loss=2.0535 | val_loss=1.3277 | val_acc=0.7188
2025-10-13 22:43:04,746 - INFO - _models.training_function_executor - Epoch 019/063 | train_loss=2.0789 | val_loss=1.3560 | val_acc=0.7188
2025-10-13 22:43:04,751 - INFO - _models.training_function_executor - Epoch 020/063 | train_loss=2.0866 | val_loss=1.3814 | val_acc=0.7188
2025-10-13 22:43:04,756 - INFO - _models.training_function_executor - Epoch 021/063 | train_loss=2.1153 | val_loss=1.4000 | val_acc=0.7188
2025-10-13 22:43:04,760 - INFO - _models.training_function_executor - Epoch 022/063 | train_loss=1.8976 | val_loss=1.4198 | val_acc=0.7188
2025-10-13 22:43:04,765 - INFO - _models.training_function_executor - Epoch 023/063 | train_loss=2.0840 | val_loss=1.4375 | val_acc=0.7188
2025-10-13 22:43:04,770 - INFO - _models.training_function_executor - Epoch 024/063 | train_loss=2.1150 | val_loss=1.4512 | val_acc=0.7188
2025-10-13 22:43:04,774 - INFO - _models.training_function_executor - Epoch 025/063 | train_loss=1.9542 | val_loss=1.4574 | val_acc=0.7188
2025-10-13 22:43:04,778 - INFO - _models.training_function_executor - Epoch 026/063 | train_loss=1.9727 | val_loss=1.4587 | val_acc=0.7188
2025-10-13 22:43:04,782 - INFO - _models.training_function_executor - Epoch 027/063 | train_loss=2.1423 | val_loss=1.4582 | val_acc=0.7188
2025-10-13 22:43:04,787 - INFO - _models.training_function_executor - Epoch 028/063 | train_loss=1.9355 | val_loss=1.4517 | val_acc=0.7188
2025-10-13 22:43:04,791 - INFO - _models.training_function_executor - Epoch 029/063 | train_loss=2.1060 | val_loss=1.4513 | val_acc=0.7188
2025-10-13 22:43:04,795 - INFO - _models.training_function_executor - Epoch 030/063 | train_loss=1.7563 | val_loss=1.4530 | val_acc=0.7188
2025-10-13 22:43:04,799 - INFO - _models.training_function_executor - Epoch 031/063 | train_loss=1.9165 | val_loss=1.4520 | val_acc=0.7188
2025-10-13 22:43:04,804 - INFO - _models.training_function_executor - Epoch 032/063 | train_loss=2.0962 | val_loss=1.4489 | val_acc=0.7188
2025-10-13 22:43:04,809 - INFO - _models.training_function_executor - Epoch 033/063 | train_loss=2.0790 | val_loss=1.4468 | val_acc=0.7188
2025-10-13 22:43:04,813 - INFO - _models.training_function_executor - Epoch 034/063 | train_loss=1.8058 | val_loss=1.4409 | val_acc=0.7188
2025-10-13 22:43:04,817 - INFO - _models.training_function_executor - Epoch 035/063 | train_loss=1.9697 | val_loss=1.4348 | val_acc=0.7188
2025-10-13 22:43:04,821 - INFO - _models.training_function_executor - Epoch 036/063 | train_loss=1.8944 | val_loss=1.4310 | val_acc=0.7188
2025-10-13 22:43:04,825 - INFO - _models.training_function_executor - Epoch 037/063 | train_loss=2.2303 | val_loss=1.4261 | val_acc=0.7188
2025-10-13 22:43:04,830 - INFO - _models.training_function_executor - Epoch 038/063 | train_loss=1.7716 | val_loss=1.4216 | val_acc=0.7188
2025-10-13 22:43:04,834 - INFO - _models.training_function_executor - Epoch 039/063 | train_loss=1.9002 | val_loss=1.4183 | val_acc=0.7188
2025-10-13 22:43:04,838 - INFO - _models.training_function_executor - Epoch 040/063 | train_loss=2.0789 | val_loss=1.4133 | val_acc=0.7188
2025-10-13 22:43:04,842 - INFO - _models.training_function_executor - Epoch 041/063 | train_loss=1.7106 | val_loss=1.4087 | val_acc=0.7188
2025-10-13 22:43:04,846 - INFO - _models.training_function_executor - Epoch 042/063 | train_loss=1.8658 | val_loss=1.4029 | val_acc=0.7188
2025-10-13 22:43:04,850 - INFO - _models.training_function_executor - Epoch 043/063 | train_loss=1.8661 | val_loss=1.3965 | val_acc=0.7188
2025-10-13 22:43:04,854 - INFO - _models.training_function_executor - Epoch 044/063 | train_loss=1.9187 | val_loss=1.3915 | val_acc=0.7188
2025-10-13 22:43:04,859 - INFO - _models.training_function_executor - Epoch 045/063 | train_loss=2.1815 | val_loss=1.3882 | val_acc=0.7188
2025-10-13 22:43:04,863 - INFO - _models.training_function_executor - Epoch 046/063 | train_loss=1.8968 | val_loss=1.3863 | val_acc=0.7188
2025-10-13 22:43:04,868 - INFO - _models.training_function_executor - Epoch 047/063 | train_loss=2.0631 | val_loss=1.3848 | val_acc=0.7188
2025-10-13 22:43:04,872 - INFO - _models.training_function_executor - Epoch 048/063 | train_loss=2.1244 | val_loss=1.3839 | val_acc=0.7188
2025-10-13 22:43:04,876 - INFO - _models.training_function_executor - Epoch 049/063 | train_loss=2.1327 | val_loss=1.3821 | val_acc=0.7188
2025-10-13 22:43:04,880 - INFO - _models.training_function_executor - Epoch 050/063 | train_loss=1.8050 | val_loss=1.3808 | val_acc=0.7188
2025-10-13 22:43:04,884 - INFO - _models.training_function_executor - Epoch 051/063 | train_loss=2.0795 | val_loss=1.3806 | val_acc=0.7188
2025-10-13 22:43:04,888 - INFO - _models.training_function_executor - Epoch 052/063 | train_loss=2.0085 | val_loss=1.3805 | val_acc=0.7188
2025-10-13 22:43:04,892 - INFO - _models.training_function_executor - Epoch 053/063 | train_loss=1.9128 | val_loss=1.3803 | val_acc=0.7188
2025-10-13 22:43:04,897 - INFO - _models.training_function_executor - Epoch 054/063 | train_loss=1.9246 | val_loss=1.3806 | val_acc=0.7188
2025-10-13 22:43:04,901 - INFO - _models.training_function_executor - Epoch 055/063 | train_loss=1.8531 | val_loss=1.3806 | val_acc=0.7188
2025-10-13 22:43:04,905 - INFO - _models.training_function_executor - Epoch 056/063 | train_loss=1.9462 | val_loss=1.3802 | val_acc=0.7188
2025-10-13 22:43:04,909 - INFO - _models.training_function_executor - Epoch 057/063 | train_loss=1.8322 | val_loss=1.3791 | val_acc=0.7188
2025-10-13 22:43:04,913 - INFO - _models.training_function_executor - Epoch 058/063 | train_loss=2.0200 | val_loss=1.3790 | val_acc=0.7188
2025-10-13 22:43:04,918 - INFO - _models.training_function_executor - Epoch 059/063 | train_loss=1.8226 | val_loss=1.3787 | val_acc=0.7188
2025-10-13 22:43:04,922 - INFO - _models.training_function_executor - Epoch 060/063 | train_loss=2.1847 | val_loss=1.3787 | val_acc=0.7188
2025-10-13 22:43:04,926 - INFO - _models.training_function_executor - Epoch 061/063 | train_loss=1.9733 | val_loss=1.3788 | val_acc=0.7188
2025-10-13 22:43:04,931 - INFO - _models.training_function_executor - Epoch 062/063 | train_loss=1.7607 | val_loss=1.3788 | val_acc=0.7188
2025-10-13 22:43:04,935 - INFO - _models.training_function_executor - Epoch 063/063 | train_loss=2.2080 | val_loss=1.3788 | val_acc=0.7188
2025-10-13 22:43:05,775 - INFO - _models.training_function_executor - Model: 38,595 parameters, 165.8KB storage
2025-10-13 22:43:05,775 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.888226091861725, 3.005462110042572, 3.1784170866012573, 3.138273596763611, 3.146394908428192, 2.7497562170028687, 2.612019419670105, 2.6825669407844543, 2.3795865178108215, 2.5477405190467834, 2.619417905807495, 2.2698881030082703, 2.649071455001831, 2.1669886708259583, 2.1128289699554443, 2.154673784971237, 2.264996111392975, 2.053476333618164, 2.0788568258285522, 2.0866220593452454, 2.115263521671295, 1.8976255357265472, 2.0839741826057434, 2.1149939596652985, 1.9541797041893005, 1.9727293848991394, 2.142348140478134, 1.9355182349681854, 2.105986773967743, 1.7563315629959106, 1.9165244102478027, 2.096182107925415, 2.0789880752563477, 1.805825799703598, 1.9696859121322632, 1.8944327533245087, 2.230253517627716, 1.7715778648853302, 1.9001576602458954, 2.07888263463974, 1.710579663515091, 1.8658116161823273, 1.8660686016082764, 1.9187186360359192, 2.1815227270126343, 1.8967711329460144, 2.0630620419979095, 2.124436676502228, 2.132727861404419, 1.8049660623073578, 2.0794779658317566, 2.008488029241562, 1.9127863049507141, 1.9246246218681335, 1.85306715965271, 1.946164846420288, 1.8322165310382843, 2.0199914276599884, 1.8226076662540436, 2.184658169746399, 1.9732969999313354, 1.7606501579284668, 2.2079548835754395], 'val_losses': [1.429875373840332, 1.429875373840332, 1.3239634037017822, 1.2388544082641602, 1.1742767095565796, 1.1249189376831055, 1.1076745986938477, 1.0858495235443115, 1.0757700204849243, 1.0781614780426025, 1.092697024345398, 1.116144061088562, 1.145755648612976, 1.1811015605926514, 1.2176215648651123, 1.2534700632095337, 1.2916464805603027, 1.327670693397522, 1.3559744358062744, 1.3813815116882324, 1.4000117778778076, 1.419776439666748, 1.4375312328338623, 1.4511773586273193, 1.4573848247528076, 1.4587329626083374, 1.458248496055603, 1.4517276287078857, 1.4512840509414673, 1.4530287981033325, 1.4520459175109863, 1.4488756656646729, 1.446824073791504, 1.4408503770828247, 1.434753179550171, 1.4310426712036133, 1.4260567426681519, 1.4216386079788208, 1.4182747602462769, 1.4132575988769531, 1.408713936805725, 1.4028970003128052, 1.3965216875076294, 1.3915470838546753, 1.3881635665893555, 1.386320948600769, 1.3848119974136353, 1.3839257955551147, 1.382071614265442, 1.380785584449768, 1.380609154701233, 1.380468726158142, 1.3803223371505737, 1.380552053451538, 1.380612850189209, 1.380210280418396, 1.379123330116272, 1.379001259803772, 1.3787281513214111, 1.378736972808838, 1.378847360610962, 1.3788455724716187, 1.3788458108901978], 'val_acc': [0.15625, 0.15625, 0.171875, 0.265625, 0.34375, 0.390625, 0.40625, 0.46875, 0.546875, 0.5625, 0.609375, 0.640625, 0.671875, 0.6875, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0644086132742091e-05, 'batch_size': 192, 'epochs': 63, 'hidden_size': 192, 'dropout': 0.1069163275336649, 'weight_decay': 2.4965493991712193e-05, 'label_smoothing': 0.01711238616469995, 'grad_clip_norm': 3.679365018533577, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 38595, 'model_storage_size_kb': 165.837890625, 'model_size_validation': 'PASS'}
2025-10-13 22:43:05,776 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:43:05,776 - INFO - _models.training_function_executor - Model: 38,595 parameters, 165.8KB (PASS 256KB limit)
2025-10-13 22:43:05,776 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.696s
2025-10-13 22:43:05,851 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:43:05,851 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.075s
2025-10-13 22:43:05,851 - INFO - bo.run_bo - Recorded observation #29: hparams={'lr': 1.0644086132742091e-05, 'batch_size': np.int64(192), 'epochs': np.int64(63), 'hidden_size': np.int64(192), 'dropout': 0.1069163275336649, 'weight_decay': 2.4965493991712193e-05, 'label_smoothing': 0.01711238616469995, 'grad_clip_norm': 3.679365018533577, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:43:05,851 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 29: {'lr': 1.0644086132742091e-05, 'batch_size': np.int64(192), 'epochs': np.int64(63), 'hidden_size': np.int64(192), 'dropout': 0.1069163275336649, 'weight_decay': 2.4965493991712193e-05, 'label_smoothing': 0.01711238616469995, 'grad_clip_norm': 3.679365018533577, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:43:05,852 - INFO - bo.run_bo - üîçBO Trial 30: Using RF surrogate + Expected Improvement
2025-10-13 22:43:05,852 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:05,852 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 30 (NaN monitoring active)
2025-10-13 22:43:05,852 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:05,852 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:05,852 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.9387896345340384e-05, 'batch_size': 256, 'epochs': 42, 'hidden_size': 130, 'dropout': 0.046741754840987255, 'weight_decay': 9.350946218711162e-06, 'label_smoothing': 0.03119149018166852, 'grad_clip_norm': 1.9421838542826795, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:05,853 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.9387896345340384e-05, 'batch_size': 256, 'epochs': 42, 'hidden_size': 130, 'dropout': 0.046741754840987255, 'weight_decay': 9.350946218711162e-06, 'label_smoothing': 0.03119149018166852, 'grad_clip_norm': 1.9421838542826795, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:09,539 - INFO - _models.training_function_executor - Epoch 001/042 | train_loss=6.9538 | val_loss=6.8395 | val_acc=0.0938
2025-10-13 22:43:09,544 - INFO - _models.training_function_executor - Epoch 002/042 | train_loss=6.9537 | val_loss=6.8395 | val_acc=0.0938
2025-10-13 22:43:09,548 - INFO - _models.training_function_executor - Epoch 003/042 | train_loss=7.0614 | val_loss=6.8395 | val_acc=0.0938
2025-10-13 22:43:09,552 - INFO - _models.training_function_executor - Epoch 004/042 | train_loss=6.9564 | val_loss=6.8395 | val_acc=0.0938
2025-10-13 22:43:09,555 - INFO - _models.training_function_executor - Epoch 005/042 | train_loss=7.1534 | val_loss=6.8395 | val_acc=0.0938
2025-10-13 22:43:09,558 - INFO - _models.training_function_executor - Epoch 006/042 | train_loss=7.0585 | val_loss=6.8395 | val_acc=0.0938
2025-10-13 22:43:09,562 - INFO - _models.training_function_executor - Epoch 007/042 | train_loss=7.2708 | val_loss=6.6399 | val_acc=0.0938
2025-10-13 22:43:09,565 - INFO - _models.training_function_executor - Epoch 008/042 | train_loss=6.7102 | val_loss=6.4547 | val_acc=0.0938
2025-10-13 22:43:09,568 - INFO - _models.training_function_executor - Epoch 009/042 | train_loss=6.5751 | val_loss=6.2673 | val_acc=0.0938
2025-10-13 22:43:09,571 - INFO - _models.training_function_executor - Epoch 010/042 | train_loss=6.4630 | val_loss=6.0896 | val_acc=0.0938
2025-10-13 22:43:09,574 - INFO - _models.training_function_executor - Epoch 011/042 | train_loss=6.2128 | val_loss=5.9141 | val_acc=0.0938
2025-10-13 22:43:09,578 - INFO - _models.training_function_executor - Epoch 012/042 | train_loss=6.0786 | val_loss=5.7403 | val_acc=0.0938
2025-10-13 22:43:09,581 - INFO - _models.training_function_executor - Epoch 013/042 | train_loss=5.9823 | val_loss=5.5791 | val_acc=0.0938
2025-10-13 22:43:09,584 - INFO - _models.training_function_executor - Epoch 014/042 | train_loss=5.6277 | val_loss=5.4192 | val_acc=0.0938
2025-10-13 22:43:09,587 - INFO - _models.training_function_executor - Epoch 015/042 | train_loss=5.8581 | val_loss=5.2669 | val_acc=0.0938
2025-10-13 22:43:09,590 - INFO - _models.training_function_executor - Epoch 016/042 | train_loss=5.3999 | val_loss=5.1214 | val_acc=0.0938
2025-10-13 22:43:09,594 - INFO - _models.training_function_executor - Epoch 017/042 | train_loss=5.1240 | val_loss=4.9818 | val_acc=0.0938
2025-10-13 22:43:09,597 - INFO - _models.training_function_executor - Epoch 018/042 | train_loss=5.2447 | val_loss=4.8525 | val_acc=0.0938
2025-10-13 22:43:09,600 - INFO - _models.training_function_executor - Epoch 019/042 | train_loss=5.0968 | val_loss=4.7321 | val_acc=0.0938
2025-10-13 22:43:09,603 - INFO - _models.training_function_executor - Epoch 020/042 | train_loss=4.8903 | val_loss=4.6162 | val_acc=0.0938
2025-10-13 22:43:09,606 - INFO - _models.training_function_executor - Epoch 021/042 | train_loss=4.8984 | val_loss=4.5071 | val_acc=0.0938
2025-10-13 22:43:09,609 - INFO - _models.training_function_executor - Epoch 022/042 | train_loss=5.0030 | val_loss=4.4102 | val_acc=0.0938
2025-10-13 22:43:09,613 - INFO - _models.training_function_executor - Epoch 023/042 | train_loss=4.5148 | val_loss=4.3169 | val_acc=0.0938
2025-10-13 22:43:09,616 - INFO - _models.training_function_executor - Epoch 024/042 | train_loss=4.4865 | val_loss=4.2328 | val_acc=0.0938
2025-10-13 22:43:09,619 - INFO - _models.training_function_executor - Epoch 025/042 | train_loss=4.4247 | val_loss=4.1531 | val_acc=0.0938
2025-10-13 22:43:09,623 - INFO - _models.training_function_executor - Epoch 026/042 | train_loss=4.5246 | val_loss=4.0819 | val_acc=0.0938
2025-10-13 22:43:09,626 - INFO - _models.training_function_executor - Epoch 027/042 | train_loss=4.0890 | val_loss=4.0228 | val_acc=0.0938
2025-10-13 22:43:09,630 - INFO - _models.training_function_executor - Epoch 028/042 | train_loss=4.2629 | val_loss=3.9700 | val_acc=0.0938
2025-10-13 22:43:09,633 - INFO - _models.training_function_executor - Epoch 029/042 | train_loss=4.1107 | val_loss=3.9194 | val_acc=0.0938
2025-10-13 22:43:09,637 - INFO - _models.training_function_executor - Epoch 030/042 | train_loss=4.0848 | val_loss=3.8748 | val_acc=0.0938
2025-10-13 22:43:09,640 - INFO - _models.training_function_executor - Epoch 031/042 | train_loss=3.9915 | val_loss=3.8387 | val_acc=0.0938
2025-10-13 22:43:09,644 - INFO - _models.training_function_executor - Epoch 032/042 | train_loss=4.0490 | val_loss=3.8085 | val_acc=0.0938
2025-10-13 22:43:09,647 - INFO - _models.training_function_executor - Epoch 033/042 | train_loss=4.1057 | val_loss=3.7804 | val_acc=0.0938
2025-10-13 22:43:09,651 - INFO - _models.training_function_executor - Epoch 034/042 | train_loss=4.1130 | val_loss=3.7602 | val_acc=0.0938
2025-10-13 22:43:09,654 - INFO - _models.training_function_executor - Epoch 035/042 | train_loss=3.9303 | val_loss=3.7401 | val_acc=0.0938
2025-10-13 22:43:09,657 - INFO - _models.training_function_executor - Epoch 036/042 | train_loss=4.1357 | val_loss=3.7273 | val_acc=0.0938
2025-10-13 22:43:09,660 - INFO - _models.training_function_executor - Epoch 037/042 | train_loss=3.6559 | val_loss=3.7148 | val_acc=0.0938
2025-10-13 22:43:09,663 - INFO - _models.training_function_executor - Epoch 038/042 | train_loss=3.9855 | val_loss=3.7097 | val_acc=0.0938
2025-10-13 22:43:09,666 - INFO - _models.training_function_executor - Epoch 039/042 | train_loss=3.8703 | val_loss=3.7046 | val_acc=0.0938
2025-10-13 22:43:09,669 - INFO - _models.training_function_executor - Epoch 040/042 | train_loss=3.9794 | val_loss=3.7026 | val_acc=0.0938
2025-10-13 22:43:09,672 - INFO - _models.training_function_executor - Epoch 041/042 | train_loss=3.8811 | val_loss=3.7021 | val_acc=0.0938
2025-10-13 22:43:09,675 - INFO - _models.training_function_executor - Epoch 042/042 | train_loss=3.9430 | val_loss=3.7017 | val_acc=0.0938
2025-10-13 22:43:10,522 - INFO - _models.training_function_executor - Model: 18,073 parameters, 77.7KB storage
2025-10-13 22:43:10,522 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [6.953787803649902, 6.953710556030273, 7.061428546905518, 6.956428527832031, 7.153407573699951, 7.0584635734558105, 7.270806789398193, 6.710180282592773, 6.575084686279297, 6.463038921356201, 6.212757587432861, 6.07857608795166, 5.982316970825195, 5.627716064453125, 5.858128547668457, 5.399885654449463, 5.123973369598389, 5.244669437408447, 5.0967698097229, 4.890283584594727, 4.898406028747559, 5.002988338470459, 4.514795780181885, 4.486479759216309, 4.424681663513184, 4.524555206298828, 4.088973045349121, 4.262933731079102, 4.110745429992676, 4.084779262542725, 3.9915311336517334, 4.049035549163818, 4.105695724487305, 4.112950801849365, 3.930294990539551, 4.135703086853027, 3.655918598175049, 3.985530138015747, 3.8702802658081055, 3.979390859603882, 3.8810505867004395, 3.9430088996887207], 'val_losses': [6.839537143707275, 6.839537143707275, 6.839537143707275, 6.839537143707275, 6.839537143707275, 6.839537143707275, 6.639868259429932, 6.4546918869018555, 6.2672505378723145, 6.089557647705078, 5.914106369018555, 5.740272521972656, 5.579136848449707, 5.419209957122803, 5.266866207122803, 5.121409893035889, 4.981825828552246, 4.852466106414795, 4.732066631317139, 4.616154670715332, 4.507077217102051, 4.410155296325684, 4.316946506500244, 4.232754707336426, 4.153080940246582, 4.081911087036133, 4.022837162017822, 3.9699974060058594, 3.9194438457489014, 3.874802589416504, 3.8387162685394287, 3.8085203170776367, 3.7803778648376465, 3.760199546813965, 3.740070104598999, 3.7272727489471436, 3.714773178100586, 3.709662914276123, 3.7045929431915283, 3.7026381492614746, 3.702078342437744, 3.7016568183898926], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.9387896345340384e-05, 'batch_size': 256, 'epochs': 42, 'hidden_size': 130, 'dropout': 0.046741754840987255, 'weight_decay': 9.350946218711162e-06, 'label_smoothing': 0.03119149018166852, 'grad_clip_norm': 1.9421838542826795, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 18073, 'model_storage_size_kb': 77.65742187500001, 'model_size_validation': 'PASS'}
2025-10-13 22:43:10,522 - INFO - _models.training_function_executor - BO Objective: base=0.0938, size_penalty=0.0000, final=0.0938
2025-10-13 22:43:10,522 - INFO - _models.training_function_executor - Model: 18,073 parameters, 77.7KB (PASS 256KB limit)
2025-10-13 22:43:10,522 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.670s
2025-10-13 22:43:10,598 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0938
2025-10-13 22:43:10,598 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.075s
2025-10-13 22:43:10,598 - INFO - bo.run_bo - Recorded observation #30: hparams={'lr': 2.9387896345340384e-05, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'hidden_size': np.int64(130), 'dropout': 0.046741754840987255, 'weight_decay': 9.350946218711162e-06, 'label_smoothing': 0.03119149018166852, 'grad_clip_norm': 1.9421838542826795, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.0938
2025-10-13 22:43:10,598 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 30: {'lr': 2.9387896345340384e-05, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'hidden_size': np.int64(130), 'dropout': 0.046741754840987255, 'weight_decay': 9.350946218711162e-06, 'label_smoothing': 0.03119149018166852, 'grad_clip_norm': 1.9421838542826795, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.0938
2025-10-13 22:43:10,598 - INFO - bo.run_bo - üîçBO Trial 31: Using RF surrogate + Expected Improvement
2025-10-13 22:43:10,598 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:10,598 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 31 (NaN monitoring active)
2025-10-13 22:43:10,598 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:10,598 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:10,598 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.007007193583244844, 'batch_size': 64, 'epochs': 137, 'hidden_size': 168, 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:43:10,599 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.007007193583244844, 'batch_size': 64, 'epochs': 137, 'hidden_size': 168, 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:43:14,467 - INFO - _models.training_function_executor - Epoch 001/137 | train_loss=3.5886 | val_loss=2.7172 | val_acc=0.1562
2025-10-13 22:43:14,476 - INFO - _models.training_function_executor - Epoch 002/137 | train_loss=12.9542 | val_loss=8.6264 | val_acc=0.7188
2025-10-13 22:43:14,483 - INFO - _models.training_function_executor - Epoch 003/137 | train_loss=4.9695 | val_loss=5.5588 | val_acc=0.0938
2025-10-13 22:43:14,490 - INFO - _models.training_function_executor - Epoch 004/137 | train_loss=3.0044 | val_loss=1.2329 | val_acc=0.7188
2025-10-13 22:43:14,497 - INFO - _models.training_function_executor - Epoch 005/137 | train_loss=1.4768 | val_loss=0.8597 | val_acc=0.7188
2025-10-13 22:43:14,503 - INFO - _models.training_function_executor - Epoch 006/137 | train_loss=0.8454 | val_loss=0.7054 | val_acc=0.7188
2025-10-13 22:43:14,510 - INFO - _models.training_function_executor - Epoch 007/137 | train_loss=0.7519 | val_loss=0.6621 | val_acc=0.7188
2025-10-13 22:43:14,516 - INFO - _models.training_function_executor - Epoch 008/137 | train_loss=0.7254 | val_loss=0.6437 | val_acc=0.7188
2025-10-13 22:43:14,523 - INFO - _models.training_function_executor - Epoch 009/137 | train_loss=0.7115 | val_loss=0.6296 | val_acc=0.7188
2025-10-13 22:43:14,529 - INFO - _models.training_function_executor - Epoch 010/137 | train_loss=0.7100 | val_loss=0.6387 | val_acc=0.7188
2025-10-13 22:43:14,536 - INFO - _models.training_function_executor - Epoch 011/137 | train_loss=0.7169 | val_loss=0.6785 | val_acc=0.7188
2025-10-13 22:43:14,542 - INFO - _models.training_function_executor - Epoch 012/137 | train_loss=0.7891 | val_loss=0.6525 | val_acc=0.7188
2025-10-13 22:43:14,550 - INFO - _models.training_function_executor - Epoch 013/137 | train_loss=0.7207 | val_loss=0.6446 | val_acc=0.7188
2025-10-13 22:43:14,558 - INFO - _models.training_function_executor - Epoch 014/137 | train_loss=0.6963 | val_loss=0.6374 | val_acc=0.7188
2025-10-13 22:43:14,567 - INFO - _models.training_function_executor - Epoch 015/137 | train_loss=0.6937 | val_loss=0.6286 | val_acc=0.7188
2025-10-13 22:43:14,574 - INFO - _models.training_function_executor - Epoch 016/137 | train_loss=0.7191 | val_loss=0.6676 | val_acc=0.7188
2025-10-13 22:43:14,581 - INFO - _models.training_function_executor - Epoch 017/137 | train_loss=0.7194 | val_loss=0.6587 | val_acc=0.7188
2025-10-13 22:43:14,588 - INFO - _models.training_function_executor - Epoch 018/137 | train_loss=0.6996 | val_loss=0.6231 | val_acc=0.7188
2025-10-13 22:43:14,594 - INFO - _models.training_function_executor - Epoch 019/137 | train_loss=0.6955 | val_loss=0.6328 | val_acc=0.7188
2025-10-13 22:43:14,600 - INFO - _models.training_function_executor - Epoch 020/137 | train_loss=0.6865 | val_loss=0.6412 | val_acc=0.7188
2025-10-13 22:43:14,607 - INFO - _models.training_function_executor - Epoch 021/137 | train_loss=0.6551 | val_loss=0.6150 | val_acc=0.7188
2025-10-13 22:43:14,613 - INFO - _models.training_function_executor - Epoch 022/137 | train_loss=0.7081 | val_loss=0.6368 | val_acc=0.7188
2025-10-13 22:43:14,619 - INFO - _models.training_function_executor - Epoch 023/137 | train_loss=0.6817 | val_loss=0.6325 | val_acc=0.7188
2025-10-13 22:43:14,626 - INFO - _models.training_function_executor - Epoch 024/137 | train_loss=0.7068 | val_loss=0.6210 | val_acc=0.7188
2025-10-13 22:43:14,632 - INFO - _models.training_function_executor - Epoch 025/137 | train_loss=0.7066 | val_loss=0.6280 | val_acc=0.7188
2025-10-13 22:43:14,638 - INFO - _models.training_function_executor - Epoch 026/137 | train_loss=0.7017 | val_loss=0.6500 | val_acc=0.7188
2025-10-13 22:43:14,644 - INFO - _models.training_function_executor - Epoch 027/137 | train_loss=0.6944 | val_loss=0.6195 | val_acc=0.7188
2025-10-13 22:43:14,651 - INFO - _models.training_function_executor - Epoch 028/137 | train_loss=0.6749 | val_loss=0.6304 | val_acc=0.7188
2025-10-13 22:43:14,657 - INFO - _models.training_function_executor - Epoch 029/137 | train_loss=0.6749 | val_loss=0.6401 | val_acc=0.7188
2025-10-13 22:43:14,664 - INFO - _models.training_function_executor - Epoch 030/137 | train_loss=0.6770 | val_loss=0.6234 | val_acc=0.7188
2025-10-13 22:43:14,670 - INFO - _models.training_function_executor - Epoch 031/137 | train_loss=0.6811 | val_loss=0.6250 | val_acc=0.7188
2025-10-13 22:43:14,677 - INFO - _models.training_function_executor - Epoch 032/137 | train_loss=0.6631 | val_loss=0.6256 | val_acc=0.7188
2025-10-13 22:43:14,684 - INFO - _models.training_function_executor - Epoch 033/137 | train_loss=0.6749 | val_loss=0.6254 | val_acc=0.7188
2025-10-13 22:43:14,690 - INFO - _models.training_function_executor - Epoch 034/137 | train_loss=0.6792 | val_loss=0.6173 | val_acc=0.7188
2025-10-13 22:43:14,696 - INFO - _models.training_function_executor - Epoch 035/137 | train_loss=0.6777 | val_loss=0.6469 | val_acc=0.7188
2025-10-13 22:43:14,702 - INFO - _models.training_function_executor - Epoch 036/137 | train_loss=0.6632 | val_loss=0.6056 | val_acc=0.7188
2025-10-13 22:43:14,708 - INFO - _models.training_function_executor - Epoch 037/137 | train_loss=0.6783 | val_loss=0.6070 | val_acc=0.7188
2025-10-13 22:43:14,714 - INFO - _models.training_function_executor - Epoch 038/137 | train_loss=0.6936 | val_loss=0.6217 | val_acc=0.7188
2025-10-13 22:43:14,720 - INFO - _models.training_function_executor - Epoch 039/137 | train_loss=0.6621 | val_loss=0.6316 | val_acc=0.7188
2025-10-13 22:43:14,727 - INFO - _models.training_function_executor - Epoch 040/137 | train_loss=0.6733 | val_loss=0.6250 | val_acc=0.7188
2025-10-13 22:43:14,733 - INFO - _models.training_function_executor - Epoch 041/137 | train_loss=0.6666 | val_loss=0.6330 | val_acc=0.7188
2025-10-13 22:43:14,739 - INFO - _models.training_function_executor - Epoch 042/137 | train_loss=0.6711 | val_loss=0.6330 | val_acc=0.7188
2025-10-13 22:43:14,746 - INFO - _models.training_function_executor - Epoch 043/137 | train_loss=0.6488 | val_loss=0.6074 | val_acc=0.7188
2025-10-13 22:43:14,752 - INFO - _models.training_function_executor - Epoch 044/137 | train_loss=0.7078 | val_loss=0.6225 | val_acc=0.7188
2025-10-13 22:43:14,758 - INFO - _models.training_function_executor - Epoch 045/137 | train_loss=0.6774 | val_loss=0.6188 | val_acc=0.7188
2025-10-13 22:43:14,765 - INFO - _models.training_function_executor - Epoch 046/137 | train_loss=0.6644 | val_loss=0.6335 | val_acc=0.7188
2025-10-13 22:43:14,771 - INFO - _models.training_function_executor - Epoch 047/137 | train_loss=0.6717 | val_loss=0.6245 | val_acc=0.7188
2025-10-13 22:43:14,777 - INFO - _models.training_function_executor - Epoch 048/137 | train_loss=0.6676 | val_loss=0.5973 | val_acc=0.7188
2025-10-13 22:43:14,783 - INFO - _models.training_function_executor - Epoch 049/137 | train_loss=0.6574 | val_loss=0.6215 | val_acc=0.7188
2025-10-13 22:43:14,790 - INFO - _models.training_function_executor - Epoch 050/137 | train_loss=0.6605 | val_loss=0.6300 | val_acc=0.7188
2025-10-13 22:43:14,796 - INFO - _models.training_function_executor - Epoch 051/137 | train_loss=0.6681 | val_loss=0.6473 | val_acc=0.7188
2025-10-13 22:43:14,802 - INFO - _models.training_function_executor - Epoch 052/137 | train_loss=0.6543 | val_loss=0.6150 | val_acc=0.7188
2025-10-13 22:43:14,809 - INFO - _models.training_function_executor - Epoch 053/137 | train_loss=0.6550 | val_loss=0.6069 | val_acc=0.7188
2025-10-13 22:43:14,816 - INFO - _models.training_function_executor - Epoch 054/137 | train_loss=0.6609 | val_loss=0.6205 | val_acc=0.7188
2025-10-13 22:43:14,822 - INFO - _models.training_function_executor - Epoch 055/137 | train_loss=0.6451 | val_loss=0.6098 | val_acc=0.7188
2025-10-13 22:43:14,829 - INFO - _models.training_function_executor - Epoch 056/137 | train_loss=0.6604 | val_loss=0.5848 | val_acc=0.7188
2025-10-13 22:43:14,835 - INFO - _models.training_function_executor - Epoch 057/137 | train_loss=0.6583 | val_loss=0.6071 | val_acc=0.7188
2025-10-13 22:43:14,842 - INFO - _models.training_function_executor - Epoch 058/137 | train_loss=0.6537 | val_loss=0.6131 | val_acc=0.7188
2025-10-13 22:43:14,848 - INFO - _models.training_function_executor - Epoch 059/137 | train_loss=0.6450 | val_loss=0.5905 | val_acc=0.7188
2025-10-13 22:43:14,854 - INFO - _models.training_function_executor - Epoch 060/137 | train_loss=0.6536 | val_loss=0.5963 | val_acc=0.7188
2025-10-13 22:43:14,861 - INFO - _models.training_function_executor - Epoch 061/137 | train_loss=0.6575 | val_loss=0.6197 | val_acc=0.7188
2025-10-13 22:43:14,868 - INFO - _models.training_function_executor - Epoch 062/137 | train_loss=0.6440 | val_loss=0.6276 | val_acc=0.7188
2025-10-13 22:43:14,874 - INFO - _models.training_function_executor - Epoch 063/137 | train_loss=0.6459 | val_loss=0.6163 | val_acc=0.7188
2025-10-13 22:43:14,881 - INFO - _models.training_function_executor - Epoch 064/137 | train_loss=0.6451 | val_loss=0.6153 | val_acc=0.7188
2025-10-13 22:43:14,887 - INFO - _models.training_function_executor - Epoch 065/137 | train_loss=0.6381 | val_loss=0.6132 | val_acc=0.7188
2025-10-13 22:43:14,894 - INFO - _models.training_function_executor - Epoch 066/137 | train_loss=0.6397 | val_loss=0.6187 | val_acc=0.7188
2025-10-13 22:43:14,901 - INFO - _models.training_function_executor - Epoch 067/137 | train_loss=0.6378 | val_loss=0.6003 | val_acc=0.7188
2025-10-13 22:43:14,907 - INFO - _models.training_function_executor - Epoch 068/137 | train_loss=0.6421 | val_loss=0.6151 | val_acc=0.7188
2025-10-13 22:43:14,914 - INFO - _models.training_function_executor - Epoch 069/137 | train_loss=0.6352 | val_loss=0.6175 | val_acc=0.7188
2025-10-13 22:43:14,920 - INFO - _models.training_function_executor - Epoch 070/137 | train_loss=0.6478 | val_loss=0.5801 | val_acc=0.7188
2025-10-13 22:43:14,927 - INFO - _models.training_function_executor - Epoch 071/137 | train_loss=0.6504 | val_loss=0.5828 | val_acc=0.7188
2025-10-13 22:43:14,933 - INFO - _models.training_function_executor - Epoch 072/137 | train_loss=0.6381 | val_loss=0.5974 | val_acc=0.7188
2025-10-13 22:43:14,940 - INFO - _models.training_function_executor - Epoch 073/137 | train_loss=0.6421 | val_loss=0.5831 | val_acc=0.7188
2025-10-13 22:43:14,946 - INFO - _models.training_function_executor - Epoch 074/137 | train_loss=0.6380 | val_loss=0.5930 | val_acc=0.7188
2025-10-13 22:43:14,952 - INFO - _models.training_function_executor - Epoch 075/137 | train_loss=0.6306 | val_loss=0.5981 | val_acc=0.7188
2025-10-13 22:43:14,959 - INFO - _models.training_function_executor - Epoch 076/137 | train_loss=0.6199 | val_loss=0.5830 | val_acc=0.7188
2025-10-13 22:43:14,966 - INFO - _models.training_function_executor - Epoch 077/137 | train_loss=0.6155 | val_loss=0.5604 | val_acc=0.7031
2025-10-13 22:43:14,972 - INFO - _models.training_function_executor - Epoch 078/137 | train_loss=0.6145 | val_loss=0.5597 | val_acc=0.7031
2025-10-13 22:43:14,978 - INFO - _models.training_function_executor - Epoch 079/137 | train_loss=0.6173 | val_loss=0.5749 | val_acc=0.7188
2025-10-13 22:43:14,985 - INFO - _models.training_function_executor - Epoch 080/137 | train_loss=0.6095 | val_loss=0.5801 | val_acc=0.7188
2025-10-13 22:43:14,991 - INFO - _models.training_function_executor - Epoch 081/137 | train_loss=0.6095 | val_loss=0.5536 | val_acc=0.7344
2025-10-13 22:43:14,998 - INFO - _models.training_function_executor - Epoch 082/137 | train_loss=0.5953 | val_loss=0.5533 | val_acc=0.7031
2025-10-13 22:43:15,004 - INFO - _models.training_function_executor - Epoch 083/137 | train_loss=0.5924 | val_loss=0.5517 | val_acc=0.7344
2025-10-13 22:43:15,010 - INFO - _models.training_function_executor - Epoch 084/137 | train_loss=0.6003 | val_loss=0.5457 | val_acc=0.7344
2025-10-13 22:43:15,017 - INFO - _models.training_function_executor - Epoch 085/137 | train_loss=0.6056 | val_loss=0.5378 | val_acc=0.7344
2025-10-13 22:43:15,024 - INFO - _models.training_function_executor - Epoch 086/137 | train_loss=0.5955 | val_loss=0.5510 | val_acc=0.7344
2025-10-13 22:43:15,030 - INFO - _models.training_function_executor - Epoch 087/137 | train_loss=0.6014 | val_loss=0.5376 | val_acc=0.7344
2025-10-13 22:43:15,036 - INFO - _models.training_function_executor - Epoch 088/137 | train_loss=0.5892 | val_loss=0.5433 | val_acc=0.7188
2025-10-13 22:43:15,043 - INFO - _models.training_function_executor - Epoch 089/137 | train_loss=0.5787 | val_loss=0.5441 | val_acc=0.7188
2025-10-13 22:43:15,049 - INFO - _models.training_function_executor - Epoch 090/137 | train_loss=0.5830 | val_loss=0.5413 | val_acc=0.7188
2025-10-13 22:43:15,056 - INFO - _models.training_function_executor - Epoch 091/137 | train_loss=0.5878 | val_loss=0.5426 | val_acc=0.7031
2025-10-13 22:43:15,064 - INFO - _models.training_function_executor - Epoch 092/137 | train_loss=0.5820 | val_loss=0.5375 | val_acc=0.7188
2025-10-13 22:43:15,072 - INFO - _models.training_function_executor - Epoch 093/137 | train_loss=0.5753 | val_loss=0.5390 | val_acc=0.7188
2025-10-13 22:43:15,082 - INFO - _models.training_function_executor - Epoch 094/137 | train_loss=0.5680 | val_loss=0.5280 | val_acc=0.7031
2025-10-13 22:43:15,095 - INFO - _models.training_function_executor - Epoch 095/137 | train_loss=0.5798 | val_loss=0.5277 | val_acc=0.7188
2025-10-13 22:43:15,107 - INFO - _models.training_function_executor - Epoch 096/137 | train_loss=0.5756 | val_loss=0.5363 | val_acc=0.7188
2025-10-13 22:43:15,115 - INFO - _models.training_function_executor - Epoch 097/137 | train_loss=0.5684 | val_loss=0.5279 | val_acc=0.7188
2025-10-13 22:43:15,121 - INFO - _models.training_function_executor - Epoch 098/137 | train_loss=0.5632 | val_loss=0.5215 | val_acc=0.7188
2025-10-13 22:43:15,128 - INFO - _models.training_function_executor - Epoch 099/137 | train_loss=0.5599 | val_loss=0.5262 | val_acc=0.7344
2025-10-13 22:43:15,134 - INFO - _models.training_function_executor - Epoch 100/137 | train_loss=0.5642 | val_loss=0.5318 | val_acc=0.7500
2025-10-13 22:43:15,140 - INFO - _models.training_function_executor - Epoch 101/137 | train_loss=0.5566 | val_loss=0.5204 | val_acc=0.7344
2025-10-13 22:43:15,146 - INFO - _models.training_function_executor - Epoch 102/137 | train_loss=0.5631 | val_loss=0.5155 | val_acc=0.7344
2025-10-13 22:43:15,152 - INFO - _models.training_function_executor - Epoch 103/137 | train_loss=0.5516 | val_loss=0.5136 | val_acc=0.7344
2025-10-13 22:43:15,158 - INFO - _models.training_function_executor - Epoch 104/137 | train_loss=0.5452 | val_loss=0.5168 | val_acc=0.7344
2025-10-13 22:43:15,165 - INFO - _models.training_function_executor - Epoch 105/137 | train_loss=0.5543 | val_loss=0.5202 | val_acc=0.7344
2025-10-13 22:43:15,172 - INFO - _models.training_function_executor - Epoch 106/137 | train_loss=0.5548 | val_loss=0.5169 | val_acc=0.7344
2025-10-13 22:43:15,178 - INFO - _models.training_function_executor - Epoch 107/137 | train_loss=0.5448 | val_loss=0.5151 | val_acc=0.7656
2025-10-13 22:43:15,184 - INFO - _models.training_function_executor - Epoch 108/137 | train_loss=0.5506 | val_loss=0.5146 | val_acc=0.7656
2025-10-13 22:43:15,191 - INFO - _models.training_function_executor - Epoch 109/137 | train_loss=0.5567 | val_loss=0.5165 | val_acc=0.7656
2025-10-13 22:43:15,197 - INFO - _models.training_function_executor - Epoch 110/137 | train_loss=0.5426 | val_loss=0.5137 | val_acc=0.7656
2025-10-13 22:43:15,204 - INFO - _models.training_function_executor - Epoch 111/137 | train_loss=0.5535 | val_loss=0.5123 | val_acc=0.7656
2025-10-13 22:43:15,210 - INFO - _models.training_function_executor - Epoch 112/137 | train_loss=0.5463 | val_loss=0.5126 | val_acc=0.7656
2025-10-13 22:43:15,216 - INFO - _models.training_function_executor - Epoch 113/137 | train_loss=0.5445 | val_loss=0.5082 | val_acc=0.7656
2025-10-13 22:43:15,222 - INFO - _models.training_function_executor - Epoch 114/137 | train_loss=0.5404 | val_loss=0.5088 | val_acc=0.7656
2025-10-13 22:43:15,228 - INFO - _models.training_function_executor - Epoch 115/137 | train_loss=0.5439 | val_loss=0.5083 | val_acc=0.7656
2025-10-13 22:43:15,234 - INFO - _models.training_function_executor - Epoch 116/137 | train_loss=0.5394 | val_loss=0.5150 | val_acc=0.7656
2025-10-13 22:43:15,240 - INFO - _models.training_function_executor - Epoch 117/137 | train_loss=0.5590 | val_loss=0.5164 | val_acc=0.7656
2025-10-13 22:43:15,247 - INFO - _models.training_function_executor - Epoch 118/137 | train_loss=0.5471 | val_loss=0.5113 | val_acc=0.7656
2025-10-13 22:43:15,253 - INFO - _models.training_function_executor - Epoch 119/137 | train_loss=0.5462 | val_loss=0.5077 | val_acc=0.7656
2025-10-13 22:43:15,260 - INFO - _models.training_function_executor - Epoch 120/137 | train_loss=0.5444 | val_loss=0.5055 | val_acc=0.7656
2025-10-13 22:43:15,266 - INFO - _models.training_function_executor - Epoch 121/137 | train_loss=0.5430 | val_loss=0.5048 | val_acc=0.7656
2025-10-13 22:43:15,273 - INFO - _models.training_function_executor - Epoch 122/137 | train_loss=0.5384 | val_loss=0.5049 | val_acc=0.7656
2025-10-13 22:43:15,280 - INFO - _models.training_function_executor - Epoch 123/137 | train_loss=0.5419 | val_loss=0.5065 | val_acc=0.7656
2025-10-13 22:43:15,286 - INFO - _models.training_function_executor - Epoch 124/137 | train_loss=0.5412 | val_loss=0.5087 | val_acc=0.7656
2025-10-13 22:43:15,293 - INFO - _models.training_function_executor - Epoch 125/137 | train_loss=0.5381 | val_loss=0.5091 | val_acc=0.7656
2025-10-13 22:43:15,299 - INFO - _models.training_function_executor - Epoch 126/137 | train_loss=0.5331 | val_loss=0.5082 | val_acc=0.7656
2025-10-13 22:43:15,306 - INFO - _models.training_function_executor - Epoch 127/137 | train_loss=0.5353 | val_loss=0.5058 | val_acc=0.7656
2025-10-13 22:43:15,312 - INFO - _models.training_function_executor - Epoch 128/137 | train_loss=0.5464 | val_loss=0.5040 | val_acc=0.7656
2025-10-13 22:43:15,319 - INFO - _models.training_function_executor - Epoch 129/137 | train_loss=0.5327 | val_loss=0.5028 | val_acc=0.7656
2025-10-13 22:43:15,325 - INFO - _models.training_function_executor - Epoch 130/137 | train_loss=0.5408 | val_loss=0.5019 | val_acc=0.7656
2025-10-13 22:43:15,332 - INFO - _models.training_function_executor - Epoch 131/137 | train_loss=0.5374 | val_loss=0.5015 | val_acc=0.7656
2025-10-13 22:43:15,338 - INFO - _models.training_function_executor - Epoch 132/137 | train_loss=0.5439 | val_loss=0.5012 | val_acc=0.7656
2025-10-13 22:43:15,345 - INFO - _models.training_function_executor - Epoch 133/137 | train_loss=0.5313 | val_loss=0.5012 | val_acc=0.7656
2025-10-13 22:43:15,351 - INFO - _models.training_function_executor - Epoch 134/137 | train_loss=0.5406 | val_loss=0.5012 | val_acc=0.7656
2025-10-13 22:43:15,358 - INFO - _models.training_function_executor - Epoch 135/137 | train_loss=0.5480 | val_loss=0.5014 | val_acc=0.7656
2025-10-13 22:43:15,364 - INFO - _models.training_function_executor - Epoch 136/137 | train_loss=0.5373 | val_loss=0.5013 | val_acc=0.7656
2025-10-13 22:43:15,371 - INFO - _models.training_function_executor - Epoch 137/137 | train_loss=0.5486 | val_loss=0.5013 | val_acc=0.7656
2025-10-13 22:43:16,209 - INFO - _models.training_function_executor - Model: 29,739 parameters, 127.8KB storage
2025-10-13 22:43:16,209 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [3.5885589718818665, 12.954193592071533, 4.969508111476898, 3.0044311583042145, 1.4767528474330902, 0.8453610390424728, 0.7518667876720428, 0.7253648042678833, 0.7115319222211838, 0.7100203186273575, 0.7168622314929962, 0.7890630215406418, 0.7206981033086777, 0.6963195204734802, 0.6936548054218292, 0.7190576195716858, 0.7194208800792694, 0.6996481120586395, 0.6955443173646927, 0.6865040361881256, 0.6550706773996353, 0.7081154137849808, 0.6816664338111877, 0.7068440765142441, 0.7066411823034286, 0.7017203122377396, 0.6943933218717575, 0.6748764365911484, 0.6749232560396194, 0.6770040541887283, 0.6810957491397858, 0.6630892753601074, 0.6749100685119629, 0.6791906207799911, 0.6777333915233612, 0.6632465422153473, 0.6782675087451935, 0.6935505717992783, 0.662050262093544, 0.673341765999794, 0.6665560901165009, 0.6711041033267975, 0.6488170176744461, 0.7077691107988358, 0.6774192005395889, 0.6643982529640198, 0.6717065125703812, 0.667629599571228, 0.657391369342804, 0.6605218052864075, 0.6680805683135986, 0.6543300449848175, 0.6549689769744873, 0.6608948111534119, 0.6450795829296112, 0.6604247689247131, 0.6582654565572739, 0.6536911576986313, 0.6449637860059738, 0.6536141633987427, 0.6575163453817368, 0.6440153121948242, 0.6458974331617355, 0.6450857073068619, 0.6380572617053986, 0.6397138088941574, 0.6378033757209778, 0.6421418637037277, 0.6351575553417206, 0.6477823108434677, 0.6503849774599075, 0.6380993872880936, 0.6421140879392624, 0.638033539056778, 0.6305873841047287, 0.619937852025032, 0.6155488044023514, 0.6145024672150612, 0.6172929555177689, 0.6095409542322159, 0.609508141875267, 0.5952546745538712, 0.5924317091703415, 0.6003384292125702, 0.6055834740400314, 0.5955193042755127, 0.6014427915215492, 0.589209072291851, 0.5787055194377899, 0.5830224007368088, 0.5877915173768997, 0.5820249170064926, 0.5753499269485474, 0.5680406093597412, 0.5798469930887222, 0.5755792036652565, 0.5684023201465607, 0.563204251229763, 0.5599357932806015, 0.5641836524009705, 0.5565899908542633, 0.5631441697478294, 0.5515858978033066, 0.5452131778001785, 0.5543162450194359, 0.5548046082258224, 0.544849306344986, 0.5505610182881355, 0.5566872507333755, 0.5425669848918915, 0.5534821152687073, 0.546335406601429, 0.5444568395614624, 0.5404391288757324, 0.5439331158995628, 0.5394126251339912, 0.5590353012084961, 0.5471230074763298, 0.5461635291576385, 0.544368252158165, 0.5430092513561249, 0.5383541211485863, 0.5419475808739662, 0.5411708727478981, 0.5381021276116371, 0.5330570712685585, 0.5352878719568253, 0.5463658571243286, 0.532745823264122, 0.5408127680420876, 0.5373866409063339, 0.5438997745513916, 0.5313164591789246, 0.5406156778335571, 0.5480172485113144, 0.5373496785759926, 0.5486261397600174], 'val_losses': [2.7171952724456787, 8.626431465148926, 5.55879020690918, 1.232949137687683, 0.8596762418746948, 0.7054446935653687, 0.6620997190475464, 0.643744945526123, 0.6296194791793823, 0.6387283802032471, 0.678475022315979, 0.652508020401001, 0.6445949673652649, 0.6373574137687683, 0.6286321878433228, 0.6676322817802429, 0.6586637496948242, 0.6230844259262085, 0.6328417658805847, 0.641188383102417, 0.6150303483009338, 0.6368395090103149, 0.6325162649154663, 0.6210187673568726, 0.6280046701431274, 0.6499838829040527, 0.6195131540298462, 0.6304312348365784, 0.6400935649871826, 0.623424768447876, 0.6250302195549011, 0.6255635023117065, 0.62539142370224, 0.6173330545425415, 0.6468690633773804, 0.6055790781974792, 0.6069638729095459, 0.6216545701026917, 0.631585955619812, 0.6249905824661255, 0.6330478191375732, 0.6329571008682251, 0.6073772311210632, 0.6224772930145264, 0.6188427209854126, 0.6334572434425354, 0.6245133280754089, 0.5973280072212219, 0.6215475797653198, 0.6300193071365356, 0.6473281979560852, 0.6149744987487793, 0.606890857219696, 0.6205306053161621, 0.6098154783248901, 0.5848486423492432, 0.607071042060852, 0.6131043434143066, 0.5905272960662842, 0.5963338017463684, 0.6196566820144653, 0.6276254653930664, 0.6162821054458618, 0.615337073802948, 0.6132241487503052, 0.6187373399734497, 0.6002973914146423, 0.6150733828544617, 0.6175193190574646, 0.5800886154174805, 0.5827792286872864, 0.5973942875862122, 0.5830569863319397, 0.5930310487747192, 0.5980880260467529, 0.5830382704734802, 0.5604181885719299, 0.5597242116928101, 0.5748959183692932, 0.5801079869270325, 0.5535669922828674, 0.5533004999160767, 0.5517386198043823, 0.5457314848899841, 0.5377862453460693, 0.5510169863700867, 0.5376397967338562, 0.543308436870575, 0.5441225171089172, 0.5413280725479126, 0.5426044464111328, 0.5375463366508484, 0.5389840006828308, 0.5279594659805298, 0.5277449488639832, 0.5362754464149475, 0.527891993522644, 0.5215316414833069, 0.5261704325675964, 0.5317522287368774, 0.5203657150268555, 0.5155045390129089, 0.5135863423347473, 0.5167901515960693, 0.5202192068099976, 0.5168627500534058, 0.515114963054657, 0.5145964622497559, 0.5165284276008606, 0.5136849284172058, 0.5122889280319214, 0.512640118598938, 0.5082078576087952, 0.5088009834289551, 0.5083112716674805, 0.514984667301178, 0.5164304971694946, 0.5112842321395874, 0.5076795816421509, 0.5054883360862732, 0.5048245191574097, 0.5049024224281311, 0.5065410137176514, 0.508671760559082, 0.5091466307640076, 0.5081697106361389, 0.5057567954063416, 0.5039647221565247, 0.5027562379837036, 0.5019152164459229, 0.5015339255332947, 0.5012341737747192, 0.5011926293373108, 0.5012257099151611, 0.5013566613197327, 0.5013010501861572, 0.5013082027435303], 'val_acc': [0.15625, 0.71875, 0.09375, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.703125, 0.703125, 0.71875, 0.71875, 0.734375, 0.703125, 0.734375, 0.734375, 0.734375, 0.734375, 0.734375, 0.71875, 0.71875, 0.71875, 0.703125, 0.71875, 0.71875, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.734375, 0.75, 0.734375, 0.734375, 0.734375, 0.734375, 0.734375, 0.734375, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.007007193583244844, 'batch_size': 64, 'epochs': 137, 'hidden_size': 168, 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 29739, 'model_storage_size_kb': 127.784765625, 'model_size_validation': 'PASS'}
2025-10-13 22:43:16,209 - INFO - _models.training_function_executor - BO Objective: base=0.7656, size_penalty=0.0000, final=0.7656
2025-10-13 22:43:16,209 - INFO - _models.training_function_executor - Model: 29,739 parameters, 127.8KB (PASS 256KB limit)
2025-10-13 22:43:16,209 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.611s
2025-10-13 22:43:16,289 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7656
2025-10-13 22:43:16,289 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.079s
2025-10-13 22:43:16,289 - INFO - bo.run_bo - Recorded observation #31: hparams={'lr': 0.007007193583244844, 'batch_size': np.int64(64), 'epochs': np.int64(137), 'hidden_size': np.int64(168), 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7656
2025-10-13 22:43:16,289 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 31: {'lr': 0.007007193583244844, 'batch_size': np.int64(64), 'epochs': np.int64(137), 'hidden_size': np.int64(168), 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7656
2025-10-13 22:43:16,289 - INFO - bo.run_bo - üîçBO Trial 32: Using RF surrogate + Expected Improvement
2025-10-13 22:43:16,289 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:16,290 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 32 (NaN monitoring active)
2025-10-13 22:43:16,290 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:16,290 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:16,290 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0025958366113884544, 'batch_size': 192, 'epochs': 51, 'hidden_size': 179, 'dropout': 0.014281898748531755, 'weight_decay': 0.009289371768333903, 'label_smoothing': 0.017579208154851547, 'grad_clip_norm': 1.726324144758495, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:16,291 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0025958366113884544, 'batch_size': 192, 'epochs': 51, 'hidden_size': 179, 'dropout': 0.014281898748531755, 'weight_decay': 0.009289371768333903, 'label_smoothing': 0.017579208154851547, 'grad_clip_norm': 1.726324144758495, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:20,018 - INFO - _models.training_function_executor - Epoch 001/051 | train_loss=3.0041 | val_loss=3.0385 | val_acc=0.7188
2025-10-13 22:43:20,024 - INFO - _models.training_function_executor - Epoch 002/051 | train_loss=2.9530 | val_loss=3.0385 | val_acc=0.7188
2025-10-13 22:43:20,030 - INFO - _models.training_function_executor - Epoch 003/051 | train_loss=6.0394 | val_loss=14.5210 | val_acc=0.0938
2025-10-13 22:43:20,034 - INFO - _models.training_function_executor - Epoch 004/051 | train_loss=16.7510 | val_loss=14.9959 | val_acc=0.1875
2025-10-13 22:43:20,039 - INFO - _models.training_function_executor - Epoch 005/051 | train_loss=13.4561 | val_loss=2.1002 | val_acc=0.7188
2025-10-13 22:43:20,043 - INFO - _models.training_function_executor - Epoch 006/051 | train_loss=2.2158 | val_loss=2.3918 | val_acc=0.6406
2025-10-13 22:43:20,047 - INFO - _models.training_function_executor - Epoch 007/051 | train_loss=2.3988 | val_loss=1.6371 | val_acc=0.7188
2025-10-13 22:43:20,052 - INFO - _models.training_function_executor - Epoch 008/051 | train_loss=1.4995 | val_loss=2.1499 | val_acc=0.3438
2025-10-13 22:43:20,056 - INFO - _models.training_function_executor - Epoch 009/051 | train_loss=2.5919 | val_loss=2.2592 | val_acc=0.3438
2025-10-13 22:43:20,060 - INFO - _models.training_function_executor - Epoch 010/051 | train_loss=2.1642 | val_loss=1.0231 | val_acc=0.7188
2025-10-13 22:43:20,064 - INFO - _models.training_function_executor - Epoch 011/051 | train_loss=1.2708 | val_loss=1.5579 | val_acc=0.7188
2025-10-13 22:43:20,068 - INFO - _models.training_function_executor - Epoch 012/051 | train_loss=1.4759 | val_loss=0.9764 | val_acc=0.6562
2025-10-13 22:43:20,072 - INFO - _models.training_function_executor - Epoch 013/051 | train_loss=1.0966 | val_loss=1.0652 | val_acc=0.5938
2025-10-13 22:43:20,076 - INFO - _models.training_function_executor - Epoch 014/051 | train_loss=1.2748 | val_loss=0.8120 | val_acc=0.6562
2025-10-13 22:43:20,080 - INFO - _models.training_function_executor - Epoch 015/051 | train_loss=0.9315 | val_loss=0.8575 | val_acc=0.7188
2025-10-13 22:43:20,084 - INFO - _models.training_function_executor - Epoch 016/051 | train_loss=0.9108 | val_loss=0.7471 | val_acc=0.6406
2025-10-13 22:43:20,088 - INFO - _models.training_function_executor - Epoch 017/051 | train_loss=0.8443 | val_loss=0.7500 | val_acc=0.6406
2025-10-13 22:43:20,092 - INFO - _models.training_function_executor - Epoch 018/051 | train_loss=0.8763 | val_loss=0.7601 | val_acc=0.7188
2025-10-13 22:43:20,096 - INFO - _models.training_function_executor - Epoch 019/051 | train_loss=0.8215 | val_loss=0.6941 | val_acc=0.7031
2025-10-13 22:43:20,102 - INFO - _models.training_function_executor - Epoch 020/051 | train_loss=0.7684 | val_loss=0.6493 | val_acc=0.6719
2025-10-13 22:43:20,107 - INFO - _models.training_function_executor - Epoch 021/051 | train_loss=0.7069 | val_loss=0.7326 | val_acc=0.7188
2025-10-13 22:43:20,112 - INFO - _models.training_function_executor - Epoch 022/051 | train_loss=0.7947 | val_loss=0.7140 | val_acc=0.7188
2025-10-13 22:43:20,116 - INFO - _models.training_function_executor - Epoch 023/051 | train_loss=0.7471 | val_loss=0.6536 | val_acc=0.7188
2025-10-13 22:43:20,121 - INFO - _models.training_function_executor - Epoch 024/051 | train_loss=0.7337 | val_loss=0.6279 | val_acc=0.7188
2025-10-13 22:43:20,125 - INFO - _models.training_function_executor - Epoch 025/051 | train_loss=0.7032 | val_loss=0.7138 | val_acc=0.7188
2025-10-13 22:43:20,129 - INFO - _models.training_function_executor - Epoch 026/051 | train_loss=0.7639 | val_loss=0.6730 | val_acc=0.7344
2025-10-13 22:43:20,133 - INFO - _models.training_function_executor - Epoch 027/051 | train_loss=0.7187 | val_loss=0.6398 | val_acc=0.6562
2025-10-13 22:43:20,137 - INFO - _models.training_function_executor - Epoch 028/051 | train_loss=0.7220 | val_loss=0.6432 | val_acc=0.7188
2025-10-13 22:43:20,142 - INFO - _models.training_function_executor - Epoch 029/051 | train_loss=0.7348 | val_loss=0.6650 | val_acc=0.7188
2025-10-13 22:43:20,146 - INFO - _models.training_function_executor - Epoch 030/051 | train_loss=0.6996 | val_loss=0.6495 | val_acc=0.7344
2025-10-13 22:43:20,150 - INFO - _models.training_function_executor - Epoch 031/051 | train_loss=0.7038 | val_loss=0.6471 | val_acc=0.6562
2025-10-13 22:43:20,155 - INFO - _models.training_function_executor - Epoch 032/051 | train_loss=0.7257 | val_loss=0.6274 | val_acc=0.7188
2025-10-13 22:43:20,159 - INFO - _models.training_function_executor - Epoch 033/051 | train_loss=0.7051 | val_loss=0.6443 | val_acc=0.7188
2025-10-13 22:43:20,163 - INFO - _models.training_function_executor - Epoch 034/051 | train_loss=0.6887 | val_loss=0.6239 | val_acc=0.7188
2025-10-13 22:43:20,167 - INFO - _models.training_function_executor - Epoch 035/051 | train_loss=0.6770 | val_loss=0.6339 | val_acc=0.7344
2025-10-13 22:43:20,171 - INFO - _models.training_function_executor - Epoch 036/051 | train_loss=0.7103 | val_loss=0.6365 | val_acc=0.7344
2025-10-13 22:43:20,175 - INFO - _models.training_function_executor - Epoch 037/051 | train_loss=0.6908 | val_loss=0.6221 | val_acc=0.7188
2025-10-13 22:43:20,179 - INFO - _models.training_function_executor - Epoch 038/051 | train_loss=0.6781 | val_loss=0.6198 | val_acc=0.7188
2025-10-13 22:43:20,183 - INFO - _models.training_function_executor - Epoch 039/051 | train_loss=0.6887 | val_loss=0.6218 | val_acc=0.7188
2025-10-13 22:43:20,187 - INFO - _models.training_function_executor - Epoch 040/051 | train_loss=0.6661 | val_loss=0.6160 | val_acc=0.7188
2025-10-13 22:43:20,191 - INFO - _models.training_function_executor - Epoch 041/051 | train_loss=0.6856 | val_loss=0.6188 | val_acc=0.7188
2025-10-13 22:43:20,195 - INFO - _models.training_function_executor - Epoch 042/051 | train_loss=0.6661 | val_loss=0.6219 | val_acc=0.7188
2025-10-13 22:43:20,200 - INFO - _models.training_function_executor - Epoch 043/051 | train_loss=0.6752 | val_loss=0.6204 | val_acc=0.7188
2025-10-13 22:43:20,205 - INFO - _models.training_function_executor - Epoch 044/051 | train_loss=0.6582 | val_loss=0.6185 | val_acc=0.7188
2025-10-13 22:43:20,210 - INFO - _models.training_function_executor - Epoch 045/051 | train_loss=0.6741 | val_loss=0.6164 | val_acc=0.7188
2025-10-13 22:43:20,214 - INFO - _models.training_function_executor - Epoch 046/051 | train_loss=0.6819 | val_loss=0.6148 | val_acc=0.7188
2025-10-13 22:43:20,219 - INFO - _models.training_function_executor - Epoch 047/051 | train_loss=0.6641 | val_loss=0.6138 | val_acc=0.7188
2025-10-13 22:43:20,223 - INFO - _models.training_function_executor - Epoch 048/051 | train_loss=0.6655 | val_loss=0.6132 | val_acc=0.7188
2025-10-13 22:43:20,227 - INFO - _models.training_function_executor - Epoch 049/051 | train_loss=0.6590 | val_loss=0.6130 | val_acc=0.7188
2025-10-13 22:43:20,232 - INFO - _models.training_function_executor - Epoch 050/051 | train_loss=0.6543 | val_loss=0.6131 | val_acc=0.7188
2025-10-13 22:43:20,236 - INFO - _models.training_function_executor - Epoch 051/051 | train_loss=0.6557 | val_loss=0.6130 | val_acc=0.7188
2025-10-13 22:43:21,070 - INFO - _models.training_function_executor - Model: 33,655 parameters, 144.6KB storage
2025-10-13 22:43:21,070 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [3.004096031188965, 2.9530298113822937, 6.039398431777954, 16.7509822845459, 13.456101655960083, 2.2158042192459106, 2.3988257944583893, 1.4994548559188843, 2.591893434524536, 2.164209485054016, 1.270776629447937, 1.4759100079536438, 1.0965604484081268, 1.2748427987098694, 0.9315143525600433, 0.9107754081487656, 0.8443269431591034, 0.8763013184070587, 0.8214814364910126, 0.7684363722801208, 0.7068718373775482, 0.7947321534156799, 0.7471121996641159, 0.7336818724870682, 0.7031897008419037, 0.7638793289661407, 0.7187346071004868, 0.7219650000333786, 0.7347655743360519, 0.6995784044265747, 0.7037540972232819, 0.725699782371521, 0.705131322145462, 0.6886549293994904, 0.6770477294921875, 0.7102785706520081, 0.6907726675271988, 0.6780554205179214, 0.6886895149946213, 0.6661252230405807, 0.6855801939964294, 0.6660645604133606, 0.6751898527145386, 0.658202737569809, 0.6740731596946716, 0.6818670481443405, 0.6641343384981155, 0.6654854416847229, 0.6589831411838531, 0.6542881429195404, 0.655688002705574], 'val_losses': [3.0384750366210938, 3.0384750366210938, 14.521020889282227, 14.99594497680664, 2.1002418994903564, 2.3918344974517822, 1.6371393203735352, 2.1498641967773438, 2.259223222732544, 1.0230599641799927, 1.5579137802124023, 0.9763956069946289, 1.065216302871704, 0.8120312094688416, 0.857485830783844, 0.747119128704071, 0.7499840259552002, 0.7601386904716492, 0.6941056251525879, 0.6493349075317383, 0.7326338887214661, 0.7140007615089417, 0.6535611152648926, 0.6278854608535767, 0.7138209939002991, 0.672986626625061, 0.6398175954818726, 0.643242359161377, 0.6650415658950806, 0.649486243724823, 0.6470767259597778, 0.6274480223655701, 0.6443079710006714, 0.6239303350448608, 0.6338600516319275, 0.6364872455596924, 0.6220771670341492, 0.6197728514671326, 0.6218180656433105, 0.6159718632698059, 0.6188039779663086, 0.6218652129173279, 0.6204185485839844, 0.6185207366943359, 0.6164416670799255, 0.6147904992103577, 0.6137692928314209, 0.6132053136825562, 0.6129604578018188, 0.6131035685539246, 0.6130391359329224], 'val_acc': [0.71875, 0.71875, 0.09375, 0.1875, 0.71875, 0.640625, 0.71875, 0.34375, 0.34375, 0.71875, 0.71875, 0.65625, 0.59375, 0.65625, 0.71875, 0.640625, 0.640625, 0.71875, 0.703125, 0.671875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.734375, 0.65625, 0.71875, 0.71875, 0.734375, 0.65625, 0.71875, 0.71875, 0.71875, 0.734375, 0.734375, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0025958366113884544, 'batch_size': 192, 'epochs': 51, 'hidden_size': 179, 'dropout': 0.014281898748531755, 'weight_decay': 0.009289371768333903, 'label_smoothing': 0.017579208154851547, 'grad_clip_norm': 1.726324144758495, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 33655, 'model_storage_size_kb': 144.611328125, 'model_size_validation': 'PASS'}
2025-10-13 22:43:21,070 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:43:21,070 - INFO - _models.training_function_executor - Model: 33,655 parameters, 144.6KB (PASS 256KB limit)
2025-10-13 22:43:21,070 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.780s
2025-10-13 22:43:21,151 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:43:21,151 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.080s
2025-10-13 22:43:21,151 - INFO - bo.run_bo - Recorded observation #32: hparams={'lr': 0.0025958366113884544, 'batch_size': np.int64(192), 'epochs': np.int64(51), 'hidden_size': np.int64(179), 'dropout': 0.014281898748531755, 'weight_decay': 0.009289371768333903, 'label_smoothing': 0.017579208154851547, 'grad_clip_norm': 1.726324144758495, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:43:21,151 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 32: {'lr': 0.0025958366113884544, 'batch_size': np.int64(192), 'epochs': np.int64(51), 'hidden_size': np.int64(179), 'dropout': 0.014281898748531755, 'weight_decay': 0.009289371768333903, 'label_smoothing': 0.017579208154851547, 'grad_clip_norm': 1.726324144758495, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:43:21,151 - INFO - bo.run_bo - üîçBO Trial 33: Using RF surrogate + Expected Improvement
2025-10-13 22:43:21,151 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:21,151 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 33 (NaN monitoring active)
2025-10-13 22:43:21,151 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:21,151 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:21,151 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.512908393234953e-05, 'batch_size': 256, 'epochs': 94, 'hidden_size': 129, 'dropout': 0.05365304988293941, 'weight_decay': 0.0007114724451015198, 'label_smoothing': 0.105632599246988, 'grad_clip_norm': 0.5614174244508897, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:21,153 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.512908393234953e-05, 'batch_size': 256, 'epochs': 94, 'hidden_size': 129, 'dropout': 0.05365304988293941, 'weight_decay': 0.0007114724451015198, 'label_smoothing': 0.105632599246988, 'grad_clip_norm': 0.5614174244508897, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:24,837 - INFO - _models.training_function_executor - Epoch 001/094 | train_loss=2.0642 | val_loss=1.1117 | val_acc=0.7188
2025-10-13 22:43:24,842 - INFO - _models.training_function_executor - Epoch 002/094 | train_loss=2.0531 | val_loss=1.1117 | val_acc=0.7188
2025-10-13 22:43:24,846 - INFO - _models.training_function_executor - Epoch 003/094 | train_loss=2.2097 | val_loss=1.1117 | val_acc=0.7188
2025-10-13 22:43:24,850 - INFO - _models.training_function_executor - Epoch 004/094 | train_loss=2.2680 | val_loss=1.1117 | val_acc=0.7188
2025-10-13 22:43:24,854 - INFO - _models.training_function_executor - Epoch 005/094 | train_loss=1.9771 | val_loss=1.0974 | val_acc=0.7188
2025-10-13 22:43:24,857 - INFO - _models.training_function_executor - Epoch 006/094 | train_loss=1.9693 | val_loss=1.0940 | val_acc=0.7188
2025-10-13 22:43:24,860 - INFO - _models.training_function_executor - Epoch 007/094 | train_loss=2.2523 | val_loss=1.1059 | val_acc=0.7188
2025-10-13 22:43:24,863 - INFO - _models.training_function_executor - Epoch 008/094 | train_loss=1.8973 | val_loss=1.1317 | val_acc=0.7188
2025-10-13 22:43:24,867 - INFO - _models.training_function_executor - Epoch 009/094 | train_loss=1.8370 | val_loss=1.1457 | val_acc=0.7188
2025-10-13 22:43:24,870 - INFO - _models.training_function_executor - Epoch 010/094 | train_loss=1.8093 | val_loss=1.1468 | val_acc=0.7188
2025-10-13 22:43:24,874 - INFO - _models.training_function_executor - Epoch 011/094 | train_loss=1.8237 | val_loss=1.1481 | val_acc=0.7188
2025-10-13 22:43:24,877 - INFO - _models.training_function_executor - Epoch 012/094 | train_loss=1.6877 | val_loss=1.1436 | val_acc=0.7188
2025-10-13 22:43:24,880 - INFO - _models.training_function_executor - Epoch 013/094 | train_loss=1.9390 | val_loss=1.1406 | val_acc=0.7188
2025-10-13 22:43:24,883 - INFO - _models.training_function_executor - Epoch 014/094 | train_loss=1.9872 | val_loss=1.1384 | val_acc=0.7188
2025-10-13 22:43:24,886 - INFO - _models.training_function_executor - Epoch 015/094 | train_loss=1.7843 | val_loss=1.1436 | val_acc=0.7188
2025-10-13 22:43:24,890 - INFO - _models.training_function_executor - Epoch 016/094 | train_loss=1.8549 | val_loss=1.1412 | val_acc=0.7188
2025-10-13 22:43:24,893 - INFO - _models.training_function_executor - Epoch 017/094 | train_loss=1.7076 | val_loss=1.1322 | val_acc=0.7188
2025-10-13 22:43:24,896 - INFO - _models.training_function_executor - Epoch 018/094 | train_loss=2.0616 | val_loss=1.1251 | val_acc=0.7188
2025-10-13 22:43:24,900 - INFO - _models.training_function_executor - Epoch 019/094 | train_loss=1.9452 | val_loss=1.1160 | val_acc=0.7188
2025-10-13 22:43:24,903 - INFO - _models.training_function_executor - Epoch 020/094 | train_loss=2.0056 | val_loss=1.1069 | val_acc=0.7188
2025-10-13 22:43:24,906 - INFO - _models.training_function_executor - Epoch 021/094 | train_loss=1.8347 | val_loss=1.1010 | val_acc=0.7188
2025-10-13 22:43:24,909 - INFO - _models.training_function_executor - Epoch 022/094 | train_loss=1.8152 | val_loss=1.0953 | val_acc=0.7188
2025-10-13 22:43:24,912 - INFO - _models.training_function_executor - Epoch 023/094 | train_loss=1.8499 | val_loss=1.0930 | val_acc=0.7188
2025-10-13 22:43:24,916 - INFO - _models.training_function_executor - Epoch 024/094 | train_loss=1.8211 | val_loss=1.0872 | val_acc=0.7188
2025-10-13 22:43:24,919 - INFO - _models.training_function_executor - Epoch 025/094 | train_loss=1.7313 | val_loss=1.0829 | val_acc=0.7188
2025-10-13 22:43:24,923 - INFO - _models.training_function_executor - Epoch 026/094 | train_loss=1.7181 | val_loss=1.0771 | val_acc=0.7188
2025-10-13 22:43:24,927 - INFO - _models.training_function_executor - Epoch 027/094 | train_loss=1.7998 | val_loss=1.0689 | val_acc=0.7188
2025-10-13 22:43:24,930 - INFO - _models.training_function_executor - Epoch 028/094 | train_loss=1.7474 | val_loss=1.0582 | val_acc=0.7188
2025-10-13 22:43:24,935 - INFO - _models.training_function_executor - Epoch 029/094 | train_loss=1.7067 | val_loss=1.0455 | val_acc=0.7188
2025-10-13 22:43:24,938 - INFO - _models.training_function_executor - Epoch 030/094 | train_loss=1.8170 | val_loss=1.0357 | val_acc=0.7188
2025-10-13 22:43:24,941 - INFO - _models.training_function_executor - Epoch 031/094 | train_loss=1.7411 | val_loss=1.0313 | val_acc=0.7188
2025-10-13 22:43:24,945 - INFO - _models.training_function_executor - Epoch 032/094 | train_loss=1.7681 | val_loss=1.0227 | val_acc=0.7188
2025-10-13 22:43:24,949 - INFO - _models.training_function_executor - Epoch 033/094 | train_loss=1.7425 | val_loss=1.0153 | val_acc=0.7188
2025-10-13 22:43:24,952 - INFO - _models.training_function_executor - Epoch 034/094 | train_loss=1.7247 | val_loss=1.0109 | val_acc=0.7188
2025-10-13 22:43:24,955 - INFO - _models.training_function_executor - Epoch 035/094 | train_loss=1.8627 | val_loss=1.0047 | val_acc=0.7188
2025-10-13 22:43:24,958 - INFO - _models.training_function_executor - Epoch 036/094 | train_loss=1.8735 | val_loss=0.9964 | val_acc=0.7188
2025-10-13 22:43:24,962 - INFO - _models.training_function_executor - Epoch 037/094 | train_loss=1.6962 | val_loss=0.9856 | val_acc=0.7188
2025-10-13 22:43:24,965 - INFO - _models.training_function_executor - Epoch 038/094 | train_loss=1.6999 | val_loss=0.9782 | val_acc=0.7188
2025-10-13 22:43:24,968 - INFO - _models.training_function_executor - Epoch 039/094 | train_loss=1.8547 | val_loss=0.9731 | val_acc=0.7188
2025-10-13 22:43:24,971 - INFO - _models.training_function_executor - Epoch 040/094 | train_loss=1.8546 | val_loss=0.9688 | val_acc=0.7188
2025-10-13 22:43:24,975 - INFO - _models.training_function_executor - Epoch 041/094 | train_loss=1.7770 | val_loss=0.9633 | val_acc=0.7188
2025-10-13 22:43:24,978 - INFO - _models.training_function_executor - Epoch 042/094 | train_loss=1.8969 | val_loss=0.9610 | val_acc=0.7188
2025-10-13 22:43:24,981 - INFO - _models.training_function_executor - Epoch 043/094 | train_loss=1.7590 | val_loss=0.9617 | val_acc=0.7188
2025-10-13 22:43:24,984 - INFO - _models.training_function_executor - Epoch 044/094 | train_loss=1.8086 | val_loss=0.9616 | val_acc=0.7188
2025-10-13 22:43:24,987 - INFO - _models.training_function_executor - Epoch 045/094 | train_loss=1.7810 | val_loss=0.9605 | val_acc=0.7188
2025-10-13 22:43:24,990 - INFO - _models.training_function_executor - Epoch 046/094 | train_loss=1.5720 | val_loss=0.9616 | val_acc=0.7188
2025-10-13 22:43:24,993 - INFO - _models.training_function_executor - Epoch 047/094 | train_loss=1.7032 | val_loss=0.9632 | val_acc=0.7188
2025-10-13 22:43:24,996 - INFO - _models.training_function_executor - Epoch 048/094 | train_loss=1.9007 | val_loss=0.9637 | val_acc=0.7188
2025-10-13 22:43:24,999 - INFO - _models.training_function_executor - Epoch 049/094 | train_loss=1.6782 | val_loss=0.9666 | val_acc=0.7188
2025-10-13 22:43:25,002 - INFO - _models.training_function_executor - Epoch 050/094 | train_loss=1.9149 | val_loss=0.9688 | val_acc=0.7188
2025-10-13 22:43:25,006 - INFO - _models.training_function_executor - Epoch 051/094 | train_loss=1.5801 | val_loss=0.9689 | val_acc=0.7188
2025-10-13 22:43:25,009 - INFO - _models.training_function_executor - Epoch 052/094 | train_loss=1.6956 | val_loss=0.9710 | val_acc=0.7188
2025-10-13 22:43:25,012 - INFO - _models.training_function_executor - Epoch 053/094 | train_loss=1.6699 | val_loss=0.9715 | val_acc=0.7188
2025-10-13 22:43:25,015 - INFO - _models.training_function_executor - Epoch 054/094 | train_loss=1.8722 | val_loss=0.9714 | val_acc=0.7188
2025-10-13 22:43:25,018 - INFO - _models.training_function_executor - Epoch 055/094 | train_loss=1.7954 | val_loss=0.9700 | val_acc=0.7188
2025-10-13 22:43:25,021 - INFO - _models.training_function_executor - Epoch 056/094 | train_loss=1.6397 | val_loss=0.9672 | val_acc=0.7188
2025-10-13 22:43:25,024 - INFO - _models.training_function_executor - Epoch 057/094 | train_loss=1.7029 | val_loss=0.9654 | val_acc=0.7188
2025-10-13 22:43:25,027 - INFO - _models.training_function_executor - Epoch 058/094 | train_loss=1.8102 | val_loss=0.9642 | val_acc=0.7188
2025-10-13 22:43:25,030 - INFO - _models.training_function_executor - Epoch 059/094 | train_loss=1.6604 | val_loss=0.9623 | val_acc=0.7188
2025-10-13 22:43:25,034 - INFO - _models.training_function_executor - Epoch 060/094 | train_loss=1.6079 | val_loss=0.9580 | val_acc=0.7188
2025-10-13 22:43:25,037 - INFO - _models.training_function_executor - Epoch 061/094 | train_loss=1.4923 | val_loss=0.9552 | val_acc=0.7188
2025-10-13 22:43:25,040 - INFO - _models.training_function_executor - Epoch 062/094 | train_loss=1.6655 | val_loss=0.9521 | val_acc=0.7188
2025-10-13 22:43:25,043 - INFO - _models.training_function_executor - Epoch 063/094 | train_loss=1.7288 | val_loss=0.9507 | val_acc=0.7188
2025-10-13 22:43:25,047 - INFO - _models.training_function_executor - Epoch 064/094 | train_loss=1.6305 | val_loss=0.9495 | val_acc=0.7188
2025-10-13 22:43:25,050 - INFO - _models.training_function_executor - Epoch 065/094 | train_loss=1.7241 | val_loss=0.9494 | val_acc=0.7188
2025-10-13 22:43:25,053 - INFO - _models.training_function_executor - Epoch 066/094 | train_loss=1.6271 | val_loss=0.9479 | val_acc=0.7188
2025-10-13 22:43:25,056 - INFO - _models.training_function_executor - Epoch 067/094 | train_loss=1.6733 | val_loss=0.9464 | val_acc=0.7188
2025-10-13 22:43:25,059 - INFO - _models.training_function_executor - Epoch 068/094 | train_loss=1.5792 | val_loss=0.9453 | val_acc=0.7188
2025-10-13 22:43:25,062 - INFO - _models.training_function_executor - Epoch 069/094 | train_loss=1.6478 | val_loss=0.9438 | val_acc=0.7188
2025-10-13 22:43:25,066 - INFO - _models.training_function_executor - Epoch 070/094 | train_loss=1.9710 | val_loss=0.9424 | val_acc=0.7188
2025-10-13 22:43:25,069 - INFO - _models.training_function_executor - Epoch 071/094 | train_loss=1.6158 | val_loss=0.9418 | val_acc=0.7188
2025-10-13 22:43:25,072 - INFO - _models.training_function_executor - Epoch 072/094 | train_loss=1.6853 | val_loss=0.9398 | val_acc=0.7188
2025-10-13 22:43:25,075 - INFO - _models.training_function_executor - Epoch 073/094 | train_loss=1.6422 | val_loss=0.9383 | val_acc=0.7188
2025-10-13 22:43:25,078 - INFO - _models.training_function_executor - Epoch 074/094 | train_loss=1.8977 | val_loss=0.9386 | val_acc=0.7188
2025-10-13 22:43:25,081 - INFO - _models.training_function_executor - Epoch 075/094 | train_loss=2.0220 | val_loss=0.9386 | val_acc=0.7188
2025-10-13 22:43:25,084 - INFO - _models.training_function_executor - Epoch 076/094 | train_loss=1.6800 | val_loss=0.9399 | val_acc=0.7188
2025-10-13 22:43:25,087 - INFO - _models.training_function_executor - Epoch 077/094 | train_loss=1.7284 | val_loss=0.9407 | val_acc=0.7188
2025-10-13 22:43:25,090 - INFO - _models.training_function_executor - Epoch 078/094 | train_loss=1.7409 | val_loss=0.9419 | val_acc=0.7188
2025-10-13 22:43:25,094 - INFO - _models.training_function_executor - Epoch 079/094 | train_loss=1.5728 | val_loss=0.9429 | val_acc=0.7188
2025-10-13 22:43:25,097 - INFO - _models.training_function_executor - Epoch 080/094 | train_loss=1.6211 | val_loss=0.9440 | val_acc=0.7188
2025-10-13 22:43:25,100 - INFO - _models.training_function_executor - Epoch 081/094 | train_loss=1.7585 | val_loss=0.9440 | val_acc=0.7188
2025-10-13 22:43:25,104 - INFO - _models.training_function_executor - Epoch 082/094 | train_loss=1.7401 | val_loss=0.9443 | val_acc=0.7188
2025-10-13 22:43:25,107 - INFO - _models.training_function_executor - Epoch 083/094 | train_loss=1.7271 | val_loss=0.9442 | val_acc=0.7188
2025-10-13 22:43:25,110 - INFO - _models.training_function_executor - Epoch 084/094 | train_loss=1.5632 | val_loss=0.9445 | val_acc=0.7188
2025-10-13 22:43:25,113 - INFO - _models.training_function_executor - Epoch 085/094 | train_loss=1.4894 | val_loss=0.9448 | val_acc=0.7188
2025-10-13 22:43:25,116 - INFO - _models.training_function_executor - Epoch 086/094 | train_loss=1.6666 | val_loss=0.9450 | val_acc=0.7188
2025-10-13 22:43:25,119 - INFO - _models.training_function_executor - Epoch 087/094 | train_loss=1.5758 | val_loss=0.9447 | val_acc=0.7188
2025-10-13 22:43:25,122 - INFO - _models.training_function_executor - Epoch 088/094 | train_loss=1.5932 | val_loss=0.9446 | val_acc=0.7188
2025-10-13 22:43:25,126 - INFO - _models.training_function_executor - Epoch 089/094 | train_loss=1.6898 | val_loss=0.9448 | val_acc=0.7188
2025-10-13 22:43:25,129 - INFO - _models.training_function_executor - Epoch 090/094 | train_loss=1.6790 | val_loss=0.9448 | val_acc=0.7188
2025-10-13 22:43:25,132 - INFO - _models.training_function_executor - Epoch 091/094 | train_loss=1.6978 | val_loss=0.9447 | val_acc=0.7188
2025-10-13 22:43:25,136 - INFO - _models.training_function_executor - Epoch 092/094 | train_loss=1.7411 | val_loss=0.9447 | val_acc=0.7188
2025-10-13 22:43:25,139 - INFO - _models.training_function_executor - Epoch 093/094 | train_loss=1.6640 | val_loss=0.9447 | val_acc=0.7188
2025-10-13 22:43:25,142 - INFO - _models.training_function_executor - Epoch 094/094 | train_loss=1.5410 | val_loss=0.9447 | val_acc=0.7188
2025-10-13 22:43:25,990 - INFO - _models.training_function_executor - Model: 17,805 parameters, 76.5KB storage
2025-10-13 22:43:25,991 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.0641589164733887, 2.053121328353882, 2.2097368240356445, 2.2679708003997803, 1.9771279096603394, 1.9693137407302856, 2.252270221710205, 1.8972792625427246, 1.837024450302124, 1.8092830181121826, 1.823699951171875, 1.6877243518829346, 1.9389994144439697, 1.9871578216552734, 1.784293532371521, 1.854921579360962, 1.7075998783111572, 2.061573028564453, 1.9452331066131592, 2.0056381225585938, 1.834672451019287, 1.8152104616165161, 1.8499270677566528, 1.8210804462432861, 1.7312606573104858, 1.7180900573730469, 1.799805998802185, 1.7473933696746826, 1.7066857814788818, 1.8169560432434082, 1.7411235570907593, 1.768080472946167, 1.742506742477417, 1.724694848060608, 1.862725853919983, 1.873543620109558, 1.696153163909912, 1.6998940706253052, 1.8547183275222778, 1.8546236753463745, 1.7770273685455322, 1.8969316482543945, 1.759036898612976, 1.8086204528808594, 1.7810450792312622, 1.5720332860946655, 1.7031941413879395, 1.9006935358047485, 1.678153157234192, 1.9148658514022827, 1.5800518989562988, 1.695643424987793, 1.6698769330978394, 1.872199535369873, 1.7954065799713135, 1.6396715641021729, 1.7029290199279785, 1.8102002143859863, 1.6604138612747192, 1.6078803539276123, 1.4923498630523682, 1.6655454635620117, 1.728773832321167, 1.6304898262023926, 1.724149465560913, 1.627072811126709, 1.6732795238494873, 1.5792078971862793, 1.6477727890014648, 1.971034049987793, 1.6157639026641846, 1.6852917671203613, 1.6421616077423096, 1.8977195024490356, 2.0219502449035645, 1.6800005435943604, 1.7283525466918945, 1.740890383720398, 1.5727508068084717, 1.6211318969726562, 1.758453130722046, 1.740113377571106, 1.727112889289856, 1.5632107257843018, 1.4893598556518555, 1.6665762662887573, 1.5757501125335693, 1.5932402610778809, 1.689834475517273, 1.6789696216583252, 1.6977975368499756, 1.7410706281661987, 1.6640067100524902, 1.5409531593322754], 'val_losses': [1.1116807460784912, 1.1116807460784912, 1.1116807460784912, 1.1116807460784912, 1.0973738431930542, 1.0940171480178833, 1.1059038639068604, 1.1316815614700317, 1.145691990852356, 1.1467957496643066, 1.1481188535690308, 1.1435811519622803, 1.1405534744262695, 1.1384341716766357, 1.1436235904693604, 1.141176462173462, 1.1322462558746338, 1.1251420974731445, 1.1160223484039307, 1.1068766117095947, 1.1009869575500488, 1.0952856540679932, 1.093015193939209, 1.0872468948364258, 1.0828937292099, 1.0771381855010986, 1.0688543319702148, 1.0581880807876587, 1.0455466508865356, 1.035668134689331, 1.0313127040863037, 1.0226564407348633, 1.015250325202942, 1.0108815431594849, 1.0046650171279907, 0.9964116811752319, 0.9855527877807617, 0.9781529307365417, 0.9730507135391235, 0.9687864184379578, 0.963329553604126, 0.9609591960906982, 0.9616956114768982, 0.9616032838821411, 0.9604929685592651, 0.9615821838378906, 0.9631551504135132, 0.9636715650558472, 0.9666085839271545, 0.9687804579734802, 0.9689454436302185, 0.9710039496421814, 0.9715190529823303, 0.9714298844337463, 0.9699652194976807, 0.9672243595123291, 0.9653809666633606, 0.9641796946525574, 0.9623164534568787, 0.9579659700393677, 0.9551505446434021, 0.9521080851554871, 0.9506519436836243, 0.9495084285736084, 0.9494438171386719, 0.9479198455810547, 0.9463843107223511, 0.9452899694442749, 0.9438152313232422, 0.9423585534095764, 0.9418168067932129, 0.939842700958252, 0.9383234977722168, 0.9386364221572876, 0.9385621547698975, 0.9399125576019287, 0.9407227635383606, 0.9418725371360779, 0.942874550819397, 0.9439675211906433, 0.9440488815307617, 0.9442621469497681, 0.9442456364631653, 0.9444966912269592, 0.9448234438896179, 0.9449539184570312, 0.9446717500686646, 0.9445552229881287, 0.9447572231292725, 0.944817841053009, 0.9447275996208191, 0.9447351694107056, 0.9447095394134521, 0.9447095394134521], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.512908393234953e-05, 'batch_size': 256, 'epochs': 94, 'hidden_size': 129, 'dropout': 0.05365304988293941, 'weight_decay': 0.0007114724451015198, 'label_smoothing': 0.105632599246988, 'grad_clip_norm': 0.5614174244508897, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 17805, 'model_storage_size_kb': 76.505859375, 'model_size_validation': 'PASS'}
2025-10-13 22:43:25,991 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:43:25,991 - INFO - _models.training_function_executor - Model: 17,805 parameters, 76.5KB (PASS 256KB limit)
2025-10-13 22:43:25,991 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.839s
2025-10-13 22:43:26,070 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:43:26,070 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.079s
2025-10-13 22:43:26,070 - INFO - bo.run_bo - Recorded observation #33: hparams={'lr': 3.512908393234953e-05, 'batch_size': np.int64(256), 'epochs': np.int64(94), 'hidden_size': np.int64(129), 'dropout': 0.05365304988293941, 'weight_decay': 0.0007114724451015198, 'label_smoothing': 0.105632599246988, 'grad_clip_norm': 0.5614174244508897, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:43:26,070 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 33: {'lr': 3.512908393234953e-05, 'batch_size': np.int64(256), 'epochs': np.int64(94), 'hidden_size': np.int64(129), 'dropout': 0.05365304988293941, 'weight_decay': 0.0007114724451015198, 'label_smoothing': 0.105632599246988, 'grad_clip_norm': 0.5614174244508897, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:43:26,071 - INFO - bo.run_bo - üîçBO Trial 34: Using RF surrogate + Expected Improvement
2025-10-13 22:43:26,071 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:26,071 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 34 (NaN monitoring active)
2025-10-13 22:43:26,071 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:26,071 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:26,071 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.350669257874682e-05, 'batch_size': 96, 'epochs': 55, 'hidden_size': 163, 'dropout': 0.15012047933848086, 'weight_decay': 0.0019168289596783188, 'label_smoothing': 0.03887302916501066, 'grad_clip_norm': 2.54924783282467, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:43:26,072 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.350669257874682e-05, 'batch_size': 96, 'epochs': 55, 'hidden_size': 163, 'dropout': 0.15012047933848086, 'weight_decay': 0.0019168289596783188, 'label_smoothing': 0.03887302916501066, 'grad_clip_norm': 2.54924783282467, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:43:29,716 - INFO - _models.training_function_executor - Epoch 001/055 | train_loss=13.5578 | val_loss=12.6833 | val_acc=0.0938
2025-10-13 22:43:29,723 - INFO - _models.training_function_executor - Epoch 002/055 | train_loss=12.7014 | val_loss=12.3370 | val_acc=0.0938
2025-10-13 22:43:29,730 - INFO - _models.training_function_executor - Epoch 003/055 | train_loss=12.5962 | val_loss=11.6501 | val_acc=0.0938
2025-10-13 22:43:29,735 - INFO - _models.training_function_executor - Epoch 004/055 | train_loss=11.4260 | val_loss=10.6318 | val_acc=0.0938
2025-10-13 22:43:29,740 - INFO - _models.training_function_executor - Epoch 005/055 | train_loss=10.5040 | val_loss=9.6279 | val_acc=0.0938
2025-10-13 22:43:29,745 - INFO - _models.training_function_executor - Epoch 006/055 | train_loss=9.4569 | val_loss=8.6412 | val_acc=0.0938
2025-10-13 22:43:29,751 - INFO - _models.training_function_executor - Epoch 007/055 | train_loss=8.4362 | val_loss=7.6711 | val_acc=0.0938
2025-10-13 22:43:29,756 - INFO - _models.training_function_executor - Epoch 008/055 | train_loss=7.6112 | val_loss=6.7354 | val_acc=0.0938
2025-10-13 22:43:29,761 - INFO - _models.training_function_executor - Epoch 009/055 | train_loss=6.9832 | val_loss=5.8222 | val_acc=0.0938
2025-10-13 22:43:29,766 - INFO - _models.training_function_executor - Epoch 010/055 | train_loss=5.8466 | val_loss=4.9632 | val_acc=0.0938
2025-10-13 22:43:29,771 - INFO - _models.training_function_executor - Epoch 011/055 | train_loss=5.6067 | val_loss=4.1455 | val_acc=0.0938
2025-10-13 22:43:29,777 - INFO - _models.training_function_executor - Epoch 012/055 | train_loss=5.2144 | val_loss=3.3826 | val_acc=0.0938
2025-10-13 22:43:29,783 - INFO - _models.training_function_executor - Epoch 013/055 | train_loss=4.2312 | val_loss=2.6614 | val_acc=0.0938
2025-10-13 22:43:29,788 - INFO - _models.training_function_executor - Epoch 014/055 | train_loss=3.5371 | val_loss=2.0054 | val_acc=0.1094
2025-10-13 22:43:29,794 - INFO - _models.training_function_executor - Epoch 015/055 | train_loss=3.4472 | val_loss=1.4451 | val_acc=0.1406
2025-10-13 22:43:29,800 - INFO - _models.training_function_executor - Epoch 016/055 | train_loss=3.0539 | val_loss=1.0736 | val_acc=0.4688
2025-10-13 22:43:29,806 - INFO - _models.training_function_executor - Epoch 017/055 | train_loss=2.9664 | val_loss=0.9584 | val_acc=0.7188
2025-10-13 22:43:29,813 - INFO - _models.training_function_executor - Epoch 018/055 | train_loss=2.8051 | val_loss=1.0035 | val_acc=0.7188
2025-10-13 22:43:29,818 - INFO - _models.training_function_executor - Epoch 019/055 | train_loss=2.4722 | val_loss=1.1065 | val_acc=0.7188
2025-10-13 22:43:29,824 - INFO - _models.training_function_executor - Epoch 020/055 | train_loss=2.7144 | val_loss=1.2207 | val_acc=0.7188
2025-10-13 22:43:29,829 - INFO - _models.training_function_executor - Epoch 021/055 | train_loss=2.4603 | val_loss=1.2843 | val_acc=0.7188
2025-10-13 22:43:29,834 - INFO - _models.training_function_executor - Epoch 022/055 | train_loss=2.7123 | val_loss=1.3155 | val_acc=0.7188
2025-10-13 22:43:29,839 - INFO - _models.training_function_executor - Epoch 023/055 | train_loss=2.7603 | val_loss=1.3203 | val_acc=0.7188
2025-10-13 22:43:29,844 - INFO - _models.training_function_executor - Epoch 024/055 | train_loss=2.4943 | val_loss=1.2668 | val_acc=0.7188
2025-10-13 22:43:29,849 - INFO - _models.training_function_executor - Epoch 025/055 | train_loss=2.6715 | val_loss=1.1901 | val_acc=0.7188
2025-10-13 22:43:29,855 - INFO - _models.training_function_executor - Epoch 026/055 | train_loss=2.7751 | val_loss=1.1310 | val_acc=0.7188
2025-10-13 22:43:29,860 - INFO - _models.training_function_executor - Epoch 027/055 | train_loss=2.4809 | val_loss=1.0875 | val_acc=0.7188
2025-10-13 22:43:29,865 - INFO - _models.training_function_executor - Epoch 028/055 | train_loss=2.9000 | val_loss=1.0533 | val_acc=0.7188
2025-10-13 22:43:29,870 - INFO - _models.training_function_executor - Epoch 029/055 | train_loss=2.5018 | val_loss=1.0301 | val_acc=0.7188
2025-10-13 22:43:29,875 - INFO - _models.training_function_executor - Epoch 030/055 | train_loss=2.2184 | val_loss=1.0061 | val_acc=0.7188
2025-10-13 22:43:29,880 - INFO - _models.training_function_executor - Epoch 031/055 | train_loss=2.3049 | val_loss=0.9853 | val_acc=0.7188
2025-10-13 22:43:29,886 - INFO - _models.training_function_executor - Epoch 032/055 | train_loss=2.0586 | val_loss=0.9679 | val_acc=0.7188
2025-10-13 22:43:29,891 - INFO - _models.training_function_executor - Epoch 033/055 | train_loss=2.1835 | val_loss=0.9674 | val_acc=0.7188
2025-10-13 22:43:29,896 - INFO - _models.training_function_executor - Epoch 034/055 | train_loss=2.8639 | val_loss=0.9649 | val_acc=0.7188
2025-10-13 22:43:29,901 - INFO - _models.training_function_executor - Epoch 035/055 | train_loss=2.4046 | val_loss=0.9612 | val_acc=0.7188
2025-10-13 22:43:29,906 - INFO - _models.training_function_executor - Epoch 036/055 | train_loss=2.3066 | val_loss=0.9680 | val_acc=0.7188
2025-10-13 22:43:29,911 - INFO - _models.training_function_executor - Epoch 037/055 | train_loss=2.6677 | val_loss=0.9784 | val_acc=0.7188
2025-10-13 22:43:29,917 - INFO - _models.training_function_executor - Epoch 038/055 | train_loss=2.4637 | val_loss=0.9791 | val_acc=0.7188
2025-10-13 22:43:29,922 - INFO - _models.training_function_executor - Epoch 039/055 | train_loss=2.4141 | val_loss=0.9789 | val_acc=0.7188
2025-10-13 22:43:29,926 - INFO - _models.training_function_executor - Epoch 040/055 | train_loss=2.3879 | val_loss=0.9754 | val_acc=0.7188
2025-10-13 22:43:29,931 - INFO - _models.training_function_executor - Epoch 041/055 | train_loss=2.3527 | val_loss=0.9739 | val_acc=0.7188
2025-10-13 22:43:29,937 - INFO - _models.training_function_executor - Epoch 042/055 | train_loss=2.5567 | val_loss=0.9722 | val_acc=0.7188
2025-10-13 22:43:29,942 - INFO - _models.training_function_executor - Epoch 043/055 | train_loss=2.3432 | val_loss=0.9709 | val_acc=0.7188
2025-10-13 22:43:29,947 - INFO - _models.training_function_executor - Epoch 044/055 | train_loss=2.4489 | val_loss=0.9694 | val_acc=0.7188
2025-10-13 22:43:29,952 - INFO - _models.training_function_executor - Epoch 045/055 | train_loss=2.3178 | val_loss=0.9653 | val_acc=0.7188
2025-10-13 22:43:29,957 - INFO - _models.training_function_executor - Epoch 046/055 | train_loss=2.7469 | val_loss=0.9633 | val_acc=0.7188
2025-10-13 22:43:29,962 - INFO - _models.training_function_executor - Epoch 047/055 | train_loss=2.5073 | val_loss=0.9640 | val_acc=0.7188
2025-10-13 22:43:29,967 - INFO - _models.training_function_executor - Epoch 048/055 | train_loss=2.5397 | val_loss=0.9657 | val_acc=0.7188
2025-10-13 22:43:29,972 - INFO - _models.training_function_executor - Epoch 049/055 | train_loss=2.2865 | val_loss=0.9668 | val_acc=0.7188
2025-10-13 22:43:29,977 - INFO - _models.training_function_executor - Epoch 050/055 | train_loss=2.3745 | val_loss=0.9685 | val_acc=0.7188
2025-10-13 22:43:29,982 - INFO - _models.training_function_executor - Epoch 051/055 | train_loss=2.2328 | val_loss=0.9688 | val_acc=0.7188
2025-10-13 22:43:29,987 - INFO - _models.training_function_executor - Epoch 052/055 | train_loss=2.3965 | val_loss=0.9686 | val_acc=0.7188
2025-10-13 22:43:29,992 - INFO - _models.training_function_executor - Epoch 053/055 | train_loss=2.3390 | val_loss=0.9684 | val_acc=0.7188
2025-10-13 22:43:29,996 - INFO - _models.training_function_executor - Epoch 054/055 | train_loss=2.2879 | val_loss=0.9680 | val_acc=0.7188
2025-10-13 22:43:30,002 - INFO - _models.training_function_executor - Epoch 055/055 | train_loss=2.2577 | val_loss=0.9680 | val_acc=0.7188
2025-10-13 22:43:30,870 - INFO - _models.training_function_executor - Model: 28,039 parameters, 60.2KB storage
2025-10-13 22:43:30,871 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [13.557808637619019, 12.701351761817932, 12.596165418624878, 11.426016807556152, 10.504036664962769, 9.456885814666748, 8.436236798763275, 7.611199259757996, 6.983201265335083, 5.846572697162628, 5.606737315654755, 5.2144200801849365, 4.231184244155884, 3.537122964859009, 3.4472128450870514, 3.053862452507019, 2.9664240777492523, 2.805140942335129, 2.472203552722931, 2.71438530087471, 2.460267663002014, 2.712285727262497, 2.760303318500519, 2.4942861795425415, 2.6714793741703033, 2.77509742975235, 2.4808612167835236, 2.900024890899658, 2.5017785727977753, 2.218383550643921, 2.3049292266368866, 2.058559149503708, 2.183469533920288, 2.8638598024845123, 2.404604136943817, 2.306634396314621, 2.667696535587311, 2.4636844396591187, 2.414097875356674, 2.387871265411377, 2.352652981877327, 2.5567151606082916, 2.343222439289093, 2.4489068388938904, 2.3178273141384125, 2.7469496726989746, 2.5072542428970337, 2.539671242237091, 2.2865380942821503, 2.37446391582489, 2.232792854309082, 2.3965397775173187, 2.3390370905399323, 2.2879213988780975, 2.257712557911873], 'val_losses': [12.683283805847168, 12.33704948425293, 11.650087356567383, 10.631753921508789, 9.627922058105469, 8.641170501708984, 7.671095371246338, 6.735433101654053, 5.822239398956299, 4.963233470916748, 4.145525932312012, 3.3826100826263428, 2.6613845825195312, 2.005411148071289, 1.4450699090957642, 1.073641300201416, 0.958359956741333, 1.0035265684127808, 1.106488585472107, 1.2206676006317139, 1.2842535972595215, 1.3155298233032227, 1.3203479051589966, 1.2668451070785522, 1.190071940422058, 1.1309609413146973, 1.0875177383422852, 1.0532971620559692, 1.0300897359848022, 1.0061368942260742, 0.9853044748306274, 0.9678632020950317, 0.9674169421195984, 0.9648609161376953, 0.9612241983413696, 0.9680097103118896, 0.9783521294593811, 0.9791090488433838, 0.9789172410964966, 0.9754056334495544, 0.9738904237747192, 0.9722408056259155, 0.9708892703056335, 0.9693831205368042, 0.9652695655822754, 0.9632589221000671, 0.9640254974365234, 0.9657288789749146, 0.9668452739715576, 0.9685034155845642, 0.9688128232955933, 0.9685836434364319, 0.9683744311332703, 0.9680114984512329, 0.9680206775665283], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.109375, 0.140625, 0.46875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.350669257874682e-05, 'batch_size': 96, 'epochs': 55, 'hidden_size': 163, 'dropout': 0.15012047933848086, 'weight_decay': 0.0019168289596783188, 'label_smoothing': 0.03887302916501066, 'grad_clip_norm': 2.54924783282467, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 28039, 'model_storage_size_kb': 60.2400390625, 'model_size_validation': 'PASS'}
2025-10-13 22:43:30,871 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:43:30,871 - INFO - _models.training_function_executor - Model: 28,039 parameters, 60.2KB (PASS 256KB limit)
2025-10-13 22:43:30,871 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.800s
2025-10-13 22:43:30,949 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:43:30,949 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.078s
2025-10-13 22:43:30,949 - INFO - bo.run_bo - Recorded observation #34: hparams={'lr': 3.350669257874682e-05, 'batch_size': np.int64(96), 'epochs': np.int64(55), 'hidden_size': np.int64(163), 'dropout': 0.15012047933848086, 'weight_decay': 0.0019168289596783188, 'label_smoothing': 0.03887302916501066, 'grad_clip_norm': 2.54924783282467, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:43:30,949 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 34: {'lr': 3.350669257874682e-05, 'batch_size': np.int64(96), 'epochs': np.int64(55), 'hidden_size': np.int64(163), 'dropout': 0.15012047933848086, 'weight_decay': 0.0019168289596783188, 'label_smoothing': 0.03887302916501066, 'grad_clip_norm': 2.54924783282467, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:43:30,949 - INFO - bo.run_bo - üîçBO Trial 35: Using RF surrogate + Expected Improvement
2025-10-13 22:43:30,949 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:30,949 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 35 (NaN monitoring active)
2025-10-13 22:43:30,949 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:30,950 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:30,950 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.5064733830424374e-05, 'batch_size': 192, 'epochs': 55, 'hidden_size': 89, 'dropout': 0.07520114394602441, 'weight_decay': 0.0003450188072128087, 'label_smoothing': 0.021501543310623067, 'grad_clip_norm': 3.2040736600014825, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:43:30,950 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.5064733830424374e-05, 'batch_size': 192, 'epochs': 55, 'hidden_size': 89, 'dropout': 0.07520114394602441, 'weight_decay': 0.0003450188072128087, 'label_smoothing': 0.021501543310623067, 'grad_clip_norm': 3.2040736600014825, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:43:35,104 - INFO - _models.training_function_executor - Epoch 001/055 | train_loss=6.1668 | val_loss=5.9572 | val_acc=0.1406
2025-10-13 22:43:35,110 - INFO - _models.training_function_executor - Epoch 002/055 | train_loss=6.2219 | val_loss=5.9572 | val_acc=0.1406
2025-10-13 22:43:35,115 - INFO - _models.training_function_executor - Epoch 003/055 | train_loss=6.2748 | val_loss=5.8755 | val_acc=0.1562
2025-10-13 22:43:35,120 - INFO - _models.training_function_executor - Epoch 004/055 | train_loss=6.3081 | val_loss=5.7244 | val_acc=0.2188
2025-10-13 22:43:35,125 - INFO - _models.training_function_executor - Epoch 005/055 | train_loss=6.0478 | val_loss=5.5793 | val_acc=0.2656
2025-10-13 22:43:35,129 - INFO - _models.training_function_executor - Epoch 006/055 | train_loss=6.0509 | val_loss=5.4443 | val_acc=0.2969
2025-10-13 22:43:35,134 - INFO - _models.training_function_executor - Epoch 007/055 | train_loss=5.7717 | val_loss=5.3196 | val_acc=0.3281
2025-10-13 22:43:35,138 - INFO - _models.training_function_executor - Epoch 008/055 | train_loss=5.7393 | val_loss=5.2087 | val_acc=0.3906
2025-10-13 22:43:35,142 - INFO - _models.training_function_executor - Epoch 009/055 | train_loss=5.8569 | val_loss=5.1117 | val_acc=0.4844
2025-10-13 22:43:35,146 - INFO - _models.training_function_executor - Epoch 010/055 | train_loss=5.3475 | val_loss=5.0250 | val_acc=0.5156
2025-10-13 22:43:35,150 - INFO - _models.training_function_executor - Epoch 011/055 | train_loss=5.3784 | val_loss=4.9485 | val_acc=0.5469
2025-10-13 22:43:35,154 - INFO - _models.training_function_executor - Epoch 012/055 | train_loss=5.3906 | val_loss=4.8820 | val_acc=0.6875
2025-10-13 22:43:35,158 - INFO - _models.training_function_executor - Epoch 013/055 | train_loss=5.4390 | val_loss=4.8281 | val_acc=0.7188
2025-10-13 22:43:35,163 - INFO - _models.training_function_executor - Epoch 014/055 | train_loss=5.0531 | val_loss=4.7819 | val_acc=0.7344
2025-10-13 22:43:35,167 - INFO - _models.training_function_executor - Epoch 015/055 | train_loss=5.3815 | val_loss=4.7409 | val_acc=0.7188
2025-10-13 22:43:35,171 - INFO - _models.training_function_executor - Epoch 016/055 | train_loss=5.0671 | val_loss=4.7056 | val_acc=0.7188
2025-10-13 22:43:35,175 - INFO - _models.training_function_executor - Epoch 017/055 | train_loss=5.1307 | val_loss=4.6737 | val_acc=0.7188
2025-10-13 22:43:35,179 - INFO - _models.training_function_executor - Epoch 018/055 | train_loss=4.9939 | val_loss=4.6446 | val_acc=0.7188
2025-10-13 22:43:35,184 - INFO - _models.training_function_executor - Epoch 019/055 | train_loss=5.1657 | val_loss=4.6164 | val_acc=0.7188
2025-10-13 22:43:35,189 - INFO - _models.training_function_executor - Epoch 020/055 | train_loss=4.9177 | val_loss=4.5885 | val_acc=0.7188
2025-10-13 22:43:35,193 - INFO - _models.training_function_executor - Epoch 021/055 | train_loss=4.7731 | val_loss=4.5615 | val_acc=0.7188
2025-10-13 22:43:35,198 - INFO - _models.training_function_executor - Epoch 022/055 | train_loss=4.7850 | val_loss=4.5386 | val_acc=0.7188
2025-10-13 22:43:35,203 - INFO - _models.training_function_executor - Epoch 023/055 | train_loss=4.6385 | val_loss=4.5140 | val_acc=0.7188
2025-10-13 22:43:35,208 - INFO - _models.training_function_executor - Epoch 024/055 | train_loss=4.8767 | val_loss=4.4881 | val_acc=0.7188
2025-10-13 22:43:35,212 - INFO - _models.training_function_executor - Epoch 025/055 | train_loss=4.7890 | val_loss=4.4641 | val_acc=0.7188
2025-10-13 22:43:35,217 - INFO - _models.training_function_executor - Epoch 026/055 | train_loss=4.7354 | val_loss=4.4401 | val_acc=0.7188
2025-10-13 22:43:35,222 - INFO - _models.training_function_executor - Epoch 027/055 | train_loss=4.8712 | val_loss=4.4168 | val_acc=0.7188
2025-10-13 22:43:35,226 - INFO - _models.training_function_executor - Epoch 028/055 | train_loss=4.7430 | val_loss=4.3925 | val_acc=0.7188
2025-10-13 22:43:35,230 - INFO - _models.training_function_executor - Epoch 029/055 | train_loss=4.6386 | val_loss=4.3699 | val_acc=0.7188
2025-10-13 22:43:35,234 - INFO - _models.training_function_executor - Epoch 030/055 | train_loss=4.7450 | val_loss=4.3483 | val_acc=0.7188
2025-10-13 22:43:35,238 - INFO - _models.training_function_executor - Epoch 031/055 | train_loss=4.5630 | val_loss=4.3256 | val_acc=0.7188
2025-10-13 22:43:35,243 - INFO - _models.training_function_executor - Epoch 032/055 | train_loss=4.8242 | val_loss=4.3072 | val_acc=0.7188
2025-10-13 22:43:35,247 - INFO - _models.training_function_executor - Epoch 033/055 | train_loss=4.8255 | val_loss=4.2899 | val_acc=0.7188
2025-10-13 22:43:35,251 - INFO - _models.training_function_executor - Epoch 034/055 | train_loss=4.5913 | val_loss=4.2753 | val_acc=0.7188
2025-10-13 22:43:35,255 - INFO - _models.training_function_executor - Epoch 035/055 | train_loss=4.6571 | val_loss=4.2618 | val_acc=0.7188
2025-10-13 22:43:35,259 - INFO - _models.training_function_executor - Epoch 036/055 | train_loss=4.4953 | val_loss=4.2501 | val_acc=0.7188
2025-10-13 22:43:35,263 - INFO - _models.training_function_executor - Epoch 037/055 | train_loss=4.5621 | val_loss=4.2383 | val_acc=0.7188
2025-10-13 22:43:35,267 - INFO - _models.training_function_executor - Epoch 038/055 | train_loss=4.6356 | val_loss=4.2291 | val_acc=0.7188
2025-10-13 22:43:35,271 - INFO - _models.training_function_executor - Epoch 039/055 | train_loss=4.4735 | val_loss=4.2196 | val_acc=0.7188
2025-10-13 22:43:35,275 - INFO - _models.training_function_executor - Epoch 040/055 | train_loss=4.3934 | val_loss=4.2072 | val_acc=0.7188
2025-10-13 22:43:35,279 - INFO - _models.training_function_executor - Epoch 041/055 | train_loss=4.4723 | val_loss=4.1990 | val_acc=0.7188
2025-10-13 22:43:35,284 - INFO - _models.training_function_executor - Epoch 042/055 | train_loss=4.4625 | val_loss=4.1905 | val_acc=0.7188
2025-10-13 22:43:35,288 - INFO - _models.training_function_executor - Epoch 043/055 | train_loss=4.6738 | val_loss=4.1843 | val_acc=0.7188
2025-10-13 22:43:35,292 - INFO - _models.training_function_executor - Epoch 044/055 | train_loss=4.5969 | val_loss=4.1795 | val_acc=0.7188
2025-10-13 22:43:35,296 - INFO - _models.training_function_executor - Epoch 045/055 | train_loss=4.4516 | val_loss=4.1746 | val_acc=0.7188
2025-10-13 22:43:35,300 - INFO - _models.training_function_executor - Epoch 046/055 | train_loss=4.3502 | val_loss=4.1711 | val_acc=0.7188
2025-10-13 22:43:35,305 - INFO - _models.training_function_executor - Epoch 047/055 | train_loss=4.4753 | val_loss=4.1681 | val_acc=0.7188
2025-10-13 22:43:35,309 - INFO - _models.training_function_executor - Epoch 048/055 | train_loss=4.4562 | val_loss=4.1656 | val_acc=0.7188
2025-10-13 22:43:35,313 - INFO - _models.training_function_executor - Epoch 049/055 | train_loss=4.3653 | val_loss=4.1627 | val_acc=0.7188
2025-10-13 22:43:35,317 - INFO - _models.training_function_executor - Epoch 050/055 | train_loss=4.6096 | val_loss=4.1611 | val_acc=0.7188
2025-10-13 22:43:35,321 - INFO - _models.training_function_executor - Epoch 051/055 | train_loss=4.4907 | val_loss=4.1605 | val_acc=0.7188
2025-10-13 22:43:35,325 - INFO - _models.training_function_executor - Epoch 052/055 | train_loss=4.5365 | val_loss=4.1603 | val_acc=0.7188
2025-10-13 22:43:35,329 - INFO - _models.training_function_executor - Epoch 053/055 | train_loss=4.4066 | val_loss=4.1600 | val_acc=0.7188
2025-10-13 22:43:35,333 - INFO - _models.training_function_executor - Epoch 054/055 | train_loss=4.4157 | val_loss=4.1600 | val_acc=0.7188
2025-10-13 22:43:35,337 - INFO - _models.training_function_executor - Epoch 055/055 | train_loss=4.0895 | val_loss=4.1600 | val_acc=0.7188
2025-10-13 22:43:36,180 - INFO - _models.training_function_executor - Model: 8,725 parameters, 37.5KB storage
2025-10-13 22:43:36,181 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [6.166794657707214, 6.221901059150696, 6.274821519851685, 6.308115363121033, 6.0477893352508545, 6.050936460494995, 5.771682143211365, 5.739273190498352, 5.856860041618347, 5.347471356391907, 5.378435254096985, 5.390591859817505, 5.439036965370178, 5.053136944770813, 5.381463050842285, 5.067056179046631, 5.130735516548157, 4.9938905239105225, 5.165677189826965, 4.917749643325806, 4.773064017295837, 4.785008192062378, 4.638545989990234, 4.876678287982941, 4.788954377174377, 4.735438942909241, 4.871200203895569, 4.74303138256073, 4.6386367082595825, 4.744999408721924, 4.562995791435242, 4.824191391468048, 4.825455546379089, 4.591330170631409, 4.657076835632324, 4.4952675104141235, 4.562100768089294, 4.6355961561203, 4.473465442657471, 4.39336359500885, 4.4722983837127686, 4.462526321411133, 4.673849940299988, 4.596899747848511, 4.45159113407135, 4.350166082382202, 4.475277304649353, 4.456228613853455, 4.3652695417404175, 4.6096232533454895, 4.490739047527313, 4.536514639854431, 4.406566739082336, 4.415727078914642, 4.089451789855957], 'val_losses': [5.957174777984619, 5.957174777984619, 5.875490188598633, 5.7243733406066895, 5.579348087310791, 5.444343566894531, 5.319642543792725, 5.208706855773926, 5.1117143630981445, 5.024950981140137, 4.94854211807251, 4.881972789764404, 4.828075408935547, 4.781929969787598, 4.740874767303467, 4.705564498901367, 4.673685073852539, 4.644594192504883, 4.616410255432129, 4.588512897491455, 4.5615153312683105, 4.538589954376221, 4.513991832733154, 4.488064289093018, 4.464112758636475, 4.4401092529296875, 4.416804790496826, 4.392547130584717, 4.369926929473877, 4.348274230957031, 4.325561046600342, 4.307189464569092, 4.28987979888916, 4.275310039520264, 4.2618088722229, 4.250110149383545, 4.238254070281982, 4.2290754318237305, 4.2196245193481445, 4.207179546356201, 4.199014186859131, 4.190474987030029, 4.184338092803955, 4.179543495178223, 4.174591541290283, 4.171123027801514, 4.168063163757324, 4.1655659675598145, 4.162695407867432, 4.161141395568848, 4.160514831542969, 4.1602582931518555, 4.160008907318115, 4.160017967224121, 4.160041332244873], 'val_acc': [0.140625, 0.140625, 0.15625, 0.21875, 0.265625, 0.296875, 0.328125, 0.390625, 0.484375, 0.515625, 0.546875, 0.6875, 0.71875, 0.734375, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.5064733830424374e-05, 'batch_size': 192, 'epochs': 55, 'hidden_size': 89, 'dropout': 0.07520114394602441, 'weight_decay': 0.0003450188072128087, 'label_smoothing': 0.021501543310623067, 'grad_clip_norm': 3.2040736600014825, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 8725, 'model_storage_size_kb': 37.490234375, 'model_size_validation': 'PASS'}
2025-10-13 22:43:36,181 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:43:36,181 - INFO - _models.training_function_executor - Model: 8,725 parameters, 37.5KB (PASS 256KB limit)
2025-10-13 22:43:36,181 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.231s
2025-10-13 22:43:36,260 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:43:36,261 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.079s
2025-10-13 22:43:36,261 - INFO - bo.run_bo - Recorded observation #35: hparams={'lr': 2.5064733830424374e-05, 'batch_size': np.int64(192), 'epochs': np.int64(55), 'hidden_size': np.int64(89), 'dropout': 0.07520114394602441, 'weight_decay': 0.0003450188072128087, 'label_smoothing': 0.021501543310623067, 'grad_clip_norm': 3.2040736600014825, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:43:36,261 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 35: {'lr': 2.5064733830424374e-05, 'batch_size': np.int64(192), 'epochs': np.int64(55), 'hidden_size': np.int64(89), 'dropout': 0.07520114394602441, 'weight_decay': 0.0003450188072128087, 'label_smoothing': 0.021501543310623067, 'grad_clip_norm': 3.2040736600014825, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:43:36,261 - INFO - bo.run_bo - üîçBO Trial 36: Using RF surrogate + Expected Improvement
2025-10-13 22:43:36,261 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:36,261 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 36 (NaN monitoring active)
2025-10-13 22:43:36,261 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:36,261 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:36,261 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.8894106650720734e-05, 'batch_size': 256, 'epochs': 51, 'hidden_size': 106, 'dropout': 0.2990190378340153, 'weight_decay': 0.0005204260949749173, 'label_smoothing': 0.016633672294433913, 'grad_clip_norm': 1.1438704993054576, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:43:36,262 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.8894106650720734e-05, 'batch_size': 256, 'epochs': 51, 'hidden_size': 106, 'dropout': 0.2990190378340153, 'weight_decay': 0.0005204260949749173, 'label_smoothing': 0.016633672294433913, 'grad_clip_norm': 1.1438704993054576, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:43:39,978 - INFO - _models.training_function_executor - Epoch 001/051 | train_loss=7.5169 | val_loss=4.7166 | val_acc=0.1875
2025-10-13 22:43:39,983 - INFO - _models.training_function_executor - Epoch 002/051 | train_loss=7.4531 | val_loss=4.7166 | val_acc=0.1875
2025-10-13 22:43:39,987 - INFO - _models.training_function_executor - Epoch 003/051 | train_loss=6.8739 | val_loss=4.7166 | val_acc=0.1875
2025-10-13 22:43:39,992 - INFO - _models.training_function_executor - Epoch 004/051 | train_loss=7.1096 | val_loss=4.7166 | val_acc=0.1875
2025-10-13 22:43:39,995 - INFO - _models.training_function_executor - Epoch 005/051 | train_loss=7.1570 | val_loss=4.7166 | val_acc=0.1875
2025-10-13 22:43:39,999 - INFO - _models.training_function_executor - Epoch 006/051 | train_loss=7.4865 | val_loss=4.5658 | val_acc=0.1875
2025-10-13 22:43:40,002 - INFO - _models.training_function_executor - Epoch 007/051 | train_loss=7.4872 | val_loss=4.4162 | val_acc=0.1875
2025-10-13 22:43:40,005 - INFO - _models.training_function_executor - Epoch 008/051 | train_loss=6.5352 | val_loss=4.2722 | val_acc=0.1875
2025-10-13 22:43:40,008 - INFO - _models.training_function_executor - Epoch 009/051 | train_loss=7.1046 | val_loss=4.1277 | val_acc=0.2031
2025-10-13 22:43:40,012 - INFO - _models.training_function_executor - Epoch 010/051 | train_loss=7.4971 | val_loss=3.9932 | val_acc=0.2500
2025-10-13 22:43:40,015 - INFO - _models.training_function_executor - Epoch 011/051 | train_loss=6.7542 | val_loss=3.8605 | val_acc=0.2969
2025-10-13 22:43:40,018 - INFO - _models.training_function_executor - Epoch 012/051 | train_loss=6.5085 | val_loss=3.7347 | val_acc=0.3125
2025-10-13 22:43:40,021 - INFO - _models.training_function_executor - Epoch 013/051 | train_loss=6.7952 | val_loss=3.6152 | val_acc=0.3281
2025-10-13 22:43:40,024 - INFO - _models.training_function_executor - Epoch 014/051 | train_loss=6.6159 | val_loss=3.4984 | val_acc=0.3438
2025-10-13 22:43:40,028 - INFO - _models.training_function_executor - Epoch 015/051 | train_loss=6.4912 | val_loss=3.3923 | val_acc=0.3594
2025-10-13 22:43:40,031 - INFO - _models.training_function_executor - Epoch 016/051 | train_loss=6.5687 | val_loss=3.2874 | val_acc=0.3906
2025-10-13 22:43:40,034 - INFO - _models.training_function_executor - Epoch 017/051 | train_loss=7.0326 | val_loss=3.1891 | val_acc=0.3906
2025-10-13 22:43:40,037 - INFO - _models.training_function_executor - Epoch 018/051 | train_loss=6.1814 | val_loss=3.1018 | val_acc=0.4531
2025-10-13 22:43:40,040 - INFO - _models.training_function_executor - Epoch 019/051 | train_loss=6.6652 | val_loss=3.0126 | val_acc=0.4844
2025-10-13 22:43:40,044 - INFO - _models.training_function_executor - Epoch 020/051 | train_loss=5.9040 | val_loss=2.9347 | val_acc=0.5469
2025-10-13 22:43:40,047 - INFO - _models.training_function_executor - Epoch 021/051 | train_loss=5.3525 | val_loss=2.8635 | val_acc=0.5625
2025-10-13 22:43:40,050 - INFO - _models.training_function_executor - Epoch 022/051 | train_loss=6.3995 | val_loss=2.7951 | val_acc=0.5781
2025-10-13 22:43:40,053 - INFO - _models.training_function_executor - Epoch 023/051 | train_loss=6.1847 | val_loss=2.7322 | val_acc=0.5781
2025-10-13 22:43:40,056 - INFO - _models.training_function_executor - Epoch 024/051 | train_loss=5.8878 | val_loss=2.6772 | val_acc=0.5781
2025-10-13 22:43:40,060 - INFO - _models.training_function_executor - Epoch 025/051 | train_loss=5.7648 | val_loss=2.6273 | val_acc=0.5938
2025-10-13 22:43:40,064 - INFO - _models.training_function_executor - Epoch 026/051 | train_loss=5.8026 | val_loss=2.5786 | val_acc=0.6094
2025-10-13 22:43:40,067 - INFO - _models.training_function_executor - Epoch 027/051 | train_loss=5.3258 | val_loss=2.5362 | val_acc=0.6406
2025-10-13 22:43:40,072 - INFO - _models.training_function_executor - Epoch 028/051 | train_loss=5.4722 | val_loss=2.4993 | val_acc=0.6406
2025-10-13 22:43:40,075 - INFO - _models.training_function_executor - Epoch 029/051 | train_loss=6.2173 | val_loss=2.4656 | val_acc=0.6562
2025-10-13 22:43:40,079 - INFO - _models.training_function_executor - Epoch 030/051 | train_loss=5.3399 | val_loss=2.4360 | val_acc=0.6719
2025-10-13 22:43:40,082 - INFO - _models.training_function_executor - Epoch 031/051 | train_loss=5.7249 | val_loss=2.4086 | val_acc=0.6719
2025-10-13 22:43:40,086 - INFO - _models.training_function_executor - Epoch 032/051 | train_loss=5.2616 | val_loss=2.3829 | val_acc=0.6875
2025-10-13 22:43:40,089 - INFO - _models.training_function_executor - Epoch 033/051 | train_loss=5.8116 | val_loss=2.3636 | val_acc=0.6719
2025-10-13 22:43:40,092 - INFO - _models.training_function_executor - Epoch 034/051 | train_loss=5.8166 | val_loss=2.3467 | val_acc=0.6719
2025-10-13 22:43:40,096 - INFO - _models.training_function_executor - Epoch 035/051 | train_loss=6.0350 | val_loss=2.3318 | val_acc=0.6719
2025-10-13 22:43:40,099 - INFO - _models.training_function_executor - Epoch 036/051 | train_loss=5.5723 | val_loss=2.3182 | val_acc=0.6562
2025-10-13 22:43:40,102 - INFO - _models.training_function_executor - Epoch 037/051 | train_loss=5.0200 | val_loss=2.3084 | val_acc=0.6562
2025-10-13 22:43:40,105 - INFO - _models.training_function_executor - Epoch 038/051 | train_loss=5.6015 | val_loss=2.2984 | val_acc=0.6562
2025-10-13 22:43:40,108 - INFO - _models.training_function_executor - Epoch 039/051 | train_loss=5.4429 | val_loss=2.2880 | val_acc=0.6562
2025-10-13 22:43:40,112 - INFO - _models.training_function_executor - Epoch 040/051 | train_loss=5.7316 | val_loss=2.2820 | val_acc=0.6562
2025-10-13 22:43:40,115 - INFO - _models.training_function_executor - Epoch 041/051 | train_loss=5.4539 | val_loss=2.2740 | val_acc=0.6562
2025-10-13 22:43:40,118 - INFO - _models.training_function_executor - Epoch 042/051 | train_loss=5.5275 | val_loss=2.2690 | val_acc=0.6562
2025-10-13 22:43:40,121 - INFO - _models.training_function_executor - Epoch 043/051 | train_loss=5.6763 | val_loss=2.2653 | val_acc=0.6406
2025-10-13 22:43:40,125 - INFO - _models.training_function_executor - Epoch 044/051 | train_loss=5.5679 | val_loss=2.2617 | val_acc=0.6406
2025-10-13 22:43:40,128 - INFO - _models.training_function_executor - Epoch 045/051 | train_loss=6.4013 | val_loss=2.2591 | val_acc=0.6406
2025-10-13 22:43:40,131 - INFO - _models.training_function_executor - Epoch 046/051 | train_loss=5.4961 | val_loss=2.2579 | val_acc=0.6562
2025-10-13 22:43:40,134 - INFO - _models.training_function_executor - Epoch 047/051 | train_loss=5.9450 | val_loss=2.2568 | val_acc=0.6562
2025-10-13 22:43:40,138 - INFO - _models.training_function_executor - Epoch 048/051 | train_loss=4.9516 | val_loss=2.2561 | val_acc=0.6562
2025-10-13 22:43:40,141 - INFO - _models.training_function_executor - Epoch 049/051 | train_loss=5.5291 | val_loss=2.2557 | val_acc=0.6562
2025-10-13 22:43:40,144 - INFO - _models.training_function_executor - Epoch 050/051 | train_loss=5.4276 | val_loss=2.2555 | val_acc=0.6562
2025-10-13 22:43:40,147 - INFO - _models.training_function_executor - Epoch 051/051 | train_loss=4.9715 | val_loss=2.2555 | val_acc=0.6562
2025-10-13 22:43:40,989 - INFO - _models.training_function_executor - Model: 12,193 parameters, 26.2KB storage
2025-10-13 22:43:40,989 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [7.516942024230957, 7.45308256149292, 6.873928070068359, 7.109570026397705, 7.156970500946045, 7.486509323120117, 7.487209320068359, 6.5352253913879395, 7.104647159576416, 7.497053146362305, 6.754150867462158, 6.508514404296875, 6.795193672180176, 6.615933895111084, 6.491166591644287, 6.568685054779053, 7.03264856338501, 6.1814422607421875, 6.665194034576416, 5.904020309448242, 5.352468967437744, 6.399537563323975, 6.1847124099731445, 5.8877854347229, 5.764759063720703, 5.802610874176025, 5.325832843780518, 5.4722089767456055, 6.217288017272949, 5.33986234664917, 5.724857330322266, 5.261620044708252, 5.811596870422363, 5.81662130355835, 6.035019874572754, 5.572261810302734, 5.020005226135254, 5.601504802703857, 5.442934513092041, 5.731608867645264, 5.453873157501221, 5.527493000030518, 5.676324844360352, 5.5679216384887695, 6.401336193084717, 5.49609375, 5.945045471191406, 4.951590538024902, 5.529115676879883, 5.427615165710449, 4.97150993347168], 'val_losses': [4.716606616973877, 4.716606616973877, 4.716606616973877, 4.716606616973877, 4.716606616973877, 4.565759181976318, 4.416184902191162, 4.272195816040039, 4.127737998962402, 3.993176221847534, 3.86047625541687, 3.7346506118774414, 3.6151928901672363, 3.4984118938446045, 3.392261028289795, 3.287449598312378, 3.1890907287597656, 3.1017870903015137, 3.012596368789673, 2.9346745014190674, 2.863499641418457, 2.795144557952881, 2.7322373390197754, 2.6772422790527344, 2.6273012161254883, 2.5785958766937256, 2.5362346172332764, 2.499251127243042, 2.465590715408325, 2.436038017272949, 2.4086356163024902, 2.3829426765441895, 2.36360502243042, 2.346743583679199, 2.331827163696289, 2.3181722164154053, 2.308420181274414, 2.2983641624450684, 2.2880358695983887, 2.281996250152588, 2.274007797241211, 2.268953800201416, 2.265291690826416, 2.261715888977051, 2.2590627670288086, 2.257922410964966, 2.25677752494812, 2.2560505867004395, 2.2557291984558105, 2.2554850578308105, 2.2554705142974854], 'val_acc': [0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.203125, 0.25, 0.296875, 0.3125, 0.328125, 0.34375, 0.359375, 0.390625, 0.390625, 0.453125, 0.484375, 0.546875, 0.5625, 0.578125, 0.578125, 0.578125, 0.59375, 0.609375, 0.640625, 0.640625, 0.65625, 0.671875, 0.671875, 0.6875, 0.671875, 0.671875, 0.671875, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.640625, 0.640625, 0.640625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.8894106650720734e-05, 'batch_size': 256, 'epochs': 51, 'hidden_size': 106, 'dropout': 0.2990190378340153, 'weight_decay': 0.0005204260949749173, 'label_smoothing': 0.016633672294433913, 'grad_clip_norm': 1.1438704993054576, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 12193, 'model_storage_size_kb': 26.195898437500002, 'model_size_validation': 'PASS'}
2025-10-13 22:43:40,989 - INFO - _models.training_function_executor - BO Objective: base=0.6562, size_penalty=0.0000, final=0.6562
2025-10-13 22:43:40,989 - INFO - _models.training_function_executor - Model: 12,193 parameters, 26.2KB (PASS 256KB limit)
2025-10-13 22:43:40,989 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.728s
2025-10-13 22:43:41,071 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6562
2025-10-13 22:43:41,071 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.081s
2025-10-13 22:43:41,071 - INFO - bo.run_bo - Recorded observation #36: hparams={'lr': 2.8894106650720734e-05, 'batch_size': np.int64(256), 'epochs': np.int64(51), 'hidden_size': np.int64(106), 'dropout': 0.2990190378340153, 'weight_decay': 0.0005204260949749173, 'label_smoothing': 0.016633672294433913, 'grad_clip_norm': 1.1438704993054576, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.6562
2025-10-13 22:43:41,071 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 36: {'lr': 2.8894106650720734e-05, 'batch_size': np.int64(256), 'epochs': np.int64(51), 'hidden_size': np.int64(106), 'dropout': 0.2990190378340153, 'weight_decay': 0.0005204260949749173, 'label_smoothing': 0.016633672294433913, 'grad_clip_norm': 1.1438704993054576, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.6562
2025-10-13 22:43:41,072 - INFO - bo.run_bo - üîçBO Trial 37: Using RF surrogate + Expected Improvement
2025-10-13 22:43:41,072 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:41,072 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 37 (NaN monitoring active)
2025-10-13 22:43:41,072 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:41,072 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:41,072 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0350417771800748e-05, 'batch_size': 192, 'epochs': 172, 'hidden_size': 184, 'dropout': 0.14880085474648416, 'weight_decay': 0.0006443264595097982, 'label_smoothing': 0.15812954022817094, 'grad_clip_norm': 2.760273364525275, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:41,073 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0350417771800748e-05, 'batch_size': 192, 'epochs': 172, 'hidden_size': 184, 'dropout': 0.14880085474648416, 'weight_decay': 0.0006443264595097982, 'label_smoothing': 0.15812954022817094, 'grad_clip_norm': 2.760273364525275, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:44,680 - INFO - _models.training_function_executor - Epoch 001/172 | train_loss=5.7588 | val_loss=5.0405 | val_acc=0.7188
2025-10-13 22:43:44,686 - INFO - _models.training_function_executor - Epoch 002/172 | train_loss=5.1975 | val_loss=5.0405 | val_acc=0.7188
2025-10-13 22:43:44,691 - INFO - _models.training_function_executor - Epoch 003/172 | train_loss=5.4585 | val_loss=4.9697 | val_acc=0.7188
2025-10-13 22:43:44,696 - INFO - _models.training_function_executor - Epoch 004/172 | train_loss=5.0928 | val_loss=4.8316 | val_acc=0.7188
2025-10-13 22:43:44,701 - INFO - _models.training_function_executor - Epoch 005/172 | train_loss=5.2848 | val_loss=4.6934 | val_acc=0.7188
2025-10-13 22:43:44,705 - INFO - _models.training_function_executor - Epoch 006/172 | train_loss=5.3500 | val_loss=4.5571 | val_acc=0.7188
2025-10-13 22:43:44,710 - INFO - _models.training_function_executor - Epoch 007/172 | train_loss=4.8562 | val_loss=4.4212 | val_acc=0.7188
2025-10-13 22:43:44,714 - INFO - _models.training_function_executor - Epoch 008/172 | train_loss=4.8602 | val_loss=4.2834 | val_acc=0.7188
2025-10-13 22:43:44,718 - INFO - _models.training_function_executor - Epoch 009/172 | train_loss=4.4194 | val_loss=4.1517 | val_acc=0.7188
2025-10-13 22:43:44,722 - INFO - _models.training_function_executor - Epoch 010/172 | train_loss=4.7180 | val_loss=4.0169 | val_acc=0.7188
2025-10-13 22:43:44,726 - INFO - _models.training_function_executor - Epoch 011/172 | train_loss=4.4223 | val_loss=3.8830 | val_acc=0.7188
2025-10-13 22:43:44,730 - INFO - _models.training_function_executor - Epoch 012/172 | train_loss=4.5270 | val_loss=3.7519 | val_acc=0.7188
2025-10-13 22:43:44,735 - INFO - _models.training_function_executor - Epoch 013/172 | train_loss=4.1566 | val_loss=3.6199 | val_acc=0.7188
2025-10-13 22:43:44,739 - INFO - _models.training_function_executor - Epoch 014/172 | train_loss=4.0342 | val_loss=3.4875 | val_acc=0.7188
2025-10-13 22:43:44,743 - INFO - _models.training_function_executor - Epoch 015/172 | train_loss=3.9029 | val_loss=3.3591 | val_acc=0.7188
2025-10-13 22:43:44,747 - INFO - _models.training_function_executor - Epoch 016/172 | train_loss=4.1896 | val_loss=3.2274 | val_acc=0.7188
2025-10-13 22:43:44,751 - INFO - _models.training_function_executor - Epoch 017/172 | train_loss=3.7198 | val_loss=3.0983 | val_acc=0.7188
2025-10-13 22:43:44,755 - INFO - _models.training_function_executor - Epoch 018/172 | train_loss=3.7898 | val_loss=2.9712 | val_acc=0.7188
2025-10-13 22:43:44,760 - INFO - _models.training_function_executor - Epoch 019/172 | train_loss=3.5585 | val_loss=2.8458 | val_acc=0.7188
2025-10-13 22:43:44,766 - INFO - _models.training_function_executor - Epoch 020/172 | train_loss=3.8362 | val_loss=2.7242 | val_acc=0.7188
2025-10-13 22:43:44,773 - INFO - _models.training_function_executor - Epoch 021/172 | train_loss=3.4262 | val_loss=2.6072 | val_acc=0.7188
2025-10-13 22:43:44,780 - INFO - _models.training_function_executor - Epoch 022/172 | train_loss=3.4919 | val_loss=2.4896 | val_acc=0.7188
2025-10-13 22:43:44,787 - INFO - _models.training_function_executor - Epoch 023/172 | train_loss=3.3169 | val_loss=2.3743 | val_acc=0.7188
2025-10-13 22:43:44,794 - INFO - _models.training_function_executor - Epoch 024/172 | train_loss=3.7134 | val_loss=2.2676 | val_acc=0.7188
2025-10-13 22:43:44,801 - INFO - _models.training_function_executor - Epoch 025/172 | train_loss=2.8758 | val_loss=2.1650 | val_acc=0.7188
2025-10-13 22:43:44,808 - INFO - _models.training_function_executor - Epoch 026/172 | train_loss=3.4160 | val_loss=2.0713 | val_acc=0.7188
2025-10-13 22:43:44,815 - INFO - _models.training_function_executor - Epoch 027/172 | train_loss=3.4287 | val_loss=1.9826 | val_acc=0.7188
2025-10-13 22:43:44,821 - INFO - _models.training_function_executor - Epoch 028/172 | train_loss=2.8285 | val_loss=1.9034 | val_acc=0.7188
2025-10-13 22:43:44,826 - INFO - _models.training_function_executor - Epoch 029/172 | train_loss=3.3809 | val_loss=1.8272 | val_acc=0.7188
2025-10-13 22:43:44,832 - INFO - _models.training_function_executor - Epoch 030/172 | train_loss=3.5359 | val_loss=1.7586 | val_acc=0.7188
2025-10-13 22:43:44,837 - INFO - _models.training_function_executor - Epoch 031/172 | train_loss=3.0791 | val_loss=1.6951 | val_acc=0.7188
2025-10-13 22:43:44,843 - INFO - _models.training_function_executor - Epoch 032/172 | train_loss=3.4866 | val_loss=1.6334 | val_acc=0.7188
2025-10-13 22:43:44,849 - INFO - _models.training_function_executor - Epoch 033/172 | train_loss=3.3302 | val_loss=1.5817 | val_acc=0.7188
2025-10-13 22:43:44,855 - INFO - _models.training_function_executor - Epoch 034/172 | train_loss=3.1104 | val_loss=1.5307 | val_acc=0.7188
2025-10-13 22:43:44,862 - INFO - _models.training_function_executor - Epoch 035/172 | train_loss=3.4681 | val_loss=1.4970 | val_acc=0.7188
2025-10-13 22:43:44,869 - INFO - _models.training_function_executor - Epoch 036/172 | train_loss=3.4246 | val_loss=1.4711 | val_acc=0.7188
2025-10-13 22:43:44,873 - INFO - _models.training_function_executor - Epoch 037/172 | train_loss=2.8298 | val_loss=1.4535 | val_acc=0.7188
2025-10-13 22:43:44,878 - INFO - _models.training_function_executor - Epoch 038/172 | train_loss=3.5862 | val_loss=1.4365 | val_acc=0.7188
2025-10-13 22:43:44,882 - INFO - _models.training_function_executor - Epoch 039/172 | train_loss=2.9011 | val_loss=1.4247 | val_acc=0.7188
2025-10-13 22:43:44,886 - INFO - _models.training_function_executor - Epoch 040/172 | train_loss=3.0341 | val_loss=1.4173 | val_acc=0.7188
2025-10-13 22:43:44,891 - INFO - _models.training_function_executor - Epoch 041/172 | train_loss=3.4815 | val_loss=1.4147 | val_acc=0.7188
2025-10-13 22:43:44,895 - INFO - _models.training_function_executor - Epoch 042/172 | train_loss=3.3599 | val_loss=1.4092 | val_acc=0.7188
2025-10-13 22:43:44,899 - INFO - _models.training_function_executor - Epoch 043/172 | train_loss=2.8513 | val_loss=1.4057 | val_acc=0.7188
2025-10-13 22:43:44,903 - INFO - _models.training_function_executor - Epoch 044/172 | train_loss=2.9475 | val_loss=1.4007 | val_acc=0.7188
2025-10-13 22:43:44,907 - INFO - _models.training_function_executor - Epoch 045/172 | train_loss=3.5128 | val_loss=1.3906 | val_acc=0.7188
2025-10-13 22:43:44,911 - INFO - _models.training_function_executor - Epoch 046/172 | train_loss=3.0033 | val_loss=1.3829 | val_acc=0.7188
2025-10-13 22:43:44,916 - INFO - _models.training_function_executor - Epoch 047/172 | train_loss=2.9734 | val_loss=1.3792 | val_acc=0.7188
2025-10-13 22:43:44,920 - INFO - _models.training_function_executor - Epoch 048/172 | train_loss=2.7209 | val_loss=1.3822 | val_acc=0.7188
2025-10-13 22:43:44,924 - INFO - _models.training_function_executor - Epoch 049/172 | train_loss=2.6942 | val_loss=1.3862 | val_acc=0.7188
2025-10-13 22:43:44,929 - INFO - _models.training_function_executor - Epoch 050/172 | train_loss=2.8932 | val_loss=1.3865 | val_acc=0.7188
2025-10-13 22:43:44,933 - INFO - _models.training_function_executor - Epoch 051/172 | train_loss=3.1393 | val_loss=1.3912 | val_acc=0.7188
2025-10-13 22:43:44,938 - INFO - _models.training_function_executor - Epoch 052/172 | train_loss=3.0581 | val_loss=1.3941 | val_acc=0.7188
2025-10-13 22:43:44,942 - INFO - _models.training_function_executor - Epoch 053/172 | train_loss=3.2775 | val_loss=1.4003 | val_acc=0.7188
2025-10-13 22:43:44,946 - INFO - _models.training_function_executor - Epoch 054/172 | train_loss=3.2827 | val_loss=1.4057 | val_acc=0.7188
2025-10-13 22:43:44,950 - INFO - _models.training_function_executor - Epoch 055/172 | train_loss=3.1004 | val_loss=1.4088 | val_acc=0.7188
2025-10-13 22:43:44,954 - INFO - _models.training_function_executor - Epoch 056/172 | train_loss=3.1128 | val_loss=1.4051 | val_acc=0.7188
2025-10-13 22:43:44,959 - INFO - _models.training_function_executor - Epoch 057/172 | train_loss=2.8815 | val_loss=1.3989 | val_acc=0.7188
2025-10-13 22:43:44,963 - INFO - _models.training_function_executor - Epoch 058/172 | train_loss=2.9629 | val_loss=1.3922 | val_acc=0.7188
2025-10-13 22:43:44,967 - INFO - _models.training_function_executor - Epoch 059/172 | train_loss=2.9484 | val_loss=1.3843 | val_acc=0.7188
2025-10-13 22:43:44,971 - INFO - _models.training_function_executor - Epoch 060/172 | train_loss=3.1448 | val_loss=1.3816 | val_acc=0.7188
2025-10-13 22:43:44,975 - INFO - _models.training_function_executor - Epoch 061/172 | train_loss=3.0206 | val_loss=1.3772 | val_acc=0.7188
2025-10-13 22:43:44,980 - INFO - _models.training_function_executor - Epoch 062/172 | train_loss=3.2813 | val_loss=1.3691 | val_acc=0.7188
2025-10-13 22:43:44,985 - INFO - _models.training_function_executor - Epoch 063/172 | train_loss=2.9678 | val_loss=1.3622 | val_acc=0.7188
2025-10-13 22:43:44,989 - INFO - _models.training_function_executor - Epoch 064/172 | train_loss=3.2475 | val_loss=1.3537 | val_acc=0.7188
2025-10-13 22:43:44,993 - INFO - _models.training_function_executor - Epoch 065/172 | train_loss=2.8803 | val_loss=1.3448 | val_acc=0.7188
2025-10-13 22:43:44,998 - INFO - _models.training_function_executor - Epoch 066/172 | train_loss=2.9462 | val_loss=1.3429 | val_acc=0.7188
2025-10-13 22:43:45,002 - INFO - _models.training_function_executor - Epoch 067/172 | train_loss=3.0270 | val_loss=1.3372 | val_acc=0.7188
2025-10-13 22:43:45,007 - INFO - _models.training_function_executor - Epoch 068/172 | train_loss=3.0950 | val_loss=1.3282 | val_acc=0.7188
2025-10-13 22:43:45,011 - INFO - _models.training_function_executor - Epoch 069/172 | train_loss=2.6026 | val_loss=1.3241 | val_acc=0.7188
2025-10-13 22:43:45,016 - INFO - _models.training_function_executor - Epoch 070/172 | train_loss=2.9144 | val_loss=1.3175 | val_acc=0.7188
2025-10-13 22:43:45,020 - INFO - _models.training_function_executor - Epoch 071/172 | train_loss=3.0786 | val_loss=1.3082 | val_acc=0.7188
2025-10-13 22:43:45,024 - INFO - _models.training_function_executor - Epoch 072/172 | train_loss=2.8584 | val_loss=1.2993 | val_acc=0.7188
2025-10-13 22:43:45,029 - INFO - _models.training_function_executor - Epoch 073/172 | train_loss=2.9207 | val_loss=1.2919 | val_acc=0.7188
2025-10-13 22:43:45,033 - INFO - _models.training_function_executor - Epoch 074/172 | train_loss=2.8416 | val_loss=1.2805 | val_acc=0.7188
2025-10-13 22:43:45,037 - INFO - _models.training_function_executor - Epoch 075/172 | train_loss=3.0734 | val_loss=1.2756 | val_acc=0.7188
2025-10-13 22:43:45,042 - INFO - _models.training_function_executor - Epoch 076/172 | train_loss=2.7104 | val_loss=1.2753 | val_acc=0.7188
2025-10-13 22:43:45,046 - INFO - _models.training_function_executor - Epoch 077/172 | train_loss=2.7447 | val_loss=1.2725 | val_acc=0.7188
2025-10-13 22:43:45,050 - INFO - _models.training_function_executor - Epoch 078/172 | train_loss=3.1109 | val_loss=1.2688 | val_acc=0.7188
2025-10-13 22:43:45,054 - INFO - _models.training_function_executor - Epoch 079/172 | train_loss=3.2437 | val_loss=1.2667 | val_acc=0.7188
2025-10-13 22:43:45,058 - INFO - _models.training_function_executor - Epoch 080/172 | train_loss=2.6001 | val_loss=1.2624 | val_acc=0.7188
2025-10-13 22:43:45,062 - INFO - _models.training_function_executor - Epoch 081/172 | train_loss=2.8129 | val_loss=1.2619 | val_acc=0.7188
2025-10-13 22:43:45,066 - INFO - _models.training_function_executor - Epoch 082/172 | train_loss=2.9535 | val_loss=1.2643 | val_acc=0.7188
2025-10-13 22:43:45,070 - INFO - _models.training_function_executor - Epoch 083/172 | train_loss=2.8273 | val_loss=1.2664 | val_acc=0.7188
2025-10-13 22:43:45,074 - INFO - _models.training_function_executor - Epoch 084/172 | train_loss=2.7883 | val_loss=1.2695 | val_acc=0.7188
2025-10-13 22:43:45,078 - INFO - _models.training_function_executor - Epoch 085/172 | train_loss=2.8911 | val_loss=1.2694 | val_acc=0.7188
2025-10-13 22:43:45,082 - INFO - _models.training_function_executor - Epoch 086/172 | train_loss=3.3572 | val_loss=1.2715 | val_acc=0.7188
2025-10-13 22:43:45,087 - INFO - _models.training_function_executor - Epoch 087/172 | train_loss=2.9705 | val_loss=1.2773 | val_acc=0.7188
2025-10-13 22:43:45,091 - INFO - _models.training_function_executor - Epoch 088/172 | train_loss=3.0501 | val_loss=1.2825 | val_acc=0.7188
2025-10-13 22:43:45,095 - INFO - _models.training_function_executor - Epoch 089/172 | train_loss=2.7734 | val_loss=1.2872 | val_acc=0.7188
2025-10-13 22:43:45,099 - INFO - _models.training_function_executor - Epoch 090/172 | train_loss=2.9828 | val_loss=1.2950 | val_acc=0.7188
2025-10-13 22:43:45,104 - INFO - _models.training_function_executor - Epoch 091/172 | train_loss=2.9271 | val_loss=1.2981 | val_acc=0.7188
2025-10-13 22:43:45,108 - INFO - _models.training_function_executor - Epoch 092/172 | train_loss=2.9654 | val_loss=1.2988 | val_acc=0.7188
2025-10-13 22:43:45,112 - INFO - _models.training_function_executor - Epoch 093/172 | train_loss=2.8819 | val_loss=1.2987 | val_acc=0.7188
2025-10-13 22:43:45,116 - INFO - _models.training_function_executor - Epoch 094/172 | train_loss=2.8565 | val_loss=1.2961 | val_acc=0.7188
2025-10-13 22:43:45,120 - INFO - _models.training_function_executor - Epoch 095/172 | train_loss=2.9198 | val_loss=1.2905 | val_acc=0.7188
2025-10-13 22:43:45,124 - INFO - _models.training_function_executor - Epoch 096/172 | train_loss=2.9068 | val_loss=1.2848 | val_acc=0.7188
2025-10-13 22:43:45,129 - INFO - _models.training_function_executor - Epoch 097/172 | train_loss=2.8924 | val_loss=1.2828 | val_acc=0.7188
2025-10-13 22:43:45,133 - INFO - _models.training_function_executor - Epoch 098/172 | train_loss=3.0829 | val_loss=1.2809 | val_acc=0.7188
2025-10-13 22:43:45,137 - INFO - _models.training_function_executor - Epoch 099/172 | train_loss=2.9755 | val_loss=1.2802 | val_acc=0.7188
2025-10-13 22:43:45,141 - INFO - _models.training_function_executor - Epoch 100/172 | train_loss=2.9215 | val_loss=1.2762 | val_acc=0.7188
2025-10-13 22:43:45,145 - INFO - _models.training_function_executor - Epoch 101/172 | train_loss=2.7199 | val_loss=1.2694 | val_acc=0.7188
2025-10-13 22:43:45,149 - INFO - _models.training_function_executor - Epoch 102/172 | train_loss=2.8744 | val_loss=1.2629 | val_acc=0.7188
2025-10-13 22:43:45,154 - INFO - _models.training_function_executor - Epoch 103/172 | train_loss=3.0920 | val_loss=1.2591 | val_acc=0.7188
2025-10-13 22:43:45,158 - INFO - _models.training_function_executor - Epoch 104/172 | train_loss=2.6344 | val_loss=1.2556 | val_acc=0.7188
2025-10-13 22:43:45,162 - INFO - _models.training_function_executor - Epoch 105/172 | train_loss=3.1467 | val_loss=1.2531 | val_acc=0.7188
2025-10-13 22:43:45,166 - INFO - _models.training_function_executor - Epoch 106/172 | train_loss=3.3299 | val_loss=1.2517 | val_acc=0.7188
2025-10-13 22:43:45,170 - INFO - _models.training_function_executor - Epoch 107/172 | train_loss=2.6985 | val_loss=1.2493 | val_acc=0.7188
2025-10-13 22:43:45,175 - INFO - _models.training_function_executor - Epoch 108/172 | train_loss=2.9354 | val_loss=1.2477 | val_acc=0.7188
2025-10-13 22:43:45,179 - INFO - _models.training_function_executor - Epoch 109/172 | train_loss=2.9608 | val_loss=1.2475 | val_acc=0.7188
2025-10-13 22:43:45,183 - INFO - _models.training_function_executor - Epoch 110/172 | train_loss=2.7471 | val_loss=1.2499 | val_acc=0.7188
2025-10-13 22:43:45,188 - INFO - _models.training_function_executor - Epoch 111/172 | train_loss=2.6604 | val_loss=1.2532 | val_acc=0.7188
2025-10-13 22:43:45,192 - INFO - _models.training_function_executor - Epoch 112/172 | train_loss=3.1145 | val_loss=1.2544 | val_acc=0.7188
2025-10-13 22:43:45,196 - INFO - _models.training_function_executor - Epoch 113/172 | train_loss=3.1524 | val_loss=1.2582 | val_acc=0.7188
2025-10-13 22:43:45,201 - INFO - _models.training_function_executor - Epoch 114/172 | train_loss=2.6572 | val_loss=1.2624 | val_acc=0.7188
2025-10-13 22:43:45,206 - INFO - _models.training_function_executor - Epoch 115/172 | train_loss=2.6762 | val_loss=1.2638 | val_acc=0.7188
2025-10-13 22:43:45,210 - INFO - _models.training_function_executor - Epoch 116/172 | train_loss=2.8400 | val_loss=1.2658 | val_acc=0.7188
2025-10-13 22:43:45,214 - INFO - _models.training_function_executor - Epoch 117/172 | train_loss=2.8445 | val_loss=1.2657 | val_acc=0.7188
2025-10-13 22:43:45,219 - INFO - _models.training_function_executor - Epoch 118/172 | train_loss=3.1574 | val_loss=1.2640 | val_acc=0.7188
2025-10-13 22:43:45,224 - INFO - _models.training_function_executor - Epoch 119/172 | train_loss=2.8392 | val_loss=1.2642 | val_acc=0.7188
2025-10-13 22:43:45,228 - INFO - _models.training_function_executor - Epoch 120/172 | train_loss=2.7142 | val_loss=1.2659 | val_acc=0.7188
2025-10-13 22:43:45,232 - INFO - _models.training_function_executor - Epoch 121/172 | train_loss=2.4740 | val_loss=1.2638 | val_acc=0.7188
2025-10-13 22:43:45,237 - INFO - _models.training_function_executor - Epoch 122/172 | train_loss=2.7898 | val_loss=1.2618 | val_acc=0.7188
2025-10-13 22:43:45,241 - INFO - _models.training_function_executor - Epoch 123/172 | train_loss=3.0466 | val_loss=1.2581 | val_acc=0.7188
2025-10-13 22:43:45,245 - INFO - _models.training_function_executor - Epoch 124/172 | train_loss=2.7679 | val_loss=1.2550 | val_acc=0.7188
2025-10-13 22:43:45,249 - INFO - _models.training_function_executor - Epoch 125/172 | train_loss=2.7582 | val_loss=1.2536 | val_acc=0.7188
2025-10-13 22:43:45,254 - INFO - _models.training_function_executor - Epoch 126/172 | train_loss=2.6724 | val_loss=1.2535 | val_acc=0.7188
2025-10-13 22:43:45,258 - INFO - _models.training_function_executor - Epoch 127/172 | train_loss=3.0964 | val_loss=1.2522 | val_acc=0.7188
2025-10-13 22:43:45,263 - INFO - _models.training_function_executor - Epoch 128/172 | train_loss=2.8190 | val_loss=1.2511 | val_acc=0.7188
2025-10-13 22:43:45,268 - INFO - _models.training_function_executor - Epoch 129/172 | train_loss=2.9383 | val_loss=1.2500 | val_acc=0.7188
2025-10-13 22:43:45,273 - INFO - _models.training_function_executor - Epoch 130/172 | train_loss=2.7159 | val_loss=1.2486 | val_acc=0.7188
2025-10-13 22:43:45,277 - INFO - _models.training_function_executor - Epoch 131/172 | train_loss=2.5042 | val_loss=1.2466 | val_acc=0.7188
2025-10-13 22:43:45,281 - INFO - _models.training_function_executor - Epoch 132/172 | train_loss=2.9541 | val_loss=1.2453 | val_acc=0.7188
2025-10-13 22:43:45,287 - INFO - _models.training_function_executor - Epoch 133/172 | train_loss=3.0701 | val_loss=1.2440 | val_acc=0.7188
2025-10-13 22:43:45,292 - INFO - _models.training_function_executor - Epoch 134/172 | train_loss=2.8932 | val_loss=1.2435 | val_acc=0.7188
2025-10-13 22:43:45,296 - INFO - _models.training_function_executor - Epoch 135/172 | train_loss=2.9812 | val_loss=1.2416 | val_acc=0.7188
2025-10-13 22:43:45,301 - INFO - _models.training_function_executor - Epoch 136/172 | train_loss=2.9993 | val_loss=1.2403 | val_acc=0.7188
2025-10-13 22:43:45,305 - INFO - _models.training_function_executor - Epoch 137/172 | train_loss=2.6940 | val_loss=1.2388 | val_acc=0.7188
2025-10-13 22:43:45,309 - INFO - _models.training_function_executor - Epoch 138/172 | train_loss=2.6108 | val_loss=1.2373 | val_acc=0.7188
2025-10-13 22:43:45,313 - INFO - _models.training_function_executor - Epoch 139/172 | train_loss=2.7302 | val_loss=1.2364 | val_acc=0.7188
2025-10-13 22:43:45,318 - INFO - _models.training_function_executor - Epoch 140/172 | train_loss=3.2135 | val_loss=1.2361 | val_acc=0.7188
2025-10-13 22:43:45,322 - INFO - _models.training_function_executor - Epoch 141/172 | train_loss=2.7094 | val_loss=1.2353 | val_acc=0.7188
2025-10-13 22:43:45,326 - INFO - _models.training_function_executor - Epoch 142/172 | train_loss=2.7881 | val_loss=1.2341 | val_acc=0.7188
2025-10-13 22:43:45,330 - INFO - _models.training_function_executor - Epoch 143/172 | train_loss=2.8284 | val_loss=1.2327 | val_acc=0.7188
2025-10-13 22:43:45,334 - INFO - _models.training_function_executor - Epoch 144/172 | train_loss=3.0289 | val_loss=1.2318 | val_acc=0.7188
2025-10-13 22:43:45,339 - INFO - _models.training_function_executor - Epoch 145/172 | train_loss=2.7912 | val_loss=1.2303 | val_acc=0.7188
2025-10-13 22:43:45,343 - INFO - _models.training_function_executor - Epoch 146/172 | train_loss=2.7017 | val_loss=1.2299 | val_acc=0.7188
2025-10-13 22:43:45,347 - INFO - _models.training_function_executor - Epoch 147/172 | train_loss=2.7229 | val_loss=1.2291 | val_acc=0.7188
2025-10-13 22:43:45,351 - INFO - _models.training_function_executor - Epoch 148/172 | train_loss=2.7635 | val_loss=1.2290 | val_acc=0.7188
2025-10-13 22:43:45,355 - INFO - _models.training_function_executor - Epoch 149/172 | train_loss=2.6932 | val_loss=1.2283 | val_acc=0.7188
2025-10-13 22:43:45,359 - INFO - _models.training_function_executor - Epoch 150/172 | train_loss=2.7053 | val_loss=1.2278 | val_acc=0.7188
2025-10-13 22:43:45,363 - INFO - _models.training_function_executor - Epoch 151/172 | train_loss=2.9514 | val_loss=1.2276 | val_acc=0.7188
2025-10-13 22:43:45,367 - INFO - _models.training_function_executor - Epoch 152/172 | train_loss=2.8830 | val_loss=1.2273 | val_acc=0.7188
2025-10-13 22:43:45,372 - INFO - _models.training_function_executor - Epoch 153/172 | train_loss=2.9148 | val_loss=1.2270 | val_acc=0.7188
2025-10-13 22:43:45,376 - INFO - _models.training_function_executor - Epoch 154/172 | train_loss=2.8532 | val_loss=1.2266 | val_acc=0.7188
2025-10-13 22:43:45,380 - INFO - _models.training_function_executor - Epoch 155/172 | train_loss=2.5359 | val_loss=1.2265 | val_acc=0.7188
2025-10-13 22:43:45,384 - INFO - _models.training_function_executor - Epoch 156/172 | train_loss=3.0791 | val_loss=1.2266 | val_acc=0.7188
2025-10-13 22:43:45,389 - INFO - _models.training_function_executor - Epoch 157/172 | train_loss=2.8840 | val_loss=1.2264 | val_acc=0.7188
2025-10-13 22:43:45,393 - INFO - _models.training_function_executor - Epoch 158/172 | train_loss=2.8016 | val_loss=1.2262 | val_acc=0.7188
2025-10-13 22:43:45,397 - INFO - _models.training_function_executor - Epoch 159/172 | train_loss=2.5787 | val_loss=1.2259 | val_acc=0.7188
2025-10-13 22:43:45,402 - INFO - _models.training_function_executor - Epoch 160/172 | train_loss=3.0676 | val_loss=1.2260 | val_acc=0.7188
2025-10-13 22:43:45,406 - INFO - _models.training_function_executor - Epoch 161/172 | train_loss=2.9564 | val_loss=1.2254 | val_acc=0.7188
2025-10-13 22:43:45,410 - INFO - _models.training_function_executor - Epoch 162/172 | train_loss=2.6468 | val_loss=1.2252 | val_acc=0.7188
2025-10-13 22:43:45,414 - INFO - _models.training_function_executor - Epoch 163/172 | train_loss=3.1606 | val_loss=1.2252 | val_acc=0.7188
2025-10-13 22:43:45,418 - INFO - _models.training_function_executor - Epoch 164/172 | train_loss=3.1092 | val_loss=1.2249 | val_acc=0.7188
2025-10-13 22:43:45,423 - INFO - _models.training_function_executor - Epoch 165/172 | train_loss=2.8856 | val_loss=1.2250 | val_acc=0.7188
2025-10-13 22:43:45,427 - INFO - _models.training_function_executor - Epoch 166/172 | train_loss=2.6694 | val_loss=1.2252 | val_acc=0.7188
2025-10-13 22:43:45,431 - INFO - _models.training_function_executor - Epoch 167/172 | train_loss=2.5121 | val_loss=1.2252 | val_acc=0.7188
2025-10-13 22:43:45,435 - INFO - _models.training_function_executor - Epoch 168/172 | train_loss=2.6769 | val_loss=1.2251 | val_acc=0.7188
2025-10-13 22:43:45,439 - INFO - _models.training_function_executor - Epoch 169/172 | train_loss=3.1168 | val_loss=1.2251 | val_acc=0.7188
2025-10-13 22:43:45,443 - INFO - _models.training_function_executor - Epoch 170/172 | train_loss=2.3751 | val_loss=1.2251 | val_acc=0.7188
2025-10-13 22:43:45,448 - INFO - _models.training_function_executor - Epoch 171/172 | train_loss=2.8340 | val_loss=1.2251 | val_acc=0.7188
2025-10-13 22:43:45,452 - INFO - _models.training_function_executor - Epoch 172/172 | train_loss=3.0140 | val_loss=1.2251 | val_acc=0.7188
2025-10-13 22:43:46,313 - INFO - _models.training_function_executor - Model: 35,515 parameters, 152.6KB storage
2025-10-13 22:43:46,313 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [5.7587971687316895, 5.197451710700989, 5.458479285240173, 5.0928425788879395, 5.284788608551025, 5.349989891052246, 4.856184005737305, 4.860244631767273, 4.419402718544006, 4.7179940938949585, 4.422301769256592, 4.526955842971802, 4.156560063362122, 4.0342190861701965, 3.9029407501220703, 4.18959641456604, 3.7198450565338135, 3.7897695302963257, 3.558549702167511, 3.836197555065155, 3.426189601421356, 3.491937518119812, 3.3169407844543457, 3.713366746902466, 2.8758087158203125, 3.416016638278961, 3.4287058115005493, 2.828470826148987, 3.380878508090973, 3.5358978509902954, 3.0791210532188416, 3.4865522980690002, 3.330190658569336, 3.1103861927986145, 3.4680806398391724, 3.4245850443840027, 2.8297556042671204, 3.586223542690277, 2.9011294841766357, 3.0340580344200134, 3.4814801812171936, 3.35989773273468, 2.851273834705353, 2.947527766227722, 3.512752890586853, 3.0032825469970703, 2.9733968377113342, 2.7208844423294067, 2.6942041516304016, 2.8932433128356934, 3.139333486557007, 3.0580620169639587, 3.2774765491485596, 3.2826889157295227, 3.1003841757774353, 3.1127761006355286, 2.8814884424209595, 2.9629247188568115, 2.948383927345276, 3.1448370218276978, 3.02062326669693, 3.281333088874817, 2.967784106731415, 3.247538387775421, 2.8802900910377502, 2.946187436580658, 3.0270230770111084, 3.0949581265449524, 2.6026411056518555, 2.914427399635315, 3.078551411628723, 2.858362078666687, 2.9206631779670715, 2.841611921787262, 3.073353946208954, 2.7104121446609497, 2.7447072863578796, 3.110945761203766, 3.2436811923980713, 2.6001245975494385, 2.8128527998924255, 2.953535556793213, 2.8273488879203796, 2.788285970687866, 2.8910659551620483, 3.3571550846099854, 2.9705212116241455, 3.050096809864044, 2.773426353931427, 2.9827980399131775, 2.9271093606948853, 2.9654191732406616, 2.881883203983307, 2.856479048728943, 2.919821709394455, 2.906774163246155, 2.8923763632774353, 3.082946240901947, 2.97548770904541, 2.9215484261512756, 2.7198621034622192, 2.8743732571601868, 3.0919569730758667, 2.634420394897461, 3.146677315235138, 3.329941749572754, 2.6985270977020264, 2.9353774785995483, 2.960802912712097, 2.7470845580101013, 2.6603567004203796, 3.114456534385681, 3.1524307131767273, 2.657185971736908, 2.6762266159057617, 2.8399683833122253, 2.8445103764533997, 3.157388150691986, 2.8391613364219666, 2.714193046092987, 2.4740272760391235, 2.789755642414093, 3.04663348197937, 2.7679368257522583, 2.758199155330658, 2.67244291305542, 3.0963857769966125, 2.8189911246299744, 2.938305675983429, 2.7158634662628174, 2.504227042198181, 2.954061269760132, 3.0700501203536987, 2.893154561519623, 2.981195390224457, 2.9993160367012024, 2.693980038166046, 2.6108234524726868, 2.7301695942878723, 3.213466763496399, 2.709444582462311, 2.788125991821289, 2.828409194946289, 3.0288580656051636, 2.79121196269989, 2.701735496520996, 2.7229193449020386, 2.7634968757629395, 2.6931674480438232, 2.705271363258362, 2.951412260532379, 2.88297700881958, 2.9147629141807556, 2.8532098531723022, 2.535858452320099, 3.079081416130066, 2.8839577436447144, 2.8015551567077637, 2.5786746740341187, 3.0676416158676147, 2.956448197364807, 2.6467514634132385, 3.160622715950012, 3.1091724038124084, 2.885645806789398, 2.6693824529647827, 2.512133777141571, 2.6769368052482605, 3.1167584657669067, 2.3750986456871033, 2.834039628505707, 3.013970375061035], 'val_losses': [5.040525913238525, 5.040525913238525, 4.96973991394043, 4.831603527069092, 4.693397045135498, 4.557080268859863, 4.421153545379639, 4.283435344696045, 4.151727676391602, 4.0169243812561035, 3.882960081100464, 3.7519149780273438, 3.619939088821411, 3.4874587059020996, 3.3590521812438965, 3.2274205684661865, 3.0983097553253174, 2.9711859226226807, 2.845792055130005, 2.7242488861083984, 2.607184648513794, 2.489565372467041, 2.3742918968200684, 2.267594337463379, 2.1650214195251465, 2.0713138580322266, 1.9825761318206787, 1.903414011001587, 1.8272042274475098, 1.7585827112197876, 1.6951342821121216, 1.6333773136138916, 1.5817409753799438, 1.5306730270385742, 1.4970159530639648, 1.471062421798706, 1.4534673690795898, 1.4364968538284302, 1.4247326850891113, 1.4173449277877808, 1.4147361516952515, 1.409194827079773, 1.4056591987609863, 1.4006918668746948, 1.390636920928955, 1.382948637008667, 1.379199743270874, 1.382188081741333, 1.3861536979675293, 1.3864952325820923, 1.3912204504013062, 1.394059181213379, 1.4002727270126343, 1.4057157039642334, 1.408823847770691, 1.405089259147644, 1.39888596534729, 1.3921858072280884, 1.3843274116516113, 1.3815958499908447, 1.3772432804107666, 1.3691142797470093, 1.3622028827667236, 1.353716254234314, 1.3447842597961426, 1.3429032564163208, 1.3372199535369873, 1.3281664848327637, 1.3241064548492432, 1.3175164461135864, 1.308213710784912, 1.2993196249008179, 1.2918665409088135, 1.2804949283599854, 1.2756202220916748, 1.2753243446350098, 1.2725415229797363, 1.2687991857528687, 1.2666876316070557, 1.2623512744903564, 1.2618659734725952, 1.2643157243728638, 1.2664034366607666, 1.2695351839065552, 1.2694275379180908, 1.2715317010879517, 1.2772501707077026, 1.282542109489441, 1.2872294187545776, 1.2949528694152832, 1.2981497049331665, 1.2988475561141968, 1.2986619472503662, 1.296120285987854, 1.2905044555664062, 1.284778118133545, 1.2828019857406616, 1.2808856964111328, 1.280249834060669, 1.2762062549591064, 1.2693511247634888, 1.2628506422042847, 1.2590770721435547, 1.2556297779083252, 1.2531344890594482, 1.2516752481460571, 1.2493339776992798, 1.247713565826416, 1.2475481033325195, 1.2498723268508911, 1.2532373666763306, 1.2544143199920654, 1.2581638097763062, 1.2623910903930664, 1.2637887001037598, 1.2657794952392578, 1.2656599283218384, 1.2639509439468384, 1.26417875289917, 1.2659204006195068, 1.2637649774551392, 1.2618062496185303, 1.2581405639648438, 1.2549607753753662, 1.2536275386810303, 1.25350022315979, 1.2522215843200684, 1.2510864734649658, 1.2500102519989014, 1.2485744953155518, 1.2466413974761963, 1.2452727556228638, 1.2439557313919067, 1.243476152420044, 1.2415828704833984, 1.2403113842010498, 1.2387984991073608, 1.2373236417770386, 1.2364100217819214, 1.2361021041870117, 1.2352969646453857, 1.2341208457946777, 1.232664704322815, 1.231756329536438, 1.2303028106689453, 1.2298780679702759, 1.2290886640548706, 1.2289930582046509, 1.2283251285552979, 1.227823257446289, 1.227603554725647, 1.2272684574127197, 1.227043867111206, 1.2266465425491333, 1.2265273332595825, 1.2265647649765015, 1.226354956626892, 1.2261565923690796, 1.2258808612823486, 1.2259849309921265, 1.2254470586776733, 1.2252445220947266, 1.2251776456832886, 1.22492516040802, 1.2249919176101685, 1.225182294845581, 1.2252001762390137, 1.2251397371292114, 1.2251372337341309, 1.2251372337341309, 1.2251355648040771, 1.2251355648040771], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0350417771800748e-05, 'batch_size': 192, 'epochs': 172, 'hidden_size': 184, 'dropout': 0.14880085474648416, 'weight_decay': 0.0006443264595097982, 'label_smoothing': 0.15812954022817094, 'grad_clip_norm': 2.760273364525275, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 35515, 'model_storage_size_kb': 152.603515625, 'model_size_validation': 'PASS'}
2025-10-13 22:43:46,313 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:43:46,313 - INFO - _models.training_function_executor - Model: 35,515 parameters, 152.6KB (PASS 256KB limit)
2025-10-13 22:43:46,313 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.241s
2025-10-13 22:43:46,394 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:43:46,395 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.081s
2025-10-13 22:43:46,395 - INFO - bo.run_bo - Recorded observation #37: hparams={'lr': 1.0350417771800748e-05, 'batch_size': np.int64(192), 'epochs': np.int64(172), 'hidden_size': np.int64(184), 'dropout': 0.14880085474648416, 'weight_decay': 0.0006443264595097982, 'label_smoothing': 0.15812954022817094, 'grad_clip_norm': 2.760273364525275, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:43:46,395 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 37: {'lr': 1.0350417771800748e-05, 'batch_size': np.int64(192), 'epochs': np.int64(172), 'hidden_size': np.int64(184), 'dropout': 0.14880085474648416, 'weight_decay': 0.0006443264595097982, 'label_smoothing': 0.15812954022817094, 'grad_clip_norm': 2.760273364525275, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:43:46,395 - INFO - bo.run_bo - üîçBO Trial 38: Using RF surrogate + Expected Improvement
2025-10-13 22:43:46,395 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:46,395 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 38 (NaN monitoring active)
2025-10-13 22:43:46,395 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:46,395 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:46,395 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.774602474723194e-05, 'batch_size': 64, 'epochs': 52, 'hidden_size': 135, 'dropout': 0.0021460652618762737, 'weight_decay': 0.0023665303556982494, 'label_smoothing': 0.02805731730432151, 'grad_clip_norm': 4.391727840161417, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:46,396 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.774602474723194e-05, 'batch_size': 64, 'epochs': 52, 'hidden_size': 135, 'dropout': 0.0021460652618762737, 'weight_decay': 0.0023665303556982494, 'label_smoothing': 0.02805731730432151, 'grad_clip_norm': 4.391727840161417, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:50,091 - INFO - _models.training_function_executor - Epoch 001/052 | train_loss=2.5378 | val_loss=2.5999 | val_acc=0.7188
2025-10-13 22:43:50,100 - INFO - _models.training_function_executor - Epoch 002/052 | train_loss=2.5056 | val_loss=2.4276 | val_acc=0.7188
2025-10-13 22:43:50,107 - INFO - _models.training_function_executor - Epoch 003/052 | train_loss=2.3003 | val_loss=2.2557 | val_acc=0.7188
2025-10-13 22:43:50,113 - INFO - _models.training_function_executor - Epoch 004/052 | train_loss=2.1802 | val_loss=2.0867 | val_acc=0.7188
2025-10-13 22:43:50,120 - INFO - _models.training_function_executor - Epoch 005/052 | train_loss=2.0317 | val_loss=1.9217 | val_acc=0.7188
2025-10-13 22:43:50,126 - INFO - _models.training_function_executor - Epoch 006/052 | train_loss=1.8430 | val_loss=1.7609 | val_acc=0.7188
2025-10-13 22:43:50,132 - INFO - _models.training_function_executor - Epoch 007/052 | train_loss=1.6314 | val_loss=1.6038 | val_acc=0.7188
2025-10-13 22:43:50,138 - INFO - _models.training_function_executor - Epoch 008/052 | train_loss=1.5147 | val_loss=1.4570 | val_acc=0.7188
2025-10-13 22:43:50,144 - INFO - _models.training_function_executor - Epoch 009/052 | train_loss=1.3475 | val_loss=1.3199 | val_acc=0.7188
2025-10-13 22:43:50,150 - INFO - _models.training_function_executor - Epoch 010/052 | train_loss=1.2573 | val_loss=1.1966 | val_acc=0.7188
2025-10-13 22:43:50,157 - INFO - _models.training_function_executor - Epoch 011/052 | train_loss=1.1076 | val_loss=1.0915 | val_acc=0.7188
2025-10-13 22:43:50,163 - INFO - _models.training_function_executor - Epoch 012/052 | train_loss=1.0773 | val_loss=1.0144 | val_acc=0.7188
2025-10-13 22:43:50,169 - INFO - _models.training_function_executor - Epoch 013/052 | train_loss=0.9925 | val_loss=0.9687 | val_acc=0.7188
2025-10-13 22:43:50,176 - INFO - _models.training_function_executor - Epoch 014/052 | train_loss=0.9671 | val_loss=0.9453 | val_acc=0.7188
2025-10-13 22:43:50,182 - INFO - _models.training_function_executor - Epoch 015/052 | train_loss=0.9357 | val_loss=0.9336 | val_acc=0.7188
2025-10-13 22:43:50,190 - INFO - _models.training_function_executor - Epoch 016/052 | train_loss=0.9050 | val_loss=0.9267 | val_acc=0.7188
2025-10-13 22:43:50,196 - INFO - _models.training_function_executor - Epoch 017/052 | train_loss=0.8859 | val_loss=0.9175 | val_acc=0.7188
2025-10-13 22:43:50,203 - INFO - _models.training_function_executor - Epoch 018/052 | train_loss=0.9145 | val_loss=0.9072 | val_acc=0.7188
2025-10-13 22:43:50,209 - INFO - _models.training_function_executor - Epoch 019/052 | train_loss=0.9000 | val_loss=0.8965 | val_acc=0.7188
2025-10-13 22:43:50,217 - INFO - _models.training_function_executor - Epoch 020/052 | train_loss=0.9079 | val_loss=0.8873 | val_acc=0.7188
2025-10-13 22:43:50,224 - INFO - _models.training_function_executor - Epoch 021/052 | train_loss=0.8974 | val_loss=0.8804 | val_acc=0.7188
2025-10-13 22:43:50,230 - INFO - _models.training_function_executor - Epoch 022/052 | train_loss=0.9256 | val_loss=0.8729 | val_acc=0.7188
2025-10-13 22:43:50,236 - INFO - _models.training_function_executor - Epoch 023/052 | train_loss=0.8870 | val_loss=0.8643 | val_acc=0.7188
2025-10-13 22:43:50,242 - INFO - _models.training_function_executor - Epoch 024/052 | train_loss=0.8892 | val_loss=0.8575 | val_acc=0.7188
2025-10-13 22:43:50,248 - INFO - _models.training_function_executor - Epoch 025/052 | train_loss=0.8696 | val_loss=0.8518 | val_acc=0.7188
2025-10-13 22:43:50,253 - INFO - _models.training_function_executor - Epoch 026/052 | train_loss=0.8423 | val_loss=0.8472 | val_acc=0.7188
2025-10-13 22:43:50,259 - INFO - _models.training_function_executor - Epoch 027/052 | train_loss=0.8137 | val_loss=0.8415 | val_acc=0.7188
2025-10-13 22:43:50,265 - INFO - _models.training_function_executor - Epoch 028/052 | train_loss=0.9159 | val_loss=0.8364 | val_acc=0.7188
2025-10-13 22:43:50,271 - INFO - _models.training_function_executor - Epoch 029/052 | train_loss=0.8323 | val_loss=0.8330 | val_acc=0.7188
2025-10-13 22:43:50,276 - INFO - _models.training_function_executor - Epoch 030/052 | train_loss=0.8405 | val_loss=0.8297 | val_acc=0.7188
2025-10-13 22:43:50,282 - INFO - _models.training_function_executor - Epoch 031/052 | train_loss=0.8565 | val_loss=0.8268 | val_acc=0.7188
2025-10-13 22:43:50,290 - INFO - _models.training_function_executor - Epoch 032/052 | train_loss=0.8388 | val_loss=0.8238 | val_acc=0.7188
2025-10-13 22:43:50,296 - INFO - _models.training_function_executor - Epoch 033/052 | train_loss=0.8294 | val_loss=0.8214 | val_acc=0.7188
2025-10-13 22:43:50,303 - INFO - _models.training_function_executor - Epoch 034/052 | train_loss=0.8299 | val_loss=0.8185 | val_acc=0.7188
2025-10-13 22:43:50,308 - INFO - _models.training_function_executor - Epoch 035/052 | train_loss=0.8516 | val_loss=0.8165 | val_acc=0.7188
2025-10-13 22:43:50,314 - INFO - _models.training_function_executor - Epoch 036/052 | train_loss=0.8220 | val_loss=0.8147 | val_acc=0.7188
2025-10-13 22:43:50,320 - INFO - _models.training_function_executor - Epoch 037/052 | train_loss=0.8587 | val_loss=0.8134 | val_acc=0.7188
2025-10-13 22:43:50,326 - INFO - _models.training_function_executor - Epoch 038/052 | train_loss=0.8397 | val_loss=0.8117 | val_acc=0.7188
2025-10-13 22:43:50,332 - INFO - _models.training_function_executor - Epoch 039/052 | train_loss=0.8403 | val_loss=0.8099 | val_acc=0.7188
2025-10-13 22:43:50,337 - INFO - _models.training_function_executor - Epoch 040/052 | train_loss=0.8236 | val_loss=0.8086 | val_acc=0.7188
2025-10-13 22:43:50,343 - INFO - _models.training_function_executor - Epoch 041/052 | train_loss=0.8982 | val_loss=0.8078 | val_acc=0.7188
2025-10-13 22:43:50,349 - INFO - _models.training_function_executor - Epoch 042/052 | train_loss=0.8344 | val_loss=0.8070 | val_acc=0.7188
2025-10-13 22:43:50,355 - INFO - _models.training_function_executor - Epoch 043/052 | train_loss=0.8096 | val_loss=0.8063 | val_acc=0.7188
2025-10-13 22:43:50,360 - INFO - _models.training_function_executor - Epoch 044/052 | train_loss=0.8363 | val_loss=0.8058 | val_acc=0.7188
2025-10-13 22:43:50,366 - INFO - _models.training_function_executor - Epoch 045/052 | train_loss=0.8304 | val_loss=0.8055 | val_acc=0.7188
2025-10-13 22:43:50,372 - INFO - _models.training_function_executor - Epoch 046/052 | train_loss=0.8216 | val_loss=0.8051 | val_acc=0.7188
2025-10-13 22:43:50,378 - INFO - _models.training_function_executor - Epoch 047/052 | train_loss=0.8237 | val_loss=0.8050 | val_acc=0.7188
2025-10-13 22:43:50,384 - INFO - _models.training_function_executor - Epoch 048/052 | train_loss=0.8107 | val_loss=0.8048 | val_acc=0.7188
2025-10-13 22:43:50,391 - INFO - _models.training_function_executor - Epoch 049/052 | train_loss=0.8308 | val_loss=0.8047 | val_acc=0.7188
2025-10-13 22:43:50,397 - INFO - _models.training_function_executor - Epoch 050/052 | train_loss=0.8218 | val_loss=0.8047 | val_acc=0.7188
2025-10-13 22:43:50,404 - INFO - _models.training_function_executor - Epoch 051/052 | train_loss=0.8435 | val_loss=0.8047 | val_acc=0.7188
2025-10-13 22:43:50,410 - INFO - _models.training_function_executor - Epoch 052/052 | train_loss=0.8222 | val_loss=0.8046 | val_acc=0.7188
2025-10-13 22:43:51,283 - INFO - _models.training_function_executor - Model: 19,443 parameters, 83.5KB storage
2025-10-13 22:43:51,284 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.537825256586075, 2.5055819153785706, 2.300297498703003, 2.180152177810669, 2.0317493677139282, 1.8429526090621948, 1.6314412355422974, 1.5146960616111755, 1.347470760345459, 1.2572984844446182, 1.1076455265283585, 1.0772595256567001, 0.9925491809844971, 0.9671434313058853, 0.9356983006000519, 0.9049708098173141, 0.8859246969223022, 0.9145233482122421, 0.8999508917331696, 0.9078563153743744, 0.8973874002695084, 0.9256249219179153, 0.8870028555393219, 0.889209508895874, 0.8695586770772934, 0.8423240929841995, 0.8137146979570389, 0.9158581048250198, 0.8323000222444534, 0.8405459076166153, 0.8564951419830322, 0.838766798377037, 0.8293927311897278, 0.8298929780721664, 0.8515666127204895, 0.8219616562128067, 0.8587236404418945, 0.8396981060504913, 0.840268924832344, 0.8236336261034012, 0.8981966525316238, 0.8343584686517715, 0.8096273988485336, 0.8363452553749084, 0.8304061889648438, 0.8215562254190445, 0.8236599713563919, 0.8107448816299438, 0.830784946680069, 0.8217630982398987, 0.8434875309467316, 0.8222088068723679], 'val_losses': [2.59991455078125, 2.427598714828491, 2.255725860595703, 2.086656093597412, 1.9216548204421997, 1.7608890533447266, 1.6038330793380737, 1.4570223093032837, 1.3198943138122559, 1.1965593099594116, 1.091493844985962, 1.0143874883651733, 0.9687028527259827, 0.9452939033508301, 0.9336482286453247, 0.9266897439956665, 0.9174798130989075, 0.9072315096855164, 0.8965338468551636, 0.8873000144958496, 0.8803524971008301, 0.8728508353233337, 0.8643004298210144, 0.8575246334075928, 0.8517703413963318, 0.8471866846084595, 0.8414539694786072, 0.8363903760910034, 0.8329822421073914, 0.8297430872917175, 0.8267533779144287, 0.8237685561180115, 0.8213934898376465, 0.8184941411018372, 0.8164603114128113, 0.8147364854812622, 0.8134142160415649, 0.8117153644561768, 0.8099032044410706, 0.8085837364196777, 0.8078380227088928, 0.8070005774497986, 0.8063192367553711, 0.8057924509048462, 0.8055041432380676, 0.8051134943962097, 0.804958164691925, 0.8047952055931091, 0.8047019839286804, 0.8046913146972656, 0.8046672344207764, 0.8046383261680603], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.774602474723194e-05, 'batch_size': 64, 'epochs': 52, 'hidden_size': 135, 'dropout': 0.0021460652618762737, 'weight_decay': 0.0023665303556982494, 'label_smoothing': 0.02805731730432151, 'grad_clip_norm': 4.391727840161417, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 19443, 'model_storage_size_kb': 83.54414062500001, 'model_size_validation': 'PASS'}
2025-10-13 22:43:51,284 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:43:51,284 - INFO - _models.training_function_executor - Model: 19,443 parameters, 83.5KB (PASS 256KB limit)
2025-10-13 22:43:51,284 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.889s
2025-10-13 22:43:51,365 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:43:51,365 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.081s
2025-10-13 22:43:51,365 - INFO - bo.run_bo - Recorded observation #38: hparams={'lr': 1.774602474723194e-05, 'batch_size': np.int64(64), 'epochs': np.int64(52), 'hidden_size': np.int64(135), 'dropout': 0.0021460652618762737, 'weight_decay': 0.0023665303556982494, 'label_smoothing': 0.02805731730432151, 'grad_clip_norm': 4.391727840161417, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:43:51,365 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 38: {'lr': 1.774602474723194e-05, 'batch_size': np.int64(64), 'epochs': np.int64(52), 'hidden_size': np.int64(135), 'dropout': 0.0021460652618762737, 'weight_decay': 0.0023665303556982494, 'label_smoothing': 0.02805731730432151, 'grad_clip_norm': 4.391727840161417, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:43:51,366 - INFO - bo.run_bo - üîçBO Trial 39: Using RF surrogate + Expected Improvement
2025-10-13 22:43:51,366 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:51,366 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 39 (NaN monitoring active)
2025-10-13 22:43:51,366 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:51,366 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:51,366 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0307046445871678e-05, 'batch_size': 192, 'epochs': 176, 'hidden_size': 107, 'dropout': 0.040056482280615764, 'weight_decay': 0.0023063310187989693, 'label_smoothing': 0.0040740698016193296, 'grad_clip_norm': 1.3390324861996743, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:51,367 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0307046445871678e-05, 'batch_size': 192, 'epochs': 176, 'hidden_size': 107, 'dropout': 0.040056482280615764, 'weight_decay': 0.0023063310187989693, 'label_smoothing': 0.0040740698016193296, 'grad_clip_norm': 1.3390324861996743, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:43:54,759 - INFO - _models.training_function_executor - Epoch 001/176 | train_loss=3.2401 | val_loss=2.9508 | val_acc=0.7188
2025-10-13 22:43:54,766 - INFO - _models.training_function_executor - Epoch 002/176 | train_loss=3.1326 | val_loss=2.9508 | val_acc=0.7188
2025-10-13 22:43:54,771 - INFO - _models.training_function_executor - Epoch 003/176 | train_loss=3.0130 | val_loss=2.9065 | val_acc=0.7188
2025-10-13 22:43:54,775 - INFO - _models.training_function_executor - Epoch 004/176 | train_loss=3.0781 | val_loss=2.8640 | val_acc=0.7188
2025-10-13 22:43:54,780 - INFO - _models.training_function_executor - Epoch 005/176 | train_loss=3.0232 | val_loss=2.8167 | val_acc=0.7188
2025-10-13 22:43:54,785 - INFO - _models.training_function_executor - Epoch 006/176 | train_loss=3.1563 | val_loss=2.7729 | val_acc=0.7188
2025-10-13 22:43:54,789 - INFO - _models.training_function_executor - Epoch 007/176 | train_loss=2.8630 | val_loss=2.7312 | val_acc=0.7188
2025-10-13 22:43:54,793 - INFO - _models.training_function_executor - Epoch 008/176 | train_loss=3.0428 | val_loss=2.6839 | val_acc=0.7188
2025-10-13 22:43:54,797 - INFO - _models.training_function_executor - Epoch 009/176 | train_loss=2.9983 | val_loss=2.6411 | val_acc=0.7188
2025-10-13 22:43:54,801 - INFO - _models.training_function_executor - Epoch 010/176 | train_loss=2.8345 | val_loss=2.6010 | val_acc=0.7188
2025-10-13 22:43:54,805 - INFO - _models.training_function_executor - Epoch 011/176 | train_loss=2.8412 | val_loss=2.5555 | val_acc=0.7188
2025-10-13 22:43:54,809 - INFO - _models.training_function_executor - Epoch 012/176 | train_loss=2.8252 | val_loss=2.5128 | val_acc=0.7188
2025-10-13 22:43:54,813 - INFO - _models.training_function_executor - Epoch 013/176 | train_loss=2.9441 | val_loss=2.4724 | val_acc=0.7188
2025-10-13 22:43:54,817 - INFO - _models.training_function_executor - Epoch 014/176 | train_loss=2.9100 | val_loss=2.4291 | val_acc=0.7188
2025-10-13 22:43:54,821 - INFO - _models.training_function_executor - Epoch 015/176 | train_loss=2.7256 | val_loss=2.3886 | val_acc=0.7188
2025-10-13 22:43:54,825 - INFO - _models.training_function_executor - Epoch 016/176 | train_loss=2.8479 | val_loss=2.3518 | val_acc=0.7188
2025-10-13 22:43:54,829 - INFO - _models.training_function_executor - Epoch 017/176 | train_loss=2.7887 | val_loss=2.3085 | val_acc=0.7188
2025-10-13 22:43:54,833 - INFO - _models.training_function_executor - Epoch 018/176 | train_loss=2.8316 | val_loss=2.2706 | val_acc=0.7188
2025-10-13 22:43:54,837 - INFO - _models.training_function_executor - Epoch 019/176 | train_loss=2.6482 | val_loss=2.2336 | val_acc=0.7188
2025-10-13 22:43:54,841 - INFO - _models.training_function_executor - Epoch 020/176 | train_loss=2.6647 | val_loss=2.1943 | val_acc=0.7188
2025-10-13 22:43:54,846 - INFO - _models.training_function_executor - Epoch 021/176 | train_loss=2.6610 | val_loss=2.1575 | val_acc=0.7188
2025-10-13 22:43:54,851 - INFO - _models.training_function_executor - Epoch 022/176 | train_loss=2.7298 | val_loss=2.1232 | val_acc=0.7188
2025-10-13 22:43:54,855 - INFO - _models.training_function_executor - Epoch 023/176 | train_loss=2.5109 | val_loss=2.0864 | val_acc=0.7188
2025-10-13 22:43:54,859 - INFO - _models.training_function_executor - Epoch 024/176 | train_loss=2.4347 | val_loss=2.0506 | val_acc=0.7188
2025-10-13 22:43:54,864 - INFO - _models.training_function_executor - Epoch 025/176 | train_loss=2.5091 | val_loss=2.0183 | val_acc=0.7188
2025-10-13 22:43:54,868 - INFO - _models.training_function_executor - Epoch 026/176 | train_loss=2.5077 | val_loss=1.9857 | val_acc=0.7188
2025-10-13 22:43:54,872 - INFO - _models.training_function_executor - Epoch 027/176 | train_loss=2.5296 | val_loss=1.9535 | val_acc=0.7188
2025-10-13 22:43:54,876 - INFO - _models.training_function_executor - Epoch 028/176 | train_loss=2.3780 | val_loss=1.9254 | val_acc=0.7188
2025-10-13 22:43:54,880 - INFO - _models.training_function_executor - Epoch 029/176 | train_loss=2.1856 | val_loss=1.8961 | val_acc=0.7188
2025-10-13 22:43:54,885 - INFO - _models.training_function_executor - Epoch 030/176 | train_loss=2.5535 | val_loss=1.8671 | val_acc=0.7188
2025-10-13 22:43:54,889 - INFO - _models.training_function_executor - Epoch 031/176 | train_loss=2.1994 | val_loss=1.8395 | val_acc=0.7188
2025-10-13 22:43:54,893 - INFO - _models.training_function_executor - Epoch 032/176 | train_loss=2.2195 | val_loss=1.8141 | val_acc=0.7188
2025-10-13 22:43:54,897 - INFO - _models.training_function_executor - Epoch 033/176 | train_loss=2.5335 | val_loss=1.7886 | val_acc=0.7188
2025-10-13 22:43:54,901 - INFO - _models.training_function_executor - Epoch 034/176 | train_loss=2.3938 | val_loss=1.7659 | val_acc=0.7188
2025-10-13 22:43:54,905 - INFO - _models.training_function_executor - Epoch 035/176 | train_loss=2.2561 | val_loss=1.7452 | val_acc=0.7188
2025-10-13 22:43:54,909 - INFO - _models.training_function_executor - Epoch 036/176 | train_loss=2.3775 | val_loss=1.7259 | val_acc=0.7188
2025-10-13 22:43:54,913 - INFO - _models.training_function_executor - Epoch 037/176 | train_loss=2.2437 | val_loss=1.7062 | val_acc=0.7188
2025-10-13 22:43:54,917 - INFO - _models.training_function_executor - Epoch 038/176 | train_loss=2.2118 | val_loss=1.6871 | val_acc=0.7188
2025-10-13 22:43:54,921 - INFO - _models.training_function_executor - Epoch 039/176 | train_loss=2.2298 | val_loss=1.6717 | val_acc=0.7188
2025-10-13 22:43:54,925 - INFO - _models.training_function_executor - Epoch 040/176 | train_loss=2.0584 | val_loss=1.6529 | val_acc=0.7188
2025-10-13 22:43:54,929 - INFO - _models.training_function_executor - Epoch 041/176 | train_loss=2.2936 | val_loss=1.6352 | val_acc=0.7188
2025-10-13 22:43:54,933 - INFO - _models.training_function_executor - Epoch 042/176 | train_loss=2.2310 | val_loss=1.6171 | val_acc=0.7188
2025-10-13 22:43:54,937 - INFO - _models.training_function_executor - Epoch 043/176 | train_loss=2.0406 | val_loss=1.5998 | val_acc=0.7188
2025-10-13 22:43:54,941 - INFO - _models.training_function_executor - Epoch 044/176 | train_loss=2.0580 | val_loss=1.5826 | val_acc=0.7188
2025-10-13 22:43:54,945 - INFO - _models.training_function_executor - Epoch 045/176 | train_loss=2.2109 | val_loss=1.5647 | val_acc=0.7188
2025-10-13 22:43:54,949 - INFO - _models.training_function_executor - Epoch 046/176 | train_loss=2.1194 | val_loss=1.5463 | val_acc=0.7188
2025-10-13 22:43:54,953 - INFO - _models.training_function_executor - Epoch 047/176 | train_loss=1.9328 | val_loss=1.5277 | val_acc=0.7188
2025-10-13 22:43:54,956 - INFO - _models.training_function_executor - Epoch 048/176 | train_loss=2.1020 | val_loss=1.5106 | val_acc=0.7188
2025-10-13 22:43:54,960 - INFO - _models.training_function_executor - Epoch 049/176 | train_loss=2.0679 | val_loss=1.4936 | val_acc=0.7188
2025-10-13 22:43:54,964 - INFO - _models.training_function_executor - Epoch 050/176 | train_loss=1.9490 | val_loss=1.4778 | val_acc=0.7188
2025-10-13 22:43:54,968 - INFO - _models.training_function_executor - Epoch 051/176 | train_loss=1.9764 | val_loss=1.4613 | val_acc=0.7188
2025-10-13 22:43:54,972 - INFO - _models.training_function_executor - Epoch 052/176 | train_loss=1.8458 | val_loss=1.4444 | val_acc=0.7188
2025-10-13 22:43:54,976 - INFO - _models.training_function_executor - Epoch 053/176 | train_loss=1.9079 | val_loss=1.4296 | val_acc=0.7188
2025-10-13 22:43:54,980 - INFO - _models.training_function_executor - Epoch 054/176 | train_loss=1.7681 | val_loss=1.4143 | val_acc=0.7188
2025-10-13 22:43:54,985 - INFO - _models.training_function_executor - Epoch 055/176 | train_loss=1.8595 | val_loss=1.3983 | val_acc=0.7188
2025-10-13 22:43:54,989 - INFO - _models.training_function_executor - Epoch 056/176 | train_loss=1.8318 | val_loss=1.3831 | val_acc=0.7188
2025-10-13 22:43:54,992 - INFO - _models.training_function_executor - Epoch 057/176 | train_loss=1.9004 | val_loss=1.3676 | val_acc=0.7188
2025-10-13 22:43:54,996 - INFO - _models.training_function_executor - Epoch 058/176 | train_loss=1.8776 | val_loss=1.3523 | val_acc=0.7188
2025-10-13 22:43:55,000 - INFO - _models.training_function_executor - Epoch 059/176 | train_loss=1.9830 | val_loss=1.3363 | val_acc=0.7188
2025-10-13 22:43:55,004 - INFO - _models.training_function_executor - Epoch 060/176 | train_loss=1.9943 | val_loss=1.3225 | val_acc=0.7188
2025-10-13 22:43:55,008 - INFO - _models.training_function_executor - Epoch 061/176 | train_loss=1.7419 | val_loss=1.3096 | val_acc=0.7188
2025-10-13 22:43:55,012 - INFO - _models.training_function_executor - Epoch 062/176 | train_loss=1.7611 | val_loss=1.2960 | val_acc=0.7188
2025-10-13 22:43:55,016 - INFO - _models.training_function_executor - Epoch 063/176 | train_loss=1.8074 | val_loss=1.2851 | val_acc=0.7188
2025-10-13 22:43:55,020 - INFO - _models.training_function_executor - Epoch 064/176 | train_loss=1.9793 | val_loss=1.2729 | val_acc=0.7188
2025-10-13 22:43:55,024 - INFO - _models.training_function_executor - Epoch 065/176 | train_loss=1.6987 | val_loss=1.2615 | val_acc=0.7188
2025-10-13 22:43:55,028 - INFO - _models.training_function_executor - Epoch 066/176 | train_loss=1.8538 | val_loss=1.2478 | val_acc=0.7188
2025-10-13 22:43:55,032 - INFO - _models.training_function_executor - Epoch 067/176 | train_loss=1.8915 | val_loss=1.2361 | val_acc=0.7188
2025-10-13 22:43:55,036 - INFO - _models.training_function_executor - Epoch 068/176 | train_loss=1.8255 | val_loss=1.2247 | val_acc=0.7188
2025-10-13 22:43:55,041 - INFO - _models.training_function_executor - Epoch 069/176 | train_loss=1.6533 | val_loss=1.2128 | val_acc=0.7188
2025-10-13 22:43:55,045 - INFO - _models.training_function_executor - Epoch 070/176 | train_loss=1.8297 | val_loss=1.1993 | val_acc=0.7188
2025-10-13 22:43:55,049 - INFO - _models.training_function_executor - Epoch 071/176 | train_loss=1.7879 | val_loss=1.1860 | val_acc=0.7188
2025-10-13 22:43:55,054 - INFO - _models.training_function_executor - Epoch 072/176 | train_loss=1.6683 | val_loss=1.1749 | val_acc=0.7188
2025-10-13 22:43:55,058 - INFO - _models.training_function_executor - Epoch 073/176 | train_loss=1.6895 | val_loss=1.1638 | val_acc=0.7188
2025-10-13 22:43:55,062 - INFO - _models.training_function_executor - Epoch 074/176 | train_loss=1.6450 | val_loss=1.1530 | val_acc=0.7188
2025-10-13 22:43:55,066 - INFO - _models.training_function_executor - Epoch 075/176 | train_loss=1.8111 | val_loss=1.1414 | val_acc=0.7188
2025-10-13 22:43:55,070 - INFO - _models.training_function_executor - Epoch 076/176 | train_loss=1.6429 | val_loss=1.1309 | val_acc=0.7188
2025-10-13 22:43:55,074 - INFO - _models.training_function_executor - Epoch 077/176 | train_loss=1.6240 | val_loss=1.1201 | val_acc=0.7188
2025-10-13 22:43:55,078 - INFO - _models.training_function_executor - Epoch 078/176 | train_loss=1.8258 | val_loss=1.1094 | val_acc=0.7188
2025-10-13 22:43:55,082 - INFO - _models.training_function_executor - Epoch 079/176 | train_loss=2.0135 | val_loss=1.1000 | val_acc=0.7188
2025-10-13 22:43:55,087 - INFO - _models.training_function_executor - Epoch 080/176 | train_loss=1.6583 | val_loss=1.0908 | val_acc=0.7188
2025-10-13 22:43:55,091 - INFO - _models.training_function_executor - Epoch 081/176 | train_loss=1.5867 | val_loss=1.0822 | val_acc=0.7188
2025-10-13 22:43:55,095 - INFO - _models.training_function_executor - Epoch 082/176 | train_loss=1.6710 | val_loss=1.0720 | val_acc=0.7188
2025-10-13 22:43:55,099 - INFO - _models.training_function_executor - Epoch 083/176 | train_loss=1.6198 | val_loss=1.0627 | val_acc=0.7188
2025-10-13 22:43:55,103 - INFO - _models.training_function_executor - Epoch 084/176 | train_loss=1.5288 | val_loss=1.0538 | val_acc=0.7188
2025-10-13 22:43:55,107 - INFO - _models.training_function_executor - Epoch 085/176 | train_loss=1.6156 | val_loss=1.0449 | val_acc=0.7188
2025-10-13 22:43:55,111 - INFO - _models.training_function_executor - Epoch 086/176 | train_loss=1.5858 | val_loss=1.0364 | val_acc=0.7188
2025-10-13 22:43:55,115 - INFO - _models.training_function_executor - Epoch 087/176 | train_loss=1.5024 | val_loss=1.0281 | val_acc=0.7188
2025-10-13 22:43:55,119 - INFO - _models.training_function_executor - Epoch 088/176 | train_loss=1.6044 | val_loss=1.0198 | val_acc=0.7188
2025-10-13 22:43:55,123 - INFO - _models.training_function_executor - Epoch 089/176 | train_loss=1.5485 | val_loss=1.0130 | val_acc=0.7188
2025-10-13 22:43:55,127 - INFO - _models.training_function_executor - Epoch 090/176 | train_loss=1.7157 | val_loss=1.0054 | val_acc=0.7188
2025-10-13 22:43:55,131 - INFO - _models.training_function_executor - Epoch 091/176 | train_loss=1.6049 | val_loss=0.9986 | val_acc=0.7188
2025-10-13 22:43:55,135 - INFO - _models.training_function_executor - Epoch 092/176 | train_loss=1.3768 | val_loss=0.9908 | val_acc=0.7188
2025-10-13 22:43:55,139 - INFO - _models.training_function_executor - Epoch 093/176 | train_loss=1.3886 | val_loss=0.9821 | val_acc=0.7188
2025-10-13 22:43:55,143 - INFO - _models.training_function_executor - Epoch 094/176 | train_loss=1.3828 | val_loss=0.9743 | val_acc=0.7188
2025-10-13 22:43:55,147 - INFO - _models.training_function_executor - Epoch 095/176 | train_loss=1.4395 | val_loss=0.9663 | val_acc=0.7188
2025-10-13 22:43:55,151 - INFO - _models.training_function_executor - Epoch 096/176 | train_loss=1.6726 | val_loss=0.9578 | val_acc=0.7188
2025-10-13 22:43:55,154 - INFO - _models.training_function_executor - Epoch 097/176 | train_loss=1.2835 | val_loss=0.9497 | val_acc=0.7188
2025-10-13 22:43:55,158 - INFO - _models.training_function_executor - Epoch 098/176 | train_loss=1.5988 | val_loss=0.9424 | val_acc=0.7188
2025-10-13 22:43:55,162 - INFO - _models.training_function_executor - Epoch 099/176 | train_loss=1.6659 | val_loss=0.9356 | val_acc=0.7188
2025-10-13 22:43:55,165 - INFO - _models.training_function_executor - Epoch 100/176 | train_loss=1.5085 | val_loss=0.9287 | val_acc=0.7188
2025-10-13 22:43:55,169 - INFO - _models.training_function_executor - Epoch 101/176 | train_loss=1.5247 | val_loss=0.9231 | val_acc=0.7188
2025-10-13 22:43:55,173 - INFO - _models.training_function_executor - Epoch 102/176 | train_loss=1.6329 | val_loss=0.9173 | val_acc=0.7188
2025-10-13 22:43:55,177 - INFO - _models.training_function_executor - Epoch 103/176 | train_loss=1.5535 | val_loss=0.9116 | val_acc=0.7188
2025-10-13 22:43:55,180 - INFO - _models.training_function_executor - Epoch 104/176 | train_loss=1.5114 | val_loss=0.9063 | val_acc=0.7188
2025-10-13 22:43:55,185 - INFO - _models.training_function_executor - Epoch 105/176 | train_loss=1.5515 | val_loss=0.9011 | val_acc=0.7188
2025-10-13 22:43:55,189 - INFO - _models.training_function_executor - Epoch 106/176 | train_loss=1.4484 | val_loss=0.8963 | val_acc=0.7188
2025-10-13 22:43:55,193 - INFO - _models.training_function_executor - Epoch 107/176 | train_loss=1.5800 | val_loss=0.8928 | val_acc=0.7188
2025-10-13 22:43:55,197 - INFO - _models.training_function_executor - Epoch 108/176 | train_loss=1.5063 | val_loss=0.8900 | val_acc=0.7188
2025-10-13 22:43:55,201 - INFO - _models.training_function_executor - Epoch 109/176 | train_loss=1.4437 | val_loss=0.8871 | val_acc=0.7188
2025-10-13 22:43:55,205 - INFO - _models.training_function_executor - Epoch 110/176 | train_loss=1.5699 | val_loss=0.8833 | val_acc=0.7188
2025-10-13 22:43:55,209 - INFO - _models.training_function_executor - Epoch 111/176 | train_loss=1.4740 | val_loss=0.8806 | val_acc=0.7188
2025-10-13 22:43:55,214 - INFO - _models.training_function_executor - Epoch 112/176 | train_loss=1.5473 | val_loss=0.8774 | val_acc=0.7188
2025-10-13 22:43:55,218 - INFO - _models.training_function_executor - Epoch 113/176 | train_loss=1.3554 | val_loss=0.8746 | val_acc=0.7188
2025-10-13 22:43:55,222 - INFO - _models.training_function_executor - Epoch 114/176 | train_loss=1.6949 | val_loss=0.8721 | val_acc=0.7188
2025-10-13 22:43:55,226 - INFO - _models.training_function_executor - Epoch 115/176 | train_loss=1.3537 | val_loss=0.8682 | val_acc=0.7188
2025-10-13 22:43:55,230 - INFO - _models.training_function_executor - Epoch 116/176 | train_loss=1.5794 | val_loss=0.8656 | val_acc=0.7188
2025-10-13 22:43:55,234 - INFO - _models.training_function_executor - Epoch 117/176 | train_loss=1.5565 | val_loss=0.8636 | val_acc=0.7188
2025-10-13 22:43:55,238 - INFO - _models.training_function_executor - Epoch 118/176 | train_loss=1.5028 | val_loss=0.8606 | val_acc=0.7188
2025-10-13 22:43:55,242 - INFO - _models.training_function_executor - Epoch 119/176 | train_loss=1.3046 | val_loss=0.8572 | val_acc=0.7188
2025-10-13 22:43:55,246 - INFO - _models.training_function_executor - Epoch 120/176 | train_loss=1.6781 | val_loss=0.8546 | val_acc=0.7188
2025-10-13 22:43:55,250 - INFO - _models.training_function_executor - Epoch 121/176 | train_loss=1.5632 | val_loss=0.8528 | val_acc=0.7188
2025-10-13 22:43:55,254 - INFO - _models.training_function_executor - Epoch 122/176 | train_loss=1.5526 | val_loss=0.8512 | val_acc=0.7188
2025-10-13 22:43:55,258 - INFO - _models.training_function_executor - Epoch 123/176 | train_loss=1.3819 | val_loss=0.8497 | val_acc=0.7188
2025-10-13 22:43:55,261 - INFO - _models.training_function_executor - Epoch 124/176 | train_loss=1.4723 | val_loss=0.8486 | val_acc=0.7188
2025-10-13 22:43:55,265 - INFO - _models.training_function_executor - Epoch 125/176 | train_loss=1.4529 | val_loss=0.8478 | val_acc=0.7188
2025-10-13 22:43:55,269 - INFO - _models.training_function_executor - Epoch 126/176 | train_loss=1.4010 | val_loss=0.8472 | val_acc=0.7188
2025-10-13 22:43:55,273 - INFO - _models.training_function_executor - Epoch 127/176 | train_loss=1.6352 | val_loss=0.8468 | val_acc=0.7188
2025-10-13 22:43:55,277 - INFO - _models.training_function_executor - Epoch 128/176 | train_loss=1.5246 | val_loss=0.8451 | val_acc=0.7188
2025-10-13 22:43:55,281 - INFO - _models.training_function_executor - Epoch 129/176 | train_loss=1.2974 | val_loss=0.8441 | val_acc=0.7188
2025-10-13 22:43:55,285 - INFO - _models.training_function_executor - Epoch 130/176 | train_loss=1.5872 | val_loss=0.8428 | val_acc=0.7188
2025-10-13 22:43:55,289 - INFO - _models.training_function_executor - Epoch 131/176 | train_loss=1.4601 | val_loss=0.8425 | val_acc=0.7188
2025-10-13 22:43:55,293 - INFO - _models.training_function_executor - Epoch 132/176 | train_loss=1.6287 | val_loss=0.8417 | val_acc=0.7188
2025-10-13 22:43:55,297 - INFO - _models.training_function_executor - Epoch 133/176 | train_loss=1.6010 | val_loss=0.8407 | val_acc=0.7188
2025-10-13 22:43:55,301 - INFO - _models.training_function_executor - Epoch 134/176 | train_loss=1.5397 | val_loss=0.8396 | val_acc=0.7188
2025-10-13 22:43:55,305 - INFO - _models.training_function_executor - Epoch 135/176 | train_loss=1.3949 | val_loss=0.8385 | val_acc=0.7188
2025-10-13 22:43:55,309 - INFO - _models.training_function_executor - Epoch 136/176 | train_loss=1.5380 | val_loss=0.8381 | val_acc=0.7188
2025-10-13 22:43:55,313 - INFO - _models.training_function_executor - Epoch 137/176 | train_loss=1.4959 | val_loss=0.8374 | val_acc=0.7188
2025-10-13 22:43:55,317 - INFO - _models.training_function_executor - Epoch 138/176 | train_loss=1.3125 | val_loss=0.8372 | val_acc=0.7188
2025-10-13 22:43:55,320 - INFO - _models.training_function_executor - Epoch 139/176 | train_loss=1.6007 | val_loss=0.8371 | val_acc=0.7188
2025-10-13 22:43:55,324 - INFO - _models.training_function_executor - Epoch 140/176 | train_loss=1.5476 | val_loss=0.8362 | val_acc=0.7188
2025-10-13 22:43:55,328 - INFO - _models.training_function_executor - Epoch 141/176 | train_loss=1.3986 | val_loss=0.8357 | val_acc=0.7188
2025-10-13 22:43:55,332 - INFO - _models.training_function_executor - Epoch 142/176 | train_loss=1.3663 | val_loss=0.8350 | val_acc=0.7188
2025-10-13 22:43:55,336 - INFO - _models.training_function_executor - Epoch 143/176 | train_loss=1.4071 | val_loss=0.8341 | val_acc=0.7188
2025-10-13 22:43:55,340 - INFO - _models.training_function_executor - Epoch 144/176 | train_loss=1.4361 | val_loss=0.8337 | val_acc=0.7188
2025-10-13 22:43:55,345 - INFO - _models.training_function_executor - Epoch 145/176 | train_loss=1.4999 | val_loss=0.8330 | val_acc=0.7188
2025-10-13 22:43:55,350 - INFO - _models.training_function_executor - Epoch 146/176 | train_loss=1.2063 | val_loss=0.8327 | val_acc=0.7188
2025-10-13 22:43:55,354 - INFO - _models.training_function_executor - Epoch 147/176 | train_loss=1.3811 | val_loss=0.8322 | val_acc=0.7188
2025-10-13 22:43:55,358 - INFO - _models.training_function_executor - Epoch 148/176 | train_loss=1.6766 | val_loss=0.8320 | val_acc=0.7188
2025-10-13 22:43:55,363 - INFO - _models.training_function_executor - Epoch 149/176 | train_loss=1.4360 | val_loss=0.8313 | val_acc=0.7188
2025-10-13 22:43:55,367 - INFO - _models.training_function_executor - Epoch 150/176 | train_loss=1.5022 | val_loss=0.8312 | val_acc=0.7188
2025-10-13 22:43:55,371 - INFO - _models.training_function_executor - Epoch 151/176 | train_loss=1.4747 | val_loss=0.8311 | val_acc=0.7188
2025-10-13 22:43:55,375 - INFO - _models.training_function_executor - Epoch 152/176 | train_loss=1.4772 | val_loss=0.8310 | val_acc=0.7188
2025-10-13 22:43:55,379 - INFO - _models.training_function_executor - Epoch 153/176 | train_loss=1.4874 | val_loss=0.8306 | val_acc=0.7188
2025-10-13 22:43:55,383 - INFO - _models.training_function_executor - Epoch 154/176 | train_loss=1.5379 | val_loss=0.8304 | val_acc=0.7188
2025-10-13 22:43:55,387 - INFO - _models.training_function_executor - Epoch 155/176 | train_loss=1.4241 | val_loss=0.8300 | val_acc=0.7188
2025-10-13 22:43:55,392 - INFO - _models.training_function_executor - Epoch 156/176 | train_loss=1.6471 | val_loss=0.8299 | val_acc=0.7188
2025-10-13 22:43:55,396 - INFO - _models.training_function_executor - Epoch 157/176 | train_loss=1.2711 | val_loss=0.8298 | val_acc=0.7188
2025-10-13 22:43:55,400 - INFO - _models.training_function_executor - Epoch 158/176 | train_loss=1.4615 | val_loss=0.8297 | val_acc=0.7188
2025-10-13 22:43:55,404 - INFO - _models.training_function_executor - Epoch 159/176 | train_loss=1.4745 | val_loss=0.8293 | val_acc=0.7188
2025-10-13 22:43:55,408 - INFO - _models.training_function_executor - Epoch 160/176 | train_loss=1.4382 | val_loss=0.8292 | val_acc=0.7188
2025-10-13 22:43:55,413 - INFO - _models.training_function_executor - Epoch 161/176 | train_loss=1.4862 | val_loss=0.8292 | val_acc=0.7188
2025-10-13 22:43:55,417 - INFO - _models.training_function_executor - Epoch 162/176 | train_loss=1.3664 | val_loss=0.8292 | val_acc=0.7188
2025-10-13 22:43:55,421 - INFO - _models.training_function_executor - Epoch 163/176 | train_loss=1.2147 | val_loss=0.8292 | val_acc=0.7188
2025-10-13 22:43:55,425 - INFO - _models.training_function_executor - Epoch 164/176 | train_loss=1.4477 | val_loss=0.8293 | val_acc=0.7188
2025-10-13 22:43:55,429 - INFO - _models.training_function_executor - Epoch 165/176 | train_loss=1.4270 | val_loss=0.8292 | val_acc=0.7188
2025-10-13 22:43:55,433 - INFO - _models.training_function_executor - Epoch 166/176 | train_loss=1.5460 | val_loss=0.8292 | val_acc=0.7188
2025-10-13 22:43:55,437 - INFO - _models.training_function_executor - Epoch 167/176 | train_loss=1.5487 | val_loss=0.8292 | val_acc=0.7188
2025-10-13 22:43:55,441 - INFO - _models.training_function_executor - Epoch 168/176 | train_loss=1.5017 | val_loss=0.8291 | val_acc=0.7188
2025-10-13 22:43:55,445 - INFO - _models.training_function_executor - Epoch 169/176 | train_loss=1.6841 | val_loss=0.8291 | val_acc=0.7188
2025-10-13 22:43:55,449 - INFO - _models.training_function_executor - Epoch 170/176 | train_loss=1.4214 | val_loss=0.8291 | val_acc=0.7188
2025-10-13 22:43:55,453 - INFO - _models.training_function_executor - Epoch 171/176 | train_loss=1.4745 | val_loss=0.8291 | val_acc=0.7188
2025-10-13 22:43:55,457 - INFO - _models.training_function_executor - Epoch 172/176 | train_loss=1.3574 | val_loss=0.8291 | val_acc=0.7188
2025-10-13 22:43:55,461 - INFO - _models.training_function_executor - Epoch 173/176 | train_loss=1.3862 | val_loss=0.8291 | val_acc=0.7188
2025-10-13 22:43:55,465 - INFO - _models.training_function_executor - Epoch 174/176 | train_loss=1.7604 | val_loss=0.8291 | val_acc=0.7188
2025-10-13 22:43:55,469 - INFO - _models.training_function_executor - Epoch 175/176 | train_loss=1.5151 | val_loss=0.8291 | val_acc=0.7188
2025-10-13 22:43:55,473 - INFO - _models.training_function_executor - Epoch 176/176 | train_loss=1.5319 | val_loss=0.8291 | val_acc=0.7188
2025-10-13 22:43:56,311 - INFO - _models.training_function_executor - Model: 12,415 parameters, 53.3KB storage
2025-10-13 22:43:56,311 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [3.240099012851715, 3.132558226585388, 3.0129786133766174, 3.0780844688415527, 3.0231998562812805, 3.1563111543655396, 2.8630154132843018, 3.042761266231537, 2.9982608556747437, 2.834508538246155, 2.8412293791770935, 2.8252201676368713, 2.9440651535987854, 2.910038471221924, 2.7256473898887634, 2.8478716611862183, 2.7886645793914795, 2.8315637707710266, 2.6481738686561584, 2.6646834015846252, 2.6609885692596436, 2.729810416698456, 2.510946273803711, 2.4346883296966553, 2.5090577006340027, 2.507664382457733, 2.5296255946159363, 2.3780236542224884, 2.185565173625946, 2.553534984588623, 2.199435293674469, 2.219481945037842, 2.5334768295288086, 2.393792510032654, 2.2561147809028625, 2.377450466156006, 2.243748724460602, 2.211757183074951, 2.229772627353668, 2.0583970844745636, 2.2936346530914307, 2.2309642136096954, 2.0406123995780945, 2.0580437183380127, 2.2108795046806335, 2.1193535923957825, 1.9327524304389954, 2.102003276348114, 2.067926287651062, 1.9489509463310242, 1.9763697385787964, 1.8458238542079926, 1.9078637659549713, 1.7681237757205963, 1.8595035374164581, 1.8317878246307373, 1.9004422426223755, 1.8775685131549835, 1.9830343127250671, 1.9942933022975922, 1.7419024109840393, 1.761093258857727, 1.8073976635932922, 1.9792612791061401, 1.6987170279026031, 1.8538102209568024, 1.8915388882160187, 1.82546928524971, 1.6533066034317017, 1.8297082483768463, 1.787882149219513, 1.6682742238044739, 1.6894581317901611, 1.6450065672397614, 1.8111034035682678, 1.6428836584091187, 1.6239698827266693, 1.8257743120193481, 2.01354256272316, 1.6583241820335388, 1.5867024958133698, 1.6710160672664642, 1.6197805106639862, 1.5288375318050385, 1.6156180202960968, 1.5858446657657623, 1.502356380224228, 1.6044392585754395, 1.5485341250896454, 1.7157280147075653, 1.604941189289093, 1.3767650127410889, 1.388573944568634, 1.3827729523181915, 1.439497858285904, 1.6725963354110718, 1.2834888398647308, 1.5987868309020996, 1.6658831536769867, 1.5084799826145172, 1.5247494578361511, 1.6328643262386322, 1.5535327792167664, 1.5114327669143677, 1.551471084356308, 1.4483919143676758, 1.579952448606491, 1.5062590539455414, 1.4436566531658173, 1.5699254274368286, 1.4739520251750946, 1.5472904443740845, 1.355392962694168, 1.6948562860488892, 1.3537059128284454, 1.5794433951377869, 1.5565108954906464, 1.5028340220451355, 1.3045919686555862, 1.6780938804149628, 1.5632211565971375, 1.5525616109371185, 1.38191819190979, 1.4723162949085236, 1.4529297053813934, 1.401005506515503, 1.6351506114006042, 1.524568349123001, 1.297423779964447, 1.587192416191101, 1.460100769996643, 1.6286741495132446, 1.6009763777256012, 1.5397031605243683, 1.3948584198951721, 1.53799968957901, 1.495921939611435, 1.3124829232692719, 1.600742518901825, 1.5475511252880096, 1.3986424803733826, 1.366284817457199, 1.4071209132671356, 1.436079978942871, 1.4998600482940674, 1.2063084542751312, 1.3810956478118896, 1.676624745130539, 1.4360249638557434, 1.5022221505641937, 1.474703073501587, 1.477247178554535, 1.4873679876327515, 1.5379306375980377, 1.4241073429584503, 1.647125482559204, 1.2711109519004822, 1.4614946842193604, 1.4745005667209625, 1.4381840527057648, 1.4861843883991241, 1.3664302825927734, 1.2147356569766998, 1.447655826807022, 1.4269640743732452, 1.5460470616817474, 1.5486954152584076, 1.5017191767692566, 1.6840989291667938, 1.421413630247116, 1.4744540750980377, 1.357446402311325, 1.3862004280090332, 1.7603880167007446, 1.5150518417358398, 1.5318962037563324], 'val_losses': [2.9508328437805176, 2.9508328437805176, 2.906531810760498, 2.8640148639678955, 2.816720485687256, 2.7728865146636963, 2.731154441833496, 2.6838791370391846, 2.6411094665527344, 2.6009984016418457, 2.555511713027954, 2.5128400325775146, 2.472437620162964, 2.429131269454956, 2.3885586261749268, 2.3517584800720215, 2.308516263961792, 2.270592212677002, 2.233638286590576, 2.194338798522949, 2.157528877258301, 2.123159408569336, 2.0864171981811523, 2.0505549907684326, 2.018322706222534, 1.985662817955017, 1.9534603357315063, 1.9254170656204224, 1.8960678577423096, 1.8671094179153442, 1.839530110359192, 1.8141071796417236, 1.7886033058166504, 1.765890121459961, 1.7452349662780762, 1.7259409427642822, 1.7061911821365356, 1.6871286630630493, 1.6716572046279907, 1.6528589725494385, 1.6351838111877441, 1.6171057224273682, 1.5997631549835205, 1.582646369934082, 1.564712643623352, 1.5462738275527954, 1.5276824235916138, 1.510626196861267, 1.4935956001281738, 1.4778122901916504, 1.4613043069839478, 1.4444283246994019, 1.4296455383300781, 1.414286494255066, 1.3982704877853394, 1.3830890655517578, 1.3676139116287231, 1.3523446321487427, 1.336269736289978, 1.3224741220474243, 1.309643268585205, 1.2959537506103516, 1.2850840091705322, 1.2729089260101318, 1.2614634037017822, 1.2478047609329224, 1.2360886335372925, 1.2247498035430908, 1.2127553224563599, 1.199297308921814, 1.186011552810669, 1.1748976707458496, 1.1637992858886719, 1.1529793739318848, 1.14137864112854, 1.1308637857437134, 1.1201225519180298, 1.1094259023666382, 1.100046992301941, 1.0907886028289795, 1.082153081893921, 1.0719839334487915, 1.06271493434906, 1.0537723302841187, 1.0449310541152954, 1.0364335775375366, 1.0280780792236328, 1.0197898149490356, 1.01296067237854, 1.0053887367248535, 0.9985981583595276, 0.9908426403999329, 0.9821065068244934, 0.9743313193321228, 0.9662933349609375, 0.95778489112854, 0.9496569633483887, 0.9423854351043701, 0.9356105923652649, 0.9287042021751404, 0.9231162667274475, 0.9173373579978943, 0.9115614295005798, 0.9062647223472595, 0.9011375308036804, 0.8963068127632141, 0.8927544355392456, 0.8899677395820618, 0.8871366381645203, 0.8833395838737488, 0.8805811405181885, 0.8773753643035889, 0.8746217489242554, 0.8721113204956055, 0.8682364225387573, 0.8656232357025146, 0.8636049628257751, 0.8606253266334534, 0.8572109937667847, 0.8545952439308167, 0.8528168797492981, 0.8512142300605774, 0.8497219681739807, 0.8486092686653137, 0.8478482365608215, 0.847199022769928, 0.8467990756034851, 0.8451065421104431, 0.8440642952919006, 0.8428264260292053, 0.8424671292304993, 0.8417178988456726, 0.8407351970672607, 0.8395625948905945, 0.8384689688682556, 0.8381395936012268, 0.8374010920524597, 0.8371680378913879, 0.8371409773826599, 0.8361777663230896, 0.8356678485870361, 0.8349655270576477, 0.8340685963630676, 0.8336721658706665, 0.833038330078125, 0.8326928615570068, 0.8322283625602722, 0.832037091255188, 0.8313162922859192, 0.8312398791313171, 0.8310524225234985, 0.8310100436210632, 0.8306449055671692, 0.8303580284118652, 0.8300116062164307, 0.8298940062522888, 0.8298241496086121, 0.829691469669342, 0.8293384909629822, 0.829242467880249, 0.8292011022567749, 0.8291646242141724, 0.8291769623756409, 0.8293346762657166, 0.8292074203491211, 0.8291712999343872, 0.8291748762130737, 0.8290702700614929, 0.82908695936203, 0.8290593028068542, 0.829067051410675, 0.8290558457374573, 0.8290548920631409, 0.8290548920631409, 0.8290529847145081, 0.8290529847145081], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0307046445871678e-05, 'batch_size': 192, 'epochs': 176, 'hidden_size': 107, 'dropout': 0.040056482280615764, 'weight_decay': 0.0023063310187989693, 'label_smoothing': 0.0040740698016193296, 'grad_clip_norm': 1.3390324861996743, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 12415, 'model_storage_size_kb': 53.34570312500001, 'model_size_validation': 'PASS'}
2025-10-13 22:43:56,311 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:43:56,311 - INFO - _models.training_function_executor - Model: 12,415 parameters, 53.3KB (PASS 256KB limit)
2025-10-13 22:43:56,311 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.945s
2025-10-13 22:43:56,391 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:43:56,392 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.080s
2025-10-13 22:43:56,392 - INFO - bo.run_bo - Recorded observation #39: hparams={'lr': 1.0307046445871678e-05, 'batch_size': np.int64(192), 'epochs': np.int64(176), 'hidden_size': np.int64(107), 'dropout': 0.040056482280615764, 'weight_decay': 0.0023063310187989693, 'label_smoothing': 0.0040740698016193296, 'grad_clip_norm': 1.3390324861996743, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:43:56,392 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 39: {'lr': 1.0307046445871678e-05, 'batch_size': np.int64(192), 'epochs': np.int64(176), 'hidden_size': np.int64(107), 'dropout': 0.040056482280615764, 'weight_decay': 0.0023063310187989693, 'label_smoothing': 0.0040740698016193296, 'grad_clip_norm': 1.3390324861996743, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:43:56,392 - INFO - bo.run_bo - üîçBO Trial 40: Using RF surrogate + Expected Improvement
2025-10-13 22:43:56,392 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:43:56,392 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 40 (NaN monitoring active)
2025-10-13 22:43:56,392 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:43:56,392 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:43:56,392 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.1117113871734633e-05, 'batch_size': 128, 'epochs': 195, 'hidden_size': 179, 'dropout': 0.010246952705228908, 'weight_decay': 0.005740731422728093, 'label_smoothing': 0.06855255742677543, 'grad_clip_norm': 1.5581739504269645, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:43:56,393 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.1117113871734633e-05, 'batch_size': 128, 'epochs': 195, 'hidden_size': 179, 'dropout': 0.010246952705228908, 'weight_decay': 0.005740731422728093, 'label_smoothing': 0.06855255742677543, 'grad_clip_norm': 1.5581739504269645, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:43:59,772 - INFO - _models.training_function_executor - Epoch 001/195 | train_loss=3.7977 | val_loss=3.3345 | val_acc=0.3906
2025-10-13 22:43:59,779 - INFO - _models.training_function_executor - Epoch 002/195 | train_loss=3.7939 | val_loss=3.3345 | val_acc=0.3906
2025-10-13 22:43:59,784 - INFO - _models.training_function_executor - Epoch 003/195 | train_loss=3.7998 | val_loss=3.2517 | val_acc=0.4375
2025-10-13 22:43:59,788 - INFO - _models.training_function_executor - Epoch 004/195 | train_loss=3.7689 | val_loss=3.0777 | val_acc=0.4531
2025-10-13 22:43:59,792 - INFO - _models.training_function_executor - Epoch 005/195 | train_loss=3.4797 | val_loss=2.9156 | val_acc=0.5000
2025-10-13 22:43:59,797 - INFO - _models.training_function_executor - Epoch 006/195 | train_loss=3.2695 | val_loss=2.7648 | val_acc=0.5469
2025-10-13 22:43:59,801 - INFO - _models.training_function_executor - Epoch 007/195 | train_loss=3.2750 | val_loss=2.6149 | val_acc=0.5625
2025-10-13 22:43:59,805 - INFO - _models.training_function_executor - Epoch 008/195 | train_loss=2.9701 | val_loss=2.4760 | val_acc=0.5781
2025-10-13 22:43:59,809 - INFO - _models.training_function_executor - Epoch 009/195 | train_loss=2.9124 | val_loss=2.3462 | val_acc=0.5938
2025-10-13 22:43:59,814 - INFO - _models.training_function_executor - Epoch 010/195 | train_loss=2.6894 | val_loss=2.2219 | val_acc=0.5938
2025-10-13 22:43:59,818 - INFO - _models.training_function_executor - Epoch 011/195 | train_loss=2.6416 | val_loss=2.1100 | val_acc=0.6406
2025-10-13 22:43:59,822 - INFO - _models.training_function_executor - Epoch 012/195 | train_loss=2.5451 | val_loss=2.0074 | val_acc=0.6406
2025-10-13 22:43:59,826 - INFO - _models.training_function_executor - Epoch 013/195 | train_loss=2.3634 | val_loss=1.9195 | val_acc=0.6719
2025-10-13 22:43:59,830 - INFO - _models.training_function_executor - Epoch 014/195 | train_loss=2.2413 | val_loss=1.8432 | val_acc=0.6562
2025-10-13 22:43:59,834 - INFO - _models.training_function_executor - Epoch 015/195 | train_loss=2.1335 | val_loss=1.7764 | val_acc=0.6562
2025-10-13 22:43:59,839 - INFO - _models.training_function_executor - Epoch 016/195 | train_loss=2.0328 | val_loss=1.7246 | val_acc=0.6562
2025-10-13 22:43:59,843 - INFO - _models.training_function_executor - Epoch 017/195 | train_loss=2.1050 | val_loss=1.6832 | val_acc=0.6562
2025-10-13 22:43:59,847 - INFO - _models.training_function_executor - Epoch 018/195 | train_loss=1.9532 | val_loss=1.6503 | val_acc=0.7188
2025-10-13 22:43:59,851 - INFO - _models.training_function_executor - Epoch 019/195 | train_loss=1.9573 | val_loss=1.6245 | val_acc=0.7188
2025-10-13 22:43:59,856 - INFO - _models.training_function_executor - Epoch 020/195 | train_loss=1.8917 | val_loss=1.5995 | val_acc=0.6875
2025-10-13 22:43:59,861 - INFO - _models.training_function_executor - Epoch 021/195 | train_loss=1.8437 | val_loss=1.5738 | val_acc=0.7188
2025-10-13 22:43:59,866 - INFO - _models.training_function_executor - Epoch 022/195 | train_loss=1.9292 | val_loss=1.5450 | val_acc=0.7188
2025-10-13 22:43:59,871 - INFO - _models.training_function_executor - Epoch 023/195 | train_loss=1.7875 | val_loss=1.5132 | val_acc=0.7188
2025-10-13 22:43:59,876 - INFO - _models.training_function_executor - Epoch 024/195 | train_loss=1.8168 | val_loss=1.4765 | val_acc=0.7188
2025-10-13 22:43:59,880 - INFO - _models.training_function_executor - Epoch 025/195 | train_loss=1.7344 | val_loss=1.4383 | val_acc=0.7188
2025-10-13 22:43:59,884 - INFO - _models.training_function_executor - Epoch 026/195 | train_loss=1.6765 | val_loss=1.3995 | val_acc=0.7188
2025-10-13 22:43:59,888 - INFO - _models.training_function_executor - Epoch 027/195 | train_loss=1.6750 | val_loss=1.3587 | val_acc=0.7188
2025-10-13 22:43:59,892 - INFO - _models.training_function_executor - Epoch 028/195 | train_loss=1.5625 | val_loss=1.3180 | val_acc=0.7188
2025-10-13 22:43:59,896 - INFO - _models.training_function_executor - Epoch 029/195 | train_loss=1.5029 | val_loss=1.2785 | val_acc=0.7188
2025-10-13 22:43:59,900 - INFO - _models.training_function_executor - Epoch 030/195 | train_loss=1.5570 | val_loss=1.2386 | val_acc=0.7031
2025-10-13 22:43:59,904 - INFO - _models.training_function_executor - Epoch 031/195 | train_loss=1.4792 | val_loss=1.1995 | val_acc=0.6875
2025-10-13 22:43:59,908 - INFO - _models.training_function_executor - Epoch 032/195 | train_loss=1.4578 | val_loss=1.1619 | val_acc=0.6875
2025-10-13 22:43:59,912 - INFO - _models.training_function_executor - Epoch 033/195 | train_loss=1.3928 | val_loss=1.1253 | val_acc=0.6875
2025-10-13 22:43:59,916 - INFO - _models.training_function_executor - Epoch 034/195 | train_loss=1.3484 | val_loss=1.0908 | val_acc=0.6875
2025-10-13 22:43:59,920 - INFO - _models.training_function_executor - Epoch 035/195 | train_loss=1.2551 | val_loss=1.0560 | val_acc=0.6875
2025-10-13 22:43:59,924 - INFO - _models.training_function_executor - Epoch 036/195 | train_loss=1.2402 | val_loss=1.0225 | val_acc=0.6875
2025-10-13 22:43:59,928 - INFO - _models.training_function_executor - Epoch 037/195 | train_loss=1.2405 | val_loss=0.9900 | val_acc=0.6875
2025-10-13 22:43:59,932 - INFO - _models.training_function_executor - Epoch 038/195 | train_loss=1.2145 | val_loss=0.9576 | val_acc=0.6875
2025-10-13 22:43:59,935 - INFO - _models.training_function_executor - Epoch 039/195 | train_loss=1.2301 | val_loss=0.9282 | val_acc=0.6875
2025-10-13 22:43:59,939 - INFO - _models.training_function_executor - Epoch 040/195 | train_loss=1.1591 | val_loss=0.9002 | val_acc=0.7031
2025-10-13 22:43:59,943 - INFO - _models.training_function_executor - Epoch 041/195 | train_loss=1.1252 | val_loss=0.8738 | val_acc=0.7031
2025-10-13 22:43:59,947 - INFO - _models.training_function_executor - Epoch 042/195 | train_loss=1.0143 | val_loss=0.8493 | val_acc=0.7031
2025-10-13 22:43:59,951 - INFO - _models.training_function_executor - Epoch 043/195 | train_loss=1.0748 | val_loss=0.8278 | val_acc=0.7031
2025-10-13 22:43:59,955 - INFO - _models.training_function_executor - Epoch 044/195 | train_loss=1.0442 | val_loss=0.8091 | val_acc=0.7188
2025-10-13 22:43:59,959 - INFO - _models.training_function_executor - Epoch 045/195 | train_loss=0.9551 | val_loss=0.7927 | val_acc=0.7188
2025-10-13 22:43:59,963 - INFO - _models.training_function_executor - Epoch 046/195 | train_loss=0.9257 | val_loss=0.7803 | val_acc=0.7188
2025-10-13 22:43:59,967 - INFO - _models.training_function_executor - Epoch 047/195 | train_loss=1.0263 | val_loss=0.7718 | val_acc=0.7188
2025-10-13 22:43:59,971 - INFO - _models.training_function_executor - Epoch 048/195 | train_loss=1.0123 | val_loss=0.7666 | val_acc=0.7188
2025-10-13 22:43:59,975 - INFO - _models.training_function_executor - Epoch 049/195 | train_loss=0.9856 | val_loss=0.7665 | val_acc=0.7188
2025-10-13 22:43:59,980 - INFO - _models.training_function_executor - Epoch 050/195 | train_loss=1.0979 | val_loss=0.7666 | val_acc=0.7188
2025-10-13 22:43:59,984 - INFO - _models.training_function_executor - Epoch 051/195 | train_loss=1.0697 | val_loss=0.7674 | val_acc=0.7188
2025-10-13 22:43:59,988 - INFO - _models.training_function_executor - Epoch 052/195 | train_loss=1.0719 | val_loss=0.7676 | val_acc=0.7188
2025-10-13 22:43:59,991 - INFO - _models.training_function_executor - Epoch 053/195 | train_loss=0.9238 | val_loss=0.7663 | val_acc=0.7188
2025-10-13 22:43:59,995 - INFO - _models.training_function_executor - Epoch 054/195 | train_loss=1.0100 | val_loss=0.7637 | val_acc=0.7188
2025-10-13 22:43:59,999 - INFO - _models.training_function_executor - Epoch 055/195 | train_loss=0.9621 | val_loss=0.7610 | val_acc=0.7188
2025-10-13 22:44:00,003 - INFO - _models.training_function_executor - Epoch 056/195 | train_loss=0.9533 | val_loss=0.7579 | val_acc=0.7188
2025-10-13 22:44:00,007 - INFO - _models.training_function_executor - Epoch 057/195 | train_loss=0.9376 | val_loss=0.7566 | val_acc=0.7188
2025-10-13 22:44:00,011 - INFO - _models.training_function_executor - Epoch 058/195 | train_loss=0.9639 | val_loss=0.7532 | val_acc=0.7188
2025-10-13 22:44:00,015 - INFO - _models.training_function_executor - Epoch 059/195 | train_loss=1.0115 | val_loss=0.7508 | val_acc=0.7188
2025-10-13 22:44:00,019 - INFO - _models.training_function_executor - Epoch 060/195 | train_loss=1.0266 | val_loss=0.7500 | val_acc=0.7188
2025-10-13 22:44:00,023 - INFO - _models.training_function_executor - Epoch 061/195 | train_loss=1.0238 | val_loss=0.7496 | val_acc=0.7188
2025-10-13 22:44:00,027 - INFO - _models.training_function_executor - Epoch 062/195 | train_loss=1.0119 | val_loss=0.7495 | val_acc=0.7188
2025-10-13 22:44:00,030 - INFO - _models.training_function_executor - Epoch 063/195 | train_loss=0.9659 | val_loss=0.7488 | val_acc=0.7188
2025-10-13 22:44:00,035 - INFO - _models.training_function_executor - Epoch 064/195 | train_loss=1.0366 | val_loss=0.7480 | val_acc=0.7188
2025-10-13 22:44:00,039 - INFO - _models.training_function_executor - Epoch 065/195 | train_loss=0.9690 | val_loss=0.7461 | val_acc=0.7188
2025-10-13 22:44:00,042 - INFO - _models.training_function_executor - Epoch 066/195 | train_loss=0.9401 | val_loss=0.7453 | val_acc=0.7188
2025-10-13 22:44:00,046 - INFO - _models.training_function_executor - Epoch 067/195 | train_loss=0.9786 | val_loss=0.7426 | val_acc=0.7188
2025-10-13 22:44:00,050 - INFO - _models.training_function_executor - Epoch 068/195 | train_loss=0.9673 | val_loss=0.7398 | val_acc=0.7188
2025-10-13 22:44:00,054 - INFO - _models.training_function_executor - Epoch 069/195 | train_loss=0.9160 | val_loss=0.7391 | val_acc=0.7188
2025-10-13 22:44:00,058 - INFO - _models.training_function_executor - Epoch 070/195 | train_loss=0.9979 | val_loss=0.7390 | val_acc=0.7188
2025-10-13 22:44:00,062 - INFO - _models.training_function_executor - Epoch 071/195 | train_loss=0.8592 | val_loss=0.7373 | val_acc=0.7188
2025-10-13 22:44:00,066 - INFO - _models.training_function_executor - Epoch 072/195 | train_loss=1.0292 | val_loss=0.7361 | val_acc=0.7188
2025-10-13 22:44:00,070 - INFO - _models.training_function_executor - Epoch 073/195 | train_loss=0.9936 | val_loss=0.7343 | val_acc=0.7188
2025-10-13 22:44:00,074 - INFO - _models.training_function_executor - Epoch 074/195 | train_loss=0.9094 | val_loss=0.7349 | val_acc=0.7188
2025-10-13 22:44:00,078 - INFO - _models.training_function_executor - Epoch 075/195 | train_loss=0.9228 | val_loss=0.7363 | val_acc=0.7188
2025-10-13 22:44:00,082 - INFO - _models.training_function_executor - Epoch 076/195 | train_loss=1.0301 | val_loss=0.7376 | val_acc=0.7188
2025-10-13 22:44:00,086 - INFO - _models.training_function_executor - Epoch 077/195 | train_loss=0.9074 | val_loss=0.7382 | val_acc=0.7188
2025-10-13 22:44:00,090 - INFO - _models.training_function_executor - Epoch 078/195 | train_loss=0.9767 | val_loss=0.7375 | val_acc=0.7188
2025-10-13 22:44:00,094 - INFO - _models.training_function_executor - Epoch 079/195 | train_loss=0.9303 | val_loss=0.7366 | val_acc=0.7188
2025-10-13 22:44:00,098 - INFO - _models.training_function_executor - Epoch 080/195 | train_loss=0.9815 | val_loss=0.7350 | val_acc=0.7188
2025-10-13 22:44:00,102 - INFO - _models.training_function_executor - Epoch 081/195 | train_loss=0.9134 | val_loss=0.7328 | val_acc=0.7188
2025-10-13 22:44:00,106 - INFO - _models.training_function_executor - Epoch 082/195 | train_loss=0.9597 | val_loss=0.7304 | val_acc=0.7188
2025-10-13 22:44:00,110 - INFO - _models.training_function_executor - Epoch 083/195 | train_loss=0.9285 | val_loss=0.7289 | val_acc=0.7188
2025-10-13 22:44:00,114 - INFO - _models.training_function_executor - Epoch 084/195 | train_loss=0.9216 | val_loss=0.7278 | val_acc=0.7188
2025-10-13 22:44:00,118 - INFO - _models.training_function_executor - Epoch 085/195 | train_loss=0.9750 | val_loss=0.7270 | val_acc=0.7188
2025-10-13 22:44:00,122 - INFO - _models.training_function_executor - Epoch 086/195 | train_loss=0.9496 | val_loss=0.7262 | val_acc=0.7188
2025-10-13 22:44:00,126 - INFO - _models.training_function_executor - Epoch 087/195 | train_loss=0.9005 | val_loss=0.7255 | val_acc=0.7188
2025-10-13 22:44:00,130 - INFO - _models.training_function_executor - Epoch 088/195 | train_loss=0.9351 | val_loss=0.7249 | val_acc=0.7188
2025-10-13 22:44:00,134 - INFO - _models.training_function_executor - Epoch 089/195 | train_loss=0.9767 | val_loss=0.7247 | val_acc=0.7188
2025-10-13 22:44:00,138 - INFO - _models.training_function_executor - Epoch 090/195 | train_loss=0.9652 | val_loss=0.7244 | val_acc=0.7188
2025-10-13 22:44:00,142 - INFO - _models.training_function_executor - Epoch 091/195 | train_loss=0.9415 | val_loss=0.7244 | val_acc=0.7188
2025-10-13 22:44:00,146 - INFO - _models.training_function_executor - Epoch 092/195 | train_loss=0.9178 | val_loss=0.7242 | val_acc=0.7188
2025-10-13 22:44:00,150 - INFO - _models.training_function_executor - Epoch 093/195 | train_loss=0.9684 | val_loss=0.7246 | val_acc=0.7188
2025-10-13 22:44:00,154 - INFO - _models.training_function_executor - Epoch 094/195 | train_loss=0.9700 | val_loss=0.7257 | val_acc=0.7188
2025-10-13 22:44:00,158 - INFO - _models.training_function_executor - Epoch 095/195 | train_loss=0.9933 | val_loss=0.7269 | val_acc=0.7188
2025-10-13 22:44:00,162 - INFO - _models.training_function_executor - Epoch 096/195 | train_loss=0.9532 | val_loss=0.7270 | val_acc=0.7188
2025-10-13 22:44:00,167 - INFO - _models.training_function_executor - Epoch 097/195 | train_loss=0.9147 | val_loss=0.7261 | val_acc=0.7188
2025-10-13 22:44:00,171 - INFO - _models.training_function_executor - Epoch 098/195 | train_loss=1.0005 | val_loss=0.7255 | val_acc=0.7188
2025-10-13 22:44:00,175 - INFO - _models.training_function_executor - Epoch 099/195 | train_loss=0.9287 | val_loss=0.7252 | val_acc=0.7188
2025-10-13 22:44:00,179 - INFO - _models.training_function_executor - Epoch 100/195 | train_loss=0.9034 | val_loss=0.7239 | val_acc=0.7188
2025-10-13 22:44:00,184 - INFO - _models.training_function_executor - Epoch 101/195 | train_loss=0.9494 | val_loss=0.7232 | val_acc=0.7188
2025-10-13 22:44:00,188 - INFO - _models.training_function_executor - Epoch 102/195 | train_loss=0.8983 | val_loss=0.7220 | val_acc=0.7188
2025-10-13 22:44:00,192 - INFO - _models.training_function_executor - Epoch 103/195 | train_loss=0.8976 | val_loss=0.7209 | val_acc=0.7188
2025-10-13 22:44:00,196 - INFO - _models.training_function_executor - Epoch 104/195 | train_loss=0.9541 | val_loss=0.7207 | val_acc=0.7188
2025-10-13 22:44:00,200 - INFO - _models.training_function_executor - Epoch 105/195 | train_loss=0.9774 | val_loss=0.7208 | val_acc=0.7188
2025-10-13 22:44:00,204 - INFO - _models.training_function_executor - Epoch 106/195 | train_loss=0.9045 | val_loss=0.7212 | val_acc=0.7188
2025-10-13 22:44:00,209 - INFO - _models.training_function_executor - Epoch 107/195 | train_loss=0.9066 | val_loss=0.7215 | val_acc=0.7188
2025-10-13 22:44:00,213 - INFO - _models.training_function_executor - Epoch 108/195 | train_loss=0.9301 | val_loss=0.7224 | val_acc=0.7188
2025-10-13 22:44:00,217 - INFO - _models.training_function_executor - Epoch 109/195 | train_loss=1.0032 | val_loss=0.7230 | val_acc=0.7188
2025-10-13 22:44:00,222 - INFO - _models.training_function_executor - Epoch 110/195 | train_loss=0.9630 | val_loss=0.7237 | val_acc=0.7188
2025-10-13 22:44:00,226 - INFO - _models.training_function_executor - Epoch 111/195 | train_loss=0.9035 | val_loss=0.7238 | val_acc=0.7188
2025-10-13 22:44:00,230 - INFO - _models.training_function_executor - Epoch 112/195 | train_loss=0.9477 | val_loss=0.7232 | val_acc=0.7188
2025-10-13 22:44:00,234 - INFO - _models.training_function_executor - Epoch 113/195 | train_loss=0.9960 | val_loss=0.7224 | val_acc=0.7188
2025-10-13 22:44:00,238 - INFO - _models.training_function_executor - Epoch 114/195 | train_loss=0.8748 | val_loss=0.7229 | val_acc=0.7188
2025-10-13 22:44:00,242 - INFO - _models.training_function_executor - Epoch 115/195 | train_loss=0.8944 | val_loss=0.7223 | val_acc=0.7188
2025-10-13 22:44:00,246 - INFO - _models.training_function_executor - Epoch 116/195 | train_loss=0.9063 | val_loss=0.7216 | val_acc=0.7188
2025-10-13 22:44:00,250 - INFO - _models.training_function_executor - Epoch 117/195 | train_loss=0.9726 | val_loss=0.7219 | val_acc=0.7188
2025-10-13 22:44:00,254 - INFO - _models.training_function_executor - Epoch 118/195 | train_loss=0.9291 | val_loss=0.7220 | val_acc=0.7188
2025-10-13 22:44:00,258 - INFO - _models.training_function_executor - Epoch 119/195 | train_loss=0.8998 | val_loss=0.7222 | val_acc=0.7188
2025-10-13 22:44:00,262 - INFO - _models.training_function_executor - Epoch 120/195 | train_loss=0.9436 | val_loss=0.7222 | val_acc=0.7188
2025-10-13 22:44:00,266 - INFO - _models.training_function_executor - Epoch 121/195 | train_loss=0.8931 | val_loss=0.7220 | val_acc=0.7188
2025-10-13 22:44:00,270 - INFO - _models.training_function_executor - Epoch 122/195 | train_loss=0.9159 | val_loss=0.7214 | val_acc=0.7188
2025-10-13 22:44:00,274 - INFO - _models.training_function_executor - Epoch 123/195 | train_loss=0.9200 | val_loss=0.7210 | val_acc=0.7188
2025-10-13 22:44:00,278 - INFO - _models.training_function_executor - Epoch 124/195 | train_loss=0.9296 | val_loss=0.7202 | val_acc=0.7188
2025-10-13 22:44:00,283 - INFO - _models.training_function_executor - Epoch 125/195 | train_loss=0.9609 | val_loss=0.7197 | val_acc=0.7188
2025-10-13 22:44:00,287 - INFO - _models.training_function_executor - Epoch 126/195 | train_loss=0.8722 | val_loss=0.7195 | val_acc=0.7188
2025-10-13 22:44:00,291 - INFO - _models.training_function_executor - Epoch 127/195 | train_loss=0.9755 | val_loss=0.7198 | val_acc=0.7188
2025-10-13 22:44:00,295 - INFO - _models.training_function_executor - Epoch 128/195 | train_loss=0.9512 | val_loss=0.7200 | val_acc=0.7188
2025-10-13 22:44:00,299 - INFO - _models.training_function_executor - Epoch 129/195 | train_loss=0.8909 | val_loss=0.7194 | val_acc=0.7188
2025-10-13 22:44:00,303 - INFO - _models.training_function_executor - Epoch 130/195 | train_loss=0.9741 | val_loss=0.7191 | val_acc=0.7188
2025-10-13 22:44:00,307 - INFO - _models.training_function_executor - Epoch 131/195 | train_loss=0.9413 | val_loss=0.7189 | val_acc=0.7188
2025-10-13 22:44:00,311 - INFO - _models.training_function_executor - Epoch 132/195 | train_loss=0.9354 | val_loss=0.7187 | val_acc=0.7188
2025-10-13 22:44:00,315 - INFO - _models.training_function_executor - Epoch 133/195 | train_loss=0.8979 | val_loss=0.7181 | val_acc=0.7188
2025-10-13 22:44:00,319 - INFO - _models.training_function_executor - Epoch 134/195 | train_loss=0.8172 | val_loss=0.7176 | val_acc=0.7188
2025-10-13 22:44:00,323 - INFO - _models.training_function_executor - Epoch 135/195 | train_loss=0.9704 | val_loss=0.7173 | val_acc=0.7188
2025-10-13 22:44:00,327 - INFO - _models.training_function_executor - Epoch 136/195 | train_loss=0.8647 | val_loss=0.7165 | val_acc=0.7188
2025-10-13 22:44:00,331 - INFO - _models.training_function_executor - Epoch 137/195 | train_loss=0.9525 | val_loss=0.7156 | val_acc=0.7188
2025-10-13 22:44:00,335 - INFO - _models.training_function_executor - Epoch 138/195 | train_loss=0.9170 | val_loss=0.7155 | val_acc=0.7188
2025-10-13 22:44:00,339 - INFO - _models.training_function_executor - Epoch 139/195 | train_loss=0.9053 | val_loss=0.7156 | val_acc=0.7188
2025-10-13 22:44:00,343 - INFO - _models.training_function_executor - Epoch 140/195 | train_loss=0.9204 | val_loss=0.7155 | val_acc=0.7188
2025-10-13 22:44:00,347 - INFO - _models.training_function_executor - Epoch 141/195 | train_loss=0.9059 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,352 - INFO - _models.training_function_executor - Epoch 142/195 | train_loss=0.9341 | val_loss=0.7146 | val_acc=0.7188
2025-10-13 22:44:00,357 - INFO - _models.training_function_executor - Epoch 143/195 | train_loss=0.8840 | val_loss=0.7146 | val_acc=0.7188
2025-10-13 22:44:00,362 - INFO - _models.training_function_executor - Epoch 144/195 | train_loss=0.9702 | val_loss=0.7145 | val_acc=0.7188
2025-10-13 22:44:00,366 - INFO - _models.training_function_executor - Epoch 145/195 | train_loss=1.0352 | val_loss=0.7146 | val_acc=0.7188
2025-10-13 22:44:00,371 - INFO - _models.training_function_executor - Epoch 146/195 | train_loss=0.9579 | val_loss=0.7148 | val_acc=0.7188
2025-10-13 22:44:00,375 - INFO - _models.training_function_executor - Epoch 147/195 | train_loss=0.9151 | val_loss=0.7148 | val_acc=0.7188
2025-10-13 22:44:00,380 - INFO - _models.training_function_executor - Epoch 148/195 | train_loss=0.9816 | val_loss=0.7152 | val_acc=0.7188
2025-10-13 22:44:00,385 - INFO - _models.training_function_executor - Epoch 149/195 | train_loss=0.9277 | val_loss=0.7154 | val_acc=0.7188
2025-10-13 22:44:00,389 - INFO - _models.training_function_executor - Epoch 150/195 | train_loss=0.9169 | val_loss=0.7154 | val_acc=0.7188
2025-10-13 22:44:00,393 - INFO - _models.training_function_executor - Epoch 151/195 | train_loss=0.9429 | val_loss=0.7153 | val_acc=0.7188
2025-10-13 22:44:00,397 - INFO - _models.training_function_executor - Epoch 152/195 | train_loss=0.8778 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,401 - INFO - _models.training_function_executor - Epoch 153/195 | train_loss=0.9645 | val_loss=0.7148 | val_acc=0.7188
2025-10-13 22:44:00,405 - INFO - _models.training_function_executor - Epoch 154/195 | train_loss=0.9698 | val_loss=0.7148 | val_acc=0.7188
2025-10-13 22:44:00,409 - INFO - _models.training_function_executor - Epoch 155/195 | train_loss=0.9154 | val_loss=0.7148 | val_acc=0.7188
2025-10-13 22:44:00,413 - INFO - _models.training_function_executor - Epoch 156/195 | train_loss=1.0158 | val_loss=0.7148 | val_acc=0.7188
2025-10-13 22:44:00,418 - INFO - _models.training_function_executor - Epoch 157/195 | train_loss=0.9201 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,422 - INFO - _models.training_function_executor - Epoch 158/195 | train_loss=0.8968 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,426 - INFO - _models.training_function_executor - Epoch 159/195 | train_loss=0.9090 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,430 - INFO - _models.training_function_executor - Epoch 160/195 | train_loss=0.9140 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,434 - INFO - _models.training_function_executor - Epoch 161/195 | train_loss=0.8756 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,438 - INFO - _models.training_function_executor - Epoch 162/195 | train_loss=0.9297 | val_loss=0.7145 | val_acc=0.7188
2025-10-13 22:44:00,442 - INFO - _models.training_function_executor - Epoch 163/195 | train_loss=0.9149 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,446 - INFO - _models.training_function_executor - Epoch 164/195 | train_loss=0.8947 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,450 - INFO - _models.training_function_executor - Epoch 165/195 | train_loss=0.9450 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,454 - INFO - _models.training_function_executor - Epoch 166/195 | train_loss=0.9783 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,458 - INFO - _models.training_function_executor - Epoch 167/195 | train_loss=0.9537 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,462 - INFO - _models.training_function_executor - Epoch 168/195 | train_loss=1.0050 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,466 - INFO - _models.training_function_executor - Epoch 169/195 | train_loss=0.8683 | val_loss=0.7148 | val_acc=0.7188
2025-10-13 22:44:00,469 - INFO - _models.training_function_executor - Epoch 170/195 | train_loss=0.9462 | val_loss=0.7147 | val_acc=0.7188
2025-10-13 22:44:00,473 - INFO - _models.training_function_executor - Epoch 171/195 | train_loss=0.9436 | val_loss=0.7148 | val_acc=0.7188
2025-10-13 22:44:00,477 - INFO - _models.training_function_executor - Epoch 172/195 | train_loss=1.0199 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,481 - INFO - _models.training_function_executor - Epoch 173/195 | train_loss=1.0493 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,486 - INFO - _models.training_function_executor - Epoch 174/195 | train_loss=0.8349 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,490 - INFO - _models.training_function_executor - Epoch 175/195 | train_loss=0.9752 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,494 - INFO - _models.training_function_executor - Epoch 176/195 | train_loss=0.9527 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,498 - INFO - _models.training_function_executor - Epoch 177/195 | train_loss=0.9267 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,502 - INFO - _models.training_function_executor - Epoch 178/195 | train_loss=0.9177 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,506 - INFO - _models.training_function_executor - Epoch 179/195 | train_loss=0.9002 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,510 - INFO - _models.training_function_executor - Epoch 180/195 | train_loss=0.9184 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,514 - INFO - _models.training_function_executor - Epoch 181/195 | train_loss=0.9571 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,518 - INFO - _models.training_function_executor - Epoch 182/195 | train_loss=0.8773 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,522 - INFO - _models.training_function_executor - Epoch 183/195 | train_loss=0.9553 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,526 - INFO - _models.training_function_executor - Epoch 184/195 | train_loss=0.9314 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,530 - INFO - _models.training_function_executor - Epoch 185/195 | train_loss=0.8851 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,534 - INFO - _models.training_function_executor - Epoch 186/195 | train_loss=0.9161 | val_loss=0.7150 | val_acc=0.7188
2025-10-13 22:44:00,538 - INFO - _models.training_function_executor - Epoch 187/195 | train_loss=0.9484 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,542 - INFO - _models.training_function_executor - Epoch 188/195 | train_loss=0.9541 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,546 - INFO - _models.training_function_executor - Epoch 189/195 | train_loss=0.9658 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,550 - INFO - _models.training_function_executor - Epoch 190/195 | train_loss=0.9202 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,555 - INFO - _models.training_function_executor - Epoch 191/195 | train_loss=0.8974 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,559 - INFO - _models.training_function_executor - Epoch 192/195 | train_loss=0.9984 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,563 - INFO - _models.training_function_executor - Epoch 193/195 | train_loss=1.0134 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,567 - INFO - _models.training_function_executor - Epoch 194/195 | train_loss=0.9290 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:00,571 - INFO - _models.training_function_executor - Epoch 195/195 | train_loss=0.8774 | val_loss=0.7149 | val_acc=0.7188
2025-10-13 22:44:01,807 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-10-13 22:44:01,808 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [3.7976683378219604, 3.793923258781433, 3.799759864807129, 3.7689183950424194, 3.4796873331069946, 3.2694647312164307, 3.275018811225891, 2.970070481300354, 2.91241717338562, 2.689441442489624, 2.6415948271751404, 2.5450645685195923, 2.3634268045425415, 2.2413241863250732, 2.1334964632987976, 2.032841682434082, 2.1050299406051636, 1.9532355070114136, 1.9573139548301697, 1.8916625380516052, 1.8436737656593323, 1.9291881918907166, 1.7875324487686157, 1.8168079853057861, 1.7343813180923462, 1.6764812469482422, 1.6749761700630188, 1.5624500513076782, 1.5029088854789734, 1.5570366978645325, 1.4792342782020569, 1.4577935934066772, 1.3927664160728455, 1.3484286665916443, 1.2551036477088928, 1.2402384281158447, 1.2404839992523193, 1.2145166993141174, 1.2301329970359802, 1.1590557098388672, 1.1252185702323914, 1.0143008530139923, 1.0747625231742859, 1.0442404747009277, 0.9551429450511932, 0.9257323145866394, 1.0263282656669617, 1.0123001635074615, 0.9855810105800629, 1.0979049801826477, 1.0696744322776794, 1.0719280242919922, 0.9237799346446991, 1.009993076324463, 0.962066262960434, 0.9532849490642548, 0.9376499652862549, 0.9638825953006744, 1.0114992260932922, 1.02657949924469, 1.0237516462802887, 1.0119068026542664, 0.9659251570701599, 1.0365760326385498, 0.9690341949462891, 0.9401314854621887, 0.9786346256732941, 0.9672997295856476, 0.915990799665451, 0.9978839159011841, 0.8591801226139069, 1.0292243659496307, 0.9936092793941498, 0.9093690812587738, 0.9228118062019348, 1.030068039894104, 0.9073512256145477, 0.9766614735126495, 0.9302692115306854, 0.9815004467964172, 0.9133637547492981, 0.9596834480762482, 0.9285062849521637, 0.9215601980686188, 0.9750142693519592, 0.9496403932571411, 0.9004870057106018, 0.9350643754005432, 0.9766731560230255, 0.9651887118816376, 0.9414592683315277, 0.9178086221218109, 0.9683570861816406, 0.9700319766998291, 0.9933269023895264, 0.9531889259815216, 0.9147134125232697, 1.0005291998386383, 0.9287265539169312, 0.9033625423908234, 0.9493566453456879, 0.898321121931076, 0.8976289629936218, 0.9540982842445374, 0.9773858487606049, 0.9045466482639313, 0.9066081345081329, 0.9300661087036133, 1.0032126307487488, 0.9630171060562134, 0.9035043716430664, 0.9476664662361145, 0.9960478246212006, 0.8747809529304504, 0.8944499790668488, 0.9062962532043457, 0.9726177752017975, 0.9291453063488007, 0.8998498916625977, 0.9435769617557526, 0.8930670022964478, 0.9159273505210876, 0.9200425446033478, 0.9296335279941559, 0.960863322019577, 0.8721832931041718, 0.9755036532878876, 0.9512233734130859, 0.8909319341182709, 0.9740977883338928, 0.9413495361804962, 0.9353554248809814, 0.8978674113750458, 0.8171760737895966, 0.9703903794288635, 0.8647199273109436, 0.9525168240070343, 0.916973888874054, 0.9052722156047821, 0.920368492603302, 0.9058992564678192, 0.9341210722923279, 0.8840175569057465, 0.9701575934886932, 1.0351560711860657, 0.9579101502895355, 0.9151038825511932, 0.9816426634788513, 0.9277414679527283, 0.9168661832809448, 0.942892849445343, 0.8777931332588196, 0.964475005865097, 0.9697690606117249, 0.915448933839798, 1.0157723426818848, 0.9201433956623077, 0.8968108594417572, 0.9089641571044922, 0.9139889180660248, 0.875595360994339, 0.9297171831130981, 0.9148533046245575, 0.8946547210216522, 0.9449913799762726, 0.9782754182815552, 0.9536659121513367, 1.0049880146980286, 0.8683167099952698, 0.9462282359600067, 0.9435549378395081, 1.0199213922023773, 1.0492641925811768, 0.8349145948886871, 0.9751692712306976, 0.9527368545532227, 0.9266692399978638, 0.9176630675792694, 0.9002287983894348, 0.9183724224567413, 0.9571346938610077, 0.8772624433040619, 0.9552747309207916, 0.9313621819019318, 0.8851045072078705, 0.9160938858985901, 0.948438972234726, 0.954067587852478, 0.9657573103904724, 0.9202330410480499, 0.8974016606807709, 0.998444676399231, 1.0134104490280151, 0.9290164709091187, 0.8774038255214691], 'val_losses': [3.3344616889953613, 3.3344616889953613, 3.2517032623291016, 3.077746868133545, 2.9156360626220703, 2.7647864818573, 2.614867687225342, 2.4759976863861084, 2.346205711364746, 2.2219057083129883, 2.109961986541748, 2.0073792934417725, 1.9195432662963867, 1.8431737422943115, 1.7763615846633911, 1.7245522737503052, 1.6831943988800049, 1.6502611637115479, 1.6244912147521973, 1.599452257156372, 1.5738335847854614, 1.545048475265503, 1.513236165046692, 1.4765005111694336, 1.4383238554000854, 1.3995318412780762, 1.3587021827697754, 1.3179540634155273, 1.2785402536392212, 1.2385642528533936, 1.1994863748550415, 1.1619446277618408, 1.125349760055542, 1.0908212661743164, 1.056013822555542, 1.0224759578704834, 0.990043044090271, 0.9576288461685181, 0.9281672239303589, 0.9001845121383667, 0.8738377094268799, 0.8493149876594543, 0.827814519405365, 0.8091368675231934, 0.7926616072654724, 0.7803087830543518, 0.7717595100402832, 0.7665534019470215, 0.7665435075759888, 0.7665852904319763, 0.7674375176429749, 0.7676032781600952, 0.7662648558616638, 0.7637144327163696, 0.7609797716140747, 0.7578979730606079, 0.7566111087799072, 0.753166675567627, 0.7507917881011963, 0.7499732375144958, 0.7496402263641357, 0.7495293021202087, 0.7487668991088867, 0.7480113506317139, 0.7461437582969666, 0.7452573180198669, 0.7425568103790283, 0.739757239818573, 0.739075243473053, 0.7389534115791321, 0.737337589263916, 0.7360751032829285, 0.734330415725708, 0.7349229454994202, 0.7362556457519531, 0.7375779747962952, 0.738240122795105, 0.7374663949012756, 0.7365965843200684, 0.7349967956542969, 0.732818603515625, 0.7304001450538635, 0.7289432883262634, 0.7277589440345764, 0.7269842624664307, 0.7262393236160278, 0.7255032062530518, 0.7248853445053101, 0.7246812582015991, 0.724419891834259, 0.7243795394897461, 0.724196195602417, 0.7245603799819946, 0.7257227897644043, 0.7269269824028015, 0.7269927263259888, 0.7260544896125793, 0.7254922389984131, 0.7252084612846375, 0.723867654800415, 0.7232064008712769, 0.7219605445861816, 0.7209317088127136, 0.720718264579773, 0.7207714319229126, 0.7211665511131287, 0.7215306162834167, 0.7223694324493408, 0.7230202555656433, 0.7236648797988892, 0.7238031625747681, 0.7232109308242798, 0.7223544120788574, 0.7228653430938721, 0.7222896814346313, 0.7216327786445618, 0.7218639254570007, 0.7220249176025391, 0.722217321395874, 0.7222201824188232, 0.7220159769058228, 0.7214030027389526, 0.720964789390564, 0.7201918959617615, 0.7196559309959412, 0.7194648385047913, 0.7197635173797607, 0.7200112342834473, 0.7193606495857239, 0.7191323041915894, 0.7189196944236755, 0.7187378406524658, 0.7181259393692017, 0.717606246471405, 0.7172527313232422, 0.7164517641067505, 0.715647280216217, 0.7154649496078491, 0.7156410217285156, 0.715450644493103, 0.7150487899780273, 0.7146409749984741, 0.7145587205886841, 0.714511513710022, 0.7145853042602539, 0.7147772312164307, 0.7147926688194275, 0.7152199149131775, 0.7154284715652466, 0.7154147028923035, 0.7152714729309082, 0.7149582505226135, 0.7148310542106628, 0.714792788028717, 0.7147526741027832, 0.714778482913971, 0.7147341370582581, 0.7146850824356079, 0.714657187461853, 0.7146613597869873, 0.7146592736244202, 0.7145155668258667, 0.7147268652915955, 0.714678168296814, 0.7147012948989868, 0.714747428894043, 0.7147203087806702, 0.7147364020347595, 0.7147912979125977, 0.7146857380867004, 0.7148159742355347, 0.7149190902709961, 0.7149361371994019, 0.7149112224578857, 0.7149928212165833, 0.7149118781089783, 0.7149527072906494, 0.7149868011474609, 0.7149893045425415, 0.7149945497512817, 0.7149798274040222, 0.71503084897995, 0.7149498462677002, 0.7149373888969421, 0.7149502038955688, 0.7149521112442017, 0.7149305939674377, 0.7149434089660645, 0.7149264812469482, 0.714937150478363, 0.7149376273155212, 0.7149083614349365, 0.7149083614349365, 0.7149083614349365, 0.7149083614349365], 'val_acc': [0.390625, 0.390625, 0.4375, 0.453125, 0.5, 0.546875, 0.5625, 0.578125, 0.59375, 0.59375, 0.640625, 0.640625, 0.671875, 0.65625, 0.65625, 0.65625, 0.65625, 0.71875, 0.71875, 0.6875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.703125, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.703125, 0.703125, 0.703125, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.1117113871734633e-05, 'batch_size': 128, 'epochs': 195, 'hidden_size': 179, 'dropout': 0.010246952705228908, 'weight_decay': 0.005740731422728093, 'label_smoothing': 0.06855255742677543, 'grad_clip_norm': 1.5581739504269645, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-10-13 22:44:01,808 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:44:01,808 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-10-13 22:44:01,808 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.416s
2025-10-13 22:44:01,893 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:44:01,893 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.085s
2025-10-13 22:44:01,893 - INFO - bo.run_bo - Recorded observation #40: hparams={'lr': 1.1117113871734633e-05, 'batch_size': np.int64(128), 'epochs': np.int64(195), 'hidden_size': np.int64(179), 'dropout': 0.010246952705228908, 'weight_decay': 0.005740731422728093, 'label_smoothing': 0.06855255742677543, 'grad_clip_norm': 1.5581739504269645, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:44:01,893 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 40: {'lr': 1.1117113871734633e-05, 'batch_size': np.int64(128), 'epochs': np.int64(195), 'hidden_size': np.int64(179), 'dropout': 0.010246952705228908, 'weight_decay': 0.005740731422728093, 'label_smoothing': 0.06855255742677543, 'grad_clip_norm': 1.5581739504269645, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:44:01,894 - INFO - bo.run_bo - üîçBO Trial 41: Using RF surrogate + Expected Improvement
2025-10-13 22:44:01,894 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:01,894 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 41 (NaN monitoring active)
2025-10-13 22:44:01,894 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:01,894 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:01,894 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006087344691476418, 'batch_size': 192, 'epochs': 146, 'hidden_size': 148, 'dropout': 0.10255687132940525, 'weight_decay': 0.009587074741198989, 'label_smoothing': 0.009723662824321423, 'grad_clip_norm': 0.7651558299324508, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:44:01,895 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006087344691476418, 'batch_size': 192, 'epochs': 146, 'hidden_size': 148, 'dropout': 0.10255687132940525, 'weight_decay': 0.009587074741198989, 'label_smoothing': 0.009723662824321423, 'grad_clip_norm': 0.7651558299324508, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-13 22:44:05,585 - INFO - _models.training_function_executor - Epoch 001/146 | train_loss=3.6461 | val_loss=2.5048 | val_acc=0.7188
2025-10-13 22:44:05,592 - INFO - _models.training_function_executor - Epoch 002/146 | train_loss=3.7314 | val_loss=39.8873 | val_acc=0.0938
2025-10-13 22:44:05,598 - INFO - _models.training_function_executor - Epoch 003/146 | train_loss=40.0221 | val_loss=39.8873 | val_acc=0.0938
2025-10-13 22:44:05,602 - INFO - _models.training_function_executor - Epoch 004/146 | train_loss=40.5119 | val_loss=21.6459 | val_acc=0.0938
2025-10-13 22:44:05,607 - INFO - _models.training_function_executor - Epoch 005/146 | train_loss=19.3900 | val_loss=10.3033 | val_acc=0.7188
2025-10-13 22:44:05,611 - INFO - _models.training_function_executor - Epoch 006/146 | train_loss=8.3236 | val_loss=8.2325 | val_acc=0.1875
2025-10-13 22:44:05,617 - INFO - _models.training_function_executor - Epoch 007/146 | train_loss=9.4979 | val_loss=7.2905 | val_acc=0.1875
2025-10-13 22:44:05,625 - INFO - _models.training_function_executor - Epoch 008/146 | train_loss=6.7466 | val_loss=1.5880 | val_acc=0.7188
2025-10-13 22:44:05,634 - INFO - _models.training_function_executor - Epoch 009/146 | train_loss=1.9723 | val_loss=1.7061 | val_acc=0.7188
2025-10-13 22:44:05,641 - INFO - _models.training_function_executor - Epoch 010/146 | train_loss=1.6918 | val_loss=1.0448 | val_acc=0.5938
2025-10-13 22:44:05,650 - INFO - _models.training_function_executor - Epoch 011/146 | train_loss=1.5721 | val_loss=0.9482 | val_acc=0.6562
2025-10-13 22:44:05,656 - INFO - _models.training_function_executor - Epoch 012/146 | train_loss=1.2120 | val_loss=1.0415 | val_acc=0.7188
2025-10-13 22:44:05,660 - INFO - _models.training_function_executor - Epoch 013/146 | train_loss=1.1539 | val_loss=0.7004 | val_acc=0.6250
2025-10-13 22:44:05,665 - INFO - _models.training_function_executor - Epoch 014/146 | train_loss=0.9442 | val_loss=0.6811 | val_acc=0.7188
2025-10-13 22:44:05,671 - INFO - _models.training_function_executor - Epoch 015/146 | train_loss=0.7734 | val_loss=0.7361 | val_acc=0.7188
2025-10-13 22:44:05,676 - INFO - _models.training_function_executor - Epoch 016/146 | train_loss=0.7908 | val_loss=0.6547 | val_acc=0.7188
2025-10-13 22:44:05,680 - INFO - _models.training_function_executor - Epoch 017/146 | train_loss=0.7219 | val_loss=0.7125 | val_acc=0.7188
2025-10-13 22:44:05,686 - INFO - _models.training_function_executor - Epoch 018/146 | train_loss=0.8103 | val_loss=0.6256 | val_acc=0.7188
2025-10-13 22:44:05,690 - INFO - _models.training_function_executor - Epoch 019/146 | train_loss=0.7432 | val_loss=0.7446 | val_acc=0.6719
2025-10-13 22:44:05,695 - INFO - _models.training_function_executor - Epoch 020/146 | train_loss=0.7938 | val_loss=0.6423 | val_acc=0.7188
2025-10-13 22:44:05,699 - INFO - _models.training_function_executor - Epoch 021/146 | train_loss=0.7194 | val_loss=0.6115 | val_acc=0.7188
2025-10-13 22:44:05,703 - INFO - _models.training_function_executor - Epoch 022/146 | train_loss=0.6906 | val_loss=0.6188 | val_acc=0.7188
2025-10-13 22:44:05,707 - INFO - _models.training_function_executor - Epoch 023/146 | train_loss=0.6921 | val_loss=0.6129 | val_acc=0.7188
2025-10-13 22:44:05,711 - INFO - _models.training_function_executor - Epoch 024/146 | train_loss=0.6918 | val_loss=0.6292 | val_acc=0.7188
2025-10-13 22:44:05,716 - INFO - _models.training_function_executor - Epoch 025/146 | train_loss=0.6901 | val_loss=0.6292 | val_acc=0.7188
2025-10-13 22:44:05,720 - INFO - _models.training_function_executor - Epoch 026/146 | train_loss=0.6840 | val_loss=0.6365 | val_acc=0.7188
2025-10-13 22:44:05,724 - INFO - _models.training_function_executor - Epoch 027/146 | train_loss=0.6788 | val_loss=0.6246 | val_acc=0.7188
2025-10-13 22:44:05,728 - INFO - _models.training_function_executor - Epoch 028/146 | train_loss=0.6814 | val_loss=0.6131 | val_acc=0.7188
2025-10-13 22:44:05,732 - INFO - _models.training_function_executor - Epoch 029/146 | train_loss=0.6736 | val_loss=0.6021 | val_acc=0.7188
2025-10-13 22:44:05,736 - INFO - _models.training_function_executor - Epoch 030/146 | train_loss=0.6745 | val_loss=0.6114 | val_acc=0.7188
2025-10-13 22:44:05,740 - INFO - _models.training_function_executor - Epoch 031/146 | train_loss=0.6757 | val_loss=0.6121 | val_acc=0.7188
2025-10-13 22:44:05,745 - INFO - _models.training_function_executor - Epoch 032/146 | train_loss=0.6757 | val_loss=0.6090 | val_acc=0.7188
2025-10-13 22:44:05,749 - INFO - _models.training_function_executor - Epoch 033/146 | train_loss=0.6748 | val_loss=0.6351 | val_acc=0.7188
2025-10-13 22:44:05,753 - INFO - _models.training_function_executor - Epoch 034/146 | train_loss=0.6936 | val_loss=0.6283 | val_acc=0.7188
2025-10-13 22:44:05,757 - INFO - _models.training_function_executor - Epoch 035/146 | train_loss=0.6800 | val_loss=0.6972 | val_acc=0.7188
2025-10-13 22:44:05,761 - INFO - _models.training_function_executor - Epoch 036/146 | train_loss=0.7253 | val_loss=0.6172 | val_acc=0.7188
2025-10-13 22:44:05,765 - INFO - _models.training_function_executor - Epoch 037/146 | train_loss=0.6649 | val_loss=0.6071 | val_acc=0.7188
2025-10-13 22:44:05,769 - INFO - _models.training_function_executor - Epoch 038/146 | train_loss=0.6809 | val_loss=0.6167 | val_acc=0.7188
2025-10-13 22:44:05,774 - INFO - _models.training_function_executor - Epoch 039/146 | train_loss=0.6846 | val_loss=0.6228 | val_acc=0.7188
2025-10-13 22:44:05,778 - INFO - _models.training_function_executor - Epoch 040/146 | train_loss=0.6779 | val_loss=0.6301 | val_acc=0.7188
2025-10-13 22:44:05,782 - INFO - _models.training_function_executor - Epoch 041/146 | train_loss=0.6882 | val_loss=0.6156 | val_acc=0.7188
2025-10-13 22:44:05,787 - INFO - _models.training_function_executor - Epoch 042/146 | train_loss=0.6702 | val_loss=0.6127 | val_acc=0.7188
2025-10-13 22:44:05,791 - INFO - _models.training_function_executor - Epoch 043/146 | train_loss=0.6701 | val_loss=0.6064 | val_acc=0.7188
2025-10-13 22:44:05,795 - INFO - _models.training_function_executor - Epoch 044/146 | train_loss=0.6579 | val_loss=0.6325 | val_acc=0.7188
2025-10-13 22:44:05,799 - INFO - _models.training_function_executor - Epoch 045/146 | train_loss=0.6672 | val_loss=0.6331 | val_acc=0.7188
2025-10-13 22:44:05,803 - INFO - _models.training_function_executor - Epoch 046/146 | train_loss=0.6832 | val_loss=0.6309 | val_acc=0.7188
2025-10-13 22:44:05,807 - INFO - _models.training_function_executor - Epoch 047/146 | train_loss=0.6774 | val_loss=0.6233 | val_acc=0.7188
2025-10-13 22:44:05,811 - INFO - _models.training_function_executor - Epoch 048/146 | train_loss=0.6688 | val_loss=0.6153 | val_acc=0.7188
2025-10-13 22:44:05,814 - INFO - _models.training_function_executor - Epoch 049/146 | train_loss=0.6670 | val_loss=0.6025 | val_acc=0.7188
2025-10-13 22:44:05,818 - INFO - _models.training_function_executor - Epoch 050/146 | train_loss=0.6827 | val_loss=0.5982 | val_acc=0.7188
2025-10-13 22:44:05,822 - INFO - _models.training_function_executor - Epoch 051/146 | train_loss=0.6668 | val_loss=0.6085 | val_acc=0.7188
2025-10-13 22:44:05,826 - INFO - _models.training_function_executor - Epoch 052/146 | train_loss=0.6723 | val_loss=0.6322 | val_acc=0.7188
2025-10-13 22:44:05,830 - INFO - _models.training_function_executor - Epoch 053/146 | train_loss=0.6685 | val_loss=0.6351 | val_acc=0.7188
2025-10-13 22:44:05,834 - INFO - _models.training_function_executor - Epoch 054/146 | train_loss=0.6710 | val_loss=0.6394 | val_acc=0.7188
2025-10-13 22:44:05,838 - INFO - _models.training_function_executor - Epoch 055/146 | train_loss=0.6817 | val_loss=0.6300 | val_acc=0.7188
2025-10-13 22:44:05,841 - INFO - _models.training_function_executor - Epoch 056/146 | train_loss=0.6827 | val_loss=0.6236 | val_acc=0.7188
2025-10-13 22:44:05,845 - INFO - _models.training_function_executor - Epoch 057/146 | train_loss=0.6550 | val_loss=0.6250 | val_acc=0.7188
2025-10-13 22:44:05,849 - INFO - _models.training_function_executor - Epoch 058/146 | train_loss=0.6644 | val_loss=0.6132 | val_acc=0.7188
2025-10-13 22:44:05,854 - INFO - _models.training_function_executor - Epoch 059/146 | train_loss=0.6570 | val_loss=0.6272 | val_acc=0.7188
2025-10-13 22:44:05,858 - INFO - _models.training_function_executor - Epoch 060/146 | train_loss=0.6664 | val_loss=0.6378 | val_acc=0.7188
2025-10-13 22:44:05,861 - INFO - _models.training_function_executor - Epoch 061/146 | train_loss=0.6650 | val_loss=0.6304 | val_acc=0.7188
2025-10-13 22:44:05,865 - INFO - _models.training_function_executor - Epoch 062/146 | train_loss=0.6753 | val_loss=0.6197 | val_acc=0.7188
2025-10-13 22:44:05,869 - INFO - _models.training_function_executor - Epoch 063/146 | train_loss=0.6645 | val_loss=0.6221 | val_acc=0.7188
2025-10-13 22:44:05,874 - INFO - _models.training_function_executor - Epoch 064/146 | train_loss=0.6559 | val_loss=0.6105 | val_acc=0.7188
2025-10-13 22:44:05,878 - INFO - _models.training_function_executor - Epoch 065/146 | train_loss=0.6538 | val_loss=0.6131 | val_acc=0.7188
2025-10-13 22:44:05,882 - INFO - _models.training_function_executor - Epoch 066/146 | train_loss=0.6712 | val_loss=0.6250 | val_acc=0.7188
2025-10-13 22:44:05,886 - INFO - _models.training_function_executor - Epoch 067/146 | train_loss=0.6470 | val_loss=0.6266 | val_acc=0.7188
2025-10-13 22:44:05,890 - INFO - _models.training_function_executor - Epoch 068/146 | train_loss=0.6564 | val_loss=0.6194 | val_acc=0.7188
2025-10-13 22:44:05,894 - INFO - _models.training_function_executor - Epoch 069/146 | train_loss=0.6409 | val_loss=0.6227 | val_acc=0.7188
2025-10-13 22:44:05,898 - INFO - _models.training_function_executor - Epoch 070/146 | train_loss=0.6536 | val_loss=0.6232 | val_acc=0.7188
2025-10-13 22:44:05,902 - INFO - _models.training_function_executor - Epoch 071/146 | train_loss=0.6604 | val_loss=0.6180 | val_acc=0.7188
2025-10-13 22:44:05,906 - INFO - _models.training_function_executor - Epoch 072/146 | train_loss=0.6505 | val_loss=0.6076 | val_acc=0.7188
2025-10-13 22:44:05,910 - INFO - _models.training_function_executor - Epoch 073/146 | train_loss=0.6560 | val_loss=0.6061 | val_acc=0.7188
2025-10-13 22:44:05,914 - INFO - _models.training_function_executor - Epoch 074/146 | train_loss=0.6825 | val_loss=0.6074 | val_acc=0.7188
2025-10-13 22:44:05,918 - INFO - _models.training_function_executor - Epoch 075/146 | train_loss=0.6660 | val_loss=0.6214 | val_acc=0.7188
2025-10-13 22:44:05,923 - INFO - _models.training_function_executor - Epoch 076/146 | train_loss=0.6749 | val_loss=0.6203 | val_acc=0.7188
2025-10-13 22:44:05,927 - INFO - _models.training_function_executor - Epoch 077/146 | train_loss=0.6542 | val_loss=0.6105 | val_acc=0.7188
2025-10-13 22:44:05,931 - INFO - _models.training_function_executor - Epoch 078/146 | train_loss=0.6545 | val_loss=0.6042 | val_acc=0.7188
2025-10-13 22:44:05,935 - INFO - _models.training_function_executor - Epoch 079/146 | train_loss=0.6553 | val_loss=0.6030 | val_acc=0.7188
2025-10-13 22:44:05,939 - INFO - _models.training_function_executor - Epoch 080/146 | train_loss=0.6609 | val_loss=0.6080 | val_acc=0.7188
2025-10-13 22:44:05,943 - INFO - _models.training_function_executor - Epoch 081/146 | train_loss=0.6564 | val_loss=0.6140 | val_acc=0.7188
2025-10-13 22:44:05,947 - INFO - _models.training_function_executor - Epoch 082/146 | train_loss=0.6489 | val_loss=0.6120 | val_acc=0.7188
2025-10-13 22:44:05,951 - INFO - _models.training_function_executor - Epoch 083/146 | train_loss=0.6467 | val_loss=0.6055 | val_acc=0.7188
2025-10-13 22:44:05,955 - INFO - _models.training_function_executor - Epoch 084/146 | train_loss=0.6516 | val_loss=0.6005 | val_acc=0.7188
2025-10-13 22:44:05,959 - INFO - _models.training_function_executor - Epoch 085/146 | train_loss=0.6514 | val_loss=0.6029 | val_acc=0.7188
2025-10-13 22:44:05,963 - INFO - _models.training_function_executor - Epoch 086/146 | train_loss=0.6607 | val_loss=0.6081 | val_acc=0.7188
2025-10-13 22:44:05,967 - INFO - _models.training_function_executor - Epoch 087/146 | train_loss=0.6577 | val_loss=0.6132 | val_acc=0.7188
2025-10-13 22:44:05,971 - INFO - _models.training_function_executor - Epoch 088/146 | train_loss=0.6563 | val_loss=0.6121 | val_acc=0.7188
2025-10-13 22:44:05,975 - INFO - _models.training_function_executor - Epoch 089/146 | train_loss=0.6391 | val_loss=0.6074 | val_acc=0.7188
2025-10-13 22:44:05,979 - INFO - _models.training_function_executor - Epoch 090/146 | train_loss=0.6467 | val_loss=0.6007 | val_acc=0.7188
2025-10-13 22:44:05,983 - INFO - _models.training_function_executor - Epoch 091/146 | train_loss=0.6510 | val_loss=0.6030 | val_acc=0.7188
2025-10-13 22:44:05,988 - INFO - _models.training_function_executor - Epoch 092/146 | train_loss=0.6544 | val_loss=0.6085 | val_acc=0.7188
2025-10-13 22:44:05,992 - INFO - _models.training_function_executor - Epoch 093/146 | train_loss=0.6486 | val_loss=0.6129 | val_acc=0.7188
2025-10-13 22:44:05,996 - INFO - _models.training_function_executor - Epoch 094/146 | train_loss=0.6442 | val_loss=0.6125 | val_acc=0.7188
2025-10-13 22:44:06,001 - INFO - _models.training_function_executor - Epoch 095/146 | train_loss=0.6536 | val_loss=0.6072 | val_acc=0.7188
2025-10-13 22:44:06,005 - INFO - _models.training_function_executor - Epoch 096/146 | train_loss=0.6527 | val_loss=0.5965 | val_acc=0.7188
2025-10-13 22:44:06,009 - INFO - _models.training_function_executor - Epoch 097/146 | train_loss=0.6409 | val_loss=0.5946 | val_acc=0.7188
2025-10-13 22:44:06,013 - INFO - _models.training_function_executor - Epoch 098/146 | train_loss=0.6423 | val_loss=0.5969 | val_acc=0.7188
2025-10-13 22:44:06,017 - INFO - _models.training_function_executor - Epoch 099/146 | train_loss=0.6367 | val_loss=0.5992 | val_acc=0.7188
2025-10-13 22:44:06,021 - INFO - _models.training_function_executor - Epoch 100/146 | train_loss=0.6483 | val_loss=0.6040 | val_acc=0.7188
2025-10-13 22:44:06,025 - INFO - _models.training_function_executor - Epoch 101/146 | train_loss=0.6416 | val_loss=0.6084 | val_acc=0.7188
2025-10-13 22:44:06,029 - INFO - _models.training_function_executor - Epoch 102/146 | train_loss=0.6558 | val_loss=0.6073 | val_acc=0.7188
2025-10-13 22:44:06,033 - INFO - _models.training_function_executor - Epoch 103/146 | train_loss=0.6377 | val_loss=0.6023 | val_acc=0.7188
2025-10-13 22:44:06,037 - INFO - _models.training_function_executor - Epoch 104/146 | train_loss=0.6449 | val_loss=0.5986 | val_acc=0.7188
2025-10-13 22:44:06,041 - INFO - _models.training_function_executor - Epoch 105/146 | train_loss=0.6481 | val_loss=0.5964 | val_acc=0.7188
2025-10-13 22:44:06,045 - INFO - _models.training_function_executor - Epoch 106/146 | train_loss=0.6378 | val_loss=0.5958 | val_acc=0.7188
2025-10-13 22:44:06,049 - INFO - _models.training_function_executor - Epoch 107/146 | train_loss=0.6579 | val_loss=0.5989 | val_acc=0.7188
2025-10-13 22:44:06,053 - INFO - _models.training_function_executor - Epoch 108/146 | train_loss=0.6461 | val_loss=0.6005 | val_acc=0.7188
2025-10-13 22:44:06,057 - INFO - _models.training_function_executor - Epoch 109/146 | train_loss=0.6570 | val_loss=0.6025 | val_acc=0.7188
2025-10-13 22:44:06,061 - INFO - _models.training_function_executor - Epoch 110/146 | train_loss=0.6474 | val_loss=0.6046 | val_acc=0.7188
2025-10-13 22:44:06,065 - INFO - _models.training_function_executor - Epoch 111/146 | train_loss=0.6557 | val_loss=0.6076 | val_acc=0.7188
2025-10-13 22:44:06,070 - INFO - _models.training_function_executor - Epoch 112/146 | train_loss=0.6465 | val_loss=0.6099 | val_acc=0.7188
2025-10-13 22:44:06,074 - INFO - _models.training_function_executor - Epoch 113/146 | train_loss=0.6443 | val_loss=0.6095 | val_acc=0.7188
2025-10-13 22:44:06,078 - INFO - _models.training_function_executor - Epoch 114/146 | train_loss=0.6498 | val_loss=0.6083 | val_acc=0.7188
2025-10-13 22:44:06,082 - INFO - _models.training_function_executor - Epoch 115/146 | train_loss=0.6381 | val_loss=0.6054 | val_acc=0.7188
2025-10-13 22:44:06,086 - INFO - _models.training_function_executor - Epoch 116/146 | train_loss=0.6446 | val_loss=0.6027 | val_acc=0.7188
2025-10-13 22:44:06,091 - INFO - _models.training_function_executor - Epoch 117/146 | train_loss=0.6448 | val_loss=0.6023 | val_acc=0.7188
2025-10-13 22:44:06,094 - INFO - _models.training_function_executor - Epoch 118/146 | train_loss=0.6570 | val_loss=0.6025 | val_acc=0.7188
2025-10-13 22:44:06,098 - INFO - _models.training_function_executor - Epoch 119/146 | train_loss=0.6315 | val_loss=0.6031 | val_acc=0.7188
2025-10-13 22:44:06,103 - INFO - _models.training_function_executor - Epoch 120/146 | train_loss=0.6460 | val_loss=0.6039 | val_acc=0.7188
2025-10-13 22:44:06,106 - INFO - _models.training_function_executor - Epoch 121/146 | train_loss=0.6350 | val_loss=0.6045 | val_acc=0.7188
2025-10-13 22:44:06,110 - INFO - _models.training_function_executor - Epoch 122/146 | train_loss=0.6272 | val_loss=0.6050 | val_acc=0.7188
2025-10-13 22:44:06,115 - INFO - _models.training_function_executor - Epoch 123/146 | train_loss=0.6553 | val_loss=0.6070 | val_acc=0.7188
2025-10-13 22:44:06,119 - INFO - _models.training_function_executor - Epoch 124/146 | train_loss=0.6439 | val_loss=0.6084 | val_acc=0.7188
2025-10-13 22:44:06,123 - INFO - _models.training_function_executor - Epoch 125/146 | train_loss=0.6473 | val_loss=0.6096 | val_acc=0.7188
2025-10-13 22:44:06,127 - INFO - _models.training_function_executor - Epoch 126/146 | train_loss=0.6391 | val_loss=0.6096 | val_acc=0.7188
2025-10-13 22:44:06,132 - INFO - _models.training_function_executor - Epoch 127/146 | train_loss=0.6424 | val_loss=0.6080 | val_acc=0.7188
2025-10-13 22:44:06,136 - INFO - _models.training_function_executor - Epoch 128/146 | train_loss=0.6376 | val_loss=0.6065 | val_acc=0.7188
2025-10-13 22:44:06,140 - INFO - _models.training_function_executor - Epoch 129/146 | train_loss=0.6548 | val_loss=0.6051 | val_acc=0.7188
2025-10-13 22:44:06,144 - INFO - _models.training_function_executor - Epoch 130/146 | train_loss=0.6455 | val_loss=0.6045 | val_acc=0.7188
2025-10-13 22:44:06,148 - INFO - _models.training_function_executor - Epoch 131/146 | train_loss=0.6498 | val_loss=0.6036 | val_acc=0.7188
2025-10-13 22:44:06,152 - INFO - _models.training_function_executor - Epoch 132/146 | train_loss=0.6367 | val_loss=0.6034 | val_acc=0.7188
2025-10-13 22:44:06,156 - INFO - _models.training_function_executor - Epoch 133/146 | train_loss=0.6324 | val_loss=0.6029 | val_acc=0.7188
2025-10-13 22:44:06,161 - INFO - _models.training_function_executor - Epoch 134/146 | train_loss=0.6340 | val_loss=0.6024 | val_acc=0.7188
2025-10-13 22:44:06,165 - INFO - _models.training_function_executor - Epoch 135/146 | train_loss=0.6380 | val_loss=0.6025 | val_acc=0.7188
2025-10-13 22:44:06,170 - INFO - _models.training_function_executor - Epoch 136/146 | train_loss=0.6316 | val_loss=0.6023 | val_acc=0.7188
2025-10-13 22:44:06,175 - INFO - _models.training_function_executor - Epoch 137/146 | train_loss=0.6500 | val_loss=0.6020 | val_acc=0.7188
2025-10-13 22:44:06,180 - INFO - _models.training_function_executor - Epoch 138/146 | train_loss=0.6328 | val_loss=0.6020 | val_acc=0.7188
2025-10-13 22:44:06,184 - INFO - _models.training_function_executor - Epoch 139/146 | train_loss=0.6343 | val_loss=0.6019 | val_acc=0.7188
2025-10-13 22:44:06,188 - INFO - _models.training_function_executor - Epoch 140/146 | train_loss=0.6470 | val_loss=0.6020 | val_acc=0.7188
2025-10-13 22:44:06,194 - INFO - _models.training_function_executor - Epoch 141/146 | train_loss=0.6491 | val_loss=0.6021 | val_acc=0.7188
2025-10-13 22:44:06,198 - INFO - _models.training_function_executor - Epoch 142/146 | train_loss=0.6343 | val_loss=0.6019 | val_acc=0.7188
2025-10-13 22:44:06,202 - INFO - _models.training_function_executor - Epoch 143/146 | train_loss=0.6253 | val_loss=0.6019 | val_acc=0.7188
2025-10-13 22:44:06,206 - INFO - _models.training_function_executor - Epoch 144/146 | train_loss=0.6354 | val_loss=0.6018 | val_acc=0.7188
2025-10-13 22:44:06,210 - INFO - _models.training_function_executor - Epoch 145/146 | train_loss=0.6310 | val_loss=0.6019 | val_acc=0.7188
2025-10-13 22:44:06,214 - INFO - _models.training_function_executor - Epoch 146/146 | train_loss=0.6285 | val_loss=0.6019 | val_acc=0.7188
2025-10-13 22:44:07,077 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-10-13 22:44:07,077 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [3.646077871322632, 3.731383800506592, 40.02208709716797, 40.51186943054199, 19.389962434768677, 8.323584914207458, 9.497882604598999, 6.74655145406723, 1.972334772348404, 1.6917771100997925, 1.5721299052238464, 1.211952731013298, 1.153908133506775, 0.9441790878772736, 0.77344611287117, 0.7908242344856262, 0.7218980193138123, 0.8103205114603043, 0.7431923449039459, 0.7937769740819931, 0.7194434404373169, 0.6906367391347885, 0.6921233683824539, 0.691818431019783, 0.6900788098573685, 0.6839900314807892, 0.6788441389799118, 0.6814210712909698, 0.6735699325799942, 0.6744522601366043, 0.6757194250822067, 0.6757123172283173, 0.6747852265834808, 0.6936163306236267, 0.6799721121788025, 0.725258931517601, 0.6649463027715683, 0.6809051632881165, 0.6846356689929962, 0.6779418140649796, 0.6882018744945526, 0.6701530069112778, 0.6701412796974182, 0.6579450517892838, 0.6671969592571259, 0.683238223195076, 0.6774241179227829, 0.6688296794891357, 0.6670138388872147, 0.6826830953359604, 0.6668184995651245, 0.6722948551177979, 0.6684978306293488, 0.6709702759981155, 0.6816503703594208, 0.6826604753732681, 0.6550046056509018, 0.6643882393836975, 0.657039999961853, 0.6664072871208191, 0.6649680435657501, 0.6752761751413345, 0.664505124092102, 0.6559421867132187, 0.6537812054157257, 0.6712080985307693, 0.6470364928245544, 0.6563755422830582, 0.6408505141735077, 0.6536297798156738, 0.6604351997375488, 0.6504548788070679, 0.6560441851615906, 0.6825385689735413, 0.6659785956144333, 0.6748550981283188, 0.6541600227355957, 0.6545019149780273, 0.6552650481462479, 0.6608560085296631, 0.6563573181629181, 0.6488864123821259, 0.6467410027980804, 0.6515540182590485, 0.6514077484607697, 0.6606906652450562, 0.6576593816280365, 0.6563128679990768, 0.6390894055366516, 0.6467035710811615, 0.651035726070404, 0.6544467955827713, 0.6485603898763657, 0.6442168653011322, 0.6536139398813248, 0.6527254283428192, 0.6408943980932236, 0.642328143119812, 0.6366683095693588, 0.648297131061554, 0.6415968984365463, 0.6557858735322952, 0.6377174407243729, 0.6448924690485001, 0.6480824053287506, 0.6377818435430527, 0.6579268127679825, 0.6461309045553207, 0.6570283621549606, 0.6473649740219116, 0.655730813741684, 0.6464672088623047, 0.6442811191082001, 0.6498200744390488, 0.6380981355905533, 0.6446076035499573, 0.6447900161147118, 0.6569859236478806, 0.6315142810344696, 0.6460167914628983, 0.6350036412477493, 0.6271893978118896, 0.6553270667791367, 0.643862396478653, 0.6473155468702316, 0.639132559299469, 0.6423701792955399, 0.6375753879547119, 0.6548461467027664, 0.6455088108778, 0.6497949957847595, 0.6367165595293045, 0.6324422359466553, 0.6339727342128754, 0.637964054942131, 0.6316441744565964, 0.6499912440776825, 0.6328068524599075, 0.6343467235565186, 0.6470272839069366, 0.649064764380455, 0.6343272477388382, 0.6253249496221542, 0.6353684663772583, 0.6309779137372971, 0.6284933388233185], 'val_losses': [2.5048229694366455, 39.887264251708984, 39.887264251708984, 21.64590072631836, 10.303345680236816, 8.232528686523438, 7.29048490524292, 1.5879968404769897, 1.7061288356781006, 1.044824242591858, 0.9481655955314636, 1.0414596796035767, 0.7004473209381104, 0.6810531616210938, 0.7361086010932922, 0.6546843647956848, 0.7125078439712524, 0.6256334185600281, 0.7446205019950867, 0.6423441767692566, 0.6115209460258484, 0.61875981092453, 0.6128656268119812, 0.6291700601577759, 0.6292293071746826, 0.6365116834640503, 0.6245737671852112, 0.6130789518356323, 0.602117121219635, 0.6113714575767517, 0.6121066808700562, 0.6089891791343689, 0.6351406574249268, 0.6283317804336548, 0.6971567869186401, 0.6171603202819824, 0.6071478128433228, 0.616726279258728, 0.6227574348449707, 0.6301000714302063, 0.615607738494873, 0.6126593351364136, 0.6063504219055176, 0.6324964165687561, 0.6331379413604736, 0.6308523416519165, 0.6232515573501587, 0.615270733833313, 0.6024916768074036, 0.5981534123420715, 0.6084715723991394, 0.6321941018104553, 0.6351445913314819, 0.6393978595733643, 0.6300114989280701, 0.6236041188240051, 0.6249525547027588, 0.6131756901741028, 0.6272072792053223, 0.6378498673439026, 0.6303781270980835, 0.6197291016578674, 0.6221132278442383, 0.6104938387870789, 0.6131401658058167, 0.6249749064445496, 0.6265723705291748, 0.6193527579307556, 0.622675359249115, 0.6232214570045471, 0.6180137395858765, 0.6075935959815979, 0.6060900688171387, 0.6073893904685974, 0.6213513612747192, 0.6203238368034363, 0.6104581356048584, 0.6041718125343323, 0.6029706597328186, 0.6079901456832886, 0.6140221357345581, 0.6120293736457825, 0.6055018901824951, 0.6005384922027588, 0.6028521656990051, 0.608064591884613, 0.613208532333374, 0.6121291518211365, 0.6073851585388184, 0.6006555557250977, 0.6029852032661438, 0.6084993481636047, 0.612903892993927, 0.6125463843345642, 0.6072273254394531, 0.59647536277771, 0.5945757627487183, 0.5968737006187439, 0.599219024181366, 0.6040230393409729, 0.6084022521972656, 0.6073424220085144, 0.6022560596466064, 0.5985519886016846, 0.5963943600654602, 0.5957946181297302, 0.598863422870636, 0.6004827618598938, 0.6024523973464966, 0.6046274304389954, 0.6076211333274841, 0.6099250316619873, 0.6095279455184937, 0.6082960367202759, 0.6053622961044312, 0.6026839017868042, 0.6023393869400024, 0.6024883389472961, 0.6031123399734497, 0.6038548350334167, 0.6045492887496948, 0.6049948334693909, 0.6070336699485779, 0.6084370017051697, 0.6096040606498718, 0.6095526814460754, 0.6080362796783447, 0.6064538359642029, 0.6051192879676819, 0.6045464277267456, 0.6035961508750916, 0.6034244298934937, 0.602944016456604, 0.6024294495582581, 0.6024694442749023, 0.6022886037826538, 0.6020323634147644, 0.6019837856292725, 0.601859986782074, 0.6020216345787048, 0.6020548939704895, 0.6019262671470642, 0.6019344329833984, 0.6018445491790771, 0.6018597483634949, 0.6018674373626709], 'val_acc': [0.71875, 0.09375, 0.09375, 0.09375, 0.71875, 0.1875, 0.1875, 0.71875, 0.71875, 0.59375, 0.65625, 0.71875, 0.625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.671875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006087344691476418, 'batch_size': 192, 'epochs': 146, 'hidden_size': 148, 'dropout': 0.10255687132940525, 'weight_decay': 0.009587074741198989, 'label_smoothing': 0.009723662824321423, 'grad_clip_norm': 0.7651558299324508, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-10-13 22:44:07,078 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:44:07,078 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-10-13 22:44:07,078 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.184s
2025-10-13 22:44:07,164 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:44:07,164 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.086s
2025-10-13 22:44:07,164 - INFO - bo.run_bo - Recorded observation #41: hparams={'lr': 0.006087344691476418, 'batch_size': np.int64(192), 'epochs': np.int64(146), 'hidden_size': np.int64(148), 'dropout': 0.10255687132940525, 'weight_decay': 0.009587074741198989, 'label_smoothing': 0.009723662824321423, 'grad_clip_norm': 0.7651558299324508, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:44:07,164 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 41: {'lr': 0.006087344691476418, 'batch_size': np.int64(192), 'epochs': np.int64(146), 'hidden_size': np.int64(148), 'dropout': 0.10255687132940525, 'weight_decay': 0.009587074741198989, 'label_smoothing': 0.009723662824321423, 'grad_clip_norm': 0.7651558299324508, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:44:07,165 - INFO - bo.run_bo - üîçBO Trial 42: Using RF surrogate + Expected Improvement
2025-10-13 22:44:07,165 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:07,165 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 42 (NaN monitoring active)
2025-10-13 22:44:07,165 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:07,165 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:07,165 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.001325606340567e-05, 'batch_size': 192, 'epochs': 47, 'hidden_size': 174, 'dropout': 0.13335737860564614, 'weight_decay': 3.840268119736549e-05, 'label_smoothing': 0.004379926555165726, 'grad_clip_norm': 2.752307064869228, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:44:07,166 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.001325606340567e-05, 'batch_size': 192, 'epochs': 47, 'hidden_size': 174, 'dropout': 0.13335737860564614, 'weight_decay': 3.840268119736549e-05, 'label_smoothing': 0.004379926555165726, 'grad_clip_norm': 2.752307064869228, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:44:10,998 - INFO - _models.training_function_executor - Epoch 001/047 | train_loss=4.3790 | val_loss=2.6602 | val_acc=0.1250
2025-10-13 22:44:11,005 - INFO - _models.training_function_executor - Epoch 002/047 | train_loss=4.3617 | val_loss=2.6602 | val_acc=0.1250
2025-10-13 22:44:11,010 - INFO - _models.training_function_executor - Epoch 003/047 | train_loss=4.0470 | val_loss=2.5845 | val_acc=0.1250
2025-10-13 22:44:11,015 - INFO - _models.training_function_executor - Epoch 004/047 | train_loss=3.8653 | val_loss=2.4292 | val_acc=0.1719
2025-10-13 22:44:11,021 - INFO - _models.training_function_executor - Epoch 005/047 | train_loss=3.6561 | val_loss=2.2838 | val_acc=0.2344
2025-10-13 22:44:11,025 - INFO - _models.training_function_executor - Epoch 006/047 | train_loss=4.3075 | val_loss=2.1453 | val_acc=0.2500
2025-10-13 22:44:11,029 - INFO - _models.training_function_executor - Epoch 007/047 | train_loss=3.5122 | val_loss=2.0160 | val_acc=0.3125
2025-10-13 22:44:11,034 - INFO - _models.training_function_executor - Epoch 008/047 | train_loss=3.6538 | val_loss=1.8926 | val_acc=0.3281
2025-10-13 22:44:11,038 - INFO - _models.training_function_executor - Epoch 009/047 | train_loss=3.9337 | val_loss=1.7758 | val_acc=0.3438
2025-10-13 22:44:11,042 - INFO - _models.training_function_executor - Epoch 010/047 | train_loss=3.8647 | val_loss=1.6733 | val_acc=0.4531
2025-10-13 22:44:11,047 - INFO - _models.training_function_executor - Epoch 011/047 | train_loss=3.0342 | val_loss=1.5789 | val_acc=0.5000
2025-10-13 22:44:11,051 - INFO - _models.training_function_executor - Epoch 012/047 | train_loss=3.1858 | val_loss=1.4936 | val_acc=0.5312
2025-10-13 22:44:11,056 - INFO - _models.training_function_executor - Epoch 013/047 | train_loss=2.9619 | val_loss=1.4200 | val_acc=0.5312
2025-10-13 22:44:11,060 - INFO - _models.training_function_executor - Epoch 014/047 | train_loss=2.8503 | val_loss=1.3538 | val_acc=0.5625
2025-10-13 22:44:11,064 - INFO - _models.training_function_executor - Epoch 015/047 | train_loss=2.8045 | val_loss=1.2968 | val_acc=0.5938
2025-10-13 22:44:11,069 - INFO - _models.training_function_executor - Epoch 016/047 | train_loss=2.7017 | val_loss=1.2471 | val_acc=0.6094
2025-10-13 22:44:11,073 - INFO - _models.training_function_executor - Epoch 017/047 | train_loss=2.7556 | val_loss=1.2082 | val_acc=0.6094
2025-10-13 22:44:11,077 - INFO - _models.training_function_executor - Epoch 018/047 | train_loss=2.9059 | val_loss=1.1749 | val_acc=0.6562
2025-10-13 22:44:11,082 - INFO - _models.training_function_executor - Epoch 019/047 | train_loss=2.8937 | val_loss=1.1492 | val_acc=0.6719
2025-10-13 22:44:11,087 - INFO - _models.training_function_executor - Epoch 020/047 | train_loss=2.4827 | val_loss=1.1275 | val_acc=0.7188
2025-10-13 22:44:11,093 - INFO - _models.training_function_executor - Epoch 021/047 | train_loss=2.4920 | val_loss=1.1112 | val_acc=0.7344
2025-10-13 22:44:11,098 - INFO - _models.training_function_executor - Epoch 022/047 | train_loss=2.7554 | val_loss=1.1006 | val_acc=0.7344
2025-10-13 22:44:11,103 - INFO - _models.training_function_executor - Epoch 023/047 | train_loss=2.5880 | val_loss=1.0933 | val_acc=0.7188
2025-10-13 22:44:11,107 - INFO - _models.training_function_executor - Epoch 024/047 | train_loss=2.3207 | val_loss=1.0887 | val_acc=0.7344
2025-10-13 22:44:11,112 - INFO - _models.training_function_executor - Epoch 025/047 | train_loss=2.5227 | val_loss=1.0869 | val_acc=0.7188
2025-10-13 22:44:11,117 - INFO - _models.training_function_executor - Epoch 026/047 | train_loss=2.4552 | val_loss=1.0867 | val_acc=0.7188
2025-10-13 22:44:11,122 - INFO - _models.training_function_executor - Epoch 027/047 | train_loss=2.7998 | val_loss=1.0864 | val_acc=0.7188
2025-10-13 22:44:11,127 - INFO - _models.training_function_executor - Epoch 028/047 | train_loss=2.3936 | val_loss=1.0886 | val_acc=0.7188
2025-10-13 22:44:11,132 - INFO - _models.training_function_executor - Epoch 029/047 | train_loss=2.7818 | val_loss=1.0893 | val_acc=0.7188
2025-10-13 22:44:11,137 - INFO - _models.training_function_executor - Epoch 030/047 | train_loss=2.4856 | val_loss=1.0907 | val_acc=0.7188
2025-10-13 22:44:11,141 - INFO - _models.training_function_executor - Epoch 031/047 | train_loss=2.5545 | val_loss=1.0923 | val_acc=0.7188
2025-10-13 22:44:11,147 - INFO - _models.training_function_executor - Epoch 032/047 | train_loss=2.3239 | val_loss=1.0943 | val_acc=0.7188
2025-10-13 22:44:11,152 - INFO - _models.training_function_executor - Epoch 033/047 | train_loss=2.4260 | val_loss=1.0961 | val_acc=0.7188
2025-10-13 22:44:11,156 - INFO - _models.training_function_executor - Epoch 034/047 | train_loss=2.5793 | val_loss=1.0974 | val_acc=0.7188
2025-10-13 22:44:11,161 - INFO - _models.training_function_executor - Epoch 035/047 | train_loss=2.2006 | val_loss=1.0989 | val_acc=0.7188
2025-10-13 22:44:11,166 - INFO - _models.training_function_executor - Epoch 036/047 | train_loss=2.4770 | val_loss=1.1005 | val_acc=0.7188
2025-10-13 22:44:11,170 - INFO - _models.training_function_executor - Epoch 037/047 | train_loss=2.7421 | val_loss=1.1010 | val_acc=0.7188
2025-10-13 22:44:11,175 - INFO - _models.training_function_executor - Epoch 038/047 | train_loss=2.3317 | val_loss=1.1016 | val_acc=0.7188
2025-10-13 22:44:11,179 - INFO - _models.training_function_executor - Epoch 039/047 | train_loss=2.4698 | val_loss=1.1021 | val_acc=0.7188
2025-10-13 22:44:11,184 - INFO - _models.training_function_executor - Epoch 040/047 | train_loss=2.5002 | val_loss=1.1030 | val_acc=0.7188
2025-10-13 22:44:11,188 - INFO - _models.training_function_executor - Epoch 041/047 | train_loss=2.5156 | val_loss=1.1043 | val_acc=0.7188
2025-10-13 22:44:11,193 - INFO - _models.training_function_executor - Epoch 042/047 | train_loss=2.4192 | val_loss=1.1047 | val_acc=0.7188
2025-10-13 22:44:11,198 - INFO - _models.training_function_executor - Epoch 043/047 | train_loss=2.3301 | val_loss=1.1049 | val_acc=0.7188
2025-10-13 22:44:11,203 - INFO - _models.training_function_executor - Epoch 044/047 | train_loss=2.2485 | val_loss=1.1052 | val_acc=0.7188
2025-10-13 22:44:11,207 - INFO - _models.training_function_executor - Epoch 045/047 | train_loss=2.2090 | val_loss=1.1052 | val_acc=0.7188
2025-10-13 22:44:11,211 - INFO - _models.training_function_executor - Epoch 046/047 | train_loss=2.5869 | val_loss=1.1052 | val_acc=0.7188
2025-10-13 22:44:11,215 - INFO - _models.training_function_executor - Epoch 047/047 | train_loss=2.5212 | val_loss=1.1052 | val_acc=0.7188
2025-10-13 22:44:12,108 - INFO - _models.training_function_executor - Model: 31,845 parameters, 136.8KB storage
2025-10-13 22:44:12,108 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [4.379042983055115, 4.3616931438446045, 4.047012269496918, 3.865312159061432, 3.65605765581131, 4.307481348514557, 3.5122427344322205, 3.6537963151931763, 3.9336859583854675, 3.8647302389144897, 3.0342414379119873, 3.185787796974182, 2.961861491203308, 2.8503156900405884, 2.804512679576874, 2.701712667942047, 2.755604386329651, 2.90592759847641, 2.8937286734580994, 2.482672095298767, 2.4920148253440857, 2.755396842956543, 2.58797425031662, 2.3206658959388733, 2.522655427455902, 2.455150604248047, 2.7998158931732178, 2.3936361372470856, 2.7818050384521484, 2.485604226589203, 2.554481327533722, 2.3239344358444214, 2.42600816488266, 2.5793464183807373, 2.2006306648254395, 2.4769791960716248, 2.742115616798401, 2.3317300975322723, 2.469799280166626, 2.5002079010009766, 2.5156251788139343, 2.419241786003113, 2.330108642578125, 2.2484894394874573, 2.2089719772338867, 2.586911618709564, 2.5212297439575195], 'val_losses': [2.660207509994507, 2.660207509994507, 2.584505319595337, 2.429203987121582, 2.2837533950805664, 2.1453347206115723, 2.0159523487091064, 1.8926074504852295, 1.7757961750030518, 1.6732524633407593, 1.57890784740448, 1.4936490058898926, 1.4200438261032104, 1.3537757396697998, 1.2967679500579834, 1.2470669746398926, 1.208178162574768, 1.1749216318130493, 1.1492335796356201, 1.127527117729187, 1.1112064123153687, 1.1006487607955933, 1.0932692289352417, 1.0886895656585693, 1.086922526359558, 1.0866905450820923, 1.0863652229309082, 1.0885568857192993, 1.0892508029937744, 1.0906774997711182, 1.0922900438308716, 1.094327449798584, 1.0960849523544312, 1.0974416732788086, 1.098888635635376, 1.1004509925842285, 1.1010456085205078, 1.1016428470611572, 1.102104902267456, 1.1030110120773315, 1.1042664051055908, 1.104724407196045, 1.104891300201416, 1.1051520109176636, 1.1051735877990723, 1.105214238166809, 1.1051651239395142], 'val_acc': [0.125, 0.125, 0.125, 0.171875, 0.234375, 0.25, 0.3125, 0.328125, 0.34375, 0.453125, 0.5, 0.53125, 0.53125, 0.5625, 0.59375, 0.609375, 0.609375, 0.65625, 0.671875, 0.71875, 0.734375, 0.734375, 0.71875, 0.734375, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.001325606340567e-05, 'batch_size': 192, 'epochs': 47, 'hidden_size': 174, 'dropout': 0.13335737860564614, 'weight_decay': 3.840268119736549e-05, 'label_smoothing': 0.004379926555165726, 'grad_clip_norm': 2.752307064869228, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 31845, 'model_storage_size_kb': 136.833984375, 'model_size_validation': 'PASS'}
2025-10-13 22:44:12,109 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:44:12,109 - INFO - _models.training_function_executor - Model: 31,845 parameters, 136.8KB (PASS 256KB limit)
2025-10-13 22:44:12,109 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.944s
2025-10-13 22:44:12,192 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:44:12,192 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.083s
2025-10-13 22:44:12,193 - INFO - bo.run_bo - Recorded observation #42: hparams={'lr': 1.001325606340567e-05, 'batch_size': np.int64(192), 'epochs': np.int64(47), 'hidden_size': np.int64(174), 'dropout': 0.13335737860564614, 'weight_decay': 3.840268119736549e-05, 'label_smoothing': 0.004379926555165726, 'grad_clip_norm': 2.752307064869228, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:44:12,193 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 42: {'lr': 1.001325606340567e-05, 'batch_size': np.int64(192), 'epochs': np.int64(47), 'hidden_size': np.int64(174), 'dropout': 0.13335737860564614, 'weight_decay': 3.840268119736549e-05, 'label_smoothing': 0.004379926555165726, 'grad_clip_norm': 2.752307064869228, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:44:12,193 - INFO - bo.run_bo - üîçBO Trial 43: Using RF surrogate + Expected Improvement
2025-10-13 22:44:12,193 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:12,193 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 43 (NaN monitoring active)
2025-10-13 22:44:12,193 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:12,193 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:12,193 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00019628842724326648, 'batch_size': 96, 'epochs': 160, 'hidden_size': 150, 'dropout': 0.12901991410993827, 'weight_decay': 0.007362352653625316, 'label_smoothing': 0.03673574445903703, 'grad_clip_norm': 1.732546835510576, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:44:12,194 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00019628842724326648, 'batch_size': 96, 'epochs': 160, 'hidden_size': 150, 'dropout': 0.12901991410993827, 'weight_decay': 0.007362352653625316, 'label_smoothing': 0.03673574445903703, 'grad_clip_norm': 1.732546835510576, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:44:15,637 - INFO - _models.training_function_executor - Epoch 001/160 | train_loss=9.6116 | val_loss=9.0994 | val_acc=0.1875
2025-10-13 22:44:15,645 - INFO - _models.training_function_executor - Epoch 002/160 | train_loss=9.2550 | val_loss=9.0994 | val_acc=0.1875
2025-10-13 22:44:15,651 - INFO - _models.training_function_executor - Epoch 003/160 | train_loss=8.3487 | val_loss=3.6386 | val_acc=0.1875
2025-10-13 22:44:15,657 - INFO - _models.training_function_executor - Epoch 004/160 | train_loss=3.6948 | val_loss=2.1203 | val_acc=0.7188
2025-10-13 22:44:15,662 - INFO - _models.training_function_executor - Epoch 005/160 | train_loss=3.0309 | val_loss=2.5785 | val_acc=0.7188
2025-10-13 22:44:15,668 - INFO - _models.training_function_executor - Epoch 006/160 | train_loss=2.9642 | val_loss=1.9401 | val_acc=0.7188
2025-10-13 22:44:15,673 - INFO - _models.training_function_executor - Epoch 007/160 | train_loss=2.7503 | val_loss=1.4485 | val_acc=0.5938
2025-10-13 22:44:15,678 - INFO - _models.training_function_executor - Epoch 008/160 | train_loss=2.2557 | val_loss=1.3244 | val_acc=0.3906
2025-10-13 22:44:15,683 - INFO - _models.training_function_executor - Epoch 009/160 | train_loss=2.7246 | val_loss=0.8218 | val_acc=0.7656
2025-10-13 22:44:15,689 - INFO - _models.training_function_executor - Epoch 010/160 | train_loss=2.4726 | val_loss=0.8086 | val_acc=0.7188
2025-10-13 22:44:15,694 - INFO - _models.training_function_executor - Epoch 011/160 | train_loss=2.0366 | val_loss=1.0274 | val_acc=0.7188
2025-10-13 22:44:15,699 - INFO - _models.training_function_executor - Epoch 012/160 | train_loss=2.4156 | val_loss=1.0246 | val_acc=0.7188
2025-10-13 22:44:15,705 - INFO - _models.training_function_executor - Epoch 013/160 | train_loss=2.1552 | val_loss=0.8139 | val_acc=0.7188
2025-10-13 22:44:15,709 - INFO - _models.training_function_executor - Epoch 014/160 | train_loss=1.9570 | val_loss=0.7456 | val_acc=0.7344
2025-10-13 22:44:15,715 - INFO - _models.training_function_executor - Epoch 015/160 | train_loss=2.0506 | val_loss=0.7561 | val_acc=0.7188
2025-10-13 22:44:15,720 - INFO - _models.training_function_executor - Epoch 016/160 | train_loss=1.8164 | val_loss=0.7139 | val_acc=0.7188
2025-10-13 22:44:15,726 - INFO - _models.training_function_executor - Epoch 017/160 | train_loss=1.8844 | val_loss=0.8185 | val_acc=0.7188
2025-10-13 22:44:15,732 - INFO - _models.training_function_executor - Epoch 018/160 | train_loss=1.5889 | val_loss=0.8352 | val_acc=0.7188
2025-10-13 22:44:15,737 - INFO - _models.training_function_executor - Epoch 019/160 | train_loss=1.7145 | val_loss=0.7035 | val_acc=0.7344
2025-10-13 22:44:15,744 - INFO - _models.training_function_executor - Epoch 020/160 | train_loss=1.7947 | val_loss=0.7465 | val_acc=0.6406
2025-10-13 22:44:15,749 - INFO - _models.training_function_executor - Epoch 021/160 | train_loss=1.6115 | val_loss=0.6977 | val_acc=0.6719
2025-10-13 22:44:15,754 - INFO - _models.training_function_executor - Epoch 022/160 | train_loss=1.8743 | val_loss=0.6886 | val_acc=0.7188
2025-10-13 22:44:15,759 - INFO - _models.training_function_executor - Epoch 023/160 | train_loss=1.4878 | val_loss=0.7705 | val_acc=0.7188
2025-10-13 22:44:15,765 - INFO - _models.training_function_executor - Epoch 024/160 | train_loss=1.5915 | val_loss=0.6858 | val_acc=0.7344
2025-10-13 22:44:15,770 - INFO - _models.training_function_executor - Epoch 025/160 | train_loss=1.5593 | val_loss=0.6691 | val_acc=0.7188
2025-10-13 22:44:15,775 - INFO - _models.training_function_executor - Epoch 026/160 | train_loss=1.5071 | val_loss=0.6698 | val_acc=0.7031
2025-10-13 22:44:15,780 - INFO - _models.training_function_executor - Epoch 027/160 | train_loss=1.4464 | val_loss=0.6990 | val_acc=0.7188
2025-10-13 22:44:15,785 - INFO - _models.training_function_executor - Epoch 028/160 | train_loss=1.2315 | val_loss=0.7822 | val_acc=0.7188
2025-10-13 22:44:15,790 - INFO - _models.training_function_executor - Epoch 029/160 | train_loss=1.3374 | val_loss=0.7358 | val_acc=0.7188
2025-10-13 22:44:15,795 - INFO - _models.training_function_executor - Epoch 030/160 | train_loss=1.3213 | val_loss=0.6659 | val_acc=0.7188
2025-10-13 22:44:15,800 - INFO - _models.training_function_executor - Epoch 031/160 | train_loss=1.4124 | val_loss=0.7328 | val_acc=0.6406
2025-10-13 22:44:15,805 - INFO - _models.training_function_executor - Epoch 032/160 | train_loss=1.2722 | val_loss=0.6778 | val_acc=0.7188
2025-10-13 22:44:15,811 - INFO - _models.training_function_executor - Epoch 033/160 | train_loss=1.2545 | val_loss=0.6911 | val_acc=0.7188
2025-10-13 22:44:15,816 - INFO - _models.training_function_executor - Epoch 034/160 | train_loss=1.2308 | val_loss=0.6916 | val_acc=0.7188
2025-10-13 22:44:15,821 - INFO - _models.training_function_executor - Epoch 035/160 | train_loss=1.3629 | val_loss=0.6643 | val_acc=0.7188
2025-10-13 22:44:15,826 - INFO - _models.training_function_executor - Epoch 036/160 | train_loss=1.2886 | val_loss=0.8062 | val_acc=0.6875
2025-10-13 22:44:15,831 - INFO - _models.training_function_executor - Epoch 037/160 | train_loss=1.2173 | val_loss=0.8215 | val_acc=0.6562
2025-10-13 22:44:15,836 - INFO - _models.training_function_executor - Epoch 038/160 | train_loss=1.3817 | val_loss=0.6799 | val_acc=0.6562
2025-10-13 22:44:15,841 - INFO - _models.training_function_executor - Epoch 039/160 | train_loss=1.2227 | val_loss=0.6746 | val_acc=0.7188
2025-10-13 22:44:15,846 - INFO - _models.training_function_executor - Epoch 040/160 | train_loss=1.0945 | val_loss=0.6627 | val_acc=0.7188
2025-10-13 22:44:15,851 - INFO - _models.training_function_executor - Epoch 041/160 | train_loss=1.2328 | val_loss=0.6791 | val_acc=0.6719
2025-10-13 22:44:15,856 - INFO - _models.training_function_executor - Epoch 042/160 | train_loss=1.1375 | val_loss=0.7245 | val_acc=0.6562
2025-10-13 22:44:15,861 - INFO - _models.training_function_executor - Epoch 043/160 | train_loss=1.2244 | val_loss=0.6914 | val_acc=0.6562
2025-10-13 22:44:15,866 - INFO - _models.training_function_executor - Epoch 044/160 | train_loss=0.9789 | val_loss=0.6979 | val_acc=0.6406
2025-10-13 22:44:15,871 - INFO - _models.training_function_executor - Epoch 045/160 | train_loss=1.1270 | val_loss=0.6901 | val_acc=0.6719
2025-10-13 22:44:15,876 - INFO - _models.training_function_executor - Epoch 046/160 | train_loss=1.1553 | val_loss=0.6794 | val_acc=0.6719
2025-10-13 22:44:15,881 - INFO - _models.training_function_executor - Epoch 047/160 | train_loss=0.9717 | val_loss=0.6746 | val_acc=0.6719
2025-10-13 22:44:15,886 - INFO - _models.training_function_executor - Epoch 048/160 | train_loss=1.0140 | val_loss=0.6779 | val_acc=0.6719
2025-10-13 22:44:15,891 - INFO - _models.training_function_executor - Epoch 049/160 | train_loss=1.1492 | val_loss=0.6760 | val_acc=0.7031
2025-10-13 22:44:15,896 - INFO - _models.training_function_executor - Epoch 050/160 | train_loss=0.9714 | val_loss=0.6807 | val_acc=0.6719
2025-10-13 22:44:15,902 - INFO - _models.training_function_executor - Epoch 051/160 | train_loss=0.9789 | val_loss=0.6800 | val_acc=0.7188
2025-10-13 22:44:15,907 - INFO - _models.training_function_executor - Epoch 052/160 | train_loss=1.0070 | val_loss=0.6827 | val_acc=0.7188
2025-10-13 22:44:15,912 - INFO - _models.training_function_executor - Epoch 053/160 | train_loss=1.0605 | val_loss=0.6911 | val_acc=0.7188
2025-10-13 22:44:15,917 - INFO - _models.training_function_executor - Epoch 054/160 | train_loss=0.9068 | val_loss=0.7029 | val_acc=0.6719
2025-10-13 22:44:15,922 - INFO - _models.training_function_executor - Epoch 055/160 | train_loss=1.0115 | val_loss=0.7040 | val_acc=0.6250
2025-10-13 22:44:15,927 - INFO - _models.training_function_executor - Epoch 056/160 | train_loss=0.9807 | val_loss=0.7049 | val_acc=0.6406
2025-10-13 22:44:15,932 - INFO - _models.training_function_executor - Epoch 057/160 | train_loss=1.0766 | val_loss=0.7006 | val_acc=0.6562
2025-10-13 22:44:15,937 - INFO - _models.training_function_executor - Epoch 058/160 | train_loss=0.9336 | val_loss=0.7022 | val_acc=0.6406
2025-10-13 22:44:15,943 - INFO - _models.training_function_executor - Epoch 059/160 | train_loss=1.0339 | val_loss=0.6966 | val_acc=0.7188
2025-10-13 22:44:15,948 - INFO - _models.training_function_executor - Epoch 060/160 | train_loss=0.9745 | val_loss=0.6991 | val_acc=0.7188
2025-10-13 22:44:15,953 - INFO - _models.training_function_executor - Epoch 061/160 | train_loss=0.9640 | val_loss=0.7002 | val_acc=0.6562
2025-10-13 22:44:15,958 - INFO - _models.training_function_executor - Epoch 062/160 | train_loss=0.8857 | val_loss=0.6964 | val_acc=0.6719
2025-10-13 22:44:15,964 - INFO - _models.training_function_executor - Epoch 063/160 | train_loss=0.9741 | val_loss=0.6896 | val_acc=0.7188
2025-10-13 22:44:15,970 - INFO - _models.training_function_executor - Epoch 064/160 | train_loss=0.9057 | val_loss=0.6890 | val_acc=0.7188
2025-10-13 22:44:15,975 - INFO - _models.training_function_executor - Epoch 065/160 | train_loss=0.8608 | val_loss=0.6962 | val_acc=0.6875
2025-10-13 22:44:15,980 - INFO - _models.training_function_executor - Epoch 066/160 | train_loss=0.8996 | val_loss=0.7004 | val_acc=0.6250
2025-10-13 22:44:15,986 - INFO - _models.training_function_executor - Epoch 067/160 | train_loss=0.8665 | val_loss=0.7068 | val_acc=0.6562
2025-10-13 22:44:15,991 - INFO - _models.training_function_executor - Epoch 068/160 | train_loss=0.9223 | val_loss=0.6979 | val_acc=0.6250
2025-10-13 22:44:15,995 - INFO - _models.training_function_executor - Epoch 069/160 | train_loss=0.8892 | val_loss=0.6903 | val_acc=0.6406
2025-10-13 22:44:16,000 - INFO - _models.training_function_executor - Epoch 070/160 | train_loss=0.8848 | val_loss=0.6827 | val_acc=0.7188
2025-10-13 22:44:16,005 - INFO - _models.training_function_executor - Epoch 071/160 | train_loss=0.9644 | val_loss=0.6833 | val_acc=0.7188
2025-10-13 22:44:16,010 - INFO - _models.training_function_executor - Epoch 072/160 | train_loss=0.8866 | val_loss=0.6845 | val_acc=0.7031
2025-10-13 22:44:16,015 - INFO - _models.training_function_executor - Epoch 073/160 | train_loss=0.9024 | val_loss=0.6816 | val_acc=0.7188
2025-10-13 22:44:16,020 - INFO - _models.training_function_executor - Epoch 074/160 | train_loss=0.9200 | val_loss=0.6804 | val_acc=0.7188
2025-10-13 22:44:16,025 - INFO - _models.training_function_executor - Epoch 075/160 | train_loss=0.8455 | val_loss=0.6781 | val_acc=0.7188
2025-10-13 22:44:16,030 - INFO - _models.training_function_executor - Epoch 076/160 | train_loss=0.8527 | val_loss=0.6789 | val_acc=0.7188
2025-10-13 22:44:16,036 - INFO - _models.training_function_executor - Epoch 077/160 | train_loss=0.8521 | val_loss=0.6813 | val_acc=0.6719
2025-10-13 22:44:16,041 - INFO - _models.training_function_executor - Epoch 078/160 | train_loss=0.8272 | val_loss=0.6834 | val_acc=0.6406
2025-10-13 22:44:16,047 - INFO - _models.training_function_executor - Epoch 079/160 | train_loss=0.8468 | val_loss=0.6885 | val_acc=0.6875
2025-10-13 22:44:16,052 - INFO - _models.training_function_executor - Epoch 080/160 | train_loss=0.8733 | val_loss=0.6938 | val_acc=0.6875
2025-10-13 22:44:16,057 - INFO - _models.training_function_executor - Epoch 081/160 | train_loss=0.8726 | val_loss=0.6881 | val_acc=0.6562
2025-10-13 22:44:16,062 - INFO - _models.training_function_executor - Epoch 082/160 | train_loss=0.8526 | val_loss=0.6837 | val_acc=0.6094
2025-10-13 22:44:16,067 - INFO - _models.training_function_executor - Epoch 083/160 | train_loss=0.8971 | val_loss=0.6844 | val_acc=0.6250
2025-10-13 22:44:16,073 - INFO - _models.training_function_executor - Epoch 084/160 | train_loss=0.8469 | val_loss=0.6837 | val_acc=0.6250
2025-10-13 22:44:16,078 - INFO - _models.training_function_executor - Epoch 085/160 | train_loss=0.7901 | val_loss=0.6784 | val_acc=0.7188
2025-10-13 22:44:16,083 - INFO - _models.training_function_executor - Epoch 086/160 | train_loss=0.8427 | val_loss=0.6786 | val_acc=0.7188
2025-10-13 22:44:16,088 - INFO - _models.training_function_executor - Epoch 087/160 | train_loss=0.8139 | val_loss=0.6771 | val_acc=0.7188
2025-10-13 22:44:16,093 - INFO - _models.training_function_executor - Epoch 088/160 | train_loss=0.8178 | val_loss=0.6783 | val_acc=0.7188
2025-10-13 22:44:16,098 - INFO - _models.training_function_executor - Epoch 089/160 | train_loss=0.9077 | val_loss=0.6786 | val_acc=0.7188
2025-10-13 22:44:16,104 - INFO - _models.training_function_executor - Epoch 090/160 | train_loss=0.8069 | val_loss=0.6768 | val_acc=0.7188
2025-10-13 22:44:16,109 - INFO - _models.training_function_executor - Epoch 091/160 | train_loss=0.7969 | val_loss=0.6741 | val_acc=0.7188
2025-10-13 22:44:16,114 - INFO - _models.training_function_executor - Epoch 092/160 | train_loss=0.8417 | val_loss=0.6739 | val_acc=0.7188
2025-10-13 22:44:16,120 - INFO - _models.training_function_executor - Epoch 093/160 | train_loss=0.8174 | val_loss=0.6739 | val_acc=0.7188
2025-10-13 22:44:16,125 - INFO - _models.training_function_executor - Epoch 094/160 | train_loss=0.7997 | val_loss=0.6742 | val_acc=0.7188
2025-10-13 22:44:16,131 - INFO - _models.training_function_executor - Epoch 095/160 | train_loss=0.8691 | val_loss=0.6742 | val_acc=0.7188
2025-10-13 22:44:16,136 - INFO - _models.training_function_executor - Epoch 096/160 | train_loss=0.8596 | val_loss=0.6745 | val_acc=0.7188
2025-10-13 22:44:16,141 - INFO - _models.training_function_executor - Epoch 097/160 | train_loss=0.8435 | val_loss=0.6748 | val_acc=0.7188
2025-10-13 22:44:16,148 - INFO - _models.training_function_executor - Epoch 098/160 | train_loss=0.8039 | val_loss=0.6776 | val_acc=0.7188
2025-10-13 22:44:16,153 - INFO - _models.training_function_executor - Epoch 099/160 | train_loss=0.8513 | val_loss=0.6796 | val_acc=0.7188
2025-10-13 22:44:16,158 - INFO - _models.training_function_executor - Epoch 100/160 | train_loss=0.8262 | val_loss=0.6799 | val_acc=0.7188
2025-10-13 22:44:16,164 - INFO - _models.training_function_executor - Epoch 101/160 | train_loss=0.8144 | val_loss=0.6786 | val_acc=0.7188
2025-10-13 22:44:16,169 - INFO - _models.training_function_executor - Epoch 102/160 | train_loss=0.8408 | val_loss=0.6776 | val_acc=0.7188
2025-10-13 22:44:16,175 - INFO - _models.training_function_executor - Epoch 103/160 | train_loss=0.8292 | val_loss=0.6758 | val_acc=0.7188
2025-10-13 22:44:16,180 - INFO - _models.training_function_executor - Epoch 104/160 | train_loss=0.8235 | val_loss=0.6753 | val_acc=0.7188
2025-10-13 22:44:16,185 - INFO - _models.training_function_executor - Epoch 105/160 | train_loss=0.8200 | val_loss=0.6757 | val_acc=0.7188
2025-10-13 22:44:16,190 - INFO - _models.training_function_executor - Epoch 106/160 | train_loss=0.8085 | val_loss=0.6768 | val_acc=0.7188
2025-10-13 22:44:16,195 - INFO - _models.training_function_executor - Epoch 107/160 | train_loss=0.7919 | val_loss=0.6770 | val_acc=0.7188
2025-10-13 22:44:16,200 - INFO - _models.training_function_executor - Epoch 108/160 | train_loss=0.8255 | val_loss=0.6771 | val_acc=0.7188
2025-10-13 22:44:16,205 - INFO - _models.training_function_executor - Epoch 109/160 | train_loss=0.8165 | val_loss=0.6767 | val_acc=0.7188
2025-10-13 22:44:16,210 - INFO - _models.training_function_executor - Epoch 110/160 | train_loss=0.8498 | val_loss=0.6778 | val_acc=0.7188
2025-10-13 22:44:16,215 - INFO - _models.training_function_executor - Epoch 111/160 | train_loss=0.8025 | val_loss=0.6788 | val_acc=0.7188
2025-10-13 22:44:16,221 - INFO - _models.training_function_executor - Epoch 112/160 | train_loss=0.7791 | val_loss=0.6798 | val_acc=0.7188
2025-10-13 22:44:16,227 - INFO - _models.training_function_executor - Epoch 113/160 | train_loss=0.7826 | val_loss=0.6805 | val_acc=0.7188
2025-10-13 22:44:16,234 - INFO - _models.training_function_executor - Epoch 114/160 | train_loss=0.8273 | val_loss=0.6809 | val_acc=0.7188
2025-10-13 22:44:16,240 - INFO - _models.training_function_executor - Epoch 115/160 | train_loss=0.8429 | val_loss=0.6815 | val_acc=0.7188
2025-10-13 22:44:16,245 - INFO - _models.training_function_executor - Epoch 116/160 | train_loss=0.8275 | val_loss=0.6814 | val_acc=0.7188
2025-10-13 22:44:16,252 - INFO - _models.training_function_executor - Epoch 117/160 | train_loss=0.8200 | val_loss=0.6812 | val_acc=0.7188
2025-10-13 22:44:16,257 - INFO - _models.training_function_executor - Epoch 118/160 | train_loss=0.8430 | val_loss=0.6809 | val_acc=0.7188
2025-10-13 22:44:16,262 - INFO - _models.training_function_executor - Epoch 119/160 | train_loss=0.7310 | val_loss=0.6811 | val_acc=0.7188
2025-10-13 22:44:16,268 - INFO - _models.training_function_executor - Epoch 120/160 | train_loss=0.8066 | val_loss=0.6812 | val_acc=0.7188
2025-10-13 22:44:16,273 - INFO - _models.training_function_executor - Epoch 121/160 | train_loss=0.8068 | val_loss=0.6811 | val_acc=0.7188
2025-10-13 22:44:16,278 - INFO - _models.training_function_executor - Epoch 122/160 | train_loss=0.8261 | val_loss=0.6806 | val_acc=0.7188
2025-10-13 22:44:16,283 - INFO - _models.training_function_executor - Epoch 123/160 | train_loss=0.7872 | val_loss=0.6807 | val_acc=0.7188
2025-10-13 22:44:16,288 - INFO - _models.training_function_executor - Epoch 124/160 | train_loss=0.7994 | val_loss=0.6807 | val_acc=0.7188
2025-10-13 22:44:16,293 - INFO - _models.training_function_executor - Epoch 125/160 | train_loss=0.8012 | val_loss=0.6810 | val_acc=0.7188
2025-10-13 22:44:16,298 - INFO - _models.training_function_executor - Epoch 126/160 | train_loss=0.8242 | val_loss=0.6808 | val_acc=0.7188
2025-10-13 22:44:16,304 - INFO - _models.training_function_executor - Epoch 127/160 | train_loss=0.7796 | val_loss=0.6807 | val_acc=0.7188
2025-10-13 22:44:16,309 - INFO - _models.training_function_executor - Epoch 128/160 | train_loss=0.8475 | val_loss=0.6804 | val_acc=0.7188
2025-10-13 22:44:16,314 - INFO - _models.training_function_executor - Epoch 129/160 | train_loss=0.8215 | val_loss=0.6801 | val_acc=0.7188
2025-10-13 22:44:16,319 - INFO - _models.training_function_executor - Epoch 130/160 | train_loss=0.8152 | val_loss=0.6799 | val_acc=0.7188
2025-10-13 22:44:16,324 - INFO - _models.training_function_executor - Epoch 131/160 | train_loss=0.7523 | val_loss=0.6798 | val_acc=0.7188
2025-10-13 22:44:16,329 - INFO - _models.training_function_executor - Epoch 132/160 | train_loss=0.7896 | val_loss=0.6797 | val_acc=0.7188
2025-10-13 22:44:16,334 - INFO - _models.training_function_executor - Epoch 133/160 | train_loss=0.8292 | val_loss=0.6797 | val_acc=0.7188
2025-10-13 22:44:16,339 - INFO - _models.training_function_executor - Epoch 134/160 | train_loss=0.8435 | val_loss=0.6798 | val_acc=0.7188
2025-10-13 22:44:16,344 - INFO - _models.training_function_executor - Epoch 135/160 | train_loss=0.7996 | val_loss=0.6797 | val_acc=0.7188
2025-10-13 22:44:16,350 - INFO - _models.training_function_executor - Epoch 136/160 | train_loss=0.7766 | val_loss=0.6794 | val_acc=0.7188
2025-10-13 22:44:16,355 - INFO - _models.training_function_executor - Epoch 137/160 | train_loss=0.8037 | val_loss=0.6794 | val_acc=0.7188
2025-10-13 22:44:16,360 - INFO - _models.training_function_executor - Epoch 138/160 | train_loss=0.7926 | val_loss=0.6791 | val_acc=0.7188
2025-10-13 22:44:16,365 - INFO - _models.training_function_executor - Epoch 139/160 | train_loss=0.8236 | val_loss=0.6789 | val_acc=0.7188
2025-10-13 22:44:16,370 - INFO - _models.training_function_executor - Epoch 140/160 | train_loss=0.8104 | val_loss=0.6787 | val_acc=0.7188
2025-10-13 22:44:16,376 - INFO - _models.training_function_executor - Epoch 141/160 | train_loss=0.8170 | val_loss=0.6785 | val_acc=0.7188
2025-10-13 22:44:16,381 - INFO - _models.training_function_executor - Epoch 142/160 | train_loss=0.7924 | val_loss=0.6784 | val_acc=0.7188
2025-10-13 22:44:16,386 - INFO - _models.training_function_executor - Epoch 143/160 | train_loss=0.7840 | val_loss=0.6783 | val_acc=0.7188
2025-10-13 22:44:16,391 - INFO - _models.training_function_executor - Epoch 144/160 | train_loss=0.8239 | val_loss=0.6783 | val_acc=0.7188
2025-10-13 22:44:16,396 - INFO - _models.training_function_executor - Epoch 145/160 | train_loss=0.7495 | val_loss=0.6783 | val_acc=0.7188
2025-10-13 22:44:16,401 - INFO - _models.training_function_executor - Epoch 146/160 | train_loss=0.7871 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,406 - INFO - _models.training_function_executor - Epoch 147/160 | train_loss=0.8232 | val_loss=0.6783 | val_acc=0.7188
2025-10-13 22:44:16,414 - INFO - _models.training_function_executor - Epoch 148/160 | train_loss=0.8009 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,422 - INFO - _models.training_function_executor - Epoch 149/160 | train_loss=0.7971 | val_loss=0.6783 | val_acc=0.7188
2025-10-13 22:44:16,432 - INFO - _models.training_function_executor - Epoch 150/160 | train_loss=0.7570 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,440 - INFO - _models.training_function_executor - Epoch 151/160 | train_loss=0.8145 | val_loss=0.6783 | val_acc=0.7188
2025-10-13 22:44:16,451 - INFO - _models.training_function_executor - Epoch 152/160 | train_loss=0.8149 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,459 - INFO - _models.training_function_executor - Epoch 153/160 | train_loss=0.7861 | val_loss=0.6783 | val_acc=0.7188
2025-10-13 22:44:16,465 - INFO - _models.training_function_executor - Epoch 154/160 | train_loss=0.7785 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,471 - INFO - _models.training_function_executor - Epoch 155/160 | train_loss=0.7776 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,477 - INFO - _models.training_function_executor - Epoch 156/160 | train_loss=0.8256 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,482 - INFO - _models.training_function_executor - Epoch 157/160 | train_loss=0.7846 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,489 - INFO - _models.training_function_executor - Epoch 158/160 | train_loss=0.7944 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,494 - INFO - _models.training_function_executor - Epoch 159/160 | train_loss=0.7985 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:16,499 - INFO - _models.training_function_executor - Epoch 160/160 | train_loss=0.7847 | val_loss=0.6782 | val_acc=0.7188
2025-10-13 22:44:17,416 - INFO - _models.training_function_executor - Model: 23,853 parameters, 102.5KB storage
2025-10-13 22:44:17,416 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [9.611565351486206, 9.25498902797699, 8.348721265792847, 3.6947890520095825, 3.030888617038727, 2.964236468076706, 2.7503159642219543, 2.2556720674037933, 2.724621057510376, 2.472591996192932, 2.036612719297409, 2.415562242269516, 2.155157297849655, 1.9569592028856277, 2.0506188571453094, 1.8163579255342484, 1.884368747472763, 1.5888745486736298, 1.714498445391655, 1.79472978413105, 1.6115155965089798, 1.8742545396089554, 1.487766519188881, 1.591542050242424, 1.5593339949846268, 1.507093921303749, 1.4463994801044464, 1.2315458431839943, 1.3374282866716385, 1.3213077411055565, 1.412367656826973, 1.272159218788147, 1.2544892728328705, 1.2308040112257004, 1.3629227131605148, 1.2885644733905792, 1.2173086106777191, 1.38165383040905, 1.2227376252412796, 1.0945339351892471, 1.2327893674373627, 1.1374931633472443, 1.2243914902210236, 0.9788719862699509, 1.1269633322954178, 1.1553342789411545, 0.9717090353369713, 1.0140314549207687, 1.1492126882076263, 0.9713515862822533, 0.9789241924881935, 1.0069969445466995, 1.0604651346802711, 0.9067552983760834, 1.0114879757165909, 0.9806851148605347, 1.0765763223171234, 0.9336176067590714, 1.0338717177510262, 0.9745363369584084, 0.9640229418873787, 0.8857335075736046, 0.9740981757640839, 0.9056556299328804, 0.8607594296336174, 0.8995824605226517, 0.866469070315361, 0.9223370924592018, 0.8892003819346428, 0.8847674876451492, 0.9643785133957863, 0.8866476491093636, 0.9023599922657013, 0.9199536070227623, 0.8454564288258553, 0.852689579129219, 0.8521445393562317, 0.8272065222263336, 0.8468356728553772, 0.8732824251055717, 0.8725820854306221, 0.8525994643568993, 0.8971049636602402, 0.8469167649745941, 0.790070578455925, 0.8426849022507668, 0.8139114007353783, 0.8178466558456421, 0.9076820686459541, 0.806926891207695, 0.796923503279686, 0.8416669517755508, 0.8174090459942818, 0.7996701747179031, 0.8691150471568108, 0.859590470790863, 0.8434881716966629, 0.8038788437843323, 0.8512697294354439, 0.8261638432741165, 0.8144276216626167, 0.8407722413539886, 0.8291559666395187, 0.8235258683562279, 0.8200354054570198, 0.8084581941366196, 0.791902557015419, 0.8255029693245888, 0.8164505586028099, 0.8498425483703613, 0.8024699091911316, 0.7791438028216362, 0.782601036131382, 0.8272669985890388, 0.8429209813475609, 0.8274857550859451, 0.8200088292360306, 0.8429609835147858, 0.7310439497232437, 0.8065765276551247, 0.806810051202774, 0.8261057287454605, 0.7871661707758904, 0.799447126686573, 0.8011883646249771, 0.8242060020565987, 0.7795965448021889, 0.8474536091089249, 0.8214572295546532, 0.8152207657694817, 0.7522725835442543, 0.7895909696817398, 0.829162023961544, 0.8435136526823044, 0.7996180132031441, 0.7765524983406067, 0.8037066385149956, 0.7925766631960869, 0.8236254453659058, 0.8103562816977501, 0.8170409575104713, 0.7924302294850349, 0.7839768603444099, 0.8238709345459938, 0.7494611889123917, 0.7870756685733795, 0.8232470154762268, 0.8009070456027985, 0.7971271499991417, 0.7569723054766655, 0.8144891858100891, 0.8149128332734108, 0.7860865965485573, 0.7784721478819847, 0.7776052355766296, 0.8256480768322945, 0.7845681980252266, 0.7943687662482262, 0.7984914630651474, 0.7846798226237297], 'val_losses': [9.099382400512695, 9.099382400512695, 3.6385834217071533, 2.1202895641326904, 2.5785367488861084, 1.94009268283844, 1.4484765529632568, 1.324413776397705, 0.8217923641204834, 0.8086302280426025, 1.027414083480835, 1.0245870351791382, 0.8138711452484131, 0.7455896139144897, 0.7560758590698242, 0.7138552665710449, 0.818544864654541, 0.8351781368255615, 0.7034767270088196, 0.7464678883552551, 0.6976596713066101, 0.6885653138160706, 0.770539402961731, 0.6857854723930359, 0.6691045761108398, 0.6698297262191772, 0.6989541053771973, 0.7822453379631042, 0.7357677221298218, 0.665862500667572, 0.7327668070793152, 0.6777716279029846, 0.6911005973815918, 0.6915656924247742, 0.6642603278160095, 0.8062137961387634, 0.8214976787567139, 0.6798859238624573, 0.6746271848678589, 0.6626763343811035, 0.6790666580200195, 0.7244929671287537, 0.6913654804229736, 0.6979020237922668, 0.6901394128799438, 0.679411768913269, 0.6745892763137817, 0.6779325604438782, 0.6760373711585999, 0.6806932687759399, 0.6799630522727966, 0.6826502084732056, 0.6911323666572571, 0.7028547525405884, 0.7039735317230225, 0.7048864364624023, 0.700579822063446, 0.702232301235199, 0.6965774297714233, 0.6991231441497803, 0.7002453207969666, 0.6963637471199036, 0.6895570755004883, 0.6889899969100952, 0.696199893951416, 0.7003676891326904, 0.7068116068840027, 0.6979028582572937, 0.6903117299079895, 0.6827200651168823, 0.6832692623138428, 0.6845036149024963, 0.6816108226776123, 0.6803800463676453, 0.678087592124939, 0.6788701415061951, 0.6813212037086487, 0.6834151744842529, 0.6884875297546387, 0.6938064098358154, 0.6881334185600281, 0.6836601495742798, 0.684446394443512, 0.6836770176887512, 0.678409218788147, 0.6786168813705444, 0.6771093606948853, 0.6782518625259399, 0.6785958409309387, 0.6768243312835693, 0.6741474270820618, 0.6739082336425781, 0.6738685965538025, 0.6741660237312317, 0.6741634011268616, 0.6744552254676819, 0.6747793555259705, 0.6776230335235596, 0.6796221137046814, 0.6799404621124268, 0.6785991191864014, 0.677622377872467, 0.6757892370223999, 0.6753324270248413, 0.6756557822227478, 0.6767882108688354, 0.6770395040512085, 0.6770985126495361, 0.6767041087150574, 0.6777512431144714, 0.6787617802619934, 0.6797609925270081, 0.6804871559143066, 0.6808724403381348, 0.6814526915550232, 0.6814174056053162, 0.681157648563385, 0.680908739566803, 0.6810621619224548, 0.6812272667884827, 0.681084930896759, 0.6805948615074158, 0.6806938052177429, 0.680666983127594, 0.6810177564620972, 0.6807790398597717, 0.6806935667991638, 0.6803900003433228, 0.6800889372825623, 0.6799035668373108, 0.6798475384712219, 0.6797158718109131, 0.6797309517860413, 0.679780900478363, 0.6796553134918213, 0.6794382333755493, 0.6793659329414368, 0.679094135761261, 0.6788680553436279, 0.6786560416221619, 0.6784864664077759, 0.678368091583252, 0.678320050239563, 0.6782670617103577, 0.6782517433166504, 0.6782463788986206, 0.6782546639442444, 0.6782426238059998, 0.6782863140106201, 0.6782199144363403, 0.6782708168029785, 0.6782044172286987, 0.6782761812210083, 0.678236722946167, 0.6782198548316956, 0.6782122850418091, 0.678196907043457, 0.6781899333000183, 0.6781883239746094, 0.6781809329986572], 'val_acc': [0.1875, 0.1875, 0.1875, 0.71875, 0.71875, 0.71875, 0.59375, 0.390625, 0.765625, 0.71875, 0.71875, 0.71875, 0.71875, 0.734375, 0.71875, 0.71875, 0.71875, 0.71875, 0.734375, 0.640625, 0.671875, 0.71875, 0.71875, 0.734375, 0.71875, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.640625, 0.71875, 0.71875, 0.71875, 0.71875, 0.6875, 0.65625, 0.65625, 0.71875, 0.71875, 0.671875, 0.65625, 0.65625, 0.640625, 0.671875, 0.671875, 0.671875, 0.671875, 0.703125, 0.671875, 0.71875, 0.71875, 0.71875, 0.671875, 0.625, 0.640625, 0.65625, 0.640625, 0.71875, 0.71875, 0.65625, 0.671875, 0.71875, 0.71875, 0.6875, 0.625, 0.65625, 0.625, 0.640625, 0.71875, 0.71875, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.671875, 0.640625, 0.6875, 0.6875, 0.65625, 0.609375, 0.625, 0.625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00019628842724326648, 'batch_size': 96, 'epochs': 160, 'hidden_size': 150, 'dropout': 0.12901991410993827, 'weight_decay': 0.007362352653625316, 'label_smoothing': 0.03673574445903703, 'grad_clip_norm': 1.732546835510576, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 23853, 'model_storage_size_kb': 102.49335937500001, 'model_size_validation': 'PASS'}
2025-10-13 22:44:17,416 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:44:17,416 - INFO - _models.training_function_executor - Model: 23,853 parameters, 102.5KB (PASS 256KB limit)
2025-10-13 22:44:17,416 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.223s
2025-10-13 22:44:17,500 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:44:17,500 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.083s
2025-10-13 22:44:17,500 - INFO - bo.run_bo - Recorded observation #43: hparams={'lr': 0.00019628842724326648, 'batch_size': np.int64(96), 'epochs': np.int64(160), 'hidden_size': np.int64(150), 'dropout': 0.12901991410993827, 'weight_decay': 0.007362352653625316, 'label_smoothing': 0.03673574445903703, 'grad_clip_norm': 1.732546835510576, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:44:17,500 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 43: {'lr': 0.00019628842724326648, 'batch_size': np.int64(96), 'epochs': np.int64(160), 'hidden_size': np.int64(150), 'dropout': 0.12901991410993827, 'weight_decay': 0.007362352653625316, 'label_smoothing': 0.03673574445903703, 'grad_clip_norm': 1.732546835510576, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:44:17,500 - INFO - bo.run_bo - üîçBO Trial 44: Using RF surrogate + Expected Improvement
2025-10-13 22:44:17,500 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:17,500 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 44 (NaN monitoring active)
2025-10-13 22:44:17,500 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:17,500 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:17,500 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0347852102364797e-05, 'batch_size': 128, 'epochs': 76, 'hidden_size': 154, 'dropout': 0.13164142087419126, 'weight_decay': 0.008303897964670199, 'label_smoothing': 0.04036894906887044, 'grad_clip_norm': 2.4210128789633476, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:44:17,501 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0347852102364797e-05, 'batch_size': 128, 'epochs': 76, 'hidden_size': 154, 'dropout': 0.13164142087419126, 'weight_decay': 0.008303897964670199, 'label_smoothing': 0.04036894906887044, 'grad_clip_norm': 2.4210128789633476, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:44:21,251 - INFO - _models.training_function_executor - Epoch 001/076 | train_loss=8.0781 | val_loss=7.5109 | val_acc=0.0938
2025-10-13 22:44:21,258 - INFO - _models.training_function_executor - Epoch 002/076 | train_loss=7.5924 | val_loss=7.5109 | val_acc=0.0938
2025-10-13 22:44:21,263 - INFO - _models.training_function_executor - Epoch 003/076 | train_loss=8.2507 | val_loss=7.4217 | val_acc=0.0938
2025-10-13 22:44:21,268 - INFO - _models.training_function_executor - Epoch 004/076 | train_loss=7.4764 | val_loss=7.2403 | val_acc=0.0938
2025-10-13 22:44:21,272 - INFO - _models.training_function_executor - Epoch 005/076 | train_loss=7.2014 | val_loss=7.0606 | val_acc=0.0938
2025-10-13 22:44:21,276 - INFO - _models.training_function_executor - Epoch 006/076 | train_loss=7.3432 | val_loss=6.8805 | val_acc=0.0938
2025-10-13 22:44:21,280 - INFO - _models.training_function_executor - Epoch 007/076 | train_loss=7.0112 | val_loss=6.7060 | val_acc=0.0938
2025-10-13 22:44:21,285 - INFO - _models.training_function_executor - Epoch 008/076 | train_loss=6.4825 | val_loss=6.5308 | val_acc=0.0938
2025-10-13 22:44:21,289 - INFO - _models.training_function_executor - Epoch 009/076 | train_loss=6.4972 | val_loss=6.3531 | val_acc=0.0938
2025-10-13 22:44:21,293 - INFO - _models.training_function_executor - Epoch 010/076 | train_loss=6.1622 | val_loss=6.1836 | val_acc=0.0938
2025-10-13 22:44:21,298 - INFO - _models.training_function_executor - Epoch 011/076 | train_loss=5.9491 | val_loss=6.0109 | val_acc=0.0938
2025-10-13 22:44:21,302 - INFO - _models.training_function_executor - Epoch 012/076 | train_loss=6.5149 | val_loss=5.8377 | val_acc=0.0938
2025-10-13 22:44:21,306 - INFO - _models.training_function_executor - Epoch 013/076 | train_loss=6.0889 | val_loss=5.6722 | val_acc=0.0938
2025-10-13 22:44:21,309 - INFO - _models.training_function_executor - Epoch 014/076 | train_loss=6.1654 | val_loss=5.5048 | val_acc=0.0938
2025-10-13 22:44:21,313 - INFO - _models.training_function_executor - Epoch 015/076 | train_loss=5.5068 | val_loss=5.3406 | val_acc=0.0938
2025-10-13 22:44:21,318 - INFO - _models.training_function_executor - Epoch 016/076 | train_loss=5.9591 | val_loss=5.1804 | val_acc=0.0938
2025-10-13 22:44:21,321 - INFO - _models.training_function_executor - Epoch 017/076 | train_loss=5.7169 | val_loss=5.0172 | val_acc=0.0938
2025-10-13 22:44:21,325 - INFO - _models.training_function_executor - Epoch 018/076 | train_loss=5.7425 | val_loss=4.8641 | val_acc=0.0938
2025-10-13 22:44:21,330 - INFO - _models.training_function_executor - Epoch 019/076 | train_loss=5.3463 | val_loss=4.7122 | val_acc=0.0938
2025-10-13 22:44:21,335 - INFO - _models.training_function_executor - Epoch 020/076 | train_loss=4.9975 | val_loss=4.5632 | val_acc=0.0938
2025-10-13 22:44:21,340 - INFO - _models.training_function_executor - Epoch 021/076 | train_loss=5.0650 | val_loss=4.4176 | val_acc=0.0938
2025-10-13 22:44:21,345 - INFO - _models.training_function_executor - Epoch 022/076 | train_loss=5.2711 | val_loss=4.2721 | val_acc=0.0938
2025-10-13 22:44:21,349 - INFO - _models.training_function_executor - Epoch 023/076 | train_loss=4.7960 | val_loss=4.1357 | val_acc=0.0938
2025-10-13 22:44:21,353 - INFO - _models.training_function_executor - Epoch 024/076 | train_loss=4.8158 | val_loss=3.9995 | val_acc=0.0938
2025-10-13 22:44:21,357 - INFO - _models.training_function_executor - Epoch 025/076 | train_loss=4.6577 | val_loss=3.8713 | val_acc=0.0938
2025-10-13 22:44:21,361 - INFO - _models.training_function_executor - Epoch 026/076 | train_loss=4.2751 | val_loss=3.7413 | val_acc=0.0938
2025-10-13 22:44:21,366 - INFO - _models.training_function_executor - Epoch 027/076 | train_loss=4.6051 | val_loss=3.6228 | val_acc=0.0938
2025-10-13 22:44:21,370 - INFO - _models.training_function_executor - Epoch 028/076 | train_loss=4.2346 | val_loss=3.5035 | val_acc=0.0938
2025-10-13 22:44:21,374 - INFO - _models.training_function_executor - Epoch 029/076 | train_loss=4.7637 | val_loss=3.3937 | val_acc=0.0938
2025-10-13 22:44:21,378 - INFO - _models.training_function_executor - Epoch 030/076 | train_loss=4.4015 | val_loss=3.2871 | val_acc=0.0938
2025-10-13 22:44:21,383 - INFO - _models.training_function_executor - Epoch 031/076 | train_loss=4.1896 | val_loss=3.1836 | val_acc=0.0938
2025-10-13 22:44:21,387 - INFO - _models.training_function_executor - Epoch 032/076 | train_loss=3.9546 | val_loss=3.0917 | val_acc=0.0938
2025-10-13 22:44:21,391 - INFO - _models.training_function_executor - Epoch 033/076 | train_loss=3.8581 | val_loss=2.9968 | val_acc=0.0938
2025-10-13 22:44:21,396 - INFO - _models.training_function_executor - Epoch 034/076 | train_loss=4.0806 | val_loss=2.9137 | val_acc=0.0938
2025-10-13 22:44:21,400 - INFO - _models.training_function_executor - Epoch 035/076 | train_loss=4.0182 | val_loss=2.8315 | val_acc=0.0938
2025-10-13 22:44:21,404 - INFO - _models.training_function_executor - Epoch 036/076 | train_loss=3.7730 | val_loss=2.7575 | val_acc=0.0938
2025-10-13 22:44:21,409 - INFO - _models.training_function_executor - Epoch 037/076 | train_loss=3.9536 | val_loss=2.6885 | val_acc=0.0938
2025-10-13 22:44:21,413 - INFO - _models.training_function_executor - Epoch 038/076 | train_loss=3.4738 | val_loss=2.6234 | val_acc=0.0938
2025-10-13 22:44:21,417 - INFO - _models.training_function_executor - Epoch 039/076 | train_loss=3.8961 | val_loss=2.5647 | val_acc=0.0938
2025-10-13 22:44:21,421 - INFO - _models.training_function_executor - Epoch 040/076 | train_loss=3.4194 | val_loss=2.5065 | val_acc=0.0938
2025-10-13 22:44:21,425 - INFO - _models.training_function_executor - Epoch 041/076 | train_loss=3.5512 | val_loss=2.4507 | val_acc=0.0938
2025-10-13 22:44:21,430 - INFO - _models.training_function_executor - Epoch 042/076 | train_loss=3.4433 | val_loss=2.4062 | val_acc=0.0938
2025-10-13 22:44:21,434 - INFO - _models.training_function_executor - Epoch 043/076 | train_loss=3.0223 | val_loss=2.3603 | val_acc=0.0938
2025-10-13 22:44:21,438 - INFO - _models.training_function_executor - Epoch 044/076 | train_loss=3.1540 | val_loss=2.3209 | val_acc=0.0938
2025-10-13 22:44:21,442 - INFO - _models.training_function_executor - Epoch 045/076 | train_loss=3.4961 | val_loss=2.2848 | val_acc=0.0938
2025-10-13 22:44:21,447 - INFO - _models.training_function_executor - Epoch 046/076 | train_loss=3.1700 | val_loss=2.2522 | val_acc=0.0781
2025-10-13 22:44:21,450 - INFO - _models.training_function_executor - Epoch 047/076 | train_loss=3.5799 | val_loss=2.2222 | val_acc=0.1250
2025-10-13 22:44:21,454 - INFO - _models.training_function_executor - Epoch 048/076 | train_loss=3.2000 | val_loss=2.1916 | val_acc=0.1562
2025-10-13 22:44:21,458 - INFO - _models.training_function_executor - Epoch 049/076 | train_loss=3.2009 | val_loss=2.1664 | val_acc=0.2188
2025-10-13 22:44:21,462 - INFO - _models.training_function_executor - Epoch 050/076 | train_loss=3.3121 | val_loss=2.1443 | val_acc=0.2031
2025-10-13 22:44:21,466 - INFO - _models.training_function_executor - Epoch 051/076 | train_loss=3.4584 | val_loss=2.1233 | val_acc=0.2031
2025-10-13 22:44:21,470 - INFO - _models.training_function_executor - Epoch 052/076 | train_loss=3.4670 | val_loss=2.1051 | val_acc=0.2344
2025-10-13 22:44:21,473 - INFO - _models.training_function_executor - Epoch 053/076 | train_loss=3.2926 | val_loss=2.0891 | val_acc=0.2344
2025-10-13 22:44:21,477 - INFO - _models.training_function_executor - Epoch 054/076 | train_loss=2.9642 | val_loss=2.0735 | val_acc=0.2500
2025-10-13 22:44:21,481 - INFO - _models.training_function_executor - Epoch 055/076 | train_loss=3.5288 | val_loss=2.0599 | val_acc=0.2812
2025-10-13 22:44:21,485 - INFO - _models.training_function_executor - Epoch 056/076 | train_loss=3.2047 | val_loss=2.0470 | val_acc=0.2969
2025-10-13 22:44:21,489 - INFO - _models.training_function_executor - Epoch 057/076 | train_loss=3.0178 | val_loss=2.0361 | val_acc=0.3281
2025-10-13 22:44:21,493 - INFO - _models.training_function_executor - Epoch 058/076 | train_loss=3.2917 | val_loss=2.0267 | val_acc=0.3438
2025-10-13 22:44:21,497 - INFO - _models.training_function_executor - Epoch 059/076 | train_loss=3.2314 | val_loss=2.0193 | val_acc=0.4062
2025-10-13 22:44:21,501 - INFO - _models.training_function_executor - Epoch 060/076 | train_loss=3.1406 | val_loss=2.0114 | val_acc=0.4219
2025-10-13 22:44:21,505 - INFO - _models.training_function_executor - Epoch 061/076 | train_loss=3.2871 | val_loss=2.0041 | val_acc=0.4375
2025-10-13 22:44:21,509 - INFO - _models.training_function_executor - Epoch 062/076 | train_loss=3.0890 | val_loss=1.9985 | val_acc=0.4375
2025-10-13 22:44:21,513 - INFO - _models.training_function_executor - Epoch 063/076 | train_loss=3.0644 | val_loss=1.9919 | val_acc=0.4375
2025-10-13 22:44:21,518 - INFO - _models.training_function_executor - Epoch 064/076 | train_loss=3.0744 | val_loss=1.9876 | val_acc=0.4531
2025-10-13 22:44:21,522 - INFO - _models.training_function_executor - Epoch 065/076 | train_loss=3.0050 | val_loss=1.9843 | val_acc=0.4531
2025-10-13 22:44:21,526 - INFO - _models.training_function_executor - Epoch 066/076 | train_loss=3.1839 | val_loss=1.9807 | val_acc=0.4688
2025-10-13 22:44:21,530 - INFO - _models.training_function_executor - Epoch 067/076 | train_loss=2.9669 | val_loss=1.9768 | val_acc=0.4688
2025-10-13 22:44:21,534 - INFO - _models.training_function_executor - Epoch 068/076 | train_loss=3.4548 | val_loss=1.9747 | val_acc=0.4844
2025-10-13 22:44:21,539 - INFO - _models.training_function_executor - Epoch 069/076 | train_loss=2.9774 | val_loss=1.9725 | val_acc=0.4844
2025-10-13 22:44:21,543 - INFO - _models.training_function_executor - Epoch 070/076 | train_loss=3.1842 | val_loss=1.9712 | val_acc=0.4844
2025-10-13 22:44:21,548 - INFO - _models.training_function_executor - Epoch 071/076 | train_loss=2.8486 | val_loss=1.9699 | val_acc=0.4844
2025-10-13 22:44:21,552 - INFO - _models.training_function_executor - Epoch 072/076 | train_loss=2.9556 | val_loss=1.9693 | val_acc=0.4844
2025-10-13 22:44:21,556 - INFO - _models.training_function_executor - Epoch 073/076 | train_loss=3.1269 | val_loss=1.9692 | val_acc=0.4844
2025-10-13 22:44:21,560 - INFO - _models.training_function_executor - Epoch 074/076 | train_loss=2.9733 | val_loss=1.9688 | val_acc=0.4844
2025-10-13 22:44:21,564 - INFO - _models.training_function_executor - Epoch 075/076 | train_loss=3.1895 | val_loss=1.9687 | val_acc=0.4844
2025-10-13 22:44:21,569 - INFO - _models.training_function_executor - Epoch 076/076 | train_loss=3.0191 | val_loss=1.9687 | val_acc=0.4844
2025-10-13 22:44:22,406 - INFO - _models.training_function_executor - Model: 25,105 parameters, 107.9KB storage
2025-10-13 22:44:22,406 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [8.078051090240479, 7.592443943023682, 8.250730037689209, 7.476364374160767, 7.201425313949585, 7.343226671218872, 7.011220932006836, 6.482497215270996, 6.497191905975342, 6.16215443611145, 5.949081897735596, 6.514865159988403, 6.088949680328369, 6.165436744689941, 5.506847381591797, 5.959090709686279, 5.716865062713623, 5.74254846572876, 5.346336126327515, 4.997482776641846, 5.064988136291504, 5.271138668060303, 4.795955181121826, 4.815786600112915, 4.657681465148926, 4.275130748748779, 4.605113983154297, 4.234607100486755, 4.763715028762817, 4.401523113250732, 4.189640760421753, 3.954620838165283, 3.858083724975586, 4.080626130104065, 4.018229961395264, 3.772952675819397, 3.9536421298980713, 3.4738372564315796, 3.8960636854171753, 3.4194029569625854, 3.551178812980652, 3.443320631980896, 3.022331714630127, 3.1540050506591797, 3.496100664138794, 3.169969916343689, 3.5798977613449097, 3.2000410556793213, 3.2009366750717163, 3.3121337890625, 3.4583702087402344, 3.466995358467102, 3.292578339576721, 2.964192032814026, 3.5288325548171997, 3.2046905755996704, 3.017776131629944, 3.291704535484314, 3.2313995361328125, 3.140643000602722, 3.287070155143738, 3.0889906883239746, 3.064405679702759, 3.0744471549987793, 3.0049805641174316, 3.1839327812194824, 2.966894030570984, 3.4548295736312866, 2.9773781299591064, 3.1841760873794556, 2.8485831022262573, 2.955582857131958, 3.1269255876541138, 2.9733009338378906, 3.1894538402557373, 3.0191394090652466], 'val_losses': [7.510862350463867, 7.510862350463867, 7.421679973602295, 7.2403082847595215, 7.060622215270996, 6.88046932220459, 6.706043720245361, 6.530758380889893, 6.35309362411499, 6.1836090087890625, 6.010889053344727, 5.837693691253662, 5.672229290008545, 5.504848480224609, 5.340554237365723, 5.180387496948242, 5.017201900482178, 4.864118576049805, 4.7122483253479, 4.56321907043457, 4.4175825119018555, 4.272118091583252, 4.13569450378418, 3.999504566192627, 3.8713271617889404, 3.741316318511963, 3.6228408813476562, 3.5035126209259033, 3.393725633621216, 3.287095546722412, 3.183600902557373, 3.0917062759399414, 2.9968409538269043, 2.9136738777160645, 2.8315351009368896, 2.7575323581695557, 2.688542366027832, 2.623394727706909, 2.564664363861084, 2.5064802169799805, 2.4506874084472656, 2.406236171722412, 2.3602864742279053, 2.3209118843078613, 2.284839630126953, 2.252206802368164, 2.222182035446167, 2.1916096210479736, 2.1663897037506104, 2.1442854404449463, 2.1233227252960205, 2.1051292419433594, 2.0891499519348145, 2.0734894275665283, 2.059913396835327, 2.0470073223114014, 2.036066770553589, 2.026733875274658, 2.0193023681640625, 2.0113675594329834, 2.0041162967681885, 1.9985071420669556, 1.9918866157531738, 1.9875825643539429, 1.9842971563339233, 1.9806532859802246, 1.9767920970916748, 1.9747016429901123, 1.972490668296814, 1.9711558818817139, 1.969873070716858, 1.969339370727539, 1.9692020416259766, 1.9687585830688477, 1.9687278270721436, 1.968727946281433], 'val_acc': [0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.09375, 0.078125, 0.125, 0.15625, 0.21875, 0.203125, 0.203125, 0.234375, 0.234375, 0.25, 0.28125, 0.296875, 0.328125, 0.34375, 0.40625, 0.421875, 0.4375, 0.4375, 0.4375, 0.453125, 0.453125, 0.46875, 0.46875, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0347852102364797e-05, 'batch_size': 128, 'epochs': 76, 'hidden_size': 154, 'dropout': 0.13164142087419126, 'weight_decay': 0.008303897964670199, 'label_smoothing': 0.04036894906887044, 'grad_clip_norm': 2.4210128789633476, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 25105, 'model_storage_size_kb': 107.87304687500001, 'model_size_validation': 'PASS'}
2025-10-13 22:44:22,406 - INFO - _models.training_function_executor - BO Objective: base=0.4844, size_penalty=0.0000, final=0.4844
2025-10-13 22:44:22,406 - INFO - _models.training_function_executor - Model: 25,105 parameters, 107.9KB (PASS 256KB limit)
2025-10-13 22:44:22,406 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.906s
2025-10-13 22:44:22,488 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.4844
2025-10-13 22:44:22,488 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.081s
2025-10-13 22:44:22,488 - INFO - bo.run_bo - Recorded observation #44: hparams={'lr': 1.0347852102364797e-05, 'batch_size': np.int64(128), 'epochs': np.int64(76), 'hidden_size': np.int64(154), 'dropout': 0.13164142087419126, 'weight_decay': 0.008303897964670199, 'label_smoothing': 0.04036894906887044, 'grad_clip_norm': 2.4210128789633476, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.4844
2025-10-13 22:44:22,488 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 44: {'lr': 1.0347852102364797e-05, 'batch_size': np.int64(128), 'epochs': np.int64(76), 'hidden_size': np.int64(154), 'dropout': 0.13164142087419126, 'weight_decay': 0.008303897964670199, 'label_smoothing': 0.04036894906887044, 'grad_clip_norm': 2.4210128789633476, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.4844
2025-10-13 22:44:22,488 - INFO - bo.run_bo - üîçBO Trial 45: Using RF surrogate + Expected Improvement
2025-10-13 22:44:22,488 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:22,488 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 45 (NaN monitoring active)
2025-10-13 22:44:22,489 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:22,489 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:22,489 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.249819093445811e-05, 'batch_size': 256, 'epochs': 161, 'hidden_size': 112, 'dropout': 0.0995672530994123, 'weight_decay': 3.1696585817753684e-06, 'label_smoothing': 0.10057656913649544, 'grad_clip_norm': 1.112779601855618, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:44:22,489 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.249819093445811e-05, 'batch_size': 256, 'epochs': 161, 'hidden_size': 112, 'dropout': 0.0995672530994123, 'weight_decay': 3.1696585817753684e-06, 'label_smoothing': 0.10057656913649544, 'grad_clip_norm': 1.112779601855618, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:44:26,180 - INFO - _models.training_function_executor - Epoch 001/161 | train_loss=2.6894 | val_loss=1.1487 | val_acc=0.6719
2025-10-13 22:44:26,185 - INFO - _models.training_function_executor - Epoch 002/161 | train_loss=2.6090 | val_loss=1.1487 | val_acc=0.6719
2025-10-13 22:44:26,189 - INFO - _models.training_function_executor - Epoch 003/161 | train_loss=2.7715 | val_loss=1.1487 | val_acc=0.6719
2025-10-13 22:44:26,193 - INFO - _models.training_function_executor - Epoch 004/161 | train_loss=2.8358 | val_loss=1.1487 | val_acc=0.6719
2025-10-13 22:44:26,197 - INFO - _models.training_function_executor - Epoch 005/161 | train_loss=2.8985 | val_loss=1.0985 | val_acc=0.6562
2025-10-13 22:44:26,200 - INFO - _models.training_function_executor - Epoch 006/161 | train_loss=2.7792 | val_loss=1.0597 | val_acc=0.6562
2025-10-13 22:44:26,204 - INFO - _models.training_function_executor - Epoch 007/161 | train_loss=2.4714 | val_loss=1.0307 | val_acc=0.6250
2025-10-13 22:44:26,207 - INFO - _models.training_function_executor - Epoch 008/161 | train_loss=2.5018 | val_loss=1.0086 | val_acc=0.7188
2025-10-13 22:44:26,211 - INFO - _models.training_function_executor - Epoch 009/161 | train_loss=2.5857 | val_loss=0.9957 | val_acc=0.7188
2025-10-13 22:44:26,214 - INFO - _models.training_function_executor - Epoch 010/161 | train_loss=2.4709 | val_loss=0.9923 | val_acc=0.7188
2025-10-13 22:44:26,217 - INFO - _models.training_function_executor - Epoch 011/161 | train_loss=2.5367 | val_loss=0.9929 | val_acc=0.7188
2025-10-13 22:44:26,221 - INFO - _models.training_function_executor - Epoch 012/161 | train_loss=2.2541 | val_loss=1.0001 | val_acc=0.7188
2025-10-13 22:44:26,225 - INFO - _models.training_function_executor - Epoch 013/161 | train_loss=2.3822 | val_loss=1.0126 | val_acc=0.7188
2025-10-13 22:44:26,229 - INFO - _models.training_function_executor - Epoch 014/161 | train_loss=2.3171 | val_loss=1.0288 | val_acc=0.7188
2025-10-13 22:44:26,233 - INFO - _models.training_function_executor - Epoch 015/161 | train_loss=2.0155 | val_loss=1.0491 | val_acc=0.7188
2025-10-13 22:44:26,236 - INFO - _models.training_function_executor - Epoch 016/161 | train_loss=2.2253 | val_loss=1.0710 | val_acc=0.7188
2025-10-13 22:44:26,240 - INFO - _models.training_function_executor - Epoch 017/161 | train_loss=2.2810 | val_loss=1.0961 | val_acc=0.7188
2025-10-13 22:44:26,243 - INFO - _models.training_function_executor - Epoch 018/161 | train_loss=2.1872 | val_loss=1.1213 | val_acc=0.7188
2025-10-13 22:44:26,246 - INFO - _models.training_function_executor - Epoch 019/161 | train_loss=2.3261 | val_loss=1.1432 | val_acc=0.7188
2025-10-13 22:44:26,250 - INFO - _models.training_function_executor - Epoch 020/161 | train_loss=2.1760 | val_loss=1.1693 | val_acc=0.7188
2025-10-13 22:44:26,253 - INFO - _models.training_function_executor - Epoch 021/161 | train_loss=2.3880 | val_loss=1.1913 | val_acc=0.7188
2025-10-13 22:44:26,256 - INFO - _models.training_function_executor - Epoch 022/161 | train_loss=1.9631 | val_loss=1.2096 | val_acc=0.7188
2025-10-13 22:44:26,260 - INFO - _models.training_function_executor - Epoch 023/161 | train_loss=2.1718 | val_loss=1.2251 | val_acc=0.7188
2025-10-13 22:44:26,263 - INFO - _models.training_function_executor - Epoch 024/161 | train_loss=2.0856 | val_loss=1.2396 | val_acc=0.7188
2025-10-13 22:44:26,267 - INFO - _models.training_function_executor - Epoch 025/161 | train_loss=1.9368 | val_loss=1.2477 | val_acc=0.7188
2025-10-13 22:44:26,272 - INFO - _models.training_function_executor - Epoch 026/161 | train_loss=1.9573 | val_loss=1.2502 | val_acc=0.7188
2025-10-13 22:44:26,276 - INFO - _models.training_function_executor - Epoch 027/161 | train_loss=1.9490 | val_loss=1.2473 | val_acc=0.7188
2025-10-13 22:44:26,279 - INFO - _models.training_function_executor - Epoch 028/161 | train_loss=2.0329 | val_loss=1.2449 | val_acc=0.7188
2025-10-13 22:44:26,283 - INFO - _models.training_function_executor - Epoch 029/161 | train_loss=2.3039 | val_loss=1.2403 | val_acc=0.7188
2025-10-13 22:44:26,287 - INFO - _models.training_function_executor - Epoch 030/161 | train_loss=2.0807 | val_loss=1.2326 | val_acc=0.7188
2025-10-13 22:44:26,290 - INFO - _models.training_function_executor - Epoch 031/161 | train_loss=2.1949 | val_loss=1.2203 | val_acc=0.7188
2025-10-13 22:44:26,294 - INFO - _models.training_function_executor - Epoch 032/161 | train_loss=2.0907 | val_loss=1.2080 | val_acc=0.7188
2025-10-13 22:44:26,297 - INFO - _models.training_function_executor - Epoch 033/161 | train_loss=2.0479 | val_loss=1.1958 | val_acc=0.7188
2025-10-13 22:44:26,301 - INFO - _models.training_function_executor - Epoch 034/161 | train_loss=2.0111 | val_loss=1.1826 | val_acc=0.7188
2025-10-13 22:44:26,305 - INFO - _models.training_function_executor - Epoch 035/161 | train_loss=2.0049 | val_loss=1.1709 | val_acc=0.7188
2025-10-13 22:44:26,308 - INFO - _models.training_function_executor - Epoch 036/161 | train_loss=2.2244 | val_loss=1.1591 | val_acc=0.7188
2025-10-13 22:44:26,312 - INFO - _models.training_function_executor - Epoch 037/161 | train_loss=2.0702 | val_loss=1.1474 | val_acc=0.7188
2025-10-13 22:44:26,316 - INFO - _models.training_function_executor - Epoch 038/161 | train_loss=1.9198 | val_loss=1.1385 | val_acc=0.7188
2025-10-13 22:44:26,319 - INFO - _models.training_function_executor - Epoch 039/161 | train_loss=2.0357 | val_loss=1.1285 | val_acc=0.7188
2025-10-13 22:44:26,322 - INFO - _models.training_function_executor - Epoch 040/161 | train_loss=2.0997 | val_loss=1.1173 | val_acc=0.7188
2025-10-13 22:44:26,325 - INFO - _models.training_function_executor - Epoch 041/161 | train_loss=1.9993 | val_loss=1.1097 | val_acc=0.7188
2025-10-13 22:44:26,329 - INFO - _models.training_function_executor - Epoch 042/161 | train_loss=2.0364 | val_loss=1.0983 | val_acc=0.7188
2025-10-13 22:44:26,332 - INFO - _models.training_function_executor - Epoch 043/161 | train_loss=2.1614 | val_loss=1.0870 | val_acc=0.7188
2025-10-13 22:44:26,335 - INFO - _models.training_function_executor - Epoch 044/161 | train_loss=1.9864 | val_loss=1.0811 | val_acc=0.7188
2025-10-13 22:44:26,338 - INFO - _models.training_function_executor - Epoch 045/161 | train_loss=1.9336 | val_loss=1.0774 | val_acc=0.7188
2025-10-13 22:44:26,341 - INFO - _models.training_function_executor - Epoch 046/161 | train_loss=2.0888 | val_loss=1.0761 | val_acc=0.7188
2025-10-13 22:44:26,344 - INFO - _models.training_function_executor - Epoch 047/161 | train_loss=1.8665 | val_loss=1.0778 | val_acc=0.7188
2025-10-13 22:44:26,348 - INFO - _models.training_function_executor - Epoch 048/161 | train_loss=2.0379 | val_loss=1.0768 | val_acc=0.7188
2025-10-13 22:44:26,351 - INFO - _models.training_function_executor - Epoch 049/161 | train_loss=1.9542 | val_loss=1.0766 | val_acc=0.7188
2025-10-13 22:44:26,354 - INFO - _models.training_function_executor - Epoch 050/161 | train_loss=2.1410 | val_loss=1.0734 | val_acc=0.7188
2025-10-13 22:44:26,357 - INFO - _models.training_function_executor - Epoch 051/161 | train_loss=2.0765 | val_loss=1.0719 | val_acc=0.7188
2025-10-13 22:44:26,360 - INFO - _models.training_function_executor - Epoch 052/161 | train_loss=2.2615 | val_loss=1.0736 | val_acc=0.7188
2025-10-13 22:44:26,363 - INFO - _models.training_function_executor - Epoch 053/161 | train_loss=2.1406 | val_loss=1.0705 | val_acc=0.7188
2025-10-13 22:44:26,366 - INFO - _models.training_function_executor - Epoch 054/161 | train_loss=1.9974 | val_loss=1.0637 | val_acc=0.7188
2025-10-13 22:44:26,370 - INFO - _models.training_function_executor - Epoch 055/161 | train_loss=1.9998 | val_loss=1.0589 | val_acc=0.7188
2025-10-13 22:44:26,373 - INFO - _models.training_function_executor - Epoch 056/161 | train_loss=1.8527 | val_loss=1.0559 | val_acc=0.7188
2025-10-13 22:44:26,377 - INFO - _models.training_function_executor - Epoch 057/161 | train_loss=2.2927 | val_loss=1.0543 | val_acc=0.7188
2025-10-13 22:44:26,380 - INFO - _models.training_function_executor - Epoch 058/161 | train_loss=1.8336 | val_loss=1.0564 | val_acc=0.7188
2025-10-13 22:44:26,383 - INFO - _models.training_function_executor - Epoch 059/161 | train_loss=2.2045 | val_loss=1.0594 | val_acc=0.7188
2025-10-13 22:44:26,386 - INFO - _models.training_function_executor - Epoch 060/161 | train_loss=1.8469 | val_loss=1.0643 | val_acc=0.7188
2025-10-13 22:44:26,389 - INFO - _models.training_function_executor - Epoch 061/161 | train_loss=2.2355 | val_loss=1.0678 | val_acc=0.7188
2025-10-13 22:44:26,393 - INFO - _models.training_function_executor - Epoch 062/161 | train_loss=2.0212 | val_loss=1.0699 | val_acc=0.7188
2025-10-13 22:44:26,396 - INFO - _models.training_function_executor - Epoch 063/161 | train_loss=2.1269 | val_loss=1.0701 | val_acc=0.7188
2025-10-13 22:44:26,399 - INFO - _models.training_function_executor - Epoch 064/161 | train_loss=1.9019 | val_loss=1.0684 | val_acc=0.7188
2025-10-13 22:44:26,402 - INFO - _models.training_function_executor - Epoch 065/161 | train_loss=1.9809 | val_loss=1.0687 | val_acc=0.7188
2025-10-13 22:44:26,406 - INFO - _models.training_function_executor - Epoch 066/161 | train_loss=1.9341 | val_loss=1.0680 | val_acc=0.7188
2025-10-13 22:44:26,409 - INFO - _models.training_function_executor - Epoch 067/161 | train_loss=1.9739 | val_loss=1.0653 | val_acc=0.7188
2025-10-13 22:44:26,412 - INFO - _models.training_function_executor - Epoch 068/161 | train_loss=1.8963 | val_loss=1.0647 | val_acc=0.7188
2025-10-13 22:44:26,415 - INFO - _models.training_function_executor - Epoch 069/161 | train_loss=1.9203 | val_loss=1.0663 | val_acc=0.7188
2025-10-13 22:44:26,418 - INFO - _models.training_function_executor - Epoch 070/161 | train_loss=2.1611 | val_loss=1.0645 | val_acc=0.7188
2025-10-13 22:44:26,421 - INFO - _models.training_function_executor - Epoch 071/161 | train_loss=2.1511 | val_loss=1.0638 | val_acc=0.7188
2025-10-13 22:44:26,424 - INFO - _models.training_function_executor - Epoch 072/161 | train_loss=2.2139 | val_loss=1.0611 | val_acc=0.7188
2025-10-13 22:44:26,428 - INFO - _models.training_function_executor - Epoch 073/161 | train_loss=1.8108 | val_loss=1.0572 | val_acc=0.7188
2025-10-13 22:44:26,431 - INFO - _models.training_function_executor - Epoch 074/161 | train_loss=1.8070 | val_loss=1.0502 | val_acc=0.7188
2025-10-13 22:44:26,434 - INFO - _models.training_function_executor - Epoch 075/161 | train_loss=1.7716 | val_loss=1.0444 | val_acc=0.7188
2025-10-13 22:44:26,438 - INFO - _models.training_function_executor - Epoch 076/161 | train_loss=1.9086 | val_loss=1.0382 | val_acc=0.7188
2025-10-13 22:44:26,441 - INFO - _models.training_function_executor - Epoch 077/161 | train_loss=2.2648 | val_loss=1.0323 | val_acc=0.7188
2025-10-13 22:44:26,444 - INFO - _models.training_function_executor - Epoch 078/161 | train_loss=2.0660 | val_loss=1.0287 | val_acc=0.7188
2025-10-13 22:44:26,448 - INFO - _models.training_function_executor - Epoch 079/161 | train_loss=2.0814 | val_loss=1.0252 | val_acc=0.7188
2025-10-13 22:44:26,451 - INFO - _models.training_function_executor - Epoch 080/161 | train_loss=1.8803 | val_loss=1.0234 | val_acc=0.7188
2025-10-13 22:44:26,454 - INFO - _models.training_function_executor - Epoch 081/161 | train_loss=2.2535 | val_loss=1.0207 | val_acc=0.7188
2025-10-13 22:44:26,458 - INFO - _models.training_function_executor - Epoch 082/161 | train_loss=2.0757 | val_loss=1.0180 | val_acc=0.7188
2025-10-13 22:44:26,461 - INFO - _models.training_function_executor - Epoch 083/161 | train_loss=1.7089 | val_loss=1.0180 | val_acc=0.7188
2025-10-13 22:44:26,464 - INFO - _models.training_function_executor - Epoch 084/161 | train_loss=2.0036 | val_loss=1.0197 | val_acc=0.7188
2025-10-13 22:44:26,468 - INFO - _models.training_function_executor - Epoch 085/161 | train_loss=2.0241 | val_loss=1.0198 | val_acc=0.7188
2025-10-13 22:44:26,471 - INFO - _models.training_function_executor - Epoch 086/161 | train_loss=2.1662 | val_loss=1.0205 | val_acc=0.7188
2025-10-13 22:44:26,476 - INFO - _models.training_function_executor - Epoch 087/161 | train_loss=1.8585 | val_loss=1.0210 | val_acc=0.7188
2025-10-13 22:44:26,479 - INFO - _models.training_function_executor - Epoch 088/161 | train_loss=2.0529 | val_loss=1.0203 | val_acc=0.7188
2025-10-13 22:44:26,483 - INFO - _models.training_function_executor - Epoch 089/161 | train_loss=2.1256 | val_loss=1.0187 | val_acc=0.7188
2025-10-13 22:44:26,486 - INFO - _models.training_function_executor - Epoch 090/161 | train_loss=1.8727 | val_loss=1.0159 | val_acc=0.7188
2025-10-13 22:44:26,490 - INFO - _models.training_function_executor - Epoch 091/161 | train_loss=2.2027 | val_loss=1.0165 | val_acc=0.7188
2025-10-13 22:44:26,493 - INFO - _models.training_function_executor - Epoch 092/161 | train_loss=2.1254 | val_loss=1.0160 | val_acc=0.7188
2025-10-13 22:44:26,496 - INFO - _models.training_function_executor - Epoch 093/161 | train_loss=2.1452 | val_loss=1.0153 | val_acc=0.7188
2025-10-13 22:44:26,500 - INFO - _models.training_function_executor - Epoch 094/161 | train_loss=1.9028 | val_loss=1.0172 | val_acc=0.7188
2025-10-13 22:44:26,503 - INFO - _models.training_function_executor - Epoch 095/161 | train_loss=1.9751 | val_loss=1.0168 | val_acc=0.7188
2025-10-13 22:44:26,506 - INFO - _models.training_function_executor - Epoch 096/161 | train_loss=2.1864 | val_loss=1.0172 | val_acc=0.7188
2025-10-13 22:44:26,510 - INFO - _models.training_function_executor - Epoch 097/161 | train_loss=1.9991 | val_loss=1.0174 | val_acc=0.7188
2025-10-13 22:44:26,513 - INFO - _models.training_function_executor - Epoch 098/161 | train_loss=2.2046 | val_loss=1.0155 | val_acc=0.7188
2025-10-13 22:44:26,517 - INFO - _models.training_function_executor - Epoch 099/161 | train_loss=1.8291 | val_loss=1.0149 | val_acc=0.7188
2025-10-13 22:44:26,520 - INFO - _models.training_function_executor - Epoch 100/161 | train_loss=2.0777 | val_loss=1.0136 | val_acc=0.7188
2025-10-13 22:44:26,524 - INFO - _models.training_function_executor - Epoch 101/161 | train_loss=1.9206 | val_loss=1.0123 | val_acc=0.7188
2025-10-13 22:44:26,527 - INFO - _models.training_function_executor - Epoch 102/161 | train_loss=1.8912 | val_loss=1.0096 | val_acc=0.7188
2025-10-13 22:44:26,531 - INFO - _models.training_function_executor - Epoch 103/161 | train_loss=2.1603 | val_loss=1.0088 | val_acc=0.7188
2025-10-13 22:44:26,534 - INFO - _models.training_function_executor - Epoch 104/161 | train_loss=1.9901 | val_loss=1.0082 | val_acc=0.7188
2025-10-13 22:44:26,537 - INFO - _models.training_function_executor - Epoch 105/161 | train_loss=2.1197 | val_loss=1.0068 | val_acc=0.7188
2025-10-13 22:44:26,541 - INFO - _models.training_function_executor - Epoch 106/161 | train_loss=2.0283 | val_loss=1.0055 | val_acc=0.7188
2025-10-13 22:44:26,544 - INFO - _models.training_function_executor - Epoch 107/161 | train_loss=2.0655 | val_loss=1.0055 | val_acc=0.7188
2025-10-13 22:44:26,548 - INFO - _models.training_function_executor - Epoch 108/161 | train_loss=1.9612 | val_loss=1.0063 | val_acc=0.7188
2025-10-13 22:44:26,551 - INFO - _models.training_function_executor - Epoch 109/161 | train_loss=1.8030 | val_loss=1.0064 | val_acc=0.7188
2025-10-13 22:44:26,554 - INFO - _models.training_function_executor - Epoch 110/161 | train_loss=2.0493 | val_loss=1.0065 | val_acc=0.7188
2025-10-13 22:44:26,558 - INFO - _models.training_function_executor - Epoch 111/161 | train_loss=1.9324 | val_loss=1.0063 | val_acc=0.7188
2025-10-13 22:44:26,561 - INFO - _models.training_function_executor - Epoch 112/161 | train_loss=1.7292 | val_loss=1.0068 | val_acc=0.7188
2025-10-13 22:44:26,564 - INFO - _models.training_function_executor - Epoch 113/161 | train_loss=1.9988 | val_loss=1.0070 | val_acc=0.7188
2025-10-13 22:44:26,568 - INFO - _models.training_function_executor - Epoch 114/161 | train_loss=2.0551 | val_loss=1.0059 | val_acc=0.7188
2025-10-13 22:44:26,571 - INFO - _models.training_function_executor - Epoch 115/161 | train_loss=1.9393 | val_loss=1.0055 | val_acc=0.7188
2025-10-13 22:44:26,575 - INFO - _models.training_function_executor - Epoch 116/161 | train_loss=1.9040 | val_loss=1.0049 | val_acc=0.7188
2025-10-13 22:44:26,579 - INFO - _models.training_function_executor - Epoch 117/161 | train_loss=1.8435 | val_loss=1.0037 | val_acc=0.7188
2025-10-13 22:44:26,583 - INFO - _models.training_function_executor - Epoch 118/161 | train_loss=2.0998 | val_loss=1.0026 | val_acc=0.7188
2025-10-13 22:44:26,586 - INFO - _models.training_function_executor - Epoch 119/161 | train_loss=1.8775 | val_loss=1.0010 | val_acc=0.7188
2025-10-13 22:44:26,590 - INFO - _models.training_function_executor - Epoch 120/161 | train_loss=2.1743 | val_loss=1.0002 | val_acc=0.7188
2025-10-13 22:44:26,593 - INFO - _models.training_function_executor - Epoch 121/161 | train_loss=1.9459 | val_loss=0.9982 | val_acc=0.7188
2025-10-13 22:44:26,596 - INFO - _models.training_function_executor - Epoch 122/161 | train_loss=2.0045 | val_loss=0.9967 | val_acc=0.7188
2025-10-13 22:44:26,599 - INFO - _models.training_function_executor - Epoch 123/161 | train_loss=1.8909 | val_loss=0.9960 | val_acc=0.7188
2025-10-13 22:44:26,602 - INFO - _models.training_function_executor - Epoch 124/161 | train_loss=2.0536 | val_loss=0.9951 | val_acc=0.7188
2025-10-13 22:44:26,606 - INFO - _models.training_function_executor - Epoch 125/161 | train_loss=2.2216 | val_loss=0.9940 | val_acc=0.7188
2025-10-13 22:44:26,609 - INFO - _models.training_function_executor - Epoch 126/161 | train_loss=2.0937 | val_loss=0.9930 | val_acc=0.7188
2025-10-13 22:44:26,612 - INFO - _models.training_function_executor - Epoch 127/161 | train_loss=2.0552 | val_loss=0.9929 | val_acc=0.7188
2025-10-13 22:44:26,616 - INFO - _models.training_function_executor - Epoch 128/161 | train_loss=2.1757 | val_loss=0.9933 | val_acc=0.7188
2025-10-13 22:44:26,619 - INFO - _models.training_function_executor - Epoch 129/161 | train_loss=2.1758 | val_loss=0.9927 | val_acc=0.7188
2025-10-13 22:44:26,622 - INFO - _models.training_function_executor - Epoch 130/161 | train_loss=2.0037 | val_loss=0.9928 | val_acc=0.7188
2025-10-13 22:44:26,625 - INFO - _models.training_function_executor - Epoch 131/161 | train_loss=2.0015 | val_loss=0.9927 | val_acc=0.7188
2025-10-13 22:44:26,629 - INFO - _models.training_function_executor - Epoch 132/161 | train_loss=1.7602 | val_loss=0.9921 | val_acc=0.7188
2025-10-13 22:44:26,632 - INFO - _models.training_function_executor - Epoch 133/161 | train_loss=2.1232 | val_loss=0.9922 | val_acc=0.7188
2025-10-13 22:44:26,635 - INFO - _models.training_function_executor - Epoch 134/161 | train_loss=2.0547 | val_loss=0.9921 | val_acc=0.7188
2025-10-13 22:44:26,639 - INFO - _models.training_function_executor - Epoch 135/161 | train_loss=1.9663 | val_loss=0.9915 | val_acc=0.7188
2025-10-13 22:44:26,642 - INFO - _models.training_function_executor - Epoch 136/161 | train_loss=1.9404 | val_loss=0.9913 | val_acc=0.7188
2025-10-13 22:44:26,645 - INFO - _models.training_function_executor - Epoch 137/161 | train_loss=1.8650 | val_loss=0.9910 | val_acc=0.7188
2025-10-13 22:44:26,649 - INFO - _models.training_function_executor - Epoch 138/161 | train_loss=1.8028 | val_loss=0.9908 | val_acc=0.7188
2025-10-13 22:44:26,653 - INFO - _models.training_function_executor - Epoch 139/161 | train_loss=1.8711 | val_loss=0.9907 | val_acc=0.7188
2025-10-13 22:44:26,656 - INFO - _models.training_function_executor - Epoch 140/161 | train_loss=1.7139 | val_loss=0.9905 | val_acc=0.7188
2025-10-13 22:44:26,659 - INFO - _models.training_function_executor - Epoch 141/161 | train_loss=1.6714 | val_loss=0.9905 | val_acc=0.7188
2025-10-13 22:44:26,663 - INFO - _models.training_function_executor - Epoch 142/161 | train_loss=2.0077 | val_loss=0.9897 | val_acc=0.7188
2025-10-13 22:44:26,666 - INFO - _models.training_function_executor - Epoch 143/161 | train_loss=2.1524 | val_loss=0.9897 | val_acc=0.7188
2025-10-13 22:44:26,669 - INFO - _models.training_function_executor - Epoch 144/161 | train_loss=2.0764 | val_loss=0.9899 | val_acc=0.7188
2025-10-13 22:44:26,672 - INFO - _models.training_function_executor - Epoch 145/161 | train_loss=1.9926 | val_loss=0.9899 | val_acc=0.7188
2025-10-13 22:44:26,676 - INFO - _models.training_function_executor - Epoch 146/161 | train_loss=2.0268 | val_loss=0.9897 | val_acc=0.7188
2025-10-13 22:44:26,680 - INFO - _models.training_function_executor - Epoch 147/161 | train_loss=1.9691 | val_loss=0.9893 | val_acc=0.7188
2025-10-13 22:44:26,683 - INFO - _models.training_function_executor - Epoch 148/161 | train_loss=1.9384 | val_loss=0.9891 | val_acc=0.7188
2025-10-13 22:44:26,686 - INFO - _models.training_function_executor - Epoch 149/161 | train_loss=2.0416 | val_loss=0.9888 | val_acc=0.7188
2025-10-13 22:44:26,689 - INFO - _models.training_function_executor - Epoch 150/161 | train_loss=1.8619 | val_loss=0.9885 | val_acc=0.7188
2025-10-13 22:44:26,692 - INFO - _models.training_function_executor - Epoch 151/161 | train_loss=1.9123 | val_loss=0.9884 | val_acc=0.7188
2025-10-13 22:44:26,695 - INFO - _models.training_function_executor - Epoch 152/161 | train_loss=1.9715 | val_loss=0.9884 | val_acc=0.7188
2025-10-13 22:44:26,699 - INFO - _models.training_function_executor - Epoch 153/161 | train_loss=2.0016 | val_loss=0.9883 | val_acc=0.7188
2025-10-13 22:44:26,702 - INFO - _models.training_function_executor - Epoch 154/161 | train_loss=1.7987 | val_loss=0.9885 | val_acc=0.7188
2025-10-13 22:44:26,705 - INFO - _models.training_function_executor - Epoch 155/161 | train_loss=2.1607 | val_loss=0.9885 | val_acc=0.7188
2025-10-13 22:44:26,708 - INFO - _models.training_function_executor - Epoch 156/161 | train_loss=2.0545 | val_loss=0.9884 | val_acc=0.7188
2025-10-13 22:44:26,712 - INFO - _models.training_function_executor - Epoch 157/161 | train_loss=1.9224 | val_loss=0.9885 | val_acc=0.7188
2025-10-13 22:44:26,715 - INFO - _models.training_function_executor - Epoch 158/161 | train_loss=1.7577 | val_loss=0.9885 | val_acc=0.7188
2025-10-13 22:44:26,718 - INFO - _models.training_function_executor - Epoch 159/161 | train_loss=2.0064 | val_loss=0.9884 | val_acc=0.7188
2025-10-13 22:44:26,722 - INFO - _models.training_function_executor - Epoch 160/161 | train_loss=2.0193 | val_loss=0.9886 | val_acc=0.7188
2025-10-13 22:44:26,726 - INFO - _models.training_function_executor - Epoch 161/161 | train_loss=1.6358 | val_loss=0.9886 | val_acc=0.7188
2025-10-13 22:44:27,650 - INFO - _models.training_function_executor - Model: 13,555 parameters, 58.2KB storage
2025-10-13 22:44:27,650 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.689385175704956, 2.6089611053466797, 2.7714667320251465, 2.835761070251465, 2.898527145385742, 2.7792422771453857, 2.471449375152588, 2.5018298625946045, 2.5857067108154297, 2.470916271209717, 2.536677360534668, 2.2541165351867676, 2.3822150230407715, 2.3171002864837646, 2.01552677154541, 2.225297451019287, 2.2810416221618652, 2.1871938705444336, 2.3260629177093506, 2.175995349884033, 2.388000249862671, 1.9631168842315674, 2.171755313873291, 2.0855674743652344, 1.9367908239364624, 1.9573495388031006, 1.9489954710006714, 2.0329248905181885, 2.303940773010254, 2.080719470977783, 2.1948904991149902, 2.090705156326294, 2.0479419231414795, 2.011134147644043, 2.004930019378662, 2.2243809700012207, 2.070241928100586, 1.9198224544525146, 2.0356945991516113, 2.0997233390808105, 1.9993146657943726, 2.036351203918457, 2.1613969802856445, 1.9863743782043457, 1.933553695678711, 2.088820219039917, 1.8665181398391724, 2.0378506183624268, 1.9541996717453003, 2.140953779220581, 2.0764665603637695, 2.2615184783935547, 2.1405837535858154, 1.9973503351211548, 1.9997987747192383, 1.852684736251831, 2.2926571369171143, 1.8336416482925415, 2.2045083045959473, 1.8468787670135498, 2.235464334487915, 2.0212149620056152, 2.1269402503967285, 1.9018977880477905, 1.980866551399231, 1.934135913848877, 1.9739186763763428, 1.8962551355361938, 1.9202826023101807, 2.161109685897827, 2.1510841846466064, 2.213895559310913, 1.8107612133026123, 1.8070197105407715, 1.77157461643219, 1.908595323562622, 2.264843702316284, 2.0660083293914795, 2.081350803375244, 1.8802772760391235, 2.2535228729248047, 2.0757009983062744, 1.7088618278503418, 2.0036017894744873, 2.0241081714630127, 2.16623854637146, 1.8585033416748047, 2.052872657775879, 2.1256449222564697, 1.8727036714553833, 2.202716588973999, 2.125352621078491, 2.1452441215515137, 1.9028244018554688, 1.9751062393188477, 2.1864147186279297, 1.9991350173950195, 2.2045722007751465, 1.8290631771087646, 2.0777201652526855, 1.920615553855896, 1.8912423849105835, 2.160339593887329, 1.9900847673416138, 2.1196680068969727, 2.0282957553863525, 2.065499782562256, 1.9612386226654053, 1.8029736280441284, 2.049300193786621, 1.9323632717132568, 1.7291995286941528, 1.9987900257110596, 2.0550668239593506, 1.9392609596252441, 1.903977394104004, 1.843491792678833, 2.099818468093872, 1.8774977922439575, 2.174281120300293, 1.945920467376709, 2.0044643878936768, 1.8909419775009155, 2.053581476211548, 2.2216415405273438, 2.0936782360076904, 2.0551962852478027, 2.175710916519165, 2.1757569313049316, 2.003666877746582, 2.0015125274658203, 1.7602202892303467, 2.1231842041015625, 2.0546891689300537, 1.9663119316101074, 1.9403893947601318, 1.8650476932525635, 1.8027875423431396, 1.8710963726043701, 1.71392822265625, 1.6714180707931519, 2.0077388286590576, 2.1524295806884766, 2.076350212097168, 1.992605209350586, 2.026848793029785, 1.9691153764724731, 1.9384394884109497, 2.041595935821533, 1.8619366884231567, 1.9123047590255737, 1.9714550971984863, 2.0015623569488525, 1.798656940460205, 2.1606597900390625, 2.054497241973877, 1.9224298000335693, 1.7576903104782104, 2.006357192993164, 2.019326686859131, 1.6357579231262207], 'val_losses': [1.148727536201477, 1.148727536201477, 1.148727536201477, 1.148727536201477, 1.098467469215393, 1.0597304105758667, 1.030746340751648, 1.0086262226104736, 0.9957016706466675, 0.9923396110534668, 0.9929109215736389, 1.000139594078064, 1.0126110315322876, 1.0288162231445312, 1.0490642786026, 1.0710413455963135, 1.0960657596588135, 1.121321201324463, 1.1431975364685059, 1.1693382263183594, 1.1913217306137085, 1.209579348564148, 1.2251427173614502, 1.2396354675292969, 1.2476578950881958, 1.2501596212387085, 1.2472965717315674, 1.2449380159378052, 1.2402925491333008, 1.2325880527496338, 1.2203385829925537, 1.2079862356185913, 1.1958227157592773, 1.1826274394989014, 1.1708952188491821, 1.1591453552246094, 1.1473556756973267, 1.1384600400924683, 1.128470540046692, 1.1172831058502197, 1.1097216606140137, 1.098337173461914, 1.0869966745376587, 1.0810742378234863, 1.077438473701477, 1.0761044025421143, 1.0777785778045654, 1.0767544507980347, 1.076576828956604, 1.073358178138733, 1.07185959815979, 1.0735887289047241, 1.0705043077468872, 1.0637317895889282, 1.0588853359222412, 1.055863618850708, 1.0543440580368042, 1.0563627481460571, 1.0594404935836792, 1.0643025636672974, 1.0678482055664062, 1.069854974746704, 1.070146083831787, 1.068434238433838, 1.0687344074249268, 1.0680124759674072, 1.0652918815612793, 1.0647058486938477, 1.0662686824798584, 1.0644699335098267, 1.0638471841812134, 1.0610787868499756, 1.0571926832199097, 1.0502063035964966, 1.044356346130371, 1.038225531578064, 1.0322893857955933, 1.0286822319030762, 1.0252196788787842, 1.0233508348464966, 1.020686149597168, 1.018019199371338, 1.0179780721664429, 1.0196542739868164, 1.0198092460632324, 1.0204644203186035, 1.0210374593734741, 1.0202739238739014, 1.0187076330184937, 1.0158542394638062, 1.0165399312973022, 1.0159963369369507, 1.0152814388275146, 1.0172275304794312, 1.0168073177337646, 1.0172094106674194, 1.017423391342163, 1.0154731273651123, 1.0149070024490356, 1.0136146545410156, 1.012291669845581, 1.0095670223236084, 1.0088469982147217, 1.0082173347473145, 1.0068457126617432, 1.0054945945739746, 1.0054675340652466, 1.006318211555481, 1.0063543319702148, 1.0065186023712158, 1.0062614679336548, 1.0067552328109741, 1.0069659948349, 1.0059494972229004, 1.0055010318756104, 1.004891276359558, 1.003706455230713, 1.0026077032089233, 1.001010775566101, 1.0001808404922485, 0.9981825351715088, 0.996699333190918, 0.9960091710090637, 0.9950602650642395, 0.9940448999404907, 0.9929569363594055, 0.9928824305534363, 0.9932621121406555, 0.9926888942718506, 0.9928425550460815, 0.9926731586456299, 0.9920850992202759, 0.9922024607658386, 0.9920613169670105, 0.9915415644645691, 0.9913241863250732, 0.9909960627555847, 0.9908300042152405, 0.990699291229248, 0.9905162453651428, 0.9905389547348022, 0.9897438287734985, 0.989740252494812, 0.9898744821548462, 0.9898703694343567, 0.9896896481513977, 0.9892730116844177, 0.9891325235366821, 0.9887752532958984, 0.9885358214378357, 0.988418459892273, 0.9883917570114136, 0.9883435368537903, 0.9885249733924866, 0.9885199069976807, 0.9884355068206787, 0.9884690046310425, 0.9884690046310425, 0.9884488582611084, 0.9885701537132263, 0.9885701537132263], 'val_acc': [0.671875, 0.671875, 0.671875, 0.671875, 0.65625, 0.65625, 0.625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.249819093445811e-05, 'batch_size': 256, 'epochs': 161, 'hidden_size': 112, 'dropout': 0.0995672530994123, 'weight_decay': 3.1696585817753684e-06, 'label_smoothing': 0.10057656913649544, 'grad_clip_norm': 1.112779601855618, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 13555, 'model_storage_size_kb': 58.24414062500001, 'model_size_validation': 'PASS'}
2025-10-13 22:44:27,650 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:44:27,650 - INFO - _models.training_function_executor - Model: 13,555 parameters, 58.2KB (PASS 256KB limit)
2025-10-13 22:44:27,650 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.162s
2025-10-13 22:44:28,196 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:44:28,197 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.546s
2025-10-13 22:44:28,197 - INFO - bo.run_bo - Recorded observation #45: hparams={'lr': 2.249819093445811e-05, 'batch_size': np.int64(256), 'epochs': np.int64(161), 'hidden_size': np.int64(112), 'dropout': 0.0995672530994123, 'weight_decay': 3.1696585817753684e-06, 'label_smoothing': 0.10057656913649544, 'grad_clip_norm': 1.112779601855618, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7188
2025-10-13 22:44:28,197 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 45: {'lr': 2.249819093445811e-05, 'batch_size': np.int64(256), 'epochs': np.int64(161), 'hidden_size': np.int64(112), 'dropout': 0.0995672530994123, 'weight_decay': 3.1696585817753684e-06, 'label_smoothing': 0.10057656913649544, 'grad_clip_norm': 1.112779601855618, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7188
2025-10-13 22:44:28,197 - INFO - bo.run_bo - üîçBO Trial 46: Using RF surrogate + Expected Improvement
2025-10-13 22:44:28,197 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:28,197 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 46 (NaN monitoring active)
2025-10-13 22:44:28,197 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:28,197 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:28,197 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0468736073911292e-05, 'batch_size': 64, 'epochs': 46, 'hidden_size': 133, 'dropout': 0.08989805890592024, 'weight_decay': 7.935407735459988e-06, 'label_smoothing': 0.05998397529269074, 'grad_clip_norm': 1.525995900841435, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:44:28,198 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0468736073911292e-05, 'batch_size': 64, 'epochs': 46, 'hidden_size': 133, 'dropout': 0.08989805890592024, 'weight_decay': 7.935407735459988e-06, 'label_smoothing': 0.05998397529269074, 'grad_clip_norm': 1.525995900841435, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:44:32,000 - INFO - _models.training_function_executor - Epoch 001/046 | train_loss=4.0001 | val_loss=3.1019 | val_acc=0.7188
2025-10-13 22:44:32,010 - INFO - _models.training_function_executor - Epoch 002/046 | train_loss=3.6342 | val_loss=3.0102 | val_acc=0.7188
2025-10-13 22:44:32,018 - INFO - _models.training_function_executor - Epoch 003/046 | train_loss=3.3962 | val_loss=2.9248 | val_acc=0.7188
2025-10-13 22:44:32,026 - INFO - _models.training_function_executor - Epoch 004/046 | train_loss=3.2844 | val_loss=2.8411 | val_acc=0.7188
2025-10-13 22:44:32,034 - INFO - _models.training_function_executor - Epoch 005/046 | train_loss=3.2731 | val_loss=2.7558 | val_acc=0.7188
2025-10-13 22:44:32,042 - INFO - _models.training_function_executor - Epoch 006/046 | train_loss=3.5555 | val_loss=2.6691 | val_acc=0.7188
2025-10-13 22:44:32,049 - INFO - _models.training_function_executor - Epoch 007/046 | train_loss=3.2313 | val_loss=2.5928 | val_acc=0.7188
2025-10-13 22:44:32,057 - INFO - _models.training_function_executor - Epoch 008/046 | train_loss=3.4325 | val_loss=2.5198 | val_acc=0.7188
2025-10-13 22:44:32,064 - INFO - _models.training_function_executor - Epoch 009/046 | train_loss=3.0468 | val_loss=2.4446 | val_acc=0.7188
2025-10-13 22:44:32,072 - INFO - _models.training_function_executor - Epoch 010/046 | train_loss=2.8749 | val_loss=2.3781 | val_acc=0.7188
2025-10-13 22:44:32,080 - INFO - _models.training_function_executor - Epoch 011/046 | train_loss=3.2135 | val_loss=2.3100 | val_acc=0.7188
2025-10-13 22:44:32,088 - INFO - _models.training_function_executor - Epoch 012/046 | train_loss=2.7852 | val_loss=2.2471 | val_acc=0.7188
2025-10-13 22:44:32,096 - INFO - _models.training_function_executor - Epoch 013/046 | train_loss=2.9444 | val_loss=2.1811 | val_acc=0.7188
2025-10-13 22:44:32,106 - INFO - _models.training_function_executor - Epoch 014/046 | train_loss=3.1089 | val_loss=2.1165 | val_acc=0.7188
2025-10-13 22:44:32,114 - INFO - _models.training_function_executor - Epoch 015/046 | train_loss=2.7441 | val_loss=2.0563 | val_acc=0.7188
2025-10-13 22:44:32,121 - INFO - _models.training_function_executor - Epoch 016/046 | train_loss=2.7460 | val_loss=2.0060 | val_acc=0.7188
2025-10-13 22:44:32,127 - INFO - _models.training_function_executor - Epoch 017/046 | train_loss=2.9696 | val_loss=1.9513 | val_acc=0.7188
2025-10-13 22:44:32,133 - INFO - _models.training_function_executor - Epoch 018/046 | train_loss=2.3303 | val_loss=1.9023 | val_acc=0.7188
2025-10-13 22:44:32,139 - INFO - _models.training_function_executor - Epoch 019/046 | train_loss=2.7330 | val_loss=1.8551 | val_acc=0.7188
2025-10-13 22:44:32,145 - INFO - _models.training_function_executor - Epoch 020/046 | train_loss=2.4748 | val_loss=1.8042 | val_acc=0.7188
2025-10-13 22:44:32,151 - INFO - _models.training_function_executor - Epoch 021/046 | train_loss=2.3352 | val_loss=1.7592 | val_acc=0.7188
2025-10-13 22:44:32,157 - INFO - _models.training_function_executor - Epoch 022/046 | train_loss=2.5853 | val_loss=1.7160 | val_acc=0.7188
2025-10-13 22:44:32,163 - INFO - _models.training_function_executor - Epoch 023/046 | train_loss=2.4153 | val_loss=1.6818 | val_acc=0.7188
2025-10-13 22:44:32,170 - INFO - _models.training_function_executor - Epoch 024/046 | train_loss=2.6282 | val_loss=1.6541 | val_acc=0.7188
2025-10-13 22:44:32,176 - INFO - _models.training_function_executor - Epoch 025/046 | train_loss=2.3363 | val_loss=1.6244 | val_acc=0.7188
2025-10-13 22:44:32,182 - INFO - _models.training_function_executor - Epoch 026/046 | train_loss=2.4324 | val_loss=1.5963 | val_acc=0.7188
2025-10-13 22:44:32,188 - INFO - _models.training_function_executor - Epoch 027/046 | train_loss=2.2831 | val_loss=1.5701 | val_acc=0.7188
2025-10-13 22:44:32,194 - INFO - _models.training_function_executor - Epoch 028/046 | train_loss=2.3808 | val_loss=1.5460 | val_acc=0.7188
2025-10-13 22:44:32,201 - INFO - _models.training_function_executor - Epoch 029/046 | train_loss=2.1772 | val_loss=1.5268 | val_acc=0.7188
2025-10-13 22:44:32,208 - INFO - _models.training_function_executor - Epoch 030/046 | train_loss=2.3620 | val_loss=1.5115 | val_acc=0.7188
2025-10-13 22:44:32,215 - INFO - _models.training_function_executor - Epoch 031/046 | train_loss=2.3862 | val_loss=1.4969 | val_acc=0.7188
2025-10-13 22:44:32,221 - INFO - _models.training_function_executor - Epoch 032/046 | train_loss=2.5366 | val_loss=1.4851 | val_acc=0.7188
2025-10-13 22:44:32,228 - INFO - _models.training_function_executor - Epoch 033/046 | train_loss=2.4928 | val_loss=1.4758 | val_acc=0.7188
2025-10-13 22:44:32,234 - INFO - _models.training_function_executor - Epoch 034/046 | train_loss=2.4095 | val_loss=1.4670 | val_acc=0.7188
2025-10-13 22:44:32,240 - INFO - _models.training_function_executor - Epoch 035/046 | train_loss=2.3522 | val_loss=1.4598 | val_acc=0.7188
2025-10-13 22:44:32,246 - INFO - _models.training_function_executor - Epoch 036/046 | train_loss=2.2488 | val_loss=1.4545 | val_acc=0.7188
2025-10-13 22:44:32,252 - INFO - _models.training_function_executor - Epoch 037/046 | train_loss=2.2285 | val_loss=1.4495 | val_acc=0.7188
2025-10-13 22:44:32,258 - INFO - _models.training_function_executor - Epoch 038/046 | train_loss=2.2284 | val_loss=1.4451 | val_acc=0.7188
2025-10-13 22:44:32,264 - INFO - _models.training_function_executor - Epoch 039/046 | train_loss=2.2562 | val_loss=1.4419 | val_acc=0.7188
2025-10-13 22:44:32,270 - INFO - _models.training_function_executor - Epoch 040/046 | train_loss=2.4652 | val_loss=1.4396 | val_acc=0.7188
2025-10-13 22:44:32,276 - INFO - _models.training_function_executor - Epoch 041/046 | train_loss=2.2545 | val_loss=1.4370 | val_acc=0.7188
2025-10-13 22:44:32,283 - INFO - _models.training_function_executor - Epoch 042/046 | train_loss=2.3143 | val_loss=1.4352 | val_acc=0.7188
2025-10-13 22:44:32,289 - INFO - _models.training_function_executor - Epoch 043/046 | train_loss=2.2981 | val_loss=1.4338 | val_acc=0.7188
2025-10-13 22:44:32,295 - INFO - _models.training_function_executor - Epoch 044/046 | train_loss=2.3877 | val_loss=1.4331 | val_acc=0.7188
2025-10-13 22:44:32,301 - INFO - _models.training_function_executor - Epoch 045/046 | train_loss=2.1196 | val_loss=1.4330 | val_acc=0.7188
2025-10-13 22:44:32,309 - INFO - _models.training_function_executor - Epoch 046/046 | train_loss=2.3954 | val_loss=1.4330 | val_acc=0.7188
2025-10-13 22:44:33,206 - INFO - _models.training_function_executor - Model: 18,889 parameters, 40.6KB storage
2025-10-13 22:44:33,206 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [4.000054836273193, 3.634220063686371, 3.396168053150177, 3.284428894519806, 3.27311909198761, 3.555451989173889, 3.231287121772766, 3.4325085878372192, 3.0467958450317383, 2.8748979568481445, 3.213478147983551, 2.785207509994507, 2.9444376826286316, 3.1088746786117554, 2.744074583053589, 2.745975613594055, 2.9696145057678223, 2.330308437347412, 2.7329646944999695, 2.4748148918151855, 2.3352456092834473, 2.5853039920330048, 2.41532626748085, 2.628178060054779, 2.336314380168915, 2.432354211807251, 2.2830927968025208, 2.380822241306305, 2.1771546602249146, 2.3620182275772095, 2.3861759901046753, 2.53658789396286, 2.4928154349327087, 2.409528434276581, 2.3522028625011444, 2.2488422095775604, 2.2284954488277435, 2.228406608104706, 2.256237745285034, 2.465236246585846, 2.254544645547867, 2.3143118023872375, 2.2981103360652924, 2.38765949010849, 2.119630068540573, 2.3953873217105865], 'val_losses': [3.101936101913452, 3.0101547241210938, 2.924820899963379, 2.841139078140259, 2.7557730674743652, 2.6690969467163086, 2.5928430557250977, 2.5198044776916504, 2.4446444511413574, 2.3781306743621826, 2.309997081756592, 2.247091293334961, 2.1811165809631348, 2.116476535797119, 2.056278705596924, 2.0060300827026367, 1.9512674808502197, 1.902263879776001, 1.8551125526428223, 1.8042447566986084, 1.7591701745986938, 1.7160426378250122, 1.6817713975906372, 1.6541248559951782, 1.6243823766708374, 1.596340537071228, 1.5700657367706299, 1.5460431575775146, 1.5268405675888062, 1.511498212814331, 1.4968763589859009, 1.485112190246582, 1.4757755994796753, 1.467016339302063, 1.4598489999771118, 1.4545098543167114, 1.4494636058807373, 1.445099949836731, 1.441868782043457, 1.4396029710769653, 1.4370321035385132, 1.435232162475586, 1.4338017702102661, 1.4331399202346802, 1.432985544204712, 1.432986855506897], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0468736073911292e-05, 'batch_size': 64, 'epochs': 46, 'hidden_size': 133, 'dropout': 0.08989805890592024, 'weight_decay': 7.935407735459988e-06, 'label_smoothing': 0.05998397529269074, 'grad_clip_norm': 1.525995900841435, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 18889, 'model_storage_size_kb': 40.5818359375, 'model_size_validation': 'PASS'}
2025-10-13 22:44:33,206 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:44:33,206 - INFO - _models.training_function_executor - Model: 18,889 parameters, 40.6KB (PASS 256KB limit)
2025-10-13 22:44:33,206 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.009s
2025-10-13 22:44:33,290 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:44:33,290 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-10-13 22:44:33,290 - INFO - bo.run_bo - Recorded observation #46: hparams={'lr': 1.0468736073911292e-05, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'hidden_size': np.int64(133), 'dropout': 0.08989805890592024, 'weight_decay': 7.935407735459988e-06, 'label_smoothing': 0.05998397529269074, 'grad_clip_norm': 1.525995900841435, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:44:33,291 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 46: {'lr': 1.0468736073911292e-05, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'hidden_size': np.int64(133), 'dropout': 0.08989805890592024, 'weight_decay': 7.935407735459988e-06, 'label_smoothing': 0.05998397529269074, 'grad_clip_norm': 1.525995900841435, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:44:33,291 - INFO - bo.run_bo - üîçBO Trial 47: Using RF surrogate + Expected Improvement
2025-10-13 22:44:33,291 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:33,291 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 47 (NaN monitoring active)
2025-10-13 22:44:33,291 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:33,291 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:33,291 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.9389861291418073e-05, 'batch_size': 128, 'epochs': 44, 'hidden_size': 179, 'dropout': 0.13270750309685966, 'weight_decay': 0.00015034218446011036, 'label_smoothing': 0.070044817973237, 'grad_clip_norm': 0.6151826415413638, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:44:33,292 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.9389861291418073e-05, 'batch_size': 128, 'epochs': 44, 'hidden_size': 179, 'dropout': 0.13270750309685966, 'weight_decay': 0.00015034218446011036, 'label_smoothing': 0.070044817973237, 'grad_clip_norm': 0.6151826415413638, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:44:36,719 - INFO - _models.training_function_executor - Epoch 001/044 | train_loss=9.0400 | val_loss=8.5788 | val_acc=0.1875
2025-10-13 22:44:36,727 - INFO - _models.training_function_executor - Epoch 002/044 | train_loss=9.5726 | val_loss=8.5788 | val_acc=0.1875
2025-10-13 22:44:36,732 - INFO - _models.training_function_executor - Epoch 003/044 | train_loss=9.8239 | val_loss=8.3454 | val_acc=0.1875
2025-10-13 22:44:36,737 - INFO - _models.training_function_executor - Epoch 004/044 | train_loss=9.0493 | val_loss=7.8828 | val_acc=0.1875
2025-10-13 22:44:36,741 - INFO - _models.training_function_executor - Epoch 005/044 | train_loss=9.4530 | val_loss=7.6561 | val_acc=0.1875
2025-10-13 22:44:36,746 - INFO - _models.training_function_executor - Epoch 006/044 | train_loss=8.6075 | val_loss=7.2051 | val_acc=0.1875
2025-10-13 22:44:36,750 - INFO - _models.training_function_executor - Epoch 007/044 | train_loss=8.3412 | val_loss=6.7549 | val_acc=0.1875
2025-10-13 22:44:36,754 - INFO - _models.training_function_executor - Epoch 008/044 | train_loss=7.4054 | val_loss=6.3169 | val_acc=0.1875
2025-10-13 22:44:36,759 - INFO - _models.training_function_executor - Epoch 009/044 | train_loss=7.8577 | val_loss=5.8865 | val_acc=0.1875
2025-10-13 22:44:36,763 - INFO - _models.training_function_executor - Epoch 010/044 | train_loss=7.0208 | val_loss=5.4644 | val_acc=0.1875
2025-10-13 22:44:36,769 - INFO - _models.training_function_executor - Epoch 011/044 | train_loss=6.1800 | val_loss=5.0569 | val_acc=0.1875
2025-10-13 22:44:36,773 - INFO - _models.training_function_executor - Epoch 012/044 | train_loss=6.0519 | val_loss=4.6577 | val_acc=0.1875
2025-10-13 22:44:36,777 - INFO - _models.training_function_executor - Epoch 013/044 | train_loss=5.7566 | val_loss=4.2787 | val_acc=0.1875
2025-10-13 22:44:36,782 - INFO - _models.training_function_executor - Epoch 014/044 | train_loss=5.2026 | val_loss=3.9102 | val_acc=0.1875
2025-10-13 22:44:36,786 - INFO - _models.training_function_executor - Epoch 015/044 | train_loss=5.5717 | val_loss=3.5619 | val_acc=0.1875
2025-10-13 22:44:36,790 - INFO - _models.training_function_executor - Epoch 016/044 | train_loss=4.8969 | val_loss=3.2267 | val_acc=0.1875
2025-10-13 22:44:36,794 - INFO - _models.training_function_executor - Epoch 017/044 | train_loss=4.7628 | val_loss=2.9195 | val_acc=0.1875
2025-10-13 22:44:36,799 - INFO - _models.training_function_executor - Epoch 018/044 | train_loss=4.2868 | val_loss=2.6301 | val_acc=0.1875
2025-10-13 22:44:36,804 - INFO - _models.training_function_executor - Epoch 019/044 | train_loss=4.2870 | val_loss=2.3655 | val_acc=0.1875
2025-10-13 22:44:36,809 - INFO - _models.training_function_executor - Epoch 020/044 | train_loss=3.4644 | val_loss=2.1260 | val_acc=0.2031
2025-10-13 22:44:36,814 - INFO - _models.training_function_executor - Epoch 021/044 | train_loss=3.8573 | val_loss=1.9195 | val_acc=0.2500
2025-10-13 22:44:36,819 - INFO - _models.training_function_executor - Epoch 022/044 | train_loss=3.4747 | val_loss=1.7425 | val_acc=0.2812
2025-10-13 22:44:36,823 - INFO - _models.training_function_executor - Epoch 023/044 | train_loss=3.8497 | val_loss=1.5950 | val_acc=0.3438
2025-10-13 22:44:36,828 - INFO - _models.training_function_executor - Epoch 024/044 | train_loss=3.8256 | val_loss=1.4789 | val_acc=0.3594
2025-10-13 22:44:36,833 - INFO - _models.training_function_executor - Epoch 025/044 | train_loss=2.8666 | val_loss=1.3885 | val_acc=0.3906
2025-10-13 22:44:36,838 - INFO - _models.training_function_executor - Epoch 026/044 | train_loss=3.1377 | val_loss=1.3251 | val_acc=0.4844
2025-10-13 22:44:36,842 - INFO - _models.training_function_executor - Epoch 027/044 | train_loss=3.0710 | val_loss=1.2794 | val_acc=0.5625
2025-10-13 22:44:36,846 - INFO - _models.training_function_executor - Epoch 028/044 | train_loss=3.0910 | val_loss=1.2484 | val_acc=0.6406
2025-10-13 22:44:36,850 - INFO - _models.training_function_executor - Epoch 029/044 | train_loss=3.5055 | val_loss=1.2281 | val_acc=0.7031
2025-10-13 22:44:36,854 - INFO - _models.training_function_executor - Epoch 030/044 | train_loss=3.3538 | val_loss=1.2168 | val_acc=0.7188
2025-10-13 22:44:36,859 - INFO - _models.training_function_executor - Epoch 031/044 | train_loss=3.0866 | val_loss=1.2118 | val_acc=0.7188
2025-10-13 22:44:36,864 - INFO - _models.training_function_executor - Epoch 032/044 | train_loss=3.0833 | val_loss=1.2105 | val_acc=0.7188
2025-10-13 22:44:36,869 - INFO - _models.training_function_executor - Epoch 033/044 | train_loss=2.7289 | val_loss=1.2114 | val_acc=0.7188
2025-10-13 22:44:36,873 - INFO - _models.training_function_executor - Epoch 034/044 | train_loss=3.2743 | val_loss=1.2137 | val_acc=0.7188
2025-10-13 22:44:36,878 - INFO - _models.training_function_executor - Epoch 035/044 | train_loss=2.5948 | val_loss=1.2173 | val_acc=0.7188
2025-10-13 22:44:36,882 - INFO - _models.training_function_executor - Epoch 036/044 | train_loss=2.8636 | val_loss=1.2205 | val_acc=0.7188
2025-10-13 22:44:36,887 - INFO - _models.training_function_executor - Epoch 037/044 | train_loss=2.9714 | val_loss=1.2233 | val_acc=0.7188
2025-10-13 22:44:36,892 - INFO - _models.training_function_executor - Epoch 038/044 | train_loss=3.1811 | val_loss=1.2258 | val_acc=0.7188
2025-10-13 22:44:36,897 - INFO - _models.training_function_executor - Epoch 039/044 | train_loss=3.1302 | val_loss=1.2273 | val_acc=0.7188
2025-10-13 22:44:36,902 - INFO - _models.training_function_executor - Epoch 040/044 | train_loss=3.3883 | val_loss=1.2290 | val_acc=0.7188
2025-10-13 22:44:36,907 - INFO - _models.training_function_executor - Epoch 041/044 | train_loss=3.1816 | val_loss=1.2298 | val_acc=0.7188
2025-10-13 22:44:36,911 - INFO - _models.training_function_executor - Epoch 042/044 | train_loss=2.8494 | val_loss=1.2299 | val_acc=0.7188
2025-10-13 22:44:36,916 - INFO - _models.training_function_executor - Epoch 043/044 | train_loss=2.5434 | val_loss=1.2302 | val_acc=0.7188
2025-10-13 22:44:36,921 - INFO - _models.training_function_executor - Epoch 044/044 | train_loss=2.8168 | val_loss=1.2301 | val_acc=0.7188
2025-10-13 22:44:37,845 - INFO - _models.training_function_executor - Model: 33,655 parameters, 144.6KB storage
2025-10-13 22:44:37,845 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [9.039971351623535, 9.572551727294922, 9.823854446411133, 9.049257278442383, 9.453028678894043, 8.607543706893921, 8.341225385665894, 7.40535831451416, 7.8576881885528564, 7.020795106887817, 6.179964065551758, 6.05185604095459, 5.756633996963501, 5.202577352523804, 5.571711778640747, 4.8969199657440186, 4.76280665397644, 4.2868125438690186, 4.287028074264526, 3.4643969535827637, 3.85727059841156, 3.4747254848480225, 3.8496522903442383, 3.825567603111267, 2.8665566444396973, 3.1377108097076416, 3.0710350275039673, 3.0910303592681885, 3.50554358959198, 3.3537644147872925, 3.086573004722595, 3.083346366882324, 2.728942036628723, 3.274261951446533, 2.594812035560608, 2.863613724708557, 2.971407651901245, 3.1811084747314453, 3.1301915645599365, 3.388250231742859, 3.1816076040267944, 2.8493975400924683, 2.5434423685073853, 2.816794991493225], 'val_losses': [8.578810691833496, 8.578810691833496, 8.345446586608887, 7.882771968841553, 7.65609073638916, 7.20513916015625, 6.754936695098877, 6.316904544830322, 5.886519908905029, 5.464437007904053, 5.056943893432617, 4.657652378082275, 4.278703689575195, 3.9102272987365723, 3.561910390853882, 3.2266554832458496, 2.919497489929199, 2.6300888061523438, 2.3655033111572266, 2.125965118408203, 1.9194751977920532, 1.7424921989440918, 1.5950114727020264, 1.4788775444030762, 1.3885107040405273, 1.3251079320907593, 1.2793593406677246, 1.2483937740325928, 1.2281057834625244, 1.2167870998382568, 1.2117869853973389, 1.2104898691177368, 1.211411476135254, 1.2136865854263306, 1.2173324823379517, 1.220524549484253, 1.2233318090438843, 1.2258061170578003, 1.227285623550415, 1.2289509773254395, 1.2297624349594116, 1.2299472093582153, 1.2301889657974243, 1.2300776243209839], 'val_acc': [0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.203125, 0.25, 0.28125, 0.34375, 0.359375, 0.390625, 0.484375, 0.5625, 0.640625, 0.703125, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.9389861291418073e-05, 'batch_size': 128, 'epochs': 44, 'hidden_size': 179, 'dropout': 0.13270750309685966, 'weight_decay': 0.00015034218446011036, 'label_smoothing': 0.070044817973237, 'grad_clip_norm': 0.6151826415413638, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 33655, 'model_storage_size_kb': 144.611328125, 'model_size_validation': 'PASS'}
2025-10-13 22:44:37,845 - INFO - _models.training_function_executor - BO Objective: base=0.7188, size_penalty=0.0000, final=0.7188
2025-10-13 22:44:37,845 - INFO - _models.training_function_executor - Model: 33,655 parameters, 144.6KB (PASS 256KB limit)
2025-10-13 22:44:37,845 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.554s
2025-10-13 22:44:37,928 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7188
2025-10-13 22:44:37,928 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.082s
2025-10-13 22:44:37,928 - INFO - bo.run_bo - Recorded observation #47: hparams={'lr': 1.9389861291418073e-05, 'batch_size': np.int64(128), 'epochs': np.int64(44), 'hidden_size': np.int64(179), 'dropout': 0.13270750309685966, 'weight_decay': 0.00015034218446011036, 'label_smoothing': 0.070044817973237, 'grad_clip_norm': 0.6151826415413638, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7188
2025-10-13 22:44:37,928 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 47: {'lr': 1.9389861291418073e-05, 'batch_size': np.int64(128), 'epochs': np.int64(44), 'hidden_size': np.int64(179), 'dropout': 0.13270750309685966, 'weight_decay': 0.00015034218446011036, 'label_smoothing': 0.070044817973237, 'grad_clip_norm': 0.6151826415413638, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7188
2025-10-13 22:44:37,928 - INFO - bo.run_bo - üîçBO Trial 48: Using RF surrogate + Expected Improvement
2025-10-13 22:44:37,928 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:37,928 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 48 (NaN monitoring active)
2025-10-13 22:44:37,928 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:37,928 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:37,928 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0608394939125808e-05, 'batch_size': 64, 'epochs': 54, 'hidden_size': 42, 'dropout': 0.16688607792739732, 'weight_decay': 4.815757332665708e-05, 'label_smoothing': 0.10466118862784853, 'grad_clip_norm': 0.5318637359958835, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:44:37,929 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0608394939125808e-05, 'batch_size': 64, 'epochs': 54, 'hidden_size': 42, 'dropout': 0.16688607792739732, 'weight_decay': 4.815757332665708e-05, 'label_smoothing': 0.10466118862784853, 'grad_clip_norm': 0.5318637359958835, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:44:41,694 - INFO - _models.training_function_executor - Epoch 001/054 | train_loss=8.9456 | val_loss=8.0737 | val_acc=0.1875
2025-10-13 22:44:41,703 - INFO - _models.training_function_executor - Epoch 002/054 | train_loss=8.6682 | val_loss=8.0326 | val_acc=0.1875
2025-10-13 22:44:41,709 - INFO - _models.training_function_executor - Epoch 003/054 | train_loss=8.8302 | val_loss=7.9830 | val_acc=0.1875
2025-10-13 22:44:41,717 - INFO - _models.training_function_executor - Epoch 004/054 | train_loss=8.7921 | val_loss=7.9354 | val_acc=0.1875
2025-10-13 22:44:41,724 - INFO - _models.training_function_executor - Epoch 005/054 | train_loss=8.4355 | val_loss=7.8828 | val_acc=0.1875
2025-10-13 22:44:41,730 - INFO - _models.training_function_executor - Epoch 006/054 | train_loss=8.5145 | val_loss=7.8351 | val_acc=0.1875
2025-10-13 22:44:41,736 - INFO - _models.training_function_executor - Epoch 007/054 | train_loss=8.4800 | val_loss=7.7858 | val_acc=0.1875
2025-10-13 22:44:41,742 - INFO - _models.training_function_executor - Epoch 008/054 | train_loss=8.7109 | val_loss=7.7349 | val_acc=0.1875
2025-10-13 22:44:41,748 - INFO - _models.training_function_executor - Epoch 009/054 | train_loss=8.4124 | val_loss=7.6872 | val_acc=0.1875
2025-10-13 22:44:41,754 - INFO - _models.training_function_executor - Epoch 010/054 | train_loss=8.5364 | val_loss=7.6416 | val_acc=0.1875
2025-10-13 22:44:41,760 - INFO - _models.training_function_executor - Epoch 011/054 | train_loss=8.3830 | val_loss=7.5945 | val_acc=0.1875
2025-10-13 22:44:41,767 - INFO - _models.training_function_executor - Epoch 012/054 | train_loss=8.1238 | val_loss=7.5485 | val_acc=0.1875
2025-10-13 22:44:41,773 - INFO - _models.training_function_executor - Epoch 013/054 | train_loss=8.2685 | val_loss=7.5065 | val_acc=0.1875
2025-10-13 22:44:41,781 - INFO - _models.training_function_executor - Epoch 014/054 | train_loss=8.1137 | val_loss=7.4568 | val_acc=0.1875
2025-10-13 22:44:41,789 - INFO - _models.training_function_executor - Epoch 015/054 | train_loss=7.8351 | val_loss=7.4155 | val_acc=0.1875
2025-10-13 22:44:41,796 - INFO - _models.training_function_executor - Epoch 016/054 | train_loss=8.0394 | val_loss=7.3745 | val_acc=0.1875
2025-10-13 22:44:41,802 - INFO - _models.training_function_executor - Epoch 017/054 | train_loss=7.7113 | val_loss=7.3357 | val_acc=0.1875
2025-10-13 22:44:41,809 - INFO - _models.training_function_executor - Epoch 018/054 | train_loss=8.1545 | val_loss=7.2966 | val_acc=0.1875
2025-10-13 22:44:41,816 - INFO - _models.training_function_executor - Epoch 019/054 | train_loss=7.9926 | val_loss=7.2579 | val_acc=0.1875
2025-10-13 22:44:41,823 - INFO - _models.training_function_executor - Epoch 020/054 | train_loss=7.4228 | val_loss=7.2227 | val_acc=0.1875
2025-10-13 22:44:41,829 - INFO - _models.training_function_executor - Epoch 021/054 | train_loss=7.9269 | val_loss=7.1871 | val_acc=0.1875
2025-10-13 22:44:41,835 - INFO - _models.training_function_executor - Epoch 022/054 | train_loss=8.0655 | val_loss=7.1486 | val_acc=0.1875
2025-10-13 22:44:41,841 - INFO - _models.training_function_executor - Epoch 023/054 | train_loss=7.7485 | val_loss=7.1181 | val_acc=0.1875
2025-10-13 22:44:41,847 - INFO - _models.training_function_executor - Epoch 024/054 | train_loss=7.7529 | val_loss=7.0876 | val_acc=0.1875
2025-10-13 22:44:41,853 - INFO - _models.training_function_executor - Epoch 025/054 | train_loss=7.8137 | val_loss=7.0635 | val_acc=0.1875
2025-10-13 22:44:41,859 - INFO - _models.training_function_executor - Epoch 026/054 | train_loss=7.5811 | val_loss=7.0298 | val_acc=0.1875
2025-10-13 22:44:41,866 - INFO - _models.training_function_executor - Epoch 027/054 | train_loss=7.6222 | val_loss=7.0056 | val_acc=0.1875
2025-10-13 22:44:41,872 - INFO - _models.training_function_executor - Epoch 028/054 | train_loss=7.2364 | val_loss=6.9810 | val_acc=0.1875
2025-10-13 22:44:41,878 - INFO - _models.training_function_executor - Epoch 029/054 | train_loss=6.9824 | val_loss=6.9548 | val_acc=0.1875
2025-10-13 22:44:41,884 - INFO - _models.training_function_executor - Epoch 030/054 | train_loss=7.3043 | val_loss=6.9333 | val_acc=0.1875
2025-10-13 22:44:41,889 - INFO - _models.training_function_executor - Epoch 031/054 | train_loss=7.3034 | val_loss=6.9160 | val_acc=0.1875
2025-10-13 22:44:41,896 - INFO - _models.training_function_executor - Epoch 032/054 | train_loss=7.8427 | val_loss=6.8950 | val_acc=0.1875
2025-10-13 22:44:41,901 - INFO - _models.training_function_executor - Epoch 033/054 | train_loss=7.3771 | val_loss=6.8736 | val_acc=0.1875
2025-10-13 22:44:41,907 - INFO - _models.training_function_executor - Epoch 034/054 | train_loss=7.4882 | val_loss=6.8568 | val_acc=0.1875
2025-10-13 22:44:41,913 - INFO - _models.training_function_executor - Epoch 035/054 | train_loss=7.5848 | val_loss=6.8436 | val_acc=0.1875
2025-10-13 22:44:41,920 - INFO - _models.training_function_executor - Epoch 036/054 | train_loss=7.6564 | val_loss=6.8273 | val_acc=0.1875
2025-10-13 22:44:41,926 - INFO - _models.training_function_executor - Epoch 037/054 | train_loss=7.3737 | val_loss=6.8162 | val_acc=0.1875
2025-10-13 22:44:41,932 - INFO - _models.training_function_executor - Epoch 038/054 | train_loss=7.3523 | val_loss=6.8034 | val_acc=0.1875
2025-10-13 22:44:41,939 - INFO - _models.training_function_executor - Epoch 039/054 | train_loss=7.2166 | val_loss=6.7955 | val_acc=0.1875
2025-10-13 22:44:41,945 - INFO - _models.training_function_executor - Epoch 040/054 | train_loss=7.5386 | val_loss=6.7885 | val_acc=0.1875
2025-10-13 22:44:41,952 - INFO - _models.training_function_executor - Epoch 041/054 | train_loss=7.1420 | val_loss=6.7819 | val_acc=0.1875
2025-10-13 22:44:41,958 - INFO - _models.training_function_executor - Epoch 042/054 | train_loss=7.4026 | val_loss=6.7750 | val_acc=0.1875
2025-10-13 22:44:41,964 - INFO - _models.training_function_executor - Epoch 043/054 | train_loss=7.6308 | val_loss=6.7683 | val_acc=0.1875
2025-10-13 22:44:41,970 - INFO - _models.training_function_executor - Epoch 044/054 | train_loss=7.2614 | val_loss=6.7645 | val_acc=0.1875
2025-10-13 22:44:41,976 - INFO - _models.training_function_executor - Epoch 045/054 | train_loss=7.5519 | val_loss=6.7581 | val_acc=0.1875
2025-10-13 22:44:41,983 - INFO - _models.training_function_executor - Epoch 046/054 | train_loss=7.5666 | val_loss=6.7554 | val_acc=0.1875
2025-10-13 22:44:41,989 - INFO - _models.training_function_executor - Epoch 047/054 | train_loss=7.6464 | val_loss=6.7539 | val_acc=0.1875
2025-10-13 22:44:41,995 - INFO - _models.training_function_executor - Epoch 048/054 | train_loss=7.7572 | val_loss=6.7511 | val_acc=0.1875
2025-10-13 22:44:42,003 - INFO - _models.training_function_executor - Epoch 049/054 | train_loss=7.5930 | val_loss=6.7504 | val_acc=0.1875
2025-10-13 22:44:42,010 - INFO - _models.training_function_executor - Epoch 050/054 | train_loss=7.5513 | val_loss=6.7499 | val_acc=0.1875
2025-10-13 22:44:42,017 - INFO - _models.training_function_executor - Epoch 051/054 | train_loss=6.8196 | val_loss=6.7486 | val_acc=0.1875
2025-10-13 22:44:42,025 - INFO - _models.training_function_executor - Epoch 052/054 | train_loss=7.4696 | val_loss=6.7485 | val_acc=0.1875
2025-10-13 22:44:42,032 - INFO - _models.training_function_executor - Epoch 053/054 | train_loss=7.8056 | val_loss=6.7469 | val_acc=0.1875
2025-10-13 22:44:42,040 - INFO - _models.training_function_executor - Epoch 054/054 | train_loss=7.4486 | val_loss=6.7469 | val_acc=0.1875
2025-10-13 22:44:42,981 - INFO - _models.training_function_executor - Model: 2,145 parameters, 9.2KB storage
2025-10-13 22:44:42,981 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [8.94556474685669, 8.66821825504303, 8.830190896987915, 8.792102575302124, 8.43551254272461, 8.514520645141602, 8.480005383491516, 8.710866928100586, 8.412429332733154, 8.536396741867065, 8.383042216300964, 8.123830199241638, 8.268503785133362, 8.11371111869812, 7.835113525390625, 8.039419054985046, 7.711267113685608, 8.154468059539795, 7.992631912231445, 7.422810912132263, 7.926922798156738, 8.065531015396118, 7.748540282249451, 7.752866744995117, 7.813695073127747, 7.581104040145874, 7.622199296951294, 7.236368417739868, 6.9824076890945435, 7.304344892501831, 7.30341100692749, 7.842741250991821, 7.377065420150757, 7.488198280334473, 7.58484947681427, 7.6563944816589355, 7.373653411865234, 7.352319240570068, 7.216555714607239, 7.538597464561462, 7.141985058784485, 7.402561068534851, 7.630778908729553, 7.26143491268158, 7.551854610443115, 7.566561222076416, 7.646417617797852, 7.757190108299255, 7.593000769615173, 7.551271915435791, 6.819641828536987, 7.469611406326294, 7.805572271347046, 7.448589444160461], 'val_losses': [8.07368278503418, 8.032639503479004, 7.98297643661499, 7.935429096221924, 7.882781982421875, 7.835078239440918, 7.7857818603515625, 7.734928131103516, 7.687224388122559, 7.641567707061768, 7.594456195831299, 7.548469543457031, 7.506495475769043, 7.456817626953125, 7.415513515472412, 7.37452507019043, 7.335726261138916, 7.296587944030762, 7.257855415344238, 7.222708225250244, 7.1870551109313965, 7.148639678955078, 7.118096351623535, 7.087599277496338, 7.063468933105469, 7.0297627449035645, 7.005569934844971, 6.981026649475098, 6.954794883728027, 6.933281421661377, 6.916037082672119, 6.894999980926514, 6.873605728149414, 6.856766700744629, 6.8435540199279785, 6.827287197113037, 6.816168308258057, 6.803439617156982, 6.795466423034668, 6.788525581359863, 6.781912326812744, 6.77495002746582, 6.768342971801758, 6.764492988586426, 6.758114814758301, 6.7553510665893555, 6.753933906555176, 6.751137733459473, 6.75038480758667, 6.749873161315918, 6.748594284057617, 6.748533248901367, 6.746875286102295, 6.746875286102295], 'val_acc': [0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0608394939125808e-05, 'batch_size': 64, 'epochs': 54, 'hidden_size': 42, 'dropout': 0.16688607792739732, 'weight_decay': 4.815757332665708e-05, 'label_smoothing': 0.10466118862784853, 'grad_clip_norm': 0.5318637359958835, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 2145, 'model_storage_size_kb': 9.216796875, 'model_size_validation': 'PASS'}
2025-10-13 22:44:42,981 - INFO - _models.training_function_executor - BO Objective: base=0.1875, size_penalty=0.0000, final=0.1875
2025-10-13 22:44:42,982 - INFO - _models.training_function_executor - Model: 2,145 parameters, 9.2KB (PASS 256KB limit)
2025-10-13 22:44:42,982 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.053s
2025-10-13 22:44:43,066 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1875
2025-10-13 22:44:43,066 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-10-13 22:44:43,066 - INFO - bo.run_bo - Recorded observation #48: hparams={'lr': 1.0608394939125808e-05, 'batch_size': np.int64(64), 'epochs': np.int64(54), 'hidden_size': np.int64(42), 'dropout': 0.16688607792739732, 'weight_decay': 4.815757332665708e-05, 'label_smoothing': 0.10466118862784853, 'grad_clip_norm': 0.5318637359958835, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.1875
2025-10-13 22:44:43,066 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 48: {'lr': 1.0608394939125808e-05, 'batch_size': np.int64(64), 'epochs': np.int64(54), 'hidden_size': np.int64(42), 'dropout': 0.16688607792739732, 'weight_decay': 4.815757332665708e-05, 'label_smoothing': 0.10466118862784853, 'grad_clip_norm': 0.5318637359958835, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.1875
2025-10-13 22:44:43,067 - INFO - bo.run_bo - üîçBO Trial 49: Using RF surrogate + Expected Improvement
2025-10-13 22:44:43,067 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:43,067 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 49 (NaN monitoring active)
2025-10-13 22:44:43,067 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:43,067 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:43,067 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0014360412672891987, 'batch_size': 256, 'epochs': 51, 'hidden_size': 120, 'dropout': 0.21347876681324096, 'weight_decay': 2.0470265897435614e-06, 'label_smoothing': 0.0845466219220468, 'grad_clip_norm': 0.8697453724004678, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:44:43,068 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0014360412672891987, 'batch_size': 256, 'epochs': 51, 'hidden_size': 120, 'dropout': 0.21347876681324096, 'weight_decay': 2.0470265897435614e-06, 'label_smoothing': 0.0845466219220468, 'grad_clip_norm': 0.8697453724004678, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-13 22:44:46,759 - INFO - _models.training_function_executor - Epoch 001/051 | train_loss=9.6136 | val_loss=7.2819 | val_acc=0.1875
2025-10-13 22:44:46,765 - INFO - _models.training_function_executor - Epoch 002/051 | train_loss=9.8363 | val_loss=7.2819 | val_acc=0.1875
2025-10-13 22:44:46,769 - INFO - _models.training_function_executor - Epoch 003/051 | train_loss=9.8527 | val_loss=7.2819 | val_acc=0.1875
2025-10-13 22:44:46,773 - INFO - _models.training_function_executor - Epoch 004/051 | train_loss=9.4365 | val_loss=7.2819 | val_acc=0.1875
2025-10-13 22:44:46,776 - INFO - _models.training_function_executor - Epoch 005/051 | train_loss=9.4872 | val_loss=7.2819 | val_acc=0.1875
2025-10-13 22:44:46,779 - INFO - _models.training_function_executor - Epoch 006/051 | train_loss=9.5801 | val_loss=1.5607 | val_acc=0.2188
2025-10-13 22:44:46,783 - INFO - _models.training_function_executor - Epoch 007/051 | train_loss=4.1311 | val_loss=2.7565 | val_acc=0.7188
2025-10-13 22:44:46,786 - INFO - _models.training_function_executor - Epoch 008/051 | train_loss=2.9604 | val_loss=3.5088 | val_acc=0.7188
2025-10-13 22:44:46,790 - INFO - _models.training_function_executor - Epoch 009/051 | train_loss=3.5563 | val_loss=3.1047 | val_acc=0.7188
2025-10-13 22:44:46,793 - INFO - _models.training_function_executor - Epoch 010/051 | train_loss=3.3614 | val_loss=2.1396 | val_acc=0.7188
2025-10-13 22:44:46,796 - INFO - _models.training_function_executor - Epoch 011/051 | train_loss=3.0826 | val_loss=1.0290 | val_acc=0.7188
2025-10-13 22:44:46,800 - INFO - _models.training_function_executor - Epoch 012/051 | train_loss=2.5075 | val_loss=0.7648 | val_acc=0.7344
2025-10-13 22:44:46,803 - INFO - _models.training_function_executor - Epoch 013/051 | train_loss=2.2188 | val_loss=0.7373 | val_acc=0.7188
2025-10-13 22:44:46,806 - INFO - _models.training_function_executor - Epoch 014/051 | train_loss=2.3976 | val_loss=0.8090 | val_acc=0.7188
2025-10-13 22:44:46,809 - INFO - _models.training_function_executor - Epoch 015/051 | train_loss=2.6199 | val_loss=1.1232 | val_acc=0.7188
2025-10-13 22:44:46,813 - INFO - _models.training_function_executor - Epoch 016/051 | train_loss=2.1707 | val_loss=1.3590 | val_acc=0.7188
2025-10-13 22:44:46,816 - INFO - _models.training_function_executor - Epoch 017/051 | train_loss=2.1479 | val_loss=1.3300 | val_acc=0.7188
2025-10-13 22:44:46,819 - INFO - _models.training_function_executor - Epoch 018/051 | train_loss=2.4162 | val_loss=1.1929 | val_acc=0.7188
2025-10-13 22:44:46,822 - INFO - _models.training_function_executor - Epoch 019/051 | train_loss=2.0789 | val_loss=1.0366 | val_acc=0.7188
2025-10-13 22:44:46,826 - INFO - _models.training_function_executor - Epoch 020/051 | train_loss=1.9030 | val_loss=0.8781 | val_acc=0.6875
2025-10-13 22:44:46,829 - INFO - _models.training_function_executor - Epoch 021/051 | train_loss=1.7889 | val_loss=0.7864 | val_acc=0.7344
2025-10-13 22:44:46,832 - INFO - _models.training_function_executor - Epoch 022/051 | train_loss=1.9365 | val_loss=0.8022 | val_acc=0.7188
2025-10-13 22:44:46,836 - INFO - _models.training_function_executor - Epoch 023/051 | train_loss=1.8236 | val_loss=0.8871 | val_acc=0.7188
2025-10-13 22:44:46,839 - INFO - _models.training_function_executor - Epoch 024/051 | train_loss=1.8053 | val_loss=0.9294 | val_acc=0.7188
2025-10-13 22:44:46,843 - INFO - _models.training_function_executor - Epoch 025/051 | train_loss=1.8080 | val_loss=0.9149 | val_acc=0.7188
2025-10-13 22:44:46,850 - INFO - _models.training_function_executor - Epoch 026/051 | train_loss=1.5017 | val_loss=0.8761 | val_acc=0.7188
2025-10-13 22:44:46,858 - INFO - _models.training_function_executor - Epoch 027/051 | train_loss=1.4374 | val_loss=0.8837 | val_acc=0.7031
2025-10-13 22:44:46,865 - INFO - _models.training_function_executor - Epoch 028/051 | train_loss=1.5163 | val_loss=0.8881 | val_acc=0.6562
2025-10-13 22:44:46,872 - INFO - _models.training_function_executor - Epoch 029/051 | train_loss=1.4023 | val_loss=0.8709 | val_acc=0.6406
2025-10-13 22:44:46,877 - INFO - _models.training_function_executor - Epoch 030/051 | train_loss=1.3746 | val_loss=0.8319 | val_acc=0.6406
2025-10-13 22:44:46,884 - INFO - _models.training_function_executor - Epoch 031/051 | train_loss=1.2902 | val_loss=0.8042 | val_acc=0.6562
2025-10-13 22:44:46,889 - INFO - _models.training_function_executor - Epoch 032/051 | train_loss=1.2686 | val_loss=0.7887 | val_acc=0.7188
2025-10-13 22:44:46,893 - INFO - _models.training_function_executor - Epoch 033/051 | train_loss=1.2987 | val_loss=0.7875 | val_acc=0.7188
2025-10-13 22:44:46,896 - INFO - _models.training_function_executor - Epoch 034/051 | train_loss=1.3763 | val_loss=0.7892 | val_acc=0.7188
2025-10-13 22:44:46,900 - INFO - _models.training_function_executor - Epoch 035/051 | train_loss=1.2755 | val_loss=0.7849 | val_acc=0.7188
2025-10-13 22:44:46,903 - INFO - _models.training_function_executor - Epoch 036/051 | train_loss=1.2750 | val_loss=0.7770 | val_acc=0.7188
2025-10-13 22:44:46,907 - INFO - _models.training_function_executor - Epoch 037/051 | train_loss=1.1620 | val_loss=0.7591 | val_acc=0.7188
2025-10-13 22:44:46,910 - INFO - _models.training_function_executor - Epoch 038/051 | train_loss=1.2379 | val_loss=0.7436 | val_acc=0.7188
2025-10-13 22:44:46,913 - INFO - _models.training_function_executor - Epoch 039/051 | train_loss=1.1816 | val_loss=0.7330 | val_acc=0.7188
2025-10-13 22:44:46,917 - INFO - _models.training_function_executor - Epoch 040/051 | train_loss=1.2243 | val_loss=0.7286 | val_acc=0.7344
2025-10-13 22:44:46,920 - INFO - _models.training_function_executor - Epoch 041/051 | train_loss=1.1831 | val_loss=0.7258 | val_acc=0.7344
2025-10-13 22:44:46,923 - INFO - _models.training_function_executor - Epoch 042/051 | train_loss=1.3445 | val_loss=0.7247 | val_acc=0.7344
2025-10-13 22:44:46,927 - INFO - _models.training_function_executor - Epoch 043/051 | train_loss=1.1903 | val_loss=0.7247 | val_acc=0.7188
2025-10-13 22:44:46,930 - INFO - _models.training_function_executor - Epoch 044/051 | train_loss=1.1772 | val_loss=0.7249 | val_acc=0.7188
2025-10-13 22:44:46,933 - INFO - _models.training_function_executor - Epoch 045/051 | train_loss=1.2750 | val_loss=0.7253 | val_acc=0.7188
2025-10-13 22:44:46,936 - INFO - _models.training_function_executor - Epoch 046/051 | train_loss=1.1477 | val_loss=0.7256 | val_acc=0.7344
2025-10-13 22:44:46,940 - INFO - _models.training_function_executor - Epoch 047/051 | train_loss=1.2749 | val_loss=0.7259 | val_acc=0.7344
2025-10-13 22:44:46,943 - INFO - _models.training_function_executor - Epoch 048/051 | train_loss=1.2623 | val_loss=0.7260 | val_acc=0.7344
2025-10-13 22:44:46,947 - INFO - _models.training_function_executor - Epoch 049/051 | train_loss=1.2377 | val_loss=0.7262 | val_acc=0.7344
2025-10-13 22:44:46,950 - INFO - _models.training_function_executor - Epoch 050/051 | train_loss=1.1365 | val_loss=0.7264 | val_acc=0.7344
2025-10-13 22:44:46,954 - INFO - _models.training_function_executor - Epoch 051/051 | train_loss=1.1532 | val_loss=0.7264 | val_acc=0.7344
2025-10-13 22:44:47,804 - INFO - _models.training_function_executor - Model: 15,483 parameters, 66.5KB storage
2025-10-13 22:44:47,804 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [9.613551139831543, 9.836325645446777, 9.852703094482422, 9.436507225036621, 9.487173080444336, 9.580123901367188, 4.131147384643555, 2.9603817462921143, 3.5562798976898193, 3.36136531829834, 3.0825674533843994, 2.5075087547302246, 2.218775749206543, 2.3976497650146484, 2.6199092864990234, 2.1707301139831543, 2.1479451656341553, 2.4162042140960693, 2.078906536102295, 1.903031349182129, 1.7889448404312134, 1.936453104019165, 1.8235664367675781, 1.8052674531936646, 1.8080487251281738, 1.5016599893569946, 1.43736732006073, 1.5162630081176758, 1.4023269414901733, 1.3745633363723755, 1.2901712656021118, 1.2685554027557373, 1.2987446784973145, 1.3763461112976074, 1.2755237817764282, 1.2749618291854858, 1.1619616746902466, 1.2378520965576172, 1.1815605163574219, 1.2242704629898071, 1.1831345558166504, 1.344530701637268, 1.1903254985809326, 1.1772445440292358, 1.274951696395874, 1.1477100849151611, 1.2749215364456177, 1.2622926235198975, 1.2376998662948608, 1.1364703178405762, 1.15321946144104], 'val_losses': [7.28192138671875, 7.28192138671875, 7.28192138671875, 7.28192138671875, 7.28192138671875, 1.5606640577316284, 2.756453514099121, 3.50875186920166, 3.104681968688965, 2.1395716667175293, 1.0289963483810425, 0.7647610902786255, 0.7373461127281189, 0.809028148651123, 1.1232057809829712, 1.3590121269226074, 1.3299944400787354, 1.1928889751434326, 1.0366199016571045, 0.8780601024627686, 0.7863864898681641, 0.8021695613861084, 0.887137770652771, 0.9294251203536987, 0.9149184226989746, 0.8760799765586853, 0.8836804628372192, 0.8881184458732605, 0.8708844780921936, 0.8318899869918823, 0.8042149543762207, 0.7887218594551086, 0.7874944806098938, 0.7891926169395447, 0.7849088907241821, 0.7769784927368164, 0.7590822577476501, 0.7435799837112427, 0.7329839468002319, 0.7286215424537659, 0.725794792175293, 0.7246507406234741, 0.7246983051300049, 0.7249444723129272, 0.7253298759460449, 0.7256101369857788, 0.7259105443954468, 0.7259986400604248, 0.7262011766433716, 0.7263733148574829, 0.7263674139976501], 'val_acc': [0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.21875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.734375, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.6875, 0.734375, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.703125, 0.65625, 0.640625, 0.640625, 0.65625, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.734375, 0.734375, 0.734375, 0.71875, 0.71875, 0.71875, 0.734375, 0.734375, 0.734375, 0.734375, 0.734375, 0.734375], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0014360412672891987, 'batch_size': 256, 'epochs': 51, 'hidden_size': 120, 'dropout': 0.21347876681324096, 'weight_decay': 2.0470265897435614e-06, 'label_smoothing': 0.0845466219220468, 'grad_clip_norm': 0.8697453724004678, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 15483, 'model_storage_size_kb': 66.52851562500001, 'model_size_validation': 'PASS'}
2025-10-13 22:44:47,804 - INFO - _models.training_function_executor - BO Objective: base=0.7344, size_penalty=0.0000, final=0.7344
2025-10-13 22:44:47,804 - INFO - _models.training_function_executor - Model: 15,483 parameters, 66.5KB (PASS 256KB limit)
2025-10-13 22:44:47,804 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.737s
2025-10-13 22:44:47,890 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7344
2025-10-13 22:44:47,890 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.085s
2025-10-13 22:44:47,890 - INFO - bo.run_bo - Recorded observation #49: hparams={'lr': 0.0014360412672891987, 'batch_size': np.int64(256), 'epochs': np.int64(51), 'hidden_size': np.int64(120), 'dropout': 0.21347876681324096, 'weight_decay': 2.0470265897435614e-06, 'label_smoothing': 0.0845466219220468, 'grad_clip_norm': 0.8697453724004678, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7344
2025-10-13 22:44:47,890 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 49: {'lr': 0.0014360412672891987, 'batch_size': np.int64(256), 'epochs': np.int64(51), 'hidden_size': np.int64(120), 'dropout': 0.21347876681324096, 'weight_decay': 2.0470265897435614e-06, 'label_smoothing': 0.0845466219220468, 'grad_clip_norm': 0.8697453724004678, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7344
2025-10-13 22:44:47,890 - INFO - bo.run_bo - üîçBO Trial 50: Using RF surrogate + Expected Improvement
2025-10-13 22:44:47,890 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-13 22:44:47,890 - INFO - evaluation.code_generation_pipeline_orchestrator - üèÉ Starting trial 50 (NaN monitoring active)
2025-10-13 22:44:47,890 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:47,890 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:47,890 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0720741945502853e-05, 'batch_size': 256, 'epochs': 44, 'hidden_size': 183, 'dropout': 0.07343164894725053, 'weight_decay': 1.262535751664838e-06, 'label_smoothing': 0.06592308110180027, 'grad_clip_norm': 1.5986307125889412, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:44:47,891 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0720741945502853e-05, 'batch_size': 256, 'epochs': 44, 'hidden_size': 183, 'dropout': 0.07343164894725053, 'weight_decay': 1.262535751664838e-06, 'label_smoothing': 0.06592308110180027, 'grad_clip_norm': 1.5986307125889412, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-13 22:44:51,658 - INFO - _models.training_function_executor - Epoch 001/044 | train_loss=3.4727 | val_loss=3.1720 | val_acc=0.7188
2025-10-13 22:44:51,664 - INFO - _models.training_function_executor - Epoch 002/044 | train_loss=3.1204 | val_loss=3.1720 | val_acc=0.7188
2025-10-13 22:44:51,668 - INFO - _models.training_function_executor - Epoch 003/044 | train_loss=3.4851 | val_loss=3.1720 | val_acc=0.7188
2025-10-13 22:44:51,672 - INFO - _models.training_function_executor - Epoch 004/044 | train_loss=3.1522 | val_loss=3.1720 | val_acc=0.7188
2025-10-13 22:44:51,676 - INFO - _models.training_function_executor - Epoch 005/044 | train_loss=3.1882 | val_loss=3.1150 | val_acc=0.7188
2025-10-13 22:44:51,679 - INFO - _models.training_function_executor - Epoch 006/044 | train_loss=3.4521 | val_loss=3.0576 | val_acc=0.7188
2025-10-13 22:44:51,683 - INFO - _models.training_function_executor - Epoch 007/044 | train_loss=2.9828 | val_loss=3.0039 | val_acc=0.7188
2025-10-13 22:44:51,686 - INFO - _models.training_function_executor - Epoch 008/044 | train_loss=3.0345 | val_loss=2.9524 | val_acc=0.7188
2025-10-13 22:44:51,689 - INFO - _models.training_function_executor - Epoch 009/044 | train_loss=3.2249 | val_loss=2.9017 | val_acc=0.7188
2025-10-13 22:44:51,692 - INFO - _models.training_function_executor - Epoch 010/044 | train_loss=2.8069 | val_loss=2.8535 | val_acc=0.7188
2025-10-13 22:44:51,695 - INFO - _models.training_function_executor - Epoch 011/044 | train_loss=2.9649 | val_loss=2.8050 | val_acc=0.7188
2025-10-13 22:44:51,699 - INFO - _models.training_function_executor - Epoch 012/044 | train_loss=2.9336 | val_loss=2.7570 | val_acc=0.7188
2025-10-13 22:44:51,702 - INFO - _models.training_function_executor - Epoch 013/044 | train_loss=2.9079 | val_loss=2.7128 | val_acc=0.7188
2025-10-13 22:44:51,705 - INFO - _models.training_function_executor - Epoch 014/044 | train_loss=3.0682 | val_loss=2.6705 | val_acc=0.7188
2025-10-13 22:44:51,709 - INFO - _models.training_function_executor - Epoch 015/044 | train_loss=2.5744 | val_loss=2.6297 | val_acc=0.7031
2025-10-13 22:44:51,712 - INFO - _models.training_function_executor - Epoch 016/044 | train_loss=3.0037 | val_loss=2.5902 | val_acc=0.7031
2025-10-13 22:44:51,715 - INFO - _models.training_function_executor - Epoch 017/044 | train_loss=2.8215 | val_loss=2.5533 | val_acc=0.7031
2025-10-13 22:44:51,719 - INFO - _models.training_function_executor - Epoch 018/044 | train_loss=2.8156 | val_loss=2.5182 | val_acc=0.7031
2025-10-13 22:44:51,722 - INFO - _models.training_function_executor - Epoch 019/044 | train_loss=2.8130 | val_loss=2.4850 | val_acc=0.7031
2025-10-13 22:44:51,725 - INFO - _models.training_function_executor - Epoch 020/044 | train_loss=2.4774 | val_loss=2.4518 | val_acc=0.7031
2025-10-13 22:44:51,729 - INFO - _models.training_function_executor - Epoch 021/044 | train_loss=2.8452 | val_loss=2.4227 | val_acc=0.7031
2025-10-13 22:44:51,732 - INFO - _models.training_function_executor - Epoch 022/044 | train_loss=2.8520 | val_loss=2.3958 | val_acc=0.7031
2025-10-13 22:44:51,735 - INFO - _models.training_function_executor - Epoch 023/044 | train_loss=2.7194 | val_loss=2.3720 | val_acc=0.7031
2025-10-13 22:44:51,739 - INFO - _models.training_function_executor - Epoch 024/044 | train_loss=2.5040 | val_loss=2.3489 | val_acc=0.7031
2025-10-13 22:44:51,743 - INFO - _models.training_function_executor - Epoch 025/044 | train_loss=2.7883 | val_loss=2.3284 | val_acc=0.7031
2025-10-13 22:44:51,746 - INFO - _models.training_function_executor - Epoch 026/044 | train_loss=2.7712 | val_loss=2.3097 | val_acc=0.7031
2025-10-13 22:44:51,750 - INFO - _models.training_function_executor - Epoch 027/044 | train_loss=3.0233 | val_loss=2.2920 | val_acc=0.6875
2025-10-13 22:44:51,755 - INFO - _models.training_function_executor - Epoch 028/044 | train_loss=2.5667 | val_loss=2.2775 | val_acc=0.6875
2025-10-13 22:44:51,758 - INFO - _models.training_function_executor - Epoch 029/044 | train_loss=2.3890 | val_loss=2.2655 | val_acc=0.6875
2025-10-13 22:44:51,762 - INFO - _models.training_function_executor - Epoch 030/044 | train_loss=2.3369 | val_loss=2.2534 | val_acc=0.6875
2025-10-13 22:44:51,765 - INFO - _models.training_function_executor - Epoch 031/044 | train_loss=2.3935 | val_loss=2.2433 | val_acc=0.6875
2025-10-13 22:44:51,769 - INFO - _models.training_function_executor - Epoch 032/044 | train_loss=2.7834 | val_loss=2.2348 | val_acc=0.6875
2025-10-13 22:44:51,773 - INFO - _models.training_function_executor - Epoch 033/044 | train_loss=2.5236 | val_loss=2.2261 | val_acc=0.6875
2025-10-13 22:44:51,776 - INFO - _models.training_function_executor - Epoch 034/044 | train_loss=2.3312 | val_loss=2.2195 | val_acc=0.6875
2025-10-13 22:44:51,780 - INFO - _models.training_function_executor - Epoch 035/044 | train_loss=2.5672 | val_loss=2.2135 | val_acc=0.6875
2025-10-13 22:44:51,783 - INFO - _models.training_function_executor - Epoch 036/044 | train_loss=2.6685 | val_loss=2.2085 | val_acc=0.6875
2025-10-13 22:44:51,787 - INFO - _models.training_function_executor - Epoch 037/044 | train_loss=2.8471 | val_loss=2.2051 | val_acc=0.6875
2025-10-13 22:44:51,790 - INFO - _models.training_function_executor - Epoch 038/044 | train_loss=2.6017 | val_loss=2.2024 | val_acc=0.6875
2025-10-13 22:44:51,794 - INFO - _models.training_function_executor - Epoch 039/044 | train_loss=2.4119 | val_loss=2.2005 | val_acc=0.6875
2025-10-13 22:44:51,797 - INFO - _models.training_function_executor - Epoch 040/044 | train_loss=2.8071 | val_loss=2.1992 | val_acc=0.6875
2025-10-13 22:44:51,801 - INFO - _models.training_function_executor - Epoch 041/044 | train_loss=2.5633 | val_loss=2.1986 | val_acc=0.6875
2025-10-13 22:44:51,804 - INFO - _models.training_function_executor - Epoch 042/044 | train_loss=2.6409 | val_loss=2.1977 | val_acc=0.6875
2025-10-13 22:44:51,807 - INFO - _models.training_function_executor - Epoch 043/044 | train_loss=2.3301 | val_loss=2.1978 | val_acc=0.6875
2025-10-13 22:44:51,811 - INFO - _models.training_function_executor - Epoch 044/044 | train_loss=2.6499 | val_loss=2.1977 | val_acc=0.6875
2025-10-13 22:44:52,663 - INFO - _models.training_function_executor - Model: 35,139 parameters, 75.5KB storage
2025-10-13 22:44:52,663 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [3.47267484664917, 3.1203787326812744, 3.4851179122924805, 3.152214527130127, 3.18821120262146, 3.4520702362060547, 2.9827513694763184, 3.0345282554626465, 3.2248711585998535, 2.8068807125091553, 2.964871644973755, 2.933629035949707, 2.9078664779663086, 3.068190336227417, 2.574406862258911, 3.003718137741089, 2.821467161178589, 2.8156023025512695, 2.8130249977111816, 2.477426290512085, 2.8451998233795166, 2.852001905441284, 2.7193961143493652, 2.503952741622925, 2.7882609367370605, 2.771172523498535, 3.023289680480957, 2.566709518432617, 2.388965606689453, 2.3368756771087646, 2.3934504985809326, 2.7833502292633057, 2.5236053466796875, 2.3312480449676514, 2.5671794414520264, 2.6684796810150146, 2.8470849990844727, 2.601710557937622, 2.4119009971618652, 2.807075262069702, 2.563314199447632, 2.640942096710205, 2.3300535678863525, 2.649899959564209], 'val_losses': [3.1720473766326904, 3.1720473766326904, 3.1720473766326904, 3.1720473766326904, 3.1149675846099854, 3.0576374530792236, 3.0038976669311523, 2.9523754119873047, 2.9017062187194824, 2.8535053730010986, 2.804973602294922, 2.757004499435425, 2.712752342224121, 2.670544147491455, 2.629743814468384, 2.590198516845703, 2.553286075592041, 2.518188953399658, 2.4849681854248047, 2.4517874717712402, 2.4227094650268555, 2.3957791328430176, 2.3720221519470215, 2.3488924503326416, 2.328422784805298, 2.3097074031829834, 2.2920167446136475, 2.2774837017059326, 2.265500545501709, 2.2533578872680664, 2.2432751655578613, 2.234802722930908, 2.2260677814483643, 2.219538688659668, 2.2135236263275146, 2.208496332168579, 2.2050654888153076, 2.2024455070495605, 2.2004878520965576, 2.199228048324585, 2.198596954345703, 2.1977131366729736, 2.1977851390838623, 2.1977148056030273], 'val_acc': [0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0720741945502853e-05, 'batch_size': 256, 'epochs': 44, 'hidden_size': 183, 'dropout': 0.07343164894725053, 'weight_decay': 1.262535751664838e-06, 'label_smoothing': 0.06592308110180027, 'grad_clip_norm': 1.5986307125889412, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 35139, 'model_storage_size_kb': 75.4939453125, 'model_size_validation': 'PASS'}
2025-10-13 22:44:52,663 - INFO - _models.training_function_executor - BO Objective: base=0.6875, size_penalty=0.0000, final=0.6875
2025-10-13 22:44:52,663 - INFO - _models.training_function_executor - Model: 35,139 parameters, 75.5KB (PASS 256KB limit)
2025-10-13 22:44:52,663 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.773s
2025-10-13 22:44:52,751 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6875
2025-10-13 22:44:52,752 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.088s
2025-10-13 22:44:52,752 - INFO - bo.run_bo - Recorded observation #50: hparams={'lr': 1.0720741945502853e-05, 'batch_size': np.int64(256), 'epochs': np.int64(44), 'hidden_size': np.int64(183), 'dropout': 0.07343164894725053, 'weight_decay': 1.262535751664838e-06, 'label_smoothing': 0.06592308110180027, 'grad_clip_norm': 1.5986307125889412, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.6875
2025-10-13 22:44:52,752 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 50: {'lr': 1.0720741945502853e-05, 'batch_size': np.int64(256), 'epochs': np.int64(44), 'hidden_size': np.int64(183), 'dropout': 0.07343164894725053, 'weight_decay': 1.262535751664838e-06, 'label_smoothing': 0.06592308110180027, 'grad_clip_norm': 1.5986307125889412, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.6875
2025-10-13 22:44:52,752 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.7656
2025-10-13 22:44:52,752 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.007007193583244844, 'batch_size': np.int64(64), 'epochs': np.int64(137), 'hidden_size': np.int64(168), 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}
2025-10-13 22:44:52,752 - INFO - visualization - Generating BO visualization charts with 50 trials...
2025-10-13 22:44:54,482 - INFO - visualization - BO summary saved to: charts/20251013_224452_BO_TinyTabularMLP-PE-Classifier/bo_summary.txt
2025-10-13 22:44:54,495 - INFO - visualization - Raw data saved to: charts/20251013_224452_BO_TinyTabularMLP-PE-Classifier/bo_raw_data.json
2025-10-13 22:44:54,495 - INFO - visualization - Numpy arrays saved to: charts/20251013_224452_BO_TinyTabularMLP-PE-Classifier/bo_raw_data.npz
2025-10-13 22:44:54,495 - INFO - visualization - BO charts saved to: charts/20251013_224452_BO_TinyTabularMLP-PE-Classifier
2025-10-13 22:44:54,495 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä BO charts saved to: charts/20251013_224452_BO_TinyTabularMLP-PE-Classifier
2025-10-13 22:44:54,933 - INFO - evaluation.code_generation_pipeline_orchestrator - üßπ GPU memory cleaned before final training
2025-10-13 22:44:54,933 - INFO - evaluation.code_generation_pipeline_orchestrator - üöÄ STEP 4: Final Training Execution
2025-10-13 22:44:54,933 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (320, 4), Val: (80, 4), Test: (100, 4)
2025-10-13 22:44:54,934 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-13 22:44:54,934 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-13 22:44:54,934 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-13 22:44:55,299 - INFO - evaluation.code_generation_pipeline_orchestrator - Deleted intermediate datasets to free memory before final training
2025-10-13 22:44:55,300 - INFO - _models.training_function_executor - Loaded training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:55,300 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-13 22:44:55,300 - INFO - _models.training_function_executor - Loaded training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:55,300 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-13 22:44:55,300 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.007007193583244844, 'batch_size': np.int64(64), 'epochs': np.int64(137), 'hidden_size': np.int64(168), 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}
2025-10-13 22:44:55,300 - INFO - evaluation.code_generation_pipeline_orchestrator - Using test set for final training evaluation
2025-10-13 22:44:55,300 - INFO - _models.training_function_executor - Using device: cuda
2025-10-13 22:44:55,300 - INFO - _models.training_function_executor - Executing training function: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:55,300 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.007007193583244844, 'batch_size': np.int64(64), 'epochs': np.int64(137), 'hidden_size': np.int64(168), 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}
2025-10-13 22:44:55,301 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.007007193583244844, 'batch_size': 64, 'epochs': 137, 'hidden_size': 168, 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-13 22:44:57,987 - INFO - _models.training_function_executor - Epoch 001/137 | train_loss=0.8751 | val_loss=0.6558 | val_acc=0.7200
2025-10-13 22:44:57,999 - INFO - _models.training_function_executor - Epoch 002/137 | train_loss=0.6151 | val_loss=0.5681 | val_acc=0.7500
2025-10-13 22:44:58,007 - INFO - _models.training_function_executor - Epoch 003/137 | train_loss=0.5210 | val_loss=0.4808 | val_acc=0.7400
2025-10-13 22:44:58,015 - INFO - _models.training_function_executor - Epoch 004/137 | train_loss=0.4283 | val_loss=0.3789 | val_acc=0.8400
2025-10-13 22:44:58,023 - INFO - _models.training_function_executor - Epoch 005/137 | train_loss=0.3495 | val_loss=0.2960 | val_acc=0.9500
2025-10-13 22:44:58,032 - INFO - _models.training_function_executor - Epoch 006/137 | train_loss=0.2749 | val_loss=0.2175 | val_acc=0.9700
2025-10-13 22:44:58,040 - INFO - _models.training_function_executor - Epoch 007/137 | train_loss=0.2147 | val_loss=0.1895 | val_acc=0.9700
2025-10-13 22:44:58,049 - INFO - _models.training_function_executor - Epoch 008/137 | train_loss=0.1943 | val_loss=0.1668 | val_acc=0.9900
2025-10-13 22:44:58,057 - INFO - _models.training_function_executor - Epoch 009/137 | train_loss=0.1763 | val_loss=0.1611 | val_acc=0.9900
2025-10-13 22:44:58,066 - INFO - _models.training_function_executor - Epoch 010/137 | train_loss=0.1760 | val_loss=0.1486 | val_acc=0.9800
2025-10-13 22:44:58,076 - INFO - _models.training_function_executor - Epoch 011/137 | train_loss=0.1572 | val_loss=0.1350 | val_acc=1.0000
2025-10-13 22:44:58,085 - INFO - _models.training_function_executor - Epoch 012/137 | train_loss=0.1466 | val_loss=0.1370 | val_acc=0.9900
2025-10-13 22:44:58,093 - INFO - _models.training_function_executor - Epoch 013/137 | train_loss=0.1441 | val_loss=0.1281 | val_acc=0.9900
2025-10-13 22:44:58,102 - INFO - _models.training_function_executor - Epoch 014/137 | train_loss=0.1416 | val_loss=0.1181 | val_acc=0.9900
2025-10-13 22:44:58,110 - INFO - _models.training_function_executor - Epoch 015/137 | train_loss=0.1355 | val_loss=0.1225 | val_acc=1.0000
2025-10-13 22:44:58,119 - INFO - _models.training_function_executor - Epoch 016/137 | train_loss=0.1417 | val_loss=0.1275 | val_acc=0.9700
2025-10-13 22:44:58,127 - INFO - _models.training_function_executor - Epoch 017/137 | train_loss=0.1342 | val_loss=0.1184 | val_acc=0.9900
2025-10-13 22:44:58,135 - INFO - _models.training_function_executor - Epoch 018/137 | train_loss=0.1295 | val_loss=0.1268 | val_acc=0.9900
2025-10-13 22:44:58,143 - INFO - _models.training_function_executor - Epoch 019/137 | train_loss=0.1285 | val_loss=0.1055 | val_acc=1.0000
2025-10-13 22:44:58,150 - INFO - _models.training_function_executor - Epoch 020/137 | train_loss=0.1198 | val_loss=0.1088 | val_acc=1.0000
2025-10-13 22:44:58,158 - INFO - _models.training_function_executor - Epoch 021/137 | train_loss=0.1315 | val_loss=0.1167 | val_acc=0.9900
2025-10-13 22:44:58,166 - INFO - _models.training_function_executor - Epoch 022/137 | train_loss=0.1266 | val_loss=0.1170 | val_acc=0.9900
2025-10-13 22:44:58,174 - INFO - _models.training_function_executor - Epoch 023/137 | train_loss=0.1237 | val_loss=0.1058 | val_acc=1.0000
2025-10-13 22:44:58,182 - INFO - _models.training_function_executor - Epoch 024/137 | train_loss=0.1187 | val_loss=0.1015 | val_acc=1.0000
2025-10-13 22:44:58,190 - INFO - _models.training_function_executor - Epoch 025/137 | train_loss=0.1179 | val_loss=0.1202 | val_acc=0.9900
2025-10-13 22:44:58,198 - INFO - _models.training_function_executor - Epoch 026/137 | train_loss=0.1152 | val_loss=0.1081 | val_acc=0.9900
2025-10-13 22:44:58,206 - INFO - _models.training_function_executor - Epoch 027/137 | train_loss=0.1153 | val_loss=0.1041 | val_acc=1.0000
2025-10-13 22:44:58,214 - INFO - _models.training_function_executor - Epoch 028/137 | train_loss=0.1160 | val_loss=0.1078 | val_acc=0.9900
2025-10-13 22:44:58,222 - INFO - _models.training_function_executor - Epoch 029/137 | train_loss=0.1167 | val_loss=0.1082 | val_acc=0.9900
2025-10-13 22:44:58,230 - INFO - _models.training_function_executor - Epoch 030/137 | train_loss=0.1140 | val_loss=0.1026 | val_acc=1.0000
2025-10-13 22:44:58,237 - INFO - _models.training_function_executor - Epoch 031/137 | train_loss=0.1268 | val_loss=0.1108 | val_acc=0.9900
2025-10-13 22:44:58,246 - INFO - _models.training_function_executor - Epoch 032/137 | train_loss=0.1199 | val_loss=0.1061 | val_acc=0.9900
2025-10-13 22:44:58,253 - INFO - _models.training_function_executor - Epoch 033/137 | train_loss=0.1147 | val_loss=0.1011 | val_acc=1.0000
2025-10-13 22:44:58,261 - INFO - _models.training_function_executor - Epoch 034/137 | train_loss=0.1104 | val_loss=0.0976 | val_acc=1.0000
2025-10-13 22:44:58,269 - INFO - _models.training_function_executor - Epoch 035/137 | train_loss=0.1159 | val_loss=0.1079 | val_acc=0.9900
2025-10-13 22:44:58,276 - INFO - _models.training_function_executor - Epoch 036/137 | train_loss=0.1200 | val_loss=0.1067 | val_acc=0.9900
2025-10-13 22:44:58,284 - INFO - _models.training_function_executor - Epoch 037/137 | train_loss=0.1170 | val_loss=0.1003 | val_acc=1.0000
2025-10-13 22:44:58,292 - INFO - _models.training_function_executor - Epoch 038/137 | train_loss=0.1075 | val_loss=0.1091 | val_acc=1.0000
2025-10-13 22:44:58,300 - INFO - _models.training_function_executor - Epoch 039/137 | train_loss=0.1117 | val_loss=0.0956 | val_acc=1.0000
2025-10-13 22:44:58,307 - INFO - _models.training_function_executor - Epoch 040/137 | train_loss=0.1088 | val_loss=0.1028 | val_acc=1.0000
2025-10-13 22:44:58,315 - INFO - _models.training_function_executor - Epoch 041/137 | train_loss=0.1092 | val_loss=0.0999 | val_acc=1.0000
2025-10-13 22:44:58,323 - INFO - _models.training_function_executor - Epoch 042/137 | train_loss=0.1063 | val_loss=0.0968 | val_acc=1.0000
2025-10-13 22:44:58,331 - INFO - _models.training_function_executor - Epoch 043/137 | train_loss=0.1096 | val_loss=0.1014 | val_acc=1.0000
2025-10-13 22:44:58,339 - INFO - _models.training_function_executor - Epoch 044/137 | train_loss=0.1136 | val_loss=0.1023 | val_acc=1.0000
2025-10-13 22:44:58,347 - INFO - _models.training_function_executor - Epoch 045/137 | train_loss=0.1072 | val_loss=0.0950 | val_acc=1.0000
2025-10-13 22:44:58,354 - INFO - _models.training_function_executor - Epoch 046/137 | train_loss=0.1116 | val_loss=0.1012 | val_acc=1.0000
2025-10-13 22:44:58,362 - INFO - _models.training_function_executor - Epoch 047/137 | train_loss=0.1091 | val_loss=0.0987 | val_acc=1.0000
2025-10-13 22:44:58,369 - INFO - _models.training_function_executor - Epoch 048/137 | train_loss=0.1049 | val_loss=0.0973 | val_acc=1.0000
2025-10-13 22:44:58,377 - INFO - _models.training_function_executor - Epoch 049/137 | train_loss=0.1081 | val_loss=0.0969 | val_acc=1.0000
2025-10-13 22:44:58,385 - INFO - _models.training_function_executor - Epoch 050/137 | train_loss=0.1060 | val_loss=0.0966 | val_acc=1.0000
2025-10-13 22:44:58,393 - INFO - _models.training_function_executor - Epoch 051/137 | train_loss=0.1039 | val_loss=0.0992 | val_acc=1.0000
2025-10-13 22:44:58,400 - INFO - _models.training_function_executor - Epoch 052/137 | train_loss=0.1077 | val_loss=0.0970 | val_acc=1.0000
2025-10-13 22:44:58,408 - INFO - _models.training_function_executor - Epoch 053/137 | train_loss=0.1064 | val_loss=0.0933 | val_acc=1.0000
2025-10-13 22:44:58,416 - INFO - _models.training_function_executor - Epoch 054/137 | train_loss=0.1054 | val_loss=0.1006 | val_acc=1.0000
2025-10-13 22:44:58,424 - INFO - _models.training_function_executor - Epoch 055/137 | train_loss=0.1078 | val_loss=0.1018 | val_acc=1.0000
2025-10-13 22:44:58,432 - INFO - _models.training_function_executor - Epoch 056/137 | train_loss=0.1056 | val_loss=0.0975 | val_acc=1.0000
2025-10-13 22:44:58,440 - INFO - _models.training_function_executor - Epoch 057/137 | train_loss=0.1045 | val_loss=0.1016 | val_acc=1.0000
2025-10-13 22:44:58,448 - INFO - _models.training_function_executor - Epoch 058/137 | train_loss=0.1104 | val_loss=0.0967 | val_acc=1.0000
2025-10-13 22:44:58,455 - INFO - _models.training_function_executor - Epoch 059/137 | train_loss=0.1037 | val_loss=0.0971 | val_acc=1.0000
2025-10-13 22:44:58,462 - INFO - _models.training_function_executor - Epoch 060/137 | train_loss=0.1056 | val_loss=0.0948 | val_acc=1.0000
2025-10-13 22:44:58,470 - INFO - _models.training_function_executor - Epoch 061/137 | train_loss=0.1018 | val_loss=0.0989 | val_acc=1.0000
2025-10-13 22:44:58,477 - INFO - _models.training_function_executor - Epoch 062/137 | train_loss=0.1038 | val_loss=0.0960 | val_acc=1.0000
2025-10-13 22:44:58,485 - INFO - _models.training_function_executor - Epoch 063/137 | train_loss=0.1052 | val_loss=0.0989 | val_acc=1.0000
2025-10-13 22:44:58,492 - INFO - _models.training_function_executor - Epoch 064/137 | train_loss=0.1040 | val_loss=0.0995 | val_acc=1.0000
2025-10-13 22:44:58,500 - INFO - _models.training_function_executor - Epoch 065/137 | train_loss=0.1054 | val_loss=0.0922 | val_acc=1.0000
2025-10-13 22:44:58,508 - INFO - _models.training_function_executor - Epoch 066/137 | train_loss=0.0990 | val_loss=0.0943 | val_acc=1.0000
2025-10-13 22:44:58,516 - INFO - _models.training_function_executor - Epoch 067/137 | train_loss=0.1051 | val_loss=0.0968 | val_acc=1.0000
2025-10-13 22:44:58,523 - INFO - _models.training_function_executor - Epoch 068/137 | train_loss=0.0997 | val_loss=0.1057 | val_acc=0.9900
2025-10-13 22:44:58,531 - INFO - _models.training_function_executor - Epoch 069/137 | train_loss=0.1031 | val_loss=0.0962 | val_acc=1.0000
2025-10-13 22:44:58,538 - INFO - _models.training_function_executor - Epoch 070/137 | train_loss=0.1051 | val_loss=0.0967 | val_acc=1.0000
2025-10-13 22:44:58,546 - INFO - _models.training_function_executor - Epoch 071/137 | train_loss=0.1051 | val_loss=0.0985 | val_acc=1.0000
2025-10-13 22:44:58,554 - INFO - _models.training_function_executor - Epoch 072/137 | train_loss=0.1028 | val_loss=0.0959 | val_acc=1.0000
2025-10-13 22:44:58,561 - INFO - _models.training_function_executor - Epoch 073/137 | train_loss=0.1010 | val_loss=0.0934 | val_acc=1.0000
2025-10-13 22:44:58,569 - INFO - _models.training_function_executor - Epoch 074/137 | train_loss=0.1007 | val_loss=0.0934 | val_acc=1.0000
2025-10-13 22:44:58,578 - INFO - _models.training_function_executor - Epoch 075/137 | train_loss=0.0982 | val_loss=0.0952 | val_acc=1.0000
2025-10-13 22:44:58,588 - INFO - _models.training_function_executor - Epoch 076/137 | train_loss=0.1004 | val_loss=0.0925 | val_acc=1.0000
2025-10-13 22:44:58,597 - INFO - _models.training_function_executor - Epoch 077/137 | train_loss=0.1046 | val_loss=0.0966 | val_acc=1.0000
2025-10-13 22:44:58,605 - INFO - _models.training_function_executor - Epoch 078/137 | train_loss=0.1013 | val_loss=0.0962 | val_acc=1.0000
2025-10-13 22:44:58,613 - INFO - _models.training_function_executor - Epoch 079/137 | train_loss=0.1034 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:58,621 - INFO - _models.training_function_executor - Epoch 080/137 | train_loss=0.1005 | val_loss=0.0926 | val_acc=1.0000
2025-10-13 22:44:58,629 - INFO - _models.training_function_executor - Epoch 081/137 | train_loss=0.0983 | val_loss=0.0947 | val_acc=1.0000
2025-10-13 22:44:58,637 - INFO - _models.training_function_executor - Epoch 082/137 | train_loss=0.1036 | val_loss=0.0961 | val_acc=1.0000
2025-10-13 22:44:58,645 - INFO - _models.training_function_executor - Epoch 083/137 | train_loss=0.1007 | val_loss=0.0946 | val_acc=1.0000
2025-10-13 22:44:58,652 - INFO - _models.training_function_executor - Epoch 084/137 | train_loss=0.1001 | val_loss=0.0959 | val_acc=1.0000
2025-10-13 22:44:58,660 - INFO - _models.training_function_executor - Epoch 085/137 | train_loss=0.1001 | val_loss=0.0966 | val_acc=1.0000
2025-10-13 22:44:58,667 - INFO - _models.training_function_executor - Epoch 086/137 | train_loss=0.1005 | val_loss=0.0919 | val_acc=1.0000
2025-10-13 22:44:58,675 - INFO - _models.training_function_executor - Epoch 087/137 | train_loss=0.1015 | val_loss=0.0913 | val_acc=1.0000
2025-10-13 22:44:58,682 - INFO - _models.training_function_executor - Epoch 088/137 | train_loss=0.1015 | val_loss=0.0937 | val_acc=1.0000
2025-10-13 22:44:58,690 - INFO - _models.training_function_executor - Epoch 089/137 | train_loss=0.0980 | val_loss=0.0938 | val_acc=1.0000
2025-10-13 22:44:58,697 - INFO - _models.training_function_executor - Epoch 090/137 | train_loss=0.0979 | val_loss=0.0913 | val_acc=1.0000
2025-10-13 22:44:58,705 - INFO - _models.training_function_executor - Epoch 091/137 | train_loss=0.0988 | val_loss=0.0916 | val_acc=1.0000
2025-10-13 22:44:58,713 - INFO - _models.training_function_executor - Epoch 092/137 | train_loss=0.0995 | val_loss=0.0943 | val_acc=1.0000
2025-10-13 22:44:58,721 - INFO - _models.training_function_executor - Epoch 093/137 | train_loss=0.0987 | val_loss=0.0930 | val_acc=1.0000
2025-10-13 22:44:58,729 - INFO - _models.training_function_executor - Epoch 094/137 | train_loss=0.1005 | val_loss=0.0934 | val_acc=1.0000
2025-10-13 22:44:58,737 - INFO - _models.training_function_executor - Epoch 095/137 | train_loss=0.1024 | val_loss=0.0942 | val_acc=1.0000
2025-10-13 22:44:58,744 - INFO - _models.training_function_executor - Epoch 096/137 | train_loss=0.0997 | val_loss=0.0944 | val_acc=1.0000
2025-10-13 22:44:58,752 - INFO - _models.training_function_executor - Epoch 097/137 | train_loss=0.0991 | val_loss=0.0922 | val_acc=1.0000
2025-10-13 22:44:58,760 - INFO - _models.training_function_executor - Epoch 098/137 | train_loss=0.0991 | val_loss=0.0922 | val_acc=1.0000
2025-10-13 22:44:58,768 - INFO - _models.training_function_executor - Epoch 099/137 | train_loss=0.1003 | val_loss=0.0948 | val_acc=1.0000
2025-10-13 22:44:58,776 - INFO - _models.training_function_executor - Epoch 100/137 | train_loss=0.0985 | val_loss=0.0942 | val_acc=1.0000
2025-10-13 22:44:58,784 - INFO - _models.training_function_executor - Epoch 101/137 | train_loss=0.1020 | val_loss=0.0940 | val_acc=1.0000
2025-10-13 22:44:58,791 - INFO - _models.training_function_executor - Epoch 102/137 | train_loss=0.0985 | val_loss=0.0946 | val_acc=1.0000
2025-10-13 22:44:58,799 - INFO - _models.training_function_executor - Epoch 103/137 | train_loss=0.0990 | val_loss=0.0938 | val_acc=1.0000
2025-10-13 22:44:58,807 - INFO - _models.training_function_executor - Epoch 104/137 | train_loss=0.0975 | val_loss=0.0938 | val_acc=1.0000
2025-10-13 22:44:58,815 - INFO - _models.training_function_executor - Epoch 105/137 | train_loss=0.0997 | val_loss=0.0944 | val_acc=1.0000
2025-10-13 22:44:58,823 - INFO - _models.training_function_executor - Epoch 106/137 | train_loss=0.0964 | val_loss=0.0939 | val_acc=1.0000
2025-10-13 22:44:58,831 - INFO - _models.training_function_executor - Epoch 107/137 | train_loss=0.0971 | val_loss=0.0925 | val_acc=1.0000
2025-10-13 22:44:58,839 - INFO - _models.training_function_executor - Epoch 108/137 | train_loss=0.0959 | val_loss=0.0921 | val_acc=1.0000
2025-10-13 22:44:58,847 - INFO - _models.training_function_executor - Epoch 109/137 | train_loss=0.0969 | val_loss=0.0918 | val_acc=1.0000
2025-10-13 22:44:58,855 - INFO - _models.training_function_executor - Epoch 110/137 | train_loss=0.0986 | val_loss=0.0924 | val_acc=1.0000
2025-10-13 22:44:58,863 - INFO - _models.training_function_executor - Epoch 111/137 | train_loss=0.1008 | val_loss=0.0935 | val_acc=1.0000
2025-10-13 22:44:58,871 - INFO - _models.training_function_executor - Epoch 112/137 | train_loss=0.0980 | val_loss=0.0936 | val_acc=1.0000
2025-10-13 22:44:58,879 - INFO - _models.training_function_executor - Epoch 113/137 | train_loss=0.0970 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:58,887 - INFO - _models.training_function_executor - Epoch 114/137 | train_loss=0.0986 | val_loss=0.0925 | val_acc=1.0000
2025-10-13 22:44:58,895 - INFO - _models.training_function_executor - Epoch 115/137 | train_loss=0.0972 | val_loss=0.0924 | val_acc=1.0000
2025-10-13 22:44:58,902 - INFO - _models.training_function_executor - Epoch 116/137 | train_loss=0.0979 | val_loss=0.0927 | val_acc=1.0000
2025-10-13 22:44:58,910 - INFO - _models.training_function_executor - Epoch 117/137 | train_loss=0.0996 | val_loss=0.0928 | val_acc=1.0000
2025-10-13 22:44:58,917 - INFO - _models.training_function_executor - Epoch 118/137 | train_loss=0.0984 | val_loss=0.0930 | val_acc=1.0000
2025-10-13 22:44:58,925 - INFO - _models.training_function_executor - Epoch 119/137 | train_loss=0.0975 | val_loss=0.0933 | val_acc=1.0000
2025-10-13 22:44:58,933 - INFO - _models.training_function_executor - Epoch 120/137 | train_loss=0.0985 | val_loss=0.0932 | val_acc=1.0000
2025-10-13 22:44:58,941 - INFO - _models.training_function_executor - Epoch 121/137 | train_loss=0.0977 | val_loss=0.0932 | val_acc=1.0000
2025-10-13 22:44:58,948 - INFO - _models.training_function_executor - Epoch 122/137 | train_loss=0.0971 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:58,956 - INFO - _models.training_function_executor - Epoch 123/137 | train_loss=0.0968 | val_loss=0.0928 | val_acc=1.0000
2025-10-13 22:44:58,964 - INFO - _models.training_function_executor - Epoch 124/137 | train_loss=0.0991 | val_loss=0.0926 | val_acc=1.0000
2025-10-13 22:44:58,972 - INFO - _models.training_function_executor - Epoch 125/137 | train_loss=0.0969 | val_loss=0.0926 | val_acc=1.0000
2025-10-13 22:44:58,979 - INFO - _models.training_function_executor - Epoch 126/137 | train_loss=0.0964 | val_loss=0.0927 | val_acc=1.0000
2025-10-13 22:44:58,987 - INFO - _models.training_function_executor - Epoch 127/137 | train_loss=0.0992 | val_loss=0.0928 | val_acc=1.0000
2025-10-13 22:44:58,995 - INFO - _models.training_function_executor - Epoch 128/137 | train_loss=0.0976 | val_loss=0.0928 | val_acc=1.0000
2025-10-13 22:44:59,003 - INFO - _models.training_function_executor - Epoch 129/137 | train_loss=0.0973 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:59,010 - INFO - _models.training_function_executor - Epoch 130/137 | train_loss=0.0973 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:59,018 - INFO - _models.training_function_executor - Epoch 131/137 | train_loss=0.0961 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:59,026 - INFO - _models.training_function_executor - Epoch 132/137 | train_loss=0.0972 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:59,033 - INFO - _models.training_function_executor - Epoch 133/137 | train_loss=0.0972 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:59,041 - INFO - _models.training_function_executor - Epoch 134/137 | train_loss=0.0979 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:59,049 - INFO - _models.training_function_executor - Epoch 135/137 | train_loss=0.0968 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:59,056 - INFO - _models.training_function_executor - Epoch 136/137 | train_loss=0.1005 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:59,064 - INFO - _models.training_function_executor - Epoch 137/137 | train_loss=0.1004 | val_loss=0.0929 | val_acc=1.0000
2025-10-13 22:44:59,941 - INFO - _models.training_function_executor - Model: 29,739 parameters, 127.8KB storage
2025-10-13 22:44:59,941 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8751166105270386, 0.6151494801044464, 0.520990526676178, 0.4283147156238556, 0.3494552314281464, 0.2748845428228378, 0.21473954916000365, 0.1943325787782669, 0.17631962895393372, 0.17602239549160004, 0.15719394087791444, 0.14655377864837646, 0.14413370192050934, 0.14162704199552537, 0.1355289563536644, 0.14166218936443328, 0.13420971781015395, 0.1294797718524933, 0.12851676493883132, 0.11983782202005386, 0.13145547062158586, 0.12664610743522645, 0.12372571229934692, 0.11874764263629914, 0.11790206134319306, 0.11524564027786255, 0.11528437435626984, 0.11604302078485489, 0.11671660095453262, 0.11402500718832016, 0.1268032670021057, 0.11987663209438323, 0.11468069702386856, 0.1104421004652977, 0.11589621752500534, 0.11996474266052246, 0.11699083894491195, 0.10745996832847596, 0.1117145374417305, 0.1088348999619484, 0.10923550128936768, 0.10631311535835267, 0.10958262532949448, 0.11363416612148285, 0.10719578117132186, 0.11155572235584259, 0.10908892899751663, 0.10489927977323532, 0.10811853259801865, 0.10603851526975631, 0.10387496948242188, 0.10769418478012086, 0.10642366856336594, 0.10543329119682313, 0.10776667296886444, 0.10564896762371064, 0.10453253090381623, 0.11040899157524109, 0.1037443831562996, 0.10562630742788315, 0.10184039771556855, 0.10381249189376832, 0.10523837804794312, 0.10400038659572601, 0.10538996011018753, 0.09896711558103562, 0.1050913766026497, 0.09974289834499359, 0.10310634821653367, 0.10511873066425323, 0.10509000271558762, 0.10281587839126587, 0.10096307098865509, 0.10068664103746414, 0.09821507632732392, 0.10039785951375961, 0.10458203256130219, 0.10132489204406739, 0.10343121737241745, 0.10048704892396927, 0.09832146316766739, 0.10358621776103974, 0.10073007941246033, 0.10005617439746857, 0.10014019012451172, 0.10051681101322174, 0.10146047770977021, 0.10153454393148423, 0.09804423600435257, 0.09793357104063034, 0.09878383874893189, 0.09953879117965699, 0.09869962483644486, 0.10052878558635711, 0.10237215012311936, 0.09969807118177414, 0.09908750802278518, 0.09912822097539901, 0.1002666249871254, 0.09852094054222107, 0.10198253095149994, 0.09850138872861862, 0.09898577332496643, 0.09750122129917145, 0.09973143935203552, 0.09639730304479599, 0.09708153307437897, 0.095929916203022, 0.09688194245100021, 0.09863891303539277, 0.10080366730690002, 0.09804097563028336, 0.09696372896432877, 0.09856037348508835, 0.09718120545148849, 0.09788261502981185, 0.0996267855167389, 0.0983680710196495, 0.09752438962459564, 0.09851322621107102, 0.09773109704256058, 0.09708452224731445, 0.09679756313562393, 0.09912594109773636, 0.09686010330915451, 0.09635369181632995, 0.09924813657999039, 0.09761054962873458, 0.09730190634727479, 0.0972928449511528, 0.09607190489768982, 0.09717511683702469, 0.09723483622074128, 0.09794122576713563, 0.09676844328641891, 0.10054326206445693, 0.1004118025302887], 'val_losses': [0.6557791256904602, 0.5680949163436889, 0.480786120891571, 0.37887933492660525, 0.2960434627532959, 0.21754741191864013, 0.18947046160697936, 0.16683827638626098, 0.161113920211792, 0.14863415360450744, 0.13495838046073913, 0.13698993682861327, 0.12807897567749024, 0.11813587337732315, 0.12253682732582093, 0.1275128346681595, 0.11841190338134766, 0.12680040121078492, 0.10551658987998963, 0.10880526125431061, 0.11672783851623535, 0.1169765418767929, 0.10579472422599792, 0.10145710617303848, 0.12017862439155579, 0.1081265538930893, 0.10405567318201064, 0.1078460294008255, 0.10815313935279847, 0.10258307456970214, 0.11079312086105347, 0.10607050657272339, 0.10109920382499694, 0.09762987315654754, 0.10786495208740235, 0.10670149207115173, 0.10032844245433807, 0.10910911500453949, 0.0955859124660492, 0.10277624517679214, 0.09993338972330093, 0.09683724343776703, 0.10138719052076339, 0.10225405573844909, 0.09502369195222854, 0.10123602390289306, 0.09871224641799926, 0.09731292217969895, 0.09690610736608506, 0.09660296618938446, 0.09919550269842148, 0.09704474538564682, 0.09328060805797576, 0.10061855971813202, 0.10182361394166946, 0.09753075867891312, 0.10160675227642059, 0.09672700822353363, 0.0971153137087822, 0.09483375161886215, 0.09886514455080032, 0.09602750420570373, 0.09891737192869186, 0.09952422469854355, 0.09218278020620346, 0.0943049293756485, 0.09677455276250839, 0.10567823141813278, 0.0961782255768776, 0.09674479961395263, 0.09853938460350037, 0.09585312068462372, 0.0933634477853775, 0.09341317534446716, 0.0951530909538269, 0.09248232632875443, 0.09657150894403457, 0.09624546974897384, 0.09289069354534149, 0.09260418474674224, 0.09468646913766861, 0.09607580602169037, 0.09455859422683716, 0.09593273878097534, 0.09659898579120636, 0.0918543890118599, 0.09127046704292298, 0.09373434513807297, 0.09380486249923706, 0.09126705288887024, 0.09164047926664352, 0.09434820860624313, 0.09301570296287537, 0.09341087192296982, 0.09421799838542938, 0.09442942231893539, 0.09216107666492462, 0.09217013776302338, 0.09475084304809571, 0.09424715101718903, 0.09403632670640945, 0.09457051545381547, 0.09378463119268417, 0.09380575358867645, 0.09443978101015091, 0.0938823425769806, 0.09247098326683044, 0.09205332607030868, 0.091798857152462, 0.09244186222553254, 0.09351243704557419, 0.09364520370960236, 0.09287383735179901, 0.09253427147865295, 0.0924450021982193, 0.09272390455007554, 0.09279654920101166, 0.09297117799520492, 0.09328844755887986, 0.09321518689393997, 0.09318750828504563, 0.0929230386018753, 0.09275526732206345, 0.09257741779088974, 0.09259853005409241, 0.09273685544729232, 0.09281838685274124, 0.09282111763954162, 0.09285717219114303, 0.09287504494190216, 0.09289436668157577, 0.09289366006851196, 0.09288587510585784, 0.09290084213018418, 0.09290642946958542, 0.09291019588708878, 0.09290976107120513], 'val_acc': [0.72, 0.75, 0.74, 0.84, 0.95, 0.97, 0.97, 0.99, 0.99, 0.98, 1.0, 0.99, 0.99, 0.99, 1.0, 0.97, 0.99, 0.99, 1.0, 1.0, 0.99, 0.99, 1.0, 1.0, 0.99, 0.99, 1.0, 0.99, 0.99, 1.0, 0.99, 0.99, 1.0, 1.0, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'model_name': 'TinyTabularMLP-PE-Classifier', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.007007193583244844, 'batch_size': 64, 'epochs': 137, 'hidden_size': 168, 'dropout': 0.11419520809214809, 'weight_decay': 0.0009757351684204792, 'label_smoothing': 0.02134180085704536, 'grad_clip_norm': 0.6172652976249735, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 29739, 'model_storage_size_kb': 127.784765625, 'model_size_validation': 'PASS'}
2025-10-13 22:44:59,941 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final test metrics from training (avoids preprocessing mismatch)
2025-10-13 22:44:59,941 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 1.0, 'macro_f1': None}
2025-10-13 22:44:59,943 - INFO - evaluation.code_generation_pipeline_orchestrator - Model and test tensors saved to: trained_models/20251013_224455_TinyTabularMLP-PE-Classifier
2025-10-13 22:44:59,943 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä STEP 5: Performance Analysis
2025-10-13 22:44:59,943 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:59,943 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.7656
2025-10-13 22:44:59,943 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: 1.0000
2025-10-13 22:44:59,943 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-10-13 22:44:59,943 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: TinyTabularMLP-PE-Classifier
2025-10-13 22:44:59,943 - INFO - evaluation.code_generation_pipeline_orchestrator - Score: 1.0000
2025-10-13 22:44:59,943 - INFO - __main__ - AI-enhanced training completed!
2025-10-13 22:44:59,943 - INFO - __main__ - Final model achieved: {'acc': 1.0, 'macro_f1': None}
2025-10-13 22:44:59,943 - INFO - __main__ - Pipeline completed successfully in single attempt
2025-10-13 22:44:59,943 - INFO - __main__ - Pipeline completed: TinyTabularMLP-PE-Classifier, metrics: {'acc': 1.0, 'macro_f1': None}
2025-10-13 22:44:59,944 - INFO - __main__ - Pipeline summary saved to charts/pipeline_summary_20251013_224459.json
2025-10-13 22:44:59,944 - INFO - __main__ - Model saved: trained_models/best_model_TinyTabularMLP-PE-Classifier_20251013_224459.pth, performance: {'acc': 1.0, 'macro_f1': None}
2025-10-13 22:44:59,944 - INFO - __main__ - AI-enhanced processing completed successfully
