2025-09-17 14:04:02,977 - INFO - __main__ - Logging system initialized successfully
2025-09-17 14:04:02,979 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-17 14:04:02,979 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-17 14:04:02,979 - INFO - __main__ - Attempting to load: X.npy
2025-09-17 14:04:03,218 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-17 14:04:03,307 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-17 14:04:03,308 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-17 14:04:03,308 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-17 14:04:03,313 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-17 14:04:03,313 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-17 14:04:03,314 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-17 14:04:03,314 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-17 14:04:03,314 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-17 14:04:03,314 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-17 14:04:03,314 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-17 14:04:03,315 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-17 14:04:03,315 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-17 14:04:03,523 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-17 14:04:03,524 - INFO - class_balancing - Class imbalance analysis:
2025-09-17 14:04:03,524 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-17 14:04:03,524 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-17 14:04:03,525 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-17 14:04:03,525 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-17 14:04:03,525 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-17 14:04:03,525 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-17 14:04:03,526 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-17 14:04:03,526 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-17 14:04:04,225 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-17 14:04:04,235 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-17 14:04:04,235 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-17 14:04:04,235 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-17 14:04:04,235 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-17 14:04:04,236 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-17 14:04:04,236 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-17 14:07:32,298 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-17 14:07:32,395 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-17 14:07:32,396 - INFO - _models.ai_code_generator - Prompt length: 3277 characters
2025-09-17 14:07:32,396 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-17 14:07:32,396 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-17 14:07:32,396 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-17 14:09:48,358 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-17 14:09:48,412 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-17 14:09:48,413 - INFO - _models.ai_code_generator - AI generated training function: Lightweight-STCT-1D-CNN-Transformer
2025-09-17 14:09:48,413 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-09-17 14:09:48,413 - INFO - _models.ai_code_generator - Reasoning: Implements a lightweight STCT-style 1D CNN + Transformer hybrid tailored to 2-lead ECG segments of length 1000. The CNN stem (k=7/5/3, stride=2 each) reduces the sequence to ~125 tokens and projects to a compact embedding. Two shallow Transformer encoder layers (num_heads, embed_dim, hidden_size tunable) with sinusoidal positional encoding capture long-range dependencies efficiently. Global average pooling feeds a linear head for 5-class AAMI classification. Training uses a class-balanced focal loss to address typical inter-patient class imbalance in MIT-BIH and lightweight on-the-fly augmentations (noise, baseline wander, time warp). The architecture is designed to stay under ~256K parameters by default while keeping strong capacity; BO can tune heads, embed_dim, hidden_size, dropout, and optimizer hyperparameters.
2025-09-17 14:09:48,413 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.74)
2025-09-17 14:09:48,413 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: Lightweight-STCT-1D-CNN-Transformer
2025-09-17 14:09:48,413 - INFO - evaluation.code_generation_pipeline_orchestrator - Reasoning: Implements a lightweight STCT-style 1D CNN + Transformer hybrid tailored to 2-lead ECG segments of length 1000. The CNN stem (k=7/5/3, stride=2 each) reduces the sequence to ~125 tokens and projects to a compact embedding. Two shallow Transformer encoder layers (num_heads, embed_dim, hidden_size tunable) with sinusoidal positional encoding capture long-range dependencies efficiently. Global average pooling feeds a linear head for 5-class AAMI classification. Training uses a class-balanced focal loss to address typical inter-patient class imbalance in MIT-BIH and lightweight on-the-fly augmentations (noise, baseline wander, time warp). The architecture is designed to stay under ~256K parameters by default while keeping strong capacity; BO can tune heads, embed_dim, hidden_size, dropout, and optimizer hyperparameters.
2025-09-17 14:09:48,414 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: [None, None, None, None, None, None, None, None, None]
2025-09-17 14:09:48,414 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-09-17 14:09:48,414 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-17 14:09:48,415 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions\training_function_torch_tensor_Lightweight-STCT-1D-CNN-Transformer_1758136188.json
2025-09-17 14:09:48,415 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions\training_function_torch_tensor_Lightweight-STCT-1D-CNN-Transformer_1758136188.json
2025-09-17 14:09:48,418 - INFO - _models.training_function_executor - Training function validation passed
2025-09-17 14:09:48,418 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-17 14:09:48,418 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: Lightweight-STCT-1D-CNN-Transformer
2025-09-17 14:09:48,419 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 62352 samples (using full dataset)
2025-09-17 14:09:48,419 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: [None, None, None, None, None, None, None, None, None]
2025-09-17 14:09:48,420 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-17 14:09:48,420 - INFO - _models.training_function_executor - Using centralized data splits for BO objective
2025-09-17 14:09:49,329 - INFO - bo.run_bo - Using default search space
2025-09-17 14:09:49,330 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-17 14:09:49,333 - INFO - bo.run_bo - Using default search space
2025-09-17 14:09:49,333 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-17 14:09:49,335 - INFO - bo.run_bo - BO Trial 1: Initial random exploration
2025-09-17 14:09:49,335 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-17 14:09:49,335 - INFO - _models.training_function_executor - Using device: cuda
2025-09-17 14:09:49,337 - INFO - _models.training_function_executor - Executing training function: Lightweight-STCT-1D-CNN-Transformer
2025-09-17 14:09:49,337 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.01535224694197351, 'epochs': 17, 'batch_size': 128, 'hidden_size': 204, 'dropout': 0.41779511056254093}
2025-09-17 14:09:49,339 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'embed_dim': 96, 'hidden_size': 204, 'num_heads': 4, 'dropout': 0.41779511056254093, 'lr': 0.01535224694197351, 'batch_size': 128, 'epochs': 17, 'weight_decay': 0.0001, 'gamma': 2.0, 'aug_prob': 0.5, 'num_layers': 2}
2025-09-17 14:09:49,340 - ERROR - _models.training_function_executor - Training execution failed: Expected X shape [N, 2, 1000]
2025-09-17 14:09:49,340 - ERROR - _models.training_function_executor - Training code: import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader


def train_model(X_train, y_train, X_val, y_val, device, **hyperparams)...
