2025-09-17 19:36:16,800 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-17 19:36:17,149 - INFO - __main__ - Logging system initialized successfully
2025-09-17 19:36:17,149 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-17 19:36:17,150 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-17 19:36:17,151 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-17 19:36:17,151 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-17 19:36:17,152 - INFO - __main__ - Attempting to load: X.npy
2025-09-17 19:36:17,278 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-17 19:36:17,365 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-17 19:36:17,366 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-17 19:36:17,366 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-17 19:36:17,366 - INFO - __main__ - Flow: Code Generation â†’ BO â†’ Evaluation
2025-09-17 19:36:17,369 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-17 19:36:17,369 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-17 19:36:17,369 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-17 19:36:17,369 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-17 19:36:17,369 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation â†’ JSON Storage â†’ BO â†’ Training Execution â†’ Evaluation
2025-09-17 19:36:17,369 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-17 19:36:17,369 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-17 19:36:17,369 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-17 19:36:17,370 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-17 19:36:17,583 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-17 19:36:17,583 - INFO - class_balancing - Class imbalance analysis:
2025-09-17 19:36:17,583 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-17 19:36:17,583 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-17 19:36:17,583 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-17 19:36:17,583 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-17 19:36:17,583 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-17 19:36:17,583 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-17 19:36:17,583 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-17 19:36:17,583 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-17 19:36:18,268 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-17 19:36:18,276 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-17 19:36:18,276 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-17 19:36:18,276 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-17 19:36:18,276 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-17 19:36:18,276 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ¤– STEP 1: AI Training Code Generation
2025-09-17 19:36:18,276 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-17 19:38:36,869 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-17 19:38:36,989 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-17 19:38:36,989 - INFO - _models.ai_code_generator - Prompt length: 2990 characters
2025-09-17 19:38:36,989 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-17 19:38:36,989 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-17 19:38:36,989 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-17 19:40:54,300 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-17 19:40:54,352 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-17 19:40:54,352 - WARNING - _models.ai_code_generator - Initial JSON parse failed: Expecting ',' delimiter: line 27 column 18 (char 14966), attempting to fix common issues
2025-09-17 19:40:54,352 - WARNING - _models.ai_code_generator - Standard fixes failed, using GPT to debug JSON
2025-09-17 19:40:54,354 - INFO - _models.ai_code_generator - Saved malformed JSON to debug_malformed_json.txt for debugging
2025-09-17 19:40:54,354 - INFO - _models.ai_code_generator - Calling GPT to debug JSON formatting issues
2025-09-17 19:40:54,354 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-17 19:40:54,354 - INFO - _models.ai_code_generator - Prompt length: 17588 characters
2025-09-17 19:40:54,354 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-17 19:40:54,354 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-17 19:40:54,354 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-17 19:41:19,187 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-17 19:41:19,190 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-17 19:41:19,190 - INFO - _models.ai_code_generator - GPT suggested correction: {"high": 192}
2025-09-17 19:41:19,190 - WARNING - _models.ai_code_generator - Original JSON is malformed, attempting partial repair
2025-09-17 19:41:19,190 - INFO - _models.ai_code_generator - Successfully applied JSON corrections
2025-09-17 19:41:19,190 - INFO - _models.ai_code_generator - Successfully repaired JSON using GPT debugging
2025-09-17 19:41:19,190 - ERROR - _models.ai_code_generator - Failed to parse code recommendation: Missing required field: model_name
2025-09-17 19:41:19,190 - ERROR - _models.ai_code_generator - Original response: {
  "model_name": "CAT-Net-1D-ECG",
  "training_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n\n\ndef train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    \"\"\"\n    Train a lightweight CAT-Net (multi-scale 1D CNN + Transformer encoder) for 5-class classification.\n\n    Args:\n        X_train (Tensor): shape [N_train, 1000, 2] or [N_train, 2, 1000], dtype float32/float64\n        y_train (Tensor): shape [N_train], dtype long/int\n        X_val (Tensor): shape [N_val, 1000, 2] or [N_val, 2, 1000]\n        y_val (Tensor): shape [N_val]\n        device (str or torch.device): e.g., 'cuda' or 'cpu'\n        **hyperparams: see bo_config for tunable hyperparameters\n\n    Returns:\n        model (nn.Module): trained model (moved to CPU)\n        metrics (dict): training history and final metrics\n    \"\"\"\n    # ---------------------- Hyperparameters with defaults ----------------------\n    hp = {\n        'lr': 1e-3,\n        'batch_size': 64,\n        'epochs': 20,\n        'hidden_size': 96,\n        'num_heads': 4,\n        'num_transformer_layers': 2,\n        'ff_multiplier': 2,\n        'dropout': 0.1,\n        'stem_channels': 32,\n        'kernel_set': 'medium',  # 'small'|'medium'|'large'\n        'loss_type': 'cross_entropy',  # 'cross_entropy'|'focal'\n        'focal_gamma': 2.0,\n        'weight_decay': 1e-4,\n        'use_weighted_sampler': True,\n        'normalize': True,\n        'pos_enc_type': 'sinusoidal',  # 'sinusoidal'|'learned'\n        'pool_type': 'mean',  # 'mean'|'meanmax'\n        'max_grad_norm': 1.0,\n    }\n    hp.update(hyperparams or {})\n\n    # Map kernel_set to actual kernel sizes\n    kernel_map = {\n        'small': [3, 5, 7],\n        'medium': [5, 7, 11],\n        'large': [7, 11, 15],\n    }\n    kernel_sizes = kernel_map.get(hp['kernel_set'], [5, 7, 11])\n\n    # ---------------------- Utilities ----------------------\n    def ensure_LC_last(x):\n        # Ensure input shape [B, L, C]\n        if x.dim() != 3:\n            raise ValueError(f\"Expected 3D tensor, got shape {tuple(x.shape)}\")\n        if x.size(-1) == 2:\n            return x\n        elif x.size(1) == 2:\n            return x.permute(0, 2, 1).contiguous()\n        else:\n            raise ValueError(\"Input must have channel dimension of size 2 in either dim 1 or dim -1\")\n\n    def compute_class_weights(y, num_classes=5):\n        with torch.no_grad():\n            y = y.long()\n            counts = torch.bincount(y, minlength=num_classes).float()\n            counts = torch.clamp(counts, min=1.0)\n            total = counts.sum()\n            weights = total / (num_classes * counts)\n        return weights\n\n    def build_sampler_weights(y, class_weights):\n        # Per-sample weight = class_weight[label]\n        return class_weights[y.long()].float()\n\n    def accuracy(preds, targets):\n        return (preds == targets).float().mean().item()\n\n    def macro_f1(preds, targets, num_classes=5, eps=1e-9):\n        f1s = []\n        for c in range(num_classes):\n            tp = ((preds == c) & (targets == c)).sum().item()\n            fp = ((preds == c) & (targets != c)).sum().item()\n            fn = ((preds != c) & (targets == c)).sum().item()\n            denom = (2*tp + fp + fn)\n            f1 = (2*tp) / (denom + eps)\n            f1s.append(f1)\n        return float(sum(f1s) / len(f1s))\n\n    class FocalLoss(nn.Module):\n        def __init__(self, class_weights=None, gamma=2.0, reduction='mean'):\n            super().__init__()\n            self.gamma = gamma\n            self.reduction = reduction\n            if class_weights is not None:\n                # normalize alpha to have mean 1.0 across classes\n                alpha = class_weights / class_weights.mean()\n            else:\n                alpha = None\n            self.register_buffer('alpha', alpha if alpha is not None else None)\n        def forward(self, logits, targets):\n            ce = F.cross_entropy(logits, targets, weight=self.alpha, reduction='none')\n            pt = torch.exp(-ce)\n            loss = ((1 - pt) ** self.gamma) * ce\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            else:\n                return loss\n\n    # ---------------------- Model Definition ----------------------\n    class MultiScaleConvStem(nn.Module):\n        def __init__(self, in_ch=2, stem_ch=32, kernel_sizes=(5,7,11), dropout=0.1):\n            super().__init__()\n            self.branches = nn.ModuleList()\n            for k in kernel_sizes:\n                padding = k // 2\n                self.branches.append(\n                    nn.Sequential(\n                        nn.Conv1d(in_ch, stem_ch, kernel_size=k, padding=padding, bias=False),\n                        nn.BatchNorm1d(stem_ch),\n                        nn.GELU(),\n                    )\n                )\n            self.dropout = nn.Dropout(dropout)\n        def forward(self, x):  # x: [B, C, L]\n            feats = [b(x) for b in self.branches]\n            out = torch.cat(feats, dim=1)  # [B, stem_ch*len(k), L]\n            return self.dropout(out)\n\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model, max_len, kind='sinusoidal', dropout=0.0):\n            super().__init__()\n            self.kind = kind\n            self.dropout = nn.Dropout(dropout)\n            if kind == 'sinusoidal':\n                pe = torch.zeros(max_len, d_model)\n                position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n                div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n                pe[:, 0::2] = torch.sin(position * div_term)\n                pe[:, 1::2] = torch.cos(position * div_term[:pe[:,1::2].shape[1]]) if d_model % 2 == 0 else torch.cos(position * div_term)\n                pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n                self.register_buffer('pe', pe)\n                self.learned = None\n            else:\n                self.learned = nn.Parameter(torch.zeros(1, max_len, d_model))\n                nn.init.trunc_normal_(self.learned, std=0.02)\n                self.register_buffer('pe', None, persistent=False)\n        def forward(self, x):  # x: [B, L, D]\n            L = x.size(1)\n            if self.kind == 'sinusoidal':\n                x = x + self.pe[:, :L, :]\n            else:\n                x = x + self.learned[:, :L, :]\n            return self.dropout(x)\n\n    class CATNet(nn.Module):\n        def __init__(self, in_ch=2, num_classes=5, hidden_size=96, stem_channels=32, kernel_sizes=(5,7,11),\n                     num_heads=4, num_layers=2, ff_multiplier=2, dropout=0.1, pos_enc_type='sinusoidal',\n                     pool_type='mean', max_len=1200):\n            super().__init__()\n            self.pool_type = pool_type\n            self.stem = MultiScaleConvStem(in_ch=in_ch, stem_ch=stem_channels, kernel_sizes=kernel_sizes, dropout=dropout)\n            stem_out = stem_channels * len(kernel_sizes)\n            self.proj = nn.Sequential(\n                nn.Conv1d(stem_out, hidden_size, kernel_size=1, bias=False),\n                nn.BatchNorm1d(hidden_size),\n                nn.GELU(),\n            )\n            self.pos_enc = PositionalEncoding(hidden_size, max_len=max_len, kind=pos_enc_type, dropout=dropout)\n            # Ensure heads divides hidden_size; adjust if needed\n            heads = max(1, min(num_heads, hidden_size))\n            # Find greatest divisor <= heads\n            if hidden_size % heads != 0:\n                for h in range(heads, 0, -1):\n                    if hidden_size % h == 0:\n                        heads = h\n                        break\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=hidden_size,\n                nhead=heads,\n                dim_feedforward=max(32, ff_multiplier * hidden_size),\n                dropout=dropout,\n                activation='gelu',\n                batch_first=True,\n                norm_first=True,\n            )\n            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n            feat_dim = hidden_size if pool_type == 'mean' else hidden_size * 2\n            self.head = nn.Sequential(\n                nn.Dropout(dropout),\n                nn.Linear(feat_dim, num_classes)\n            )\n        def forward(self, x):  # x: [B, L, C]\n            x = x.transpose(1, 2)  # [B, C, L]\n            x = self.stem(x)\n            x = self.proj(x)       # [B, H, L]\n            x = x.transpose(1, 2)  # [B, L, H]\n            x = self.pos_enc(x)\n            x = self.encoder(x)    # [B, L, H]\n            if self.pool_type == 'mean':\n                feat = x.mean(dim=1)\n            else:  # 'meanmax'\n                feat = torch.cat([x.mean(dim=1), x.amax(dim=1)], dim=-1)\n            logits = self.head(feat)\n            return logits\n\n    # ---------------------- Data Preparation ----------------------\n    dev = torch.device(device)\n\n    X_train = ensure_LC_last(X_train)\n    X_val = ensure_LC_last(X_val)\n    X_train = X_train.float()\n    X_val = X_val.float()\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    # Normalization (per-channel across batch and time)\n    if hp['normalize']:\n        with torch.no_grad():\n            mean = X_train.mean(dim=(0, 1), keepdim=True)\n            std = X_train.std(dim=(0, 1), keepdim=True).clamp(min=1e-6)\n            X_train = (X_train - mean) / std\n            X_val = (X_val - mean) / std\n\n    # Class weights\n    class_weights = compute_class_weights(y_train, num_classes=5)\n\n    # Sampler\n    if hp['use_weighted_sampler']:\n        sample_weights = build_sampler_weights(y_train, class_weights).cpu()\n        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=hp['batch_size'], sampler=sampler, drop_last=False)\n    else:\n        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=hp['batch_size'], shuffle=True, drop_last=False)\n\n    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=hp['batch_size'], shuffle=False, drop_last=False)\n\n    # ---------------------- Model, Optimizer, Loss ----------------------\n    seq_len = X_train.size(1)\n    model = CATNet(\n        in_ch=2,\n        num_classes=5,\n        hidden_size=int(hp['hidden_size']),\n        stem_channels=int(hp['stem_channels']),\n        kernel_sizes=tuple(kernel_sizes),\n        num_heads=int(hp['num_heads']),\n        num_layers=int(hp['num_transformer_layers']),\n        ff_multiplier=int(hp['ff_multiplier']),\n        dropout=float(hp['dropout']),\n        pos_enc_type=hp['pos_enc_type'],\n        pool_type=hp['pool_type'],\n        max_len=int(max(seq_len, 1024)),\n    ).to(dev)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=float(hp['lr']), weight_decay=float(hp['weight_decay']))\n\n    if hp['loss_type'] == 'focal':\n        criterion = FocalLoss(class_weights=class_weights.to(dev), gamma=float(hp['focal_gamma']))\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights.to(dev))\n\n    # ---------------------- Training Loop ----------------------\n    train_losses, val_losses = [], []\n    train_accs, val_accs, val_f1s = [], [], []\n    best_val_acc = -1.0\n    best_epoch = -1\n\n    for epoch in range(int(hp['epochs'])):\n        model.train()\n        total_loss = 0.0\n        total_correct = 0\n        total_samples = 0\n        for xb, yb in train_loader:\n            xb = xb.to(dev, non_blocking=True)\n            yb = yb.to(dev, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if hp['max_grad_norm'] and hp['max_grad_norm'] > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(hp['max_grad_norm']))\n            optimizer.step()\n            total_loss += loss.item() * xb.size(0)\n            preds = logits.argmax(dim=1)\n            total_correct += (preds == yb).sum().item()\n            total_samples += xb.size(0)\n        epoch_train_loss = total_loss / max(total_samples, 1)\n        epoch_train_acc = total_correct / max(total_samples, 1)\n\n        model.eval()\n        v_total_loss = 0.0\n        v_total_correct = 0\n        v_total_samples = 0\n        all_preds = []\n        all_targets = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(dev, non_blocking=True)\n                yb = yb.to(dev, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                v_total_loss += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                v_total_correct += (preds == yb).sum().item()\n                v_total_samples += xb.size(0)\n                all_preds.append(preds.cpu())\n                all_targets.append(yb.cpu())\n        all_preds = torch.cat(all_preds) if all_preds else torch.empty(0, dtype=torch.long)\n        all_targets = torch.cat(all_targets) if all_targets else torch.empty(0, dtype=torch.long)\n        epoch_val_loss = v_total_loss / max(v_total_samples, 1)\n        epoch_val_acc = v_total_correct / max(v_total_samples, 1)\n        epoch_val_f1 = macro_f1(all_preds, all_targets, num_classes=5)\n\n        train_losses.append(epoch_train_loss)\n        val_losses.append(epoch_val_loss)\n        train_accs.append(epoch_train_acc)\n        val_accs.append(epoch_val_acc)\n        val_f1s.append(epoch_val_f1)\n\n        if epoch_val_acc > best_val_acc:\n            best_val_acc = epoch_val_acc\n            best_epoch = epoch\n\n    model_cpu = model.to('cpu')\n\n    metrics = {\n        'train_loss': train_losses,\n        'val_loss': val_losses,\n        'train_acc': train_accs,\n        'val_acc': val_accs,\n        'val_macro_f1': val_f1s,\n        'best_val_acc': best_val_acc,\n        'best_epoch': best_epoch,\n        'class_weights': class_weights.cpu().tolist(),\n    }\n\n    return model_cpu, metrics\n",
  "bo_config": {
    "lr": {
      "default": 0.001,
      "type": "Real",
      "low": 1e-5,
      "high": 1e-1,
      "prior": "log-uniform"
    },
    "batch_size": {
      "default": 64,
      "type": "Categorical",
      "categories": [32, 64, 128, 256]
    },
    "epochs": {
      "default": 20,
      "type": "Integer",
      "low": 10,
      "high": 50
    },
    "hidden_size": {
      "default": 96,
      "type": "Integer",
      "low": 64,
      "high": 192"
    },
    "num_heads": {
      "default": 4,
      "type": "Integer",
      "low": 1,
      "high": 4
    },
    "num_transformer_layers": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 2
    },
    "ff_multiplier": {
      "default": 2,
      "type": "Integer",
      "low": 1,
      "high": 3
    },
    "dropout": {
      "default": 0.1,
      "type": "Real",
      "low": 0.0,
      "high": 0.5
    },
    "stem_channels": {
      "default": 32,
      "type": "Integer",
      "low": 16,
      "high": 48
    },
    "kernel_set": {
      "default": "medium",
      "type": "Categorical",
      "categories": ["small", "medium", "large"]
    },
    "loss_type": {
      "default": "cross_entropy",
      "type": "Categorical",
      "categories": ["cross_entropy", "focal"]
    },
    "focal_gamma": {
      "default": 2.0,
      "type": "Real",
      "low": 0.5,
      "high": 5.0
    },
    "weight_decay": {
      "default": 0.0001,
      "type": "Real",
      "low": 1e-6,
      "high": 1e-2,
      "prior": "log-uniform"
    },
    "use_weighted_sampler": {
      "default": true,
      "type": "Categorical",
      "categories": [true, false]
    },
    "normalize": {
      "default": true,
      "type": "Categorical",
      "categories": [true, false]
    },
    "pos_enc_type": {
      "default": "sinusoidal",
      "type": "Categorical",
      "categories": ["sinusoidal", "learned"]
    },
    "pool_type": {
      "default": "mean",
      "type": "Categorical",
      "categories": ["mean", "meanmax"]
    },
    "max_grad_norm": {
      "default": 1.0,
      "type": "Real",
      "low": 0.0,
      "high": 5.0
    }
  },
  "reasoning": "Implements a compact CAT-Net aligned with literature for 5-class MIT-BIH: a 1D multi-scale convolutional stem over the two ECG leads, followed by positional encoding and 1â€“2 lightweight Transformer encoder layers (hidden_sizeâ‰ˆ96 by default, 2â€“4 heads), then global pooling and a linear 5-way classifier. The training loop supports class imbalance via weighted cross-entropy or focal loss and optional WeightedRandomSampler, matching reported best practices (inter-patient robustness and class balancing). Hyperparameters are exposed for Bayesian Optimization while keeping the model under a tight parameter budget.",
  "confidence": 0.86
}
