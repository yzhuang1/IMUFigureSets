2025-09-20 18:48:16,104 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 18:48:16,464 - INFO - __main__ - Logging system initialized successfully
2025-09-20 18:48:16,465 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-20 18:48:16,465 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-20 18:48:16,466 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-20 18:48:16,466 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-20 18:48:16,466 - INFO - __main__ - Attempting to load: X.npy
2025-09-20 18:48:16,592 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-20 18:48:16,681 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-20 18:48:16,682 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-20 18:48:16,682 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-20 18:48:16,682 - INFO - __main__ - Flow: Code Generation â†’ BO â†’ Evaluation
2025-09-20 18:48:16,685 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 18:48:16,685 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-20 18:48:16,685 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-20 18:48:16,685 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-20 18:48:16,685 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation â†’ JSON Storage â†’ BO â†’ Training Execution â†’ Evaluation
2025-09-20 18:48:16,685 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-20 18:48:16,685 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-20 18:48:16,685 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-20 18:48:16,685 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-20 18:48:16,897 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-20 18:48:16,897 - INFO - class_balancing - Class imbalance analysis:
2025-09-20 18:48:16,897 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-20 18:48:16,897 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-20 18:48:16,897 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-20 18:48:16,897 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-20 18:48:16,897 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-20 18:48:16,897 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-20 18:48:16,897 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-20 18:48:16,897 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-20 18:48:17,584 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-20 18:48:17,592 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-20 18:48:17,592 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-20 18:48:17,592 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-20 18:48:17,592 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-20 18:48:17,592 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ¤– STEP 1: AI Training Code Generation
2025-09-20 18:48:17,592 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-20 18:48:17,592 - INFO - _models.ai_code_generator - Prompt length: 2552 characters
2025-09-20 18:48:17,592 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-20 18:48:17,592 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-20 18:48:17,592 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-20 18:49:25,250 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-20 18:49:25,328 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-20 18:49:25,328 - INFO - _models.ai_code_generator - AI generated training function: CompactMLP_Quantized_5Class
2025-09-20 18:49:25,328 - INFO - _models.ai_code_generator - Confidence: 0.90
2025-09-20 18:49:25,328 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: CompactMLP_Quantized_5Class
2025-09-20 18:49:25,328 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'hidden_size', 'hidden_size2', 'dropout', 'weight_decay', 'scheduler_gamma', 'clip_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-20 18:49:25,328 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.90
2025-09-20 18:49:25,330 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ’¾ STEP 2: Save Training Function to JSON
2025-09-20 18:49:25,334 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions\training_function_torch_tensor_CompactMLP_Quantized_5Class_1758412165.json
2025-09-20 18:49:25,334 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions\training_function_torch_tensor_CompactMLP_Quantized_5Class_1758412165.json
2025-09-20 18:49:25,334 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ” STEP 3: Bayesian Optimization
2025-09-20 18:49:25,334 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: CompactMLP_Quantized_5Class
2025-09-20 18:49:25,334 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“¦ Installing dependencies for GPT-generated training code...
2025-09-20 18:49:25,335 - INFO - package_installer - ðŸ” Analyzing GPT-generated code for package dependencies...
2025-09-20 18:49:25,340 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-20 18:49:25,340 - INFO - package_installer - Available packages: {'torch'}
2025-09-20 18:49:25,340 - INFO - package_installer - Missing packages: set()
2025-09-20 18:49:25,340 - INFO - package_installer - âœ… All required packages are already available
2025-09-20 18:49:25,340 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-09-20 18:49:25,420 - INFO - data_splitting - Created BO subset: 5000 samples
2025-09-20 18:49:25,420 - INFO - data_splitting - BO subset class distribution: [3600  766   96  100  438]
2025-09-20 18:49:25,420 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 5000 samples (using bo_sample_num=5000)
2025-09-20 18:49:25,420 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'hidden_size', 'hidden_size2', 'dropout', 'weight_decay', 'scheduler_gamma', 'clip_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-20 18:49:25,420 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 18:49:25,500 - INFO - data_splitting - Created BO subset: 5000 samples
2025-09-20 18:49:25,501 - INFO - data_splitting - BO subset class distribution: [3600  766   96  100  438]
2025-09-20 18:49:25,501 - INFO - _models.training_function_executor - Using BO subset for optimization: 5000 samples (bo_sample_num=5000)
2025-09-20 18:49:25,511 - INFO - _models.training_function_executor - BO splits - Train: 4000, Val: 1000
2025-09-20 18:49:25,789 - INFO - bo.run_bo - Converted GPT search space: 12 parameters
2025-09-20 18:49:25,789 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-20 18:49:25,789 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-20 18:49:25,790 - INFO - bo.run_bo - ðŸ”BO Trial 1: Initial random exploration
2025-09-20 18:49:25,790 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 18:49:25,790 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 18:49:25,790 - INFO - _models.training_function_executor - Executing training function: CompactMLP_Quantized_5Class
2025-09-20 18:49:25,790 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': 76, 'hidden_size': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:49:25,792 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': 76, 'hidden_size': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:49:47,409 - INFO - _models.training_function_executor - Model parameter count: 0
2025-09-20 18:49:47,409 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0630042932033539, 0.8100161437988281, 0.7756705734729766, 0.7362625324726104, 0.7110688819885254, 0.6921723937988281, 0.680000744342804, 0.6434110636711121, 0.6333451461791992, 0.6303787918090821, 0.6085199227333069, 0.5920601246356965, 0.5841034934520721, 0.5513688631057739, 0.5372376729249955, 0.5334494158029556, 0.5191134077310562, 0.5155237120389938, 0.5056425845623016, 0.48888365721702576, 0.48363538408279416, 0.47788875579833984, 0.472236906170845, 0.4730058830976486, 0.4627269349694252, 0.4616931103467941, 0.457583115696907, 0.4609626567363739, 0.45158645474910736, 0.44982519698143003, 0.4573373712301254, 0.4695888296365738, 0.44755121672153475, 0.4415955964922905, 0.4418516935110092, 0.44621297311782837, 0.4452100110054016, 0.4476619679927826, 0.4454323081970215, 0.44616697549819945, 0.4588426603674889, 0.4420214453935623, 0.4470096360445023, 0.4432302368283272, 0.4436245604157448, 0.4432666189074516, 0.4329356977343559, 0.4397826626896858, 0.43649786442518235, 0.44050996100902556, 0.45430701649189, 0.4429174849390984, 0.4346522458791733, 0.44218304252624513, 0.4436681746840477, 0.4450424758791924, 0.4433378826379776, 0.43778401231765746, 0.43676088869571683, 0.4405360787510872, 0.43801455014944074, 0.44671871811151503, 0.4476237450838089, 0.4425518023371696, 0.4474830737113953, 0.44158177423477174, 0.4369043107628822, 0.4457743293046951, 0.45012946259975434, 0.43965288203954694, 0.4393695030212402, 0.4406240283846855, 0.43837752187252044, 0.4572830617427826, 0.4440938274860382, 0.4401582365036011], 'val_losses': [0.7949762415885925, 0.7935121078491211, 0.7521205396652222, 0.7307493271827697, 0.6855473513603211, 0.7230033850669861, 0.7237953100204467, 0.7123883333206177, 0.7143658981323242, 0.7103975009918213, 0.6954145021438599, 0.6962215497493743, 0.6899778430461884, 0.6921648948192597, 0.7109972717761993, 0.7260900347232818, 0.797902006149292, 0.784148018836975, 0.7547572810649872, 0.7536406474113464, 0.7671373937129974, 0.8041272118091584, 0.789661913394928, 0.8102341456413269, 0.8092585651874542, 0.8247195079326629, 0.8175701932907105, 0.8250098347663879, 0.8375695557594299, 0.836868028640747, 0.8431979277133942, 0.8399266259670257, 0.8421065142154693, 0.8439447128772736, 0.8421367757320404, 0.8481429760456085, 0.8486108300685883, 0.8462453091144562, 0.8452460095882416, 0.8453578975200653, 0.8454600410461426, 0.8458220958709717, 0.8464805676937103, 0.8475726025104523, 0.8486340198516845, 0.8498983793258666, 0.8502898845672607, 0.8506955933570862, 0.8514251234531403, 0.8519379093647003, 0.8522439312934875, 0.8525148267745972, 0.8528636200428009, 0.8531342566013336, 0.8533319573402405, 0.8535279247760773, 0.85380033659935, 0.8539505612850189, 0.8542063519954681, 0.8543443508148193, 0.8544340949058533, 0.8545635092258453, 0.8545740950107574, 0.8546335940361023, 0.8546694529056549, 0.8547481951713563, 0.8547488923072815, 0.8547181942462921, 0.8547857947349549, 0.8547843420505523, 0.8548251733779907, 0.8548324539661407, 0.854831660747528, 0.8548189797401429, 0.8548291847705841, 0.8548301827907562], 'val_acc': [0.755, 0.747, 0.775, 0.77, 0.781, 0.79, 0.781, 0.783, 0.786, 0.783, 0.791, 0.79, 0.795, 0.799, 0.8, 0.809, 0.804, 0.802, 0.809, 0.809, 0.808, 0.804, 0.808, 0.805, 0.807, 0.802, 0.805, 0.803, 0.803, 0.804, 0.804, 0.804, 0.801, 0.801, 0.801, 0.8, 0.802, 0.803, 0.802, 0.802, 0.802, 0.802, 0.803, 0.802, 0.802, 0.802, 0.802, 0.802, 0.801, 0.801, 0.801, 0.802, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801, 0.801], 'num_parameters': 0, 'final_val_acc': 0.801, 'best_val_acc': 0.809, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'model_name': 'CompactMLP_Quantized_5Class', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': 76, 'hidden_size': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 0, 'model_size_validation': 'PASS'}
2025-09-20 18:49:47,409 - INFO - _models.training_function_executor - BO Objective: base=0.8010, size_penalty=0.0000, final=0.8010
2025-09-20 18:49:47,409 - INFO - _models.training_function_executor - Model size: 0 parameters (PASS 256K limit)
2025-09-20 18:49:47,409 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 21.619s
2025-09-20 18:49:47,409 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8010
2025-09-20 18:49:47,409 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 18:49:47,409 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': np.int64(76), 'hidden_size': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, value=0.8010
2025-09-20 18:49:47,409 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': np.int64(76), 'hidden_size': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True} -> 0.8010
2025-09-20 18:49:47,410 - INFO - bo.run_bo - ðŸ”BO Trial 2: Initial random exploration
2025-09-20 18:49:47,410 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 18:49:47,410 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 18:49:47,411 - INFO - _models.training_function_executor - Executing training function: CompactMLP_Quantized_5Class
2025-09-20 18:49:47,411 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.07579479953348009, 'batch_size': 128, 'epochs': 42, 'hidden_size': 33, 'hidden_size2': 79, 'dropout': 0.49610577964560887, 'weight_decay': 0.0002950706670790534, 'scheduler_gamma': 0.9411363209127539, 'clip_norm': 0.035331526098587036, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:49:47,412 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.07579479953348009, 'batch_size': 128, 'epochs': 42, 'hidden_size': 33, 'hidden_size2': 79, 'dropout': 0.49610577964560887, 'weight_decay': 0.0002950706670790534, 'scheduler_gamma': 0.9411363209127539, 'clip_norm': 0.035331526098587036, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:49:52,972 - INFO - _models.training_function_executor - Model parameter count: 0
2025-09-20 18:49:52,972 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [15.14135352420807, 1.0680729560852051, 1.1615038285255432, 1.3252028999328613, 1.1179652047157287, 1.2807828269004822, 0.9007276496887207, 1.1152908883094788, 0.9507138476371765, 0.9173267698287964, 1.2357146863937378, 0.9126807918548584, 1.4994178800582885, 1.0237131385803222, 0.9508377795219422, 1.0282983055114747, 0.8984218053817749, 0.8919232444763183, 0.9576836671829224, 0.8927789707183837, 0.8924109582901001, 0.8921218709945679, 1.0217261781692506, 0.9430907163619995, 0.8930712699890136, 1.034080096244812, 1.3283589639663695, 0.9110844860076904, 0.9329717869758606, 0.9400806503295899, 0.892095639705658, 0.8918493671417236, 0.8999698991775513, 0.8914499640464782, 0.8951872272491455, 0.9023258242607116, 0.892842755317688, 0.8916953206062317, 0.894240583896637, 0.8929885053634643, 0.8914334077835083, 0.8915565757751465], 'val_losses': [0.9524502563476562, 0.9006249423027038, 0.9003120679855346, 0.8933243551254273, 0.8919094552993775, 0.8932364163398743, 0.8918976407051087, 0.8948544917106629, 0.8919018287658691, 0.8916398320198059, 0.893343894481659, 0.8916542668342591, 0.8917057719230652, 0.8913154497146606, 0.8935843997001648, 0.891465862751007, 0.8914509882926941, 0.8916395378112792, 0.8915807266235352, 0.8920236668586731, 0.891362564086914, 0.8912619886398315, 0.8912724800109864, 0.8912203016281128, 0.8913243522644043, 0.891355393409729, 0.8914696431159973, 0.891191969871521, 0.891327576637268, 0.8912097444534302, 0.8913031306266784, 0.8912224502563476, 0.8912538595199585, 0.8911915168762207, 0.8911780157089233, 0.8911834406852722, 0.8912108683586121, 0.8912354249954224, 0.8912609219551086, 0.8912179899215699, 0.8911792287826538, 0.891175769329071], 'val_acc': [0.719, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72], 'num_parameters': 0, 'final_val_acc': 0.72, 'best_val_acc': 0.72, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'model_name': 'CompactMLP_Quantized_5Class', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.07579479953348009, 'batch_size': 128, 'epochs': 42, 'hidden_size': 33, 'hidden_size2': 79, 'dropout': 0.49610577964560887, 'weight_decay': 0.0002950706670790534, 'scheduler_gamma': 0.9411363209127539, 'clip_norm': 0.035331526098587036, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 0, 'model_size_validation': 'PASS'}
2025-09-20 18:49:52,972 - INFO - _models.training_function_executor - BO Objective: base=0.7200, size_penalty=0.0000, final=0.7200
2025-09-20 18:49:52,972 - INFO - _models.training_function_executor - Model size: 0 parameters (PASS 256K limit)
2025-09-20 18:49:52,972 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.562s
2025-09-20 18:49:52,972 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7200
2025-09-20 18:49:52,972 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 18:49:52,972 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 0.07579479953348009, 'batch_size': 128, 'epochs': np.int64(42), 'hidden_size': np.int64(33), 'hidden_size2': np.int64(79), 'dropout': 0.49610577964560887, 'weight_decay': 0.0002950706670790534, 'scheduler_gamma': 0.9411363209127539, 'clip_norm': 0.035331526098587036, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, value=0.7200
2025-09-20 18:49:52,972 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 0.07579479953348009, 'batch_size': 128, 'epochs': np.int64(42), 'hidden_size': np.int64(33), 'hidden_size2': np.int64(79), 'dropout': 0.49610577964560887, 'weight_decay': 0.0002950706670790534, 'scheduler_gamma': 0.9411363209127539, 'clip_norm': 0.035331526098587036, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True} -> 0.7200
2025-09-20 18:49:52,973 - INFO - bo.run_bo - ðŸ”BO Trial 3: Initial random exploration
2025-09-20 18:49:52,973 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 18:49:52,973 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 18:49:52,973 - INFO - _models.training_function_executor - Executing training function: CompactMLP_Quantized_5Class
2025-09-20 18:49:52,973 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.536960311060885e-05, 'batch_size': 128, 'epochs': 19, 'hidden_size': 93, 'hidden_size2': 77, 'dropout': 0.3925879806965069, 'weight_decay': 6.290644294586153e-06, 'scheduler_gamma': 0.9266209313236281, 'clip_norm': 2.962072844310213, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:49:52,975 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.536960311060885e-05, 'batch_size': 128, 'epochs': 19, 'hidden_size': 93, 'hidden_size2': 77, 'dropout': 0.3925879806965069, 'weight_decay': 6.290644294586153e-06, 'scheduler_gamma': 0.9266209313236281, 'clip_norm': 2.962072844310213, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:49:55,533 - INFO - _models.training_function_executor - Model parameter count: 0
2025-09-20 18:49:55,533 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.4547115964889525, 1.3541534013748169, 1.2839962854385376, 1.225340413093567, 1.1841212196350097, 1.1545797538757325, 1.1330814094543458, 1.1191108827590943, 1.1031933193206787, 1.1038021125793458, 1.0852611513137818, 1.0706837301254273, 1.0716910877227783, 1.0674990148544312, 1.0671032609939575, 1.0517923669815064, 1.0669392976760865, 1.0561568269729613, 1.0544857964515686], 'val_losses': [1.3745000371932983, 1.2775559873580933, 1.2065682306289673, 1.1535769939422607, 1.114207929611206, 1.0856833591461181, 1.064025899887085, 1.0473225002288817, 1.0351784610748291, 1.0254612922668458, 1.01769162940979, 1.0112303495407104, 1.0055429430007934, 1.0008295879364013, 0.9970699901580811, 0.9934227085113525, 0.990521354675293, 0.9878737287521362, 0.9857178974151611], 'val_acc': [0.688, 0.709, 0.716, 0.718, 0.719, 0.719, 0.719, 0.719, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72], 'num_parameters': 0, 'final_val_acc': 0.72, 'best_val_acc': 0.72, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'model_name': 'CompactMLP_Quantized_5Class', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.536960311060885e-05, 'batch_size': 128, 'epochs': 19, 'hidden_size': 93, 'hidden_size2': 77, 'dropout': 0.3925879806965069, 'weight_decay': 6.290644294586153e-06, 'scheduler_gamma': 0.9266209313236281, 'clip_norm': 2.962072844310213, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 0, 'model_size_validation': 'PASS'}
2025-09-20 18:49:55,533 - INFO - _models.training_function_executor - BO Objective: base=0.7200, size_penalty=0.0000, final=0.7200
2025-09-20 18:49:55,533 - INFO - _models.training_function_executor - Model size: 0 parameters (PASS 256K limit)
2025-09-20 18:49:55,533 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 2.560s
2025-09-20 18:49:55,734 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7200
2025-09-20 18:49:55,734 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.201s
2025-09-20 18:49:55,734 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 1.536960311060885e-05, 'batch_size': 128, 'epochs': np.int64(19), 'hidden_size': np.int64(93), 'hidden_size2': np.int64(77), 'dropout': 0.3925879806965069, 'weight_decay': 6.290644294586153e-06, 'scheduler_gamma': 0.9266209313236281, 'clip_norm': 2.962072844310213, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, value=0.7200
2025-09-20 18:49:55,734 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 1.536960311060885e-05, 'batch_size': 128, 'epochs': np.int64(19), 'hidden_size': np.int64(93), 'hidden_size2': np.int64(77), 'dropout': 0.3925879806965069, 'weight_decay': 6.290644294586153e-06, 'scheduler_gamma': 0.9266209313236281, 'clip_norm': 2.962072844310213, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True} -> 0.7200
2025-09-20 18:49:55,734 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.8010
2025-09-20 18:49:55,734 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': np.int64(76), 'hidden_size': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:49:55,737 - INFO - visualization - Generating BO visualization charts with 3 trials...
2025-09-20 18:49:57,600 - INFO - visualization - BO summary saved to: charts\BO_CompactMLP_Quantized_5Class_20250920_184955\bo_summary.txt
2025-09-20 18:49:57,601 - INFO - visualization - BO charts saved to: charts\BO_CompactMLP_Quantized_5Class_20250920_184955
2025-09-20 18:49:57,601 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“Š BO charts saved to: charts\BO_CompactMLP_Quantized_5Class_20250920_184955
2025-09-20 18:49:57,613 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸš€ STEP 4: Final Training Execution
2025-09-20 18:49:57,613 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-20 18:49:57,799 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 18:49:57,810 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 18:49:57,823 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 18:49:57,832 - INFO - _models.training_function_executor - Loaded training function: CompactMLP_Quantized_5Class
2025-09-20 18:49:57,833 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-20 18:49:57,833 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': np.int64(76), 'hidden_size': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:49:57,833 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 18:49:57,981 - INFO - _models.training_function_executor - Executing training function: CompactMLP_Quantized_5Class
2025-09-20 18:49:57,981 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': np.int64(76), 'hidden_size': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:49:57,982 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': 76, 'hidden_size': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:53:22,959 - INFO - _models.training_function_executor - Model parameter count: 0
2025-09-20 18:53:22,959 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9122397163770825, 0.8917875939497684, 0.8919512969833808, 0.8917347645415434, 0.8915076564815203, 0.891716851983533, 0.8914773615484919, 0.8916564095517397, 0.8915213986502328, 0.89140539429335, 0.8914597214893427, 0.8914251974705418, 0.8914567458935518, 0.8913524810466751, 0.8913128799023203, 0.8912989491687934, 0.8913076845234456, 0.8912953720058359, 0.8912582659396345, 0.8912784353723121, 0.8912573225658802, 0.891241534122392, 0.8912534100730226, 0.8912387896798186, 0.8912353964213285, 0.891222095336547, 0.8912267950217821, 0.8912199179745715, 0.8912075148072354, 0.8912156608311195, 0.891212819692318, 0.8912085389364406, 0.8912070856729124, 0.8912033387299242, 0.8912021051479896, 0.8912001283721342, 0.8912000061515435, 0.8911991283094396, 0.8911972115445156, 0.8911965963069974, 0.891196470262532, 0.8911957636822102, 0.891195613977521, 0.891195151766683, 0.8911946176192047, 0.8911944650705088, 0.8911940755849852, 0.8911940263526001, 0.8911936557952557, 0.8911934463903248, 0.8911934275816418, 0.8911931974799899, 0.8912669070835771, 0.8911929704613368, 0.8911929282553214, 0.8911928350483788, 0.8911927789091203, 0.8911926955008564, 0.8911926799902652, 0.8911926985599561, 0.8911926035605724, 0.8911925685960193, 0.8911925692890965, 0.8911925260315156, 0.8911925147032872, 0.8911925115724899, 0.8911924888204362, 0.8911924607627566, 0.8911924495779234, 0.8911924881751574, 0.8911924575363624, 0.8911924490043422, 0.8911924278296369, 0.8911924234321812, 0.8911924138724947, 0.891192465901088], 'val_losses': [0.8911767383848628, 0.8914924549700395, 0.8912673911649741, 0.8915097150461365, 0.8920717699871615, 0.8918211868151737, 0.8916893531101147, 0.8913206479001454, 0.8912673341650827, 0.8913904945848221, 0.8911288964750772, 0.8910910161401581, 0.8910990185521105, 0.891078384448068, 0.8911054518928482, 0.8911516943197278, 0.8911149537347057, 0.8910732570327307, 0.8910842621459074, 0.8910634343820308, 0.89106297701182, 0.8910608343063883, 0.8910792532707487, 0.8910626668161532, 0.891059330330113, 0.8910623773420169, 0.8910586369459407, 0.8910581414812109, 0.8910604015080833, 0.8910586432069078, 0.8910586365038495, 0.8910579079077121, 0.8910576477071811, 0.8910576932127017, 0.8910577293865138, 0.8910577109740135, 0.8910578057607524, 0.8910576875073357, 0.8910577909805689, 0.8910577020724479, 0.891057632885178, 0.8910576452697053, 0.8910575736688566, 0.8910577327380429, 0.8910575903787085, 0.8910577143793105, 0.8910576554198799, 0.8910578137960584, 0.8910576608862776, 0.8910577128260172, 0.8910576645126201, 0.8910576336797473, 0.8910575777910582, 0.8910576519787378, 0.8910577761346692, 0.8910575294059707, 0.8910576619855314, 0.891057724033626, 0.8910577765648119, 0.8910576436148505, 0.8910578271902263, 0.89105764651234, 0.8910577018394539, 0.8910575947578008, 0.8910576215341883, 0.891057602978307, 0.8910576665498241, 0.8910575900501272, 0.8910577505053291, 0.8910575926011127, 0.891057560340405, 0.8910576833253921, 0.8910576664422883, 0.8910576153867312, 0.8910577888358293, 0.8910576422467575], 'val_acc': [0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923, 0.720056129096923], 'num_parameters': 0, 'final_val_acc': 0.720056129096923, 'best_val_acc': 0.720056129096923, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'model_name': 'CompactMLP_Quantized_5Class', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.01535224694197351, 'batch_size': 32, 'epochs': 76, 'hidden_size': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 4.20705395028794e-06, 'scheduler_gamma': 0.8586544582130617, 'clip_norm': 4.330880728874677, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 0, 'model_size_validation': 'PASS'}
2025-09-20 18:53:22,960 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final validation metrics from training (avoids preprocessing mismatch)
2025-09-20 18:53:22,960 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 0.720056129096923, 'macro_f1': None}
2025-09-20 18:53:22,990 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“Š STEP 5: Performance Analysis
2025-09-20 18:53:22,990 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: CompactMLP_Quantized_5Class
2025-09-20 18:53:22,990 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.8010
2025-09-20 18:53:22,990 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: 0.7201
2025-09-20 18:53:22,990 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-09-20 18:53:22,990 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: CompactMLP_Quantized_5Class
2025-09-20 18:53:22,990 - INFO - evaluation.code_generation_pipeline_orchestrator - Score: 0.7201
2025-09-20 18:53:22,990 - INFO - __main__ - AI-enhanced training completed!
2025-09-20 18:53:22,990 - INFO - __main__ - Final model achieved: {'acc': 0.720056129096923, 'macro_f1': None}
2025-09-20 18:53:22,990 - INFO - __main__ - Pipeline completed successfully in single attempt
2025-09-20 18:53:22,991 - INFO - __main__ - Pipeline completed: CompactMLP_Quantized_5Class, metrics: {'acc': 0.720056129096923, 'macro_f1': None}
2025-09-20 18:53:22,992 - INFO - __main__ - Pipeline summary saved to charts/pipeline_summary_20250920_185322.json
2025-09-20 18:53:22,998 - INFO - __main__ - Model saved: trained_models/best_model_CompactMLP_Quantized_5Class_20250920_185322.pth, performance: {'acc': 0.720056129096923, 'macro_f1': None}
2025-09-20 18:53:22,999 - INFO - __main__ - AI-enhanced processing completed successfully
