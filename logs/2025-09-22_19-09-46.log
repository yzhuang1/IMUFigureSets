2025-09-22 19:09:46,866 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 19:09:46,980 - INFO - __main__ - Logging system initialized successfully
2025-09-22 19:09:46,980 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-22 19:09:46,980 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-22 19:09:46,981 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-22 19:09:46,981 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-22 19:09:46,981 - INFO - __main__ - Attempting to load: X.npy
2025-09-22 19:09:47,022 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-22 19:09:47,062 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-22 19:09:47,062 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-22 19:09:47,062 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-22 19:09:47,062 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-22 19:09:47,064 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-22 19:09:47,064 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-22 19:09:47,064 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-22 19:09:47,064 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-22 19:09:47,064 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-22 19:09:47,064 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-22 19:09:47,064 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-22 19:09:47,064 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-22 19:09:47,064 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-22 19:09:47,166 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-22 19:09:47,166 - INFO - class_balancing - Class imbalance analysis:
2025-09-22 19:09:47,166 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-22 19:09:47,166 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-22 19:09:47,166 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-22 19:09:47,166 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-22 19:09:47,166 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-22 19:09:47,166 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-22 19:09:47,166 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-22 19:09:47,166 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-22 19:09:47,338 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-22 19:09:47,339 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-22 19:09:47,339 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-22 19:09:47,339 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-22 19:09:47,339 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-22 19:09:47,339 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-22 19:09:47,339 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-22 19:13:05,350 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 19:13:05,374 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 19:13:05,374 - INFO - _models.ai_code_generator - Prompt length: 3633 characters
2025-09-22 19:13:05,374 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 19:13:05,374 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 19:13:05,374 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 19:15:49,063 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 19:15:49,079 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 19:15:49,079 - INFO - _models.ai_code_generator - AI generated training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:15:49,079 - INFO - _models.ai_code_generator - Confidence: 0.90
2025-09-22 19:15:49,079 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.73)
2025-09-22 19:15:49,079 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:15:49,079 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['epochs', 'batch_size', 'lr', 'weight_decay', 'optimizer', 'grad_clip_norm', 'tcn_channels', 'num_tcn_blocks', 'dropout', 'd_model', 'num_heads', 'num_encoder_layers', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'class_weighting', 'use_smote_tomek', 'smote_k_neighbors', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calib_batches']
2025-09-22 19:15:49,079 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.90
2025-09-22 19:15:49,080 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-22 19:15:49,080 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_MB-MHA-TCN-TinyQuant_1758586549.json
2025-09-22 19:15:49,080 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_MB-MHA-TCN-TinyQuant_1758586549.json
2025-09-22 19:15:49,080 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-22 19:15:49,080 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:15:49,080 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 19:15:49,081 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 19:15:49,084 - INFO - package_installer - Extracted imports from code: {'torch', 'imblearn'}
2025-09-22 19:15:49,084 - INFO - package_installer - Available packages: {'torch'}
2025-09-22 19:15:49,084 - INFO - package_installer - Missing packages: {'imblearn'}
2025-09-22 19:15:49,084 - INFO - package_installer - üì¶ Installing 1 missing packages...
2025-09-22 19:15:49,084 - INFO - package_installer - Installing 1 packages: {'imblearn'}
2025-09-22 19:15:49,084 - INFO - package_installer - Installing package: imblearn
2025-09-22 19:15:49,716 - INFO - package_installer - ‚úÖ Successfully installed: imblearn
2025-09-22 19:15:49,717 - INFO - package_installer - Successfully installed 1 packages: {'imblearn'}
2025-09-22 19:15:49,717 - INFO - package_installer - ‚úÖ All packages installed successfully
2025-09-22 19:15:49,717 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 19:15:49,717 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 19:15:49,717 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 19:15:49,717 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['epochs', 'batch_size', 'lr', 'weight_decay', 'optimizer', 'grad_clip_norm', 'tcn_channels', 'num_tcn_blocks', 'dropout', 'd_model', 'num_heads', 'num_encoder_layers', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'class_weighting', 'use_smote_tomek', 'smote_k_neighbors', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calib_batches']
2025-09-22 19:15:49,717 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 19:15:49,717 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 19:15:49,717 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 19:15:49,751 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 19:15:49,879 - INFO - bo.run_bo - Converted GPT search space: 22 parameters
2025-09-22 19:15:49,880 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 19:15:49,880 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 19:15:49,881 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 19:15:49,881 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 19:15:49,881 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 19:15:49,881 - INFO - _models.training_function_executor - Executing training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:15:49,881 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 184, 'batch_size': 512, 'lr': 0.0008471801418819981, 'weight_decay': 3.90796715682288e-05, 'optimizer': 'adam', 'grad_clip_norm': 0.7799726016810133, 'tcn_channels': 18, 'num_tcn_blocks': 3, 'dropout': 0.4330880728874677, 'd_model': 51, 'num_heads': 4, 'num_encoder_layers': 2, 'use_focal_loss': True, 'focal_gamma': 3.8879950890672994, 'label_smoothing': 0.18771054180315008, 'class_weighting': 'none', 'use_smote_tomek': False, 'smote_k_neighbors': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 17}
2025-09-22 19:15:49,882 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 184, 'batch_size': 512, 'lr': 0.0008471801418819981, 'weight_decay': 3.90796715682288e-05, 'optimizer': 'adam', 'grad_clip_norm': 0.7799726016810133, 'tcn_channels': 18, 'num_tcn_blocks': 3, 'dropout': 0.4330880728874677, 'd_model': 51, 'num_heads': 4, 'num_encoder_layers': 2, 'use_focal_loss': True, 'focal_gamma': 3.8879950890672994, 'label_smoothing': 0.18771054180315008, 'class_weighting': 'none', 'use_smote_tomek': False, 'smote_k_neighbors': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 17}
2025-09-22 19:15:49,889 - ERROR - _models.training_function_executor - Training execution failed: The expanded size of the tensor (25) must match the existing size (26) at non-singleton dimension 1.  Target sizes: [1000, 25].  Tensor sizes: [1000, 26]
2025-09-22 19:15:49,889 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 19:15:49,889 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 19:15:49,889 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 19:15:49,889 - INFO - _models.ai_code_generator - Prompt length: 21215 characters
2025-09-22 19:15:49,889 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 19:15:49,890 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 19:15:49,890 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 19:18:32,904 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 19:18:32,907 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 19:18:32,907 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250922_191832_attempt1.txt
2025-09-22 19:18:32,907 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hparams):\n    import io\n    import math\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader\n    from typing import List, Tuple\n    \n    # ---------------------------\n    # Defaults & Hyperparameters\n    # ---------------------------\n    defaults = {\n        # optimization\n        'epochs': 20,\n        'batch_size': 256,\n        'lr': 3e-4,\n        'weight_decay': 1e-4,\n        'optimizer': 'adamw',  # ['adam','adamw']\n        'grad_clip_norm': 1.0,\n        # model\n        'tcn_channels': 12,            # base channels after stem\n        'num_tcn_blocks': 2,          # number of residual MB-TCN blocks\n        'dropout': 0.1,\n        'd_model': 32,\n        'num_heads': 4,\n        'num_encoder_layers': 1,      # 1-2 as recommended\n        # loss & imbalance\n        'use_focal_loss': True,\n        'focal_gamma': 2.0,\n        'label_smoothing': 0.0,\n        'class_weighting': 'auto',    # ['none','auto']\n        'use_smote_tomek': False,\n        'smote_k_neighbors': 5,\n        # quantization\n        'quantization_bits': 8,       # [8,16,32]\n        'quantize_weights': True,\n        'quantize_activations': False,\n        'calib_batches': 4,           # used if static calibration is enabled\n    }\n    # Merge provided hyperparameters\n    for k, v in defaults.items():\n        if k not in hparams:\n            hparams[k] = v\n    \n    # ---------------------------\n    # Assertions & Setup\n    # ---------------------------\n    if not isinstance(device, torch.device):\n        device = torch.device(device)\n    assert device.type == 'cuda', 'Training must run on GPU (cuda).'\n    torch.backends.cudnn.benchmark = True\n    \n    num_classes = 5\n    seq_len = 1000\n    in_channels = 2\n    \n    # Ensure heads divide d_model\n    def _fix_heads(d_model, num_heads):\n        # ensure num_heads is a divisor of d_model\n        if d_model % num_heads == 0:\n            return num_heads\n        # pick the largest divisor <= num_heads, else fallback to 1\n        divisors = [d for d in range(min(d_model, num_heads), 0, -1) if d_model % d == 0]\n        return divisors[0] if len(divisors) > 0 else 1\n    hparams['num_heads'] = _fix_heads(int(hparams['d_model']), int(hparams['num_heads']))\n    \n    # ---------------------------\n    # Optional SMOTE+Tomek on train only (before DataLoader)\n    # ---------------------------\n    def maybe_smote_tomek(Xt, yt, use_smote, k_neighbors):\n        if not use_smote:\n            return Xt, yt\n        try:\n            from imblearn.combine import SMOTETomek\n        except Exception:\n            return Xt, yt  # fallback silently if imblearn not available\n        # Expect Xt shape: (N, 2, 1000) or (N, 1000, 2)\n        Xt_cpu = Xt.detach().cpu()\n        if Xt_cpu.dim() != 3:\n            raise ValueError('Expected X tensor with 3 dims: (N, C, L) or (N, L, C)')\n        if Xt_cpu.shape[1] == seq_len and Xt_cpu.shape[2] == in_channels:\n            Xt_cpu = Xt_cpu.permute(0, 2, 1)  # (N, 2, 1000)\n        # Flatten to 2D for SMOTE\n        N = Xt_cpu.shape[0]\n        X_flat = Xt_cpu.reshape(N, -1).numpy()\n        y_np = yt.detach().cpu().numpy()\n        smt = SMOTETomek(k_neighbors=max(1, int(k_neighbors)))\n        X_res, y_res = smt.fit_resample(X_flat, y_np)\n        X_res_t = torch.from_numpy(X_res).float().reshape(-1, in_channels, seq_len)\n        y_res_t = torch.from_numpy(y_res).long()\n        return X_res_t, y_res_t\n    \n    # ---------------------------\n    # Dataset & Dataloaders\n    # ---------------------------\n    class ECGDataset(Dataset):\n        def __init__(self, X, y):\n            super().__init__()\n            self.X = X\n            self.y = y\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            # Ensure shape (C, L) for Conv1d\n            if x.dim() == 2 and x.shape[0] == seq_len and x.shape[1] == in_channels:\n                x = x.permute(1, 0)  # (2, 1000)\n            elif x.dim() == 2 and x.shape[0] == in_channels and x.shape[1] == seq_len:\n                pass\n            else:\n                raise ValueError('Each sample must have shape (1000, 2) or (2, 1000).')\n            y = self.y[idx]\n            return x, y\n    \n    # Apply optional SMOTE/Tomek to training set only (on CPU) before loaders\n    X_train_bal, y_train_bal = maybe_smote_tomek(X_train, y_train, bool(hparams['use_smote_tomek']), int(hparams['smote_k_neighbors']))\n    \n    train_ds = ECGDataset(X_train_bal, y_train_bal)\n    val_ds = ECGDataset(X_val, y_val)\n    \n    train_loader = DataLoader(train_ds, batch_size=int(hparams['batch_size']), shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=int(hparams['batch_size']), shuffle=False, num_workers=0, pin_memory=False)\n    \n    # ---------------------------\n    # Model Definition: MB-MHA-TCN Tiny\n    # ---------------------------\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model: int, max_len: int = 5000):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            if d_model % 2 == 1:\n                pe[:, 1::2] = torch.cos(position * div_term[:-1])\n            else:\n                pe[:, 1::2] = torch.cos(position * div_term)\n            pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n            self.register_buffer('pe', pe)\n        def forward(self, x):  # x: (B, L, d_model)\n            L = x.size(1)\n            return x + self.pe[:, :L, :]\n    \n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, channels: int, kernel_size: int, dilation: int, dropout: float):\n            super().__init__()\n            padding = (kernel_size - 1) // 2 * dilation\n            self.dw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size, padding=padding, dilation=dilation, groups=channels, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.pw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.do = nn.Dropout(p=dropout)\n        def forward(self, x):\n            x = self.dw[x.__class__] if False else self.dw(x)\n            x = self.pw(x)\n            x = self.do(x)\n            return x\n    \n    class MBTCNBlock(nn.Module):\n        def __init__(self, channels: int, dropout: float):\n            super().__init__()\n            ks = [7, 15, 31]\n            dil = [1, 2, 4]\n            self.branches = nn.ModuleList([\n                DepthwiseSeparableConv1d(channels, ks[0], dil[0], dropout),\n                DepthwiseSeparableConv1d(channels, ks[1], dil[1], dropout),\n                DepthwiseSeparableConv1d(channels, ks[2], dil[2], dropout),\n            ])\n            self.merge = nn.Sequential(\n                nn.Conv1d(channels * 3, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n            )\n            self.act = nn.ReLU(inplace=True)\n        def forward(self, x):\n            outs = [b(x) for b in self.branches]\n            y = torch.cat(outs, dim=1)\n            y = self.merge(y)\n            y = y + x  # residual\n            return self.act(y)\n    \n    class TransformerEncoderBlock(nn.Module):\n        def __init__(self, d_model: int, nhead: int, dropout: float):\n            super().__init__()\n            self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n            self.ln1 = nn.LayerNorm(d_model)\n            self.ffn = nn.Sequential(\n                nn.Linear(d_model, d_model * 2),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(d_model * 2, d_model),\n            )\n            self.ln2 = nn.LayerNorm(d_model)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):  # x: (B, L, D)\n            attn_out, _ = self.mha(x, x, x, need_weights=False)\n            x = self.ln1(x + self.drop(attn_out))\n            x = self.ln2(x + self.drop(self.ffn(x)))\n            return x\n    \n    class MB_MHA_TCN_Tiny(nn.Module):\n        def __init__(self, in_ch: int, base_ch: int, d_model: int, nhead: int, num_layers: int, dropout: float, num_classes: int, use_act_quant: bool = False):\n            super().__init__()\n            self.use_act_quant = use_act_quant\n            if use_act_quant:\n                from torch.ao.quantization import QuantStub, DeQuantStub\n                self.quant = QuantStub()\n                self.dequant = DeQuantStub()\n            else:\n                self.quant = None\n                self.dequant = None\n            # Stem\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=7, padding=3, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.ReLU(inplace=True),\n            )\n            # MB-TCN blocks\n            self.tcn_blocks = nn.Sequential(*[MBTCNBlock(base_ch, dropout) for _ in range(int(hparams['num_tcn_blocks']))])\n            # Projection to d_model\n            self.proj = nn.Sequential(\n                nn.Conv1d(base_ch, d_model, kernel_size=1, bias=False),\n                nn.BatchNorm1d(d_model),\n                nn.ReLU(inplace=True),\n            )\n            # Transformer encoder(s)\n            self.posenc = PositionalEncoding(d_model, max_len=seq_len)\n            self.encoder = nn.Sequential(*[TransformerEncoderBlock(d_model, nhead, dropout) for _ in range(int(hparams['num_encoder_layers']))])\n            # Head\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Linear(d_model, num_classes)\n        def forward(self, x):  # x: (B, C, L)\n            if self.quant is not None:\n                x = self.quant(x)\n            x = self.stem(x)\n            x = self.tcn_blocks(x)\n            x = self.proj(x)            # (B, D, L)\n            x = x.transpose(1, 2)       # (B, L, D)\n            x = self.posenc(x)\n            x = self.encoder(x)\n            x = x.transpose(1, 2)       # (B, D, L)\n            x = self.pool(x).squeeze(-1)  # (B, D)\n            x = self.fc(x)              # (B, num_classes)\n            if self.dequant is not None:\n                x = self.dequant(x)\n            return x\n        def fuse_for_quant(self):\n            # Try to fuse typical conv-bn-relu patterns\n            for name, module in self.named_modules():\n                pass  # kept simple; prepare/convert will still quantize supported ops\n    \n    # ---------------------------\n    # Loss functions\n    # ---------------------------\n    def compute_class_weights(y: torch.Tensor, num_classes: int) -> torch.Tensor:\n        with torch.no_grad():\n            counts = torch.bincount(y.view(-1).cpu(), minlength=num_classes).float()\n            counts[counts == 0] = 1.0\n            inv = 1.0 / counts\n            w = inv / inv.sum() * num_classes\n            return w\n    \n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0):\n            super().__init__()\n            self.gamma = gamma\n            self.weight = weight\n            self.label_smoothing = label_smoothing\n        def forward(self, logits, targets):\n            ce = F.cross_entropy(logits, targets, weight=self.weight, reduction='none', label_smoothing=self.label_smoothing)\n            pt = torch.softmax(logits, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n            pt = pt.clamp(1e-6, 1 - 1e-6)\n            loss = ((1 - pt) ** self.gamma) * ce\n            return loss.mean()\n    \n    # ---------------------------\n    # Build model, optimizer, criterion\n    # ---------------------------\n    model = MB_MHA_TCN_Tiny(\n        in_ch=in_channels,\n        base_ch=int(hparams['tcn_channels']),\n        d_model=int(hparams['d_model']),\n        nhead=int(hparams['num_heads']),\n        num_layers=int(hparams['num_encoder_layers']),\n        dropout=float(hparams['dropout']),\n        num_classes=num_classes,\n        use_act_quant=bool(hparams['quantize_activations']) and int(hparams['quantization_bits']) == 8,\n    ).to(device)\n    \n    # Class weights\n    if hparams['class_weighting'] == 'auto':\n        cls_w = compute_class_weights(y_train_bal, num_classes).to(device)\n    else:\n        cls_w = None\n    \n    # Criterion\n    if bool(hparams['use_focal_loss']):\n        criterion = FocalLoss(gamma=float(hparams['focal_gamma']), weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    else:\n        criterion = nn.CrossEntropyLoss(weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    \n    # Optimizer\n    if hparams['optimizer'].lower() == 'adamw':\n        optimizer = optim.AdamW(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    else:\n        optimizer = optim.Adam(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    \n    # ---------------------------\n    # Training Loop\n    # ---------------------------\n    train_losses: List[float] = []\n    val_losses: List[float] = []\n    val_accs: List[float] = []\n    \n    for epoch in range(int(hparams['epochs'])):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False).float()\n            yb = yb.to(device, non_blocking=False).long()\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if float(hparams['grad_clip_norm']) > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(hparams['grad_clip_norm']))\n            optimizer.step()\n            bs = xb.size(0)\n            running_loss += loss.detach().item() * bs\n            n_train += bs\n        train_loss_epoch = running_loss / max(1, n_train)\n        train_losses.append(train_loss_epoch)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False).float()\n                yb = yb.to(device, non_blocking=False).long()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        val_loss_epoch = val_loss / max(1, total)\n        val_acc_epoch = correct / max(1, total)\n        val_losses.append(val_loss_epoch)\n        val_accs.append(val_acc_epoch)\n        print(f\"Epoch {epoch+1}/{hparams['epochs']} - train_loss: {train_loss_epoch:.6f} - val_loss: {val_loss_epoch:.6f} - val_acc: {val_acc_epoch:.4f}\")\n    \n    # ---------------------------\n    # Post-Training Quantization (on CPU)\n    # ---------------------------\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n    quant_bits = int(hparams['quantization_bits'])\n    q_weights = bool(hparams['quantize_weights'])\n    q_acts = bool(hparams['quantize_activations'])\n    \n    quantized_model = model_cpu\n    \n    if quant_bits == 32 or not q_weights:\n        # No quantization (float32)\n        pass\n    elif quant_bits == 16:\n        # FP16 cast (weights + activations during inference if run in fp16-capable device)\n        quantized_model = model_cpu.half()\n    elif quant_bits == 8:\n        if q_acts:\n            # Static quantization with calibration\n            from torch.ao.quantization import get_default_qconfig\n            from torch.ao.quantization import prepare, convert\n            torch.backends.quantized.engine = 'fbgemm'\n            quantized_model.fuse_for_quant()\n            quantized_model.qconfig = get_default_qconfig('fbgemm')\n            prepare(quantized_model, inplace=True)\n            # Calibration on a few batches from training data (CPU)\n            calib_batches = max(1, int(hparams['calib_batches']))\n            quantized_model.eval()\n            with torch.inference_mode():\n                seen = 0\n                for xb, _ in DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=0, pin_memory=False):\n                    xb = xb.float()\n                    _ = quantized_model(xb)\n                    seen += 1\n                    if seen >= calib_batches:\n                        break\n            convert(quantized_model, inplace=True)\n        else:\n            # Dynamic quantization for Linear layers (covers MHA and FFN); convs remain FP32\n            from torch.ao.quantization import quantize_dynamic\n            quantized_model = quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)\n    else:\n        pass\n    \n    # ---------------------------\n    # Compute serialized size (KB)\n    # ---------------------------\n    with io.BytesIO() as f:\n        try:\n            torch.save(quantized_model.state_dict(), f)\n        except Exception:\n            # fallback to full model if state_dict fails\n            torch.save(quantized_model, f)\n        size_bytes = f.tell()\n    size_kb = size_bytes / 1024.0\n    \n    # Enforce size constraint by design; print warning if exceeded\n    if size_kb > 256.0:\n        print(f\"Warning: Quantized model size {size_kb:.2f} KB exceeds 256KB. Consider reducing d_model/tcn_channels.\")\n    \n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_kb': size_kb,\n        'used_hparams': hparams\n    }\n    return quantized_model, metrics\n"}
2025-09-22 19:18:32,907 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-22 19:18:32,907 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hparams):\n    import io\n    import math\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader\n    from typing import List, Tuple\n    \n    # ---------------------------\n    # Defaults & Hyperparameters\n    # ---------------------------\n    defaults = {\n        # optimization\n        'epochs': 20,\n        'batch_size': 256,\n        'lr': 3e-4,\n        'weight_decay': 1e-4,\n        'optimizer': 'adamw',  # ['adam','adamw']\n        'grad_clip_norm': 1.0,\n        # model\n        'tcn_channels': 12,            # base channels after stem\n        'num_tcn_blocks': 2,          # number of residual MB-TCN blocks\n        'dropout': 0.1,\n        'd_model': 32,\n        'num_heads': 4,\n        'num_encoder_layers': 1,      # 1-2 as recommended\n        # loss & imbalance\n        'use_focal_loss': True,\n        'focal_gamma': 2.0,\n        'label_smoothing': 0.0,\n        'class_weighting': 'auto',    # ['none','auto']\n        'use_smote_tomek': False,\n        'smote_k_neighbors': 5,\n        # quantization\n        'quantization_bits': 8,       # [8,16,32]\n        'quantize_weights': True,\n        'quantize_activations': False,\n        'calib_batches': 4,           # used if static calibration is enabled\n    }\n    # Merge provided hyperparameters\n    for k, v in defaults.items():\n        if k not in hparams:\n            hparams[k] = v\n    \n    # ---------------------------\n    # Assertions & Setup\n    # ---------------------------\n    if not isinstance(device, torch.device):\n        device = torch.device(device)\n    assert device.type == 'cuda', 'Training must run on GPU (cuda).'\n    torch.backends.cudnn.benchmark = True\n    \n    num_classes = 5\n    seq_len = 1000\n    in_channels = 2\n    \n    # Ensure heads divide d_model\n    def _fix_heads(d_model, num_heads):\n        # ensure num_heads is a divisor of d_model\n        if d_model % num_heads == 0:\n            return num_heads\n        # pick the largest divisor <= num_heads, else fallback to 1\n        divisors = [d for d in range(min(d_model, num_heads), 0, -1) if d_model % d == 0]\n        return divisors[0] if len(divisors) > 0 else 1\n    hparams['num_heads'] = _fix_heads(int(hparams['d_model']), int(hparams['num_heads']))\n    \n    # ---------------------------\n    # Optional SMOTE+Tomek on train only (before DataLoader)\n    # ---------------------------\n    def maybe_smote_tomek(Xt, yt, use_smote, k_neighbors):\n        if not use_smote:\n            return Xt, yt\n        try:\n            from imblearn.combine import SMOTETomek\n        except Exception:\n            return Xt, yt  # fallback silently if imblearn not available\n        # Expect Xt shape: (N, 2, 1000) or (N, 1000, 2)\n        Xt_cpu = Xt.detach().cpu()\n        if Xt_cpu.dim() != 3:\n            raise ValueError('Expected X tensor with 3 dims: (N, C, L) or (N, L, C)')\n        if Xt_cpu.shape[1] == seq_len and Xt_cpu.shape[2] == in_channels:\n            Xt_cpu = Xt_cpu.permute(0, 2, 1)  # (N, 2, 1000)\n        # Flatten to 2D for SMOTE\n        N = Xt_cpu.shape[0]\n        X_flat = Xt_cpu.reshape(N, -1).numpy()\n        y_np = yt.detach().cpu().numpy()\n        smt = SMOTETomek(k_neighbors=max(1, int(k_neighbors)))\n        X_res, y_res = smt.fit_resample(X_flat, y_np)\n        X_res_t = torch.from_numpy(X_res).float().reshape(-1, in_channels, seq_len)\n        y_res_t = torch.from_numpy(y_res).long()\n        return X_res_t, y_res_t\n    \n    # ---------------------------\n    # Dataset & Dataloaders\n    # ---------------------------\n    class ECGDataset(Dataset):\n        def __init__(self, X, y):\n            super().__init__()\n            self.X = X\n            self.y = y\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            # Ensure shape (C, L) for Conv1d\n            if x.dim() == 2 and x.shape[0] == seq_len and x.shape[1] == in_channels:\n                x = x.permute(1, 0)  # (2, 1000)\n            elif x.dim() == 2 and x.shape[0] == in_channels and x.shape[1] == seq_len:\n                pass\n            else:\n                raise ValueError('Each sample must have shape (1000, 2) or (2, 1000).')\n            y = self.y[idx]\n            return x, y\n    \n    # Apply optional SMOTE/Tomek to training set only (on CPU) before loaders\n    X_train_bal, y_train_bal = maybe_smote_tomek(X_train, y_train, bool(hparams['use_smote_tomek']), int(hparams['smote_k_neighbors']))\n    \n    train_ds = ECGDataset(X_train_bal, y_train_bal)\n    val_ds = ECGDataset(X_val, y_val)\n    \n    train_loader = DataLoader(train_ds, batch_size=int(hparams['batch_size']), shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=int(hparams['batch_size']), shuffle=False, num_workers=0, pin_memory=False)\n    \n    # ---------------------------\n    # Model Definition: MB-MHA-TCN Tiny\n    # ---------------------------\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model: int, max_len: int = 5000):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            if d_model % 2 == 1:\n                pe[:, 1::2] = torch.cos(position * div_term[:-1])\n            else:\n                pe[:, 1::2] = torch.cos(position * div_term)\n            pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n            self.register_buffer('pe', pe)\n        def forward(self, x):  # x: (B, L, d_model)\n            L = x.size(1)\n            return x + self.pe[:, :L, :]\n    \n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, channels: int, kernel_size: int, dilation: int, dropout: float):\n            super().__init__()\n            padding = (kernel_size - 1) // 2 * dilation\n            self.dw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size, padding=padding, dilation=dilation, groups=channels, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.pw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.do = nn.Dropout(p=dropout)\n        def forward(self, x):\n            x = self.dw[x.__class__] if False else self.dw(x)\n            x = self.pw(x)\n            x = self.do(x)\n            return x\n    \n    class MBTCNBlock(nn.Module):\n        def __init__(self, channels: int, dropout: float):\n            super().__init__()\n            ks = [7, 15, 31]\n            dil = [1, 2, 4]\n            self.branches = nn.ModuleList([\n                DepthwiseSeparableConv1d(channels, ks[0], dil[0], dropout),\n                DepthwiseSeparableConv1d(channels, ks[1], dil[1], dropout),\n                DepthwiseSeparableConv1d(channels, ks[2], dil[2], dropout),\n            ])\n            self.merge = nn.Sequential(\n                nn.Conv1d(channels * 3, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n            )\n            self.act = nn.ReLU(inplace=True)\n        def forward(self, x):\n            outs = [b(x) for b in self.branches]\n            y = torch.cat(outs, dim=1)\n            y = self.merge(y)\n            y = y + x  # residual\n            return self.act(y)\n    \n    class TransformerEncoderBlock(nn.Module):\n        def __init__(self, d_model: int, nhead: int, dropout: float):\n            super().__init__()\n            self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n            self.ln1 = nn.LayerNorm(d_model)\n            self.ffn = nn.Sequential(\n                nn.Linear(d_model, d_model * 2),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(d_model * 2, d_model),\n            )\n            self.ln2 = nn.LayerNorm(d_model)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):  # x: (B, L, D)\n            attn_out, _ = self.mha(x, x, x, need_weights=False)\n            x = self.ln1(x + self.drop(attn_out))\n            x = self.ln2(x + self.drop(self.ffn(x)))\n            return x\n    \n    class MB_MHA_TCN_Tiny(nn.Module):\n        def __init__(self, in_ch: int, base_ch: int, d_model: int, nhead: int, num_layers: int, dropout: float, num_classes: int, use_act_quant: bool = False):\n            super().__init__()\n            self.use_act_quant = use_act_quant\n            if use_act_quant:\n                from torch.ao.quantization import QuantStub, DeQuantStub\n                self.quant = QuantStub()\n                self.dequant = DeQuantStub()\n            else:\n                self.quant = None\n                self.dequant = None\n            # Stem\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=7, padding=3, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.ReLU(inplace=True),\n            )\n            # MB-TCN blocks\n            self.tcn_blocks = nn.Sequential(*[MBTCNBlock(base_ch, dropout) for _ in range(int(hparams['num_tcn_blocks']))])\n            # Projection to d_model\n            self.proj = nn.Sequential(\n                nn.Conv1d(base_ch, d_model, kernel_size=1, bias=False),\n                nn.BatchNorm1d(d_model),\n                nn.ReLU(inplace=True),\n            )\n            # Transformer encoder(s)\n            self.posenc = PositionalEncoding(d_model, max_len=seq_len)\n            self.encoder = nn.Sequential(*[TransformerEncoderBlock(d_model, nhead, dropout) for _ in range(int(hparams['num_encoder_layers']))])\n            # Head\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Linear(d_model, num_classes)\n        def forward(self, x):  # x: (B, C, L)\n            if self.quant is not None:\n                x = self.quant(x)\n            x = self.stem(x)\n            x = self.tcn_blocks(x)\n            x = self.proj(x)            # (B, D, L)\n            x = x.transpose(1, 2)       # (B, L, D)\n            x = self.posenc(x)\n            x = self.encoder(x)\n            x = x.transpose(1, 2)       # (B, D, L)\n            x = self.pool(x).squeeze(-1)  # (B, D)\n            x = self.fc(x)              # (B, num_classes)\n            if self.dequant is not None:\n                x = self.dequant(x)\n            return x\n        def fuse_for_quant(self):\n            # Try to fuse typical conv-bn-relu patterns\n            for name, module in self.named_modules():\n                pass  # kept simple; prepare/convert will still quantize supported ops\n    \n    # ---------------------------\n    # Loss functions\n    # ---------------------------\n    def compute_class_weights(y: torch.Tensor, num_classes: int) -> torch.Tensor:\n        with torch.no_grad():\n            counts = torch.bincount(y.view(-1).cpu(), minlength=num_classes).float()\n            counts[counts == 0] = 1.0\n            inv = 1.0 / counts\n            w = inv / inv.sum() * num_classes\n            return w\n    \n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0):\n            super().__init__()\n            self.gamma = gamma\n            self.weight = weight\n            self.label_smoothing = label_smoothing\n        def forward(self, logits, targets):\n            ce = F.cross_entropy(logits, targets, weight=self.weight, reduction='none', label_smoothing=self.label_smoothing)\n            pt = torch.softmax(logits, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n            pt = pt.clamp(1e-6, 1 - 1e-6)\n            loss = ((1 - pt) ** self.gamma) * ce\n            return loss.mean()\n    \n    # ---------------------------\n    # Build model, optimizer, criterion\n    # ---------------------------\n    model = MB_MHA_TCN_Tiny(\n        in_ch=in_channels,\n        base_ch=int(hparams['tcn_channels']),\n        d_model=int(hparams['d_model']),\n        nhead=int(hparams['num_heads']),\n        num_layers=int(hparams['num_encoder_layers']),\n        dropout=float(hparams['dropout']),\n        num_classes=num_classes,\n        use_act_quant=bool(hparams['quantize_activations']) and int(hparams['quantization_bits']) == 8,\n    ).to(device)\n    \n    # Class weights\n    if hparams['class_weighting'] == 'auto':\n        cls_w = compute_class_weights(y_train_bal, num_classes).to(device)\n    else:\n        cls_w = None\n    \n    # Criterion\n    if bool(hparams['use_focal_loss']):\n        criterion = FocalLoss(gamma=float(hparams['focal_gamma']), weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    else:\n        criterion = nn.CrossEntropyLoss(weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    \n    # Optimizer\n    if hparams['optimizer'].lower() == 'adamw':\n        optimizer = optim.AdamW(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    else:\n        optimizer = optim.Adam(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    \n    # ---------------------------\n    # Training Loop\n    # ---------------------------\n    train_losses: List[float] = []\n    val_losses: List[float] = []\n    val_accs: List[float] = []\n    \n    for epoch in range(int(hparams['epochs'])):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False).float()\n            yb = yb.to(device, non_blocking=False).long()\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if float(hparams['grad_clip_norm']) > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(hparams['grad_clip_norm']))\n            optimizer.step()\n            bs = xb.size(0)\n            running_loss += loss.detach().item() * bs\n            n_train += bs\n        train_loss_epoch = running_loss / max(1, n_train)\n        train_losses.append(train_loss_epoch)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False).float()\n                yb = yb.to(device, non_blocking=False).long()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        val_loss_epoch = val_loss / max(1, total)\n        val_acc_epoch = correct / max(1, total)\n        val_losses.append(val_loss_epoch)\n        val_accs.append(val_acc_epoch)\n        print(f\"Epoch {epoch+1}/{hparams['epochs']} - train_loss: {train_loss_epoch:.6f} - val_loss: {val_loss_epoch:.6f} - val_acc: {val_acc_epoch:.4f}\")\n    \n    # ---------------------------\n    # Post-Training Quantization (on CPU)\n    # ---------------------------\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n    quant_bits = int(hparams['quantization_bits'])\n    q_weights = bool(hparams['quantize_weights'])\n    q_acts = bool(hparams['quantize_activations'])\n    \n    quantized_model = model_cpu\n    \n    if quant_bits == 32 or not q_weights:\n        # No quantization (float32)\n        pass\n    elif quant_bits == 16:\n        # FP16 cast (weights + activations during inference if run in fp16-capable device)\n        quantized_model = model_cpu.half()\n    elif quant_bits == 8:\n        if q_acts:\n            # Static quantization with calibration\n            from torch.ao.quantization import get_default_qconfig\n            from torch.ao.quantization import prepare, convert\n            torch.backends.quantized.engine = 'fbgemm'\n            quantized_model.fuse_for_quant()\n            quantized_model.qconfig = get_default_qconfig('fbgemm')\n            prepare(quantized_model, inplace=True)\n            # Calibration on a few batches from training data (CPU)\n            calib_batches = max(1, int(hparams['calib_batches']))\n            quantized_model.eval()\n            with torch.inference_mode():\n                seen = 0\n                for xb, _ in DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=0, pin_memory=False):\n                    xb = xb.float()\n                    _ = quantized_model(xb)\n                    seen += 1\n                    if seen >= calib_batches:\n                        break\n            convert(quantized_model, inplace=True)\n        else:\n            # Dynamic quantization for Linear layers (covers MHA and FFN); convs remain FP32\n            from torch.ao.quantization import quantize_dynamic\n            quantized_model = quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)\n    else:\n        pass\n    \n    # ---------------------------\n    # Compute serialized size (KB)\n    # ---------------------------\n    with io.BytesIO() as f:\n        try:\n            torch.save(quantized_model.state_dict(), f)\n        except Exception:\n            # fallback to full model if state_dict fails\n            torch.save(quantized_model, f)\n        size_bytes = f.tell()\n    size_kb = size_bytes / 1024.0\n    \n    # Enforce size constraint by design; print warning if exceeded\n    if size_kb > 256.0:\n        print(f\"Warning: Quantized model size {size_kb:.2f} KB exceeds 256KB. Consider reducing d_model/tcn_channels.\")\n    \n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_kb': size_kb,\n        'used_hparams': hparams\n    }\n    return quantized_model, metrics\n"}
2025-09-22 19:18:32,907 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-22 19:18:32,907 - ERROR - _models.training_function_executor - BO training objective failed: The expanded size of the tensor (25) must match the existing size (26) at non-singleton dimension 1.  Target sizes: [1000, 25].  Tensor sizes: [1000, 26]
2025-09-22 19:18:32,907 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 163.026s
2025-09-22 19:18:32,907 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: The expanded size of the tensor (25) must match the existing size (26) at non-singleton dimension 1.  Target sizes: [1000, 25].  Tensor sizes: [1000, 26]
2025-09-22 19:18:32,907 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-22 19:18:35,911 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-22 19:18:35,911 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß GPT has fixed the training function, reloading from JSON file
2025-09-22 19:18:35,912 - INFO - evaluation.code_generation_pipeline_orchestrator - Reloading fixed training function from: generated_training_functions/training_function_torch_tensor_MB-MHA-TCN-TinyQuant_1758586549.json
2025-09-22 19:18:35,912 - INFO - _models.training_function_executor - Loaded training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:18:35,912 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-22 19:18:35,912 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:18:35,912 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-22 19:18:35,912 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 1/4
2025-09-22 19:18:35,912 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 1: üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 19:18:35,912 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 19:18:35,915 - INFO - package_installer - Extracted imports from code: {'torch', 'imblearn'}
2025-09-22 19:18:35,915 - INFO - package_installer - Available packages: {'torch', 'imblearn'}
2025-09-22 19:18:35,915 - INFO - package_installer - Missing packages: set()
2025-09-22 19:18:35,915 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-22 19:18:35,915 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 19:18:35,915 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 19:18:35,915 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 19:18:35,915 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['epochs', 'batch_size', 'lr', 'weight_decay', 'optimizer', 'grad_clip_norm', 'tcn_channels', 'num_tcn_blocks', 'dropout', 'd_model', 'num_heads', 'num_encoder_layers', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'class_weighting', 'use_smote_tomek', 'smote_k_neighbors', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calib_batches']
2025-09-22 19:18:35,915 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 19:18:35,915 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 19:18:35,915 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 19:18:35,948 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 19:18:35,980 - INFO - bo.run_bo - Converted GPT search space: 22 parameters
2025-09-22 19:18:35,980 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 19:18:35,980 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 19:18:35,981 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 19:18:35,981 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 19:18:35,981 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 19:18:35,981 - INFO - _models.training_function_executor - Executing training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:18:35,981 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 184, 'batch_size': 512, 'lr': 0.0008471801418819981, 'weight_decay': 3.90796715682288e-05, 'optimizer': 'adam', 'grad_clip_norm': 0.7799726016810133, 'tcn_channels': 18, 'num_tcn_blocks': 3, 'dropout': 0.4330880728874677, 'd_model': 51, 'num_heads': 4, 'num_encoder_layers': 2, 'use_focal_loss': True, 'focal_gamma': 3.8879950890672994, 'label_smoothing': 0.18771054180315008, 'class_weighting': 'none', 'use_smote_tomek': False, 'smote_k_neighbors': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 17}
2025-09-22 19:18:35,982 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 184, 'batch_size': 512, 'lr': 0.0008471801418819981, 'weight_decay': 3.90796715682288e-05, 'optimizer': 'adam', 'grad_clip_norm': 0.7799726016810133, 'tcn_channels': 18, 'num_tcn_blocks': 3, 'dropout': 0.4330880728874677, 'd_model': 51, 'num_heads': 4, 'num_encoder_layers': 2, 'use_focal_loss': True, 'focal_gamma': 3.8879950890672994, 'label_smoothing': 0.18771054180315008, 'class_weighting': 'none', 'use_smote_tomek': False, 'smote_k_neighbors': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 17}
2025-09-22 19:18:35,984 - ERROR - _models.training_function_executor - Training execution failed: The expanded size of the tensor (25) must match the existing size (26) at non-singleton dimension 1.  Target sizes: [1000, 25].  Tensor sizes: [1000, 26]
2025-09-22 19:18:35,985 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 19:18:35,985 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 19:18:35,985 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 19:18:35,985 - INFO - _models.ai_code_generator - Prompt length: 21215 characters
2025-09-22 19:18:35,985 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 19:18:35,985 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 19:18:35,985 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 19:20:06,397 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 19:20:06,401 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 19:20:06,401 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250922_192006_attempt1.txt
2025-09-22 19:20:06,401 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hparams):\n    import io\n    import math\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader\n    from typing import List, Tuple\n    \n    # ---------------------------\n    # Defaults & Hyperparameters\n    # ---------------------------\n    defaults = {\n        # optimization\n        'epochs': 20,\n        'batch_size': 256,\n        'lr': 3e-4,\n        'weight_decay': 1e-4,\n        'optimizer': 'adamw',  # ['adam','adamw']\n        'grad_clip_norm': 1.0,\n        # model\n        'tcn_channels': 12,            # base channels after stem\n        'num_tcn_blocks': 2,          # number of residual MB-TCN blocks\n        'dropout': 0.1,\n        'd_model': 32,\n        'num_heads': 4,\n        'num_encoder_layers': 1,      # 1-2 as recommended\n        # loss & imbalance\n        'use_focal_loss': True,\n        'focal_gamma': 2.0,\n        'label_smoothing': 0.0,\n        'class_weighting': 'auto',    # ['none','auto']\n        'use_smote_tomek': False,\n        'smote_k_neighbors': 5,\n        # quantization\n        'quantization_bits': 8,       # [8,16,32]\n        'quantize_weights': True,\n        'quantize_activations': False,\n        'calib_batches': 4,           # used if static calibration is enabled\n    }\n    # Merge provided hyperparameters\n    for k, v in defaults.items():\n        if k not in hparams:\n            hparams[k] = v\n    \n    # ---------------------------\n    # Assertions & Setup\n    # ---------------------------\n    if not isinstance(device, torch.device):\n        device = torch.device(device)\n    assert device.type == 'cuda', 'Training must run on GPU (cuda).'\n    torch.backends.cudnn.benchmark = True\n    \n    num_classes = 5\n    seq_len = 1000\n    in_channels = 2\n    \n    # Ensure heads divide d_model\n    def _fix_heads(d_model, num_heads):\n        # ensure num_heads is a divisor of d_model\n        if d_model % num_heads == 0:\n            return num_heads\n        # pick the largest divisor <= num_heads, else fallback to 1\n        divisors = [d for d in range(min(d_model, num_heads), 0, -1) if d_model % d == 0]\n        return divisors[0] if len(divisors) > 0 else 1\n    hparams['num_heads'] = _fix_heads(int(hparams['d_model']), int(hparams['num_heads']))\n    \n    # ---------------------------\n    # Optional SMOTE+Tomek on train only (before DataLoader)\n    # ---------------------------\n    def maybe_smote_tomek(Xt, yt, use_smote, k_neighbors):\n        if not use_smote:\n            return Xt, yt\n        try:\n            from imblearn.combine import SMOTETomek\n        except Exception:\n            return Xt, yt  # fallback silently if imblearn not available\n        # Expect Xt shape: (N, 2, 1000) or (N, 1000, 2)\n        Xt_cpu = Xt.detach().cpu()\n        if Xt_cpu.dim() != 3:\n            raise ValueError('Expected X tensor with 3 dims: (N, C, L) or (N, L, C)')\n        if Xt_cpu.shape[1] == seq_len and Xt_cpu.shape[2] == in_channels:\n            Xt_cpu = Xt_cpu.permute(0, 2, 1)  # (N, 2, 1000)\n        # Flatten to 2D for SMOTE\n        N = Xt_cpu.shape[0]\n        X_flat = Xt_cpu.reshape(N, -1).numpy()\n        y_np = yt.detach().cpu().numpy()\n        smt = SMOTETomek(k_neighbors=max(1, int(k_neighbors)))\n        X_res, y_res = smt.fit_resample(X_flat, y_np)\n        X_res_t = torch.from_numpy(X_res).float().reshape(-1, in_channels, seq_len)\n        y_res_t = torch.from_numpy(y_res).long()\n        return X_res_t, y_res_t\n    \n    # ---------------------------\n    # Dataset & Dataloaders\n    # ---------------------------\n    class ECGDataset(Dataset):\n        def __init__(self, X, y):\n            super().__init__()\n            self.X = X\n            self.y = y\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            # Ensure shape (C, L) for Conv1d\n            if x.dim() == 2 and x.shape[0] == seq_len and x.shape[1] == in_channels:\n                x = x.permute(1, 0)  # (2, 1000)\n            elif x.dim() == 2 and x.shape[0] == in_channels and x.shape[1] == seq_len:\n                pass\n            else:\n                raise ValueError('Each sample must have shape (1000, 2) or (2, 1000).')\n            y = self.y[idx]\n            return x, y\n    \n    # Apply optional SMOTE/Tomek to training set only (on CPU) before loaders\n    X_train_bal, y_train_bal = maybe_smote_tomek(X_train, y_train, bool(hparams['use_smote_tomek']), int(hparams['smote_k_neighbors']))\n    \n    train_ds = ECGDataset(X_train_bal, y_train_bal)\n    val_ds = ECGDataset(X_val, y_val)\n    \n    train_loader = DataLoader(train_ds, batch_size=int(hparams['batch_size']), shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=int(hparams['batch_size']), shuffle=False, num_workers=0, pin_memory=False)\n    \n    # ---------------------------\n    # Model Definition: MB-MHA-TCN Tiny\n    # ---------------------------\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model: int, max_len: int = 5000):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            # Handle odd d_model safely\n            pe[:, 0::2] = torch.sin(position * div_term)\n            odd_dim = d_model // 2\n            pe[:, 1::2] = torch.cos(position * div_term[:odd_dim])\n            pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n            self.register_buffer('pe', pe)\n        def forward(self, x):  # x: (B, L, d_model)\n            L = x.size(1)\n            return x + self.pe[:, :L, :]\n    \n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, channels: int, kernel_size: int, dilation: int, dropout: float):\n            super().__init__()\n            padding = (kernel_size - 1) // 2 * dilation\n            self.dw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size, padding=padding, dilation=dilation, groups=channels, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.pw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.do = nn.Dropout(p=dropout)\n        def forward(self, x):\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.do(x)\n            return x\n    \n    class MBTCNBlock(nn.Module):\n        def __init__(self, channels: int, dropout: float):\n            super().__init__()\n            ks = [7, 15, 31]\n            dil = [1, 2, 4]\n            self.branches = nn.ModuleList([\n                DepthwiseSeparableConv1d(channels, ks[0], dil[0], dropout),\n                DepthwiseSeparableConv1d(channels, ks[1], dil[1], dropout),\n                DepthwiseSeparableConv1d(channels, ks[2], dil[2], dropout),\n            ])\n            self.merge = nn.Sequential(\n                nn.Conv1d(channels * 3, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n            )\n            self.act = nn.ReLU(inplace=True)\n        def forward(self, x):\n            outs = [b(x) for b in self.branches]\n            y = torch.cat(outs, dim=1)\n            y = self.merge(y)\n            y = y + x  # residual\n            return self.act(y)\n    \n    class TransformerEncoderBlock(nn.Module):\n        def __init__(self, d_model: int, nhead: int, dropout: float):\n            super().__init__()\n            self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n            self.ln1 = nn.LayerNorm(d_model)\n            self.ffn = nn.Sequential(\n                nn.Linear(d_model, d_model * 2),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(d_model * 2, d_model),\n            )\n            self.ln2 = nn.LayerNorm(d_model)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):  # x: (B, L, D)\n            attn_out, _ = self.mha(x, x, x, need_weights=False)\n            x = self.ln1(x + self.drop(attn_out))\n            x = self.ln2(x + self.drop(self.ffn(x)))\n            return x\n    \n    class MB_MHA_TCN_Tiny(nn.Module):\n        def __init__(self, in_ch: int, base_ch: int, d_model: int, nhead: int, num_layers: int, dropout: float, num_classes: int, use_act_quant: bool = False):\n            super().__init__()\n            self.use_act_quant = use_act_quant\n            if use_act_quant:\n                from torch.ao.quantization import QuantStub, DeQuantStub\n                self.quant = QuantStub()\n                self.dequant = DeQuantStub()\n            else:\n                self.quant = None\n                self.dequant = None\n            # Stem\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=7, padding=3, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.ReLU(inplace=True),\n            )\n            # MB-TCN blocks\n            self.tcn_blocks = nn.Sequential(*[MBTCNBlock(base_ch, dropout) for _ in range(int(hparams['num_tcn_blocks']))])\n            # Projection to d_model\n            self.proj = nn.Sequential(\n                nn.Conv1d(base_ch, d_model, kernel_size=1, bias=False),\n                nn.BatchNorm1d(d_model),\n                nn.ReLU(inplace=True),\n            )\n            # Transformer encoder(s)\n            self.posenc = PositionalEncoding(d_model, max_len=seq_len)\n            self.encoder = nn.Sequential(*[TransformerEncoderBlock(d_model, nhead, dropout) for _ in range(int(hparams['num_encoder_layers']))])\n            # Head\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Linear(d_model, num_classes)\n        def forward(self, x):  # x: (B, C, L)\n            if self.quant is not None:\n                x = self.quant(x)\n            x = self.stem(x)\n            x = self.tcn_blocks(x)\n            x = self.proj(x)            # (B, D, L)\n            x = x.transpose(1, 2)       # (B, L, D)\n            x = self.posenc(x)\n            x = self.encoder(x)\n            x = x.transpose(1, 2)       # (B, D, L)\n            x = self.pool(x).squeeze(-1)  # (B, D)\n            x = self.fc(x)              # (B, num_classes)\n            if self.dequant is not None:\n                x = self.dequant(x)\n            return x\n        def fuse_for_quant(self):\n            # Try to fuse typical conv-bn-relu patterns\n            for name, module in self.named_modules():\n                pass  # kept simple; prepare/convert will still quantize supported ops\n    \n    # ---------------------------\n    # Loss functions\n    # ---------------------------\n    def compute_class_weights(y: torch.Tensor, num_classes: int) -> torch.Tensor:\n        with torch.no_grad():\n            counts = torch.bincount(y.view(-1).cpu(), minlength=num_classes).float()\n            counts[counts == 0] = 1.0\n            inv = 1.0 / counts\n            w = inv / inv.sum() * num_classes\n            return w\n    \n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0):\n            super().__init__()\n            self.gamma = gamma\n            self.weight = weight\n            self.label_smoothing = label_smoothing\n        def forward(self, logits, targets):\n            ce = F.cross_entropy(logits, targets, weight=self.weight, reduction='none', label_smoothing=self.label_smoothing)\n            pt = torch.softmax(logits, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n            pt = pt.clamp(1e-6, 1 - 1e-6)\n            loss = ((1 - pt) ** self.gamma) * ce\n            return loss.mean()\n    \n    # ---------------------------\n    # Build model, optimizer, criterion\n    # ---------------------------\n    model = MB_MHA_TCN_Tiny(\n        in_ch=in_channels,\n        base_ch=int(hparams['tcn_channels']),\n        d_model=int(hparams['d_model']),\n        nhead=int(hparams['num_heads']),\n        num_layers=int(hparams['num_encoder_layers']),\n        dropout=float(hparams['dropout']),\n        num_classes=num_classes,\n        use_act_quant=bool(hparams['quantize_activations']) and int(hparams['quantization_bits']) == 8,\n    ).to(device)\n    \n    # Class weights\n    if hparams['class_weighting'] == 'auto':\n        cls_w = compute_class_weights(y_train_bal, num_classes).to(device)\n    else:\n        cls_w = None\n    \n    # Criterion\n    if bool(hparams['use_focal_loss']):\n        criterion = FocalLoss(gamma=float(hparams['focal_gamma']), weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    else:\n        criterion = nn.CrossEntropyLoss(weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    \n    # Optimizer\n    if hparams['optimizer'].lower() == 'adamw':\n        optimizer = optim.AdamW(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    else:\n        optimizer = optim.Adam(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    \n    # ---------------------------\n    # Training Loop\n    # ---------------------------\n    train_losses: List[float] = []\n    val_losses: List[float] = []\n    val_accs: List[float] = []\n    \n    for epoch in range(int(hparams['epochs'])):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False).float()\n            yb = yb.to(device, non_blocking=False).long()\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if float(hparams['grad_clip_norm']) > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(hparams['grad_clip_norm']))\n            optimizer.step()\n            bs = xb.size(0)\n            running_loss += loss.detach().item() * bs\n            n_train += bs\n        train_loss_epoch = running_loss / max(1, n_train)\n        train_losses.append(train_loss_epoch)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False).float()\n                yb = yb.to(device, non_blocking=False).long()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        val_loss_epoch = val_loss / max(1, total)\n        val_acc_epoch = correct / max(1, total)\n        val_losses.append(val_loss_epoch)\n        val_accs.append(val_acc_epoch)\n        print(f\"Epoch {epoch+1}/{hparams['epochs']} - train_loss: {train_loss_epoch:.6f} - val_loss: {val_loss_epoch:.6f} - val_acc: {val_acc_epoch:.4f}\")\n    \n    # ---------------------------\n    # Post-Training Quantization (on CPU)\n    # ---------------------------\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n    quant_bits = int(hparams['quantization_bits'])\n    q_weights = bool(hparams['quantize_weights'])\n    q_acts = bool(hparams['quantize_activations'])\n    \n    quantized_model = model_cpu\n    \n    if quant_bits == 32 or not q_weights:\n        # No quantization (float32)\n        pass\n    elif quant_bits == 16:\n        # FP16 cast (weights + activations during inference if run in fp16-capable device)\n        quantized_model = model_cpu.half()\n    elif quant_bits == 8:\n        if q_acts:\n            # Static quantization with calibration\n            from torch.ao.quantization import get_default_qconfig\n            from torch.ao.quantization import prepare, convert\n            torch.backends.quantized.engine = 'fbgemm'\n            quantized_model.fuse_for_quant()\n            quantized_model.qconfig = get_default_qconfig('fbgemm')\n            prepare(quantized_model, inplace=True)\n            # Calibration on a few batches from training data (CPU)\n            calib_batches = max(1, int(hparams['calib_batches']))\n            quantized_model.eval()\n            with torch.inference_mode():\n                seen = 0\n                for xb, _ in DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=0, pin_memory=False):\n                    xb = xb.float()\n                    _ = quantized_model(xb)\n                    seen += 1\n                    if seen >= calib_batches:\n                        break\n            convert(quantized_model, inplace=True)\n        else:\n            # Dynamic quantization for Linear layers (covers MHA and FFN); convs remain FP32\n            from torch.ao.quantization import quantize_dynamic\n            quantized_model = quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)\n    else:\n        pass\n    \n    # ---------------------------\n    # Compute serialized size (KB)\n    # ---------------------------\n    with io.BytesIO() as f:\n        try:\n            torch.save(quantized_model.state_dict(), f)\n        except Exception:\n            # fallback to full model if state_dict fails\n            torch.save(quantized_model, f)\n        size_bytes = f.tell()\n    size_kb = size_bytes / 1024.0\n    \n    # Enforce size constraint by design; print warning if exceeded\n    if size_kb > 256.0:\n        print(f\"Warning: Quantized model size {size_kb:.2f} KB exceeds 256KB. Consider reducing d_model/tcn_channels.\")\n    \n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_kb': size_kb,\n        'used_hparams': hparams\n    }\n    return quantized_model, metrics\n"}
2025-09-22 19:20:06,401 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-22 19:20:06,401 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hparams):\n    import io\n    import math\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader\n    from typing import List, Tuple\n    \n    # ---------------------------\n    # Defaults & Hyperparameters\n    # ---------------------------\n    defaults = {\n        # optimization\n        'epochs': 20,\n        'batch_size': 256,\n        'lr': 3e-4,\n        'weight_decay': 1e-4,\n        'optimizer': 'adamw',  # ['adam','adamw']\n        'grad_clip_norm': 1.0,\n        # model\n        'tcn_channels': 12,            # base channels after stem\n        'num_tcn_blocks': 2,          # number of residual MB-TCN blocks\n        'dropout': 0.1,\n        'd_model': 32,\n        'num_heads': 4,\n        'num_encoder_layers': 1,      # 1-2 as recommended\n        # loss & imbalance\n        'use_focal_loss': True,\n        'focal_gamma': 2.0,\n        'label_smoothing': 0.0,\n        'class_weighting': 'auto',    # ['none','auto']\n        'use_smote_tomek': False,\n        'smote_k_neighbors': 5,\n        # quantization\n        'quantization_bits': 8,       # [8,16,32]\n        'quantize_weights': True,\n        'quantize_activations': False,\n        'calib_batches': 4,           # used if static calibration is enabled\n    }\n    # Merge provided hyperparameters\n    for k, v in defaults.items():\n        if k not in hparams:\n            hparams[k] = v\n    \n    # ---------------------------\n    # Assertions & Setup\n    # ---------------------------\n    if not isinstance(device, torch.device):\n        device = torch.device(device)\n    assert device.type == 'cuda', 'Training must run on GPU (cuda).'\n    torch.backends.cudnn.benchmark = True\n    \n    num_classes = 5\n    seq_len = 1000\n    in_channels = 2\n    \n    # Ensure heads divide d_model\n    def _fix_heads(d_model, num_heads):\n        # ensure num_heads is a divisor of d_model\n        if d_model % num_heads == 0:\n            return num_heads\n        # pick the largest divisor <= num_heads, else fallback to 1\n        divisors = [d for d in range(min(d_model, num_heads), 0, -1) if d_model % d == 0]\n        return divisors[0] if len(divisors) > 0 else 1\n    hparams['num_heads'] = _fix_heads(int(hparams['d_model']), int(hparams['num_heads']))\n    \n    # ---------------------------\n    # Optional SMOTE+Tomek on train only (before DataLoader)\n    # ---------------------------\n    def maybe_smote_tomek(Xt, yt, use_smote, k_neighbors):\n        if not use_smote:\n            return Xt, yt\n        try:\n            from imblearn.combine import SMOTETomek\n        except Exception:\n            return Xt, yt  # fallback silently if imblearn not available\n        # Expect Xt shape: (N, 2, 1000) or (N, 1000, 2)\n        Xt_cpu = Xt.detach().cpu()\n        if Xt_cpu.dim() != 3:\n            raise ValueError('Expected X tensor with 3 dims: (N, C, L) or (N, L, C)')\n        if Xt_cpu.shape[1] == seq_len and Xt_cpu.shape[2] == in_channels:\n            Xt_cpu = Xt_cpu.permute(0, 2, 1)  # (N, 2, 1000)\n        # Flatten to 2D for SMOTE\n        N = Xt_cpu.shape[0]\n        X_flat = Xt_cpu.reshape(N, -1).numpy()\n        y_np = yt.detach().cpu().numpy()\n        smt = SMOTETomek(k_neighbors=max(1, int(k_neighbors)))\n        X_res, y_res = smt.fit_resample(X_flat, y_np)\n        X_res_t = torch.from_numpy(X_res).float().reshape(-1, in_channels, seq_len)\n        y_res_t = torch.from_numpy(y_res).long()\n        return X_res_t, y_res_t\n    \n    # ---------------------------\n    # Dataset & Dataloaders\n    # ---------------------------\n    class ECGDataset(Dataset):\n        def __init__(self, X, y):\n            super().__init__()\n            self.X = X\n            self.y = y\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            # Ensure shape (C, L) for Conv1d\n            if x.dim() == 2 and x.shape[0] == seq_len and x.shape[1] == in_channels:\n                x = x.permute(1, 0)  # (2, 1000)\n            elif x.dim() == 2 and x.shape[0] == in_channels and x.shape[1] == seq_len:\n                pass\n            else:\n                raise ValueError('Each sample must have shape (1000, 2) or (2, 1000).')\n            y = self.y[idx]\n            return x, y\n    \n    # Apply optional SMOTE/Tomek to training set only (on CPU) before loaders\n    X_train_bal, y_train_bal = maybe_smote_tomek(X_train, y_train, bool(hparams['use_smote_tomek']), int(hparams['smote_k_neighbors']))\n    \n    train_ds = ECGDataset(X_train_bal, y_train_bal)\n    val_ds = ECGDataset(X_val, y_val)\n    \n    train_loader = DataLoader(train_ds, batch_size=int(hparams['batch_size']), shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=int(hparams['batch_size']), shuffle=False, num_workers=0, pin_memory=False)\n    \n    # ---------------------------\n    # Model Definition: MB-MHA-TCN Tiny\n    # ---------------------------\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model: int, max_len: int = 5000):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            # Handle odd d_model safely\n            pe[:, 0::2] = torch.sin(position * div_term)\n            odd_dim = d_model // 2\n            pe[:, 1::2] = torch.cos(position * div_term[:odd_dim])\n            pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n            self.register_buffer('pe', pe)\n        def forward(self, x):  # x: (B, L, d_model)\n            L = x.size(1)\n            return x + self.pe[:, :L, :]\n    \n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, channels: int, kernel_size: int, dilation: int, dropout: float):\n            super().__init__()\n            padding = (kernel_size - 1) // 2 * dilation\n            self.dw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size, padding=padding, dilation=dilation, groups=channels, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.pw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.do = nn.Dropout(p=dropout)\n        def forward(self, x):\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.do(x)\n            return x\n    \n    class MBTCNBlock(nn.Module):\n        def __init__(self, channels: int, dropout: float):\n            super().__init__()\n            ks = [7, 15, 31]\n            dil = [1, 2, 4]\n            self.branches = nn.ModuleList([\n                DepthwiseSeparableConv1d(channels, ks[0], dil[0], dropout),\n                DepthwiseSeparableConv1d(channels, ks[1], dil[1], dropout),\n                DepthwiseSeparableConv1d(channels, ks[2], dil[2], dropout),\n            ])\n            self.merge = nn.Sequential(\n                nn.Conv1d(channels * 3, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n            )\n            self.act = nn.ReLU(inplace=True)\n        def forward(self, x):\n            outs = [b(x) for b in self.branches]\n            y = torch.cat(outs, dim=1)\n            y = self.merge(y)\n            y = y + x  # residual\n            return self.act(y)\n    \n    class TransformerEncoderBlock(nn.Module):\n        def __init__(self, d_model: int, nhead: int, dropout: float):\n            super().__init__()\n            self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n            self.ln1 = nn.LayerNorm(d_model)\n            self.ffn = nn.Sequential(\n                nn.Linear(d_model, d_model * 2),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(d_model * 2, d_model),\n            )\n            self.ln2 = nn.LayerNorm(d_model)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):  # x: (B, L, D)\n            attn_out, _ = self.mha(x, x, x, need_weights=False)\n            x = self.ln1(x + self.drop(attn_out))\n            x = self.ln2(x + self.drop(self.ffn(x)))\n            return x\n    \n    class MB_MHA_TCN_Tiny(nn.Module):\n        def __init__(self, in_ch: int, base_ch: int, d_model: int, nhead: int, num_layers: int, dropout: float, num_classes: int, use_act_quant: bool = False):\n            super().__init__()\n            self.use_act_quant = use_act_quant\n            if use_act_quant:\n                from torch.ao.quantization import QuantStub, DeQuantStub\n                self.quant = QuantStub()\n                self.dequant = DeQuantStub()\n            else:\n                self.quant = None\n                self.dequant = None\n            # Stem\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=7, padding=3, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.ReLU(inplace=True),\n            )\n            # MB-TCN blocks\n            self.tcn_blocks = nn.Sequential(*[MBTCNBlock(base_ch, dropout) for _ in range(int(hparams['num_tcn_blocks']))])\n            # Projection to d_model\n            self.proj = nn.Sequential(\n                nn.Conv1d(base_ch, d_model, kernel_size=1, bias=False),\n                nn.BatchNorm1d(d_model),\n                nn.ReLU(inplace=True),\n            )\n            # Transformer encoder(s)\n            self.posenc = PositionalEncoding(d_model, max_len=seq_len)\n            self.encoder = nn.Sequential(*[TransformerEncoderBlock(d_model, nhead, dropout) for _ in range(int(hparams['num_encoder_layers']))])\n            # Head\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Linear(d_model, num_classes)\n        def forward(self, x):  # x: (B, C, L)\n            if self.quant is not None:\n                x = self.quant(x)\n            x = self.stem(x)\n            x = self.tcn_blocks(x)\n            x = self.proj(x)            # (B, D, L)\n            x = x.transpose(1, 2)       # (B, L, D)\n            x = self.posenc(x)\n            x = self.encoder(x)\n            x = x.transpose(1, 2)       # (B, D, L)\n            x = self.pool(x).squeeze(-1)  # (B, D)\n            x = self.fc(x)              # (B, num_classes)\n            if self.dequant is not None:\n                x = self.dequant(x)\n            return x\n        def fuse_for_quant(self):\n            # Try to fuse typical conv-bn-relu patterns\n            for name, module in self.named_modules():\n                pass  # kept simple; prepare/convert will still quantize supported ops\n    \n    # ---------------------------\n    # Loss functions\n    # ---------------------------\n    def compute_class_weights(y: torch.Tensor, num_classes: int) -> torch.Tensor:\n        with torch.no_grad():\n            counts = torch.bincount(y.view(-1).cpu(), minlength=num_classes).float()\n            counts[counts == 0] = 1.0\n            inv = 1.0 / counts\n            w = inv / inv.sum() * num_classes\n            return w\n    \n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0):\n            super().__init__()\n            self.gamma = gamma\n            self.weight = weight\n            self.label_smoothing = label_smoothing\n        def forward(self, logits, targets):\n            ce = F.cross_entropy(logits, targets, weight=self.weight, reduction='none', label_smoothing=self.label_smoothing)\n            pt = torch.softmax(logits, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n            pt = pt.clamp(1e-6, 1 - 1e-6)\n            loss = ((1 - pt) ** self.gamma) * ce\n            return loss.mean()\n    \n    # ---------------------------\n    # Build model, optimizer, criterion\n    # ---------------------------\n    model = MB_MHA_TCN_Tiny(\n        in_ch=in_channels,\n        base_ch=int(hparams['tcn_channels']),\n        d_model=int(hparams['d_model']),\n        nhead=int(hparams['num_heads']),\n        num_layers=int(hparams['num_encoder_layers']),\n        dropout=float(hparams['dropout']),\n        num_classes=num_classes,\n        use_act_quant=bool(hparams['quantize_activations']) and int(hparams['quantization_bits']) == 8,\n    ).to(device)\n    \n    # Class weights\n    if hparams['class_weighting'] == 'auto':\n        cls_w = compute_class_weights(y_train_bal, num_classes).to(device)\n    else:\n        cls_w = None\n    \n    # Criterion\n    if bool(hparams['use_focal_loss']):\n        criterion = FocalLoss(gamma=float(hparams['focal_gamma']), weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    else:\n        criterion = nn.CrossEntropyLoss(weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    \n    # Optimizer\n    if hparams['optimizer'].lower() == 'adamw':\n        optimizer = optim.AdamW(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    else:\n        optimizer = optim.Adam(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    \n    # ---------------------------\n    # Training Loop\n    # ---------------------------\n    train_losses: List[float] = []\n    val_losses: List[float] = []\n    val_accs: List[float] = []\n    \n    for epoch in range(int(hparams['epochs'])):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False).float()\n            yb = yb.to(device, non_blocking=False).long()\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if float(hparams['grad_clip_norm']) > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(hparams['grad_clip_norm']))\n            optimizer.step()\n            bs = xb.size(0)\n            running_loss += loss.detach().item() * bs\n            n_train += bs\n        train_loss_epoch = running_loss / max(1, n_train)\n        train_losses.append(train_loss_epoch)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False).float()\n                yb = yb.to(device, non_blocking=False).long()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        val_loss_epoch = val_loss / max(1, total)\n        val_acc_epoch = correct / max(1, total)\n        val_losses.append(val_loss_epoch)\n        val_accs.append(val_acc_epoch)\n        print(f\"Epoch {epoch+1}/{hparams['epochs']} - train_loss: {train_loss_epoch:.6f} - val_loss: {val_loss_epoch:.6f} - val_acc: {val_acc_epoch:.4f}\")\n    \n    # ---------------------------\n    # Post-Training Quantization (on CPU)\n    # ---------------------------\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n    quant_bits = int(hparams['quantization_bits'])\n    q_weights = bool(hparams['quantize_weights'])\n    q_acts = bool(hparams['quantize_activations'])\n    \n    quantized_model = model_cpu\n    \n    if quant_bits == 32 or not q_weights:\n        # No quantization (float32)\n        pass\n    elif quant_bits == 16:\n        # FP16 cast (weights + activations during inference if run in fp16-capable device)\n        quantized_model = model_cpu.half()\n    elif quant_bits == 8:\n        if q_acts:\n            # Static quantization with calibration\n            from torch.ao.quantization import get_default_qconfig\n            from torch.ao.quantization import prepare, convert\n            torch.backends.quantized.engine = 'fbgemm'\n            quantized_model.fuse_for_quant()\n            quantized_model.qconfig = get_default_qconfig('fbgemm')\n            prepare(quantized_model, inplace=True)\n            # Calibration on a few batches from training data (CPU)\n            calib_batches = max(1, int(hparams['calib_batches']))\n            quantized_model.eval()\n            with torch.inference_mode():\n                seen = 0\n                for xb, _ in DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=0, pin_memory=False):\n                    xb = xb.float()\n                    _ = quantized_model(xb)\n                    seen += 1\n                    if seen >= calib_batches:\n                        break\n            convert(quantized_model, inplace=True)\n        else:\n            # Dynamic quantization for Linear layers (covers MHA and FFN); convs remain FP32\n            from torch.ao.quantization import quantize_dynamic\n            quantized_model = quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)\n    else:\n        pass\n    \n    # ---------------------------\n    # Compute serialized size (KB)\n    # ---------------------------\n    with io.BytesIO() as f:\n        try:\n            torch.save(quantized_model.state_dict(), f)\n        except Exception:\n            # fallback to full model if state_dict fails\n            torch.save(quantized_model, f)\n        size_bytes = f.tell()\n    size_kb = size_bytes / 1024.0\n    \n    # Enforce size constraint by design; print warning if exceeded\n    if size_kb > 256.0:\n        print(f\"Warning: Quantized model size {size_kb:.2f} KB exceeds 256KB. Consider reducing d_model/tcn_channels.\")\n    \n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_kb': size_kb,\n        'used_hparams': hparams\n    }\n    return quantized_model, metrics\n"}
2025-09-22 19:20:06,401 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-22 19:20:06,401 - ERROR - _models.training_function_executor - BO training objective failed: The expanded size of the tensor (25) must match the existing size (26) at non-singleton dimension 1.  Target sizes: [1000, 25].  Tensor sizes: [1000, 26]
2025-09-22 19:20:06,401 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 90.421s
2025-09-22 19:20:06,402 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: The expanded size of the tensor (25) must match the existing size (26) at non-singleton dimension 1.  Target sizes: [1000, 25].  Tensor sizes: [1000, 26]
2025-09-22 19:20:06,402 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-22 19:20:09,404 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-22 19:20:09,404 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß GPT has fixed the training function, reloading from JSON file
2025-09-22 19:20:09,404 - INFO - evaluation.code_generation_pipeline_orchestrator - Reloading fixed training function from: generated_training_functions/training_function_torch_tensor_MB-MHA-TCN-TinyQuant_1758586549.json
2025-09-22 19:20:09,404 - INFO - _models.training_function_executor - Loaded training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:20:09,405 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-22 19:20:09,405 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:20:09,405 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-22 19:20:09,405 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 2/4
2025-09-22 19:20:09,405 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 2: üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 19:20:09,405 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 19:20:09,407 - INFO - package_installer - Extracted imports from code: {'torch', 'imblearn'}
2025-09-22 19:20:09,407 - INFO - package_installer - Available packages: {'torch', 'imblearn'}
2025-09-22 19:20:09,407 - INFO - package_installer - Missing packages: set()
2025-09-22 19:20:09,407 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-22 19:20:09,407 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 19:20:09,407 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 19:20:09,407 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 19:20:09,407 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['epochs', 'batch_size', 'lr', 'weight_decay', 'optimizer', 'grad_clip_norm', 'tcn_channels', 'num_tcn_blocks', 'dropout', 'd_model', 'num_heads', 'num_encoder_layers', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'class_weighting', 'use_smote_tomek', 'smote_k_neighbors', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calib_batches']
2025-09-22 19:20:09,408 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 19:20:09,408 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 19:20:09,408 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 19:20:09,442 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 19:20:09,475 - INFO - bo.run_bo - Converted GPT search space: 22 parameters
2025-09-22 19:20:09,475 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 19:20:09,476 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 19:20:09,477 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 19:20:09,477 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 19:20:09,477 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 19:20:09,477 - INFO - _models.training_function_executor - Executing training function: MB-MHA-TCN-TinyQuant
2025-09-22 19:20:09,477 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 184, 'batch_size': 512, 'lr': 0.0008471801418819981, 'weight_decay': 3.90796715682288e-05, 'optimizer': 'adam', 'grad_clip_norm': 0.7799726016810133, 'tcn_channels': 18, 'num_tcn_blocks': 3, 'dropout': 0.4330880728874677, 'd_model': 51, 'num_heads': 4, 'num_encoder_layers': 2, 'use_focal_loss': True, 'focal_gamma': 3.8879950890672994, 'label_smoothing': 0.18771054180315008, 'class_weighting': 'none', 'use_smote_tomek': False, 'smote_k_neighbors': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 17}
2025-09-22 19:20:09,478 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 184, 'batch_size': 512, 'lr': 0.0008471801418819981, 'weight_decay': 3.90796715682288e-05, 'optimizer': 'adam', 'grad_clip_norm': 0.7799726016810133, 'tcn_channels': 18, 'num_tcn_blocks': 3, 'dropout': 0.4330880728874677, 'd_model': 51, 'num_heads': 4, 'num_encoder_layers': 2, 'use_focal_loss': True, 'focal_gamma': 3.8879950890672994, 'label_smoothing': 0.18771054180315008, 'class_weighting': 'none', 'use_smote_tomek': False, 'smote_k_neighbors': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 17}
2025-09-22 19:20:09,480 - ERROR - _models.training_function_executor - Training execution failed: The expanded size of the tensor (25) must match the existing size (26) at non-singleton dimension 1.  Target sizes: [1000, 25].  Tensor sizes: [1000, 26]
2025-09-22 19:20:09,481 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 19:20:09,481 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 19:20:09,481 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 19:20:09,481 - INFO - _models.ai_code_generator - Prompt length: 21215 characters
2025-09-22 19:20:09,481 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 19:20:09,481 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 19:20:09,481 - INFO - _models.ai_code_generator - Model parameter: gpt-5
