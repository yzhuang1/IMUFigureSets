2025-10-03 00:28:52,079 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-03 00:28:52,186 - INFO - __main__ - Logging system initialized successfully
2025-10-03 00:28:52,186 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-10-03 00:28:52,186 - INFO - __main__ - Starting real data processing from data/dataset1/ directory
2025-10-03 00:28:52,186 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-10-03 00:28:52,186 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-10-03 00:28:52,186 - INFO - __main__ - Attempting to load: X.npy
2025-10-03 00:28:52,228 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-10-03 00:28:52,267 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-10-03 00:28:52,267 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-10-03 00:28:52,267 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-10-03 00:28:52,267 - INFO - __main__ - Flow: Code Generation â†’ BO â†’ Evaluation
2025-10-03 00:28:52,269 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-03 00:28:52,269 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-10-03 00:28:52,270 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-10-03 00:28:52,270 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-10-03 00:28:52,270 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation â†’ JSON Storage â†’ BO â†’ Training Execution â†’ Evaluation
2025-10-03 00:28:52,270 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-10-03 00:28:52,270 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-10-03 00:28:52,270 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-10-03 00:28:52,270 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-10-03 00:28:52,364 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-10-03 00:28:52,364 - INFO - class_balancing - Class imbalance analysis:
2025-10-03 00:28:52,364 - INFO - class_balancing -   Strategy: severe_imbalance
2025-10-03 00:28:52,364 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-10-03 00:28:52,364 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-10-03 00:28:52,364 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-10-03 00:28:52,364 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-10-03 00:28:52,365 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-10-03 00:28:52,365 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-10-03 00:28:52,365 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-10-03 00:28:52,535 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-10-03 00:28:52,536 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-10-03 00:28:52,536 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-10-03 00:28:52,536 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-10-03 00:28:52,536 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-10-03 00:28:52,536 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ¤– STEP 1: AI Training Code Generation
2025-10-03 00:28:52,536 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-10-03 00:32:21,196 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-03 00:32:21,260 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-03 00:32:21,261 - INFO - _models.ai_code_generator - Prompt length: 4327 characters
2025-10-03 00:32:21,261 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-03 00:32:21,261 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-03 00:32:21,261 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-03 00:36:34,046 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-03 00:36:34,047 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-03 00:36:34,047 - INFO - _models.ai_code_generator - AI generated training function: CATNet1D-TransTiny
2025-10-03 00:36:34,047 - INFO - _models.ai_code_generator - Confidence: 0.83
2025-10-03 00:36:34,047 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-10-03 00:36:34,047 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: CATNet1D-TransTiny
2025-10-03 00:36:34,047 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'base_channels', 'ds_kernel_size', 'n_ds_blocks', 'd_model', 'head_dim', 'num_transformer_layers', 'ff_multiplier', 'dropout', 'weight_decay', 'gamma_focal', 'effective_beta', 'label_smoothing', 'use_amp', 'use_smote_tomek', 'smote_target_min_frac', 'smote_max_multiplier', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'quant_calibrate_batches']
2025-10-03 00:36:34,047 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.83
2025-10-03 00:36:34,047 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ’¾ STEP 2: Save Training Function to JSON
2025-10-03 00:36:34,048 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 00:36:34,048 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 00:36:34,048 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ” STEP 3: Bayesian Optimization
2025-10-03 00:36:34,048 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: CATNet1D-TransTiny
2025-10-03 00:36:34,048 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“¦ Installing dependencies for GPT-generated training code...
2025-10-03 00:36:34,048 - INFO - package_installer - ðŸ” Analyzing GPT-generated code for package dependencies...
2025-10-03 00:36:34,051 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-03 00:36:34,051 - INFO - package_installer - Available packages: {'torch'}
2025-10-03 00:36:34,051 - INFO - package_installer - Missing packages: set()
2025-10-03 00:36:34,051 - INFO - package_installer - âœ… All required packages are already available
2025-10-03 00:36:34,051 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-10-03 00:36:34,051 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 00:36:34,051 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-03 00:36:34,051 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'base_channels', 'ds_kernel_size', 'n_ds_blocks', 'd_model', 'head_dim', 'num_transformer_layers', 'ff_multiplier', 'dropout', 'weight_decay', 'gamma_focal', 'effective_beta', 'label_smoothing', 'use_amp', 'use_smote_tomek', 'smote_target_min_frac', 'smote_max_multiplier', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'quant_calibrate_batches']
2025-10-03 00:36:34,051 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-03 00:36:34,051 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 00:36:34,051 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-03 00:36:34,084 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-03 00:36:34,214 - INFO - bo.run_bo - Converted GPT search space: 23 parameters
2025-10-03 00:36:34,214 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-03 00:36:34,214 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-03 00:36:34,215 - INFO - bo.run_bo - ðŸ”BO Trial 1: Initial random exploration
2025-10-03 00:36:34,215 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:36:34,215 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:36:34,215 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 00:36:34,215 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 00:36:34,217 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 00:44:15,515 - INFO - _models.training_function_executor - Model: 17,461 parameters, 75.0KB storage
2025-10-03 00:44:15,516 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2409315660905365, 0.09637767903138239, 0.07715992797983204, 0.06479983210850554, 0.055971490020996696, 0.049124711367981674, 0.04424280563682772, 0.04209567801996042, 0.03882023178177829, 0.03437906376035682, 0.03321931353718922, 0.031733153876337294, 0.03080341928698823, 0.027176575513701524, 0.02739264388734072, 0.02609120358048572, 0.025390435676374144, 0.025427775475996745, 0.02375972067291134, 0.022035008136685436, 0.02028641551594953, 0.022573680929425594, 0.019955667992930057, 0.018123591581112816, 0.01910984960714001, 0.01966602673222594, 0.01870826919560553, 0.017079778057466848, 0.017358550263923817, 0.01664211292989098, 0.016679813249557578, 0.014931936696920951, 0.015915323415089753, 0.01641791090503381, 0.015587530884956048, 0.014680658460341032, 0.014733396644993327, 0.015257289290426771, 0.013985421752343897, 0.01446280203336622, 0.012694940744517975, 0.012773995265791168, 0.014270398851245618, 0.011245401254316518, 0.014941032035773301, 0.01206447330889306, 0.012520799387707799, 0.0139583647845498, 0.011654353826083817, 0.012002322512122335, 0.012244189936731242, 0.010932922985349678, 0.011947562955578642, 0.011683142836351149, 0.010810974332586508, 0.011605505001007946, 0.009576919538114247, 0.012511147037806666, 0.011150884329452975, 0.010822020808787847, 0.010570442536574451, 0.010272914028136187, 0.009372154055584836, 0.010614197305651394, 0.010330803989720645, 0.00942966580887786, 0.009842944398740673, 0.009632613171829858, 0.010449024486374543, 0.009625510842078333, 0.009332367012673289, 0.00973687046886464, 0.008823662032646121, 0.009975830086340055, 0.009939389455565785, 0.008157635936078728], 'val_losses': [0.10542460417944662, 0.06069220316755848, 0.0600963635576681, 0.042518275105748585, 0.0618363604306159, 0.03662216953504744, 0.0338165495820533, 0.03519145491845834, 0.030459322874564872, 0.03362286291375165, 0.03677133129378217, 0.030177784399600303, 0.03040169605600432, 0.03445949246729993, 0.028730711541801988, 0.03399163373668844, 0.026033668657126485, 0.03151189120337713, 0.031002379137013973, 0.033116266981855755, 0.02770542097508407, 0.025440756602628446, 0.032085107165893925, 0.029877921530192485, 0.028219543297537855, 0.038024956659892195, 0.027920843889361977, 0.029761349042284058, 0.026934377140545362, 0.024576847985631436, 0.024596853823294962, 0.023629137854792096, 0.029327942460099492, 0.026419572025731666, 0.02350431184549631, 0.02428190996400128, 0.025022081447310832, 0.027940112958798627, 0.028133172333210634, 0.038315430859540464, 0.02596187798331785, 0.03296183187842683, 0.02365503688045887, 0.027936657313983284, 0.026684134238914296, 0.027398411713749668, 0.023611420568308873, 0.02596763975319894, 0.025337201032856176, 0.024994413158474164, 0.02507059902882549, 0.027573591972215927, 0.02877528643401968, 0.025416554135590505, 0.02718905839439383, 0.03392776962644608, 0.03038895425296057, 0.026442813092478385, 0.025726844792231748, 0.024780719576137672, 0.024910401724220113, 0.028947577419107442, 0.02772530918020665, 0.028680363168914043, 0.023663978515615993, 0.02968950381183005, 0.029037304773372213, 0.02897477671672058, 0.032557780683216124, 0.026464806723293693, 0.025712462792787064, 0.026250513515318118, 0.029194945732056683, 0.03869091308351418, 0.023715683497005858, 0.030528979588805502], 'val_acc': [0.8892369377271019, 0.9423631123919308, 0.9418619220649042, 0.953765192331788, 0.9275779977446436, 0.9610324520736749, 0.9609071544919183, 0.9650419746898885, 0.9661696529256986, 0.9631625109635384, 0.9586517980202982, 0.9680491166520486, 0.9641648916175918, 0.9528881092594913, 0.9644154867811051, 0.9605312617466483, 0.9700538779601554, 0.9587770956020549, 0.9579000125297582, 0.9602806665831349, 0.9693020924696153, 0.9746898884851523, 0.960656559328405, 0.9656684625986719, 0.9750657812304222, 0.9557699536398947, 0.9705550682871821, 0.9677985214885353, 0.9749404836486656, 0.9758175667209623, 0.9758175667209623, 0.9741886981581256, 0.9681744142338052, 0.976318757047989, 0.9744392933216389, 0.9760681618844756, 0.9758175667209623, 0.9711815561959655, 0.9718080441047487, 0.9523869189324646, 0.9735622102493422, 0.9654178674351584, 0.9745645909033955, 0.9751910788121789, 0.977822328029069, 0.9739381029946123, 0.9761934594662323, 0.9740634005763689, 0.976318757047989, 0.9783235183560958, 0.9764440546297456, 0.9780729231925824, 0.9768199473750157, 0.978198220774339, 0.9786994111013657, 0.9615336424007017, 0.9734369126675855, 0.9743139957398822, 0.9756922691392056, 0.9738128054128555, 0.9786994111013657, 0.9703044731236687, 0.9696779852148854, 0.9731863175040721, 0.9786994111013657, 0.9768199473750157, 0.977822328029069, 0.9768199473750157, 0.9711815561959655, 0.9756922691392056, 0.9793258990101491, 0.9771958401202857, 0.9725598295952889, 0.9579000125297582, 0.9783235183560958, 0.9776970304473124], 'final_model_size_bytes': 214480, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}, 'model_parameter_count': 17461, 'model_storage_size_kb': 75.02773437500001, 'model_size_validation': 'PASS'}
2025-10-03 00:44:15,516 - INFO - _models.training_function_executor - BO Objective: base=0.9777, size_penalty=0.0000, final=0.9777
2025-10-03 00:44:15,516 - INFO - _models.training_function_executor - Model: 17,461 parameters, 75.0KB (PASS 256KB limit)
2025-10-03 00:44:15,516 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 461.301s
2025-10-03 00:44:15,516 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9777
2025-10-03 00:44:15,516 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-03 00:44:15,516 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': np.int64(76), 'base_channels': np.int64(10), 'ds_kernel_size': 5, 'n_ds_blocks': np.int64(1), 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': np.int64(3), 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': np.int64(8)}, value=0.9777
2025-10-03 00:44:15,516 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': np.int64(76), 'base_channels': np.int64(10), 'ds_kernel_size': 5, 'n_ds_blocks': np.int64(1), 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': np.int64(3), 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': np.int64(8)} -> 0.9777
2025-10-03 00:44:15,517 - INFO - bo.run_bo - ðŸ”BO Trial 2: Initial random exploration
2025-10-03 00:44:15,517 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:44:15,517 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:44:15,517 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 00:44:15,517 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 7.52374288453486e-05, 'batch_size': 128, 'epochs': 66, 'base_channels': 8, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.08526206184364578, 'weight_decay': 1.8205657658407253e-06, 'gamma_focal': 2.897771074506667, 'effective_beta': 0.9964666401041485, 'label_smoothing': 0.08083973481164614, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.28948155927925495, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 3}
2025-10-03 00:44:15,518 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 7.52374288453486e-05, 'batch_size': 128, 'epochs': 66, 'base_channels': 8, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.08526206184364578, 'weight_decay': 1.8205657658407253e-06, 'gamma_focal': 2.897771074506667, 'effective_beta': 0.9964666401041485, 'label_smoothing': 0.08083973481164614, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.28948155927925495, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 3}
2025-10-03 00:58:47,881 - ERROR - _models.training_function_executor - Training execution failed: Quantized model still exceeds 256KB (size=411613 bytes). Reduce model width/depth.
2025-10-03 00:58:47,882 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-03 00:58:47,882 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-03 00:58:47,882 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-03 00:58:47,882 - INFO - _models.ai_code_generator - Prompt length: 20043 characters
2025-10-03 00:58:47,882 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-03 00:58:47,882 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-03 00:58:47,882 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-03 01:00:44,253 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-03 01:00:44,254 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-03 01:00:44,254 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251003_010044_attempt1.txt
2025-10-03 01:00:44,254 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code":"def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Hyperparameters\n    lr=0.001,\n    batch_size=128,\n    epochs=20,\n    base_channels=8,\n    ds_kernel_size=7,\n    n_ds_blocks=2,\n    d_model=64,\n    head_dim=16,\n    num_transformer_layers=1,\n    ff_multiplier=2,\n    dropout=0.1,\n    weight_decay=1e-4,\n    gamma_focal=2.0,\n    effective_beta=0.999,\n    label_smoothing=0.0,\n    use_amp=True,\n    # SMOTE-Tomek like oversampling (offline)\n    use_smote_tomek=True,\n    smote_target_min_frac=0.20,\n    smote_max_multiplier=1,\n    # Quantization params\n    quantization_bits=8,            # {8,16,32}\n    quantize_weights=True,\n    quantize_activations=False,\n    quant_calibrate_batches=2,\n):\n    \"\"\"\n    Train CATNet1D-TransTiny on GPU and return a quantized model and training metrics.\n    X_*: torch tensors. Shapes accepted: (N, 1000, 2) or (N, 2, 1000). Targets as (N,) longs.\n    \"\"\"\n    # Robust device handling\n    device = torch.device(device)\n\n    # Ensure correct input layout (N, C, L) with C=2, L=1000\n    def to_NCL(x):\n        if x.dim() != 3:\n            raise ValueError('Input X must be 3D: (N, 1000, 2) or (N, 2, 1000)')\n        if x.shape[1] == 2:  # (N, 2, L)\n            return x\n        elif x.shape[2] == 2:  # (N, L, 2) -> (N, 2, L)\n            return x.transpose(1, 2).contiguous()\n        else:\n            raise ValueError('Expected one dimension to be 2 (channels)')\n\n    X_train = to_NCL(X_train)\n    X_val = to_NCL(X_val)\n    seq_len = X_train.shape[-1]\n    in_ch = X_train.shape[1]\n\n    # Optional offline SMOTE-like oversampling (lightweight)\n    if use_smote_tomek:\n        X_train_cpu = X_train.detach().cpu()\n        y_train_cpu = y_train.detach().cpu().long()\n        X_train_cpu, y_train_cpu = smote_oversample_tensor(\n            X_train_cpu, y_train_cpu, target_min_frac=smote_target_min_frac, max_multiplier=smote_max_multiplier\n        )\n        X_train = X_train_cpu\n        y_train = y_train_cpu\n\n    # Datasets & Loaders (pin_memory=False as required)\n    train_ds = TensorDataset(X_train, y_train.long())\n    val_ds = TensorDataset(X_val, y_val.long())\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CATNet1DTransTiny(\n        in_ch=in_ch,\n        seq_len=seq_len,\n        base_ch=base_channels,\n        ds_kernel=ds_kernel_size,\n        n_ds_blocks=n_ds_blocks,\n        d_model=d_model,\n        head_dim=head_dim,\n        n_layers=num_transformer_layers,\n        ff_multiplier=ff_multiplier,\n        dropout=dropout,\n        n_classes=5,\n    ).to(device)\n\n    # Ensure positional encoding buffer is not saved in state_dict to keep model size small\n    try:\n        if hasattr(model, 'pos') and hasattr(model.pos, '_buffers') and 'pe' in model.pos._buffers:\n            if hasattr(model.pos, '_non_persistent_buffers_set'):\n                model.pos._non_persistent_buffers_set.add('pe')\n    except Exception:\n        pass\n\n    # Optimizer & loss\n    class_counts = {int(c.item()): int((y_train == c).sum().item()) for c in torch.unique(y_train)}\n    criterion = FocalLossWithEN(class_counts, num_classes=5, gamma=gamma_focal, beta=effective_beta, label_smoothing=label_smoothing, device=device).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == 'cuda')\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp and device.type == 'cuda'):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            running_loss += loss.detach().item() * xb.size(0)\n            n_train += xb.size(0)\n        train_loss = running_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n        val_loss = val_running_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # ---------------------\n    # Post-training quantization\n    # ---------------------\n    model_cpu = model.to('cpu').eval()\n\n    def measure_size_bytes(state_dict):\n        buf = io.BytesIO()\n        torch.save(state_dict, buf)\n        return buf.getbuffer().nbytes\n\n    def apply_quantization(model_fp):\n        # If user requests no quantization (32-bit), just return model\n        if not quantize_weights and not quantize_activations:\n            return model_fp\n        # 16-bit: lightweight FP16 cast (mainly effective on GPU)\n        if quantization_bits == 16:\n            model_half = model_fp.half()\n            return model_half\n        # 8-bit dynamic quantization for Linear and MHA weights\n        if quantization_bits == 8 and quantize_weights:\n            qdq_types = {nn.Linear}\n            if hasattr(nn, 'MultiheadAttention'):\n                qdq_types.add(nn.MultiheadAttention)\n            q_model = torch.ao.quantization.quantize_dynamic(\n                model_fp, {t for t in qdq_types}, dtype=torch.qint8\n            )\n            return q_model\n        # Default fallback: return original\n        return model_fp\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Enforce final size <= 256KB by forcing int8 dynamic if needed\n    size_bytes = measure_size_bytes(quantized_model.state_dict())\n    limit = 256 * 1024\n    if size_bytes > limit:\n        print(f\"Warning: quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Forcing int8 dynamic quantization.\")\n        quantized_model = torch.ao.quantization.quantize_dynamic(quantized_model, {nn.Linear, getattr(nn, 'MultiheadAttention', nn.Linear)}, dtype=torch.qint8)\n        size_bytes = measure_size_bytes(quantized_model.state_dict())\n        if size_bytes > limit:\n            raise RuntimeError(f\"Quantized model still exceeds 256KB (size={size_bytes} bytes). Reduce model width/depth.\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_bytes': size_bytes,\n    }\n\n    return quantized_model, metrics\n"}
2025-10-03 01:00:44,254 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-03 01:00:44,254 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code":"def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Hyperparameters\n    lr=0.001,\n    batch_size=128,\n    epochs=20,\n    base_channels=8,\n    ds_kernel_size=7,\n    n_ds_blocks=2,\n    d_model=64,\n    head_dim=16,\n    num_transformer_layers=1,\n    ff_multiplier=2,\n    dropout=0.1,\n    weight_decay=1e-4,\n    gamma_focal=2.0,\n    effective_beta=0.999,\n    label_smoothing=0.0,\n    use_amp=True,\n    # SMOTE-Tomek like oversampling (offline)\n    use_smote_tomek=True,\n    smote_target_min_frac=0.20,\n    smote_max_multiplier=1,\n    # Quantization params\n    quantization_bits=8,            # {8,16,32}\n    quantize_weights=True,\n    quantize_activations=False,\n    quant_calibrate_batches=2,\n):\n    \"\"\"\n    Train CATNet1D-TransTiny on GPU and return a quantized model and training metrics.\n    X_*: torch tensors. Shapes accepted: (N, 1000, 2) or (N, 2, 1000). Targets as (N,) longs.\n    \"\"\"\n    # Robust device handling\n    device = torch.device(device)\n\n    # Ensure correct input layout (N, C, L) with C=2, L=1000\n    def to_NCL(x):\n        if x.dim() != 3:\n            raise ValueError('Input X must be 3D: (N, 1000, 2) or (N, 2, 1000)')\n        if x.shape[1] == 2:  # (N, 2, L)\n            return x\n        elif x.shape[2] == 2:  # (N, L, 2) -> (N, 2, L)\n            return x.transpose(1, 2).contiguous()\n        else:\n            raise ValueError('Expected one dimension to be 2 (channels)')\n\n    X_train = to_NCL(X_train)\n    X_val = to_NCL(X_val)\n    seq_len = X_train.shape[-1]\n    in_ch = X_train.shape[1]\n\n    # Optional offline SMOTE-like oversampling (lightweight)\n    if use_smote_tomek:\n        X_train_cpu = X_train.detach().cpu()\n        y_train_cpu = y_train.detach().cpu().long()\n        X_train_cpu, y_train_cpu = smote_oversample_tensor(\n            X_train_cpu, y_train_cpu, target_min_frac=smote_target_min_frac, max_multiplier=smote_max_multiplier\n        )\n        X_train = X_train_cpu\n        y_train = y_train_cpu\n\n    # Datasets & Loaders (pin_memory=False as required)\n    train_ds = TensorDataset(X_train, y_train.long())\n    val_ds = TensorDataset(X_val, y_val.long())\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CATNet1DTransTiny(\n        in_ch=in_ch,\n        seq_len=seq_len,\n        base_ch=base_channels,\n        ds_kernel=ds_kernel_size,\n        n_ds_blocks=n_ds_blocks,\n        d_model=d_model,\n        head_dim=head_dim,\n        n_layers=num_transformer_layers,\n        ff_multiplier=ff_multiplier,\n        dropout=dropout,\n        n_classes=5,\n    ).to(device)\n\n    # Ensure positional encoding buffer is not saved in state_dict to keep model size small\n    try:\n        if hasattr(model, 'pos') and hasattr(model.pos, '_buffers') and 'pe' in model.pos._buffers:\n            if hasattr(model.pos, '_non_persistent_buffers_set'):\n                model.pos._non_persistent_buffers_set.add('pe')\n    except Exception:\n        pass\n\n    # Optimizer & loss\n    class_counts = {int(c.item()): int((y_train == c).sum().item()) for c in torch.unique(y_train)}\n    criterion = FocalLossWithEN(class_counts, num_classes=5, gamma=gamma_focal, beta=effective_beta, label_smoothing=label_smoothing, device=device).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == 'cuda')\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp and device.type == 'cuda'):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            running_loss += loss.detach().item() * xb.size(0)\n            n_train += xb.size(0)\n        train_loss = running_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n        val_loss = val_running_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # ---------------------\n    # Post-training quantization\n    # ---------------------\n    model_cpu = model.to('cpu').eval()\n\n    def measure_size_bytes(state_dict):\n        buf = io.BytesIO()\n        torch.save(state_dict, buf)\n        return buf.getbuffer().nbytes\n\n    def apply_quantization(model_fp):\n        # If user requests no quantization (32-bit), just return model\n        if not quantize_weights and not quantize_activations:\n            return model_fp\n        # 16-bit: lightweight FP16 cast (mainly effective on GPU)\n        if quantization_bits == 16:\n            model_half = model_fp.half()\n            return model_half\n        # 8-bit dynamic quantization for Linear and MHA weights\n        if quantization_bits == 8 and quantize_weights:\n            qdq_types = {nn.Linear}\n            if hasattr(nn, 'MultiheadAttention'):\n                qdq_types.add(nn.MultiheadAttention)\n            q_model = torch.ao.quantization.quantize_dynamic(\n                model_fp, {t for t in qdq_types}, dtype=torch.qint8\n            )\n            return q_model\n        # Default fallback: return original\n        return model_fp\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Enforce final size <= 256KB by forcing int8 dynamic if needed\n    size_bytes = measure_size_bytes(quantized_model.state_dict())\n    limit = 256 * 1024\n    if size_bytes > limit:\n        print(f\"Warning: quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Forcing int8 dynamic quantization.\")\n        quantized_model = torch.ao.quantization.quantize_dynamic(quantized_model, {nn.Linear, getattr(nn, 'MultiheadAttention', nn.Linear)}, dtype=torch.qint8)\n        size_bytes = measure_size_bytes(quantized_model.state_dict())\n        if size_bytes > limit:\n            raise RuntimeError(f\"Quantized model still exceeds 256KB (size={size_bytes} bytes). Reduce model width/depth.\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_bytes': size_bytes,\n    }\n\n    return quantized_model, metrics\n"}
2025-10-03 01:00:44,254 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-03 01:00:44,254 - ERROR - _models.training_function_executor - BO training objective failed: Quantized model still exceeds 256KB (size=411613 bytes). Reduce model width/depth.
2025-10-03 01:00:44,254 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 988.738s
2025-10-03 01:00:44,260 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 2 FAILED with error: Quantized model still exceeds 256KB (size=411613 bytes). Reduce model width/depth.
2025-10-03 01:00:44,260 - INFO - evaluation.code_generation_pipeline_orchestrator - â³ Waiting for GPT to finish debugging...
2025-10-03 01:00:47,263 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… GPT provided fixes after 3s - requesting BO restart
2025-10-03 01:00:47,263 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ”§ Applying GPT fixes to original JSON file
2025-10-03 01:00:47,264 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 01:00:47,264 - INFO - _models.training_function_executor - Loaded training function: CATNet1D-TransTiny
2025-10-03 01:00:47,264 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-03 01:00:47,264 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Updated training_code with GPT fix
2025-10-03 01:00:47,264 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ’¾ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 01:00:47,265 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Reloaded fixed training function: CATNet1D-TransTiny
2025-10-03 01:00:47,265 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Applied GPT fixes, restarting BO from trial 0
2025-10-03 01:00:47,265 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ”„ BO Restart attempt 1/4
2025-10-03 01:00:47,265 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 1: ðŸ“¦ Installing dependencies for GPT-generated training code...
2025-10-03 01:00:47,265 - INFO - package_installer - ðŸ” Analyzing GPT-generated code for package dependencies...
2025-10-03 01:00:47,266 - INFO - package_installer - Extracted imports from code: set()
2025-10-03 01:00:47,266 - INFO - package_installer - âœ… No external packages required
2025-10-03 01:00:47,266 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-10-03 01:00:47,266 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 01:00:47,266 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-03 01:00:47,266 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'base_channels', 'ds_kernel_size', 'n_ds_blocks', 'd_model', 'head_dim', 'num_transformer_layers', 'ff_multiplier', 'dropout', 'weight_decay', 'gamma_focal', 'effective_beta', 'label_smoothing', 'use_amp', 'use_smote_tomek', 'smote_target_min_frac', 'smote_max_multiplier', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'quant_calibrate_batches']
2025-10-03 01:00:47,266 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-03 01:00:47,266 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 01:00:47,266 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-03 01:00:47,299 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-03 01:00:47,333 - INFO - bo.run_bo - Converted GPT search space: 23 parameters
2025-10-03 01:00:47,333 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-03 01:00:47,333 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-03 01:00:47,334 - INFO - bo.run_bo - ðŸ”BO Trial 1: Initial random exploration
2025-10-03 01:00:47,334 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 01:00:47,334 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:00:47,334 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:00:47,334 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 01:00:47,335 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 01:00:47,335 - ERROR - _models.training_function_executor - Training execution failed: name 'torch' is not defined
2025-10-03 01:00:47,335 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-03 01:00:47,335 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-03 01:00:47,335 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-03 01:00:47,335 - INFO - _models.ai_code_generator - Prompt length: 11371 characters
2025-10-03 01:00:47,335 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-03 01:00:47,335 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-03 01:00:47,335 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-03 01:01:40,770 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-03 01:01:40,771 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-03 01:01:40,771 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251003_010140_attempt1.txt
2025-10-03 01:01:40,771 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Hyperparameters\n    lr=0.001,\n    batch_size=128,\n    epochs=20,\n    base_channels=8,\n    ds_kernel_size=7,\n    n_ds_blocks=2,\n    d_model=64,\n    head_dim=16,\n    num_transformer_layers=1,\n    ff_multiplier=2,\n    dropout=0.1,\n    weight_decay=1e-4,\n    gamma_focal=2.0,\n    effective_beta=0.999,\n    label_smoothing=0.0,\n    use_amp=True,\n    # SMOTE-Tomek like oversampling (offline)\n    use_smote_tomek=True,\n    smote_target_min_frac=0.20,\n    smote_max_multiplier=1,\n    # Quantization params\n    quantization_bits=8,            # {8,16,32}\n    quantize_weights=True,\n    quantize_activations=False,\n    quant_calibrate_batches=2,\n):\n    \"\"\"\n    Train CATNet1D-TransTiny on GPU and return a quantized model and training metrics.\n    X_*: torch tensors. Shapes accepted: (N, 1000, 2) or (N, 2, 1000). Targets as (N,) longs.\n    \"\"\"\n    # Required imports to avoid NameError\n    import io\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # Robust device handling\n    device = torch.device(device)\n\n    # Ensure correct input layout (N, C, L) with C=2, L=1000\n    def to_NCL(x):\n        if x.dim() != 3:\n            raise ValueError('Input X must be 3D: (N, 1000, 2) or (N, 2, 1000)')\n        if x.shape[1] == 2:  # (N, 2, L)\n            return x\n        elif x.shape[2] == 2:  # (N, L, 2) -> (N, 2, L)\n            return x.transpose(1, 2).contiguous()\n        else:\n            raise ValueError('Expected one dimension to be 2 (channels)')\n\n    X_train = to_NCL(X_train)\n    X_val = to_NCL(X_val)\n    seq_len = X_train.shape[-1]\n    in_ch = X_train.shape[1]\n\n    # Optional offline SMOTE-like oversampling (lightweight)\n    if use_smote_tomek:\n        X_train_cpu = X_train.detach().cpu()\n        y_train_cpu = y_train.detach().cpu().long()\n        X_train_cpu, y_train_cpu = smote_oversample_tensor(\n            X_train_cpu, y_train_cpu, target_min_frac=smote_target_min_frac, max_multiplier=smote_max_multiplier\n        )\n        X_train = X_train_cpu\n        y_train = y_train_cpu\n\n    # Datasets & Loaders (pin_memory=False as required)\n    train_ds = TensorDataset(X_train, y_train.long())\n    val_ds = TensorDataset(X_val, y_val.long())\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CATNet1DTransTiny(\n        in_ch=in_ch,\n        seq_len=seq_len,\n        base_ch=base_channels,\n        ds_kernel=ds_kernel_size,\n        n_ds_blocks=n_ds_blocks,\n        d_model=d_model,\n        head_dim=head_dim,\n        n_layers=num_transformer_layers,\n        ff_multiplier=ff_multiplier,\n        dropout=dropout,\n        n_classes=5,\n    ).to(device)\n\n    # Ensure positional encoding buffer is not saved in state_dict to keep model size small\n    try:\n        if hasattr(model, 'pos') and hasattr(model.pos, '_buffers') and 'pe' in model.pos._buffers:\n            if hasattr(model.pos, '_non_persistent_buffers_set'):\n                model.pos._non_persistent_buffers_set.add('pe')\n    except Exception:\n        pass\n\n    # Optimizer & loss\n    class_counts = {int(c.item()): int((y_train == c).sum().item()) for c in torch.unique(y_train)}\n    criterion = FocalLossWithEN(class_counts, num_classes=5, gamma=gamma_focal, beta=effective_beta, label_smoothing=label_smoothing, device=device).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == 'cuda')\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp and device.type == 'cuda'):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            running_loss += loss.detach().item() * xb.size(0)\n            n_train += xb.size(0)\n        train_loss = running_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n        val_loss = val_running_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # ---------------------\n    # Post-training quantization\n    # ---------------------\n    model_cpu = model.to('cpu').eval()\n\n    def measure_size_bytes(state_dict):\n        buf = io.BytesIO()\n        torch.save(state_dict, buf)\n        return buf.getbuffer().nbytes\n\n    def apply_quantization(model_fp):\n        # If user requests no quantization (32-bit), just return model\n        if not quantize_weights and not quantize_activations:\n            return model_fp\n        # 16-bit: lightweight FP16 cast (mainly effective on GPU)\n        if quantization_bits == 16:\n            model_half = model_fp.half()\n            return model_half\n        # 8-bit dynamic quantization for Linear and MHA weights\n        if quantization_bits == 8 and quantize_weights:\n            qdq_types = {nn.Linear}\n            if hasattr(nn, 'MultiheadAttention'):\n                qdq_types.add(nn.MultiheadAttention)\n            q_model = torch.ao.quantization.quantize_dynamic(\n                model_fp, {t for t in qdq_types}, dtype=torch.qint8\n            )\n            return q_model\n        # Default fallback: return original\n        return model_fp\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Enforce final size <= 256KB by forcing int8 dynamic if needed\n    size_bytes = measure_size_bytes(quantized_model.state_dict())\n    limit = 256 * 1024\n    if size_bytes > limit:\n        print(f\"Warning: quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Forcing int8 dynamic quantization.\")\n        quantized_model = torch.ao.quantization.quantize_dynamic(quantized_model, {nn.Linear, getattr(nn, 'MultiheadAttention', nn.Linear)}, dtype=torch.qint8)\n        size_bytes = measure_size_bytes(quantized_model.state_dict())\n        if size_bytes > limit:\n            raise RuntimeError(f\"Quantized model still exceeds 256KB (size={size_bytes} bytes). Reduce model width/depth.\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_bytes': size_bytes,\n    }\n\n    return quantized_model, metrics\n"}
2025-10-03 01:01:40,771 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-03 01:01:40,771 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Hyperparameters\n    lr=0.001,\n    batch_size=128,\n    epochs=20,\n    base_channels=8,\n    ds_kernel_size=7,\n    n_ds_blocks=2,\n    d_model=64,\n    head_dim=16,\n    num_transformer_layers=1,\n    ff_multiplier=2,\n    dropout=0.1,\n    weight_decay=1e-4,\n    gamma_focal=2.0,\n    effective_beta=0.999,\n    label_smoothing=0.0,\n    use_amp=True,\n    # SMOTE-Tomek like oversampling (offline)\n    use_smote_tomek=True,\n    smote_target_min_frac=0.20,\n    smote_max_multiplier=1,\n    # Quantization params\n    quantization_bits=8,            # {8,16,32}\n    quantize_weights=True,\n    quantize_activations=False,\n    quant_calibrate_batches=2,\n):\n    \"\"\"\n    Train CATNet1D-TransTiny on GPU and return a quantized model and training metrics.\n    X_*: torch tensors. Shapes accepted: (N, 1000, 2) or (N, 2, 1000). Targets as (N,) longs.\n    \"\"\"\n    # Required imports to avoid NameError\n    import io\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # Robust device handling\n    device = torch.device(device)\n\n    # Ensure correct input layout (N, C, L) with C=2, L=1000\n    def to_NCL(x):\n        if x.dim() != 3:\n            raise ValueError('Input X must be 3D: (N, 1000, 2) or (N, 2, 1000)')\n        if x.shape[1] == 2:  # (N, 2, L)\n            return x\n        elif x.shape[2] == 2:  # (N, L, 2) -> (N, 2, L)\n            return x.transpose(1, 2).contiguous()\n        else:\n            raise ValueError('Expected one dimension to be 2 (channels)')\n\n    X_train = to_NCL(X_train)\n    X_val = to_NCL(X_val)\n    seq_len = X_train.shape[-1]\n    in_ch = X_train.shape[1]\n\n    # Optional offline SMOTE-like oversampling (lightweight)\n    if use_smote_tomek:\n        X_train_cpu = X_train.detach().cpu()\n        y_train_cpu = y_train.detach().cpu().long()\n        X_train_cpu, y_train_cpu = smote_oversample_tensor(\n            X_train_cpu, y_train_cpu, target_min_frac=smote_target_min_frac, max_multiplier=smote_max_multiplier\n        )\n        X_train = X_train_cpu\n        y_train = y_train_cpu\n\n    # Datasets & Loaders (pin_memory=False as required)\n    train_ds = TensorDataset(X_train, y_train.long())\n    val_ds = TensorDataset(X_val, y_val.long())\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CATNet1DTransTiny(\n        in_ch=in_ch,\n        seq_len=seq_len,\n        base_ch=base_channels,\n        ds_kernel=ds_kernel_size,\n        n_ds_blocks=n_ds_blocks,\n        d_model=d_model,\n        head_dim=head_dim,\n        n_layers=num_transformer_layers,\n        ff_multiplier=ff_multiplier,\n        dropout=dropout,\n        n_classes=5,\n    ).to(device)\n\n    # Ensure positional encoding buffer is not saved in state_dict to keep model size small\n    try:\n        if hasattr(model, 'pos') and hasattr(model.pos, '_buffers') and 'pe' in model.pos._buffers:\n            if hasattr(model.pos, '_non_persistent_buffers_set'):\n                model.pos._non_persistent_buffers_set.add('pe')\n    except Exception:\n        pass\n\n    # Optimizer & loss\n    class_counts = {int(c.item()): int((y_train == c).sum().item()) for c in torch.unique(y_train)}\n    criterion = FocalLossWithEN(class_counts, num_classes=5, gamma=gamma_focal, beta=effective_beta, label_smoothing=label_smoothing, device=device).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == 'cuda')\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp and device.type == 'cuda'):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            running_loss += loss.detach().item() * xb.size(0)\n            n_train += xb.size(0)\n        train_loss = running_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n        val_loss = val_running_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # ---------------------\n    # Post-training quantization\n    # ---------------------\n    model_cpu = model.to('cpu').eval()\n\n    def measure_size_bytes(state_dict):\n        buf = io.BytesIO()\n        torch.save(state_dict, buf)\n        return buf.getbuffer().nbytes\n\n    def apply_quantization(model_fp):\n        # If user requests no quantization (32-bit), just return model\n        if not quantize_weights and not quantize_activations:\n            return model_fp\n        # 16-bit: lightweight FP16 cast (mainly effective on GPU)\n        if quantization_bits == 16:\n            model_half = model_fp.half()\n            return model_half\n        # 8-bit dynamic quantization for Linear and MHA weights\n        if quantization_bits == 8 and quantize_weights:\n            qdq_types = {nn.Linear}\n            if hasattr(nn, 'MultiheadAttention'):\n                qdq_types.add(nn.MultiheadAttention)\n            q_model = torch.ao.quantization.quantize_dynamic(\n                model_fp, {t for t in qdq_types}, dtype=torch.qint8\n            )\n            return q_model\n        # Default fallback: return original\n        return model_fp\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Enforce final size <= 256KB by forcing int8 dynamic if needed\n    size_bytes = measure_size_bytes(quantized_model.state_dict())\n    limit = 256 * 1024\n    if size_bytes > limit:\n        print(f\"Warning: quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Forcing int8 dynamic quantization.\")\n        quantized_model = torch.ao.quantization.quantize_dynamic(quantized_model, {nn.Linear, getattr(nn, 'MultiheadAttention', nn.Linear)}, dtype=torch.qint8)\n        size_bytes = measure_size_bytes(quantized_model.state_dict())\n        if size_bytes > limit:\n            raise RuntimeError(f\"Quantized model still exceeds 256KB (size={size_bytes} bytes). Reduce model width/depth.\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_bytes': size_bytes,\n    }\n\n    return quantized_model, metrics\n"}
2025-10-03 01:01:40,772 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-03 01:01:40,772 - ERROR - _models.training_function_executor - BO training objective failed: name 'torch' is not defined
2025-10-03 01:01:40,772 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 53.437s
2025-10-03 01:01:40,772 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name 'torch' is not defined
2025-10-03 01:01:40,772 - INFO - evaluation.code_generation_pipeline_orchestrator - â³ Waiting for GPT to finish debugging...
2025-10-03 01:01:43,774 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… GPT provided fixes after 3s - requesting BO restart
2025-10-03 01:01:43,774 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ”§ Applying GPT fixes to original JSON file
2025-10-03 01:01:43,775 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 01:01:43,775 - INFO - _models.training_function_executor - Loaded training function: CATNet1D-TransTiny
2025-10-03 01:01:43,775 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-03 01:01:43,775 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Updated training_code with GPT fix
2025-10-03 01:01:43,775 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ’¾ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 01:01:43,775 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Reloaded fixed training function: CATNet1D-TransTiny
2025-10-03 01:01:43,775 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Applied GPT fixes, restarting BO from trial 0
2025-10-03 01:01:43,775 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ”„ BO Restart attempt 2/4
2025-10-03 01:01:43,775 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 2: ðŸ“¦ Installing dependencies for GPT-generated training code...
2025-10-03 01:01:43,775 - INFO - package_installer - ðŸ” Analyzing GPT-generated code for package dependencies...
2025-10-03 01:01:43,776 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-03 01:01:43,776 - INFO - package_installer - Available packages: {'torch'}
2025-10-03 01:01:43,776 - INFO - package_installer - Missing packages: set()
2025-10-03 01:01:43,776 - INFO - package_installer - âœ… All required packages are already available
2025-10-03 01:01:43,776 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-10-03 01:01:43,777 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 01:01:43,777 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-03 01:01:43,777 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'base_channels', 'ds_kernel_size', 'n_ds_blocks', 'd_model', 'head_dim', 'num_transformer_layers', 'ff_multiplier', 'dropout', 'weight_decay', 'gamma_focal', 'effective_beta', 'label_smoothing', 'use_amp', 'use_smote_tomek', 'smote_target_min_frac', 'smote_max_multiplier', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'quant_calibrate_batches']
2025-10-03 01:01:43,777 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-03 01:01:43,777 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 01:01:43,777 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-03 01:01:43,810 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-03 01:01:43,844 - INFO - bo.run_bo - Converted GPT search space: 23 parameters
2025-10-03 01:01:43,844 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-03 01:01:43,845 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-03 01:01:43,845 - INFO - bo.run_bo - ðŸ”BO Trial 1: Initial random exploration
2025-10-03 01:01:43,845 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 01:01:43,845 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:01:43,845 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:01:43,845 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 01:01:43,846 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 01:01:43,892 - ERROR - _models.training_function_executor - Training execution failed: name 'smote_oversample_tensor' is not defined
2025-10-03 01:01:43,892 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-03 01:01:43,892 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-03 01:01:43,892 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-03 01:01:43,892 - INFO - _models.ai_code_generator - Prompt length: 11548 characters
2025-10-03 01:01:43,892 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-03 01:01:43,892 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-03 01:01:43,892 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-03 01:02:51,636 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-03 01:02:51,637 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-03 01:02:51,637 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251003_010251_attempt1.txt
2025-10-03 01:02:51,637 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code":"def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Hyperparameters\n    lr=0.001,\n    batch_size=128,\n    epochs=20,\n    base_channels=8,\n    ds_kernel_size=7,\n    n_ds_blocks=2,\n    d_model=64,\n    head_dim=16,\n    num_transformer_layers=1,\n    ff_multiplier=2,\n    dropout=0.1,\n    weight_decay=1e-4,\n    gamma_focal=2.0,\n    effective_beta=0.999,\n    label_smoothing=0.0,\n    use_amp=True,\n    # SMOTE-Tomek like oversampling (offline)\n    use_smote_tomek=True,\n    smote_target_min_frac=0.20,\n    smote_max_multiplier=1,\n    # Quantization params\n    quantization_bits=8,            # {8,16,32}\n    quantize_weights=True,\n    quantize_activations=False,\n    quant_calibrate_batches=2,\n):\n    \"\"\"\n    Train CATNet1D-TransTiny on GPU and return a quantized model and training metrics.\n    X_*: torch tensors. Shapes accepted: (N, 1000, 2) or (N, 2, 1000). Targets as (N,) longs.\n    \"\"\"\n    # Required imports to avoid NameError\n    import io\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # Robust device handling\n    device = torch.device(device)\n\n    # Ensure correct input layout (N, C, L) with C=2, L=1000\n    def to_NCL(x):\n        if x.dim() != 3:\n            raise ValueError('Input X must be 3D: (N, 1000, 2) or (N, 2, 1000)')\n        if x.shape[1] == 2:  # (N, 2, L)\n            return x\n        elif x.shape[2] == 2:  # (N, L, 2) -> (N, 2, L)\n            return x.transpose(1, 2).contiguous()\n        else:\n            raise ValueError('Expected one dimension to be 2 (channels)')\n\n    # Lightweight SMOTE-like oversampling for tensors\n    def smote_oversample_tensor(X, y, target_min_frac=0.2, max_multiplier=1):\n        \"\"\"\n        X: (N, C, L) float tensor on CPU\n        y: (N,) long tensor on CPU\n        Returns oversampled (X, y) tensors on CPU.\n        \"\"\"\n        if X.numel() == 0:\n            return X, y\n        X = X.clone()\n        y = y.clone()\n        N = X.size(0)\n        classes, counts = y.unique(return_counts=True)\n        if classes.numel() == 0:\n            return X, y\n        min_target = int((target_min_frac * N) + 0.999)  # ceil\n        synth_X_list = []\n        synth_y_list = []\n        for c, n_c in zip(classes.tolist(), counts.tolist()):\n            n_c = int(n_c)\n            idx = (y == c).nonzero(as_tuple=False).squeeze(1)\n            desired = max(n_c, min_target)\n            if max_multiplier is not None and max_multiplier > 0:\n                desired = min(desired, n_c * (1 + int(max_multiplier)))\n            add_k = max(0, desired - n_c)\n            if add_k == 0:\n                continue\n            X_c = X[idx]\n            if n_c == 1:\n                base = X_c[0].unsqueeze(0).repeat(add_k, 1, 1)\n                std = X_c[0].std()\n                noise = torch.randn_like(base) * 0.01 * (std + 1e-6)\n                synth = base + noise\n            else:\n                i1 = torch.randint(0, n_c, (add_k,))\n                i2 = torch.randint(0, n_c, (add_k,))\n                x1 = X_c[i1]\n                x2 = X_c[i2]\n                alpha = torch.rand(add_k, 1, 1)\n                synth = x1 + alpha * (x2 - x1)\n            synth_X_list.append(synth)\n            synth_y_list.append(torch.full((add_k,), c, dtype=y.dtype))\n        if synth_X_list:\n            X_new = torch.cat([X] + synth_X_list, dim=0)\n            y_new = torch.cat([y] + synth_y_list, dim=0)\n            perm = torch.randperm(X_new.size(0))\n            return X_new[perm], y_new[perm]\n        else:\n            return X, y\n\n    X_train = to_NCL(X_train)\n    X_val = to_NCL(X_val)\n    seq_len = X_train.shape[-1]\n    in_ch = X_train.shape[1]\n\n    # Optional offline SMOTE-like oversampling (lightweight)\n    if use_smote_tomek:\n        X_train_cpu = X_train.detach().cpu()\n        y_train_cpu = y_train.detach().cpu().long()\n        X_train_cpu, y_train_cpu = smote_oversample_tensor(\n            X_train_cpu, y_train_cpu, target_min_frac=smote_target_min_frac, max_multiplier=smote_max_multiplier\n        )\n        X_train = X_train_cpu\n        y_train = y_train_cpu\n\n    # Datasets & Loaders (pin_memory=False as required)\n    train_ds = TensorDataset(X_train, y_train.long())\n    val_ds = TensorDataset(X_val, y_val.long())\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CATNet1DTransTiny(\n        in_ch=in_ch,\n        seq_len=seq_len,\n        base_ch=base_channels,\n        ds_kernel=ds_kernel_size,\n        n_ds_blocks=n_ds_blocks,\n        d_model=d_model,\n        head_dim=head_dim,\n        n_layers=num_transformer_layers,\n        ff_multiplier=ff_multiplier,\n        dropout=dropout,\n        n_classes=5,\n    ).to(device)\n\n    # Ensure positional encoding buffer is not saved in state_dict to keep model size small\n    try:\n        if hasattr(model, 'pos') and hasattr(model.pos, '_buffers') and 'pe' in model.pos._buffers:\n            if hasattr(model.pos, '_non_persistent_buffers_set'):\n                model.pos._non_persistent_buffers_set.add('pe')\n    except Exception:\n        pass\n\n    # Optimizer & loss\n    class_counts = {int(c.item()): int((y_train == c).sum().item()) for c in torch.unique(y_train)}\n    criterion = FocalLossWithEN(class_counts, num_classes=5, gamma=gamma_focal, beta=effective_beta, label_smoothing=label_smoothing, device=device).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == 'cuda')\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp and device.type == 'cuda'):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            running_loss += loss.detach().item() * xb.size(0)\n            n_train += xb.size(0)\n        train_loss = running_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n        val_loss = val_running_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # ---------------------\n    # Post-training quantization\n    # ---------------------\n    model_cpu = model.to('cpu').eval()\n\n    def measure_size_bytes(state_dict):\n        buf = io.BytesIO()\n        torch.save(state_dict, buf)\n        return buf.getbuffer().nbytes\n\n    def apply_quantization(model_fp):\n        # If user requests no quantization (32-bit), just return model\n        if not quantize_weights and not quantize_activations:\n            return model_fp\n        # 16-bit: lightweight FP16 cast (mainly effective on GPU)\n        if quantization_bits == 16:\n            model_half = model_fp.half()\n            return model_half\n        # 8-bit dynamic quantization for Linear and MHA weights\n        if quantization_bits == 8 and quantize_weights:\n            qdq_types = {nn.Linear}\n            if hasattr(nn, 'MultiheadAttention'):\n                qdq_types.add(nn.MultiheadAttention)\n            q_model = torch.ao.quantization.quantize_dynamic(\n                model_fp, {t for t in qdq_types}, dtype=torch.qint8\n            )\n            return q_model\n        # Default fallback: return original\n        return model_fp\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Enforce final size <= 256KB by forcing int8 dynamic if needed\n    size_bytes = measure_size_bytes(quantized_model.state_dict())\n    limit = 256 * 1024\n    if size_bytes > limit:\n        print(f\"Warning: quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Forcing int8 dynamic quantization.\")\n        quantized_model = torch.ao.quantization.quantize_dynamic(quantized_model, {nn.Linear, getattr(nn, 'MultiheadAttention', nn.Linear)}, dtype=torch.qint8)\n        size_bytes = measure_size_bytes(quantized_model.state_dict())\n        if size_bytes > limit:\n            raise RuntimeError(f\"Quantized model still exceeds 256KB (size={size_bytes} bytes). Reduce model width/depth.\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_bytes': size_bytes,\n    }\n\n    return quantized_model, metrics\n"}
2025-10-03 01:02:51,637 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-03 01:02:51,637 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code":"def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Hyperparameters\n    lr=0.001,\n    batch_size=128,\n    epochs=20,\n    base_channels=8,\n    ds_kernel_size=7,\n    n_ds_blocks=2,\n    d_model=64,\n    head_dim=16,\n    num_transformer_layers=1,\n    ff_multiplier=2,\n    dropout=0.1,\n    weight_decay=1e-4,\n    gamma_focal=2.0,\n    effective_beta=0.999,\n    label_smoothing=0.0,\n    use_amp=True,\n    # SMOTE-Tomek like oversampling (offline)\n    use_smote_tomek=True,\n    smote_target_min_frac=0.20,\n    smote_max_multiplier=1,\n    # Quantization params\n    quantization_bits=8,            # {8,16,32}\n    quantize_weights=True,\n    quantize_activations=False,\n    quant_calibrate_batches=2,\n):\n    \"\"\"\n    Train CATNet1D-TransTiny on GPU and return a quantized model and training metrics.\n    X_*: torch tensors. Shapes accepted: (N, 1000, 2) or (N, 2, 1000). Targets as (N,) longs.\n    \"\"\"\n    # Required imports to avoid NameError\n    import io\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # Robust device handling\n    device = torch.device(device)\n\n    # Ensure correct input layout (N, C, L) with C=2, L=1000\n    def to_NCL(x):\n        if x.dim() != 3:\n            raise ValueError('Input X must be 3D: (N, 1000, 2) or (N, 2, 1000)')\n        if x.shape[1] == 2:  # (N, 2, L)\n            return x\n        elif x.shape[2] == 2:  # (N, L, 2) -> (N, 2, L)\n            return x.transpose(1, 2).contiguous()\n        else:\n            raise ValueError('Expected one dimension to be 2 (channels)')\n\n    # Lightweight SMOTE-like oversampling for tensors\n    def smote_oversample_tensor(X, y, target_min_frac=0.2, max_multiplier=1):\n        \"\"\"\n        X: (N, C, L) float tensor on CPU\n        y: (N,) long tensor on CPU\n        Returns oversampled (X, y) tensors on CPU.\n        \"\"\"\n        if X.numel() == 0:\n            return X, y\n        X = X.clone()\n        y = y.clone()\n        N = X.size(0)\n        classes, counts = y.unique(return_counts=True)\n        if classes.numel() == 0:\n            return X, y\n        min_target = int((target_min_frac * N) + 0.999)  # ceil\n        synth_X_list = []\n        synth_y_list = []\n        for c, n_c in zip(classes.tolist(), counts.tolist()):\n            n_c = int(n_c)\n            idx = (y == c).nonzero(as_tuple=False).squeeze(1)\n            desired = max(n_c, min_target)\n            if max_multiplier is not None and max_multiplier > 0:\n                desired = min(desired, n_c * (1 + int(max_multiplier)))\n            add_k = max(0, desired - n_c)\n            if add_k == 0:\n                continue\n            X_c = X[idx]\n            if n_c == 1:\n                base = X_c[0].unsqueeze(0).repeat(add_k, 1, 1)\n                std = X_c[0].std()\n                noise = torch.randn_like(base) * 0.01 * (std + 1e-6)\n                synth = base + noise\n            else:\n                i1 = torch.randint(0, n_c, (add_k,))\n                i2 = torch.randint(0, n_c, (add_k,))\n                x1 = X_c[i1]\n                x2 = X_c[i2]\n                alpha = torch.rand(add_k, 1, 1)\n                synth = x1 + alpha * (x2 - x1)\n            synth_X_list.append(synth)\n            synth_y_list.append(torch.full((add_k,), c, dtype=y.dtype))\n        if synth_X_list:\n            X_new = torch.cat([X] + synth_X_list, dim=0)\n            y_new = torch.cat([y] + synth_y_list, dim=0)\n            perm = torch.randperm(X_new.size(0))\n            return X_new[perm], y_new[perm]\n        else:\n            return X, y\n\n    X_train = to_NCL(X_train)\n    X_val = to_NCL(X_val)\n    seq_len = X_train.shape[-1]\n    in_ch = X_train.shape[1]\n\n    # Optional offline SMOTE-like oversampling (lightweight)\n    if use_smote_tomek:\n        X_train_cpu = X_train.detach().cpu()\n        y_train_cpu = y_train.detach().cpu().long()\n        X_train_cpu, y_train_cpu = smote_oversample_tensor(\n            X_train_cpu, y_train_cpu, target_min_frac=smote_target_min_frac, max_multiplier=smote_max_multiplier\n        )\n        X_train = X_train_cpu\n        y_train = y_train_cpu\n\n    # Datasets & Loaders (pin_memory=False as required)\n    train_ds = TensorDataset(X_train, y_train.long())\n    val_ds = TensorDataset(X_val, y_val.long())\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CATNet1DTransTiny(\n        in_ch=in_ch,\n        seq_len=seq_len,\n        base_ch=base_channels,\n        ds_kernel=ds_kernel_size,\n        n_ds_blocks=n_ds_blocks,\n        d_model=d_model,\n        head_dim=head_dim,\n        n_layers=num_transformer_layers,\n        ff_multiplier=ff_multiplier,\n        dropout=dropout,\n        n_classes=5,\n    ).to(device)\n\n    # Ensure positional encoding buffer is not saved in state_dict to keep model size small\n    try:\n        if hasattr(model, 'pos') and hasattr(model.pos, '_buffers') and 'pe' in model.pos._buffers:\n            if hasattr(model.pos, '_non_persistent_buffers_set'):\n                model.pos._non_persistent_buffers_set.add('pe')\n    except Exception:\n        pass\n\n    # Optimizer & loss\n    class_counts = {int(c.item()): int((y_train == c).sum().item()) for c in torch.unique(y_train)}\n    criterion = FocalLossWithEN(class_counts, num_classes=5, gamma=gamma_focal, beta=effective_beta, label_smoothing=label_smoothing, device=device).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == 'cuda')\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp and device.type == 'cuda'):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            running_loss += loss.detach().item() * xb.size(0)\n            n_train += xb.size(0)\n        train_loss = running_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n        val_loss = val_running_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n\n    # ---------------------\n    # Post-training quantization\n    # ---------------------\n    model_cpu = model.to('cpu').eval()\n\n    def measure_size_bytes(state_dict):\n        buf = io.BytesIO()\n        torch.save(state_dict, buf)\n        return buf.getbuffer().nbytes\n\n    def apply_quantization(model_fp):\n        # If user requests no quantization (32-bit), just return model\n        if not quantize_weights and not quantize_activations:\n            return model_fp\n        # 16-bit: lightweight FP16 cast (mainly effective on GPU)\n        if quantization_bits == 16:\n            model_half = model_fp.half()\n            return model_half\n        # 8-bit dynamic quantization for Linear and MHA weights\n        if quantization_bits == 8 and quantize_weights:\n            qdq_types = {nn.Linear}\n            if hasattr(nn, 'MultiheadAttention'):\n                qdq_types.add(nn.MultiheadAttention)\n            q_model = torch.ao.quantization.quantize_dynamic(\n                model_fp, {t for t in qdq_types}, dtype=torch.qint8\n            )\n            return q_model\n        # Default fallback: return original\n        return model_fp\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Enforce final size <= 256KB by forcing int8 dynamic if needed\n    size_bytes = measure_size_bytes(quantized_model.state_dict())\n    limit = 256 * 1024\n    if size_bytes > limit:\n        print(f\"Warning: quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Forcing int8 dynamic quantization.\")\n        quantized_model = torch.ao.quantization.quantize_dynamic(quantized_model, {nn.Linear, getattr(nn, 'MultiheadAttention', nn.Linear)}, dtype=torch.qint8)\n        size_bytes = measure_size_bytes(quantized_model.state_dict())\n        if size_bytes > limit:\n            raise RuntimeError(f\"Quantized model still exceeds 256KB (size={size_bytes} bytes). Reduce model width/depth.\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_bytes': size_bytes,\n    }\n\n    return quantized_model, metrics\n"}
2025-10-03 01:02:51,637 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-03 01:02:51,638 - ERROR - _models.training_function_executor - BO training objective failed: name 'smote_oversample_tensor' is not defined
2025-10-03 01:02:51,638 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 67.792s
2025-10-03 01:02:51,642 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name 'smote_oversample_tensor' is not defined
2025-10-03 01:02:51,642 - INFO - evaluation.code_generation_pipeline_orchestrator - â³ Waiting for GPT to finish debugging...
2025-10-03 01:02:54,645 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… GPT provided fixes after 3s - requesting BO restart
2025-10-03 01:02:54,645 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ”§ Applying GPT fixes to original JSON file
2025-10-03 01:02:54,646 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 01:02:54,646 - INFO - _models.training_function_executor - Loaded training function: CATNet1D-TransTiny
2025-10-03 01:02:54,646 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-03 01:02:54,646 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Updated training_code with GPT fix
2025-10-03 01:02:54,646 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ’¾ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 01:02:54,646 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Reloaded fixed training function: CATNet1D-TransTiny
2025-10-03 01:02:54,646 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Applied GPT fixes, restarting BO from trial 0
2025-10-03 01:02:54,646 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ”„ BO Restart attempt 3/4
2025-10-03 01:02:54,646 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 3: ðŸ“¦ Installing dependencies for GPT-generated training code...
2025-10-03 01:02:54,646 - INFO - package_installer - ðŸ” Analyzing GPT-generated code for package dependencies...
2025-10-03 01:02:54,648 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-03 01:02:54,648 - INFO - package_installer - Available packages: {'torch'}
2025-10-03 01:02:54,648 - INFO - package_installer - Missing packages: set()
2025-10-03 01:02:54,648 - INFO - package_installer - âœ… All required packages are already available
2025-10-03 01:02:54,648 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-10-03 01:02:54,648 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 01:02:54,648 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-03 01:02:54,648 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'base_channels', 'ds_kernel_size', 'n_ds_blocks', 'd_model', 'head_dim', 'num_transformer_layers', 'ff_multiplier', 'dropout', 'weight_decay', 'gamma_focal', 'effective_beta', 'label_smoothing', 'use_amp', 'use_smote_tomek', 'smote_target_min_frac', 'smote_max_multiplier', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'quant_calibrate_batches']
2025-10-03 01:02:54,648 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-03 01:02:54,648 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 01:02:54,648 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-03 01:02:54,681 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-03 01:02:54,713 - INFO - bo.run_bo - Converted GPT search space: 23 parameters
2025-10-03 01:02:54,713 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-03 01:02:54,714 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-03 01:02:54,714 - INFO - bo.run_bo - ðŸ”BO Trial 1: Initial random exploration
2025-10-03 01:02:54,714 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 01:02:54,714 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:02:54,714 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:02:54,714 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 01:02:54,715 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 01:02:54,880 - ERROR - _models.training_function_executor - Training execution failed: name 'CATNet1DTransTiny' is not defined
2025-10-03 01:02:54,880 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-03 01:02:54,880 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-03 01:02:54,880 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-03 01:02:54,880 - INFO - _models.ai_code_generator - Prompt length: 13509 characters
2025-10-03 01:02:54,880 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-03 01:02:54,880 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-03 01:02:54,880 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-03 01:05:12,767 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-03 01:05:12,768 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-03 01:05:12,768 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251003_010512_attempt1.txt
2025-10-03 01:05:12,768 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Hyperparameters\n    lr=0.001,\n    batch_size=128,\n    epochs=20,\n    base_channels=8,\n    ds_kernel_size=7,\n    n_ds_blocks=2,\n    d_model=64,\n    head_dim=16,\n    num_transformer_layers=1,\n    ff_multiplier=2,\n    dropout=0.1,\n    weight_decay=1e-4,\n    gamma_focal=2.0,\n    effective_beta=0.999,\n    label_smoothing=0.0,\n    use_amp=True,\n    # SMOTE-Tomek like oversampling (offline)\n    use_smote_tomek=True,\n    smote_target_min_frac=0.20,\n    smote_max_multiplier=1,\n    # Quantization params\n    quantization_bits=8,            # {8,16,32}\n    quantize_weights=True,\n    quantize_activations=False,\n    quant_calibrate_batches=2,\n):\n    '''\n    Train CATNet1D-TransTiny on GPU and return a quantized model and training metrics.\n    X_*: torch tensors. Shapes accepted: (N, 1000, 2) or (N, 2, 1000). Targets as (N,) longs.\n    '''\n    # Required imports to avoid NameError\n    import io\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # Robust device handling\n    device = torch.device(device)\n\n    # Ensure correct input layout (N, C, L) with C=2, L=1000\n    def to_NCL(x):\n        if x.dim() != 3:\n            raise ValueError('Input X must be 3D: (N, 1000, 2) or (N, 2, 1000)')\n        if x.shape[1] == 2:  # (N, 2, L)\n            return x\n        elif x.shape[2] == 2:  # (N, L, 2) -> (N, 2, L)\n            return x.transpose(1, 2).contiguous()\n        else:\n            raise ValueError('Expected one dimension to be 2 (channels)')\n\n    # Lightweight SMOTE-like oversampling for tensors\n    def smote_oversample_tensor(X, y, target_min_frac=0.2, max_multiplier=1):\n        '''\n        X: (N, C, L) float tensor on CPU\n        y: (N,) long tensor on CPU\n        Returns oversampled (X, y) tensors on CPU.\n        '''\n        if X.numel() == 0:\n            return X, y\n        X = X.clone()\n        y = y.clone()\n        N = X.size(0)\n        classes, counts = y.unique(return_counts=True)\n        if classes.numel() == 0:\n            return X, y\n        min_target = int((target_min_frac * N) + 0.999)  # ceil\n        synth_X_list = []\n        synth_y_list = []\n        for c, n_c in zip(classes.tolist(), counts.tolist()):\n            n_c = int(n_c)\n            idx = (y == c).nonzero(as_tuple=False).squeeze(1)\n            desired = max(n_c, min_target)\n            if max_multiplier is not None and max_multiplier > 0:\n                desired = min(desired, n_c * (1 + int(max_multiplier)))\n            add_k = max(0, desired - n_c)\n            if add_k == 0:\n                continue\n            X_c = X[idx]\n            if n_c == 1:\n                base = X_c[0].unsqueeze(0).repeat(add_k, 1, 1)\n                std = X_c[0].std()\n                noise = torch.randn_like(base) * 0.01 * (std + 1e-6)\n                synth = base + noise\n            else:\n                i1 = torch.randint(0, n_c, (add_k,))\n                i2 = torch.randint(0, n_c, (add_k,))\n                x1 = X_c[i1]\n                x2 = X_c[i2]\n                alpha = torch.rand(add_k, 1, 1)\n                synth = x1 + alpha * (x2 - x1)\n            synth_X_list.append(synth)\n            synth_y_list.append(torch.full((add_k,), c, dtype=y.dtype))\n        if synth_X_list:\n            X_new = torch.cat([X] + synth_X_list, dim=0)\n            y_new = torch.cat([y] + synth_y_list, dim=0)\n            perm = torch.randperm(X_new.size(0))\n            return X_new[perm], y_new[perm]\n        else:\n            return X, y\n\n    # Minimal model and criterion definitions to fix NameError\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model, max_len=4096, dropout=0.0):\n            super().__init__()\n            self.dropout = nn.Dropout(dropout)\n            pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n            position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            pe = pe.unsqueeze(1)  # (L,1,D)\n            self.register_buffer('pe', pe)\n        def forward(self, x):  # x: (L,N,D)\n            L = x.size(0)\n            x = x + self.pe[:L]\n            return self.dropout(x)\n\n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, in_ch, out_ch, kernel_size, stride=1, padding=None, dropout=0.0):\n            super().__init__()\n            if padding is None:\n                padding = kernel_size // 2\n            self.dw = nn.Conv1d(in_ch, in_ch, kernel_size, stride=stride, padding=padding, groups=in_ch, bias=False)\n            self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.SiLU()\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class CATNet1DTransTiny(nn.Module):\n        def __init__(self, in_ch, seq_len, base_ch=8, ds_kernel=7, n_ds_blocks=2, d_model=64, head_dim=16, n_layers=1, ff_multiplier=2, dropout=0.1, n_classes=5):\n            super().__init__()\n            ch = base_ch\n            blocks = []\n            c_in = in_ch\n            L = seq_len\n            for _ in range(n_ds_blocks):\n                stride = 2\n                blocks.append(DepthwiseSeparableConv1d(c_in, ch, kernel_size=ds_kernel, stride=stride, dropout=dropout))\n                c_in = ch\n                L = (L + stride - 1) // stride\n            self.ds = nn.Sequential(*blocks) if blocks else nn.Identity()\n            self.proj = nn.Conv1d(c_in, d_model, kernel_size=1, bias=False)\n            nhead = max(1, d_model // max(1, head_dim))\n            encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=d_model * ff_multiplier, dropout=dropout, batch_first=False)\n            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n            self.pos = PositionalEncoding(d_model, max_len=max(2048, seq_len), dropout=dropout)\n            self.norm = nn.LayerNorm(d_model)\n            self.head = nn.Linear(d_model, n_classes)\n        def forward(self, x):  # x: (N,C,L)\n            x = self.ds(x)\n            x = self.proj(x)  # (N, d_model, L')\n            x = x.transpose(1, 2).transpose(0, 1)  # (L', N, d_model)\n            x = self.pos(x)\n            x = self.encoder(x)\n            x = self.norm(x)\n            x = x.mean(dim=0)  # (N, d_model)\n            logits = self.head(x)\n            return logits\n\n    class FocalLossWithEN(nn.Module):\n        def __init__(self, class_counts, num_classes, gamma=2.0, beta=0.999, label_smoothing=0.0, device=None):\n            super().__init__()\n            self.gamma = gamma\n            self.beta = beta\n            self.label_smoothing = label_smoothing\n            counts = torch.zeros(num_classes, dtype=torch.float32)\n            if isinstance(class_counts, dict) and class_counts:\n                for k, v in class_counts.items():\n                    if 0 <= int(k) < num_classes:\n                        counts[int(k)] = max(1.0, float(v))\n            else:\n                counts[:] = 1.0\n            eff_num = 1.0 - torch.pow(torch.tensor(beta, dtype=torch.float32), counts)\n            weights = (1.0 - beta) / eff_num\n            weights[torch.isinf(weights) | torch.isnan(weights)] = 0.0\n            weights = weights / (weights.sum() + 1e-9) * num_classes\n            if device is not None:\n                weights = weights.to(device)\n            self.register_buffer('class_weights', weights)\n        def forward(self, logits, targets):\n            log_probs = F.log_softmax(logits, dim=1)\n            probs = log_probs.exp()\n            N, C = logits.shape\n            smooth = self.label_smoothing\n            if smooth > 0.0:\n                with torch.no_grad():\n                    true_dist = torch.full_like(log_probs, smooth / (C - 1))\n                    true_dist.scatter_(1, targets.view(-1, 1), 1.0 - smooth)\n                ce = -(true_dist * log_probs).sum(dim=1)\n            else:\n                ce = F.nll_loss(log_probs, targets, reduction='none')\n            p_t = probs.gather(1, targets.view(-1, 1)).squeeze(1).clamp_(1e-8, 1.0)\n            focal = (1.0 - p_t) ** self.gamma\n            w = self.class_weights.gather(0, targets.view(-1))\n            loss = (focal * ce * w).mean()\n            return loss\n\n    X_train = to_NCL(X_train)\n    X_val = to_NCL(X_val)\n    seq_len = X_train.shape[-1]\n    in_ch = X_train.shape[1]\n\n    # Optional offline SMOTE-like oversampling (lightweight)\n    if use_smote_tomek:\n        X_train_cpu = X_train.detach().cpu()\n        y_train_cpu = y_train.detach().cpu().long()\n        X_train_cpu, y_train_cpu = smote_oversample_tensor(\n            X_train_cpu, y_train_cpu, target_min_frac=smote_target_min_frac, max_multiplier=smote_max_multiplier\n        )\n        X_train = X_train_cpu\n        y_train = y_train_cpu\n\n    # Datasets & Loaders (pin_memory=False as required)\n    train_ds = TensorDataset(X_train, y_train.long())\n    val_ds = TensorDataset(X_val, y_val.long())\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CATNet1DTransTiny(\n        in_ch=in_ch,\n        seq_len=seq_len,\n        base_ch=base_channels,\n        ds_kernel=ds_kernel_size,\n        n_ds_blocks=n_ds_blocks,\n        d_model=d_model,\n        head_dim=head_dim,\n        n_layers=num_transformer_layers,\n        ff_multiplier=ff_multiplier,\n        dropout=dropout,\n        n_classes=5,\n    ).to(device)\n\n    # Ensure positional encoding buffer is not saved in state_dict to keep model size small\n    try:\n        if hasattr(model, 'pos') and hasattr(model.pos, '_buffers') and 'pe' in model.pos._buffers:\n            if hasattr(model.pos, '_non_persistent_buffers_set'):\n                model.pos._non_persistent_buffers_set.add('pe')\n    except Exception:\n        pass\n\n    # Optimizer & loss\n    class_counts = {int(c.item()): int((y_train == c).sum().item()) for c in torch.unique(y_train)}\n    criterion = FocalLossWithEN(class_counts, num_classes=5, gamma=gamma_focal, beta=effective_beta, label_smoothing=label_smoothing, device=device).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == 'cuda')\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp and device.type == 'cuda'):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            running_loss += loss.detach().item() * xb.size(0)\n            n_train += xb.size(0)\n        train_loss = running_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n        val_loss = val_running_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}')\n\n    # ---------------------\n    # Post-training quantization\n    # ---------------------\n    model_cpu = model.to('cpu').eval()\n\n    def measure_size_bytes(state_dict):\n        buf = io.BytesIO()\n        torch.save(state_dict, buf)\n        return buf.getbuffer().nbytes\n\n    def apply_quantization(model_fp):\n        # If user requests no quantization (32-bit), just return model\n        if not quantize_weights and not quantize_activations:\n            return model_fp\n        # 16-bit: lightweight FP16 cast (mainly effective on GPU)\n        if quantization_bits == 16:\n            model_half = model_fp.half()\n            return model_half\n        # 8-bit dynamic quantization for Linear and MHA weights\n        if quantization_bits == 8 and quantize_weights:\n            qdq_types = {nn.Linear}\n            if hasattr(nn, 'MultiheadAttention'):\n                qdq_types.add(nn.MultiheadAttention)\n            q_model = torch.ao.quantization.quantize_dynamic(\n                model_fp, {t for t in qdq_types}, dtype=torch.qint8\n            )\n            return q_model\n        # Default fallback: return original\n        return model_fp\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Enforce final size <= 256KB by forcing int8 dynamic if needed\n    size_bytes = measure_size_bytes(quantized_model.state_dict())\n    limit = 256 * 1024\n    if size_bytes > limit:\n        print(f'Warning: quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Forcing int8 dynamic quantization.')\n        quantized_model = torch.ao.quantization.quantize_dynamic(quantized_model, {nn.Linear, getattr(nn, 'MultiheadAttention', nn.Linear)}, dtype=torch.qint8)\n        size_bytes = measure_size_bytes(quantized_model.state_dict())\n        if size_bytes > limit:\n            raise RuntimeError(f'Quantized model still exceeds 256KB (size={size_bytes} bytes). Reduce model width/depth.')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_bytes': size_bytes,\n    }\n\n    return quantized_model, metrics\n"}
2025-10-03 01:05:12,768 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-03 01:05:12,768 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Hyperparameters\n    lr=0.001,\n    batch_size=128,\n    epochs=20,\n    base_channels=8,\n    ds_kernel_size=7,\n    n_ds_blocks=2,\n    d_model=64,\n    head_dim=16,\n    num_transformer_layers=1,\n    ff_multiplier=2,\n    dropout=0.1,\n    weight_decay=1e-4,\n    gamma_focal=2.0,\n    effective_beta=0.999,\n    label_smoothing=0.0,\n    use_amp=True,\n    # SMOTE-Tomek like oversampling (offline)\n    use_smote_tomek=True,\n    smote_target_min_frac=0.20,\n    smote_max_multiplier=1,\n    # Quantization params\n    quantization_bits=8,            # {8,16,32}\n    quantize_weights=True,\n    quantize_activations=False,\n    quant_calibrate_batches=2,\n):\n    '''\n    Train CATNet1D-TransTiny on GPU and return a quantized model and training metrics.\n    X_*: torch tensors. Shapes accepted: (N, 1000, 2) or (N, 2, 1000). Targets as (N,) longs.\n    '''\n    # Required imports to avoid NameError\n    import io\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, TensorDataset\n\n    # Robust device handling\n    device = torch.device(device)\n\n    # Ensure correct input layout (N, C, L) with C=2, L=1000\n    def to_NCL(x):\n        if x.dim() != 3:\n            raise ValueError('Input X must be 3D: (N, 1000, 2) or (N, 2, 1000)')\n        if x.shape[1] == 2:  # (N, 2, L)\n            return x\n        elif x.shape[2] == 2:  # (N, L, 2) -> (N, 2, L)\n            return x.transpose(1, 2).contiguous()\n        else:\n            raise ValueError('Expected one dimension to be 2 (channels)')\n\n    # Lightweight SMOTE-like oversampling for tensors\n    def smote_oversample_tensor(X, y, target_min_frac=0.2, max_multiplier=1):\n        '''\n        X: (N, C, L) float tensor on CPU\n        y: (N,) long tensor on CPU\n        Returns oversampled (X, y) tensors on CPU.\n        '''\n        if X.numel() == 0:\n            return X, y\n        X = X.clone()\n        y = y.clone()\n        N = X.size(0)\n        classes, counts = y.unique(return_counts=True)\n        if classes.numel() == 0:\n            return X, y\n        min_target = int((target_min_frac * N) + 0.999)  # ceil\n        synth_X_list = []\n        synth_y_list = []\n        for c, n_c in zip(classes.tolist(), counts.tolist()):\n            n_c = int(n_c)\n            idx = (y == c).nonzero(as_tuple=False).squeeze(1)\n            desired = max(n_c, min_target)\n            if max_multiplier is not None and max_multiplier > 0:\n                desired = min(desired, n_c * (1 + int(max_multiplier)))\n            add_k = max(0, desired - n_c)\n            if add_k == 0:\n                continue\n            X_c = X[idx]\n            if n_c == 1:\n                base = X_c[0].unsqueeze(0).repeat(add_k, 1, 1)\n                std = X_c[0].std()\n                noise = torch.randn_like(base) * 0.01 * (std + 1e-6)\n                synth = base + noise\n            else:\n                i1 = torch.randint(0, n_c, (add_k,))\n                i2 = torch.randint(0, n_c, (add_k,))\n                x1 = X_c[i1]\n                x2 = X_c[i2]\n                alpha = torch.rand(add_k, 1, 1)\n                synth = x1 + alpha * (x2 - x1)\n            synth_X_list.append(synth)\n            synth_y_list.append(torch.full((add_k,), c, dtype=y.dtype))\n        if synth_X_list:\n            X_new = torch.cat([X] + synth_X_list, dim=0)\n            y_new = torch.cat([y] + synth_y_list, dim=0)\n            perm = torch.randperm(X_new.size(0))\n            return X_new[perm], y_new[perm]\n        else:\n            return X, y\n\n    # Minimal model and criterion definitions to fix NameError\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model, max_len=4096, dropout=0.0):\n            super().__init__()\n            self.dropout = nn.Dropout(dropout)\n            pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n            position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            pe = pe.unsqueeze(1)  # (L,1,D)\n            self.register_buffer('pe', pe)\n        def forward(self, x):  # x: (L,N,D)\n            L = x.size(0)\n            x = x + self.pe[:L]\n            return self.dropout(x)\n\n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, in_ch, out_ch, kernel_size, stride=1, padding=None, dropout=0.0):\n            super().__init__()\n            if padding is None:\n                padding = kernel_size // 2\n            self.dw = nn.Conv1d(in_ch, in_ch, kernel_size, stride=stride, padding=padding, groups=in_ch, bias=False)\n            self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.SiLU()\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class CATNet1DTransTiny(nn.Module):\n        def __init__(self, in_ch, seq_len, base_ch=8, ds_kernel=7, n_ds_blocks=2, d_model=64, head_dim=16, n_layers=1, ff_multiplier=2, dropout=0.1, n_classes=5):\n            super().__init__()\n            ch = base_ch\n            blocks = []\n            c_in = in_ch\n            L = seq_len\n            for _ in range(n_ds_blocks):\n                stride = 2\n                blocks.append(DepthwiseSeparableConv1d(c_in, ch, kernel_size=ds_kernel, stride=stride, dropout=dropout))\n                c_in = ch\n                L = (L + stride - 1) // stride\n            self.ds = nn.Sequential(*blocks) if blocks else nn.Identity()\n            self.proj = nn.Conv1d(c_in, d_model, kernel_size=1, bias=False)\n            nhead = max(1, d_model // max(1, head_dim))\n            encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=d_model * ff_multiplier, dropout=dropout, batch_first=False)\n            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n            self.pos = PositionalEncoding(d_model, max_len=max(2048, seq_len), dropout=dropout)\n            self.norm = nn.LayerNorm(d_model)\n            self.head = nn.Linear(d_model, n_classes)\n        def forward(self, x):  # x: (N,C,L)\n            x = self.ds(x)\n            x = self.proj(x)  # (N, d_model, L')\n            x = x.transpose(1, 2).transpose(0, 1)  # (L', N, d_model)\n            x = self.pos(x)\n            x = self.encoder(x)\n            x = self.norm(x)\n            x = x.mean(dim=0)  # (N, d_model)\n            logits = self.head(x)\n            return logits\n\n    class FocalLossWithEN(nn.Module):\n        def __init__(self, class_counts, num_classes, gamma=2.0, beta=0.999, label_smoothing=0.0, device=None):\n            super().__init__()\n            self.gamma = gamma\n            self.beta = beta\n            self.label_smoothing = label_smoothing\n            counts = torch.zeros(num_classes, dtype=torch.float32)\n            if isinstance(class_counts, dict) and class_counts:\n                for k, v in class_counts.items():\n                    if 0 <= int(k) < num_classes:\n                        counts[int(k)] = max(1.0, float(v))\n            else:\n                counts[:] = 1.0\n            eff_num = 1.0 - torch.pow(torch.tensor(beta, dtype=torch.float32), counts)\n            weights = (1.0 - beta) / eff_num\n            weights[torch.isinf(weights) | torch.isnan(weights)] = 0.0\n            weights = weights / (weights.sum() + 1e-9) * num_classes\n            if device is not None:\n                weights = weights.to(device)\n            self.register_buffer('class_weights', weights)\n        def forward(self, logits, targets):\n            log_probs = F.log_softmax(logits, dim=1)\n            probs = log_probs.exp()\n            N, C = logits.shape\n            smooth = self.label_smoothing\n            if smooth > 0.0:\n                with torch.no_grad():\n                    true_dist = torch.full_like(log_probs, smooth / (C - 1))\n                    true_dist.scatter_(1, targets.view(-1, 1), 1.0 - smooth)\n                ce = -(true_dist * log_probs).sum(dim=1)\n            else:\n                ce = F.nll_loss(log_probs, targets, reduction='none')\n            p_t = probs.gather(1, targets.view(-1, 1)).squeeze(1).clamp_(1e-8, 1.0)\n            focal = (1.0 - p_t) ** self.gamma\n            w = self.class_weights.gather(0, targets.view(-1))\n            loss = (focal * ce * w).mean()\n            return loss\n\n    X_train = to_NCL(X_train)\n    X_val = to_NCL(X_val)\n    seq_len = X_train.shape[-1]\n    in_ch = X_train.shape[1]\n\n    # Optional offline SMOTE-like oversampling (lightweight)\n    if use_smote_tomek:\n        X_train_cpu = X_train.detach().cpu()\n        y_train_cpu = y_train.detach().cpu().long()\n        X_train_cpu, y_train_cpu = smote_oversample_tensor(\n            X_train_cpu, y_train_cpu, target_min_frac=smote_target_min_frac, max_multiplier=smote_max_multiplier\n        )\n        X_train = X_train_cpu\n        y_train = y_train_cpu\n\n    # Datasets & Loaders (pin_memory=False as required)\n    train_ds = TensorDataset(X_train, y_train.long())\n    val_ds = TensorDataset(X_val, y_val.long())\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CATNet1DTransTiny(\n        in_ch=in_ch,\n        seq_len=seq_len,\n        base_ch=base_channels,\n        ds_kernel=ds_kernel_size,\n        n_ds_blocks=n_ds_blocks,\n        d_model=d_model,\n        head_dim=head_dim,\n        n_layers=num_transformer_layers,\n        ff_multiplier=ff_multiplier,\n        dropout=dropout,\n        n_classes=5,\n    ).to(device)\n\n    # Ensure positional encoding buffer is not saved in state_dict to keep model size small\n    try:\n        if hasattr(model, 'pos') and hasattr(model.pos, '_buffers') and 'pe' in model.pos._buffers:\n            if hasattr(model.pos, '_non_persistent_buffers_set'):\n                model.pos._non_persistent_buffers_set.add('pe')\n    except Exception:\n        pass\n\n    # Optimizer & loss\n    class_counts = {int(c.item()): int((y_train == c).sum().item()) for c in torch.unique(y_train)}\n    criterion = FocalLossWithEN(class_counts, num_classes=5, gamma=gamma_focal, beta=effective_beta, label_smoothing=label_smoothing, device=device).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and device.type == 'cuda')\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    # Training loop\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp and device.type == 'cuda'):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            running_loss += loss.detach().item() * xb.size(0)\n            n_train += xb.size(0)\n        train_loss = running_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_running_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n        val_loss = val_running_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}')\n\n    # ---------------------\n    # Post-training quantization\n    # ---------------------\n    model_cpu = model.to('cpu').eval()\n\n    def measure_size_bytes(state_dict):\n        buf = io.BytesIO()\n        torch.save(state_dict, buf)\n        return buf.getbuffer().nbytes\n\n    def apply_quantization(model_fp):\n        # If user requests no quantization (32-bit), just return model\n        if not quantize_weights and not quantize_activations:\n            return model_fp\n        # 16-bit: lightweight FP16 cast (mainly effective on GPU)\n        if quantization_bits == 16:\n            model_half = model_fp.half()\n            return model_half\n        # 8-bit dynamic quantization for Linear and MHA weights\n        if quantization_bits == 8 and quantize_weights:\n            qdq_types = {nn.Linear}\n            if hasattr(nn, 'MultiheadAttention'):\n                qdq_types.add(nn.MultiheadAttention)\n            q_model = torch.ao.quantization.quantize_dynamic(\n                model_fp, {t for t in qdq_types}, dtype=torch.qint8\n            )\n            return q_model\n        # Default fallback: return original\n        return model_fp\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Enforce final size <= 256KB by forcing int8 dynamic if needed\n    size_bytes = measure_size_bytes(quantized_model.state_dict())\n    limit = 256 * 1024\n    if size_bytes > limit:\n        print(f'Warning: quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Forcing int8 dynamic quantization.')\n        quantized_model = torch.ao.quantization.quantize_dynamic(quantized_model, {nn.Linear, getattr(nn, 'MultiheadAttention', nn.Linear)}, dtype=torch.qint8)\n        size_bytes = measure_size_bytes(quantized_model.state_dict())\n        if size_bytes > limit:\n            raise RuntimeError(f'Quantized model still exceeds 256KB (size={size_bytes} bytes). Reduce model width/depth.')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_bytes': size_bytes,\n    }\n\n    return quantized_model, metrics\n"}
2025-10-03 01:05:12,768 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-03 01:05:12,768 - ERROR - _models.training_function_executor - BO training objective failed: name 'CATNet1DTransTiny' is not defined
2025-10-03 01:05:12,768 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 138.054s
2025-10-03 01:05:12,773 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name 'CATNet1DTransTiny' is not defined
2025-10-03 01:05:12,773 - INFO - evaluation.code_generation_pipeline_orchestrator - â³ Waiting for GPT to finish debugging...
2025-10-03 01:05:15,776 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… GPT provided fixes after 3s - requesting BO restart
2025-10-03 01:05:15,776 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ”§ Applying GPT fixes to original JSON file
2025-10-03 01:05:15,777 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 01:05:15,777 - INFO - _models.training_function_executor - Loaded training function: CATNet1D-TransTiny
2025-10-03 01:05:15,777 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-03 01:05:15,777 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Updated training_code with GPT fix
2025-10-03 01:05:15,777 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ’¾ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CATNet1D-TransTiny_1759469794.json
2025-10-03 01:05:15,777 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Reloaded fixed training function: CATNet1D-TransTiny
2025-10-03 01:05:15,777 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… Applied GPT fixes, restarting BO from trial 0
2025-10-03 01:05:15,777 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ”„ BO Restart attempt 4/4
2025-10-03 01:05:15,777 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 4: ðŸ“¦ Installing dependencies for GPT-generated training code...
2025-10-03 01:05:15,777 - INFO - package_installer - ðŸ” Analyzing GPT-generated code for package dependencies...
2025-10-03 01:05:15,780 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-03 01:05:15,780 - INFO - package_installer - Available packages: {'torch'}
2025-10-03 01:05:15,780 - INFO - package_installer - Missing packages: set()
2025-10-03 01:05:15,780 - INFO - package_installer - âœ… All required packages are already available
2025-10-03 01:05:15,780 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-10-03 01:05:15,780 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 01:05:15,780 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-03 01:05:15,780 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'base_channels', 'ds_kernel_size', 'n_ds_blocks', 'd_model', 'head_dim', 'num_transformer_layers', 'ff_multiplier', 'dropout', 'weight_decay', 'gamma_focal', 'effective_beta', 'label_smoothing', 'use_amp', 'use_smote_tomek', 'smote_target_min_frac', 'smote_max_multiplier', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'quant_calibrate_batches']
2025-10-03 01:05:15,780 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-03 01:05:15,780 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-03 01:05:15,780 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-03 01:05:15,813 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-03 01:05:15,849 - INFO - bo.run_bo - Converted GPT search space: 23 parameters
2025-10-03 01:05:15,849 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-03 01:05:15,849 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-03 01:05:15,850 - INFO - bo.run_bo - ðŸ”BO Trial 1: Initial random exploration
2025-10-03 01:05:15,850 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 01:05:15,850 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:05:15,850 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:05:15,850 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 01:05:15,851 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}
2025-10-03 01:08:56,126 - INFO - _models.training_function_executor - Model: 13,303 parameters, 57.2KB storage
2025-10-03 01:08:56,126 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.304795724850402, 0.20059176914150423, 0.16935397406571018, 0.1486567653846554, 0.13303125607385807, 0.1210803523076886, 0.11423371266221996, 0.10603383794849966, 0.10085457620918153, 0.09742188918245855, 0.09043354246700945, 0.08799467238964309, 0.08516161429215731, 0.08272626549108839, 0.07955546683920252, 0.07694144027761678, 0.07542899511481858, 0.07330353930262573, 0.07042992573674652, 0.06687110096103667, 0.06767570088383013, 0.06562615684012835, 0.06368463130263252, 0.06234567737674236, 0.061437472329776685, 0.06020552936260097, 0.05862588858520447, 0.0588820710973436, 0.058449873353668226, 0.056087455101087876, 0.05525257030238772, 0.05427633702072818, 0.05459745690830215, 0.05386615854237198, 0.05138488741714267, 0.05152581940288672, 0.04999953857322383, 0.05110218212676045, 0.048647081699377065, 0.05034631087509109, 0.04958258204318072, 0.049117732920926534, 0.0465492887858255, 0.04710949866817427, 0.04770137714705297, 0.045140611379018813, 0.04425607050311554, 0.044800487918001494, 0.04473400698665344, 0.044481682059160615, 0.044448958152457835, 0.04340881847527133, 0.04308793631817187, 0.04256841621542415, 0.04348410511531172, 0.04163382984231528, 0.041568976059583096, 0.04089697560225215, 0.04009933409556204, 0.04089289126333076, 0.03893201770943868, 0.03893174337517795, 0.039743997801612245, 0.039328721361169224, 0.03965818671714809, 0.03931165116563287, 0.03745468818401505, 0.03699785464550483, 0.03758765186524537, 0.03770687226293765, 0.037088972812673995, 0.037840162482768926, 0.036532037018215946, 0.035825404144684964, 0.03646974151451381, 0.034339776350255], 'val_losses': [0.16489107405134137, 0.12082259592787692, 0.11993162254801132, 0.10302073345785077, 0.09295436399552984, 0.07783381147628708, 0.08394410750397971, 0.10176280750664922, 0.08917809027957671, 0.06698359891187874, 0.07101016375892609, 0.06553671379265458, 0.07269975522857698, 0.08032749875093549, 0.0747795360555641, 0.06235161886672196, 0.06137875866253653, 0.06677962199288522, 0.06192762235802915, 0.062446924888718745, 0.057593618180612594, 0.06307231923599109, 0.05911749694811463, 0.07109334771103047, 0.06663558484822822, 0.06272464734545896, 0.0624208456572099, 0.05642378979040826, 0.055223121550160535, 0.06437236984848393, 0.05401191798543709, 0.06196045085307934, 0.052547864417073244, 0.06485609303753323, 0.057737101494422044, 0.06517130905633253, 0.05727946991841904, 0.05353119325132212, 0.05310236133869362, 0.06528979163654704, 0.054444982954627096, 0.05665249446664384, 0.0850703176729453, 0.056850680606942657, 0.05894858453106781, 0.05551874231327269, 0.05672721682022335, 0.06985591419014084, 0.04930525027283629, 0.054803377420901356, 0.06102799414624966, 0.05311258535100738, 0.059166933192471606, 0.051880223011181864, 0.05865659530612628, 0.054101582843884534, 0.0663433404416014, 0.053757867965949474, 0.05737109275415956, 0.053479297644153094, 0.05585373551224784, 0.049590352391960656, 0.05263610082250119, 0.054728204540113894, 0.05232528786671713, 0.05959118079207861, 0.05456840217649347, 0.050826826872547505, 0.049539827789176365, 0.05229073643960715, 0.05814974967429041, 0.06919409875925912, 0.04947453786596486, 0.05619354405945785, 0.05566993531163153, 0.07237343831799291], 'val_acc': [0.8193208871068789, 0.8852274151108883, 0.8859792006014284, 0.8952512216514221, 0.9001378273399323, 0.9243202606189701, 0.9164265129682997, 0.891116401453452, 0.9096604435534394, 0.9391053752662574, 0.9307104372885603, 0.9394812680115274, 0.9322140082696404, 0.9170530008770831, 0.9221902017291066, 0.9383535897757174, 0.9419872196466608, 0.9373512091216639, 0.9391053752662574, 0.9365994236311239, 0.9451196591905776, 0.9409848389926074, 0.9447437664453076, 0.9284550808169403, 0.9348452574865305, 0.9431148978824708, 0.9399824583385541, 0.9505074552061146, 0.9475003132439543, 0.9432401954642275, 0.9528881092594913, 0.9414860293196341, 0.9535145971682747, 0.9394812680115274, 0.9487532890615211, 0.9383535897757174, 0.945746147099361, 0.9513845382784112, 0.9567723342939481, 0.9408595414108508, 0.954516977822328, 0.9528881092594913, 0.9175541912041097, 0.9487532890615211, 0.946497932589901, 0.9500062648790878, 0.9496303721338178, 0.9309610324520736, 0.9553940608946248, 0.9513845382784112, 0.945746147099361, 0.951885728605438, 0.9481268011527377, 0.955268763312868, 0.9426137075554442, 0.9488785866432777, 0.9412354341561208, 0.9531387044230046, 0.9496303721338178, 0.9566470367121914, 0.9561458463851648, 0.9566470367121914, 0.9510086455331412, 0.9505074552061146, 0.9560205488034081, 0.9467485277534143, 0.9490038842250345, 0.9548928705675981, 0.9573988222027315, 0.9590276907655683, 0.9528881092594913, 0.9307104372885603, 0.951885728605438, 0.954141085077058, 0.9560205488034081, 0.9279538904899135], 'final_model_size_bytes': 61531, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': 76, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 8}, 'model_parameter_count': 13303, 'model_storage_size_kb': 57.161328125000004, 'model_size_validation': 'PASS'}
2025-10-03 01:08:56,126 - INFO - _models.training_function_executor - BO Objective: base=0.9280, size_penalty=0.0000, final=0.9280
2025-10-03 01:08:56,126 - INFO - _models.training_function_executor - Model: 13,303 parameters, 57.2KB (PASS 256KB limit)
2025-10-03 01:08:56,126 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 220.276s
2025-10-03 01:08:56,126 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9280
2025-10-03 01:08:56,126 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-03 01:08:56,126 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': np.int64(76), 'base_channels': np.int64(10), 'ds_kernel_size': 5, 'n_ds_blocks': np.int64(1), 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': np.int64(3), 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': np.int64(8)}, value=0.9280
2025-10-03 01:08:56,126 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.002452612631133679, 'batch_size': 64, 'epochs': np.int64(76), 'base_channels': np.int64(10), 'ds_kernel_size': 5, 'n_ds_blocks': np.int64(1), 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.010292247147901225, 'weight_decay': 0.007579479953348009, 'gamma_focal': 2.664885281600844, 'effective_beta': 0.9212126771567598, 'label_smoothing': 0.018182496720710064, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23366475107128332, 'smote_max_multiplier': np.int64(3), 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': np.int64(8)} -> 0.9280
2025-10-03 01:08:56,127 - INFO - bo.run_bo - ðŸ”BO Trial 2: Initial random exploration
2025-10-03 01:08:56,127 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 01:08:56,127 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:08:56,127 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:08:56,127 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 7.52374288453486e-05, 'batch_size': 128, 'epochs': 66, 'base_channels': 8, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.08526206184364578, 'weight_decay': 1.8205657658407253e-06, 'gamma_focal': 2.897771074506667, 'effective_beta': 0.9964666401041485, 'label_smoothing': 0.08083973481164614, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.28948155927925495, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 3}
2025-10-03 01:08:56,129 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 7.52374288453486e-05, 'batch_size': 128, 'epochs': 66, 'base_channels': 8, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.08526206184364578, 'weight_decay': 1.8205657658407253e-06, 'gamma_focal': 2.897771074506667, 'effective_beta': 0.9964666401041485, 'label_smoothing': 0.08083973481164614, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.28948155927925495, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 3}
2025-10-03 01:11:40,878 - INFO - _models.training_function_executor - Model: 51,131 parameters, 219.7KB storage
2025-10-03 01:11:40,878 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5443069871652145, 0.419133663900903, 0.352648089029379, 0.3105723381501918, 0.27863463687685547, 0.25695190690183267, 0.24087675036930292, 0.22752925146148376, 0.2198342243810436, 0.2124983065859504, 0.20387943457750696, 0.20029468995807873, 0.19347805789022166, 0.1879349752329311, 0.18408477644932547, 0.17974692234845155, 0.17541447451641315, 0.1715526439043138, 0.16795933736326724, 0.16421604747852175, 0.16097980360900277, 0.1575470976008425, 0.1551595128260422, 0.15362297797109564, 0.14941340242480514, 0.14748371452319484, 0.145052292989717, 0.14162272182110935, 0.13940342455909596, 0.13743653025912794, 0.13328548651699518, 0.13131006193833644, 0.13058412529241162, 0.1280508467145851, 0.12520491887372545, 0.12300998502050732, 0.12117749491448351, 0.12001409174965061, 0.11854907778808152, 0.11587040928223734, 0.11538046017789909, 0.11307783699307815, 0.11203564961092148, 0.10984470394658022, 0.10823352849797976, 0.10726156780142047, 0.10554514693692366, 0.10617924714002329, 0.10198403975202108, 0.10267880264262527, 0.10121903944234206, 0.10008447629802676, 0.09887971880473209, 0.09801730821426136, 0.09760790679268556, 0.09524724058725118, 0.09475885872460676, 0.09491345965164923, 0.09338949503395845, 0.09271712536165015, 0.09201534618025643, 0.09122225814778868, 0.08950841077569291, 0.08912264705715182, 0.08979612034904097, 0.08773653532024835], 'val_losses': [0.3232345086894766, 0.3032351244460848, 0.2554039397255817, 0.22799843338748416, 0.2070589445465087, 0.18495084757031752, 0.1744503573249746, 0.17116377321293055, 0.17244016576748328, 0.1595134046696553, 0.15107503362338864, 0.14196833067483403, 0.14257312735852823, 0.13869807158865557, 0.13591263176332574, 0.13446499488882785, 0.13289209124685275, 0.1211276872611228, 0.129752287728687, 0.12252425405110143, 0.1185794246684428, 0.12154033636892966, 0.11823362078624566, 0.11644641875981716, 0.10913060865796848, 0.10258684807247334, 0.10381412995838459, 0.10341348294334414, 0.10060417074082326, 0.0988401614111806, 0.09447201908889412, 0.0981306359517277, 0.09024203741308472, 0.09201506776120218, 0.09284999957846125, 0.08866124591023354, 0.08844654270747807, 0.09007524550913301, 0.08422009659308625, 0.09080356609670459, 0.08620713360009225, 0.08442330705728944, 0.09289803354345218, 0.07962302839325837, 0.08362446870533928, 0.07745749199506917, 0.07998775571954383, 0.07706298399238176, 0.08062499479932483, 0.07359509612833284, 0.0823382345182139, 0.08283089799602264, 0.07935580340969191, 0.07544955300640274, 0.07059015600458152, 0.07691627008871482, 0.07630861867319565, 0.07402926727022209, 0.07258736274043295, 0.07278918600287029, 0.07428025559399334, 0.07832279507863822, 0.07617713782836913, 0.07189394283445538, 0.07176513453650753, 0.0691202777298885], 'val_acc': [0.7540408470116526, 0.7269765693522116, 0.7616839994988097, 0.7883723844129809, 0.8047863676231048, 0.8257110637764691, 0.8338554065906528, 0.832602430773086, 0.8369878461345696, 0.8468863550933466, 0.8546548051622603, 0.860543791504824, 0.8641774213757675, 0.8660568851021175, 0.8689387294825209, 0.8680616464102243, 0.8721964666081945, 0.8802155118406215, 0.875454203733868, 0.8800902142588648, 0.884976819947375, 0.8812178924946749, 0.8803408094223781, 0.8882345570730484, 0.8976318757047989, 0.9025184813933091, 0.9013908031574991, 0.9016413983210124, 0.9062774088460093, 0.9067785991730359, 0.9130434782608695, 0.9062774088460093, 0.9175541912041097, 0.914797644405463, 0.9131687758426262, 0.915549429896003, 0.9183059766946498, 0.914797644405463, 0.9198095476757299, 0.9089086580628993, 0.9199348452574865, 0.9181806791128931, 0.9096604435534394, 0.9261997243453202, 0.915925322641273, 0.9255732364365368, 0.9233178799649167, 0.9255732364365368, 0.9249467485277534, 0.9268262122541034, 0.9188071670216765, 0.9185565718581631, 0.9223154993108633, 0.9295827590527502, 0.9303345445432903, 0.9263250219270768, 0.9218143089838366, 0.9293321638892369, 0.9298333542162637, 0.9277032953264002, 0.9274527001628868, 0.9213131186568099, 0.9250720461095101, 0.930835734870317, 0.9298333542162637, 0.9329657937601804], 'final_model_size_bytes': 215046, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 7.52374288453486e-05, 'batch_size': 128, 'epochs': 66, 'base_channels': 8, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.08526206184364578, 'weight_decay': 1.8205657658407253e-06, 'gamma_focal': 2.897771074506667, 'effective_beta': 0.9964666401041485, 'label_smoothing': 0.08083973481164614, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.28948155927925495, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 3}, 'model_parameter_count': 51131, 'model_storage_size_kb': 219.70351562500002, 'model_size_validation': 'PASS'}
2025-10-03 01:11:40,878 - INFO - _models.training_function_executor - BO Objective: base=0.9330, size_penalty=0.0000, final=0.9330
2025-10-03 01:11:40,878 - INFO - _models.training_function_executor - Model: 51,131 parameters, 219.7KB (PASS 256KB limit)
2025-10-03 01:11:40,878 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 164.751s
2025-10-03 01:11:40,878 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9330
2025-10-03 01:11:40,878 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-03 01:11:40,878 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 7.52374288453486e-05, 'batch_size': 128, 'epochs': np.int64(66), 'base_channels': np.int64(8), 'ds_kernel_size': 7, 'n_ds_blocks': np.int64(2), 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.08526206184364578, 'weight_decay': 1.8205657658407253e-06, 'gamma_focal': 2.897771074506667, 'effective_beta': 0.9964666401041485, 'label_smoothing': 0.08083973481164614, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.28948155927925495, 'smote_max_multiplier': np.int64(2), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': np.int64(3)}, value=0.9330
2025-10-03 01:11:40,878 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 7.52374288453486e-05, 'batch_size': 128, 'epochs': np.int64(66), 'base_channels': np.int64(8), 'ds_kernel_size': 7, 'n_ds_blocks': np.int64(2), 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.08526206184364578, 'weight_decay': 1.8205657658407253e-06, 'gamma_focal': 2.897771074506667, 'effective_beta': 0.9964666401041485, 'label_smoothing': 0.08083973481164614, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.28948155927925495, 'smote_max_multiplier': np.int64(2), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': np.int64(3)} -> 0.9330
2025-10-03 01:11:40,879 - INFO - bo.run_bo - ðŸ”BO Trial 3: Initial random exploration
2025-10-03 01:11:40,879 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 01:11:40,879 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:11:40,879 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:11:40,879 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005345166110646823, 'batch_size': 128, 'epochs': 8, 'base_channels': 7, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.3875664116805574, 'weight_decay': 0.00572790447079963, 'gamma_focal': 2.789654700855298, 'effective_beta': 0.9597302078832274, 'label_smoothing': 0.0921874235023117, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.06582955111868832, 'smote_max_multiplier': 3, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 01:11:40,880 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005345166110646823, 'batch_size': 128, 'epochs': 8, 'base_channels': 7, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.3875664116805574, 'weight_decay': 0.00572790447079963, 'gamma_focal': 2.789654700855298, 'effective_beta': 0.9597302078832274, 'label_smoothing': 0.0921874235023117, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.06582955111868832, 'smote_max_multiplier': 3, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 01:11:57,251 - INFO - _models.training_function_executor - Model: 42,783 parameters, 183.8KB storage
2025-10-03 01:11:57,251 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3101299460279852, 0.2111719916142881, 0.18757552636333463, 0.1775742498535623, 0.16772708505267322, 0.1609797397803273, 0.1562275723929361, 0.14814654971964827], 'val_losses': [0.33649941374822123, 0.30277044678104803, 0.2568013510441454, 0.24782833407142082, 0.24549000736481413, 0.24392417073249817, 0.23135678040146335, 0.23467008378359028], 'val_acc': [0.7053000877083072, 0.7352462097481519, 0.792507204610951, 0.8065405337676983, 0.814308983836612, 0.8016539280791881, 0.8217015411602556, 0.8451321889487533], 'final_model_size_bytes': 181766, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005345166110646823, 'batch_size': 128, 'epochs': 8, 'base_channels': 7, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.3875664116805574, 'weight_decay': 0.00572790447079963, 'gamma_focal': 2.789654700855298, 'effective_beta': 0.9597302078832274, 'label_smoothing': 0.0921874235023117, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.06582955111868832, 'smote_max_multiplier': 3, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 8}, 'model_parameter_count': 42783, 'model_storage_size_kb': 183.833203125, 'model_size_validation': 'PASS'}
2025-10-03 01:11:57,251 - INFO - _models.training_function_executor - BO Objective: base=0.8451, size_penalty=0.0000, final=0.8451
2025-10-03 01:11:57,251 - INFO - _models.training_function_executor - Model: 42,783 parameters, 183.8KB (PASS 256KB limit)
2025-10-03 01:11:57,251 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 16.372s
2025-10-03 01:11:57,338 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8451
2025-10-03 01:11:57,338 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.086s
2025-10-03 01:11:57,338 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.005345166110646823, 'batch_size': 128, 'epochs': np.int64(8), 'base_channels': np.int64(7), 'ds_kernel_size': 7, 'n_ds_blocks': np.int64(2), 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.3875664116805574, 'weight_decay': 0.00572790447079963, 'gamma_focal': 2.789654700855298, 'effective_beta': 0.9597302078832274, 'label_smoothing': 0.0921874235023117, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.06582955111868832, 'smote_max_multiplier': np.int64(3), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': np.int64(8)}, value=0.8451
2025-10-03 01:11:57,338 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.005345166110646823, 'batch_size': 128, 'epochs': np.int64(8), 'base_channels': np.int64(7), 'ds_kernel_size': 7, 'n_ds_blocks': np.int64(2), 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.3875664116805574, 'weight_decay': 0.00572790447079963, 'gamma_focal': 2.789654700855298, 'effective_beta': 0.9597302078832274, 'label_smoothing': 0.0921874235023117, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.06582955111868832, 'smote_max_multiplier': np.int64(3), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': np.int64(8)} -> 0.8451
2025-10-03 01:11:57,338 - INFO - bo.run_bo - ðŸ”BO Trial 4: Using RF surrogate + Expected Improvement
2025-10-03 01:11:57,338 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 01:11:57,338 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:11:57,338 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:11:57,338 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00011756010900231844, 'batch_size': 64, 'epochs': 60, 'base_channels': 12, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.20416591474634455, 'weight_decay': 1.8862985330373645e-05, 'gamma_focal': 2.502385968839412, 'effective_beta': 0.9862712665706926, 'label_smoothing': 0.08483967676822196, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.07515400245820475, 'smote_max_multiplier': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 2}
2025-10-03 01:11:57,339 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00011756010900231844, 'batch_size': 64, 'epochs': 60, 'base_channels': 12, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.20416591474634455, 'weight_decay': 1.8862985330373645e-05, 'gamma_focal': 2.502385968839412, 'effective_beta': 0.9862712665706926, 'label_smoothing': 0.08483967676822196, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.07515400245820475, 'smote_max_multiplier': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 2}
2025-10-03 01:15:23,365 - INFO - _models.training_function_executor - Model: 26,083 parameters, 112.1KB storage
2025-10-03 01:15:23,365 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5014920927222043, 0.384057096869653, 0.3389380780949092, 0.30395644379276493, 0.2749561154207039, 0.2551286551202113, 0.2439883389413324, 0.23339937901303637, 0.22617826023843918, 0.21787566599622227, 0.21207137818737673, 0.21028815126617167, 0.20191358283180733, 0.19716480443515103, 0.19389516881480845, 0.18785043491524042, 0.1845139995256718, 0.18214227131277766, 0.1773060168963273, 0.17420250064350878, 0.1714975493682615, 0.16902826585264985, 0.16611294995572695, 0.16467858939875474, 0.16153990684275688, 0.15941117990240664, 0.15892188612281097, 0.15550115661288827, 0.15377719108549834, 0.1521817794891796, 0.1506397735420233, 0.14745817449674278, 0.1457861962702688, 0.14685248957480382, 0.14411764003630592, 0.1430862597595831, 0.14152409751155992, 0.13985858536023577, 0.13838727261370765, 0.13759103982797363, 0.13612129926466796, 0.1361279018200125, 0.1346244917676151, 0.13394745401627917, 0.13352706462403732, 0.13345597804082734, 0.12866681569730004, 0.1287562024502096, 0.1278330805455193, 0.12708098364975584, 0.12622949830844213, 0.12554071117938734, 0.12404885164687061, 0.12208762423257286, 0.12396266989732224, 0.12213144543697624, 0.12017186264272646, 0.12032036097349424, 0.11956894038572631, 0.11966466770415483], 'val_losses': [0.33472939490592357, 0.2968832178158265, 0.2776820709544485, 0.2603753653217596, 0.22834376290453628, 0.216626477652647, 0.21177482878320902, 0.20629144893218634, 0.18580120497591707, 0.18548617221190059, 0.1715805869721751, 0.17310050043444664, 0.1747469298953237, 0.16816431466177761, 0.16680512378171278, 0.17557571915557937, 0.16148112151926286, 0.16834248707320154, 0.16106834950254847, 0.1559908706701843, 0.16287591719766292, 0.1765661445936304, 0.14955774950564407, 0.15716693331433215, 0.14859820737951218, 0.158478830277158, 0.1487257144864438, 0.14741839345718682, 0.14890505080439814, 0.16207415147422774, 0.1459852294573254, 0.14879506378021623, 0.15340946683450804, 0.1459834742280583, 0.14931626098767034, 0.15391901612203382, 0.17305455863841748, 0.1686336389834862, 0.14841706757108725, 0.15157135186723683, 0.15325815906864168, 0.14828921981270698, 0.15425723217462273, 0.1563013049817865, 0.14780007766917033, 0.15378934583924136, 0.14399385949660476, 0.15057321358087022, 0.143239237923563, 0.15170811906470016, 0.15917420701124124, 0.1507374739463191, 0.16185958608340806, 0.17358724150445853, 0.1432097232210324, 0.15361139276111027, 0.17697894522427107, 0.16249418784623276, 0.17444870362655515, 0.15007363052453154], 'val_acc': [0.7577997744643529, 0.7519107881217892, 0.7530384663575993, 0.7765944117278537, 0.8160631499812053, 0.8334795138453828, 0.8362360606440296, 0.8371131437163263, 0.8606690890865806, 0.8619220649041474, 0.8715699786994111, 0.8724470617717078, 0.8641774213757675, 0.8716952762811677, 0.8744518230798145, 0.8673098609196842, 0.8802155118406215, 0.876205989224408, 0.8777095602054881, 0.8848515223656184, 0.8777095602054881, 0.8699411101365744, 0.8904899135446686, 0.8840997368750783, 0.8897381280541286, 0.8872321764189951, 0.8931211627615587, 0.8891116401453452, 0.8917428893622353, 0.8767071795514346, 0.891868186943992, 0.8884851522365619, 0.8867309860919684, 0.892995865179802, 0.8919934845257487, 0.8856033078561584, 0.8728229545169778, 0.875454203733868, 0.8892369377271019, 0.8770830722967047, 0.8838491417115649, 0.8901140207993986, 0.8759553940608946, 0.8819696779852149, 0.8889863425635885, 0.8760806916426513, 0.8902393183811552, 0.8800902142588648, 0.885352712692645, 0.8833479513845383, 0.8749530134068413, 0.8877333667460218, 0.8742012279163012, 0.8606690890865806, 0.8916175917804786, 0.8767071795514346, 0.8577872447061772, 0.8681869439919809, 0.8619220649041474, 0.8802155118406215], 'final_model_size_bytes': 116791, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00011756010900231844, 'batch_size': 64, 'epochs': 60, 'base_channels': 12, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.20416591474634455, 'weight_decay': 1.8862985330373645e-05, 'gamma_focal': 2.502385968839412, 'effective_beta': 0.9862712665706926, 'label_smoothing': 0.08483967676822196, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.07515400245820475, 'smote_max_multiplier': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 2}, 'model_parameter_count': 26083, 'model_storage_size_kb': 112.07539062500001, 'model_size_validation': 'PASS'}
2025-10-03 01:15:23,365 - INFO - _models.training_function_executor - BO Objective: base=0.8802, size_penalty=0.0000, final=0.8802
2025-10-03 01:15:23,365 - INFO - _models.training_function_executor - Model: 26,083 parameters, 112.1KB (PASS 256KB limit)
2025-10-03 01:15:23,365 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 206.027s
2025-10-03 01:15:23,453 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8802
2025-10-03 01:15:23,453 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.088s
2025-10-03 01:15:23,453 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 0.00011756010900231844, 'batch_size': np.int64(64), 'epochs': np.int64(60), 'base_channels': np.int64(12), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.20416591474634455, 'weight_decay': 1.8862985330373645e-05, 'gamma_focal': 2.502385968839412, 'effective_beta': 0.9862712665706926, 'label_smoothing': 0.08483967676822196, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.07515400245820475, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(2)}, value=0.8802
2025-10-03 01:15:23,453 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 0.00011756010900231844, 'batch_size': np.int64(64), 'epochs': np.int64(60), 'base_channels': np.int64(12), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.20416591474634455, 'weight_decay': 1.8862985330373645e-05, 'gamma_focal': 2.502385968839412, 'effective_beta': 0.9862712665706926, 'label_smoothing': 0.08483967676822196, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.07515400245820475, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(2)} -> 0.8802
2025-10-03 01:15:23,453 - INFO - bo.run_bo - ðŸ”BO Trial 5: Using RF surrogate + Expected Improvement
2025-10-03 01:15:23,453 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 01:15:23,453 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:15:23,453 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:15:23,453 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00014715142264888387, 'batch_size': 512, 'epochs': 31, 'base_channels': 10, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.40423286610155945, 'weight_decay': 0.007814906122009628, 'gamma_focal': 1.237249063557337, 'effective_beta': 0.9549204596902404, 'label_smoothing': 0.0533582618252394, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.27507782676310144, 'smote_max_multiplier': 1, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 01:15:23,455 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00014715142264888387, 'batch_size': 512, 'epochs': 31, 'base_channels': 10, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.40423286610155945, 'weight_decay': 0.007814906122009628, 'gamma_focal': 1.237249063557337, 'effective_beta': 0.9549204596902404, 'label_smoothing': 0.0533582618252394, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.27507782676310144, 'smote_max_multiplier': 1, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 01:17:35,792 - INFO - _models.training_function_executor - Model: 3,734 parameters, 4.0KB storage
2025-10-03 01:17:35,792 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8996590379485173, 0.6490973091433939, 0.6069353057175835, 0.5904274321612912, 0.5754820223074638, 0.5627545198805555, 0.5535732737914462, 0.5461809748497627, 0.5396133172873533, 0.5333903559576624, 0.5265989356335514, 0.5199969390805896, 0.5124171807798599, 0.5050855795811267, 0.4969552545676243, 0.4916743297208406, 0.4858842116323393, 0.4794039765093794, 0.47217464135862947, 0.46448426295487394, 0.45521543493539146, 0.4480665323719197, 0.44128724715055667, 0.4338317523109649, 0.42802450453410174, 0.42092028032626905, 0.41389556139071804, 0.4077226291523091, 0.4015777106833735, 0.3947214313231625, 0.3905349395326873], 'val_losses': [0.6510756820339471, 0.6145665850679374, 0.6048473240125002, 0.5957003864126537, 0.5847551617582345, 0.5829996224887208, 0.5776892167600888, 0.5764094644240487, 0.5672401575169278, 0.5648713079688992, 0.5601338415557527, 0.5536496637832728, 0.5476181766933373, 0.5433554151664326, 0.5332077249311412, 0.5250385362370662, 0.5173294379319513, 0.5081486886096709, 0.49822756903106713, 0.4889397059350575, 0.48152614667845495, 0.47849311044141174, 0.47339273596180365, 0.4708223333672135, 0.47034805461572027, 0.4750259366344351, 0.47340299966729327, 0.4793482633845995, 0.49308445665445294, 0.48935425324840903, 0.49744109085531946], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7184563337927578, 0.7150732990853277, 0.714572108758301, 0.7179551434657311, 0.7227164515724847, 0.7308607943866683, 0.7339932339305851, 0.7380027565467987, 0.7413857912542288, 0.7422628743265255, 0.7426387670717955, 0.7440170404711189, 0.7460218017792256, 0.745520611452199, 0.7468988848515223, 0.7480265630873324, 0.7491542413231425, 0.7485277534143591, 0.747400075178549, 0.7490289437413858, 0.7461470993609823, 0.7445182307981456, 0.7370003758927453, 0.7326149605312617, 0.7259741886981581, 0.7217140709184313, 0.7239694273900513, 0.7247212128805914], 'final_model_size_bytes': 32131, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00014715142264888387, 'batch_size': 512, 'epochs': 31, 'base_channels': 10, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.40423286610155945, 'weight_decay': 0.007814906122009628, 'gamma_focal': 1.237249063557337, 'effective_beta': 0.9549204596902404, 'label_smoothing': 0.0533582618252394, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.27507782676310144, 'smote_max_multiplier': 1, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}, 'model_parameter_count': 3734, 'model_storage_size_kb': 4.0111328125000005, 'model_size_validation': 'PASS'}
2025-10-03 01:17:35,792 - INFO - _models.training_function_executor - BO Objective: base=0.7247, size_penalty=0.0000, final=0.7247
2025-10-03 01:17:35,792 - INFO - _models.training_function_executor - Model: 3,734 parameters, 4.0KB (PASS 256KB limit)
2025-10-03 01:17:35,792 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 132.339s
2025-10-03 01:17:35,881 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7247
2025-10-03 01:17:35,881 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.089s
2025-10-03 01:17:35,881 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 0.00014715142264888387, 'batch_size': np.int64(512), 'epochs': np.int64(31), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.40423286610155945, 'weight_decay': 0.007814906122009628, 'gamma_focal': 1.237249063557337, 'effective_beta': 0.9549204596902404, 'label_smoothing': 0.0533582618252394, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.27507782676310144, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)}, value=0.7247
2025-10-03 01:17:35,881 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 0.00014715142264888387, 'batch_size': np.int64(512), 'epochs': np.int64(31), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.40423286610155945, 'weight_decay': 0.007814906122009628, 'gamma_focal': 1.237249063557337, 'effective_beta': 0.9549204596902404, 'label_smoothing': 0.0533582618252394, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.27507782676310144, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)} -> 0.7247
2025-10-03 01:17:35,881 - INFO - bo.run_bo - ðŸ”BO Trial 6: Using RF surrogate + Expected Improvement
2025-10-03 01:17:35,881 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 01:17:35,882 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:17:35,882 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:17:35,882 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 6.296631971841532e-05, 'batch_size': 64, 'epochs': 75, 'base_channels': 6, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.2699371835246243, 'weight_decay': 0.0002202950611696494, 'gamma_focal': 1.1300007751870624, 'effective_beta': 0.9240474375323064, 'label_smoothing': 0.07522206481810087, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.34949813905462046, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 1}
2025-10-03 01:17:35,883 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 6.296631971841532e-05, 'batch_size': 64, 'epochs': 75, 'base_channels': 6, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.2699371835246243, 'weight_decay': 0.0002202950611696494, 'gamma_focal': 1.1300007751870624, 'effective_beta': 0.9240474375323064, 'label_smoothing': 0.07522206481810087, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.34949813905462046, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 1}
2025-10-03 01:19:59,086 - INFO - _models.training_function_executor - Model: 13,159 parameters, 28.3KB storage
2025-10-03 01:19:59,086 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8064048921830769, 0.6115673494750506, 0.5823839224098477, 0.5667754630133857, 0.5564497563170747, 0.5456766571141356, 0.5374266106616777, 0.5290328773386298, 0.5170692167806248, 0.5067945044065625, 0.4999256565510112, 0.49552331010198114, 0.49356364068170766, 0.48818892511259293, 0.4859193333373894, 0.481703787622896, 0.4756817660325602, 0.47434788606532274, 0.4685351580462614, 0.46481156375695487, 0.46057510424055365, 0.45350421879840697, 0.44850540755909607, 0.44505063730071437, 0.4405560365299554, 0.4371049366829535, 0.43126162687489306, 0.43084315908727044, 0.42798326026059086, 0.42285907543086804, 0.4194368073296712, 0.4200984588602344, 0.4162683082198983, 0.4131280793160591, 0.40922708679656356, 0.40661180628065635, 0.4049464777484034, 0.40270485356368396, 0.3997107018544419, 0.39611135249026147, 0.3969972298710141, 0.3925808410732959, 0.39011685121751377, 0.3869740410533401, 0.38526471969168147, 0.38324688264228224, 0.3792599896313579, 0.3791577898443662, 0.37602447791083565, 0.3748152628095214, 0.3718467548770108, 0.36976817029275993, 0.3660056051207011, 0.3654801370569314, 0.3648197392432182, 0.36489161452348456, 0.36206900943503006, 0.3589765009157334, 0.35774263001412887, 0.35641114756964887, 0.35524952569331414, 0.35290785540921404, 0.3513658763292615, 0.3502552368441639, 0.3484403994839371, 0.34667007889069623, 0.34681031018279906, 0.34563688587635283, 0.34435546450396026, 0.3421184963614698, 0.3403508317654277, 0.3419689970017525, 0.3380035444432675, 0.3385234929783894, 0.33662677159252774], 'val_losses': [0.6299741591834854, 0.6013647934545358, 0.5828010289834236, 0.569602138120688, 0.5622644736278088, 0.5519760245780063, 0.5521434047454851, 0.5371593728338533, 0.5280248100014352, 0.5279085223663721, 0.506349086653184, 0.5033528266612068, 0.5040535011725862, 0.5013631944286839, 0.49415075500392924, 0.5099906278737968, 0.4988619815868597, 0.5025751037438432, 0.5028890764454883, 0.4996346871799999, 0.48488427555033803, 0.4922089478102054, 0.48383895509703956, 0.4879339819242147, 0.4910442435290338, 0.4886367794580439, 0.49444454539062654, 0.49843392673029874, 0.4987469441967491, 0.48529508175922803, 0.4923000202421227, 0.48720773408891205, 0.49537540355909887, 0.49035898631056096, 0.4934423629195898, 0.5030750931429543, 0.48824327919749355, 0.4942665573589008, 0.5048905739679983, 0.47859261032110345, 0.483282565420813, 0.4919715956620832, 0.47089909805927105, 0.45838705127945134, 0.4741229336037102, 0.47771289805364614, 0.4775615591906378, 0.47570306558221254, 0.4674548200349016, 0.4697226252775117, 0.4743950413165423, 0.47190503019346147, 0.47924107956925127, 0.47721826741904816, 0.4936985545156295, 0.48394855698193695, 0.4750780064335833, 0.47994366839346975, 0.47255640468073434, 0.4776819376229433, 0.502696657932099, 0.47687456900875397, 0.49569392388005284, 0.4685598443607805, 0.4825728660895851, 0.4767676372192599, 0.48711474994686427, 0.4882951876249637, 0.47617122707189496, 0.4519488962128891, 0.502422115637267, 0.4586863937742085, 0.4759890031783208, 0.4857936329096873, 0.5105640603335126], 'val_acc': [0.7198346071920811, 0.7155744894123544, 0.7282295451697782, 0.7364991855657186, 0.7356221024934219, 0.730234306477885, 0.7282295451697782, 0.730234306477885, 0.7266006766069415, 0.728354842751535, 0.7334920436035585, 0.7329908532765318, 0.7380027565467987, 0.7446435283799022, 0.7447688259616589, 0.7334920436035585, 0.7417616839994988, 0.7421375767447689, 0.7319884726224783, 0.7318631750407217, 0.7427640646535522, 0.7361232928204485, 0.7402581130184187, 0.7391304347826086, 0.7361232928204485, 0.7347450194211251, 0.7342438290940985, 0.7313619847136951, 0.7351209121663952, 0.7395063275278787, 0.7357474000751786, 0.7395063275278787, 0.7377521613832853, 0.7395063275278787, 0.7411351960907154, 0.7364991855657186, 0.7452700162886856, 0.7411351960907154, 0.7403834106001754, 0.7530384663575993, 0.7522866808670593, 0.7442676356346323, 0.7576744768825961, 0.7638140583886731, 0.7611828091717829, 0.7552938228292194, 0.762686380152863, 0.7591780478636763, 0.7609322140082696, 0.7650670342062398, 0.7693271519859667, 0.7663200100238066, 0.7671970930961033, 0.7629369753163764, 0.7590527502819195, 0.7630622728981331, 0.7687006640771833, 0.76957774714948, 0.776469114146097, 0.7698283423129934, 0.7571732865555695, 0.7723342939481268, 0.7594286430271896, 0.7819822077433906, 0.7664453076055632, 0.7737125673474502, 0.77032953264002, 0.7659441172785365, 0.7747149480015035, 0.7874953013406841, 0.7580503696278662, 0.7809798270893372, 0.7784738754542038, 0.7689512592406966, 0.752036085703546], 'final_model_size_bytes': 34715, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 6.296631971841532e-05, 'batch_size': 64, 'epochs': 75, 'base_channels': 6, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.2699371835246243, 'weight_decay': 0.0002202950611696494, 'gamma_focal': 1.1300007751870624, 'effective_beta': 0.9240474375323064, 'label_smoothing': 0.07522206481810087, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.34949813905462046, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 1}, 'model_parameter_count': 13159, 'model_storage_size_kb': 28.271289062500003, 'model_size_validation': 'PASS'}
2025-10-03 01:19:59,086 - INFO - _models.training_function_executor - BO Objective: base=0.7520, size_penalty=0.0000, final=0.7520
2025-10-03 01:19:59,086 - INFO - _models.training_function_executor - Model: 13,159 parameters, 28.3KB (PASS 256KB limit)
2025-10-03 01:19:59,086 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 143.205s
2025-10-03 01:19:59,173 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7520
2025-10-03 01:19:59,173 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.087s
2025-10-03 01:19:59,173 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 6.296631971841532e-05, 'batch_size': np.int64(64), 'epochs': np.int64(75), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.2699371835246243, 'weight_decay': 0.0002202950611696494, 'gamma_focal': 1.1300007751870624, 'effective_beta': 0.9240474375323064, 'label_smoothing': 0.07522206481810087, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.34949813905462046, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(1)}, value=0.7520
2025-10-03 01:19:59,173 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 6.296631971841532e-05, 'batch_size': np.int64(64), 'epochs': np.int64(75), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.2699371835246243, 'weight_decay': 0.0002202950611696494, 'gamma_focal': 1.1300007751870624, 'effective_beta': 0.9240474375323064, 'label_smoothing': 0.07522206481810087, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.34949813905462046, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(1)} -> 0.7520
2025-10-03 01:19:59,173 - INFO - bo.run_bo - ðŸ”BO Trial 7: Using RF surrogate + Expected Improvement
2025-10-03 01:19:59,173 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 01:19:59,173 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:19:59,173 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:19:59,173 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0010960543186601737, 'batch_size': 64, 'epochs': 77, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.08805016404232004, 'weight_decay': 2.0981614600038584e-05, 'gamma_focal': 2.7137676946765574, 'effective_beta': 0.9620847848970309, 'label_smoothing': 0.09046613006459925, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.17914592475884517, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 01:19:59,174 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0010960543186601737, 'batch_size': 64, 'epochs': 77, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.08805016404232004, 'weight_decay': 2.0981614600038584e-05, 'gamma_focal': 2.7137676946765574, 'effective_beta': 0.9620847848970309, 'label_smoothing': 0.09046613006459925, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.17914592475884517, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 01:23:52,440 - INFO - _models.training_function_executor - Model: 11,259 parameters, 48.4KB storage
2025-10-03 01:23:52,440 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.36295851130284534, 0.23334564108260317, 0.19850015912040728, 0.17779352532663642, 0.1603242057350211, 0.14424052269397997, 0.1331024367047713, 0.12467270762299021, 0.11654635904230111, 0.11048166826933428, 0.10548949587631193, 0.09905236180436355, 0.09576907181141121, 0.09119159468774374, 0.0886235542191149, 0.08626702088468473, 0.08319168196722358, 0.08235193109350361, 0.07977753769446198, 0.0763368606375121, 0.0759016346599323, 0.07363882905711565, 0.07186304446506848, 0.07048418550814052, 0.06952699139514758, 0.06907966761183712, 0.0655766531876986, 0.06608221322729028, 0.06501110909055156, 0.06428363382623467, 0.06301797593861966, 0.061918558837203604, 0.06066002043853238, 0.05924397904688097, 0.05925234788743055, 0.057897478403345796, 0.05768083511710679, 0.05825983764422842, 0.055956030545228186, 0.05543081536438188, 0.05507038971204985, 0.05368575745508071, 0.0541010410127816, 0.05221900489301419, 0.05225383024172319, 0.052886716276426764, 0.05081589322348143, 0.050135003699370906, 0.049777470910479835, 0.04903025792450074, 0.0498963991066706, 0.048710742161013246, 0.04805290904110314, 0.048946094788039615, 0.047049205411475496, 0.04743682446324676, 0.0463800265764766, 0.046005256666182215, 0.04600837828290337, 0.04580213774329119, 0.044353118731986815, 0.04427157191449927, 0.04318739716169527, 0.04382855162843904, 0.04263852621605291, 0.042991131819916005, 0.04433373029081425, 0.04234182744759972, 0.0428193228265659, 0.04182600332018782, 0.041835671869248324, 0.041457795613473375, 0.04027413586464199, 0.04039009587637664, 0.040890777377838555, 0.03978586634231377, 0.04148519442308324], 'val_losses': [0.19968490124069918, 0.16387857436805184, 0.14803823639375346, 0.13125802265953232, 0.1130125688064772, 0.11607103499161123, 0.09714994639206793, 0.07980292874191537, 0.09317361003787068, 0.08143642918722416, 0.08143961039002924, 0.08596923917834434, 0.07663627366921967, 0.07913558262584353, 0.06798333153193972, 0.08569048854428236, 0.06693234345530855, 0.07793935001561134, 0.07320439855383387, 0.06962498670165931, 0.0659729585129954, 0.06262104815370889, 0.06168029119295967, 0.0595087977694533, 0.05670551746213605, 0.04968695007680039, 0.06647664791374723, 0.06710153344265725, 0.0563883022539987, 0.05392217311516634, 0.05585908360733542, 0.05499466463630323, 0.056335022494938124, 0.05490077376111739, 0.05069207019759455, 0.0524549099205132, 0.05129865988344048, 0.06195073824503296, 0.05321659041502589, 0.05969652710997092, 0.049253988106916044, 0.05893752559722627, 0.05038574258048289, 0.05614170891485661, 0.05625270812482467, 0.05195653536589787, 0.05199836032172669, 0.053951936423995116, 0.04901597687694277, 0.060790692736011835, 0.051746546369815676, 0.05252029692812262, 0.054328740823091144, 0.057262890245319685, 0.04555538703536139, 0.056221326984172926, 0.04777739041768105, 0.05807861585864117, 0.054996447061449794, 0.05672046844914599, 0.04952469773010836, 0.04995757345679171, 0.061084158626165536, 0.043441161093235675, 0.050209808122385804, 0.05150450381654917, 0.052549195384272746, 0.0544394298710182, 0.05336170410089752, 0.047459674027536376, 0.0622653172798002, 0.046244636403933274, 0.048320120337044976, 0.0497184186107741, 0.05212366667688976, 0.0495632546527346, 0.05014753512699283], 'val_acc': [0.8163137451447187, 0.8646786117027941, 0.8777095602054881, 0.878085452950758, 0.9026437789750658, 0.8906152111264253, 0.9170530008770831, 0.9320887106878837, 0.9137952637514096, 0.9322140082696404, 0.9295827590527502, 0.9239443678737, 0.9373512091216639, 0.9299586517980203, 0.9443678737000376, 0.9215637138203233, 0.9421125172284175, 0.9314622227791004, 0.9349705550682872, 0.9424884099736875, 0.9389800776845006, 0.9468738253351711, 0.9477509084074678, 0.9463726350081444, 0.953389299586518, 0.9567723342939481, 0.9376018042851773, 0.9372259115399073, 0.9492544793885478, 0.9568976318757048, 0.9485026938980078, 0.954516977822328, 0.9536398947500313, 0.951885728605438, 0.9561458463851648, 0.9570229294574615, 0.9561458463851648, 0.9419872196466608, 0.9527628116777346, 0.9481268011527377, 0.9610324520736749, 0.9505074552061146, 0.9596541786743515, 0.9512592406966546, 0.9491291818067912, 0.961784237564215, 0.9551434657311114, 0.9556446560581381, 0.961408344818945, 0.9416113269013908, 0.9605312617466483, 0.9546422754040848, 0.9560205488034081, 0.952637514095978, 0.9610324520736749, 0.9492544793885478, 0.961408344818945, 0.9548928705675981, 0.953389299586518, 0.9491291818067912, 0.9602806665831349, 0.9607818569101616, 0.9527628116777346, 0.9640395940358351, 0.962160130309485, 0.9551434657311114, 0.9589023931838115, 0.9577747149480015, 0.9531387044230046, 0.9622854278912417, 0.9404836486655808, 0.9611577496554317, 0.9630372133817817, 0.9601553690013783, 0.9562711439669215, 0.9572735246209748, 0.9584012028567849], 'final_model_size_bytes': 53339, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0010960543186601737, 'batch_size': 64, 'epochs': 77, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.08805016404232004, 'weight_decay': 2.0981614600038584e-05, 'gamma_focal': 2.7137676946765574, 'effective_beta': 0.9620847848970309, 'label_smoothing': 0.09046613006459925, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.17914592475884517, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}, 'model_parameter_count': 11259, 'model_storage_size_kb': 48.378515625000006, 'model_size_validation': 'PASS'}
2025-10-03 01:23:52,440 - INFO - _models.training_function_executor - BO Objective: base=0.9584, size_penalty=0.0000, final=0.9584
2025-10-03 01:23:52,440 - INFO - _models.training_function_executor - Model: 11,259 parameters, 48.4KB (PASS 256KB limit)
2025-10-03 01:23:52,440 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 233.267s
2025-10-03 01:23:52,528 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9584
2025-10-03 01:23:52,529 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.088s
2025-10-03 01:23:52,529 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.0010960543186601737, 'batch_size': np.int64(64), 'epochs': np.int64(77), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.08805016404232004, 'weight_decay': 2.0981614600038584e-05, 'gamma_focal': 2.7137676946765574, 'effective_beta': 0.9620847848970309, 'label_smoothing': 0.09046613006459925, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.17914592475884517, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)}, value=0.9584
2025-10-03 01:23:52,529 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.0010960543186601737, 'batch_size': np.int64(64), 'epochs': np.int64(77), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.08805016404232004, 'weight_decay': 2.0981614600038584e-05, 'gamma_focal': 2.7137676946765574, 'effective_beta': 0.9620847848970309, 'label_smoothing': 0.09046613006459925, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.17914592475884517, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)} -> 0.9584
2025-10-03 01:23:52,529 - INFO - bo.run_bo - ðŸ”BO Trial 8: Using RF surrogate + Expected Improvement
2025-10-03 01:23:52,529 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 01:23:52,529 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:23:52,529 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:23:52,529 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0161888211084902e-05, 'batch_size': 256, 'epochs': 91, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.06279102569065716, 'weight_decay': 1.6895440344798236e-05, 'gamma_focal': 1.0126575465740009, 'effective_beta': 0.9887609679221783, 'label_smoothing': 0.06600143259310934, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.1306662813212453, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 2}
2025-10-03 01:23:52,530 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0161888211084902e-05, 'batch_size': 256, 'epochs': 91, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.06279102569065716, 'weight_decay': 1.6895440344798236e-05, 'gamma_focal': 1.0126575465740009, 'effective_beta': 0.9887609679221783, 'label_smoothing': 0.06600143259310934, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.1306662813212453, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 2}
2025-10-03 01:51:24,708 - INFO - _models.training_function_executor - Model: 26,162 parameters, 112.4KB storage
2025-10-03 01:51:24,708 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.303018662793347, 0.9880918147123449, 0.9156301062380748, 0.8974573835545515, 0.8878871269404225, 0.8799973675427459, 0.8718532194986726, 0.8622616048495593, 0.848651654724683, 0.8301773756024853, 0.8103812815831272, 0.7922405963092833, 0.777261245670335, 0.7646345516129558, 0.7536342149933656, 0.743242774729517, 0.7331109484011019, 0.7235350849084985, 0.7131648277852798, 0.7024381164340295, 0.6932592187106976, 0.6828347055730005, 0.6728314413531091, 0.6619366217980232, 0.650792136001556, 0.6390937295089774, 0.6259455615263394, 0.6142567138913357, 0.6034418897036149, 0.5923289063875058, 0.5826793489504061, 0.5739267937707896, 0.5654635058920889, 0.5581845500443205, 0.5512111122491563, 0.5439663200061248, 0.5364235255723637, 0.5300290974848807, 0.5226997908564008, 0.5155696742556005, 0.5084527271469189, 0.5015583947041858, 0.49563410758148346, 0.4890880430089695, 0.48390934529359597, 0.4777132126234891, 0.4719852680888885, 0.46825536013602076, 0.4630295261909632, 0.45910207724329233, 0.45419291235267595, 0.4500382038278027, 0.44653173425441917, 0.44247573009127866, 0.4391227177668179, 0.4369722731092066, 0.4315167105509413, 0.4283010023527894, 0.42593036779457855, 0.42199026586401606, 0.41925150511615183, 0.41620697468380835, 0.41412622112394243, 0.4113780852946642, 0.4078244874208615, 0.40534295760688044, 0.40227791013077646, 0.4008512601341649, 0.3978319079240575, 0.3968067447196432, 0.39371507516137444, 0.3929638848032116, 0.3902671031059347, 0.3875834138865039, 0.3848919287965952, 0.3840036490169229, 0.38121978092716213, 0.3806676060466552, 0.37831860580553495, 0.3764387591265407, 0.3748454833156848, 0.3728624862164095, 0.3723913853954463, 0.3707497491555677, 0.3686112004291315, 0.3673732122564249, 0.3655321472806635, 0.3636259037086972, 0.36254313614852174, 0.36213573017293876, 0.3608446585644194], 'val_losses': [1.0146457802501034, 0.7920230207191107, 0.7332970104171525, 0.7116610437403823, 0.70120777135773, 0.6942082734398339, 0.6882352462434452, 0.6799531091635216, 0.668033890229182, 0.6512086542981144, 0.6378175976237264, 0.6266111792769442, 0.6156506694404273, 0.6094193722002041, 0.6021833096421642, 0.5941808903005216, 0.5877034546490705, 0.5843620142711462, 0.5739071244599601, 0.5723365652853946, 0.5621251812399128, 0.5540187529245872, 0.5501765418830872, 0.5414528103605755, 0.532470057425441, 0.5249519265208862, 0.5232346136869307, 0.5133653724870203, 0.5018200964508551, 0.4969501392634616, 0.4935066674701434, 0.4848768520797349, 0.48505143613433527, 0.4749341209069811, 0.46679636755767523, 0.46337691263856184, 0.4560725443861476, 0.4510779690061382, 0.4501089030355607, 0.4381919191783121, 0.4320635934840048, 0.4254551000903385, 0.4268575888045404, 0.41981710568807906, 0.4174856635961388, 0.4147359356032086, 0.4052285445302579, 0.40080086589457026, 0.4033125861442981, 0.3958874966077765, 0.39588876910084786, 0.39004090301764366, 0.3829704919019419, 0.3842439557127062, 0.3783696281358781, 0.3790193047720363, 0.3758699960158054, 0.36960042960630923, 0.3674483108887949, 0.36371921920534395, 0.3664455996795251, 0.3623114245911168, 0.35453104351101894, 0.3522720782959346, 0.3474073069254836, 0.3506253925186547, 0.3508464474865345, 0.34649071628744776, 0.34216012655680467, 0.33840000354964367, 0.34609441496333804, 0.3318918168865907, 0.3324263409487721, 0.3340727504916455, 0.33495086205213265, 0.3278506412689525, 0.32736847418514564, 0.32521952640336643, 0.32340799856642977, 0.3273532830403721, 0.3214708612670487, 0.3221709891199662, 0.32013516030414946, 0.3230863229110566, 0.31956224318077026, 0.3133975415133605, 0.31052934550623623, 0.3133310236745334, 0.3257736310319873, 0.3118431867177199, 0.31211915860168915], 'val_acc': [0.7002881844380403, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7199599047738379, 0.7198346071920811, 0.7195840120285678, 0.7244706177170781, 0.7301090088961283, 0.7272271645157249, 0.7250971056258614, 0.7271018669339682, 0.7296078185691016, 0.7347450194211251, 0.7392557323643654, 0.7375015662197719, 0.7426387670717955, 0.7426387670717955, 0.7462723969427391, 0.7510337050494925, 0.7565467986467861, 0.7638140583886731, 0.7680741761683999, 0.7713319132940735, 0.7709560205488034, 0.7719584012028567, 0.7724595915298835, 0.7694524495677233, 0.76694649793259, 0.7717078060393434, 0.7681994737501566, 0.7737125673474502, 0.7755920310738003, 0.7779726851271771, 0.7848640521237941, 0.7897506578123042, 0.791379526375141, 0.7987720836987846, 0.8016539280791881, 0.8069164265129684, 0.8042851772960782, 0.8064152361859416, 0.8109259491291818, 0.8084199974940484, 0.8126801152737753, 0.8148101741636387, 0.8128054128555319, 0.8178173161257988, 0.8140583886730987, 0.8188196967798521, 0.8267134444305225, 0.8224533266507956, 0.8268387420122791, 0.8272146347575492, 0.8254604686129557, 0.8312241573737627, 0.8329783235183561, 0.8313494549555194, 0.8312241573737627, 0.8319759428643028, 0.8369878461345696, 0.837990226788623, 0.8391179050244331, 0.8374890364615963, 0.8366119533892996, 0.8396190953514597, 0.8388673098609197, 0.8419997494048365, 0.8337301090088961, 0.8441298082946999, 0.8437539155494299, 0.8428768324771332, 0.8408720711690264, 0.84488159378524, 0.84488159378524, 0.8457586768575367, 0.8468863550933466, 0.8451321889487533, 0.8477634381656434, 0.8471369502568601, 0.8500187946372635, 0.8471369502568601, 0.8493923067284801, 0.8530259365994236, 0.8559077809798271, 0.8541536148352337, 0.8471369502568601, 0.8554065906528004, 0.8541536148352337], 'final_model_size_bytes': 182669, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0161888211084902e-05, 'batch_size': 256, 'epochs': 91, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.06279102569065716, 'weight_decay': 1.6895440344798236e-05, 'gamma_focal': 1.0126575465740009, 'effective_beta': 0.9887609679221783, 'label_smoothing': 0.06600143259310934, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.1306662813212453, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 2}, 'model_parameter_count': 26162, 'model_storage_size_kb': 112.41484375, 'model_size_validation': 'PASS'}
2025-10-03 01:51:24,708 - INFO - _models.training_function_executor - BO Objective: base=0.8542, size_penalty=0.0000, final=0.8542
2025-10-03 01:51:24,708 - INFO - _models.training_function_executor - Model: 26,162 parameters, 112.4KB (PASS 256KB limit)
2025-10-03 01:51:24,708 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 1652.179s
2025-10-03 01:51:24,799 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8542
2025-10-03 01:51:24,799 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.091s
2025-10-03 01:51:24,799 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 1.0161888211084902e-05, 'batch_size': np.int64(256), 'epochs': np.int64(91), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(3), 'dropout': 0.06279102569065716, 'weight_decay': 1.6895440344798236e-05, 'gamma_focal': 1.0126575465740009, 'effective_beta': 0.9887609679221783, 'label_smoothing': 0.06600143259310934, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.1306662813212453, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(2)}, value=0.8542
2025-10-03 01:51:24,799 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 1.0161888211084902e-05, 'batch_size': np.int64(256), 'epochs': np.int64(91), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(3), 'dropout': 0.06279102569065716, 'weight_decay': 1.6895440344798236e-05, 'gamma_focal': 1.0126575465740009, 'effective_beta': 0.9887609679221783, 'label_smoothing': 0.06600143259310934, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.1306662813212453, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(2)} -> 0.8542
2025-10-03 01:51:24,799 - INFO - bo.run_bo - ðŸ”BO Trial 9: Using RF surrogate + Expected Improvement
2025-10-03 01:51:24,799 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 01:51:24,799 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:51:24,799 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:51:24,799 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0004863135045413435, 'batch_size': 64, 'epochs': 85, 'base_channels': 13, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.1390611882707584, 'weight_decay': 0.0001670768779010366, 'gamma_focal': 2.753482686206805, 'effective_beta': 0.9095871097057731, 'label_smoothing': 0.04305539465574032, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.35794994042595346, 'smote_max_multiplier': 0, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 7}
2025-10-03 01:51:24,800 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0004863135045413435, 'batch_size': 64, 'epochs': 85, 'base_channels': 13, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.1390611882707584, 'weight_decay': 0.0001670768779010366, 'gamma_focal': 2.753482686206805, 'effective_beta': 0.9095871097057731, 'label_smoothing': 0.04305539465574032, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.35794994042595346, 'smote_max_multiplier': 0, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 7}
2025-10-03 01:55:06,257 - INFO - _models.training_function_executor - Model: 13,415 parameters, 57.6KB storage
2025-10-03 01:55:06,258 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3239284067747606, 0.22679082754989713, 0.18389559477307207, 0.15599628002561505, 0.14046268076469168, 0.12940845222165143, 0.12008794663112847, 0.11358857588325606, 0.10678538074237012, 0.10221024671782379, 0.09821080953173107, 0.09567439487434488, 0.09273703385304229, 0.09067948059723072, 0.08721467418669608, 0.08643560823532392, 0.08421412691148612, 0.082586489153374, 0.08184299418968848, 0.07830951440964666, 0.07921241977925782, 0.0778201892353503, 0.07546716122952066, 0.07564961806271558, 0.07427548875466432, 0.07306081953824652, 0.07196760825812583, 0.0713619169438165, 0.07021357246375534, 0.0696152503922625, 0.06981760418637166, 0.06946161451940856, 0.06831627548594219, 0.06741813215481451, 0.06694393476908157, 0.06480351848279169, 0.0661812044823188, 0.06500811212265906, 0.06374996394610766, 0.06452002247100362, 0.06291738475962277, 0.06117978697912894, 0.06218737091446559, 0.060922950416821095, 0.06078424660436858, 0.059530113009202114, 0.05890185061177152, 0.058737457590570044, 0.0591092103415928, 0.05903400587897602, 0.05878742520061683, 0.05795803380255987, 0.05718752140956998, 0.056689636580510905, 0.05654006286056606, 0.055460048285084954, 0.05566513546039201, 0.0558674344788586, 0.05521742048970411, 0.05431029858638334, 0.05433765812250537, 0.054623104946163445, 0.053160828094589524, 0.054554591648376546, 0.052271895788086484, 0.051913891995539065, 0.0521396174113163, 0.05014012886506926, 0.0515597641025303, 0.04996341685294644, 0.051613229831410326, 0.05091134904057681, 0.05006334487796558, 0.05051028371581688, 0.04866712037124412, 0.050152297941325724, 0.048380200027989116, 0.04884829145567058, 0.0491013339003948, 0.048563841939592366, 0.04925805573307833, 0.04879027760881094, 0.04676491728104018, 0.04775713123113904, 0.0476094712466641], 'val_losses': [0.2524159021387123, 0.19980416783091867, 0.1620622019938935, 0.1459902861955963, 0.14518109148993943, 0.1342774110003352, 0.11852407851265181, 0.11881154856090631, 0.10588294700816854, 0.10324607824057419, 0.1012832570562923, 0.10430165788849827, 0.09940519490355224, 0.09475562016761717, 0.10183519118449963, 0.0983875107737565, 0.09822037182174191, 0.10767060221772552, 0.09386292795626505, 0.09571123137358749, 0.10093567011880929, 0.0910788547061407, 0.09676661837625258, 0.0902463705815135, 0.0906752716507653, 0.09085189380930743, 0.08432147553795756, 0.087063102319775, 0.08343423942010395, 0.08782281223514178, 0.08546521320757958, 0.08879091295177605, 0.09118676155921347, 0.09034903170306377, 0.08624175529613813, 0.08521496599859618, 0.07761268507768122, 0.08807404456939513, 0.08288732413592277, 0.09037242180055999, 0.07522581106192067, 0.09825218441365073, 0.07215093366933367, 0.07879646680899065, 0.07889272917662776, 0.0861682413481274, 0.08202880590336317, 0.0707826826554793, 0.080505701897288, 0.08230781748045089, 0.07831291134995844, 0.08996967948546551, 0.0831665374752185, 0.07638690432925942, 0.08961107859581936, 0.07241386387442575, 0.08439635681920804, 0.07453932182305797, 0.08606049452056833, 0.08156294896853267, 0.07604635416968486, 0.06668621868683033, 0.08331064076489963, 0.08072533906477015, 0.07502883992918362, 0.08432321181916874, 0.08116066561891178, 0.07859922370812361, 0.10030431736271388, 0.07969719131568548, 0.08059950285859462, 0.07769887950124335, 0.08940000184204268, 0.07545245681067723, 0.08212938810101339, 0.08667246239882277, 0.08184651382404705, 0.07965054526101704, 0.09115894191091244, 0.08329759871073944, 0.08421213399301927, 0.07675316628097159, 0.07858831243412848, 0.07938508116023431, 0.07970970185153124], 'val_acc': [0.7485277534143591, 0.8159378523994487, 0.8532765317629369, 0.8719458714446812, 0.8782107505325147, 0.8893622353088585, 0.8963788998872322, 0.9003884225034456, 0.914045858914923, 0.9144217516601929, 0.9120410976068162, 0.9061521112642525, 0.9107881217892495, 0.9200601428392432, 0.9135446685878963, 0.914797644405463, 0.9069038967547927, 0.8996366370129056, 0.9149229419872197, 0.9144217516601929, 0.9023931838115524, 0.9114146096980328, 0.9165518105500564, 0.9107881217892495, 0.9056509209372259, 0.9131687758426262, 0.9199348452574865, 0.9201854404209999, 0.9188071670216765, 0.9243202606189701, 0.9183059766946498, 0.9178047863676231, 0.9225660944743767, 0.914045858914923, 0.9189324646034331, 0.9236937727101867, 0.9225660944743767, 0.9130434782608695, 0.9122916927703295, 0.9065280040095226, 0.9342187695777472, 0.8982583636135822, 0.9298333542162637, 0.9238190702919433, 0.9270768074176169, 0.9229419872196467, 0.9124169903520862, 0.931587520360857, 0.92319258238316, 0.9208119283297832, 0.9211878210750533, 0.9137952637514096, 0.9225660944743767, 0.9229419872196467, 0.9185565718581631, 0.9314622227791004, 0.9211878210750533, 0.929708056634507, 0.9087833604811427, 0.9277032953264002, 0.9274527001628868, 0.9424884099736875, 0.9255732364365368, 0.9215637138203233, 0.9303345445432903, 0.9152988347324896, 0.9139205613331662, 0.9267009146723468, 0.908658062899386, 0.9263250219270768, 0.9215637138203233, 0.9204360355845133, 0.9214384162385666, 0.9260744267635634, 0.9277032953264002, 0.9059015161007392, 0.9269515098358602, 0.9219396065655933, 0.9074050870818193, 0.9152988347324896, 0.9141711564966796, 0.9267009146723468, 0.92206490414735, 0.9261997243453202, 0.9194336549304598], 'final_model_size_bytes': 61915, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0004863135045413435, 'batch_size': 64, 'epochs': 85, 'base_channels': 13, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.1390611882707584, 'weight_decay': 0.0001670768779010366, 'gamma_focal': 2.753482686206805, 'effective_beta': 0.9095871097057731, 'label_smoothing': 0.04305539465574032, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.35794994042595346, 'smote_max_multiplier': 0, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 7}, 'model_parameter_count': 13415, 'model_storage_size_kb': 57.64257812500001, 'model_size_validation': 'PASS'}
2025-10-03 01:55:06,258 - INFO - _models.training_function_executor - BO Objective: base=0.9194, size_penalty=0.0000, final=0.9194
2025-10-03 01:55:06,258 - INFO - _models.training_function_executor - Model: 13,415 parameters, 57.6KB (PASS 256KB limit)
2025-10-03 01:55:06,258 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 221.459s
2025-10-03 01:55:06,350 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9194
2025-10-03 01:55:06,350 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-10-03 01:55:06,350 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 0.0004863135045413435, 'batch_size': np.int64(64), 'epochs': np.int64(85), 'base_channels': np.int64(13), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.1390611882707584, 'weight_decay': 0.0001670768779010366, 'gamma_focal': 2.753482686206805, 'effective_beta': 0.9095871097057731, 'label_smoothing': 0.04305539465574032, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.35794994042595346, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(7)}, value=0.9194
2025-10-03 01:55:06,350 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 0.0004863135045413435, 'batch_size': np.int64(64), 'epochs': np.int64(85), 'base_channels': np.int64(13), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.1390611882707584, 'weight_decay': 0.0001670768779010366, 'gamma_focal': 2.753482686206805, 'effective_beta': 0.9095871097057731, 'label_smoothing': 0.04305539465574032, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.35794994042595346, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(7)} -> 0.9194
2025-10-03 01:55:06,350 - INFO - bo.run_bo - ðŸ”BO Trial 10: Using RF surrogate + Expected Improvement
2025-10-03 01:55:06,350 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 01:55:06,350 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:55:06,350 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:55:06,351 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003258672790607511, 'batch_size': 64, 'epochs': 32, 'base_channels': 16, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.08261334294043161, 'weight_decay': 0.00017290907631982807, 'gamma_focal': 2.9693980538905023, 'effective_beta': 0.9773258136399281, 'label_smoothing': 0.08751402647062467, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.33108058830788284, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 6}
2025-10-03 01:55:06,352 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003258672790607511, 'batch_size': 64, 'epochs': 32, 'base_channels': 16, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.08261334294043161, 'weight_decay': 0.00017290907631982807, 'gamma_focal': 2.9693980538905023, 'effective_beta': 0.9773258136399281, 'label_smoothing': 0.08751402647062467, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.33108058830788284, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 6}
2025-10-03 01:56:37,325 - INFO - _models.training_function_executor - Model: 9,799 parameters, 42.1KB storage
2025-10-03 01:56:37,325 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3898974288059569, 0.2862403253239897, 0.22898973224799418, 0.19063975579264958, 0.16643887443907907, 0.14800384036455932, 0.1368207578119372, 0.12582052227022525, 0.11581907565920797, 0.11058150052804673, 0.10400892132999591, 0.09859882072569698, 0.09519956010134825, 0.09111377841052969, 0.08751822001331976, 0.0848914703267949, 0.08108822178163813, 0.07858097219557954, 0.07597692279485434, 0.0738908876187526, 0.07255837720173068, 0.07105863447546408, 0.06832818348712145, 0.06676549433534912, 0.06771282631504268, 0.06329858731113236, 0.062274117305387976, 0.061975643685582435, 0.059668808942095135, 0.05883480763549885, 0.05761204189630797, 0.0563851378504775], 'val_losses': [0.26354172031931106, 0.1748889216581632, 0.1372738722858834, 0.12566331683162937, 0.1096214177897962, 0.10013103392313573, 0.08281952099119148, 0.10175268494956641, 0.0836277232358232, 0.09423215595134561, 0.0735043642759816, 0.07998053548800857, 0.07716810842001592, 0.07513147893704474, 0.06656582098159705, 0.07427031657362235, 0.05977572584724295, 0.06542494966917028, 0.06559807325033837, 0.06574830719999257, 0.07517011496689427, 0.07164914663304603, 0.05403104914304921, 0.054416710512483646, 0.053267986831427847, 0.052233874930383686, 0.05566728526959517, 0.05539008351051035, 0.05601967208375908, 0.04765214175153923, 0.054455683169292037, 0.042248803653105835], 'val_acc': [0.715699786994111, 0.8319759428643028, 0.8635509334669841, 0.8876080691642652, 0.8978824708683122, 0.9089086580628993, 0.930459842125047, 0.9052750281919559, 0.9251973436912667, 0.9112893121162762, 0.9327151985966671, 0.9352211502318005, 0.9294574614709936, 0.9292068663074803, 0.937727101866934, 0.9319634131061271, 0.9451196591905776, 0.9412354341561208, 0.932339305851397, 0.9394812680115274, 0.9144217516601929, 0.9259491291818068, 0.9467485277534143, 0.9482520987344945, 0.9530134068412479, 0.9536398947500313, 0.9451196591905776, 0.9434907906277409, 0.9442425761182809, 0.9512592406966546, 0.944994361608821, 0.9582759052750282], 'final_model_size_bytes': 49862, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003258672790607511, 'batch_size': 64, 'epochs': 32, 'base_channels': 16, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.08261334294043161, 'weight_decay': 0.00017290907631982807, 'gamma_focal': 2.9693980538905023, 'effective_beta': 0.9773258136399281, 'label_smoothing': 0.08751402647062467, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.33108058830788284, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 6}, 'model_parameter_count': 9799, 'model_storage_size_kb': 42.105078125000006, 'model_size_validation': 'PASS'}
2025-10-03 01:56:37,326 - INFO - _models.training_function_executor - BO Objective: base=0.9583, size_penalty=0.0000, final=0.9583
2025-10-03 01:56:37,326 - INFO - _models.training_function_executor - Model: 9,799 parameters, 42.1KB (PASS 256KB limit)
2025-10-03 01:56:37,326 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 90.975s
2025-10-03 01:56:37,420 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9583
2025-10-03 01:56:37,420 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-10-03 01:56:37,420 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 0.0003258672790607511, 'batch_size': np.int64(64), 'epochs': np.int64(32), 'base_channels': np.int64(16), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.08261334294043161, 'weight_decay': 0.00017290907631982807, 'gamma_focal': 2.9693980538905023, 'effective_beta': 0.9773258136399281, 'label_smoothing': 0.08751402647062467, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.33108058830788284, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(6)}, value=0.9583
2025-10-03 01:56:37,420 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 0.0003258672790607511, 'batch_size': np.int64(64), 'epochs': np.int64(32), 'base_channels': np.int64(16), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.08261334294043161, 'weight_decay': 0.00017290907631982807, 'gamma_focal': 2.9693980538905023, 'effective_beta': 0.9773258136399281, 'label_smoothing': 0.08751402647062467, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.33108058830788284, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(6)} -> 0.9583
2025-10-03 01:56:37,420 - INFO - bo.run_bo - ðŸ”BO Trial 11: Using RF surrogate + Expected Improvement
2025-10-03 01:56:37,420 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 01:56:37,421 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:56:37,421 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:56:37,421 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00025103156465219255, 'batch_size': 128, 'epochs': 15, 'base_channels': 10, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.08277536151657579, 'weight_decay': 0.0002901217004935376, 'gamma_focal': 2.87638981769481, 'effective_beta': 0.9003158740650935, 'label_smoothing': 0.09280002182156129, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.2995140757214525, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 2}
2025-10-03 01:56:37,422 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00025103156465219255, 'batch_size': 128, 'epochs': 15, 'base_channels': 10, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.08277536151657579, 'weight_decay': 0.0002901217004935376, 'gamma_focal': 2.87638981769481, 'effective_beta': 0.9003158740650935, 'label_smoothing': 0.09280002182156129, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.2995140757214525, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 2}
2025-10-03 01:57:15,217 - INFO - _models.training_function_executor - Model: 13,311 parameters, 57.2KB storage
2025-10-03 01:57:15,217 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4562040544974101, 0.3279400111420946, 0.2738059856726085, 0.24396946902913574, 0.22516966690158793, 0.2131161234092656, 0.20281017844801102, 0.1940738327856591, 0.186933011646657, 0.1807839879017694, 0.17401160720979056, 0.16814025703286523, 0.16347858117770786, 0.15832256105103823, 0.1546586683426696], 'val_losses': [0.29418241642177473, 0.2438864609887764, 0.20989574019606588, 0.1912021656427194, 0.18173228752144804, 0.17576775082504073, 0.1726555024254906, 0.1652972235437354, 0.15199841136599884, 0.14643620975318372, 0.13856109968553343, 0.13351357913363804, 0.13978916392815083, 0.1311593234983397, 0.13146547895858884], 'val_acc': [0.7502819195589525, 0.7354968049116652, 0.7760932214008269, 0.8021551184062148, 0.8180679112893121, 0.8185691016163388, 0.8243327903771457, 0.8304723718832226, 0.8535271269264503, 0.8599173035960406, 0.869314622227791, 0.8772083698784613, 0.8684375391554943, 0.876957774714948, 0.8797143215135947], 'final_model_size_bytes': 61595, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00025103156465219255, 'batch_size': 128, 'epochs': 15, 'base_channels': 10, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.08277536151657579, 'weight_decay': 0.0002901217004935376, 'gamma_focal': 2.87638981769481, 'effective_beta': 0.9003158740650935, 'label_smoothing': 0.09280002182156129, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.2995140757214525, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 2}, 'model_parameter_count': 13311, 'model_storage_size_kb': 57.195703125, 'model_size_validation': 'PASS'}
2025-10-03 01:57:15,217 - INFO - _models.training_function_executor - BO Objective: base=0.8797, size_penalty=0.0000, final=0.8797
2025-10-03 01:57:15,217 - INFO - _models.training_function_executor - Model: 13,311 parameters, 57.2KB (PASS 256KB limit)
2025-10-03 01:57:15,217 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 37.797s
2025-10-03 01:57:15,312 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8797
2025-10-03 01:57:15,312 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-10-03 01:57:15,312 - INFO - bo.run_bo - Recorded observation #11: hparams={'lr': 0.00025103156465219255, 'batch_size': np.int64(128), 'epochs': np.int64(15), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.08277536151657579, 'weight_decay': 0.0002901217004935376, 'gamma_focal': 2.87638981769481, 'effective_beta': 0.9003158740650935, 'label_smoothing': 0.09280002182156129, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.2995140757214525, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(2)}, value=0.8797
2025-10-03 01:57:15,313 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 11: {'lr': 0.00025103156465219255, 'batch_size': np.int64(128), 'epochs': np.int64(15), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.08277536151657579, 'weight_decay': 0.0002901217004935376, 'gamma_focal': 2.87638981769481, 'effective_beta': 0.9003158740650935, 'label_smoothing': 0.09280002182156129, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.2995140757214525, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(2)} -> 0.8797
2025-10-03 01:57:15,313 - INFO - bo.run_bo - ðŸ”BO Trial 12: Using RF surrogate + Expected Improvement
2025-10-03 01:57:15,313 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 01:57:15,313 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 01:57:15,313 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 01:57:15,313 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 5.9018234520537745e-05, 'batch_size': 64, 'epochs': 96, 'base_channels': 15, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.0695170726451609, 'weight_decay': 0.00024135442126025424, 'gamma_focal': 1.6811332993889132, 'effective_beta': 0.995506133044267, 'label_smoothing': 0.059509985680484746, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.2843409747171856, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 01:57:15,314 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 5.9018234520537745e-05, 'batch_size': 64, 'epochs': 96, 'base_channels': 15, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.0695170726451609, 'weight_decay': 0.00024135442126025424, 'gamma_focal': 1.6811332993889132, 'effective_beta': 0.995506133044267, 'label_smoothing': 0.059509985680484746, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.2843409747171856, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 02:03:19,785 - INFO - _models.training_function_executor - Model: 26,517 parameters, 113.9KB storage
2025-10-03 02:03:19,785 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7023407913443594, 0.5290056970793391, 0.44929851834866513, 0.41777651223738854, 0.39684999312039565, 0.37762693587221613, 0.36357781786129034, 0.3502683039178623, 0.3365451988793919, 0.3265625593737366, 0.3169072764617746, 0.30425344697424034, 0.2929834633549877, 0.2833337089477682, 0.2731272129734883, 0.26573900242909465, 0.2577029897877836, 0.25112595823323236, 0.2451420961961044, 0.24083626754527152, 0.23473258189871332, 0.23109045651767768, 0.22681174702961754, 0.2236218799908039, 0.21872237527104824, 0.21859152544516944, 0.21709604484707828, 0.21190583802681273, 0.2108156812068308, 0.20973176254687867, 0.20493754904581105, 0.2036794214018221, 0.20119613415278564, 0.19742597062290287, 0.1981081067932608, 0.19399254519655457, 0.19406939345225396, 0.19083226518304136, 0.18848157121933987, 0.1857795410877088, 0.18513048204405116, 0.18361596819445256, 0.18214290574135694, 0.17942556719234687, 0.17981808302936045, 0.1775185328012758, 0.17552828307558194, 0.17206606492880525, 0.17201477301807078, 0.1715033919563014, 0.17140454080643905, 0.16798130065833075, 0.16726887112437241, 0.16479503741183846, 0.1657925486896672, 0.16280176371770833, 0.1635804406197116, 0.16183493413808941, 0.15964244310435982, 0.15685513860724817, 0.15543958166361987, 0.15600090055019522, 0.15425985772244843, 0.1547677520345258, 0.15128111872287334, 0.15053190750232878, 0.1488552450102226, 0.14784866878011757, 0.14702340478224799, 0.14648970025836472, 0.1448552442526011, 0.1433441284969373, 0.14401858783579968, 0.14328701640500863, 0.1423577108718653, 0.13935065960778922, 0.13713860150720317, 0.13667753274058367, 0.13817499709953862, 0.13591316398732492, 0.13440424075288857, 0.13554820827014738, 0.13372802743543907, 0.13202561252733497, 0.13168486557810732, 0.1314025032626141, 0.1301910865960476, 0.1278494282935003, 0.12924874770201084, 0.12756318580529874, 0.12748261711359726, 0.12629462610140144, 0.125636609830194, 0.12451197603529307, 0.12514376068055233, 0.12462522085975594], 'val_losses': [0.4864084341781149, 0.3821319725120326, 0.36214365385115227, 0.35679770310993647, 0.3286078807655962, 0.3208044500480752, 0.30419107745269114, 0.309813502232632, 0.27669822377693143, 0.25866296791056465, 0.24388821300013205, 0.24783116052165963, 0.2366221959433928, 0.22431905840613045, 0.2185930578429483, 0.20671400290027833, 0.2234389410589799, 0.1812101953462571, 0.1934001909804858, 0.17486376802943102, 0.1719555150114897, 0.18351802098252185, 0.16631187230550232, 0.17986193839626316, 0.1666460408882948, 0.16649505456909772, 0.16274860033679317, 0.1620369715973886, 0.1710110417849706, 0.16938542559053976, 0.16258017497262983, 0.16113870263346125, 0.16012539981567178, 0.14790091463168034, 0.15456804152087542, 0.16230453102343095, 0.16099755923605522, 0.1440440425886171, 0.15191592672757376, 0.14400967870917242, 0.1480907880973135, 0.13907781139327954, 0.1564741788130702, 0.14859639141601722, 0.13638191420364135, 0.14597445277642104, 0.13797676510237883, 0.14723162196670883, 0.15374749767748191, 0.13613574308398194, 0.12963403282387634, 0.14650769667930283, 0.14122138675659346, 0.13813404534924614, 0.1341387324969641, 0.1280837560593223, 0.13198988015655289, 0.13900471406351309, 0.13721275090697027, 0.1497188523297859, 0.13075281369134964, 0.1422445720623884, 0.13151822924005285, 0.1223921335803254, 0.12863392296134343, 0.138009737100223, 0.13307509059920794, 0.12096254338246959, 0.12857007278884924, 0.12462382897307081, 0.12923844457195452, 0.12661748536303594, 0.12247633515048365, 0.12230432982668664, 0.1119220567263393, 0.13755562436220686, 0.12718295525815937, 0.12060692719765108, 0.13194197357585893, 0.12222576878667102, 0.1241571701565507, 0.12179042612710195, 0.10850769004779506, 0.12140591773804889, 0.11364162248199766, 0.12453537336691267, 0.11401315335996048, 0.12313394667924966, 0.11644921570833337, 0.11356038753120183, 0.12450739198098609, 0.11647844693592327, 0.1134818933541024, 0.10910234940300727, 0.10737129153426825, 0.10810333229073933], 'val_acc': [0.7136950256860043, 0.7486530509961158, 0.761934594662323, 0.7611828091717829, 0.7788497681994737, 0.7952637514095978, 0.8020298208244581, 0.7951384538278411, 0.823831600050119, 0.8356095727352462, 0.8503946873825335, 0.8446309986217266, 0.8546548051622603, 0.8643027189575241, 0.8683122415737377, 0.8777095602054881, 0.8625485528129307, 0.8899887232176419, 0.8839744392933216, 0.8968800902142589, 0.900639017666959, 0.8913669966169653, 0.9015161007392557, 0.8928705675980454, 0.9032702668838491, 0.9013908031574991, 0.9038967547926325, 0.907906277408846, 0.8977571732865556, 0.9003884225034456, 0.907154491918306, 0.9061521112642525, 0.907906277408846, 0.9137952637514096, 0.9091592532264128, 0.9046485402831725, 0.9048991354466859, 0.9169277032953264, 0.9116652048615461, 0.9185565718581631, 0.9119158000250596, 0.9181806791128931, 0.9059015161007392, 0.9139205613331662, 0.9206866307480266, 0.9127928830973562, 0.9223154993108633, 0.914045858914923, 0.9107881217892495, 0.9261997243453202, 0.9268262122541034, 0.9139205613331662, 0.9165518105500564, 0.9188071670216765, 0.9253226412730234, 0.9267009146723468, 0.92544793885478, 0.9183059766946498, 0.9168024057135697, 0.9131687758426262, 0.9214384162385666, 0.915549429896003, 0.9178047863676231, 0.9268262122541034, 0.9278285929081569, 0.9152988347324896, 0.9226913920561333, 0.9270768074176169, 0.9193083573487032, 0.9243202606189701, 0.9215637138203233, 0.9226913920561333, 0.9244455582007267, 0.9280791880716702, 0.931587520360857, 0.915925322641273, 0.9203107380027565, 0.9268262122541034, 0.9160506202230297, 0.9219396065655933, 0.9256985340182935, 0.9249467485277534, 0.9355970429770706, 0.9230672848014033, 0.9292068663074803, 0.9210625234932965, 0.9303345445432903, 0.9200601428392432, 0.9260744267635634, 0.9284550808169403, 0.9189324646034331, 0.9278285929081569, 0.9270768074176169, 0.9313369251973437, 0.9330910913419371, 0.930083949379777], 'final_model_size_bytes': 120674, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 5.9018234520537745e-05, 'batch_size': 64, 'epochs': 96, 'base_channels': 15, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.0695170726451609, 'weight_decay': 0.00024135442126025424, 'gamma_focal': 1.6811332993889132, 'effective_beta': 0.995506133044267, 'label_smoothing': 0.059509985680484746, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.2843409747171856, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}, 'model_parameter_count': 26517, 'model_storage_size_kb': 113.940234375, 'model_size_validation': 'PASS'}
2025-10-03 02:03:19,785 - INFO - _models.training_function_executor - BO Objective: base=0.9301, size_penalty=0.0000, final=0.9301
2025-10-03 02:03:19,785 - INFO - _models.training_function_executor - Model: 26,517 parameters, 113.9KB (PASS 256KB limit)
2025-10-03 02:03:19,785 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 364.472s
2025-10-03 02:03:19,883 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9301
2025-10-03 02:03:19,883 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-10-03 02:03:19,883 - INFO - bo.run_bo - Recorded observation #12: hparams={'lr': 5.9018234520537745e-05, 'batch_size': np.int64(64), 'epochs': np.int64(96), 'base_channels': np.int64(15), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.0695170726451609, 'weight_decay': 0.00024135442126025424, 'gamma_focal': 1.6811332993889132, 'effective_beta': 0.995506133044267, 'label_smoothing': 0.059509985680484746, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.2843409747171856, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(8)}, value=0.9301
2025-10-03 02:03:19,883 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 12: {'lr': 5.9018234520537745e-05, 'batch_size': np.int64(64), 'epochs': np.int64(96), 'base_channels': np.int64(15), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.0695170726451609, 'weight_decay': 0.00024135442126025424, 'gamma_focal': 1.6811332993889132, 'effective_beta': 0.995506133044267, 'label_smoothing': 0.059509985680484746, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.2843409747171856, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(8)} -> 0.9301
2025-10-03 02:03:19,883 - INFO - bo.run_bo - ðŸ”BO Trial 13: Using RF surrogate + Expected Improvement
2025-10-03 02:03:19,883 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:03:19,883 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:03:19,883 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:03:19,883 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.4949148155337513e-05, 'batch_size': 64, 'epochs': 43, 'base_channels': 15, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.020841986168450736, 'weight_decay': 2.022714312037532e-06, 'gamma_focal': 1.0982743108046367, 'effective_beta': 0.9684804906597135, 'label_smoothing': 0.0900953603819674, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.3418251399422344, 'smote_max_multiplier': 0, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 02:03:19,885 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.4949148155337513e-05, 'batch_size': 64, 'epochs': 43, 'base_channels': 15, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.020841986168450736, 'weight_decay': 2.022714312037532e-06, 'gamma_focal': 1.0982743108046367, 'effective_beta': 0.9684804906597135, 'label_smoothing': 0.0900953603819674, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.3418251399422344, 'smote_max_multiplier': 0, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 02:05:12,136 - INFO - _models.training_function_executor - Model: 14,258 parameters, 15.3KB storage
2025-10-03 02:05:12,137 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7441754019361605, 0.6134664291849755, 0.5781568169773056, 0.5394046607779459, 0.4965629592984593, 0.4639096376272131, 0.4412027427872571, 0.4228003658285432, 0.4038351549645681, 0.3861264368955105, 0.37064441678831944, 0.3568124479938001, 0.34326385809005633, 0.33074903512992115, 0.3202805025479408, 0.31454239656885397, 0.30658517529543744, 0.3008963506188835, 0.29296670643338446, 0.28984043010997507, 0.28497904070643365, 0.28105592143060243, 0.27849428819350847, 0.27413255948491316, 0.2726090867500035, 0.26717495508997285, 0.26513139751420317, 0.26222796241162294, 0.2582189963399828, 0.25682672262807926, 0.2531488414421919, 0.24934242412723798, 0.24716376094840445, 0.2444096634012339, 0.24074044118428542, 0.23913498390030113, 0.23792730744342191, 0.23467669814672887, 0.2316285331291509, 0.22872646431036694, 0.22841973923406023, 0.22365339983377086, 0.22196856448374622], 'val_losses': [0.6380767103290665, 0.602124194287967, 0.566259763960312, 0.51879286403092, 0.4755610574786936, 0.44791735630482904, 0.4312624375076465, 0.4090643585523617, 0.3866879471283989, 0.37151752243908615, 0.3525873233860947, 0.342401481829719, 0.32584966272545135, 0.3139933763649989, 0.30288080167589654, 0.29614801771351607, 0.2889469416069665, 0.28338085238183147, 0.28225931425528367, 0.27855539543895164, 0.27098109867097503, 0.26669190015586425, 0.26755606562231526, 0.2610666337369931, 0.2608920492716353, 0.2524380575701671, 0.25388206040974237, 0.24819317653862968, 0.2475115664769139, 0.2427161518620184, 0.23908057886359835, 0.24142381139431004, 0.23515186736740842, 0.23070626682639794, 0.2310305786019294, 0.2247949441771895, 0.22906649268602552, 0.22269432402586165, 0.21825073939295225, 0.21599911723522566, 0.21513284424681844, 0.21314363220426286, 0.21056687275862399], 'val_acc': [0.7182057386292444, 0.7129432401954642, 0.715699786994111, 0.7630622728981331, 0.7938854780102744, 0.8086705926575617, 0.8140583886730987, 0.8181932088710688, 0.8254604686129557, 0.8354842751534896, 0.8466357599298333, 0.8574113519609071, 0.8664327778473876, 0.8739506327527878, 0.8790878336048115, 0.8819696779852149, 0.8874827715825084, 0.8901140207993986, 0.8891116401453452, 0.8909911038716952, 0.8957524119784488, 0.898383661195339, 0.8957524119784488, 0.900639017666959, 0.9003884225034456, 0.9038967547926325, 0.9028943741385791, 0.9052750281919559, 0.9043979451196592, 0.9059015161007392, 0.9084074677358727, 0.9076556822453327, 0.9087833604811427, 0.9120410976068162, 0.9106628242074928, 0.9134193710061396, 0.9114146096980328, 0.9145470492419496, 0.9154241323142464, 0.9161759178047864, 0.9158000250595163, 0.9173035960405964, 0.9198095476757299], 'final_model_size_bytes': 100782, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.4949148155337513e-05, 'batch_size': 64, 'epochs': 43, 'base_channels': 15, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.020841986168450736, 'weight_decay': 2.022714312037532e-06, 'gamma_focal': 1.0982743108046367, 'effective_beta': 0.9684804906597135, 'label_smoothing': 0.0900953603819674, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.3418251399422344, 'smote_max_multiplier': 0, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}, 'model_parameter_count': 14258, 'model_storage_size_kb': 15.316210937500001, 'model_size_validation': 'PASS'}
2025-10-03 02:05:12,137 - INFO - _models.training_function_executor - BO Objective: base=0.9198, size_penalty=0.0000, final=0.9198
2025-10-03 02:05:12,137 - INFO - _models.training_function_executor - Model: 14,258 parameters, 15.3KB (PASS 256KB limit)
2025-10-03 02:05:12,137 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 112.253s
2025-10-03 02:05:12,235 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9198
2025-10-03 02:05:12,235 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.099s
2025-10-03 02:05:12,235 - INFO - bo.run_bo - Recorded observation #13: hparams={'lr': 1.4949148155337513e-05, 'batch_size': np.int64(64), 'epochs': np.int64(43), 'base_channels': np.int64(15), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.020841986168450736, 'weight_decay': 2.022714312037532e-06, 'gamma_focal': 1.0982743108046367, 'effective_beta': 0.9684804906597135, 'label_smoothing': 0.0900953603819674, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.3418251399422344, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(8)}, value=0.9198
2025-10-03 02:05:12,235 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 13: {'lr': 1.4949148155337513e-05, 'batch_size': np.int64(64), 'epochs': np.int64(43), 'base_channels': np.int64(15), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.020841986168450736, 'weight_decay': 2.022714312037532e-06, 'gamma_focal': 1.0982743108046367, 'effective_beta': 0.9684804906597135, 'label_smoothing': 0.0900953603819674, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.3418251399422344, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(8)} -> 0.9198
2025-10-03 02:05:12,236 - INFO - bo.run_bo - ðŸ”BO Trial 14: Using RF surrogate + Expected Improvement
2025-10-03 02:05:12,236 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:05:12,236 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:05:12,236 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:05:12,236 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.000529818111489406, 'batch_size': 256, 'epochs': 84, 'base_channels': 14, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.23657126644619125, 'weight_decay': 3.8846852589445604e-05, 'gamma_focal': 2.9681527967524293, 'effective_beta': 0.9941151542814002, 'label_smoothing': 0.06885949940354641, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.30096981433995784, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 02:05:12,237 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.000529818111489406, 'batch_size': 256, 'epochs': 84, 'base_channels': 14, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.23657126644619125, 'weight_decay': 3.8846852589445604e-05, 'gamma_focal': 2.9681527967524293, 'effective_beta': 0.9941151542814002, 'label_smoothing': 0.06885949940354641, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.30096981433995784, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 02:08:02,769 - INFO - _models.training_function_executor - Model: 13,805 parameters, 59.3KB storage
2025-10-03 02:08:02,769 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.49080122404555787, 0.3430940158909175, 0.2990513596872358, 0.28044595987194276, 0.2654295345188533, 0.2568186673076643, 0.2469372284962663, 0.23938809994000676, 0.23123277509340404, 0.22604275233081778, 0.2189097405049579, 0.21389166392253553, 0.2093107134793289, 0.20436250707703996, 0.19902258804554085, 0.19463099030838393, 0.19179091578164392, 0.1861630918337564, 0.18233309140235496, 0.17926271535457014, 0.17605951166464734, 0.17039008305747508, 0.16742296184796093, 0.16550044945042613, 0.1625248483886289, 0.1594950520170858, 0.1573132904235832, 0.1543189446396757, 0.15237601963024347, 0.15069153089139986, 0.14766533594298287, 0.1441866546849292, 0.1422065475179694, 0.14153542720989715, 0.1378230468058261, 0.13717212654317945, 0.13618604387333103, 0.13244141477592247, 0.13232025574046272, 0.12963168204906628, 0.12864285465394423, 0.12739425838240115, 0.12487003256471993, 0.12258441970010463, 0.12186158138309709, 0.12038984359896249, 0.11999126615551739, 0.11690309393854931, 0.11632030620609726, 0.11540678193033028, 0.1124228875610673, 0.1114748325186814, 0.11117539098722597, 0.10960703881956471, 0.10748874208318117, 0.10655404869935439, 0.10500180655062978, 0.10463743657739759, 0.10320750230085761, 0.10410347322475261, 0.10133820068633688, 0.09882114253770816, 0.09843686734036493, 0.09688021675496711, 0.09685590804635819, 0.09702749297037809, 0.09554429952110594, 0.09388426731298846, 0.09474334393358236, 0.0924005024813801, 0.09176557678489881, 0.09077030704303123, 0.09037940115406085, 0.08921923578848076, 0.08924023789361267, 0.08793249286155774, 0.0877088247360671, 0.08778159527808205, 0.0861064919056247, 0.08614687114184537, 0.08491484792029723, 0.08485202177970971, 0.08515471398590582, 0.08384116496322434], 'val_losses': [0.2917605935519029, 0.2514100720665536, 0.24456640968397736, 0.24030842945257863, 0.2184072865936156, 0.2172306599449042, 0.21644561665893244, 0.19744286398931846, 0.2237572605176582, 0.21020553270563747, 0.1995530987176855, 0.17481951027746384, 0.1765470921511549, 0.18743290846926752, 0.19532937485276122, 0.1968796297547483, 0.1949352626177082, 0.20214124699423686, 0.20931198646693341, 0.19973503506515217, 0.20421806218301186, 0.21499107810746249, 0.18592543510726903, 0.22063288817786883, 0.17144395470667745, 0.23232467859412118, 0.18947379056365699, 0.18891498671167553, 0.1785904573015948, 0.18883887686786638, 0.20391777385380494, 0.181319232054065, 0.19961746567630362, 0.19668347557663335, 0.19396901069902833, 0.17636075699294648, 0.18092377394598358, 0.1945971114573331, 0.18079252645108218, 0.20644473342922542, 0.1677424202521087, 0.15528431513435767, 0.1471138126656176, 0.14769059069181142, 0.14091085251329716, 0.1459626092612482, 0.14805662631381725, 0.13034166738576697, 0.13879644651609604, 0.1329590002600654, 0.15342328865433483, 0.1534253604389898, 0.14264856100356857, 0.1338771410044173, 0.12087643970270352, 0.11623145558433381, 0.11838146864218702, 0.1390175626392301, 0.12704487877449003, 0.10508764428345052, 0.11607287359811237, 0.11308619917004314, 0.12754043788389968, 0.10510244755203443, 0.10691858334902078, 0.11521716297579288, 0.1022305600170096, 0.10924867203450042, 0.1120726156353394, 0.11075906664909776, 0.10907510233499916, 0.10999916710683934, 0.09900789432504872, 0.10799983615850822, 0.08878909924511338, 0.09334407747351964, 0.10502269135477915, 0.09825323865633533, 0.09634336982099294, 0.10417969470271646, 0.10649183700429461, 0.09757254365306497, 0.08689065673677354, 0.09329560609918768], 'val_acc': [0.7262247838616714, 0.6886355093346699, 0.7103119909785741, 0.6773587269765694, 0.7308607943866683, 0.7417616839994988, 0.7526625736123292, 0.7806039343440672, 0.7366244831474753, 0.7724595915298835, 0.7918807167021676, 0.8302217767197093, 0.8244580879589024, 0.8092970805663451, 0.7981455957900012, 0.8108006515474251, 0.8085452950758051, 0.8115524370379652, 0.8007768450068914, 0.8191955895251222, 0.8037839869690515, 0.7926325021927076, 0.8300964791379526, 0.7816063149981205, 0.8372384412980829, 0.7773461972183937, 0.814308983836612, 0.8183185064528254, 0.8308482646284927, 0.8199473750156622, 0.8056634506954016, 0.8333542162636262, 0.8206991605062023, 0.8206991605062023, 0.8278411226663326, 0.8505199849642902, 0.8458839744392933, 0.8364866558075429, 0.852900639017667, 0.8267134444305225, 0.854404209998747, 0.8661821826838741, 0.8739506327527878, 0.8734494424257612, 0.8822202731487282, 0.8748277158250846, 0.8724470617717078, 0.8894875328906152, 0.8867309860919684, 0.8838491417115649, 0.8763312868061647, 0.8644280165392808, 0.876581881969678, 0.884600927202105, 0.893747650670342, 0.8975065781230422, 0.8923693772710186, 0.8764565843879213, 0.8823455707304849, 0.8992607442676356, 0.8926199724345321, 0.8972559829595289, 0.8881092594912918, 0.899887232176419, 0.9041473499561459, 0.9001378273399323, 0.9056509209372259, 0.8988848515223656, 0.8963788998872322, 0.8971306853777722, 0.8977571732865556, 0.899887232176419, 0.9075303846635759, 0.8992607442676356, 0.9226913920561333, 0.9084074677358727, 0.899887232176419, 0.9074050870818193, 0.909033955644656, 0.901014910412229, 0.9016413983210124, 0.9065280040095226, 0.9152988347324896, 0.9137952637514096], 'final_model_size_bytes': 65734, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.000529818111489406, 'batch_size': 256, 'epochs': 84, 'base_channels': 14, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.23657126644619125, 'weight_decay': 3.8846852589445604e-05, 'gamma_focal': 2.9681527967524293, 'effective_beta': 0.9941151542814002, 'label_smoothing': 0.06885949940354641, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.30096981433995784, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}, 'model_parameter_count': 13805, 'model_storage_size_kb': 59.31835937500001, 'model_size_validation': 'PASS'}
2025-10-03 02:08:02,769 - INFO - _models.training_function_executor - BO Objective: base=0.9138, size_penalty=0.0000, final=0.9138
2025-10-03 02:08:02,769 - INFO - _models.training_function_executor - Model: 13,805 parameters, 59.3KB (PASS 256KB limit)
2025-10-03 02:08:02,769 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 170.533s
2025-10-03 02:08:02,868 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9138
2025-10-03 02:08:02,868 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.099s
2025-10-03 02:08:02,868 - INFO - bo.run_bo - Recorded observation #14: hparams={'lr': 0.000529818111489406, 'batch_size': np.int64(256), 'epochs': np.int64(84), 'base_channels': np.int64(14), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.23657126644619125, 'weight_decay': 3.8846852589445604e-05, 'gamma_focal': 2.9681527967524293, 'effective_beta': 0.9941151542814002, 'label_smoothing': 0.06885949940354641, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.30096981433995784, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)}, value=0.9138
2025-10-03 02:08:02,868 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 14: {'lr': 0.000529818111489406, 'batch_size': np.int64(256), 'epochs': np.int64(84), 'base_channels': np.int64(14), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.23657126644619125, 'weight_decay': 3.8846852589445604e-05, 'gamma_focal': 2.9681527967524293, 'effective_beta': 0.9941151542814002, 'label_smoothing': 0.06885949940354641, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.30096981433995784, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)} -> 0.9138
2025-10-03 02:08:02,868 - INFO - bo.run_bo - ðŸ”BO Trial 15: Using RF surrogate + Expected Improvement
2025-10-03 02:08:02,869 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:08:02,869 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:08:02,869 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:08:02,869 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002886862290693138, 'batch_size': 64, 'epochs': 80, 'base_channels': 13, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.20780439212751645, 'weight_decay': 0.0018280397346479398, 'gamma_focal': 2.3865388388597695, 'effective_beta': 0.9461846088554161, 'label_smoothing': 0.005050772703064355, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.22544307936764713, 'smote_max_multiplier': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:08:02,870 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002886862290693138, 'batch_size': 64, 'epochs': 80, 'base_channels': 13, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.20780439212751645, 'weight_decay': 0.0018280397346479398, 'gamma_focal': 2.3865388388597695, 'effective_beta': 0.9461846088554161, 'label_smoothing': 0.005050772703064355, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.22544307936764713, 'smote_max_multiplier': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:14:18,525 - INFO - _models.training_function_executor - Model: 51,621 parameters, 221.8KB storage
2025-10-03 02:14:18,525 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.22829535645113752, 0.13349129101206358, 0.11432983209602636, 0.10230220685248047, 0.09489337317625711, 0.08826183713978403, 0.08230337060837878, 0.07672004212449376, 0.07397732889456576, 0.07186229750446346, 0.06970191552137643, 0.06727534657120107, 0.06387700395340556, 0.06040811321011343, 0.06021156489122774, 0.05922708507468892, 0.05759689605403069, 0.05655014976596293, 0.05600219120427906, 0.05460192992809719, 0.05197026184750553, 0.05220070956866225, 0.05103443751121954, 0.050023746255499206, 0.049529483280155225, 0.049436734669363054, 0.04709159419408729, 0.047626417819322454, 0.04592448546363958, 0.04690494040984227, 0.04477042888150241, 0.04462494631205259, 0.044657888406089974, 0.04330364415256432, 0.04401135052542144, 0.0435190277538032, 0.04149267621168225, 0.041682838881436445, 0.040875048782401405, 0.04160660464381139, 0.04144738275171842, 0.03892546321417075, 0.04193220735762047, 0.0395112112717637, 0.0388285184683553, 0.03862729934894459, 0.037859187181686016, 0.037997861630243476, 0.03629801457168012, 0.03720752717150194, 0.03760921778982833, 0.037598254648983226, 0.03609111745081322, 0.0354016908975344, 0.03663004833798927, 0.03713037747333933, 0.03560386826927955, 0.03551444370273671, 0.03605740492308609, 0.03517519668149222, 0.03446939282508572, 0.03432356729635308, 0.03383946494112893, 0.034986519697751704, 0.0351808911773326, 0.03398871936093774, 0.03373276363192721, 0.03501706270921299, 0.03375477503729719, 0.033074034463470806, 0.032504676673011115, 0.03290946901120444, 0.03323860470097835, 0.03361925873892, 0.0325486515155476, 0.031868869687972, 0.03353204253232558, 0.032361359119390073, 0.03194187439975089, 0.032811685243641135], 'val_losses': [0.1367606050895578, 0.15019728335678062, 0.10493906146638182, 0.10165799048991449, 0.08626781756616358, 0.08219507777153916, 0.08627532491391127, 0.06722360435028883, 0.07984973382974475, 0.06893982414108121, 0.06542784555656109, 0.06385040935654043, 0.06435518768937976, 0.06634695685316097, 0.059862517786177606, 0.06703809702710808, 0.05705612356349865, 0.06216166959626231, 0.05934324353007263, 0.05995669071919084, 0.06588845517331833, 0.05606025647824307, 0.050863118902690845, 0.048814088143496984, 0.057864097872969054, 0.04895172132975684, 0.05460049988777323, 0.056845699228199785, 0.05095810545945643, 0.04770268051005324, 0.050424274594402005, 0.05094018038896716, 0.054320899732555605, 0.05118788865321024, 0.04773046372136501, 0.0540414716968589, 0.04720775115581073, 0.04817130095587385, 0.051170093882929306, 0.05378096259158591, 0.044354131154257646, 0.050753994293015206, 0.04374890567055921, 0.04124578675858479, 0.051153934821640504, 0.0577183714732313, 0.05583010076106065, 0.045419085722768114, 0.05624522258596542, 0.04517824317507988, 0.049067257146999034, 0.043497091017093086, 0.05651821566414466, 0.05285325916753558, 0.0642724330068501, 0.05585935381901039, 0.05333579119536202, 0.04312261386393246, 0.04577399694785903, 0.06174834039169152, 0.05393930178057683, 0.05226999161002511, 0.048870124133804836, 0.04852082682550588, 0.04633286258149993, 0.041051733577450095, 0.061405014352192186, 0.03973561927607119, 0.048713138912565325, 0.06060716445591497, 0.04185536723640416, 0.04771007583443346, 0.07244715905252248, 0.05406748632088354, 0.04630553920338692, 0.047500745015135536, 0.053352322175028985, 0.05293679715151357, 0.04213439555652696, 0.07071025374669536], 'val_acc': [0.886856283673725, 0.8862297957649418, 0.9213131186568099, 0.9215637138203233, 0.9343440671595038, 0.9325899010149105, 0.930459842125047, 0.9480015035709811, 0.9295827590527502, 0.9389800776845006, 0.944994361608821, 0.9407342438290941, 0.9433654930459842, 0.9393559704297707, 0.9503821576243578, 0.9463726350081444, 0.9488785866432777, 0.9528881092594913, 0.9511339431148979, 0.9466232301716577, 0.9447437664453076, 0.9511339431148979, 0.9497556697155745, 0.9546422754040848, 0.9521363237689513, 0.9542663826588147, 0.9365994236311239, 0.9510086455331412, 0.9548928705675981, 0.955268763312868, 0.9601553690013783, 0.9543916802405713, 0.9407342438290941, 0.9452449567723343, 0.9622854278912417, 0.9456208495176043, 0.962160130309485, 0.9570229294574615, 0.9496303721338178, 0.9525122165142212, 0.9594035835108382, 0.9586517980202982, 0.9587770956020549, 0.9597794762561083, 0.9517604310236812, 0.947249718080441, 0.9477509084074678, 0.9646660819446184, 0.9350958526500438, 0.9599047738378649, 0.9542663826588147, 0.9565217391304348, 0.9487532890615211, 0.9535145971682747, 0.939606565593284, 0.9501315624608445, 0.9448690640270643, 0.9649166771081318, 0.960656559328405, 0.9412354341561208, 0.9538904899135446, 0.9458714446811176, 0.9558952512216514, 0.954141085077058, 0.9587770956020549, 0.9647913795263752, 0.9338428768324771, 0.9645407843628618, 0.9594035835108382, 0.953389299586518, 0.963287808545295, 0.9471244204986844, 0.9229419872196467, 0.9463726350081444, 0.9609071544919183, 0.9584012028567849, 0.9532640020047614, 0.9568976318757048, 0.9641648916175918, 0.9214384162385666], 'final_model_size_bytes': 216966, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002886862290693138, 'batch_size': 64, 'epochs': 80, 'base_channels': 13, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.20780439212751645, 'weight_decay': 0.0018280397346479398, 'gamma_focal': 2.3865388388597695, 'effective_beta': 0.9461846088554161, 'label_smoothing': 0.005050772703064355, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.22544307936764713, 'smote_max_multiplier': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}, 'model_parameter_count': 51621, 'model_storage_size_kb': 221.80898437500002, 'model_size_validation': 'PASS'}
2025-10-03 02:14:18,525 - INFO - _models.training_function_executor - BO Objective: base=0.9214, size_penalty=0.0000, final=0.9214
2025-10-03 02:14:18,525 - INFO - _models.training_function_executor - Model: 51,621 parameters, 221.8KB (PASS 256KB limit)
2025-10-03 02:14:18,525 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 375.657s
2025-10-03 02:14:18,626 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9214
2025-10-03 02:14:18,626 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-10-03 02:14:18,626 - INFO - bo.run_bo - Recorded observation #15: hparams={'lr': 0.002886862290693138, 'batch_size': np.int64(64), 'epochs': np.int64(80), 'base_channels': np.int64(13), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.20780439212751645, 'weight_decay': 0.0018280397346479398, 'gamma_focal': 2.3865388388597695, 'effective_beta': 0.9461846088554161, 'label_smoothing': 0.005050772703064355, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.22544307936764713, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)}, value=0.9214
2025-10-03 02:14:18,626 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 15: {'lr': 0.002886862290693138, 'batch_size': np.int64(64), 'epochs': np.int64(80), 'base_channels': np.int64(13), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.20780439212751645, 'weight_decay': 0.0018280397346479398, 'gamma_focal': 2.3865388388597695, 'effective_beta': 0.9461846088554161, 'label_smoothing': 0.005050772703064355, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.22544307936764713, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)} -> 0.9214
2025-10-03 02:14:18,626 - INFO - bo.run_bo - ðŸ”BO Trial 16: Using RF surrogate + Expected Improvement
2025-10-03 02:14:18,626 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:14:18,626 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:14:18,626 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:14:18,626 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00021554182281446956, 'batch_size': 64, 'epochs': 95, 'base_channels': 12, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.24837192130904923, 'weight_decay': 0.0031530339201876175, 'gamma_focal': 1.9823340937841518, 'effective_beta': 0.989637650774549, 'label_smoothing': 0.09806317248578972, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.26387457833988776, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 02:14:18,627 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00021554182281446956, 'batch_size': 64, 'epochs': 95, 'base_channels': 12, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.24837192130904923, 'weight_decay': 0.0031530339201876175, 'gamma_focal': 1.9823340937841518, 'effective_beta': 0.989637650774549, 'label_smoothing': 0.09806317248578972, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.26387457833988776, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 02:17:30,878 - INFO - _models.training_function_executor - Model: 13,659 parameters, 58.7KB storage
2025-10-03 02:17:30,878 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.44823860606241134, 0.377561026881244, 0.33774138345749155, 0.30774166329486646, 0.28369732563090766, 0.2664498202800303, 0.25472927800591, 0.24510399065952612, 0.23523300843770142, 0.22613999208341068, 0.2162991157031181, 0.20951108495043366, 0.20255932725852316, 0.19601948758554508, 0.191528338384995, 0.1858852687132365, 0.18139187450639227, 0.17714748306146402, 0.17211661173368395, 0.16868368755459237, 0.1647602265080283, 0.16401235703197453, 0.16031622761559472, 0.15874343282296516, 0.15467258370322579, 0.15089712230457092, 0.15018605745805333, 0.14942410051695698, 0.14682039733601182, 0.14637982421045975, 0.14358295684378275, 0.14217174888643014, 0.1393331344773386, 0.1356201698733484, 0.1360872588395658, 0.13434146056562327, 0.13497588463055027, 0.13143029922940203, 0.13210615829784866, 0.13145556105645914, 0.12785113273350154, 0.12600558462817596, 0.1260004678268303, 0.12399183713263544, 0.12176631261235665, 0.12269443007197083, 0.1223905515461937, 0.11872840899191893, 0.1211940044014913, 0.11695765907224533, 0.11608368762207875, 0.11553763362687546, 0.11611883274862403, 0.11608540755032157, 0.11403325774972646, 0.11422969352243777, 0.11164817007693402, 0.11065546627811773, 0.10920148444011692, 0.10897729146077445, 0.1075973857374014, 0.10847359094517282, 0.10690733531518967, 0.10681196290855997, 0.10479174051154315, 0.10681107997589943, 0.10527070296939198, 0.10354724419184938, 0.10360346402593325, 0.10304721518783862, 0.102803748152459, 0.10035668149475196, 0.10334460916895542, 0.10069353731671017, 0.09975839743433682, 0.09902107621753833, 0.0999936552214913, 0.09873221163561413, 0.09796669620860565, 0.09758065009798725, 0.09830392787621418, 0.09644418917249753, 0.09471173679405177, 0.09589051956492506, 0.09661427128682985, 0.09413853199031125, 0.0946945692441, 0.09354707517185647, 0.09603217634076676, 0.0932712490314664, 0.0946986710411323, 0.09180884793528196, 0.09270860760128782, 0.0904497066710212, 0.09107732977932252], 'val_losses': [0.4125894958207279, 0.34722221513810786, 0.30132561560046656, 0.2674473754575252, 0.2565376233431334, 0.2374497060964481, 0.23038493717445674, 0.21985709873144677, 0.21569771645199742, 0.212870166889876, 0.19871956080547432, 0.20272866921337634, 0.1763815618934764, 0.17321472352772274, 0.18309195556634353, 0.17034962778177465, 0.16797511545828508, 0.16395732994841208, 0.15882623708814983, 0.1649185839772986, 0.1657782505850881, 0.155288328643379, 0.1524147585234148, 0.1508278114584182, 0.14231536404099354, 0.14337653078511312, 0.15025483782507845, 0.14903290077433404, 0.14707895697223677, 0.15170549372225803, 0.14205428245601656, 0.1435917670647276, 0.13620501306795293, 0.13202094008111967, 0.12920181987306079, 0.13878111250706968, 0.14070671732104403, 0.13241513165351354, 0.142396583598186, 0.13922807988343705, 0.12757346125676913, 0.1461786321325718, 0.1270612868961274, 0.14371675550900312, 0.13147534533649918, 0.13041202665680393, 0.1249723659655367, 0.1358020842442229, 0.12650012783169404, 0.1274208989834079, 0.11851398079612771, 0.1255189494402908, 0.13055966055500762, 0.11526640001280672, 0.13030561348923972, 0.11922136588982958, 0.11728085455194241, 0.1205516146450895, 0.11991085401865612, 0.12010379190780872, 0.12472322820504347, 0.1277917921795177, 0.12832078786048506, 0.11964371415897652, 0.1353770063595132, 0.11096239721598802, 0.12399806053775726, 0.14025203108903295, 0.12453035812028189, 0.11522352801748587, 0.12956509884979445, 0.11407567300495237, 0.10861737932788147, 0.12398789599985949, 0.11443385005729148, 0.1186312257967233, 0.10334250244852625, 0.1034395038094822, 0.11669875067457895, 0.11722721685694507, 0.11548098833089543, 0.1194058717195215, 0.11991529441252491, 0.10715417113949102, 0.1261297769432841, 0.11477759146409186, 0.10738565009696459, 0.10795248508436565, 0.10831993849650016, 0.11318458436653737, 0.12505978917850766, 0.11188680339311018, 0.10468732616052549, 0.12133038024838265, 0.10165413276300202], 'val_acc': [0.7426387670717955, 0.7735872697656935, 0.8105500563839118, 0.8338554065906528, 0.8438792131311865, 0.8591655181055006, 0.8666833730109009, 0.874702418243328, 0.885352712692645, 0.8864803909284551, 0.8948753289061521, 0.8903646159629119, 0.9076556822453327, 0.9097857411351961, 0.9055256233554693, 0.9084074677358727, 0.908658062899386, 0.9105375266257362, 0.9104122290439794, 0.9056509209372259, 0.907906277408846, 0.9142964540784363, 0.9201854404209999, 0.9208119283297832, 0.9233178799649167, 0.9193083573487032, 0.916677108131813, 0.9209372259115399, 0.9195589525122165, 0.9144217516601929, 0.9210625234932965, 0.9194336549304598, 0.9245708557824834, 0.9261997243453202, 0.9305851397068037, 0.92319258238316, 0.92544793885478, 0.930083949379777, 0.9219396065655933, 0.9223154993108633, 0.9307104372885603, 0.914045858914923, 0.9272021049993735, 0.9179300839493798, 0.9279538904899135, 0.9298333542162637, 0.930835734870317, 0.9183059766946498, 0.9295827590527502, 0.9280791880716702, 0.9364741260493672, 0.9255732364365368, 0.92319258238316, 0.9347199599047739, 0.9282044856534268, 0.9338428768324771, 0.9324646034331537, 0.9294574614709936, 0.9273274025811302, 0.9298333542162637, 0.9268262122541034, 0.9250720461095101, 0.9243202606189701, 0.9314622227791004, 0.915925322641273, 0.9373512091216639, 0.9261997243453202, 0.9127928830973562, 0.9259491291818068, 0.9357223405588272, 0.92319258238316, 0.9335922816689638, 0.9391053752662574, 0.9285803783986969, 0.9345946623230171, 0.9312116276155871, 0.9448690640270643, 0.9436160882094976, 0.9345946623230171, 0.9344693647412605, 0.9317128179426137, 0.9303345445432903, 0.9318381155243703, 0.9408595414108508, 0.9249467485277534, 0.9362235308858539, 0.939606565593284, 0.9411101365743642, 0.9399824583385541, 0.9376018042851773, 0.9255732364365368, 0.9409848389926074, 0.9453702543540909, 0.9325899010149105, 0.9488785866432777], 'final_model_size_bytes': 65158, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00021554182281446956, 'batch_size': 64, 'epochs': 95, 'base_channels': 12, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.24837192130904923, 'weight_decay': 0.0031530339201876175, 'gamma_focal': 1.9823340937841518, 'effective_beta': 0.989637650774549, 'label_smoothing': 0.09806317248578972, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.26387457833988776, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}, 'model_parameter_count': 13659, 'model_storage_size_kb': 58.691015625000006, 'model_size_validation': 'PASS'}
2025-10-03 02:17:30,879 - INFO - _models.training_function_executor - BO Objective: base=0.9489, size_penalty=0.0000, final=0.9489
2025-10-03 02:17:30,879 - INFO - _models.training_function_executor - Model: 13,659 parameters, 58.7KB (PASS 256KB limit)
2025-10-03 02:17:30,879 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 192.253s
2025-10-03 02:17:31,100 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9489
2025-10-03 02:17:31,100 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.222s
2025-10-03 02:17:31,100 - INFO - bo.run_bo - Recorded observation #16: hparams={'lr': 0.00021554182281446956, 'batch_size': np.int64(64), 'epochs': np.int64(95), 'base_channels': np.int64(12), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.24837192130904923, 'weight_decay': 0.0031530339201876175, 'gamma_focal': 1.9823340937841518, 'effective_beta': 0.989637650774549, 'label_smoothing': 0.09806317248578972, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.26387457833988776, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(8)}, value=0.9489
2025-10-03 02:17:31,101 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 16: {'lr': 0.00021554182281446956, 'batch_size': np.int64(64), 'epochs': np.int64(95), 'base_channels': np.int64(12), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.24837192130904923, 'weight_decay': 0.0031530339201876175, 'gamma_focal': 1.9823340937841518, 'effective_beta': 0.989637650774549, 'label_smoothing': 0.09806317248578972, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.26387457833988776, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(8)} -> 0.9489
2025-10-03 02:17:31,101 - INFO - bo.run_bo - ðŸ”BO Trial 17: Using RF surrogate + Expected Improvement
2025-10-03 02:17:31,101 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:17:31,101 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:17:31,101 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:17:31,101 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.0830182839147697e-05, 'batch_size': 64, 'epochs': 91, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.37299620172366976, 'weight_decay': 0.0001491387335807562, 'gamma_focal': 2.409364604745279, 'effective_beta': 0.9742780962507559, 'label_smoothing': 0.0950005434558016, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.19708675014779753, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 02:17:31,103 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.0830182839147697e-05, 'batch_size': 64, 'epochs': 91, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.37299620172366976, 'weight_decay': 0.0001491387335807562, 'gamma_focal': 2.409364604745279, 'effective_beta': 0.9742780962507559, 'label_smoothing': 0.0950005434558016, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.19708675014779753, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 02:28:55,473 - INFO - _models.training_function_executor - Model: 13,339 parameters, 28.7KB storage
2025-10-03 02:28:55,474 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6622202206308844, 0.46445029535019633, 0.4377542465950423, 0.42857096494747215, 0.42282396026080077, 0.4174333568012991, 0.40747588646052285, 0.39409086941884797, 0.3825702864443438, 0.3730530784846181, 0.36528232788522785, 0.3599702766230429, 0.3542447355879999, 0.3504338824606717, 0.3471068309212345, 0.34516285655660794, 0.3421284280687265, 0.33980889768094547, 0.3374969271956218, 0.33558852832338565, 0.33371302411994125, 0.33169089496850823, 0.329778663381592, 0.3282407624446012, 0.3267007857902798, 0.3251006275474235, 0.3230249229795827, 0.32159329641891504, 0.31965249195880235, 0.31892149206245657, 0.3165906930210275, 0.3146623389145017, 0.31194266084646616, 0.3092989324123023, 0.3062925470873134, 0.3033111473215824, 0.3021396970047231, 0.2991475137998835, 0.29633367824247486, 0.29519041079897235, 0.29215926100719813, 0.29026426033552766, 0.29002561356696227, 0.28884525498884034, 0.28713982109848285, 0.2850658757760255, 0.28392225012584155, 0.2817345262605385, 0.28127504448239726, 0.2783906615705033, 0.2777299178396106, 0.2762599143649807, 0.27602553379657785, 0.27409402457047005, 0.27323878642494986, 0.27200627346850353, 0.2710787997803167, 0.26881792923610087, 0.2669653799356242, 0.266864998595359, 0.26594778233897814, 0.2640071546221438, 0.2624968030400038, 0.2616781931954794, 0.2605315927935734, 0.25823075097369674, 0.2574530299374272, 0.257181458764031, 0.25490301246332825, 0.25525242072020626, 0.2534195093040504, 0.25220877188315494, 0.25106910079520656, 0.2492859422146697, 0.24890333603405546, 0.24809125445065924, 0.24744395344457062, 0.24648830229259241, 0.24488124216123433, 0.24344157986547124, 0.24298876616394885, 0.24224510926665643, 0.24149507109446994, 0.23939926223171515, 0.23773207672247704, 0.2365964461719318, 0.23665678598500395, 0.23531568590454174, 0.2351485646191506, 0.23320467958960645, 0.23217274482225642], 'val_losses': [0.4828655779391598, 0.4503419245675518, 0.4414023437486258, 0.4397778696709745, 0.43515719104613776, 0.42936306517086237, 0.4272987434864343, 0.41329086919862096, 0.4034712675960272, 0.4015950320276636, 0.3911838982505856, 0.390617917062286, 0.3911245461992566, 0.3812513207637969, 0.37687542543138214, 0.3796524576989351, 0.3720352305735342, 0.3675233870670022, 0.36111033406230447, 0.3676124668863689, 0.36886496627753484, 0.3660073979854464, 0.3618876591330407, 0.3547601104846889, 0.3601028881654332, 0.36181719658473666, 0.3613691751318548, 0.35621696932022173, 0.3513092790916889, 0.3629770836440592, 0.35657334683189684, 0.3552987500063654, 0.35306315737370486, 0.33860170561901953, 0.35005467049819516, 0.3417934644694257, 0.3374512659855137, 0.33117692026334994, 0.33802698317543184, 0.33299466462724414, 0.3372584009003242, 0.33142264708811264, 0.3291771825341828, 0.32926267132306813, 0.3403366887917451, 0.3331675046242474, 0.33436985901088856, 0.32361309502361085, 0.3302158595976144, 0.3368150857834421, 0.3406770496145255, 0.32179372956797286, 0.32604672735592666, 0.3336777493157881, 0.32163751401290036, 0.3234999750830329, 0.32594839124180086, 0.31889558814175667, 0.3343221099174138, 0.3270010439812756, 0.3277599703375371, 0.3271271353033219, 0.33875111692786114, 0.3369806110261452, 0.33178906164183053, 0.32441340529325685, 0.34042539509999814, 0.3335901942823394, 0.32914261309115633, 0.34805575936991623, 0.3310459386643693, 0.34008624483552913, 0.3362038181533402, 0.33719140373738904, 0.3530182272602839, 0.3392910117636108, 0.34478445902472377, 0.35250996904046955, 0.34782993927170436, 0.34876591273379376, 0.34048247660346054, 0.3383801740641702, 0.3417372582192081, 0.3681351930750684, 0.3413984710344798, 0.3684849505615091, 0.3652221726237525, 0.34907405326545965, 0.3300064079759414, 0.3537750922898693, 0.3427311589717566], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7198346071920811, 0.7195840120285678, 0.7183310362110011, 0.7184563337927578, 0.7208369878461346, 0.7260994862799148, 0.7279789500062649, 0.7314872822954517, 0.7331161508582884, 0.7341185315123419, 0.7380027565467987, 0.739005137200852, 0.7391304347826086, 0.7427640646535522, 0.7441423380528756, 0.7470241824332791, 0.7431399573988222, 0.7436411477258489, 0.7437664453076056, 0.7440170404711189, 0.7450194211251723, 0.7441423380528756, 0.7432652549805788, 0.7433905525623355, 0.745144718706929, 0.7448941235434157, 0.7441423380528756, 0.7461470993609823, 0.7457712066157123, 0.7461470993609823, 0.7457712066157123, 0.7402581130184187, 0.7436411477258489, 0.745144718706929, 0.7500313243954392, 0.7450194211251723, 0.7481518606690891, 0.7497807292319258, 0.7500313243954392, 0.7517854905400326, 0.7541661445934094, 0.7517854905400326, 0.7532890615211126, 0.7536649542663827, 0.7570479889738128, 0.7559203107380028, 0.7534143591028694, 0.7547926325021928, 0.7581756672096228, 0.7593033454454329, 0.7577997744643529, 0.7601804285177296, 0.7613081067535397, 0.761934594662323, 0.763438165643403, 0.7606816188447563, 0.7624357849893497, 0.7641899511339432, 0.7636887608069164, 0.7603057260994863, 0.760806916426513, 0.762310487407593, 0.7663200100238066, 0.7591780478636763, 0.7599298333542163, 0.7599298333542163, 0.7529131687758426, 0.7564215010650295, 0.7537902518481393, 0.7559203107380028, 0.7536649542663827, 0.7476506703420625, 0.7502819195589525, 0.7472747775967924, 0.7447688259616589, 0.7475253727603057, 0.7457712066157123, 0.747400075178549, 0.7504072171407092, 0.7496554316501691, 0.7334920436035585, 0.7487783485778724, 0.7343691266758552, 0.7373762686380153, 0.7465229921062524, 0.752036085703546, 0.7422628743265255, 0.7490289437413858], 'final_model_size_bytes': 35035, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.0830182839147697e-05, 'batch_size': 64, 'epochs': 91, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.37299620172366976, 'weight_decay': 0.0001491387335807562, 'gamma_focal': 2.409364604745279, 'effective_beta': 0.9742780962507559, 'label_smoothing': 0.0950005434558016, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.19708675014779753, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}, 'model_parameter_count': 13339, 'model_storage_size_kb': 28.658007812500003, 'model_size_validation': 'PASS'}
2025-10-03 02:28:55,474 - INFO - _models.training_function_executor - BO Objective: base=0.7490, size_penalty=0.0000, final=0.7490
2025-10-03 02:28:55,474 - INFO - _models.training_function_executor - Model: 13,339 parameters, 28.7KB (PASS 256KB limit)
2025-10-03 02:28:55,474 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 684.373s
2025-10-03 02:28:55,577 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7490
2025-10-03 02:28:55,577 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.103s
2025-10-03 02:28:55,577 - INFO - bo.run_bo - Recorded observation #17: hparams={'lr': 2.0830182839147697e-05, 'batch_size': np.int64(64), 'epochs': np.int64(91), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.37299620172366976, 'weight_decay': 0.0001491387335807562, 'gamma_focal': 2.409364604745279, 'effective_beta': 0.9742780962507559, 'label_smoothing': 0.0950005434558016, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.19708675014779753, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)}, value=0.7490
2025-10-03 02:28:55,577 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 17: {'lr': 2.0830182839147697e-05, 'batch_size': np.int64(64), 'epochs': np.int64(91), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.37299620172366976, 'weight_decay': 0.0001491387335807562, 'gamma_focal': 2.409364604745279, 'effective_beta': 0.9742780962507559, 'label_smoothing': 0.0950005434558016, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.19708675014779753, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)} -> 0.7490
2025-10-03 02:28:55,577 - INFO - bo.run_bo - ðŸ”BO Trial 18: Using RF surrogate + Expected Improvement
2025-10-03 02:28:55,577 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:28:55,578 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:28:55,578 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:28:55,578 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00021244799145115915, 'batch_size': 256, 'epochs': 23, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.245277766796073, 'weight_decay': 0.0002568766256801204, 'gamma_focal': 1.1760655974399525, 'effective_beta': 0.9270690455355142, 'label_smoothing': 0.09456186344693199, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.2605720237662167, 'smote_max_multiplier': 3, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:28:55,579 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00021244799145115915, 'batch_size': 256, 'epochs': 23, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.245277766796073, 'weight_decay': 0.0002568766256801204, 'gamma_focal': 1.1760655974399525, 'effective_beta': 0.9270690455355142, 'label_smoothing': 0.09456186344693199, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.2605720237662167, 'smote_max_multiplier': 3, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:29:32,389 - INFO - _models.training_function_executor - Model: 17,921 parameters, 77.0KB storage
2025-10-03 02:29:32,390 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6885977026361146, 0.5714959663186117, 0.5300050097849757, 0.49473300100649986, 0.4628995059072249, 0.4421021550370051, 0.4250450717477882, 0.4064147117055904, 0.3875302433513876, 0.3731915307013988, 0.3626008897313079, 0.351476548750426, 0.34237998513463963, 0.337030162955385, 0.3256096664879274, 0.31750803375342723, 0.31451042421579667, 0.30972744864157986, 0.3030888858844294, 0.2992666043828716, 0.29355224955986753, 0.28883403303091215, 0.2876698317464438], 'val_losses': [0.6076077934704154, 0.5642950228283451, 0.5220298815895301, 0.4834236678825018, 0.45461195560942136, 0.4550994070284857, 0.4538261849428983, 0.4412392147007689, 0.46082922237108503, 0.49556945655492596, 0.5208257811848583, 0.4964124549004297, 0.5168967426846371, 0.4998308457933621, 0.5098991139746746, 0.46596190897224393, 0.4939551711000307, 0.5233290844648617, 0.5326076911858278, 0.4862873321853026, 0.511310640477459, 0.5126341388027197, 0.5255470754075299], 'val_acc': [0.7154491918305976, 0.7445182307981456, 0.7515348953765192, 0.7471494800150357, 0.7811051246710938, 0.7787244706177171, 0.7737125673474502, 0.7747149480015035, 0.7525372760305726, 0.713444430522491, 0.6991605062022302, 0.715323894248841, 0.708808419997494, 0.721463475754918, 0.7155744894123544, 0.7560456083197594, 0.7413857912542288, 0.7225911539907279, 0.7138203232677609, 0.7480265630873324, 0.7486530509961158, 0.7501566219771958, 0.7420122791630122], 'final_model_size_bytes': 86370, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00021244799145115915, 'batch_size': 256, 'epochs': 23, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.245277766796073, 'weight_decay': 0.0002568766256801204, 'gamma_focal': 1.1760655974399525, 'effective_beta': 0.9270690455355142, 'label_smoothing': 0.09456186344693199, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.2605720237662167, 'smote_max_multiplier': 3, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}, 'model_parameter_count': 17921, 'model_storage_size_kb': 77.00429687500001, 'model_size_validation': 'PASS'}
2025-10-03 02:29:32,390 - INFO - _models.training_function_executor - BO Objective: base=0.7420, size_penalty=0.0000, final=0.7420
2025-10-03 02:29:32,390 - INFO - _models.training_function_executor - Model: 17,921 parameters, 77.0KB (PASS 256KB limit)
2025-10-03 02:29:32,390 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 36.812s
2025-10-03 02:29:32,614 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7420
2025-10-03 02:29:32,614 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.225s
2025-10-03 02:29:32,614 - INFO - bo.run_bo - Recorded observation #18: hparams={'lr': 0.00021244799145115915, 'batch_size': np.int64(256), 'epochs': np.int64(23), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.245277766796073, 'weight_decay': 0.0002568766256801204, 'gamma_focal': 1.1760655974399525, 'effective_beta': 0.9270690455355142, 'label_smoothing': 0.09456186344693199, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.2605720237662167, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)}, value=0.7420
2025-10-03 02:29:32,615 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 18: {'lr': 0.00021244799145115915, 'batch_size': np.int64(256), 'epochs': np.int64(23), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.245277766796073, 'weight_decay': 0.0002568766256801204, 'gamma_focal': 1.1760655974399525, 'effective_beta': 0.9270690455355142, 'label_smoothing': 0.09456186344693199, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.2605720237662167, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)} -> 0.7420
2025-10-03 02:29:32,615 - INFO - bo.run_bo - ðŸ”BO Trial 19: Using RF surrogate + Expected Improvement
2025-10-03 02:29:32,615 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:29:32,615 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:29:32,615 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:29:32,615 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00030713171045728093, 'batch_size': 64, 'epochs': 93, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.2201885352745983, 'weight_decay': 0.001034848216068382, 'gamma_focal': 1.5381628367306308, 'effective_beta': 0.9763607902271948, 'label_smoothing': 0.08537412380123355, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.37030343562275364, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 6}
2025-10-03 02:29:32,616 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00030713171045728093, 'batch_size': 64, 'epochs': 93, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.2201885352745983, 'weight_decay': 0.001034848216068382, 'gamma_focal': 1.5381628367306308, 'effective_beta': 0.9763607902271948, 'label_smoothing': 0.08537412380123355, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.37030343562275364, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 6}
2025-10-03 02:31:46,885 - INFO - _models.training_function_executor - Model: 3,964 parameters, 4.3KB storage
2025-10-03 02:31:46,886 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5281600696462566, 0.4348902051820681, 0.3988030378828146, 0.36310667908905303, 0.32347616890698405, 0.30036855311847527, 0.2835072145364049, 0.2715111467605119, 0.26086669675957996, 0.254978641398986, 0.25039179237609677, 0.243046149024725, 0.23632593779166128, 0.23411350748135146, 0.22548467919491394, 0.2233822900600363, 0.21726452341454408, 0.21564697294225818, 0.21182409989498868, 0.20781061842748666, 0.20499787588889962, 0.20332251044736935, 0.19835006340293027, 0.19719849875056722, 0.19451539035306195, 0.1922270004346295, 0.19048361181960766, 0.1864605393639163, 0.1860091277569625, 0.18316169879639488, 0.1837852252768353, 0.18095301698700877, 0.18088989450659843, 0.1785048546180672, 0.17731702753202333, 0.1772334686878484, 0.1749991249263098, 0.17389584092240515, 0.17231423411457303, 0.1734748882451396, 0.17182765766281355, 0.168903774934126, 0.16869546937010055, 0.1670823948349835, 0.16665890388146634, 0.16407017388096004, 0.1627360192157196, 0.16237630806316003, 0.1618934937935442, 0.15815750840450704, 0.1579670845508609, 0.15853493823316114, 0.15664402500069896, 0.15588326519867182, 0.15830722931647956, 0.15428102161525592, 0.15252546461341832, 0.15420950108443168, 0.15106084082930643, 0.15033034053963273, 0.15075191706129584, 0.14933506432091334, 0.14969259521042058, 0.14808664200106006, 0.14745268533140804, 0.1479245034328205, 0.14721982178318208, 0.14674244809261977, 0.14290710690960834, 0.1440624631354337, 0.14410352194356474, 0.14045885236123315, 0.14142567168122913, 0.14236415699565447, 0.14278079909952393, 0.1397447235119323, 0.13843165118557332, 0.13961808304904985, 0.13922153394437653, 0.13875437073480276, 0.13850576162601524, 0.13812714135221485, 0.13534014022510324, 0.13677859648348442, 0.1364640772195178, 0.1352216050741114, 0.13413948751595095, 0.13481352997306773, 0.1334050202130323, 0.13150346958992554, 0.13260777512390737, 0.13460284036935483, 0.13256491556601674], 'val_losses': [0.47433392765333504, 0.4172105834853334, 0.3780709683611315, 0.32382425490349936, 0.3026736023756604, 0.29679973608508564, 0.2839715125013781, 0.2766077082574763, 0.2814853846253854, 0.2448683677479492, 0.2580823779849238, 0.25792081961766505, 0.2683643839040685, 0.24857308615786497, 0.2676490463886833, 0.2351550992604874, 0.22210053569962696, 0.2329179465289015, 0.2283765262730542, 0.23076564053211457, 0.2182948165824652, 0.21543392549199936, 0.22037225451832457, 0.2540177027301044, 0.20215078062684655, 0.21906642208585106, 0.218762120210748, 0.2073355259367016, 0.22549697356185375, 0.20767489579212037, 0.211826666047879, 0.2057149613049974, 0.20291341952032022, 0.21586132229547006, 0.2126146036654976, 0.22451155508666867, 0.22229772373285228, 0.2068635227489839, 0.20224099498918488, 0.22687317792346284, 0.20004007684050787, 0.2092856966811394, 0.2169098471369664, 0.22957357047285565, 0.21083665863118245, 0.19783236313218716, 0.1901319423692072, 0.1960677171321818, 0.19477446049044805, 0.19527233066288185, 0.19842513292269842, 0.18546880467794485, 0.20677853320501097, 0.18121377943587055, 0.18453717131301614, 0.2014087242577372, 0.18223132520760976, 0.19895788379337476, 0.1759995751390032, 0.18159730855294778, 0.17540993782612424, 0.19041313872706575, 0.1759805349333665, 0.18361753562334382, 0.17074092282170178, 0.1809999519493956, 0.18263258644511654, 0.19016951343280125, 0.18029338549535506, 0.1828107227920336, 0.16879883705598792, 0.1787856255627981, 0.17699213152152227, 0.16685020786757576, 0.16566225986793784, 0.18325234016049044, 0.1679831855377149, 0.17073869369686015, 0.18946850118915204, 0.19214218267707683, 0.16880953690190464, 0.1608513619962185, 0.17997896218715104, 0.17360950541136305, 0.15489912925731747, 0.1648269202021067, 0.16461668059448298, 0.164589068324565, 0.16482491875266897, 0.1698960023627008, 0.17829610244169702, 0.16615030478675358, 0.17899970040742863], 'val_acc': [0.7314872822954517, 0.7490289437413858, 0.7655682245332666, 0.8065405337676983, 0.829971181556196, 0.8416238566595665, 0.8576619471244205, 0.8587896253602305, 0.8569101616338805, 0.884225034456835, 0.8715699786994111, 0.8705675980453578, 0.8653050996115775, 0.8825961658939983, 0.8709434907906277, 0.8891116401453452, 0.9028943741385791, 0.8909911038716952, 0.8951259240696654, 0.8958777095602055, 0.9012655055757424, 0.9057762185189826, 0.9041473499561459, 0.8809672973311615, 0.9116652048615461, 0.9012655055757424, 0.8997619345946624, 0.9100363362987094, 0.8932464603433153, 0.9084074677358727, 0.9072797895000626, 0.9076556822453327, 0.9048991354466859, 0.9057762185189826, 0.9041473499561459, 0.9028943741385791, 0.9033955644656058, 0.9050244330284426, 0.9089086580628993, 0.8921187821075053, 0.9114146096980328, 0.9075303846635759, 0.9026437789750658, 0.8952512216514221, 0.9025184813933091, 0.9095351459716827, 0.9200601428392432, 0.9146723468237064, 0.915173537150733, 0.9080315749906027, 0.9149229419872197, 0.9160506202230297, 0.9081568725723593, 0.9209372259115399, 0.9114146096980328, 0.9125422879338428, 0.9218143089838366, 0.9115399072797895, 0.9253226412730234, 0.9176794887858665, 0.9180553815311364, 0.9131687758426262, 0.92319258238316, 0.9221902017291066, 0.9265756170905901, 0.9158000250595163, 0.9190577621851899, 0.9116652048615461, 0.9208119283297832, 0.9198095476757299, 0.9273274025811302, 0.9251973436912667, 0.915549429896003, 0.9270768074176169, 0.9275779977446436, 0.9181806791128931, 0.9282044856534268, 0.9268262122541034, 0.9188071670216765, 0.915173537150733, 0.9250720461095101, 0.9240696654554567, 0.9209372259115399, 0.92469615336424, 0.9333416865054505, 0.9288309735622102, 0.9294574614709936, 0.9295827590527502, 0.9310863300338303, 0.9249467485277534, 0.9263250219270768, 0.9280791880716702, 0.9175541912041097], 'final_model_size_bytes': 35182, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00030713171045728093, 'batch_size': 64, 'epochs': 93, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.2201885352745983, 'weight_decay': 0.001034848216068382, 'gamma_focal': 1.5381628367306308, 'effective_beta': 0.9763607902271948, 'label_smoothing': 0.08537412380123355, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.37030343562275364, 'smote_max_multiplier': 3, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 6}, 'model_parameter_count': 3964, 'model_storage_size_kb': 4.2582031250000005, 'model_size_validation': 'PASS'}
2025-10-03 02:31:46,886 - INFO - _models.training_function_executor - BO Objective: base=0.9176, size_penalty=0.0000, final=0.9176
2025-10-03 02:31:46,886 - INFO - _models.training_function_executor - Model: 3,964 parameters, 4.3KB (PASS 256KB limit)
2025-10-03 02:31:46,886 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 134.271s
2025-10-03 02:31:46,989 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9176
2025-10-03 02:31:46,989 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.103s
2025-10-03 02:31:46,989 - INFO - bo.run_bo - Recorded observation #19: hparams={'lr': 0.00030713171045728093, 'batch_size': np.int64(64), 'epochs': np.int64(93), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.2201885352745983, 'weight_decay': 0.001034848216068382, 'gamma_focal': 1.5381628367306308, 'effective_beta': 0.9763607902271948, 'label_smoothing': 0.08537412380123355, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.37030343562275364, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(6)}, value=0.9176
2025-10-03 02:31:46,989 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 19: {'lr': 0.00030713171045728093, 'batch_size': np.int64(64), 'epochs': np.int64(93), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.2201885352745983, 'weight_decay': 0.001034848216068382, 'gamma_focal': 1.5381628367306308, 'effective_beta': 0.9763607902271948, 'label_smoothing': 0.08537412380123355, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.37030343562275364, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(6)} -> 0.9176
2025-10-03 02:31:46,989 - INFO - bo.run_bo - ðŸ”BO Trial 20: Using RF surrogate + Expected Improvement
2025-10-03 02:31:46,989 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:31:46,989 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:31:46,989 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:31:46,989 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003051306523487582, 'batch_size': 64, 'epochs': 47, 'base_channels': 14, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.24223946188180046, 'weight_decay': 0.0001618672422584477, 'gamma_focal': 1.6897620542218994, 'effective_beta': 0.9388587875290463, 'label_smoothing': 0.09796011695216533, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.21567778801103243, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:31:46,991 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003051306523487582, 'batch_size': 64, 'epochs': 47, 'base_channels': 14, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.24223946188180046, 'weight_decay': 0.0001618672422584477, 'gamma_focal': 1.6897620542218994, 'effective_beta': 0.9388587875290463, 'label_smoothing': 0.09796011695216533, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.21567778801103243, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:34:30,158 - INFO - _models.training_function_executor - Model: 22,349 parameters, 96.0KB storage
2025-10-03 02:34:30,158 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5663947542145464, 0.39405660748761345, 0.3233042869203224, 0.2891788073166434, 0.265334053025424, 0.24620493228803988, 0.22962355212254548, 0.218706140928674, 0.20806778935300996, 0.2010469364753521, 0.191604840177884, 0.18494358652141077, 0.18022676911593632, 0.17209323210424, 0.16755925825151038, 0.1616054329415513, 0.16047646633571586, 0.15480909591643932, 0.15314545432254423, 0.1484862865060439, 0.1431493617151861, 0.14121138797878646, 0.13906450135562762, 0.13716241280659675, 0.13397803127971672, 0.13249876192360363, 0.1289741265915041, 0.12915178781073855, 0.1266232811463584, 0.12470914580432843, 0.12392535851430252, 0.12305475178231812, 0.12186937385514965, 0.11954625462934995, 0.11623186226108753, 0.11762819727244693, 0.1149780796503023, 0.11559076377759375, 0.11214975412319272, 0.11204026526852076, 0.11086977607301687, 0.11058825691395109, 0.10906098801097727, 0.10748978745122852, 0.10516627088286615, 0.10503101000655891, 0.10441772171434481], 'val_losses': [0.47159185006704835, 0.43953468226098935, 0.4710257650579281, 0.3329994781448076, 0.32226315021231094, 0.3153380379724915, 0.3108335074857428, 0.32880007434818864, 0.34786299824931244, 0.34466526901090133, 0.2706851552279876, 0.2533944483340914, 0.28337796072010824, 0.3126691709132649, 0.3515523375256918, 0.38120072410154754, 0.3656439425883743, 0.3055720101090216, 0.21647017365215326, 0.3166351740916799, 0.3059714107495653, 0.3483390794811863, 0.38210749442065634, 0.32630403686766185, 0.4630403650828642, 0.3601334448626071, 0.3235908426982002, 0.3086013740621523, 0.367724743788291, 0.395384850223894, 0.33669452609132816, 0.3064406480037573, 0.4328742474112172, 0.2665728905149875, 0.48033249082085544, 0.3367842767019347, 0.3541284478326113, 0.41214237935246417, 0.4026929391511863, 0.352558556848444, 0.35008769484334623, 0.31848575668277607, 0.3321478776264991, 0.48584780927484694, 0.2861625739557524, 0.3310015355079877, 0.39610733048328256], 'val_acc': [0.7022929457461471, 0.7462723969427391, 0.76957774714948, 0.8298458839744393, 0.8425009397318631, 0.8501440922190202, 0.8495176043102368, 0.84600927202105, 0.8438792131311865, 0.8476381405838868, 0.8738253351710312, 0.8774589650419747, 0.8674351585014409, 0.8501440922190202, 0.8403708808419997, 0.8159378523994487, 0.816188447562962, 0.8455080816940233, 0.8800902142588648, 0.8414985590778098, 0.8378649292068663, 0.8259616589399824, 0.8200726725974189, 0.8402455832602431, 0.7854905400325773, 0.8136824959278286, 0.8324771331913294, 0.8329783235183561, 0.8121789249467485, 0.8105500563839118, 0.8282170154116025, 0.8430021300588899, 0.8002756546798647, 0.8577872447061772, 0.7799774464352838, 0.8278411226663326, 0.8232051121413356, 0.8036586893872948, 0.8084199974940484, 0.8080441047487783, 0.8066658313494549, 0.8263375516852525, 0.8133066031825585, 0.7738378649292069, 0.852900639017667, 0.8260869565217391, 0.8034080942237815], 'final_model_size_bytes': 104034, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003051306523487582, 'batch_size': 64, 'epochs': 47, 'base_channels': 14, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.24223946188180046, 'weight_decay': 0.0001618672422584477, 'gamma_focal': 1.6897620542218994, 'effective_beta': 0.9388587875290463, 'label_smoothing': 0.09796011695216533, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.21567778801103243, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}, 'model_parameter_count': 22349, 'model_storage_size_kb': 96.030859375, 'model_size_validation': 'PASS'}
2025-10-03 02:34:30,158 - INFO - _models.training_function_executor - BO Objective: base=0.8034, size_penalty=0.0000, final=0.8034
2025-10-03 02:34:30,158 - INFO - _models.training_function_executor - Model: 22,349 parameters, 96.0KB (PASS 256KB limit)
2025-10-03 02:34:30,158 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 163.169s
2025-10-03 02:34:30,263 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8034
2025-10-03 02:34:30,263 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-10-03 02:34:30,264 - INFO - bo.run_bo - Recorded observation #20: hparams={'lr': 0.0003051306523487582, 'batch_size': np.int64(64), 'epochs': np.int64(47), 'base_channels': np.int64(14), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(3), 'dropout': 0.24223946188180046, 'weight_decay': 0.0001618672422584477, 'gamma_focal': 1.6897620542218994, 'effective_beta': 0.9388587875290463, 'label_smoothing': 0.09796011695216533, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.21567778801103243, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)}, value=0.8034
2025-10-03 02:34:30,264 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 20: {'lr': 0.0003051306523487582, 'batch_size': np.int64(64), 'epochs': np.int64(47), 'base_channels': np.int64(14), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(3), 'dropout': 0.24223946188180046, 'weight_decay': 0.0001618672422584477, 'gamma_focal': 1.6897620542218994, 'effective_beta': 0.9388587875290463, 'label_smoothing': 0.09796011695216533, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.21567778801103243, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)} -> 0.8034
2025-10-03 02:34:30,264 - INFO - bo.run_bo - ðŸ”BO Trial 21: Using RF surrogate + Expected Improvement
2025-10-03 02:34:30,264 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:34:30,264 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:34:30,264 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:34:30,264 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007312673915130287, 'batch_size': 512, 'epochs': 95, 'base_channels': 6, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.11136251551446272, 'weight_decay': 8.045097668924474e-05, 'gamma_focal': 1.6779878326477409, 'effective_beta': 0.9377793368250665, 'label_smoothing': 0.09304706421407995, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.11279045809412691, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 6}
2025-10-03 02:34:30,265 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007312673915130287, 'batch_size': 512, 'epochs': 95, 'base_channels': 6, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.11136251551446272, 'weight_decay': 8.045097668924474e-05, 'gamma_focal': 1.6779878326477409, 'effective_beta': 0.9377793368250665, 'label_smoothing': 0.09304706421407995, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.11279045809412691, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 6}
2025-10-03 02:36:55,067 - INFO - _models.training_function_executor - Model: 11,087 parameters, 23.8KB storage
2025-10-03 02:36:55,069 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6341422026363063, 0.4793864630877596, 0.42996303497376354, 0.40978421083059924, 0.39057217871594224, 0.3579891241105006, 0.32707998530535276, 0.3082971064080885, 0.2958026352250008, 0.2856864200800697, 0.2797375145251786, 0.26699473781383, 0.2556901994801007, 0.24725022161249283, 0.23792700256266713, 0.22679083504458886, 0.21517717837167072, 0.20448934116993273, 0.19981710432455047, 0.19153150986023865, 0.18439620054603142, 0.1784379464428784, 0.1763104569790406, 0.1697641774315107, 0.16701173865996413, 0.16394600488439903, 0.15959264527390976, 0.15756978497226087, 0.15551079130727644, 0.15315997594347186, 0.1503434419170775, 0.14941104926983223, 0.14540693705248414, 0.1452338041771109, 0.14418844656810886, 0.14309309837241052, 0.14029681235267097, 0.1397141759146635, 0.13696449197026134, 0.13550391297712625, 0.1340718580894115, 0.13365778030533393, 0.13322918972709, 0.12989657072770663, 0.13117023440611728, 0.13046525204130885, 0.12819951944139762, 0.1269326046502699, 0.12599622138215422, 0.12544334706705443, 0.12396268986105542, 0.12301264474549374, 0.12208022300520513, 0.12008536125258784, 0.11967026532215384, 0.11953290248630982, 0.11863087386796563, 0.11718460875368879, 0.11850813698791768, 0.116325433055093, 0.11621774569481935, 0.11552511304999691, 0.11435116519783611, 0.11448310057360461, 0.1144076340868838, 0.1125374531644525, 0.11532885701848732, 0.11221723909449638, 0.1101014279602793, 0.11075757566855911, 0.10896751828870016, 0.11012418056071725, 0.10945124614424706, 0.10836139167941827, 0.10757335597341197, 0.10995001983383793, 0.10546216013522254, 0.10559471887786806, 0.10676134382339138, 0.10681646753103591, 0.10529876444423998, 0.10468408982045847, 0.10288925644339272, 0.10378899204105684, 0.10297854637344786, 0.10192279485675868, 0.10157517138094292, 0.10009765396555671, 0.10021468683147428, 0.09918481147889001, 0.09982939862237405, 0.10053276909469225, 0.10046837542102073, 0.10146178039361992, 0.09686662660766335], 'val_losses': [0.522304317046341, 0.4473171327522174, 0.42562224937477083, 0.4056589180082475, 0.37975576767167446, 0.34234512960189356, 0.3238365966198872, 0.3071893719470138, 0.2949495896922961, 0.30176085213523896, 0.28714395400994164, 0.26709099831833244, 0.2604891880347815, 0.2556255518374415, 0.24591282083081237, 0.23471555210320427, 0.2278622597728572, 0.22236642252493735, 0.2076430474120324, 0.21173046151769592, 0.19600805868004337, 0.19400566616925868, 0.19900111303474652, 0.19899381035298216, 0.19958769650951302, 0.19206985302503776, 0.1842618288663437, 0.18472644538079389, 0.18360237790862804, 0.185017728152811, 0.17681894674775833, 0.18895540201690858, 0.17162015890921287, 0.1710173893491661, 0.176302197369817, 0.18686126679454645, 0.16383813136002726, 0.18319048711514505, 0.16467937860157597, 0.181438323303118, 0.16664641280212195, 0.15836748393790911, 0.1632380647373116, 0.15487250994320068, 0.16224214878801924, 0.15705106825349685, 0.16663075060440236, 0.16393012950301186, 0.16251753737582642, 0.1585918935658773, 0.18080284330065485, 0.15513513188413802, 0.1519963750059093, 0.1552894855763676, 0.16562040365999944, 0.14218024225771808, 0.14929282537357744, 0.15095177858922285, 0.1463132061212676, 0.13660017796334073, 0.15690713383181773, 0.14367707142957156, 0.1510559782738704, 0.15475698049923423, 0.14749146201805938, 0.14705003887254364, 0.16272619454585255, 0.13732970678190914, 0.14108208687797463, 0.14703086873498003, 0.14004661060487875, 0.13809500241089906, 0.13360022015140524, 0.14683797968769266, 0.1692697951693147, 0.16289699134365418, 0.136294094365113, 0.131507849375148, 0.1353174671540059, 0.1346190185784965, 0.13455493118422115, 0.13227004337625953, 0.1339713280070412, 0.13177057437887169, 0.1483689417849954, 0.13833825543752484, 0.13668167237843537, 0.1308180549874101, 0.13203808210641724, 0.13273584990927995, 0.12861020268391374, 0.1429214952075979, 0.13357408842509022, 0.14635194233486698, 0.12788574661755678], 'val_acc': [0.7200852023555946, 0.7610575115900263, 0.7618092970805663, 0.7685753664954267, 0.7808545295075805, 0.8136824959278286, 0.8288435033203859, 0.8392432026061897, 0.8423756421501065, 0.8397443929332163, 0.8453827841122666, 0.8550306979075304, 0.8615461721588773, 0.8668086705926575, 0.8674351585014409, 0.8789625360230547, 0.8835985465480516, 0.8941235434156121, 0.899887232176419, 0.9035208620473625, 0.9111640145345195, 0.9089086580628993, 0.9067785991730359, 0.907906277408846, 0.9066533015912793, 0.9109134193710061, 0.9163012153865431, 0.9109134193710061, 0.9139205613331662, 0.9111640145345195, 0.9160506202230297, 0.9070291943365493, 0.9225660944743767, 0.9223154993108633, 0.9218143089838366, 0.9066533015912793, 0.9259491291818068, 0.9112893121162762, 0.9236937727101867, 0.9114146096980328, 0.9186818694399198, 0.9251973436912667, 0.9174288936223531, 0.9260744267635634, 0.92469615336424, 0.9241949630372134, 0.9168024057135697, 0.9158000250595163, 0.9215637138203233, 0.9204360355845133, 0.9084074677358727, 0.9236937727101867, 0.9270768074176169, 0.9258238316000501, 0.9175541912041097, 0.9337175792507204, 0.92469615336424, 0.9221902017291066, 0.9285803783986969, 0.9317128179426137, 0.9224407968926199, 0.9310863300338303, 0.9268262122541034, 0.9221902017291066, 0.9239443678737, 0.9249467485277534, 0.9203107380027565, 0.9344693647412605, 0.930083949379777, 0.9265756170905901, 0.9295827590527502, 0.932339305851397, 0.9339681744142339, 0.9258238316000501, 0.9156747274777597, 0.9204360355845133, 0.9333416865054505, 0.938478887357474, 0.9299586517980203, 0.9333416865054505, 0.9355970429770706, 0.9317128179426137, 0.9325899010149105, 0.9359729357223405, 0.9263250219270768, 0.9287056759804536, 0.9333416865054505, 0.9364741260493672, 0.9364741260493672, 0.9363488284676106, 0.9354717453953139, 0.929708056634507, 0.9349705550682872, 0.9287056759804536, 0.9391053752662574], 'final_model_size_bytes': 30555, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007312673915130287, 'batch_size': 512, 'epochs': 95, 'base_channels': 6, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.11136251551446272, 'weight_decay': 8.045097668924474e-05, 'gamma_focal': 1.6779878326477409, 'effective_beta': 0.9377793368250665, 'label_smoothing': 0.09304706421407995, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.11279045809412691, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 6}, 'model_parameter_count': 11087, 'model_storage_size_kb': 23.8197265625, 'model_size_validation': 'PASS'}
2025-10-03 02:36:55,069 - INFO - _models.training_function_executor - BO Objective: base=0.9391, size_penalty=0.0000, final=0.9391
2025-10-03 02:36:55,069 - INFO - _models.training_function_executor - Model: 11,087 parameters, 23.8KB (PASS 256KB limit)
2025-10-03 02:36:55,069 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 144.805s
2025-10-03 02:36:55,175 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9391
2025-10-03 02:36:55,176 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.106s
2025-10-03 02:36:55,176 - INFO - bo.run_bo - Recorded observation #21: hparams={'lr': 0.0007312673915130287, 'batch_size': np.int64(512), 'epochs': np.int64(95), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.11136251551446272, 'weight_decay': 8.045097668924474e-05, 'gamma_focal': 1.6779878326477409, 'effective_beta': 0.9377793368250665, 'label_smoothing': 0.09304706421407995, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.11279045809412691, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(6)}, value=0.9391
2025-10-03 02:36:55,176 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 21: {'lr': 0.0007312673915130287, 'batch_size': np.int64(512), 'epochs': np.int64(95), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.11136251551446272, 'weight_decay': 8.045097668924474e-05, 'gamma_focal': 1.6779878326477409, 'effective_beta': 0.9377793368250665, 'label_smoothing': 0.09304706421407995, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.11279045809412691, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(6)} -> 0.9391
2025-10-03 02:36:55,176 - INFO - bo.run_bo - ðŸ”BO Trial 22: Using RF surrogate + Expected Improvement
2025-10-03 02:36:55,176 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:36:55,176 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:36:55,176 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:36:55,176 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 6.940516907524761e-05, 'batch_size': 64, 'epochs': 76, 'base_channels': 6, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.22704451225426636, 'weight_decay': 2.2520473993750402e-05, 'gamma_focal': 1.2951633448798727, 'effective_beta': 0.9567675896844992, 'label_smoothing': 0.08821757703671951, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.12171112333913126, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 02:36:55,177 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 6.940516907524761e-05, 'batch_size': 64, 'epochs': 76, 'base_channels': 6, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.22704451225426636, 'weight_decay': 2.2520473993750402e-05, 'gamma_focal': 1.2951633448798727, 'effective_beta': 0.9567675896844992, 'label_smoothing': 0.08821757703671951, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.12171112333913126, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}
2025-10-03 02:41:50,006 - INFO - _models.training_function_executor - Model: 9,007 parameters, 19.4KB storage
2025-10-03 02:41:50,006 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8234476246099048, 0.7437435664843408, 0.6994150380352985, 0.6662780610889389, 0.6431582919129143, 0.6273655241990843, 0.61634065067556, 0.6031650701715389, 0.5929076568470264, 0.5822000862581892, 0.5706446156504883, 0.5618183940472284, 0.5522952446954944, 0.5467765035628753, 0.5377140521007547, 0.5313980735757742, 0.5274609814589687, 0.5210466986885203, 0.5164798269467603, 0.5137900322006008, 0.5074980130385792, 0.5038116244871136, 0.4989660520555924, 0.494557793538727, 0.48980935714347196, 0.4857153835568531, 0.4807844098228269, 0.4780401055425871, 0.47362359201105775, 0.4680539535110148, 0.4669350085972481, 0.4610640127427677, 0.4596266935138774, 0.4550138208262127, 0.45262228926135273, 0.44862778497550426, 0.44606615884875217, 0.4436656601090148, 0.44099592577646385, 0.4391070433932696, 0.4368345310018776, 0.43287722041002524, 0.43164472766044226, 0.4290496983532807, 0.4271597688317046, 0.4232587596720104, 0.42130499923830483, 0.41879019148494145, 0.41710235610044044, 0.4170575171159358, 0.4164706435567556, 0.41372505338551707, 0.4098661452543165, 0.40817566601189303, 0.4073231807503051, 0.4050788958201478, 0.40490533860264616, 0.40412369538938736, 0.39980793343034504, 0.40089970529549684, 0.3996729705300501, 0.3978363427871835, 0.3961252643867587, 0.39550598427613587, 0.3962857394825161, 0.39162959882358034, 0.39079860219583656, 0.39106732091075275, 0.3878062004992178, 0.3904614891001343, 0.38591222443066403, 0.3861300758707348, 0.3847384001755456, 0.3834848901825168, 0.3829085361037163, 0.382711952300759], 'val_losses': [0.6054952506388119, 0.5853439926832276, 0.5776752407218322, 0.577292679015175, 0.5503495513698299, 0.5452706890213147, 0.5276552838253625, 0.523642607028585, 0.5051260396713094, 0.5143873219113051, 0.49943283945602096, 0.4970988118436429, 0.47208326563188685, 0.46568762201173103, 0.47029180848841945, 0.45011330038111186, 0.4675501970185207, 0.4550929947531831, 0.4361706246873846, 0.45106237806363925, 0.44684302732544484, 0.4405159330087233, 0.470223132794908, 0.44075091893235896, 0.42428368217230233, 0.4463540349683581, 0.4600451509802581, 0.4140330965754054, 0.41892628375636415, 0.41288949718915674, 0.4321197540393196, 0.41799320064530815, 0.4087927666557058, 0.4072340383421821, 0.41729712987391215, 0.40339402287037146, 0.39525987066925056, 0.414303366801984, 0.40538439495553646, 0.40558225108767315, 0.384799699905955, 0.3849229483455961, 0.43592822701093653, 0.39087075816555855, 0.3888013617628843, 0.3871812539700502, 0.39350026094992646, 0.3972867458270854, 0.38396375707823627, 0.4076522064677995, 0.3979821883113055, 0.3892375405659847, 0.3897341839010413, 0.3979430807187972, 0.44510465827506224, 0.39027909450387016, 0.3873885703996494, 0.42479072856045713, 0.39714863152898894, 0.39383272498252325, 0.3905853470335806, 0.36899640829084274, 0.4251233103422873, 0.3732972668012182, 0.38677773489762096, 0.3758370794907591, 0.38458090617668356, 0.3880891182072822, 0.38923770211316816, 0.3970218946828891, 0.4141566087400207, 0.39648413912438313, 0.38732946829051396, 0.3961931579342433, 0.3970247822445058, 0.40470313987043655], 'val_acc': [0.7167021676481644, 0.7210875830096479, 0.7212128805914045, 0.6832477133191329, 0.6926450319508833, 0.6933968174414233, 0.6975316376393935, 0.6972810424758802, 0.7030447312366871, 0.6984087207116902, 0.7110637764691141, 0.7109384788873575, 0.7341185315123419, 0.7422628743265255, 0.7440170404711189, 0.7628116777346198, 0.7526625736123292, 0.7603057260994863, 0.7733366746021801, 0.7629369753163764, 0.7698283423129934, 0.7722089963663701, 0.7517854905400326, 0.7658188196967799, 0.7822328029069039, 0.760055130935973, 0.7440170404711189, 0.7797268512717704, 0.7769703044731237, 0.7832351835609572, 0.7581756672096228, 0.7767197093096103, 0.783360481142714, 0.7821075053251473, 0.7698283423129934, 0.7809798270893372, 0.7782232802906904, 0.7700789374765067, 0.7774714948001503, 0.7762185189825836, 0.7951384538278411, 0.7936348828467611, 0.7636887608069164, 0.7973938102994612, 0.7946372635008144, 0.7985214885352713, 0.7942613707555444, 0.7970179175541912, 0.7990226788622979, 0.7871194085954141, 0.7931336925197344, 0.8054128555318882, 0.8006515474251347, 0.7936348828467611, 0.7664453076055632, 0.8036586893872948, 0.8086705926575617, 0.7851146472873074, 0.7996491667710813, 0.8113018418744519, 0.8082946999122917, 0.8179426137075555, 0.7872447061771708, 0.8217015411602556, 0.8110512467109385, 0.8220774339055257, 0.8133066031825585, 0.8195714822703922, 0.8110512467109385, 0.807793509585265, 0.8056634506954016, 0.8116777346197218, 0.8204485653426888, 0.8125548176920185, 0.816188447562962, 0.8087958902393184], 'final_model_size_bytes': 26395, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 6.940516907524761e-05, 'batch_size': 64, 'epochs': 76, 'base_channels': 6, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.22704451225426636, 'weight_decay': 2.2520473993750402e-05, 'gamma_focal': 1.2951633448798727, 'effective_beta': 0.9567675896844992, 'label_smoothing': 0.08821757703671951, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.12171112333913126, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 8}, 'model_parameter_count': 9007, 'model_storage_size_kb': 19.3509765625, 'model_size_validation': 'PASS'}
2025-10-03 02:41:50,006 - INFO - _models.training_function_executor - BO Objective: base=0.8088, size_penalty=0.0000, final=0.8088
2025-10-03 02:41:50,006 - INFO - _models.training_function_executor - Model: 9,007 parameters, 19.4KB (PASS 256KB limit)
2025-10-03 02:41:50,006 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 294.830s
2025-10-03 02:41:50,113 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8088
2025-10-03 02:41:50,113 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.107s
2025-10-03 02:41:50,113 - INFO - bo.run_bo - Recorded observation #22: hparams={'lr': 6.940516907524761e-05, 'batch_size': np.int64(64), 'epochs': np.int64(76), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.22704451225426636, 'weight_decay': 2.2520473993750402e-05, 'gamma_focal': 1.2951633448798727, 'effective_beta': 0.9567675896844992, 'label_smoothing': 0.08821757703671951, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.12171112333913126, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(8)}, value=0.8088
2025-10-03 02:41:50,113 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 22: {'lr': 6.940516907524761e-05, 'batch_size': np.int64(64), 'epochs': np.int64(76), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.22704451225426636, 'weight_decay': 2.2520473993750402e-05, 'gamma_focal': 1.2951633448798727, 'effective_beta': 0.9567675896844992, 'label_smoothing': 0.08821757703671951, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.12171112333913126, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(8)} -> 0.8088
2025-10-03 02:41:50,113 - INFO - bo.run_bo - ðŸ”BO Trial 23: Using RF surrogate + Expected Improvement
2025-10-03 02:41:50,113 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:41:50,113 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:41:50,113 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:41:50,113 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0009619395903820557, 'batch_size': 512, 'epochs': 92, 'base_channels': 12, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.221277012075841, 'weight_decay': 2.7986513402972983e-05, 'gamma_focal': 1.874405551702616, 'effective_beta': 0.9335148481813224, 'label_smoothing': 0.09612037052947485, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.21828043310487244, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 02:41:50,115 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0009619395903820557, 'batch_size': 512, 'epochs': 92, 'base_channels': 12, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.221277012075841, 'weight_decay': 2.7986513402972983e-05, 'gamma_focal': 1.874405551702616, 'effective_beta': 0.9335148481813224, 'label_smoothing': 0.09612037052947485, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.21828043310487244, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 02:45:25,782 - INFO - _models.training_function_executor - Model: 9,443 parameters, 40.6KB storage
2025-10-03 02:45:25,782 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5014462791651901, 0.3972635131108859, 0.3363105185019269, 0.3079389087654865, 0.2861402428184159, 0.26923550826279186, 0.2554259844614674, 0.24147181051161243, 0.2323278031343058, 0.22097811451368424, 0.21100022947891303, 0.2041847917537165, 0.19503683758146823, 0.19295857849304224, 0.184689110808682, 0.18113082976966813, 0.17718353758256575, 0.17344551287675244, 0.17040397273208832, 0.16786646426818172, 0.1663282346715008, 0.16341736156536485, 0.15905516341067688, 0.158436125051414, 0.1537359721424869, 0.1513394892819748, 0.14856375699125726, 0.14689441248426774, 0.14536592312311428, 0.14343328651771192, 0.14242684273266, 0.1406503601794822, 0.1393786282167882, 0.13764202484385887, 0.13592034180529625, 0.1324187071083542, 0.130499204507232, 0.13078872511116735, 0.1298667062422284, 0.12829874481135164, 0.12658587962559836, 0.1265557255400223, 0.12409385804967862, 0.12425066976857112, 0.12248640367914418, 0.12290682098004235, 0.12024999448820503, 0.12010007985740273, 0.11891874660806459, 0.11791998727174009, 0.11819158434047251, 0.1163523680594759, 0.11725724377985552, 0.11684167584838401, 0.11351355286607115, 0.11244892041648394, 0.1134633706533733, 0.11317019128806534, 0.110188144548532, 0.10979670212286605, 0.1107339688361716, 0.11212559161617378, 0.10857870711738767, 0.10757373190261672, 0.10789481654524695, 0.10777276726605596, 0.10864464577419419, 0.10804430317901875, 0.1084411507566628, 0.10559426481442483, 0.10458914128961903, 0.10259085375686915, 0.1058200664009116, 0.10419391755403139, 0.1016729830340948, 0.10220957370771634, 0.10273274812718193, 0.10042927710563658, 0.10003820689905557, 0.10030018340506709, 0.09705762062093984, 0.0982770308769575, 0.10098097474153145, 0.09974471352222147, 0.09854086561373734, 0.0972544105128175, 0.09523386826853486, 0.09894902619235636, 0.09511885899167362, 0.09537333067649738, 0.0953737006067046, 0.0956656866976855], 'val_losses': [0.45489872602379244, 0.36680532563391405, 0.340575693633798, 0.3194787983547221, 0.30550174848460687, 0.30341945316555236, 0.2767301862355746, 0.29497878553126183, 0.263660333349567, 0.28595739393929703, 0.25099098227209593, 0.28162528551875043, 0.2243227968332, 0.23899515877763605, 0.21348482112532913, 0.1937605715786779, 0.20366367207980815, 0.22516588927390746, 0.22322335930571732, 0.22274662220803676, 0.20123965598529867, 0.2101123922806368, 0.19705715071227434, 0.21177253108240582, 0.2087151457337027, 0.18965124425217705, 0.20281734769467707, 0.17481034241505636, 0.18126990181501162, 0.18963808311382163, 0.18964964574275944, 0.18743221501355004, 0.18135438953563543, 0.1700133929320138, 0.16138217093248203, 0.2073790165747994, 0.18400238091194934, 0.18371397142025808, 0.18244208497423647, 0.18359514244919267, 0.1806552071138785, 0.17824912736841855, 0.17702084652964684, 0.20412424324733125, 0.19127955784804837, 0.20022154937649622, 0.17573692567731455, 0.20004522300628386, 0.20959914609268934, 0.20636094027243077, 0.18544274302872332, 0.18800081268593327, 0.23327100309994328, 0.24301419310694633, 0.19908237878488516, 0.17931486740699015, 0.21408842181953297, 0.20526563809070653, 0.1925177019205686, 0.16846816665941636, 0.1908997684164523, 0.18002615141631098, 0.20787993662153595, 0.16892512265138646, 0.18271597958667105, 0.17434616703733796, 0.16900478357017645, 0.19939055244414194, 0.17798832036314893, 0.18025895759375557, 0.17890510050228195, 0.17605097992552238, 0.17285308578665312, 0.18931329849481493, 0.1489433996615561, 0.16302018923398592, 0.16968021915343123, 0.18886189043051377, 0.192610248460266, 0.20761326976683397, 0.1530010416952534, 0.17824992560276842, 0.1928176372728018, 0.15975781320375454, 0.1544046102068565, 0.17030388566802698, 0.16616259952368795, 0.1664806295103663, 0.16548867991933724, 0.17949803055729308, 0.1467028500430791, 0.15742966826025398], 'val_acc': [0.6972810424758802, 0.7807292319258239, 0.7864929206866308, 0.7896253602305475, 0.7957649417366245, 0.7884976819947375, 0.8185691016163388, 0.8050369627866182, 0.8393685001879464, 0.8139330910913419, 0.8488911164014534, 0.823079814559579, 0.870066407718331, 0.8561583761433404, 0.8798396190953515, 0.9008896128304724, 0.8869815812554818, 0.8721964666081945, 0.8750783109885979, 0.8753289061521112, 0.8879839619095351, 0.8899887232176419, 0.8939982458338555, 0.8891116401453452, 0.8912416990352087, 0.9000125297581757, 0.8968800902142589, 0.9125422879338428, 0.9089086580628993, 0.9087833604811427, 0.9020172910662824, 0.9060268136824959, 0.9081568725723593, 0.9188071670216765, 0.9234431775466734, 0.8928705675980454, 0.9092845508081694, 0.9099110387169528, 0.9081568725723593, 0.909033955644656, 0.9126675855155996, 0.9112893121162762, 0.914045858914923, 0.8944994361608821, 0.9139205613331662, 0.9027690765568225, 0.9186818694399198, 0.9003884225034456, 0.8977571732865556, 0.9066533015912793, 0.9121663951885729, 0.9047738378649292, 0.8763312868061647, 0.891868186943992, 0.9075303846635759, 0.9164265129682997, 0.8971306853777722, 0.9016413983210124, 0.899887232176419, 0.9210625234932965, 0.901766695902769, 0.9214384162385666, 0.9085327653176294, 0.9223154993108633, 0.9184312742764065, 0.9236937727101867, 0.9245708557824834, 0.8988848515223656, 0.9171782984588397, 0.92356847512843, 0.9225660944743767, 0.9218143089838366, 0.9229419872196467, 0.9130434782608695, 0.932339305851397, 0.9224407968926199, 0.9248214509459968, 0.9059015161007392, 0.9060268136824959, 0.9001378273399323, 0.9333416865054505, 0.9139205613331662, 0.9054003257737125, 0.9230672848014033, 0.9264503195088335, 0.9194336549304598, 0.9229419872196467, 0.9214384162385666, 0.9219396065655933, 0.9076556822453327, 0.9319634131061271, 0.9263250219270768], 'final_model_size_bytes': 48262, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0009619395903820557, 'batch_size': 512, 'epochs': 92, 'base_channels': 12, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.221277012075841, 'weight_decay': 2.7986513402972983e-05, 'gamma_focal': 1.874405551702616, 'effective_beta': 0.9335148481813224, 'label_smoothing': 0.09612037052947485, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.21828043310487244, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}, 'model_parameter_count': 9443, 'model_storage_size_kb': 40.575390625000004, 'model_size_validation': 'PASS'}
2025-10-03 02:45:25,782 - INFO - _models.training_function_executor - BO Objective: base=0.9263, size_penalty=0.0000, final=0.9263
2025-10-03 02:45:25,782 - INFO - _models.training_function_executor - Model: 9,443 parameters, 40.6KB (PASS 256KB limit)
2025-10-03 02:45:25,782 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 215.669s
2025-10-03 02:45:25,891 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9263
2025-10-03 02:45:25,892 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.109s
2025-10-03 02:45:25,892 - INFO - bo.run_bo - Recorded observation #23: hparams={'lr': 0.0009619395903820557, 'batch_size': np.int64(512), 'epochs': np.int64(92), 'base_channels': np.int64(12), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.221277012075841, 'weight_decay': 2.7986513402972983e-05, 'gamma_focal': 1.874405551702616, 'effective_beta': 0.9335148481813224, 'label_smoothing': 0.09612037052947485, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.21828043310487244, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)}, value=0.9263
2025-10-03 02:45:25,892 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 23: {'lr': 0.0009619395903820557, 'batch_size': np.int64(512), 'epochs': np.int64(92), 'base_channels': np.int64(12), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.221277012075841, 'weight_decay': 2.7986513402972983e-05, 'gamma_focal': 1.874405551702616, 'effective_beta': 0.9335148481813224, 'label_smoothing': 0.09612037052947485, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.21828043310487244, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)} -> 0.9263
2025-10-03 02:45:25,892 - INFO - bo.run_bo - ðŸ”BO Trial 24: Using RF surrogate + Expected Improvement
2025-10-03 02:45:25,892 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:45:25,892 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:45:25,892 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:45:25,892 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.000324203386268388, 'batch_size': 64, 'epochs': 64, 'base_channels': 14, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.007448024364713536, 'weight_decay': 9.717229937829644e-05, 'gamma_focal': 1.169271829469713, 'effective_beta': 0.9184135919495827, 'label_smoothing': 0.09981709914231367, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23492564587987158, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:45:25,894 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.000324203386268388, 'batch_size': 64, 'epochs': 64, 'base_channels': 14, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.007448024364713536, 'weight_decay': 9.717229937829644e-05, 'gamma_focal': 1.169271829469713, 'effective_beta': 0.9184135919495827, 'label_smoothing': 0.09981709914231367, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23492564587987158, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:49:06,246 - INFO - _models.training_function_executor - Model: 51,757 parameters, 111.2KB storage
2025-10-03 02:49:06,246 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5526443331454644, 0.31132292085429036, 0.2490286580827084, 0.2145280005872304, 0.19303641596448534, 0.17564810249140633, 0.16139671369186287, 0.14855791307283983, 0.13995296776278998, 0.1338214427807608, 0.12453759971290089, 0.11857502065269399, 0.1117735900783043, 0.10814213009558621, 0.10123300326593908, 0.0980228362731955, 0.09240568093432872, 0.090609474577915, 0.08749192641289741, 0.08519805973601059, 0.0819351351032728, 0.07715016860418741, 0.07625080348456702, 0.07244937480317305, 0.07201983463903301, 0.07180318598944864, 0.069586210436379, 0.0656716742943596, 0.06428881860671667, 0.06166481883350661, 0.06147814908182475, 0.058226353946724284, 0.05865524549842298, 0.055351392988235076, 0.0584016470257591, 0.053059242204848575, 0.05280999510016745, 0.05177332239548065, 0.05111770703427906, 0.05006238517445935, 0.048873433919325436, 0.04712796898671343, 0.04572739043915448, 0.04838661192955722, 0.04332001635216651, 0.04634850972693672, 0.04192706136069758, 0.042942126225735455, 0.03898980060676348, 0.04230353855506151, 0.04071180701596433, 0.038750564389269916, 0.038005108242758115, 0.03865305907612231, 0.03598883840840861, 0.03681496240672742, 0.03565590873133644, 0.03428922855089418, 0.034550659045139945, 0.033067121968523, 0.03223091666025444, 0.03309984160426738, 0.031553666728126685, 0.032686908832241], 'val_losses': [0.2679246335049169, 0.19351878312279028, 0.1665764402445509, 0.16300428946907547, 0.15506556153935966, 0.13554134092083284, 0.1271975852232606, 0.12866917455865542, 0.10716620858914122, 0.1030835688577202, 0.09645430148282665, 0.10205209950892313, 0.10017812822624161, 0.10152062771090617, 0.09697482115048213, 0.09082365753436025, 0.08870471408083062, 0.08204794202647313, 0.08869485508535249, 0.08276696194720856, 0.08206084331777845, 0.10328457072281984, 0.07711185655144578, 0.07908202073894009, 0.08188671594708057, 0.07869520749443532, 0.07294441018449141, 0.07910356376086272, 0.07940683622450447, 0.0822236053426229, 0.07092018818858273, 0.07821430968485475, 0.07878829880871131, 0.0796857580156079, 0.07232081263140917, 0.0737310698562331, 0.07434587153915236, 0.07023420451562994, 0.08471279816585083, 0.07232620931443916, 0.07044885092369522, 0.07609539452971499, 0.06957087077572895, 0.07097090282784924, 0.08400778955810427, 0.07303240400495244, 0.07220132802418787, 0.07116649034267947, 0.07347394466258249, 0.07770319509858731, 0.07453140767837076, 0.07516668528195178, 0.0734585788319873, 0.07626197484595303, 0.07137901309535492, 0.0737270460666055, 0.07248444759058573, 0.10423268876291131, 0.07301414891743238, 0.07433141476527565, 0.07555887275972308, 0.07718089241205399, 0.07390096184678908, 0.0818709894994225], 'val_acc': [0.8967547926325022, 0.9325899010149105, 0.936975316376394, 0.9352211502318005, 0.938854780102744, 0.9451196591905776, 0.946497932589901, 0.945746147099361, 0.954141085077058, 0.954141085077058, 0.9579000125297582, 0.9555193584763814, 0.9556446560581381, 0.955268763312868, 0.9568976318757048, 0.9589023931838115, 0.9599047738378649, 0.9631625109635384, 0.9597794762561083, 0.9635384037088084, 0.9627866182182684, 0.9536398947500313, 0.9650419746898885, 0.9657937601804285, 0.9639142964540784, 0.9669214384162386, 0.9698032827966421, 0.9671720335797519, 0.9640395940358351, 0.9640395940358351, 0.9705550682871821, 0.9679238190702919, 0.9676732239067786, 0.968299711815562, 0.9715574489412354, 0.9710562586142087, 0.9693020924696153, 0.9728104247588022, 0.9666708432527252, 0.9729357223405588, 0.9754416739756923, 0.9719333416865055, 0.9749404836486656, 0.9714321513594788, 0.9657937601804285, 0.9720586392682621, 0.9713068537777221, 0.9750657812304222, 0.9705550682871821, 0.9739381029946123, 0.9716827465229921, 0.9729357223405588, 0.9730610199223155, 0.9748151860669089, 0.9751910788121789, 0.9745645909033955, 0.976694649793259, 0.9538904899135446, 0.9740634005763689, 0.9745645909033955, 0.9718080441047487, 0.9716827465229921, 0.9725598295952889, 0.9679238190702919], 'final_model_size_bytes': 114118, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.000324203386268388, 'batch_size': 64, 'epochs': 64, 'base_channels': 14, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.007448024364713536, 'weight_decay': 9.717229937829644e-05, 'gamma_focal': 1.169271829469713, 'effective_beta': 0.9184135919495827, 'label_smoothing': 0.09981709914231367, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.23492564587987158, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}, 'model_parameter_count': 51757, 'model_storage_size_kb': 111.19667968750001, 'model_size_validation': 'PASS'}
2025-10-03 02:49:06,246 - INFO - _models.training_function_executor - BO Objective: base=0.9679, size_penalty=0.0000, final=0.9679
2025-10-03 02:49:06,246 - INFO - _models.training_function_executor - Model: 51,757 parameters, 111.2KB (PASS 256KB limit)
2025-10-03 02:49:06,246 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 220.354s
2025-10-03 02:49:06,355 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9679
2025-10-03 02:49:06,355 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.109s
2025-10-03 02:49:06,355 - INFO - bo.run_bo - Recorded observation #24: hparams={'lr': 0.000324203386268388, 'batch_size': np.int64(64), 'epochs': np.int64(64), 'base_channels': np.int64(14), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.007448024364713536, 'weight_decay': 9.717229937829644e-05, 'gamma_focal': 1.169271829469713, 'effective_beta': 0.9184135919495827, 'label_smoothing': 0.09981709914231367, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.23492564587987158, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)}, value=0.9679
2025-10-03 02:49:06,355 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 24: {'lr': 0.000324203386268388, 'batch_size': np.int64(64), 'epochs': np.int64(64), 'base_channels': np.int64(14), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.007448024364713536, 'weight_decay': 9.717229937829644e-05, 'gamma_focal': 1.169271829469713, 'effective_beta': 0.9184135919495827, 'label_smoothing': 0.09981709914231367, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.23492564587987158, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)} -> 0.9679
2025-10-03 02:49:06,355 - INFO - bo.run_bo - ðŸ”BO Trial 25: Using RF surrogate + Expected Improvement
2025-10-03 02:49:06,355 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:49:06,355 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:49:06,355 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:49:06,355 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 7.195485075768843e-05, 'batch_size': 128, 'epochs': 56, 'base_channels': 15, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.0639488666739856, 'weight_decay': 0.0002863484737371365, 'gamma_focal': 1.0622865054871737, 'effective_beta': 0.9631974701480782, 'label_smoothing': 0.09589449271838434, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.2594117910983217, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 02:49:06,357 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 7.195485075768843e-05, 'batch_size': 128, 'epochs': 56, 'base_channels': 15, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.0639488666739856, 'weight_decay': 0.0002863484737371365, 'gamma_focal': 1.0622865054871737, 'effective_beta': 0.9631974701480782, 'label_smoothing': 0.09589449271838434, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.2594117910983217, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 02:56:17,680 - INFO - _models.training_function_executor - Model: 43,211 parameters, 185.7KB storage
2025-10-03 02:56:17,680 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9210125828506814, 0.7601629054433195, 0.6963947369702284, 0.654929790024196, 0.6217575289389387, 0.5952187190775117, 0.5745053204344303, 0.5543886149516323, 0.5374105829685905, 0.5214118700554776, 0.50621407878843, 0.4924748443173936, 0.47568402700350737, 0.4568865220969408, 0.44245507840782916, 0.42909306938643765, 0.41948618186035863, 0.41144010330313696, 0.40574118057420533, 0.39906846848726796, 0.39206746621123856, 0.38598318832227396, 0.38109842214261774, 0.3758263243845209, 0.369396873035808, 0.364750043174798, 0.35925404547083906, 0.35438474817020166, 0.34964850965081884, 0.34395534576140513, 0.3404476550662452, 0.3362175376830394, 0.3314187166400189, 0.326647770225301, 0.3235313451920359, 0.32103436507771116, 0.31735262164403943, 0.31360130699291505, 0.31023089395130077, 0.30598662723530623, 0.30199797644742815, 0.3013255870471384, 0.2988303474890739, 0.29540736327053596, 0.29278730691259836, 0.2899185085390518, 0.2884638003250725, 0.2839622412158355, 0.2807556397088018, 0.2795505622414697, 0.27689554523942067, 0.27555695599752983, 0.2742878194583201, 0.2698379577295134, 0.2689788633453782, 0.26517831938278924], 'val_losses': [0.6625136727080461, 0.6027727862443113, 0.5777340390608732, 0.5424241926560738, 0.5012793309772811, 0.49227008979327397, 0.4887561579348673, 0.4584302542435467, 0.44989268739948507, 0.43946229382208635, 0.41929045970794504, 0.4267824816626873, 0.39753290604766844, 0.3844297632823357, 0.35804582739806895, 0.35673030947827855, 0.35422283697249285, 0.3481431066945089, 0.340913362531282, 0.34847358810903134, 0.35314701944346893, 0.3135232062021483, 0.31481499120438106, 0.3033959757335472, 0.3032935942948992, 0.2984703192434847, 0.2914235022223813, 0.29494738809940824, 0.27793401549363134, 0.29210111505927605, 0.2859144222690055, 0.27790474952529387, 0.2632358949437743, 0.27137875557505925, 0.26410054047743103, 0.25672067461336723, 0.24940034059291644, 0.24617179261272884, 0.25385306052636747, 0.2604257162240734, 0.24613852910963518, 0.2375287873701499, 0.2319266543160135, 0.24223376349195355, 0.23274027429126196, 0.23303382936620037, 0.23777694256140494, 0.22902078298851147, 0.22281021150658714, 0.23646245511077638, 0.2181223870134043, 0.232345850239002, 0.21487232704962306, 0.21475826455878816, 0.22041904210835198, 0.21107388858642215], 'val_acc': [0.7058012780353339, 0.7227164515724847, 0.7250971056258614, 0.7352462097481519, 0.7579250720461095, 0.7650670342062398, 0.7678235810048866, 0.785239944869064, 0.7903771457210876, 0.800902142588648, 0.8027816063149982, 0.7980202982082446, 0.8065405337676983, 0.8201979701791755, 0.8343565969176795, 0.8344818944994361, 0.8369878461345696, 0.8411226663325397, 0.8437539155494299, 0.8418744518230799, 0.8319759428643028, 0.8562836737250971, 0.8549054003257737, 0.8641774213757675, 0.8600426011777973, 0.863175040721714, 0.8731988472622478, 0.8699411101365744, 0.8833479513845383, 0.860919684250094, 0.8718205738629244, 0.8759553940608946, 0.886104498183185, 0.8835985465480516, 0.8869815812554818, 0.891492294198722, 0.8968800902142589, 0.8976318757047989, 0.8938729482520987, 0.8921187821075053, 0.8977571732865556, 0.9032702668838491, 0.9066533015912793, 0.9036461596291192, 0.9085327653176294, 0.9042726475379025, 0.9031449693020924, 0.9065280040095226, 0.9104122290439794, 0.9013908031574991, 0.9115399072797895, 0.9048991354466859, 0.9146723468237064, 0.9119158000250596, 0.9112893121162762, 0.9170530008770831], 'final_model_size_bytes': 181083, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 7.195485075768843e-05, 'batch_size': 128, 'epochs': 56, 'base_channels': 15, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.0639488666739856, 'weight_decay': 0.0002863484737371365, 'gamma_focal': 1.0622865054871737, 'effective_beta': 0.9631974701480782, 'label_smoothing': 0.09589449271838434, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.2594117910983217, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}, 'model_parameter_count': 43211, 'model_storage_size_kb': 185.67226562500002, 'model_size_validation': 'PASS'}
2025-10-03 02:56:17,680 - INFO - _models.training_function_executor - BO Objective: base=0.9171, size_penalty=0.0000, final=0.9171
2025-10-03 02:56:17,680 - INFO - _models.training_function_executor - Model: 43,211 parameters, 185.7KB (PASS 256KB limit)
2025-10-03 02:56:17,680 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 431.325s
2025-10-03 02:56:17,790 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9171
2025-10-03 02:56:17,790 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.109s
2025-10-03 02:56:17,790 - INFO - bo.run_bo - Recorded observation #25: hparams={'lr': 7.195485075768843e-05, 'batch_size': np.int64(128), 'epochs': np.int64(56), 'base_channels': np.int64(15), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.0639488666739856, 'weight_decay': 0.0002863484737371365, 'gamma_focal': 1.0622865054871737, 'effective_beta': 0.9631974701480782, 'label_smoothing': 0.09589449271838434, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.2594117910983217, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)}, value=0.9171
2025-10-03 02:56:17,790 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 25: {'lr': 7.195485075768843e-05, 'batch_size': np.int64(128), 'epochs': np.int64(56), 'base_channels': np.int64(15), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.0639488666739856, 'weight_decay': 0.0002863484737371365, 'gamma_focal': 1.0622865054871737, 'effective_beta': 0.9631974701480782, 'label_smoothing': 0.09589449271838434, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.2594117910983217, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)} -> 0.9171
2025-10-03 02:56:17,790 - INFO - bo.run_bo - ðŸ”BO Trial 26: Using RF surrogate + Expected Improvement
2025-10-03 02:56:17,790 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:56:17,790 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:56:17,790 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:56:17,790 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0010249839995496405, 'batch_size': 128, 'epochs': 61, 'base_channels': 11, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.23723833029107422, 'weight_decay': 0.0008564278713561599, 'gamma_focal': 1.6029985684141757, 'effective_beta': 0.9185509773556877, 'label_smoothing': 0.09699329380434667, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.17705567104097814, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:56:17,792 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0010249839995496405, 'batch_size': 128, 'epochs': 61, 'base_channels': 11, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.23723833029107422, 'weight_decay': 0.0008564278713561599, 'gamma_focal': 1.6029985684141757, 'effective_beta': 0.9185509773556877, 'label_smoothing': 0.09699329380434667, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.17705567104097814, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 02:57:55,202 - INFO - _models.training_function_executor - Model: 43,189 parameters, 185.6KB storage
2025-10-03 02:57:55,203 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4299745876060237, 0.27250205344045175, 0.22161648268417408, 0.19808899092473725, 0.18277492973976475, 0.1711040297965711, 0.1618309801627799, 0.15770470971874145, 0.15039511586190263, 0.14138736134310456, 0.14056449558051298, 0.13443890462015645, 0.13015004877562705, 0.12780715640446066, 0.12390108996419288, 0.12146346031577614, 0.11717910348192286, 0.11742369508293679, 0.10991374518160638, 0.1104513484993622, 0.10567049240795617, 0.10282977032616814, 0.10433371866887006, 0.09979250426495638, 0.0994518331485985, 0.09786148930247204, 0.09767537196119698, 0.09433362491174921, 0.09241126177295857, 0.09230892726799617, 0.09054762239237422, 0.08985391240148613, 0.08697330495175445, 0.08500683979070511, 0.08554559406667366, 0.08270152643730448, 0.08408587952550109, 0.08082222660777488, 0.08043409368552551, 0.0786147337071139, 0.07810997675809182, 0.07926245574364955, 0.07616559568670868, 0.07734133076063113, 0.07747326978280335, 0.07430247132555344, 0.07455688342697112, 0.07388523704861391, 0.07263324442275665, 0.0723912872060612, 0.07204372445879056, 0.07274570888774166, 0.0714195206898128, 0.06845285703833287, 0.06967995496669166, 0.06919035307998381, 0.0699042763834196, 0.06977780813040453, 0.06791824668447942, 0.06736717530322382, 0.06669018866911895], 'val_losses': [0.3519293263642326, 0.21519697657236106, 0.18841586323694662, 0.19646086410488972, 0.19028164635115646, 0.16031459978373333, 0.19039326852805752, 0.15588334778200308, 0.17156168378753264, 0.1885494790600664, 0.16974617081908203, 0.13776344161580087, 0.1539354162198972, 0.15095033541474856, 0.14011428441608212, 0.12867756275353467, 0.12510398001388012, 0.13582296578369707, 0.11634571070514098, 0.12721584756158374, 0.12293568068022806, 0.14158257415716308, 0.12608289408412676, 0.12790127506657226, 0.1021160259852208, 0.11426975213989192, 0.1216692847463813, 0.11548750099532662, 0.1144328197244265, 0.10948295840780443, 0.1138275264106647, 0.11101277647737183, 0.10639415183692122, 0.10641748179672147, 0.10103605376562817, 0.10459471065091275, 0.1183033864238017, 0.1121180983541036, 0.10382148693668679, 0.09942233928883827, 0.10197699638035733, 0.10900027019691384, 0.09009970112180608, 0.11661445502275333, 0.10833471395470208, 0.10963434954437004, 0.10868168577165356, 0.10226559547303449, 0.09876461894948177, 0.09914886594180879, 0.09760426163445617, 0.09122479102629018, 0.11834874897989887, 0.09685827290099928, 0.10941052458300027, 0.09971114422982684, 0.10210173933775768, 0.10206374534239014, 0.09869422128872381, 0.0944231460219845, 0.08875104476850727], 'val_acc': [0.7983961909535146, 0.8858539030196717, 0.9145470492419496, 0.9091592532264128, 0.9051497306101992, 0.9178047863676231, 0.9084074677358727, 0.9345946623230171, 0.9181806791128931, 0.9119158000250596, 0.9201854404209999, 0.9334669840872071, 0.9259491291818068, 0.9278285929081569, 0.9408595414108508, 0.9378523994486906, 0.946122039844631, 0.9394812680115274, 0.9453702543540909, 0.9439919809547676, 0.9423631123919308, 0.938854780102744, 0.9426137075554442, 0.9363488284676106, 0.9538904899135446, 0.9487532890615211, 0.9429896003007142, 0.9467485277534143, 0.9480015035709811, 0.955268763312868, 0.9448690640270643, 0.9521363237689513, 0.9521363237689513, 0.9577747149480015, 0.9568976318757048, 0.953765192331788, 0.9557699536398947, 0.9536398947500313, 0.9540157874953014, 0.9563964415486781, 0.955268763312868, 0.9517604310236812, 0.9639142964540784, 0.9452449567723343, 0.9528881092594913, 0.9507580503696279, 0.9563964415486781, 0.9528881092594913, 0.9579000125297582, 0.9591529883473249, 0.961408344818945, 0.961408344818945, 0.9469991229169277, 0.9579000125297582, 0.9490038842250345, 0.9584012028567849, 0.9557699536398947, 0.955268763312868, 0.9556446560581381, 0.9576494173662449, 0.9644154867811051], 'final_model_size_bytes': 183366, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0010249839995496405, 'batch_size': 128, 'epochs': 61, 'base_channels': 11, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.23723833029107422, 'weight_decay': 0.0008564278713561599, 'gamma_focal': 1.6029985684141757, 'effective_beta': 0.9185509773556877, 'label_smoothing': 0.09699329380434667, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.17705567104097814, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 5}, 'model_parameter_count': 43189, 'model_storage_size_kb': 185.577734375, 'model_size_validation': 'PASS'}
2025-10-03 02:57:55,203 - INFO - _models.training_function_executor - BO Objective: base=0.9644, size_penalty=0.0000, final=0.9644
2025-10-03 02:57:55,203 - INFO - _models.training_function_executor - Model: 43,189 parameters, 185.6KB (PASS 256KB limit)
2025-10-03 02:57:55,203 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 97.412s
2025-10-03 02:57:55,312 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9644
2025-10-03 02:57:55,312 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.110s
2025-10-03 02:57:55,312 - INFO - bo.run_bo - Recorded observation #26: hparams={'lr': 0.0010249839995496405, 'batch_size': np.int64(128), 'epochs': np.int64(61), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.23723833029107422, 'weight_decay': 0.0008564278713561599, 'gamma_focal': 1.6029985684141757, 'effective_beta': 0.9185509773556877, 'label_smoothing': 0.09699329380434667, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.17705567104097814, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)}, value=0.9644
2025-10-03 02:57:55,313 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 26: {'lr': 0.0010249839995496405, 'batch_size': np.int64(128), 'epochs': np.int64(61), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.23723833029107422, 'weight_decay': 0.0008564278713561599, 'gamma_focal': 1.6029985684141757, 'effective_beta': 0.9185509773556877, 'label_smoothing': 0.09699329380434667, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.17705567104097814, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)} -> 0.9644
2025-10-03 02:57:55,313 - INFO - bo.run_bo - ðŸ”BO Trial 27: Using RF surrogate + Expected Improvement
2025-10-03 02:57:55,313 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 02:57:55,313 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 02:57:55,313 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 02:57:55,313 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0006022440520018222, 'batch_size': 256, 'epochs': 77, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.23669458509533992, 'weight_decay': 4.8998922589907114e-05, 'gamma_focal': 2.4212467091320296, 'effective_beta': 0.9400220053294943, 'label_smoothing': 0.09850484469471253, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.2878988636540029, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 2}
2025-10-03 02:57:55,314 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0006022440520018222, 'batch_size': 256, 'epochs': 77, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.23669458509533992, 'weight_decay': 4.8998922589907114e-05, 'gamma_focal': 2.4212467091320296, 'effective_beta': 0.9400220053294943, 'label_smoothing': 0.09850484469471253, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.2878988636540029, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 2}
2025-10-03 03:08:04,226 - INFO - _models.training_function_executor - Model: 34,487 parameters, 74.1KB storage
2025-10-03 03:08:04,228 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.36769314688018245, 0.27393855262201494, 0.22649727544177203, 0.19432501451426856, 0.17607136376612176, 0.1625802108962946, 0.15125218117765118, 0.1433226107226291, 0.1378683840599633, 0.13509964902251861, 0.13065460917873908, 0.12856463124146494, 0.12276695457243984, 0.11979546139605887, 0.11688104954187618, 0.11674365249423012, 0.11059481081603946, 0.10849605831648292, 0.10605996140440996, 0.10369821474891078, 0.10230157266052109, 0.10143972251880394, 0.0985843534546792, 0.09635202948128756, 0.09547763942917857, 0.09562234077906619, 0.09246834590589963, 0.09045169206905443, 0.08848786561387272, 0.09196151032751188, 0.0874634073238562, 0.08527133218621757, 0.08422979882839265, 0.0835029521078143, 0.08346196879553913, 0.0814675569265434, 0.0807660158946599, 0.07903383967268532, 0.0803353922174426, 0.07864214620565414, 0.07936073827388027, 0.07628557339688274, 0.07504906063511442, 0.07620132596320131, 0.07400307068973803, 0.07395961274167476, 0.07206208159313802, 0.07256665744529954, 0.07110496743798397, 0.07207427370812634, 0.06974720116480063, 0.06979808234712502, 0.06974006467344945, 0.06783612408272564, 0.06820045546674192, 0.0665457852464143, 0.06733136254801277, 0.06582469642848496, 0.06574126027791238, 0.06458330801615637, 0.06314474207835774, 0.06508130667977337, 0.0634882334852336, 0.06326617609918504, 0.06368224091422467, 0.06255902735396174, 0.06355075556479708, 0.06155898214178669, 0.0625843271240627, 0.061419560456102855, 0.061104649946098696, 0.05979402859468894, 0.058840655067438054, 0.06016027813013143, 0.05916120609137788, 0.0593238824627974, 0.05846527362634965], 'val_losses': [0.3063234898802362, 0.25819049659281385, 0.22780625896112705, 0.2214780763928685, 0.2237192887760108, 0.20359811370324377, 0.18243006941796006, 0.18874448053506127, 0.17232326893799885, 0.17137422286699386, 0.1746052042155797, 0.1788715829774261, 0.16529299194256236, 0.17464962542101847, 0.16634544001838764, 0.1705508240238495, 0.17259476756283312, 0.15222453419060775, 0.1970085768364468, 0.16628441298953453, 0.19433439783532583, 0.18971441796871225, 0.1639263794664422, 0.17111550217925897, 0.1702117078709223, 0.16932977269786503, 0.15727449791493564, 0.21430774588899168, 0.17852537754431597, 0.19871574343408413, 0.16601619728039327, 0.1530112256409754, 0.14104686280514808, 0.15606926373932956, 0.1842772642288452, 0.16498147858785905, 0.18613463335505645, 0.14304550099068156, 0.13538504219110442, 0.15731252787958902, 0.16079871547116087, 0.134656354068007, 0.15037600021324365, 0.15641198775819362, 0.1353760976594069, 0.13893336580445215, 0.1447380662446123, 0.14169926807898625, 0.13550710051882134, 0.12236534140265655, 0.12825082008170388, 0.1368391701353924, 0.15478919020587528, 0.12228686646954515, 0.1532989065553174, 0.1269988836840779, 0.13343230286810784, 0.1332627539328653, 0.12737584981519826, 0.13711996489472741, 0.12293851865874901, 0.12150013037357335, 0.12088893282803329, 0.12121761658670849, 0.1071941560110253, 0.12087189366294393, 0.1212427792838456, 0.12309543596684014, 0.13870295127924934, 0.11742962925763614, 0.11505152467139278, 0.10840983118998677, 0.12198307008100115, 0.11759792830827256, 0.1088937542526932, 0.11451833800196483, 0.10652820016339733], 'val_acc': [0.7588021551184062, 0.792507204610951, 0.815812554817692, 0.8129307104372886, 0.8277158250845759, 0.8539030196717203, 0.878085452950758, 0.8738253351710312, 0.8844756296203483, 0.8877333667460218, 0.8772083698784613, 0.8848515223656184, 0.884225034456835, 0.8838491417115649, 0.8908658062899386, 0.8818443804034583, 0.8843503320385917, 0.8976318757047989, 0.8531512341811803, 0.8829720586392683, 0.8735747400075179, 0.8734494424257612, 0.8927452700162887, 0.8823455707304849, 0.8852274151108883, 0.8896128304723718, 0.8874827715825084, 0.8511464728730735, 0.8748277158250846, 0.8706928956271144, 0.8901140207993986, 0.8950006264879088, 0.9096604435534394, 0.891868186943992, 0.8798396190953515, 0.8943741385791254, 0.8833479513845383, 0.9069038967547927, 0.9061521112642525, 0.8987595539406089, 0.8976318757047989, 0.908282170154116, 0.9008896128304724, 0.9069038967547927, 0.9111640145345195, 0.915173537150733, 0.8982583636135822, 0.9121663951885729, 0.9096604435534394, 0.92544793885478, 0.9052750281919559, 0.9120410976068162, 0.9122916927703295, 0.9236937727101867, 0.9035208620473625, 0.9131687758426262, 0.9109134193710061, 0.914045858914923, 0.92206490414735, 0.9142964540784363, 0.9194336549304598, 0.9216890114020799, 0.9196842500939731, 0.9234431775466734, 0.9350958526500438, 0.9263250219270768, 0.929708056634507, 0.9198095476757299, 0.9116652048615461, 0.9292068663074803, 0.9278285929081569, 0.9340934719959905, 0.92206490414735, 0.9221902017291066, 0.9324646034331537, 0.9324646034331537, 0.938102994612204], 'final_model_size_bytes': 77339, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0006022440520018222, 'batch_size': 256, 'epochs': 77, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.23669458509533992, 'weight_decay': 4.8998922589907114e-05, 'gamma_focal': 2.4212467091320296, 'effective_beta': 0.9400220053294943, 'label_smoothing': 0.09850484469471253, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.2878988636540029, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 2}, 'model_parameter_count': 34487, 'model_storage_size_kb': 74.09316406250001, 'model_size_validation': 'PASS'}
2025-10-03 03:08:04,228 - INFO - _models.training_function_executor - BO Objective: base=0.9381, size_penalty=0.0000, final=0.9381
2025-10-03 03:08:04,228 - INFO - _models.training_function_executor - Model: 34,487 parameters, 74.1KB (PASS 256KB limit)
2025-10-03 03:08:04,228 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 608.915s
2025-10-03 03:08:04,338 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9381
2025-10-03 03:08:04,338 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.110s
2025-10-03 03:08:04,338 - INFO - bo.run_bo - Recorded observation #27: hparams={'lr': 0.0006022440520018222, 'batch_size': np.int64(256), 'epochs': np.int64(77), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.23669458509533992, 'weight_decay': 4.8998922589907114e-05, 'gamma_focal': 2.4212467091320296, 'effective_beta': 0.9400220053294943, 'label_smoothing': 0.09850484469471253, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.2878988636540029, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(2)}, value=0.9381
2025-10-03 03:08:04,338 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 27: {'lr': 0.0006022440520018222, 'batch_size': np.int64(256), 'epochs': np.int64(77), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.23669458509533992, 'weight_decay': 4.8998922589907114e-05, 'gamma_focal': 2.4212467091320296, 'effective_beta': 0.9400220053294943, 'label_smoothing': 0.09850484469471253, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.2878988636540029, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(2)} -> 0.9381
2025-10-03 03:08:04,338 - INFO - bo.run_bo - ðŸ”BO Trial 28: Using RF surrogate + Expected Improvement
2025-10-03 03:08:04,338 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:08:04,338 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:08:04,339 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:08:04,339 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.000800771158623669, 'batch_size': 64, 'epochs': 27, 'base_channels': 7, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.057036567577573596, 'weight_decay': 1.1078822183192881e-06, 'gamma_focal': 1.3376507829937383, 'effective_beta': 0.9407876907010586, 'label_smoothing': 0.0960647722624982, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.06262429573916597, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 03:08:04,340 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.000800771158623669, 'batch_size': 64, 'epochs': 27, 'base_channels': 7, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.057036567577573596, 'weight_decay': 1.1078822183192881e-06, 'gamma_focal': 1.3376507829937383, 'effective_beta': 0.9407876907010586, 'label_smoothing': 0.0960647722624982, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.06262429573916597, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 03:08:50,304 - INFO - _models.training_function_executor - Model: 34,545 parameters, 74.2KB storage
2025-10-03 03:08:50,305 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.44577116370518527, 0.26277686944601025, 0.20942343022408605, 0.18204808082296536, 0.1616054885438319, 0.14931499691200978, 0.1414287851318935, 0.1291868719692859, 0.12511570001497674, 0.11716004652898458, 0.1145969469978842, 0.10704056111346466, 0.10446219585192472, 0.10162215683314822, 0.09779741366237521, 0.09852103827606334, 0.09382867033836989, 0.09165511011628752, 0.08929701804654029, 0.08942638335960902, 0.08648345763991565, 0.08459224822697804, 0.08430449853780496, 0.08516714227348272, 0.07946322011539848, 0.07995786211531185, 0.07897999946099062], 'val_losses': [0.2797587184496145, 0.20337300977283754, 0.16620091069851464, 0.15607858549809564, 0.13675394679736283, 0.13080155952659606, 0.11843182515064034, 0.11300104181028416, 0.10285399218115901, 0.1051916958991279, 0.10697108530028096, 0.0902887414433711, 0.08748937868979353, 0.09297357582521124, 0.08848253871824056, 0.08217340738352356, 0.08096890382925201, 0.08108343104083292, 0.07950640687771032, 0.08298755172477182, 0.09074280164737239, 0.07616533690547976, 0.08031364382732305, 0.07212550209408311, 0.08364350839286143, 0.0790592355033073, 0.07335730734547417], 'val_acc': [0.8661821826838741, 0.9139205613331662, 0.9277032953264002, 0.9314622227791004, 0.9376018042851773, 0.9452449567723343, 0.9468738253351711, 0.9497556697155745, 0.9548928705675981, 0.9543916802405713, 0.9527628116777346, 0.9602806665831349, 0.9592782859290816, 0.9626613206365117, 0.9597794762561083, 0.9639142964540784, 0.9657937601804285, 0.9642901891993485, 0.9657937601804285, 0.9637889988723217, 0.9548928705675981, 0.9649166771081318, 0.9642901891993485, 0.9667961408344818, 0.9607818569101616, 0.9659190577621852, 0.9681744142338052], 'final_model_size_bytes': 79750, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.000800771158623669, 'batch_size': 64, 'epochs': 27, 'base_channels': 7, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.057036567577573596, 'weight_decay': 1.1078822183192881e-06, 'gamma_focal': 1.3376507829937383, 'effective_beta': 0.9407876907010586, 'label_smoothing': 0.0960647722624982, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.06262429573916597, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}, 'model_parameter_count': 34545, 'model_storage_size_kb': 74.2177734375, 'model_size_validation': 'PASS'}
2025-10-03 03:08:50,305 - INFO - _models.training_function_executor - BO Objective: base=0.9682, size_penalty=0.0000, final=0.9682
2025-10-03 03:08:50,305 - INFO - _models.training_function_executor - Model: 34,545 parameters, 74.2KB (PASS 256KB limit)
2025-10-03 03:08:50,305 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 45.966s
2025-10-03 03:08:50,415 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9682
2025-10-03 03:08:50,415 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.110s
2025-10-03 03:08:50,415 - INFO - bo.run_bo - Recorded observation #28: hparams={'lr': 0.000800771158623669, 'batch_size': np.int64(64), 'epochs': np.int64(27), 'base_channels': np.int64(7), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.057036567577573596, 'weight_decay': 1.1078822183192881e-06, 'gamma_focal': 1.3376507829937383, 'effective_beta': 0.9407876907010586, 'label_smoothing': 0.0960647722624982, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.06262429573916597, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)}, value=0.9682
2025-10-03 03:08:50,415 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 28: {'lr': 0.000800771158623669, 'batch_size': np.int64(64), 'epochs': np.int64(27), 'base_channels': np.int64(7), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.057036567577573596, 'weight_decay': 1.1078822183192881e-06, 'gamma_focal': 1.3376507829937383, 'effective_beta': 0.9407876907010586, 'label_smoothing': 0.0960647722624982, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.06262429573916597, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)} -> 0.9682
2025-10-03 03:08:50,416 - INFO - bo.run_bo - ðŸ”BO Trial 29: Using RF surrogate + Expected Improvement
2025-10-03 03:08:50,416 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 03:08:50,416 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:08:50,416 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:08:50,416 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0019960074611107726, 'batch_size': 128, 'epochs': 24, 'base_channels': 12, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.22656295438109775, 'weight_decay': 0.0009562382476590434, 'gamma_focal': 1.5900259417489861, 'effective_beta': 0.9219792269880017, 'label_smoothing': 0.09194625882851266, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.36409923414513456, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 03:08:50,417 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0019960074611107726, 'batch_size': 128, 'epochs': 24, 'base_channels': 12, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.22656295438109775, 'weight_decay': 0.0009562382476590434, 'gamma_focal': 1.5900259417489861, 'effective_beta': 0.9219792269880017, 'label_smoothing': 0.09194625882851266, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.36409923414513456, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 03:09:45,631 - INFO - _models.training_function_executor - Model: 35,035 parameters, 150.5KB storage
2025-10-03 03:09:45,632 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.39714547331812716, 0.25089137081988183, 0.21345637412095575, 0.19780033136733816, 0.18235481499410103, 0.17371210070330484, 0.16435247767230374, 0.1579575336298182, 0.14978001727364182, 0.14703799783389968, 0.14434724627452153, 0.13733147816771094, 0.13198493915046577, 0.13288907373500072, 0.12698173880202726, 0.12512253806253118, 0.12176205901257456, 0.12037923637931255, 0.11853421701652993, 0.11504316930427619, 0.11426641292331143, 0.11119893499945682, 0.10950100520420383, 0.11020772179614757], 'val_losses': [0.30133866126555847, 0.24284517838234682, 0.22425310057725034, 0.22417169262847272, 0.2591334453312142, 0.23173727366973876, 0.17602123990897667, 0.1621951972908199, 0.1601604229550402, 0.18259938662250214, 0.16179653876068417, 0.16685935110367192, 0.16110035581702115, 0.13825017107211174, 0.15993404731877, 0.16444662668239413, 0.13049333451565248, 0.1415962762724575, 0.12515979901267957, 0.13998173881317943, 0.1302270169392398, 0.1377429130224936, 0.1281423971781365, 0.13720377395810557], 'val_acc': [0.8050369627866182, 0.8808419997494048, 0.8917428893622353, 0.8881092594912918, 0.8721964666081945, 0.8898634256358853, 0.9087833604811427, 0.9221902017291066, 0.9230672848014033, 0.9059015161007392, 0.9244455582007267, 0.9225660944743767, 0.9245708557824834, 0.9376018042851773, 0.9239443678737, 0.9268262122541034, 0.9416113269013908, 0.9365994236311239, 0.9394812680115274, 0.9339681744142339, 0.9429896003007142, 0.9372259115399073, 0.9477509084074678, 0.9355970429770706], 'final_model_size_bytes': 150662, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0019960074611107726, 'batch_size': 128, 'epochs': 24, 'base_channels': 12, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.22656295438109775, 'weight_decay': 0.0009562382476590434, 'gamma_focal': 1.5900259417489861, 'effective_beta': 0.9219792269880017, 'label_smoothing': 0.09194625882851266, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.36409923414513456, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 4}, 'model_parameter_count': 35035, 'model_storage_size_kb': 150.541015625, 'model_size_validation': 'PASS'}
2025-10-03 03:09:45,632 - INFO - _models.training_function_executor - BO Objective: base=0.9356, size_penalty=0.0000, final=0.9356
2025-10-03 03:09:45,632 - INFO - _models.training_function_executor - Model: 35,035 parameters, 150.5KB (PASS 256KB limit)
2025-10-03 03:09:45,632 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 55.216s
2025-10-03 03:09:45,865 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9356
2025-10-03 03:09:45,865 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.233s
2025-10-03 03:09:45,865 - INFO - bo.run_bo - Recorded observation #29: hparams={'lr': 0.0019960074611107726, 'batch_size': np.int64(128), 'epochs': np.int64(24), 'base_channels': np.int64(12), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.22656295438109775, 'weight_decay': 0.0009562382476590434, 'gamma_focal': 1.5900259417489861, 'effective_beta': 0.9219792269880017, 'label_smoothing': 0.09194625882851266, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.36409923414513456, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)}, value=0.9356
2025-10-03 03:09:45,865 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 29: {'lr': 0.0019960074611107726, 'batch_size': np.int64(128), 'epochs': np.int64(24), 'base_channels': np.int64(12), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.22656295438109775, 'weight_decay': 0.0009562382476590434, 'gamma_focal': 1.5900259417489861, 'effective_beta': 0.9219792269880017, 'label_smoothing': 0.09194625882851266, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.36409923414513456, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)} -> 0.9356
2025-10-03 03:09:45,865 - INFO - bo.run_bo - ðŸ”BO Trial 30: Using RF surrogate + Expected Improvement
2025-10-03 03:09:45,865 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:09:45,865 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:09:45,865 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:09:45,866 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0020168282031783973, 'batch_size': 128, 'epochs': 31, 'base_channels': 6, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.23465671148543615, 'weight_decay': 2.5835314568806234e-05, 'gamma_focal': 2.4861370688961344, 'effective_beta': 0.9659723416951612, 'label_smoothing': 0.09861598501902999, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.16739128102302298, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 2}
2025-10-03 03:09:45,867 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0020168282031783973, 'batch_size': 128, 'epochs': 31, 'base_channels': 6, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.23465671148543615, 'weight_decay': 2.5835314568806234e-05, 'gamma_focal': 2.4861370688961344, 'effective_beta': 0.9659723416951612, 'label_smoothing': 0.09861598501902999, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.16739128102302298, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 2}
2025-10-03 03:12:01,154 - INFO - _models.training_function_executor - Model: 21,703 parameters, 46.6KB storage
2025-10-03 03:12:01,155 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.38211964317144814, 0.22647816438920385, 0.1836103562409406, 0.16287001491377315, 0.15171161434212235, 0.13708941631054544, 0.12933939914929185, 0.12400204790886901, 0.11346014162370434, 0.1107402956272851, 0.1071723694197836, 0.10331539813905746, 0.09968605362777753, 0.09787728423818107, 0.09465971851584844, 0.09093985450333045, 0.0891708309086474, 0.08556364837716299, 0.08419060994800208, 0.08292644333173041, 0.08195063026143455, 0.07897361492680248, 0.0777949855962475, 0.07674515307325316, 0.0771093298691894, 0.07531616607905225, 0.07234599184890401, 0.07221530197635433, 0.071085914140595, 0.07063331594047004, 0.06975268061664876], 'val_losses': [0.24244237766874088, 0.203680447299608, 0.1989266398205164, 0.30569811969110505, 0.19562476173803153, 0.20580685035452898, 0.18233835618570027, 0.2159809932073485, 0.2785103896882746, 0.21468483840458794, 0.2931335209089926, 0.24332978339396, 0.263007846288313, 0.31414600745610854, 0.2599908549255961, 0.3681589674231194, 0.27448998174673633, 0.23175298546642428, 0.30763712070920596, 0.25227950616238964, 0.28101900660174634, 0.27070664029763075, 0.2792805262136812, 0.4048789735949289, 0.22637265864813363, 0.3907000111198234, 0.32635862697576346, 0.2891195137882245, 0.34374999974981135, 0.3013753594013552, 0.3204581741177786], 'val_acc': [0.800902142588648, 0.8366119533892996, 0.836862548552813, 0.7618092970805663, 0.8260869565217391, 0.8312241573737627, 0.8483899260744268, 0.8372384412980829, 0.7991479764440547, 0.8491417115649668, 0.8005262498433781, 0.8108006515474251, 0.7975191078812179, 0.7777220899636637, 0.8118030322014785, 0.7021676481643905, 0.8021551184062148, 0.8218268387420122, 0.7774714948001503, 0.816188447562962, 0.8190702919433654, 0.8027816063149982, 0.7948878586643278, 0.7348703170028819, 0.8273399323393058, 0.7823581004886606, 0.7814810174163639, 0.7926325021927076, 0.7933842876832478, 0.8213256484149856, 0.807793509585265], 'final_model_size_bytes': 55991, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0020168282031783973, 'batch_size': 128, 'epochs': 31, 'base_channels': 6, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.23465671148543615, 'weight_decay': 2.5835314568806234e-05, 'gamma_focal': 2.4861370688961344, 'effective_beta': 0.9659723416951612, 'label_smoothing': 0.09861598501902999, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.16739128102302298, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 2}, 'model_parameter_count': 21703, 'model_storage_size_kb': 46.627539062500006, 'model_size_validation': 'PASS'}
2025-10-03 03:12:01,155 - INFO - _models.training_function_executor - BO Objective: base=0.8078, size_penalty=0.0000, final=0.8078
2025-10-03 03:12:01,155 - INFO - _models.training_function_executor - Model: 21,703 parameters, 46.6KB (PASS 256KB limit)
2025-10-03 03:12:01,155 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 135.289s
2025-10-03 03:12:01,267 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8078
2025-10-03 03:12:01,267 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.112s
2025-10-03 03:12:01,267 - INFO - bo.run_bo - Recorded observation #30: hparams={'lr': 0.0020168282031783973, 'batch_size': np.int64(128), 'epochs': np.int64(31), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(3), 'dropout': 0.23465671148543615, 'weight_decay': 2.5835314568806234e-05, 'gamma_focal': 2.4861370688961344, 'effective_beta': 0.9659723416951612, 'label_smoothing': 0.09861598501902999, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.16739128102302298, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(2)}, value=0.8078
2025-10-03 03:12:01,267 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 30: {'lr': 0.0020168282031783973, 'batch_size': np.int64(128), 'epochs': np.int64(31), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(3), 'dropout': 0.23465671148543615, 'weight_decay': 2.5835314568806234e-05, 'gamma_focal': 2.4861370688961344, 'effective_beta': 0.9659723416951612, 'label_smoothing': 0.09861598501902999, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.16739128102302298, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(2)} -> 0.8078
2025-10-03 03:12:01,268 - INFO - bo.run_bo - ðŸ”BO Trial 31: Using RF surrogate + Expected Improvement
2025-10-03 03:12:01,268 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:12:01,268 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:12:01,268 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:12:01,268 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00603267132423552, 'batch_size': 512, 'epochs': 30, 'base_channels': 13, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.22830060886209214, 'weight_decay': 2.3151269012373345e-06, 'gamma_focal': 1.8611293305042147, 'effective_beta': 0.9041294707361419, 'label_smoothing': 0.0872646489871901, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1495853761266028, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 03:12:01,269 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00603267132423552, 'batch_size': 512, 'epochs': 30, 'base_channels': 13, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.22830060886209214, 'weight_decay': 2.3151269012373345e-06, 'gamma_focal': 1.8611293305042147, 'effective_beta': 0.9041294707361419, 'label_smoothing': 0.0872646489871901, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1495853761266028, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 03:13:01,930 - INFO - _models.training_function_executor - Model: 9,259 parameters, 19.9KB storage
2025-10-03 03:13:01,930 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4615947259767116, 0.3096752082577468, 0.23821127851520293, 0.19833244774360478, 0.18462449773066408, 0.1670792630443566, 0.15330582146966037, 0.15472700966025146, 0.14896845913536808, 0.13982582583482472, 0.13182579673397485, 0.1335652536732046, 0.12586127779625175, 0.12502462410142248, 0.12314181463019283, 0.11947409556292017, 0.12132171408488847, 0.11969648634302053, 0.11667459406976476, 0.113395764307693, 0.11390358058931953, 0.11220886056781043, 0.10618283497272599, 0.10594878412781512, 0.1069255119985896, 0.10636133639157029, 0.10438030560125187, 0.10057149236461137, 0.10229329455271514, 0.09837280598486613], 'val_losses': [0.3750807562241532, 0.2967649461213038, 0.26835636901924176, 0.21045182814478292, 0.19326914024261913, 0.19950048589621283, 0.24155752300514102, 0.20832511066247789, 0.213512274346216, 0.2115474000634772, 0.223553077732616, 0.24597636156102468, 0.22397598953955664, 0.22325034753979933, 0.20153332132621601, 0.2529929679948097, 0.16916457562611795, 0.2617536880119808, 0.23862217990486653, 0.18297447171027223, 0.20628882866353263, 0.17368264087249335, 0.1948270443171102, 0.23555758466451424, 0.21143919697957683, 0.1757501083132229, 0.2451951214020271, 0.20816371210415163, 0.1733763692164209, 0.15923588379121575], 'val_acc': [0.7288560330785616, 0.791379526375141, 0.8374890364615963, 0.8824708683122415, 0.892995865179802, 0.8912416990352087, 0.8579125422879338, 0.8785866432777847, 0.8798396190953515, 0.8793384287683248, 0.886856283673725, 0.8600426011777973, 0.876205989224408, 0.8748277158250846, 0.8904899135446686, 0.8673098609196842, 0.9144217516601929, 0.8730735496804911, 0.8906152111264253, 0.9097857411351961, 0.8960030071419621, 0.9089086580628993, 0.8772083698784613, 0.8904899135446686, 0.9031449693020924, 0.9096604435534394, 0.8971306853777722, 0.9062774088460093, 0.9152988347324896, 0.9100363362987094], 'final_model_size_bytes': 26843, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00603267132423552, 'batch_size': 512, 'epochs': 30, 'base_channels': 13, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.22830060886209214, 'weight_decay': 2.3151269012373345e-06, 'gamma_focal': 1.8611293305042147, 'effective_beta': 0.9041294707361419, 'label_smoothing': 0.0872646489871901, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1495853761266028, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}, 'model_parameter_count': 9259, 'model_storage_size_kb': 19.892382812500003, 'model_size_validation': 'PASS'}
2025-10-03 03:13:01,930 - INFO - _models.training_function_executor - BO Objective: base=0.9100, size_penalty=0.0000, final=0.9100
2025-10-03 03:13:01,931 - INFO - _models.training_function_executor - Model: 9,259 parameters, 19.9KB (PASS 256KB limit)
2025-10-03 03:13:01,931 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 60.663s
2025-10-03 03:13:02,043 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9100
2025-10-03 03:13:02,043 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.112s
2025-10-03 03:13:02,043 - INFO - bo.run_bo - Recorded observation #31: hparams={'lr': 0.00603267132423552, 'batch_size': np.int64(512), 'epochs': np.int64(30), 'base_channels': np.int64(13), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.22830060886209214, 'weight_decay': 2.3151269012373345e-06, 'gamma_focal': 1.8611293305042147, 'effective_beta': 0.9041294707361419, 'label_smoothing': 0.0872646489871901, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1495853761266028, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)}, value=0.9100
2025-10-03 03:13:02,043 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 31: {'lr': 0.00603267132423552, 'batch_size': np.int64(512), 'epochs': np.int64(30), 'base_channels': np.int64(13), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.22830060886209214, 'weight_decay': 2.3151269012373345e-06, 'gamma_focal': 1.8611293305042147, 'effective_beta': 0.9041294707361419, 'label_smoothing': 0.0872646489871901, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1495853761266028, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)} -> 0.9100
2025-10-03 03:13:02,043 - INFO - bo.run_bo - ðŸ”BO Trial 32: Using RF surrogate + Expected Improvement
2025-10-03 03:13:02,043 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:13:02,043 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:13:02,043 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:13:02,043 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007539773182874796, 'batch_size': 64, 'epochs': 19, 'base_channels': 16, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.20928082062777015, 'weight_decay': 4.083555363503924e-06, 'gamma_focal': 1.1056394015629194, 'effective_beta': 0.903071367350097, 'label_smoothing': 0.09686677242835841, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.0686980182753616, 'smote_max_multiplier': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 03:13:02,045 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007539773182874796, 'batch_size': 64, 'epochs': 19, 'base_channels': 16, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.20928082062777015, 'weight_decay': 4.083555363503924e-06, 'gamma_focal': 1.1056394015629194, 'effective_beta': 0.903071367350097, 'label_smoothing': 0.09686677242835841, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.0686980182753616, 'smote_max_multiplier': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 03:13:46,475 - INFO - _models.training_function_executor - Model: 3,954 parameters, 4.2KB storage
2025-10-03 03:13:46,475 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5976225933483922, 0.46138057970632873, 0.4145538049004155, 0.3802905560686138, 0.36172554255785566, 0.3509873025837284, 0.3381434165800419, 0.3282345679192044, 0.3169088645700671, 0.3065937710149136, 0.2996019836349492, 0.2922156680305964, 0.29131711076437805, 0.27715387613041004, 0.27577642290153953, 0.2700211025685949, 0.2667566197547279, 0.2670858390944233, 0.26298687875864796], 'val_losses': [0.4187013130448782, 0.3611951409134855, 0.35443638149560686, 0.34597254935929744, 0.33770739171472414, 0.32687245235976653, 0.3494847816096677, 0.3244983463445084, 0.3405306019657254, 0.31764436885014374, 0.28333520031919335, 0.30392027407597066, 0.2996953881305197, 0.2710315574979501, 0.30107953906611923, 0.2520052173609364, 0.2637381476875595, 0.29768406623005195, 0.26670644798533927], 'val_acc': [0.7842375642150107, 0.8290940984838993, 0.829971181556196, 0.8417491542413231, 0.8527753414359103, 0.8537777220899636, 0.8451321889487533, 0.8555318882345571, 0.8473875454203734, 0.8753289061521112, 0.8873574740007518, 0.8804661070041349, 0.8800902142588648, 0.8980077684500689, 0.8896128304723718, 0.9087833604811427, 0.9003884225034456, 0.8956271143966922, 0.899135446685879], 'final_model_size_bytes': 32963, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007539773182874796, 'batch_size': 64, 'epochs': 19, 'base_channels': 16, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.20928082062777015, 'weight_decay': 4.083555363503924e-06, 'gamma_focal': 1.1056394015629194, 'effective_beta': 0.903071367350097, 'label_smoothing': 0.09686677242835841, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.0686980182753616, 'smote_max_multiplier': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}, 'model_parameter_count': 3954, 'model_storage_size_kb': 4.2474609375000005, 'model_size_validation': 'PASS'}
2025-10-03 03:13:46,475 - INFO - _models.training_function_executor - BO Objective: base=0.8991, size_penalty=0.0000, final=0.8991
2025-10-03 03:13:46,475 - INFO - _models.training_function_executor - Model: 3,954 parameters, 4.2KB (PASS 256KB limit)
2025-10-03 03:13:46,475 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 44.432s
2025-10-03 03:13:46,589 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8991
2025-10-03 03:13:46,589 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.114s
2025-10-03 03:13:46,589 - INFO - bo.run_bo - Recorded observation #32: hparams={'lr': 0.0007539773182874796, 'batch_size': np.int64(64), 'epochs': np.int64(19), 'base_channels': np.int64(16), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.20928082062777015, 'weight_decay': 4.083555363503924e-06, 'gamma_focal': 1.1056394015629194, 'effective_beta': 0.903071367350097, 'label_smoothing': 0.09686677242835841, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.0686980182753616, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)}, value=0.8991
2025-10-03 03:13:46,589 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 32: {'lr': 0.0007539773182874796, 'batch_size': np.int64(64), 'epochs': np.int64(19), 'base_channels': np.int64(16), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.20928082062777015, 'weight_decay': 4.083555363503924e-06, 'gamma_focal': 1.1056394015629194, 'effective_beta': 0.903071367350097, 'label_smoothing': 0.09686677242835841, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.0686980182753616, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)} -> 0.8991
2025-10-03 03:13:46,590 - INFO - bo.run_bo - ðŸ”BO Trial 33: Using RF surrogate + Expected Improvement
2025-10-03 03:13:46,590 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:13:46,590 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:13:46,590 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:13:46,590 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0005656578768263601, 'batch_size': 512, 'epochs': 65, 'base_channels': 10, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 3}
2025-10-03 03:13:46,591 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0005656578768263601, 'batch_size': 512, 'epochs': 65, 'base_channels': 10, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 3}
2025-10-03 03:18:19,645 - INFO - _models.training_function_executor - Model: 101,329 parameters, 217.7KB storage
2025-10-03 03:18:19,646 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4667194505558723, 0.32345733807343713, 0.2273159665273357, 0.1756075951111323, 0.15022030256867608, 0.13322751607971225, 0.11897100351046706, 0.10806381067736229, 0.10062600917703649, 0.09324494233370738, 0.08909670086500807, 0.08444535566234045, 0.07900018294834349, 0.07605097054967462, 0.07130539015225366, 0.07105101382298189, 0.06728649731975535, 0.062191879602517715, 0.06475406329416823, 0.06096764225389629, 0.05780878172414205, 0.057117675669103626, 0.0548665631608333, 0.054230730657269736, 0.05271752811477036, 0.0502063054314264, 0.04961638654532694, 0.04916526842305144, 0.047660732248550206, 0.04631877179214598, 0.04414103044709201, 0.044797459766511456, 0.04262558560564853, 0.04293037578475224, 0.04025410054033272, 0.039819331551499595, 0.0408297177836291, 0.03690410199662152, 0.03665525276945919, 0.03644503840731842, 0.0368786636946288, 0.03713123841064805, 0.036516356119115904, 0.033437560825533605, 0.03197885859439432, 0.03379113049470807, 0.03205479590398859, 0.029920026188454834, 0.0312864476881908, 0.03122022274487285, 0.03061331160483465, 0.028636846843541754, 0.02795065259858226, 0.02776091396215808, 0.027726150703424855, 0.02651064808805761, 0.02719336282972427, 0.02820918204422318, 0.026789678515489995, 0.024961094049834823, 0.02409863413388823, 0.02372334094112037, 0.023816490952904945, 0.023540423466321823, 0.025321823535493764], 'val_losses': [0.394976714502494, 0.24779771997462297, 0.1806964675274249, 0.1466693046427269, 0.13881806417405343, 0.12743165549378604, 0.10686464987386216, 0.1098202694272311, 0.09750035590559987, 0.09304447796967316, 0.08716645857274659, 0.08768258255841872, 0.08610682281418233, 0.08010033931727398, 0.08413020143692183, 0.07529112057266282, 0.07270182278849789, 0.07155529948299413, 0.06755970355252577, 0.06792458793333472, 0.06499098949605787, 0.06738533376069718, 0.07335388267550712, 0.06141099970670604, 0.0630663037547498, 0.061341040375992674, 0.06160863212864884, 0.06882431455653885, 0.06009741573956374, 0.06169337938638756, 0.056839935665455636, 0.05835593602417137, 0.06493612679248764, 0.05714376366318818, 0.05948979752445158, 0.05552479356915681, 0.055986675673850926, 0.05719779329970731, 0.0674212496121371, 0.055919959722956145, 0.0562025700107566, 0.05146422754152125, 0.05556366345771063, 0.05515502480111569, 0.05379377680323862, 0.052085321729862966, 0.057051431358723155, 0.05432263924241395, 0.05292863587692124, 0.05620080500826204, 0.05461776707601001, 0.05522292852775149, 0.054284071496328065, 0.055518793294810964, 0.0605256810575227, 0.05263330179172461, 0.05304345965120312, 0.05989423075492722, 0.05233491048340047, 0.05146287480417521, 0.05227091789708641, 0.05842462206970494, 0.049919710331091736, 0.051771399100021584, 0.05736710401754857], 'val_acc': [0.7708307229670467, 0.8416238566595665, 0.9016413983210124, 0.9181806791128931, 0.9215637138203233, 0.9298333542162637, 0.9434907906277409, 0.9378523994486906, 0.9471244204986844, 0.9487532890615211, 0.9513845382784112, 0.9517604310236812, 0.9516351334419245, 0.9551434657311114, 0.954141085077058, 0.9575241197844881, 0.9589023931838115, 0.9607818569101616, 0.9615336424007017, 0.9612830472371883, 0.9624107254729983, 0.9630372133817817, 0.9600300714196216, 0.9652925698534018, 0.9627866182182684, 0.9650419746898885, 0.9656684625986719, 0.961784237564215, 0.9646660819446184, 0.9650419746898885, 0.9674226287432652, 0.9652925698534018, 0.9654178674351584, 0.9676732239067786, 0.9685503069790753, 0.967547926325022, 0.9677985214885353, 0.9671720335797519, 0.9597794762561083, 0.969427390051372, 0.9655431650169152, 0.969427390051372, 0.9703044731236687, 0.9711815561959655, 0.9710562586142087, 0.9711815561959655, 0.9720586392682621, 0.9703044731236687, 0.9665455456709685, 0.9696779852148854, 0.968675604560832, 0.9705550682871821, 0.9693020924696153, 0.970930961032452, 0.9699285803783987, 0.970930961032452, 0.9731863175040721, 0.9659190577621852, 0.9708056634506954, 0.9720586392682621, 0.9730610199223155, 0.9720586392682621, 0.9716827465229921, 0.9728104247588022, 0.9698032827966421], 'final_model_size_bytes': 217442, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0005656578768263601, 'batch_size': 512, 'epochs': 65, 'base_channels': 10, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 3}, 'model_parameter_count': 101329, 'model_storage_size_kb': 217.69902343750002, 'model_size_validation': 'PASS'}
2025-10-03 03:18:19,646 - INFO - _models.training_function_executor - BO Objective: base=0.9698, size_penalty=0.0000, final=0.9698
2025-10-03 03:18:19,646 - INFO - _models.training_function_executor - Model: 101,329 parameters, 217.7KB (PASS 256KB limit)
2025-10-03 03:18:19,646 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 273.056s
2025-10-03 03:18:19,760 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9698
2025-10-03 03:18:19,760 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.114s
2025-10-03 03:18:19,760 - INFO - bo.run_bo - Recorded observation #33: hparams={'lr': 0.0005656578768263601, 'batch_size': np.int64(512), 'epochs': np.int64(65), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(3)}, value=0.9698
2025-10-03 03:18:19,760 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 33: {'lr': 0.0005656578768263601, 'batch_size': np.int64(512), 'epochs': np.int64(65), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(3)} -> 0.9698
2025-10-03 03:18:19,760 - INFO - bo.run_bo - ðŸ”BO Trial 34: Using RF surrogate + Expected Improvement
2025-10-03 03:18:19,760 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:18:19,761 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:18:19,761 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:18:19,761 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00010121668519461623, 'batch_size': 64, 'epochs': 90, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.10931582600466264, 'weight_decay': 0.00013652457112683076, 'gamma_focal': 1.2815553755671512, 'effective_beta': 0.9286105243611961, 'label_smoothing': 0.09903362954625808, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.17849105169889184, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 7}
2025-10-03 03:18:19,762 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00010121668519461623, 'batch_size': 64, 'epochs': 90, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.10931582600466264, 'weight_decay': 0.00013652457112683076, 'gamma_focal': 1.2815553755671512, 'effective_beta': 0.9286105243611961, 'label_smoothing': 0.09903362954625808, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.17849105169889184, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 7}
2025-10-03 03:23:10,842 - INFO - _models.training_function_executor - Model: 51,127 parameters, 219.7KB storage
2025-10-03 03:23:10,842 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5671954625935138, 0.4664487630624586, 0.4023013150955626, 0.35430943467012604, 0.32851895314202784, 0.3104991313677229, 0.2961562952378622, 0.28417080548369517, 0.27496145129543614, 0.26697277826763305, 0.2563627875919603, 0.2503629205801827, 0.24347491776947314, 0.233979793339739, 0.22852412075960998, 0.2250568803704495, 0.21826242794127323, 0.21540463918912972, 0.21069644913775198, 0.20848994560338305, 0.20408178131330829, 0.20198253525077178, 0.19744455013546106, 0.19577756261006263, 0.19242321836135443, 0.18956227157362623, 0.18772433398268304, 0.1867366748064438, 0.18240137379244822, 0.18207054987283247, 0.18028043032988297, 0.17850055272942986, 0.17722254330051074, 0.17349089275766308, 0.17041418529190158, 0.1689159520316071, 0.16931498586861699, 0.16636107140160167, 0.1649637507854785, 0.1649309759976791, 0.1596385595128974, 0.15859206558754504, 0.1575838839554441, 0.15457793139148818, 0.15295017543164083, 0.15371845174010518, 0.15281919222410859, 0.1506232384591011, 0.1517464122403401, 0.14633122983871463, 0.14603705908430403, 0.14600067624356466, 0.14565314976089758, 0.1449384578467405, 0.14143072985596022, 0.14229987950254136, 0.1412085129668516, 0.13896419645509664, 0.13888116379373522, 0.13621947184007605, 0.13776044168388793, 0.1357291442715326, 0.1327877581050453, 0.13304844588116924, 0.1318026993081664, 0.13016641348006638, 0.1305159178077269, 0.12824285569127647, 0.12804110152221834, 0.1275697256918726, 0.1267955209629856, 0.12623552763247906, 0.12555245650470717, 0.12583152408771797, 0.12477509349050578, 0.1226304977209415, 0.12194494514909636, 0.12245423333827958, 0.11989348581798229, 0.1192313882985306, 0.11844304055222905, 0.11841394010193383, 0.11723372637695754, 0.11763010127030828, 0.11715448091563786, 0.11576565324044656, 0.11482387691940672, 0.1147897270517242, 0.11479982261153886, 0.11375645409423568], 'val_losses': [0.4955152835019653, 0.4357764791156277, 0.3643098930908674, 0.32851429034737234, 0.3227313929591587, 0.2961033868238511, 0.2900297845975302, 0.2704327034182513, 0.2570151270048285, 0.2676229687292554, 0.2397445639248346, 0.2338307268349035, 0.23736576924174968, 0.23091559506212467, 0.22253559375000453, 0.21838232389242931, 0.2157095720980523, 0.2210286896605343, 0.22301029292111588, 0.21323769258879252, 0.21248189041099516, 0.1932640443031478, 0.19264804307208475, 0.19219255026547985, 0.1969906613837506, 0.18944883747046162, 0.20499317318316823, 0.19647968223916157, 0.17964403451893088, 0.1882207026298356, 0.2077400734319736, 0.17520710200635192, 0.17412248415698117, 0.18468612794319858, 0.16944267700943577, 0.16271100777377911, 0.171410721835823, 0.17370371495500392, 0.19746887804855157, 0.16730836702235555, 0.16534249260676204, 0.16108045103316648, 0.16899918816267676, 0.17152943302031343, 0.15252631212248716, 0.15553330550246364, 0.15532343855269823, 0.1544478862372396, 0.15593073228340282, 0.1568869461826238, 0.1534618602713432, 0.1541426546359149, 0.15640297846684492, 0.14777241718379228, 0.16125025571535262, 0.15268757832541738, 0.14646765010919416, 0.15126386384903645, 0.14553718045942018, 0.14586279660653775, 0.14618146405895466, 0.14670813753370085, 0.14396091725067184, 0.1455547213292991, 0.14604813452166374, 0.1479687051166793, 0.15558183772188128, 0.14913046460651452, 0.15219362009334886, 0.1389195948443367, 0.13578322946898339, 0.1359383575959009, 0.14023458528803967, 0.14053497104145293, 0.1346372193228017, 0.13872395485523814, 0.134625902584495, 0.1318485432923729, 0.146895473477775, 0.13965216546395626, 0.13961447024237855, 0.13855602782205179, 0.12855034276830002, 0.12937320488957949, 0.1316154450570233, 0.1370690470948104, 0.12988318184939351, 0.1480627027710732, 0.1312658430105492, 0.12607002038545528], 'val_acc': [0.7599298333542163, 0.7884976819947375, 0.8235810048866057, 0.8501440922190202, 0.8438792131311865, 0.8572860543791505, 0.8561583761433404, 0.8719458714446812, 0.8789625360230547, 0.8691893246460344, 0.8927452700162887, 0.8961283047237188, 0.8909911038716952, 0.898383661195339, 0.900263124921689, 0.9040220523743892, 0.9066533015912793, 0.9022678862297958, 0.8958777095602055, 0.9015161007392557, 0.9025184813933091, 0.9150482395689763, 0.9137952637514096, 0.913294073424383, 0.9124169903520862, 0.9158000250595163, 0.9050244330284426, 0.9069038967547927, 0.9198095476757299, 0.914797644405463, 0.9001378273399323, 0.9206866307480266, 0.92206490414735, 0.9156747274777597, 0.9233178799649167, 0.9285803783986969, 0.9244455582007267, 0.9230672848014033, 0.9081568725723593, 0.9244455582007267, 0.9277032953264002, 0.928956271143967, 0.9251973436912667, 0.9226913920561333, 0.9376018042851773, 0.936975316376394, 0.9352211502318005, 0.9362235308858539, 0.9328404961784238, 0.9332163889236937, 0.9343440671595038, 0.9357223405588272, 0.9335922816689638, 0.9382282921939606, 0.9302092469615336, 0.9345946623230171, 0.9373512091216639, 0.9386041849392307, 0.9416113269013908, 0.9365994236311239, 0.9421125172284175, 0.9364741260493672, 0.9418619220649042, 0.9414860293196341, 0.9402330535020674, 0.9374765067034206, 0.9328404961784238, 0.9355970429770706, 0.9349705550682872, 0.9424884099736875, 0.9458714446811176, 0.9452449567723343, 0.9422378148101742, 0.9429896003007142, 0.9452449567723343, 0.9401077559203107, 0.9443678737000376, 0.9467485277534143, 0.938102994612204, 0.9427390051372009, 0.9427390051372009, 0.9453702543540909, 0.9490038842250345, 0.9480015035709811, 0.9458714446811176, 0.9427390051372009, 0.9476256108257111, 0.938102994612204, 0.9473750156621977, 0.9497556697155745], 'final_model_size_bytes': 212827, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00010121668519461623, 'batch_size': 64, 'epochs': 90, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.10931582600466264, 'weight_decay': 0.00013652457112683076, 'gamma_focal': 1.2815553755671512, 'effective_beta': 0.9286105243611961, 'label_smoothing': 0.09903362954625808, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.17849105169889184, 'smote_max_multiplier': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 7}, 'model_parameter_count': 51127, 'model_storage_size_kb': 219.68632812500002, 'model_size_validation': 'PASS'}
2025-10-03 03:23:10,842 - INFO - _models.training_function_executor - BO Objective: base=0.9498, size_penalty=0.0000, final=0.9498
2025-10-03 03:23:10,842 - INFO - _models.training_function_executor - Model: 51,127 parameters, 219.7KB (PASS 256KB limit)
2025-10-03 03:23:10,842 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 291.082s
2025-10-03 03:23:10,958 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9498
2025-10-03 03:23:10,958 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.116s
2025-10-03 03:23:10,958 - INFO - bo.run_bo - Recorded observation #34: hparams={'lr': 0.00010121668519461623, 'batch_size': np.int64(64), 'epochs': np.int64(90), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.10931582600466264, 'weight_decay': 0.00013652457112683076, 'gamma_focal': 1.2815553755671512, 'effective_beta': 0.9286105243611961, 'label_smoothing': 0.09903362954625808, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.17849105169889184, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(7)}, value=0.9498
2025-10-03 03:23:10,958 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 34: {'lr': 0.00010121668519461623, 'batch_size': np.int64(64), 'epochs': np.int64(90), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.10931582600466264, 'weight_decay': 0.00013652457112683076, 'gamma_focal': 1.2815553755671512, 'effective_beta': 0.9286105243611961, 'label_smoothing': 0.09903362954625808, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.17849105169889184, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(7)} -> 0.9498
2025-10-03 03:23:10,959 - INFO - bo.run_bo - ðŸ”BO Trial 35: Using RF surrogate + Expected Improvement
2025-10-03 03:23:10,959 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:23:10,959 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:23:10,959 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:23:10,959 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.415341909991218e-05, 'batch_size': 64, 'epochs': 63, 'base_channels': 8, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.0037682731434530723, 'weight_decay': 0.0033787422666905096, 'gamma_focal': 2.0921850665835318, 'effective_beta': 0.9278158727343669, 'label_smoothing': 0.09955147224118127, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1274264206480535, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 6}
2025-10-03 03:23:10,960 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.415341909991218e-05, 'batch_size': 64, 'epochs': 63, 'base_channels': 8, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.0037682731434530723, 'weight_decay': 0.0033787422666905096, 'gamma_focal': 2.0921850665835318, 'effective_beta': 0.9278158727343669, 'label_smoothing': 0.09955147224118127, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1274264206480535, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 6}
2025-10-03 03:28:15,703 - INFO - _models.training_function_executor - Model: 13,231 parameters, 56.9KB storage
2025-10-03 03:28:15,703 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6823379102272449, 0.4960093109146439, 0.46431214861118775, 0.4529127442162351, 0.44586608264562244, 0.4414217417836059, 0.43820054493642624, 0.4350051542248257, 0.4323054836452693, 0.429416920320733, 0.42624974107023833, 0.422887459739469, 0.41919036150535044, 0.41531834840173315, 0.41236910581984576, 0.4091265998929865, 0.4059764578132347, 0.40258818560198834, 0.3997342547550165, 0.39666316465966567, 0.3941465754329055, 0.3913523147280412, 0.3876683775040192, 0.3847101297774149, 0.3818419189893844, 0.378074164535902, 0.3749037327338993, 0.3733828154083325, 0.3698034386820345, 0.3670228905127438, 0.36376361977362087, 0.36072126660054066, 0.35819536613513586, 0.3559614643470889, 0.35356766892544506, 0.350630168749852, 0.3489394801814197, 0.3458328351443324, 0.3439482225653813, 0.34151135715566566, 0.3400588100158462, 0.3374563262921036, 0.335047378807936, 0.33285526320871783, 0.3320907075479134, 0.32996336144152705, 0.3272021323609568, 0.32415602764188434, 0.3235017488163948, 0.32187418264365153, 0.3192345635924681, 0.31784365882783316, 0.3156867446128436, 0.31360791849376035, 0.3115610026435304, 0.310271523847368, 0.30806010980203974, 0.30577485664175463, 0.30448212682653764, 0.301942076664149, 0.3002304471881005, 0.2992817318332497, 0.2965132010497272], 'val_losses': [0.5357830622727643, 0.47468190037984626, 0.45847928401163507, 0.4521803727179538, 0.4472413301363339, 0.4444474314160103, 0.4408332561695281, 0.43799137287966783, 0.43680807516817133, 0.4319966276095873, 0.42895186609111174, 0.4268681938831048, 0.42134482953396374, 0.41799004268204115, 0.4147597391337115, 0.410373900135632, 0.40789916750677035, 0.40347089986725865, 0.40090720324330487, 0.3966373353169655, 0.39225041712455144, 0.39053927776643527, 0.38705191334347544, 0.38479772528271494, 0.38096451446415863, 0.3761790334214962, 0.37399814660545283, 0.36921598204857214, 0.36594580348206257, 0.36312805534589115, 0.3654082902331293, 0.3579472556865211, 0.3599007283665904, 0.3519465204315696, 0.35092562192656795, 0.3476639223504285, 0.34331924737231684, 0.3418875707953126, 0.3405353865099497, 0.3380375208617631, 0.33714658123496005, 0.3334230569976267, 0.3314494979574244, 0.32982224505873137, 0.32765545924830713, 0.3246937594610473, 0.3235211776990279, 0.3198098195139111, 0.3202368556078985, 0.3169372379481083, 0.3148831490876755, 0.3121925812439129, 0.3110767125284264, 0.3090516304782333, 0.3071087530334855, 0.3047488807246524, 0.30345753404919684, 0.3013622309200866, 0.3004088187472382, 0.30234840527025625, 0.2952561424931941, 0.29365512395604126, 0.2923410899760714], 'val_acc': [0.7200852023555946, 0.7199599047738379, 0.7180804410474878, 0.7162009773211377, 0.7146974063400576, 0.715323894248841, 0.7195840120285678, 0.7210875830096479, 0.7200852023555946, 0.7215887733366746, 0.7235935346447814, 0.723718832226538, 0.7255982959528882, 0.7264753790251848, 0.7273524620974815, 0.7282295451697782, 0.7271018669339682, 0.7272271645157249, 0.7268512717704548, 0.7266006766069415, 0.7250971056258614, 0.7268512717704548, 0.7281042475880215, 0.7281042475880215, 0.7294825209873449, 0.7311113895501817, 0.730610199223155, 0.7327402581130185, 0.7351209121663952, 0.7348703170028819, 0.7331161508582884, 0.737125673474502, 0.7349956145846385, 0.7412604936724722, 0.740508708181932, 0.7485277534143591, 0.7506578123042226, 0.7516601929582759, 0.7525372760305726, 0.7556697155744894, 0.7552938228292194, 0.7618092970805663, 0.7579250720461095, 0.7601804285177296, 0.7609322140082696, 0.7638140583886731, 0.7663200100238066, 0.7712066157123167, 0.7668212003508332, 0.77032953264002, 0.7737125673474502, 0.7707054253852901, 0.7724595915298835, 0.7792256609447438, 0.7744643528379902, 0.7759679238190703, 0.7759679238190703, 0.779100363362987, 0.7818569101616338, 0.7780979827089337, 0.7816063149981205, 0.784488159378524, 0.7839869690514973], 'final_model_size_bytes': 61211, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.415341909991218e-05, 'batch_size': 64, 'epochs': 63, 'base_channels': 8, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.0037682731434530723, 'weight_decay': 0.0033787422666905096, 'gamma_focal': 2.0921850665835318, 'effective_beta': 0.9278158727343669, 'label_smoothing': 0.09955147224118127, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1274264206480535, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 6}, 'model_parameter_count': 13231, 'model_storage_size_kb': 56.851953125, 'model_size_validation': 'PASS'}
2025-10-03 03:28:15,703 - INFO - _models.training_function_executor - BO Objective: base=0.7840, size_penalty=0.0000, final=0.7840
2025-10-03 03:28:15,703 - INFO - _models.training_function_executor - Model: 13,231 parameters, 56.9KB (PASS 256KB limit)
2025-10-03 03:28:15,703 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 304.744s
2025-10-03 03:28:15,819 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7840
2025-10-03 03:28:15,819 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.116s
2025-10-03 03:28:15,819 - INFO - bo.run_bo - Recorded observation #35: hparams={'lr': 1.415341909991218e-05, 'batch_size': np.int64(64), 'epochs': np.int64(63), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.0037682731434530723, 'weight_decay': 0.0033787422666905096, 'gamma_focal': 2.0921850665835318, 'effective_beta': 0.9278158727343669, 'label_smoothing': 0.09955147224118127, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1274264206480535, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(6)}, value=0.7840
2025-10-03 03:28:15,819 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 35: {'lr': 1.415341909991218e-05, 'batch_size': np.int64(64), 'epochs': np.int64(63), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.0037682731434530723, 'weight_decay': 0.0033787422666905096, 'gamma_focal': 2.0921850665835318, 'effective_beta': 0.9278158727343669, 'label_smoothing': 0.09955147224118127, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1274264206480535, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(6)} -> 0.7840
2025-10-03 03:28:15,820 - INFO - bo.run_bo - ðŸ”BO Trial 36: Using RF surrogate + Expected Improvement
2025-10-03 03:28:15,820 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:28:15,820 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:28:15,820 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:28:15,820 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00017987084809638657, 'batch_size': 64, 'epochs': 72, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.17694807149145522, 'weight_decay': 5.3696234131656556e-05, 'gamma_focal': 2.0663018628118044, 'effective_beta': 0.9285414950471608, 'label_smoothing': 0.09803020471541724, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.234996715609275, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 03:28:15,821 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00017987084809638657, 'batch_size': 64, 'epochs': 72, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.17694807149145522, 'weight_decay': 5.3696234131656556e-05, 'gamma_focal': 2.0663018628118044, 'effective_beta': 0.9285414950471608, 'label_smoothing': 0.09803020471541724, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.234996715609275, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 03:30:37,984 - INFO - _models.training_function_executor - Model: 9,231 parameters, 19.8KB storage
2025-10-03 03:30:37,984 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.48797796336745564, 0.39464888324027436, 0.3658737827924336, 0.3348153439613488, 0.30991228414620725, 0.293491720445614, 0.2767151749955978, 0.263480169248324, 0.25086436010865654, 0.23930801783510083, 0.23089261483062057, 0.2190859404442749, 0.21293002628372287, 0.20634950277642738, 0.2020137736571073, 0.19723986471199328, 0.1924876961376876, 0.1879736667897055, 0.18601837945304464, 0.17997583211578305, 0.1769928579655625, 0.17409908228971283, 0.17209878145828733, 0.1683931652089746, 0.16736485519517996, 0.1626419348850124, 0.162398462438667, 0.15973748432358745, 0.15700089314352267, 0.15440518177138554, 0.15385183454758578, 0.15087057386729455, 0.14972730244912213, 0.14717169414644526, 0.14688675792911904, 0.14462860774654823, 0.14315566506630173, 0.14182495293712324, 0.14044375040008883, 0.1380499133169959, 0.13753117705246254, 0.13558677231277197, 0.13433156356169265, 0.1351604106206533, 0.1315782640262478, 0.1320975500557804, 0.13078344009457923, 0.1288215556110695, 0.12763607336158453, 0.12713835491943143, 0.12737159836919812, 0.12398064038387027, 0.12437394113016058, 0.12526666334107964, 0.12393904374735294, 0.12168365706291517, 0.12133174620005577, 0.12044169323391266, 0.12099714975598588, 0.11969226907097456, 0.11868712982229992, 0.11800717504952547, 0.11607675914611598, 0.11493034960538512, 0.11615217095276835, 0.1139495328289989, 0.11256126197588809, 0.11271534416945593, 0.11186262348049056, 0.11210492190264283, 0.11332318182383844, 0.11317932109178071], 'val_losses': [0.4176751375683805, 0.3951932346144202, 0.35335725908843424, 0.3136903041026867, 0.2979230622520215, 0.2776814643571347, 0.26217608207778637, 0.24747311772633698, 0.25160705010935175, 0.2335700583594617, 0.23115443100630229, 0.20527753163520712, 0.2223209787574183, 0.21299073164877685, 0.2154691284136162, 0.21218071567834493, 0.18780536079948937, 0.2015368810579108, 0.1898522613405091, 0.17767974717782684, 0.17785302212779466, 0.18228905573263232, 0.1863082809166724, 0.17558049755477842, 0.17322329049301005, 0.166414022534365, 0.17805971713546911, 0.17592398439743004, 0.19677164689240384, 0.1646363229533176, 0.1589642127850027, 0.16736964081143202, 0.1607893770207044, 0.15185093368820088, 0.1620213045905391, 0.1651130243883801, 0.14724770469280385, 0.15125268547392073, 0.14533094702039487, 0.14692591692771084, 0.1434759690073062, 0.1592929587407625, 0.14365199001060439, 0.13895272418890228, 0.13903794914545636, 0.13688494916970292, 0.1392122639545954, 0.1288927555114102, 0.13477884156307604, 0.13173721112659098, 0.1453476511147361, 0.12918344240735055, 0.12313421325898831, 0.1195735866081377, 0.12081306755202946, 0.12358084021449134, 0.13259730377204063, 0.1253537183839633, 0.1274321968394732, 0.129628791154652, 0.11470920576909673, 0.12319037619856363, 0.11448141693688024, 0.12082226365214675, 0.12536796359719898, 0.11646772242121911, 0.11771589748411082, 0.11452410149796088, 0.10880300132381544, 0.11306536395152683, 0.13071425707679152, 0.11077532718323807], 'val_acc': [0.7220899636637013, 0.7388798396190953, 0.7755920310738003, 0.7976444054629745, 0.8184438040345822, 0.8319759428643028, 0.8421250469865932, 0.8515223656183436, 0.8452574865305099, 0.8591655181055006, 0.8572860543791505, 0.8820949755669716, 0.8576619471244205, 0.8659315875203608, 0.8574113519609071, 0.8664327778473876, 0.8852274151108883, 0.8760806916426513, 0.8814684876581882, 0.891116401453452, 0.8942488409973688, 0.8886104498183185, 0.8888610449818318, 0.8967547926325022, 0.8938729482520987, 0.9018919934845258, 0.8951259240696654, 0.8958777095602055, 0.8839744392933216, 0.8970053877960156, 0.907154491918306, 0.8941235434156121, 0.9035208620473625, 0.9072797895000626, 0.8966294950507455, 0.8977571732865556, 0.9114146096980328, 0.9101616338804661, 0.9112893121162762, 0.9048991354466859, 0.9124169903520862, 0.8973812805412855, 0.9094098483899261, 0.9124169903520862, 0.913294073424383, 0.9158000250595163, 0.9124169903520862, 0.9195589525122165, 0.9158000250595163, 0.9186818694399198, 0.9094098483899261, 0.9215637138203233, 0.92319258238316, 0.9274527001628868, 0.930835734870317, 0.9288309735622102, 0.9176794887858665, 0.9224407968926199, 0.9230672848014033, 0.9203107380027565, 0.9344693647412605, 0.9226913920561333, 0.9329657937601804, 0.9280791880716702, 0.9230672848014033, 0.9338428768324771, 0.9309610324520736, 0.9302092469615336, 0.9367247212128806, 0.9357223405588272, 0.9171782984588397, 0.9337175792507204], 'final_model_size_bytes': 29126, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00017987084809638657, 'batch_size': 64, 'epochs': 72, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.17694807149145522, 'weight_decay': 5.3696234131656556e-05, 'gamma_focal': 2.0663018628118044, 'effective_beta': 0.9285414950471608, 'label_smoothing': 0.09803020471541724, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.234996715609275, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}, 'model_parameter_count': 9231, 'model_storage_size_kb': 19.8322265625, 'model_size_validation': 'PASS'}
2025-10-03 03:30:37,984 - INFO - _models.training_function_executor - BO Objective: base=0.9337, size_penalty=0.0000, final=0.9337
2025-10-03 03:30:37,984 - INFO - _models.training_function_executor - Model: 9,231 parameters, 19.8KB (PASS 256KB limit)
2025-10-03 03:30:37,984 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 142.164s
2025-10-03 03:30:38,100 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9337
2025-10-03 03:30:38,100 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.116s
2025-10-03 03:30:38,100 - INFO - bo.run_bo - Recorded observation #36: hparams={'lr': 0.00017987084809638657, 'batch_size': np.int64(64), 'epochs': np.int64(72), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.17694807149145522, 'weight_decay': 5.3696234131656556e-05, 'gamma_focal': 2.0663018628118044, 'effective_beta': 0.9285414950471608, 'label_smoothing': 0.09803020471541724, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.234996715609275, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)}, value=0.9337
2025-10-03 03:30:38,100 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 36: {'lr': 0.00017987084809638657, 'batch_size': np.int64(64), 'epochs': np.int64(72), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.17694807149145522, 'weight_decay': 5.3696234131656556e-05, 'gamma_focal': 2.0663018628118044, 'effective_beta': 0.9285414950471608, 'label_smoothing': 0.09803020471541724, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.234996715609275, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)} -> 0.9337
2025-10-03 03:30:38,101 - INFO - bo.run_bo - ðŸ”BO Trial 37: Using RF surrogate + Expected Improvement
2025-10-03 03:30:38,101 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 03:30:38,101 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:30:38,101 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:30:38,101 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003063129669035553, 'batch_size': 64, 'epochs': 35, 'base_channels': 6, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.0032622528175739194, 'weight_decay': 1.0724854300578e-05, 'gamma_focal': 2.7289767397539384, 'effective_beta': 0.9527647762260386, 'label_smoothing': 0.09818137115821837, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.189152354050242, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 3}
2025-10-03 03:30:38,103 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003063129669035553, 'batch_size': 64, 'epochs': 35, 'base_channels': 6, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.0032622528175739194, 'weight_decay': 1.0724854300578e-05, 'gamma_focal': 2.7289767397539384, 'effective_beta': 0.9527647762260386, 'label_smoothing': 0.09818137115821837, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.189152354050242, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 3}
2025-10-03 03:46:16,796 - INFO - _models.training_function_executor - Model: 67,823 parameters, 145.7KB storage
2025-10-03 03:46:16,797 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.19231234081781504, 0.11127053552150495, 0.09166246307089272, 0.08371756930108791, 0.07511456015647901, 0.07600491226496181, 0.06831960669273691, 0.0641351877530105, 0.05797885115150539, 0.060364993599816466, 0.057813367914523156, 0.058868405943771386, 0.05494159786933586, 0.051528030283056164, 0.0499182106554835, 0.04857460753627233, 0.048093557009566856, 0.04479333222043609, 0.045251622429516136, 0.042530951498214196, 0.04060935754335618, 0.04261158254726098, 0.039619810482463685, 0.03794864464973767, 0.039774263379700636, 0.04000933300772031, 0.03901646434289861, 0.036409622770760534, 0.03812825617110318, 0.034644819750719744, 0.03326024265051788, 0.03269504517821179, 0.03209704451403687, 0.03206497173066337, 0.03242707740198874], 'val_losses': [0.11404569960394056, 0.09855741521866604, 0.08684142786239774, 0.07838960266205888, 0.07560533874204561, 0.07775820874993088, 0.07171817650032916, 0.066875872568982, 0.05828302948005276, 0.07028329510289824, 0.07335934353612626, 0.05718070438348505, 0.06739165455610766, 0.058526966867851984, 0.05631155284585369, 0.062319046341264596, 0.051985694494347215, 0.05726810508414537, 0.04836723289365324, 0.05703792791421992, 0.052305465771610944, 0.04910793226567475, 0.04947213477179335, 0.04970291298540474, 0.04762108038185294, 0.05676434838731491, 0.05722400665940415, 0.053729909960525464, 0.04875690722272086, 0.050190529394547094, 0.04817988251892192, 0.045764137152337354, 0.047237501924400294, 0.049457271353610846, 0.050821740556937264], 'val_acc': [0.9062774088460093, 0.9215637138203233, 0.9236937727101867, 0.938478887357474, 0.9317128179426137, 0.9402330535020674, 0.9437413857912542, 0.9451196591905776, 0.9498809672973312, 0.9389800776845006, 0.9409848389926074, 0.9506327527878712, 0.9437413857912542, 0.9508833479513845, 0.9520110261871946, 0.9491291818067912, 0.9556446560581381, 0.9530134068412479, 0.9589023931838115, 0.9550181681493547, 0.9555193584763814, 0.9579000125297582, 0.9566470367121914, 0.9535145971682747, 0.9584012028567849, 0.9523869189324646, 0.9556446560581381, 0.953389299586518, 0.9607818569101616, 0.9594035835108382, 0.9594035835108382, 0.9611577496554317, 0.9616589399824583, 0.9600300714196216, 0.9589023931838115], 'final_model_size_bytes': 148215, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003063129669035553, 'batch_size': 64, 'epochs': 35, 'base_channels': 6, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.0032622528175739194, 'weight_decay': 1.0724854300578e-05, 'gamma_focal': 2.7289767397539384, 'effective_beta': 0.9527647762260386, 'label_smoothing': 0.09818137115821837, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.189152354050242, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 3}, 'model_parameter_count': 67823, 'model_storage_size_kb': 145.7134765625, 'model_size_validation': 'PASS'}
2025-10-03 03:46:16,797 - INFO - _models.training_function_executor - BO Objective: base=0.9589, size_penalty=0.0000, final=0.9589
2025-10-03 03:46:16,797 - INFO - _models.training_function_executor - Model: 67,823 parameters, 145.7KB (PASS 256KB limit)
2025-10-03 03:46:16,797 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 938.696s
2025-10-03 03:46:16,913 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9589
2025-10-03 03:46:16,913 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.117s
2025-10-03 03:46:16,913 - INFO - bo.run_bo - Recorded observation #37: hparams={'lr': 0.003063129669035553, 'batch_size': np.int64(64), 'epochs': np.int64(35), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.0032622528175739194, 'weight_decay': 1.0724854300578e-05, 'gamma_focal': 2.7289767397539384, 'effective_beta': 0.9527647762260386, 'label_smoothing': 0.09818137115821837, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.189152354050242, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(3)}, value=0.9589
2025-10-03 03:46:16,913 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 37: {'lr': 0.003063129669035553, 'batch_size': np.int64(64), 'epochs': np.int64(35), 'base_channels': np.int64(6), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.0032622528175739194, 'weight_decay': 1.0724854300578e-05, 'gamma_focal': 2.7289767397539384, 'effective_beta': 0.9527647762260386, 'label_smoothing': 0.09818137115821837, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.189152354050242, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(3)} -> 0.9589
2025-10-03 03:46:16,914 - INFO - bo.run_bo - ðŸ”BO Trial 38: Using RF surrogate + Expected Improvement
2025-10-03 03:46:16,914 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:46:16,914 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:46:16,914 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:46:16,914 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0001664694645615061, 'batch_size': 64, 'epochs': 83, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.21039380566532462, 'weight_decay': 0.007114879904098962, 'gamma_focal': 1.030595736517539, 'effective_beta': 0.9764242604366347, 'label_smoothing': 0.09943974515620842, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.1792187853154319, 'smote_max_multiplier': 3, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 03:46:16,915 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0001664694645615061, 'batch_size': 64, 'epochs': 83, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.21039380566532462, 'weight_decay': 0.007114879904098962, 'gamma_focal': 1.030595736517539, 'effective_beta': 0.9764242604366347, 'label_smoothing': 0.09943974515620842, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.1792187853154319, 'smote_max_multiplier': 3, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 03:53:54,037 - INFO - _models.training_function_executor - Model: 26,314 parameters, 113.1KB storage
2025-10-03 03:53:54,037 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7739304946491249, 0.5978040583573453, 0.5294840365405624, 0.47600377215549583, 0.42906684764188785, 0.398789563837198, 0.37552365634664275, 0.35169188968910203, 0.3372445295119838, 0.32256973822104273, 0.30372489738070074, 0.2933238274895219, 0.28489490262364675, 0.2718109455439064, 0.26286956595484356, 0.2523183990745175, 0.24733271752279812, 0.23843818802736316, 0.2324066858869338, 0.22624277146338015, 0.22281390486143074, 0.21652882269562904, 0.21060198960088888, 0.20417062315611947, 0.20080323423817054, 0.19956181006623155, 0.1948375756253888, 0.1872541227968077, 0.18817237311375018, 0.18479435437105263, 0.18048549725851779, 0.17738708444814646, 0.17283746348736959, 0.17352270777654968, 0.17216181041793677, 0.16593394801304087, 0.16137882647320137, 0.16057081259618894, 0.16088937047209628, 0.15902492924335104, 0.15638366614014426, 0.15429156140904263, 0.151307072991052, 0.1512685320963955, 0.14654684974765908, 0.14954332233213044, 0.1469733446873511, 0.14280864428423623, 0.14041185998265943, 0.14250657873841704, 0.13932111308568992, 0.13917822714473405, 0.13673465250894576, 0.13716610678193025, 0.13878717447856367, 0.1319583824028836, 0.1362818034188797, 0.13394067424985817, 0.12832507034922908, 0.1283387619478, 0.13091780343080459, 0.1275359021863249, 0.13148154646622345, 0.12533588779560417, 0.12412366700042485, 0.12528221119259098, 0.12567207392737703, 0.12525146657344274, 0.12007960758387834, 0.12092262247107892, 0.12087855541844567, 0.11875210276164282, 0.1226940806652737, 0.11615398482500384, 0.1229294213246992, 0.11661236945796531, 0.11902287067923983, 0.11397295750022593, 0.1136544096514024, 0.11608636985147286, 0.1124556168783105, 0.11153024829021178, 0.1146945869893101], 'val_losses': [0.5978058792380907, 0.6775912048598307, 0.5236311353898977, 0.4984853480740786, 0.5032846367070885, 0.43080002744982615, 0.4280861422369137, 0.5705588095402631, 0.4408215535805796, 0.48218924214540904, 0.3771538408966743, 0.3516438996309386, 0.4177831294557203, 0.36352379910032667, 0.36170949265332825, 0.36919471581931285, 0.4471352691639039, 0.42275047718231396, 0.4125452902200243, 0.4355898656622238, 0.3766980258956348, 0.34806944153733366, 0.4205077308907035, 0.40495841595152987, 0.4047886504817466, 0.2938649518614409, 0.34026640192068186, 0.28110452060740687, 0.39084703442656354, 0.3579786555791854, 0.40978214294446497, 0.29230542824674915, 0.27423073340740856, 0.32147222374214773, 0.32856388497421307, 0.25831341576552336, 0.2657629463729696, 0.2959433694543613, 0.32957944932654304, 0.4757732780949691, 0.28340661709446935, 0.3054602443387732, 0.39194774057106063, 0.286091114689482, 0.283055496035087, 0.31226998682033147, 0.2994905558464118, 0.2522133522528243, 0.3127795596078529, 0.27984162864492673, 0.27010330722873005, 0.2007645828045635, 0.2766459073365862, 0.3989384081963832, 0.18396475984236602, 0.32947934193653566, 0.3795748262152369, 0.18305056301024814, 0.21593949284235078, 0.2195452626950011, 0.2784811024893766, 0.26354985006873344, 0.21643703859852545, 0.23454743915371332, 0.23384526581815604, 0.1650658957151419, 0.18729126402988514, 0.24647363161117627, 0.18826440502461778, 0.27324111687780067, 0.27774895982490777, 0.22230250582840191, 0.19378140057526858, 0.20282182771077267, 0.262244899105479, 0.2391993399924644, 0.2725461633518187, 0.22698471578025473, 0.16353296315165156, 0.26192943447664735, 0.31204081228421365, 0.2756693995606853, 0.18895459043294052], 'val_acc': [0.7272271645157249, 0.6728480140333292, 0.7544167397569227, 0.7729607818569102, 0.7809798270893372, 0.8225786242325523, 0.8171908282170154, 0.7517854905400326, 0.8210750532514722, 0.7953890489913544, 0.8409973687507831, 0.8511464728730735, 0.8242074927953891, 0.8233304097230923, 0.8450068913669966, 0.8327277283548428, 0.7922566094474377, 0.8065405337676983, 0.8124295201102619, 0.8110512467109385, 0.8344818944994361, 0.8451321889487533, 0.8149354717453953, 0.8218268387420122, 0.821200350833229, 0.8767071795514346, 0.8596667084325272, 0.8778348577872447, 0.8384914171156497, 0.8397443929332163, 0.8263375516852525, 0.8743265254980579, 0.8854780102744017, 0.861295576995364, 0.8552812930710437, 0.8874827715825084, 0.8921187821075053, 0.8740759303345446, 0.8639268262122541, 0.799398571607568, 0.8757047988973813, 0.8723217641899511, 0.8388673098609197, 0.8851021175291317, 0.8807167021676482, 0.8810925949129181, 0.8871068788372385, 0.9026437789750658, 0.8739506327527878, 0.8884851522365619, 0.8907405087081819, 0.9181806791128931, 0.8871068788372385, 0.838742012279163, 0.9294574614709936, 0.8689387294825209, 0.8402455832602431, 0.9272021049993735, 0.9154241323142464, 0.9163012153865431, 0.886104498183185, 0.8968800902142589, 0.915173537150733, 0.9070291943365493, 0.907906277408846, 0.9368500187946373, 0.9299586517980203, 0.9013908031574991, 0.9245708557824834, 0.8953765192331788, 0.8943741385791254, 0.9131687758426262, 0.9261997243453202, 0.9193083573487032, 0.8978824708683122, 0.9094098483899261, 0.8977571732865556, 0.9139205613331662, 0.938102994612204, 0.9000125297581757, 0.8825961658939983, 0.9015161007392557, 0.9258238316000501], 'final_model_size_bytes': 185464, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0001664694645615061, 'batch_size': 64, 'epochs': 83, 'base_channels': 8, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 3, 'dropout': 0.21039380566532462, 'weight_decay': 0.007114879904098962, 'gamma_focal': 1.030595736517539, 'effective_beta': 0.9764242604366347, 'label_smoothing': 0.09943974515620842, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.1792187853154319, 'smote_max_multiplier': 3, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}, 'model_parameter_count': 26314, 'model_storage_size_kb': 113.06796875, 'model_size_validation': 'PASS'}
2025-10-03 03:53:54,038 - INFO - _models.training_function_executor - BO Objective: base=0.9258, size_penalty=0.0000, final=0.9258
2025-10-03 03:53:54,038 - INFO - _models.training_function_executor - Model: 26,314 parameters, 113.1KB (PASS 256KB limit)
2025-10-03 03:53:54,038 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 457.124s
2025-10-03 03:53:54,154 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9258
2025-10-03 03:53:54,154 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.117s
2025-10-03 03:53:54,154 - INFO - bo.run_bo - Recorded observation #38: hparams={'lr': 0.0001664694645615061, 'batch_size': np.int64(64), 'epochs': np.int64(83), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(3), 'dropout': 0.21039380566532462, 'weight_decay': 0.007114879904098962, 'gamma_focal': 1.030595736517539, 'effective_beta': 0.9764242604366347, 'label_smoothing': 0.09943974515620842, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.1792187853154319, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)}, value=0.9258
2025-10-03 03:53:54,154 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 38: {'lr': 0.0001664694645615061, 'batch_size': np.int64(64), 'epochs': np.int64(83), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(3), 'dropout': 0.21039380566532462, 'weight_decay': 0.007114879904098962, 'gamma_focal': 1.030595736517539, 'effective_beta': 0.9764242604366347, 'label_smoothing': 0.09943974515620842, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.1792187853154319, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)} -> 0.9258
2025-10-03 03:53:54,155 - INFO - bo.run_bo - ðŸ”BO Trial 39: Using RF surrogate + Expected Improvement
2025-10-03 03:53:54,155 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 03:53:54,155 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:53:54,155 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:53:54,155 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0005947369650524916, 'batch_size': 128, 'epochs': 63, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.035378074367861674, 'weight_decay': 5.418473104591447e-06, 'gamma_focal': 1.3603138601767732, 'effective_beta': 0.9949693697877503, 'label_smoothing': 0.09776402375775896, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.11278120357225027, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 4}
2025-10-03 03:53:54,156 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0005947369650524916, 'batch_size': 128, 'epochs': 63, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.035378074367861674, 'weight_decay': 5.418473104591447e-06, 'gamma_focal': 1.3603138601767732, 'effective_beta': 0.9949693697877503, 'label_smoothing': 0.09776402375775896, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.11278120357225027, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 4}
2025-10-03 03:57:14,838 - INFO - _models.training_function_executor - Model: 68,353 parameters, 293.7KB storage
2025-10-03 03:57:14,838 - WARNING - _models.training_function_executor - Model storage 293.7KB exceeds 256KB limit!
2025-10-03 03:57:14,838 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.522782428695645, 0.27649419805677594, 0.20884283120161615, 0.17698114354001807, 0.15747420374721174, 0.14010648467636586, 0.12953420903752833, 0.11783298896086263, 0.10842896330540705, 0.09875846692123401, 0.09377706869709039, 0.08595578347265301, 0.08406805029372018, 0.07850060728553097, 0.07439801942996212, 0.07172692625658798, 0.06495020504894512, 0.06577093364865894, 0.05925494621802048, 0.058681197426392495, 0.055668999887909805, 0.05457058678208333, 0.052207406697396715, 0.050343537214545264, 0.047667246047079954, 0.04580018699741109, 0.04581887486669765, 0.04338827229128415, 0.04358519162443749, 0.04021948512105046, 0.03893754044016762, 0.03713222257344527, 0.0377179834955312, 0.035291305065236256, 0.03654533775272442, 0.03453618905678571, 0.03206818031755031, 0.030156938610850153, 0.030574058304796117, 0.029848200213096403, 0.030267579183185504, 0.028421899814403195, 0.02898741339589506, 0.027052863401703684, 0.02659291957136791, 0.02462342078511967, 0.024483998015255595, 0.025719205617681297, 0.02472488446084581, 0.023575015773253428, 0.022057714659696123, 0.02443490063711534, 0.020819935455238064, 0.024055409767290474, 0.022563339031519095, 0.01893332751031881, 0.02131585798444374, 0.019867556829994547, 0.01993610296693366, 0.021024627536586026, 0.01874333420396528, 0.020534631301435916, 0.01823083048378835], 'val_losses': [0.2735800892151981, 0.16643002069529667, 0.1465866432226835, 0.14444048017685374, 0.12043025794856427, 0.10889332676991352, 0.10223175702522518, 0.09334911903803576, 0.10731146283995731, 0.09878670614198282, 0.090389519463116, 0.0824808732614289, 0.08656457860044721, 0.07789928012661372, 0.07898376066162538, 0.07321846782937114, 0.0739958386323518, 0.06990105487465066, 0.07710539015324369, 0.08084527507019199, 0.07363688449500543, 0.073639980277058, 0.06378091614641591, 0.07410678998253845, 0.06989458418963412, 0.07198614449283561, 0.07434719533023194, 0.07034793172380319, 0.07104033485844745, 0.06717462855246623, 0.06826895265527842, 0.07468943725237495, 0.06319982553354378, 0.06752936057153612, 0.07111788247106907, 0.06492251304326357, 0.06927127011989445, 0.07301253421856876, 0.07236475812553866, 0.08048200330972044, 0.06890774773574536, 0.06999882349930914, 0.07538945698645491, 0.06483100525509249, 0.06805254996024551, 0.07412110768420074, 0.07568097604484639, 0.07401326216468834, 0.07612395434432752, 0.0728963763044526, 0.07906672683384705, 0.07493409391573357, 0.0818587495687387, 0.07109049330781149, 0.06980478516752417, 0.07903991442263807, 0.0773505280712705, 0.08257075116204493, 0.0850136277557928, 0.07382447547093174, 0.08487055542818243, 0.07370021544016926, 0.0910696642539388], 'val_acc': [0.8640521237940108, 0.9230672848014033, 0.9303345445432903, 0.9339681744142339, 0.9453702543540909, 0.9491291818067912, 0.9511339431148979, 0.9577747149480015, 0.955268763312868, 0.955268763312868, 0.9594035835108382, 0.9634131061270518, 0.962160130309485, 0.9656684625986719, 0.9644154867811051, 0.9654178674351584, 0.967547926325022, 0.9671720335797519, 0.9670467359979953, 0.9620348327277284, 0.9662949505074552, 0.9698032827966421, 0.9713068537777221, 0.9667961408344818, 0.9676732239067786, 0.9670467359979953, 0.9679238190702919, 0.9724345320135321, 0.9716827465229921, 0.9724345320135321, 0.9705550682871821, 0.9684250093973187, 0.9734369126675855, 0.9736875078310988, 0.9715574489412354, 0.9719333416865055, 0.9743139957398822, 0.9720586392682621, 0.970179175541912, 0.9667961408344818, 0.9734369126675855, 0.9728104247588022, 0.9689261997243453, 0.9734369126675855, 0.9761934594662323, 0.9730610199223155, 0.9739381029946123, 0.9731863175040721, 0.9739381029946123, 0.9719333416865055, 0.9718080441047487, 0.9721839368500188, 0.9711815561959655, 0.9736875078310988, 0.9759428643027189, 0.9718080441047487, 0.9734369126675855, 0.9749404836486656, 0.9728104247588022, 0.9733116150858289, 0.9718080441047487, 0.9751910788121789, 0.9703044731236687], 'final_model_size_bytes': 151458, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0005947369650524916, 'batch_size': 128, 'epochs': 63, 'base_channels': 11, 'ds_kernel_size': 5, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.035378074367861674, 'weight_decay': 5.418473104591447e-06, 'gamma_focal': 1.3603138601767732, 'effective_beta': 0.9949693697877503, 'label_smoothing': 0.09776402375775896, 'use_amp': True, 'use_smote_tomek': True, 'smote_target_min_frac': 0.11278120357225027, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'quant_calibrate_batches': 4}, 'model_parameter_count': 68353, 'model_storage_size_kb': 293.704296875, 'model_size_validation': 'FAIL'}
2025-10-03 03:57:14,838 - INFO - _models.training_function_executor - BO Objective: base=0.9703, size_penalty=0.0736, final=0.8967
2025-10-03 03:57:14,839 - INFO - _models.training_function_executor - Model: 68,353 parameters, 293.7KB (FAIL 256KB limit)
2025-10-03 03:57:14,839 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 200.684s
2025-10-03 03:57:14,957 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8967
2025-10-03 03:57:14,957 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.118s
2025-10-03 03:57:14,957 - INFO - bo.run_bo - Recorded observation #39: hparams={'lr': 0.0005947369650524916, 'batch_size': np.int64(128), 'epochs': np.int64(63), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.035378074367861674, 'weight_decay': 5.418473104591447e-06, 'gamma_focal': 1.3603138601767732, 'effective_beta': 0.9949693697877503, 'label_smoothing': 0.09776402375775896, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.11278120357225027, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(4)}, value=0.8967
2025-10-03 03:57:14,957 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 39: {'lr': 0.0005947369650524916, 'batch_size': np.int64(128), 'epochs': np.int64(63), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.035378074367861674, 'weight_decay': 5.418473104591447e-06, 'gamma_focal': 1.3603138601767732, 'effective_beta': 0.9949693697877503, 'label_smoothing': 0.09776402375775896, 'use_amp': np.True_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.11278120357225027, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(4)} -> 0.8967
2025-10-03 03:57:14,957 - INFO - bo.run_bo - ðŸ”BO Trial 40: Using RF surrogate + Expected Improvement
2025-10-03 03:57:14,957 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 03:57:14,957 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:57:14,957 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:57:14,957 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0011461128825834086, 'batch_size': 128, 'epochs': 31, 'base_channels': 9, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.03581941372830034, 'weight_decay': 0.00024451716040947355, 'gamma_focal': 1.2039390800065732, 'effective_beta': 0.912260266036551, 'label_smoothing': 0.09090242121965915, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1852098902961939, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 03:57:14,959 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0011461128825834086, 'batch_size': 128, 'epochs': 31, 'base_channels': 9, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.03581941372830034, 'weight_decay': 0.00024451716040947355, 'gamma_focal': 1.2039390800065732, 'effective_beta': 0.912260266036551, 'label_smoothing': 0.09090242121965915, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1852098902961939, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 03:58:32,565 - INFO - _models.training_function_executor - Model: 9,273 parameters, 19.9KB storage
2025-10-03 03:58:32,565 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5456149276943802, 0.384942265220727, 0.290138015777542, 0.21969318289160486, 0.19458715318961314, 0.17819529195746073, 0.1622482431147573, 0.1537167894285887, 0.14502829615171065, 0.14149381276424886, 0.13621624543550107, 0.13078274998498904, 0.12603457486960343, 0.12308896827654063, 0.11785233868509778, 0.11621619587976473, 0.11614881572454372, 0.10980478478567315, 0.10761747658495444, 0.10549015917507804, 0.10692167123992917, 0.10323023915811765, 0.10005718027364788, 0.09759900968470371, 0.09500952741128327, 0.09569297741705421, 0.0952457032171877, 0.09118082405678914, 0.0899787998778649, 0.08987841094955108, 0.08775698451931359], 'val_losses': [0.43286161202563483, 0.3172252044751969, 0.25262144330994946, 0.1872803874462264, 0.17430467294386015, 0.16048078034145463, 0.14420126906733005, 0.14255536130579652, 0.14441927196335455, 0.13296338507587965, 0.1355901165476659, 0.12192715761386994, 0.11735044436475833, 0.1146764768841362, 0.10767135830835775, 0.11154651598156044, 0.10489280302114407, 0.109789029855079, 0.0994128427121547, 0.1078964592645064, 0.10457257432434855, 0.09877685856742977, 0.09378495037298037, 0.104602370532051, 0.08897912390170697, 0.09193652133481751, 0.08885422510105093, 0.09539687796043672, 0.09050401857421744, 0.09559768212049202, 0.08954990965959168], 'val_acc': [0.783736373887984, 0.8546548051622603, 0.8972559829595289, 0.92319258238316, 0.9249467485277534, 0.9354717453953139, 0.9416113269013908, 0.9406089462473374, 0.9416113269013908, 0.9454955519358477, 0.9418619220649042, 0.9453702543540909, 0.9505074552061146, 0.9516351334419245, 0.9520110261871946, 0.9491291818067912, 0.952637514095978, 0.954516977822328, 0.9555193584763814, 0.9555193584763814, 0.9497556697155745, 0.955268763312868, 0.9601553690013783, 0.9570229294574615, 0.9624107254729983, 0.9596541786743515, 0.9615336424007017, 0.9587770956020549, 0.9600300714196216, 0.9573988222027315, 0.9601553690013783], 'final_model_size_bytes': 29190, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0011461128825834086, 'batch_size': 128, 'epochs': 31, 'base_channels': 9, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.03581941372830034, 'weight_decay': 0.00024451716040947355, 'gamma_focal': 1.2039390800065732, 'effective_beta': 0.912260266036551, 'label_smoothing': 0.09090242121965915, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1852098902961939, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}, 'model_parameter_count': 9273, 'model_storage_size_kb': 19.922460937500002, 'model_size_validation': 'PASS'}
2025-10-03 03:58:32,565 - INFO - _models.training_function_executor - BO Objective: base=0.9602, size_penalty=0.0000, final=0.9602
2025-10-03 03:58:32,565 - INFO - _models.training_function_executor - Model: 9,273 parameters, 19.9KB (PASS 256KB limit)
2025-10-03 03:58:32,565 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 77.608s
2025-10-03 03:58:32,683 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9602
2025-10-03 03:58:32,683 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.117s
2025-10-03 03:58:32,683 - INFO - bo.run_bo - Recorded observation #40: hparams={'lr': 0.0011461128825834086, 'batch_size': np.int64(128), 'epochs': np.int64(31), 'base_channels': np.int64(9), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.03581941372830034, 'weight_decay': 0.00024451716040947355, 'gamma_focal': 1.2039390800065732, 'effective_beta': 0.912260266036551, 'label_smoothing': 0.09090242121965915, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1852098902961939, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)}, value=0.9602
2025-10-03 03:58:32,683 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 40: {'lr': 0.0011461128825834086, 'batch_size': np.int64(128), 'epochs': np.int64(31), 'base_channels': np.int64(9), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.03581941372830034, 'weight_decay': 0.00024451716040947355, 'gamma_focal': 1.2039390800065732, 'effective_beta': 0.912260266036551, 'label_smoothing': 0.09090242121965915, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1852098902961939, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)} -> 0.9602
2025-10-03 03:58:32,684 - INFO - bo.run_bo - ðŸ”BO Trial 41: Using RF surrogate + Expected Improvement
2025-10-03 03:58:32,684 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 03:58:32,684 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 03:58:32,684 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 03:58:32,684 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 5.941484168194521e-05, 'batch_size': 64, 'epochs': 54, 'base_channels': 8, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.007737150609025302, 'weight_decay': 1.1513999005010297e-06, 'gamma_focal': 1.4184475894937236, 'effective_beta': 0.9308052908646784, 'label_smoothing': 0.09928953461574798, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18549743176459843, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 6}
2025-10-03 03:58:32,685 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 5.941484168194521e-05, 'batch_size': 64, 'epochs': 54, 'base_channels': 8, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.007737150609025302, 'weight_decay': 1.1513999005010297e-06, 'gamma_focal': 1.4184475894937236, 'effective_beta': 0.9308052908646784, 'label_smoothing': 0.09928953461574798, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18549743176459843, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 6}
2025-10-03 04:01:24,568 - INFO - _models.training_function_executor - Model: 42,735 parameters, 183.6KB storage
2025-10-03 04:01:24,568 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6037942333071019, 0.50241498136731, 0.44479377170954604, 0.4096240535813966, 0.38254460206949836, 0.3619039575453073, 0.35002542612670123, 0.33805020696252813, 0.32775783110870343, 0.3177966245642074, 0.3087876891782139, 0.30001889196429693, 0.2923873765958609, 0.2854516329573154, 0.2744594042413505, 0.26553638187481476, 0.25729096932593126, 0.24989149821539128, 0.24124487959957322, 0.2333166913802405, 0.22678610245483852, 0.22090567522612378, 0.2142662354014599, 0.20709246272923776, 0.2051775803128938, 0.19906463673749006, 0.19420680539637056, 0.19056966050530877, 0.1858876389459056, 0.18286653958008717, 0.18019998656976802, 0.1757033449923735, 0.17150997369104476, 0.1698111087294803, 0.165543762970433, 0.16281605376466243, 0.16102122745121294, 0.15861625556865813, 0.1557179463317948, 0.1550231283945506, 0.15359347746515095, 0.1512404046363409, 0.1494371053610358, 0.14821933962235986, 0.1464911666158013, 0.14402920328386631, 0.14318829110622242, 0.1410581454323321, 0.1392094871623883, 0.13856387480379692, 0.13765228645595337, 0.1365960671434881, 0.13423311087293732, 0.13385469246709034], 'val_losses': [0.5437536977381803, 0.4689024824230403, 0.42519783954649637, 0.3915072666188706, 0.3705438356428812, 0.34960582996128703, 0.33548788622706893, 0.32655597673541187, 0.3164168262639271, 0.30138681303766224, 0.2972929056795927, 0.28546936193506994, 0.27820380106745707, 0.2698311049147158, 0.25990978458212693, 0.2512737029817764, 0.24769319544055382, 0.23592324060330847, 0.22919073141901256, 0.22369890721754507, 0.2190674392856791, 0.21516408928951333, 0.20566723823861033, 0.20003895250952733, 0.20319319395153554, 0.1950026800658317, 0.19014259943753284, 0.18350979946427678, 0.18065641348806483, 0.17674967120455823, 0.17599566224642735, 0.1711868640941391, 0.17454973678550928, 0.16647186117869184, 0.16863367870933593, 0.16674201968483662, 0.1586528868939461, 0.15684086999981614, 0.1559900304545527, 0.15281308649880085, 0.1497211465951629, 0.14924440878052384, 0.14803444643902489, 0.1461093753255141, 0.14639129760106723, 0.1446695548951693, 0.14247575507189691, 0.13965740576414457, 0.14117359831815315, 0.140880005162262, 0.13765975020572874, 0.13741812799272943, 0.1341606661641886, 0.1341603282601474], 'val_acc': [0.728354842751535, 0.746648289688009, 0.7753414359102869, 0.7892494674852776, 0.7884976819947375, 0.8045357724595915, 0.8116777346197218, 0.8136824959278286, 0.8205738629244456, 0.8344818944994361, 0.839493797769703, 0.8497681994737502, 0.8577872447061772, 0.8638015286304974, 0.876205989224408, 0.8798396190953515, 0.8775842626237313, 0.8944994361608821, 0.8903646159629119, 0.8961283047237188, 0.8948753289061521, 0.9000125297581757, 0.9074050870818193, 0.9102869314622227, 0.9059015161007392, 0.9105375266257362, 0.9144217516601929, 0.9195589525122165, 0.9201854404209999, 0.9216890114020799, 0.9224407968926199, 0.9239443678737, 0.9189324646034331, 0.9275779977446436, 0.9260744267635634, 0.9240696654554567, 0.9299586517980203, 0.9317128179426137, 0.9322140082696404, 0.9329657937601804, 0.9353464478135572, 0.9339681744142339, 0.931587520360857, 0.9359729357223405, 0.9368500187946373, 0.9368500187946373, 0.9372259115399073, 0.9382282921939606, 0.9373512091216639, 0.937727101866934, 0.939606565593284, 0.940358351083824, 0.9399824583385541, 0.9422378148101742], 'final_model_size_bytes': 179227, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 5.941484168194521e-05, 'batch_size': 64, 'epochs': 54, 'base_channels': 8, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.007737150609025302, 'weight_decay': 1.1513999005010297e-06, 'gamma_focal': 1.4184475894937236, 'effective_beta': 0.9308052908646784, 'label_smoothing': 0.09928953461574798, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18549743176459843, 'smote_max_multiplier': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 6}, 'model_parameter_count': 42735, 'model_storage_size_kb': 183.62695312500003, 'model_size_validation': 'PASS'}
2025-10-03 04:01:24,568 - INFO - _models.training_function_executor - BO Objective: base=0.9422, size_penalty=0.0000, final=0.9422
2025-10-03 04:01:24,568 - INFO - _models.training_function_executor - Model: 42,735 parameters, 183.6KB (PASS 256KB limit)
2025-10-03 04:01:24,568 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 171.884s
2025-10-03 04:01:24,686 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9422
2025-10-03 04:01:24,686 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.118s
2025-10-03 04:01:24,686 - INFO - bo.run_bo - Recorded observation #41: hparams={'lr': 5.941484168194521e-05, 'batch_size': np.int64(64), 'epochs': np.int64(54), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.007737150609025302, 'weight_decay': 1.1513999005010297e-06, 'gamma_focal': 1.4184475894937236, 'effective_beta': 0.9308052908646784, 'label_smoothing': 0.09928953461574798, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.18549743176459843, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(6)}, value=0.9422
2025-10-03 04:01:24,686 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 41: {'lr': 5.941484168194521e-05, 'batch_size': np.int64(64), 'epochs': np.int64(54), 'base_channels': np.int64(8), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.007737150609025302, 'weight_decay': 1.1513999005010297e-06, 'gamma_focal': 1.4184475894937236, 'effective_beta': 0.9308052908646784, 'label_smoothing': 0.09928953461574798, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.18549743176459843, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(6)} -> 0.9422
2025-10-03 04:01:24,687 - INFO - bo.run_bo - ðŸ”BO Trial 42: Using RF surrogate + Expected Improvement
2025-10-03 04:01:24,687 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 04:01:24,687 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:01:24,687 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:01:24,687 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0004882338207479475, 'batch_size': 512, 'epochs': 21, 'base_channels': 11, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.05376580733794335, 'weight_decay': 5.2639165684154834e-05, 'gamma_focal': 1.7055802417303418, 'effective_beta': 0.9654445186406021, 'label_smoothing': 0.09922693240196566, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18311660110048672, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 1}
2025-10-03 04:01:24,688 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0004882338207479475, 'batch_size': 512, 'epochs': 21, 'base_channels': 11, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.05376580733794335, 'weight_decay': 5.2639165684154834e-05, 'gamma_focal': 1.7055802417303418, 'effective_beta': 0.9654445186406021, 'label_smoothing': 0.09922693240196566, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18311660110048672, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 1}
2025-10-03 04:06:48,016 - INFO - _models.training_function_executor - Model: 26,362 parameters, 113.3KB storage
2025-10-03 04:06:48,016 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5143475912027526, 0.39279960637119027, 0.2994657951703747, 0.25036903482447576, 0.21427824244141688, 0.1831310754332354, 0.16010058501676575, 0.14240031215735882, 0.1311455730613609, 0.12381308131391153, 0.11563390353147546, 0.11251353108729426, 0.10586447278363541, 0.10492318644067068, 0.09871508198809595, 0.09497338359290876, 0.091426066816445, 0.088576439131732, 0.08766713880285604, 0.08428118902122946, 0.08076998094075832], 'val_losses': [0.45788292740837494, 0.33986750704022434, 0.2733396012567067, 0.23611907095049367, 0.2103535994605423, 0.17223133483583983, 0.1580716345266447, 0.14511934725051684, 0.13238890543477366, 0.128460755868822, 0.12288674137867449, 0.12570843238991733, 0.11003279690641819, 0.11680471956031602, 0.10435544288859692, 0.10184025023048077, 0.09841981699626975, 0.1009780575341934, 0.09229943502025487, 0.09447762098916267, 0.08906250394476549], 'val_acc': [0.7418869815812554, 0.7884976819947375, 0.8384914171156497, 0.870066407718331, 0.8863550933466984, 0.9183059766946498, 0.9244455582007267, 0.9285803783986969, 0.9352211502318005, 0.9347199599047739, 0.9354717453953139, 0.9376018042851773, 0.9428643027189575, 0.9360982333040972, 0.9444931712817942, 0.9463726350081444, 0.9495050745520611, 0.9480015035709811, 0.9496303721338178, 0.9506327527878712, 0.9535145971682747], 'final_model_size_bytes': 166541, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0004882338207479475, 'batch_size': 512, 'epochs': 21, 'base_channels': 11, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.05376580733794335, 'weight_decay': 5.2639165684154834e-05, 'gamma_focal': 1.7055802417303418, 'effective_beta': 0.9654445186406021, 'label_smoothing': 0.09922693240196566, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18311660110048672, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 1}, 'model_parameter_count': 26362, 'model_storage_size_kb': 113.27421875, 'model_size_validation': 'PASS'}
2025-10-03 04:06:48,017 - INFO - _models.training_function_executor - BO Objective: base=0.9535, size_penalty=0.0000, final=0.9535
2025-10-03 04:06:48,017 - INFO - _models.training_function_executor - Model: 26,362 parameters, 113.3KB (PASS 256KB limit)
2025-10-03 04:06:48,017 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 323.330s
2025-10-03 04:06:48,137 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9535
2025-10-03 04:06:48,137 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.121s
2025-10-03 04:06:48,137 - INFO - bo.run_bo - Recorded observation #42: hparams={'lr': 0.0004882338207479475, 'batch_size': np.int64(512), 'epochs': np.int64(21), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.05376580733794335, 'weight_decay': 5.2639165684154834e-05, 'gamma_focal': 1.7055802417303418, 'effective_beta': 0.9654445186406021, 'label_smoothing': 0.09922693240196566, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.18311660110048672, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(1)}, value=0.9535
2025-10-03 04:06:48,137 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 42: {'lr': 0.0004882338207479475, 'batch_size': np.int64(512), 'epochs': np.int64(21), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.05376580733794335, 'weight_decay': 5.2639165684154834e-05, 'gamma_focal': 1.7055802417303418, 'effective_beta': 0.9654445186406021, 'label_smoothing': 0.09922693240196566, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.18311660110048672, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(1)} -> 0.9535
2025-10-03 04:06:48,138 - INFO - bo.run_bo - ðŸ”BO Trial 43: Using RF surrogate + Expected Improvement
2025-10-03 04:06:48,138 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 04:06:48,138 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:06:48,138 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:06:48,138 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.828898348222212e-05, 'batch_size': 128, 'epochs': 13, 'base_channels': 7, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.234951472736742, 'weight_decay': 0.0006830158858248096, 'gamma_focal': 2.519200623184112, 'effective_beta': 0.9802518350623161, 'label_smoothing': 0.09888398358414087, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1893068794997887, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 04:06:48,139 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.828898348222212e-05, 'batch_size': 128, 'epochs': 13, 'base_channels': 7, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.234951472736742, 'weight_decay': 0.0006830158858248096, 'gamma_focal': 2.519200623184112, 'effective_beta': 0.9802518350623161, 'label_smoothing': 0.09888398358414087, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1893068794997887, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 04:08:05,430 - INFO - _models.training_function_executor - Model: 25,903 parameters, 111.3KB storage
2025-10-03 04:08:05,430 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5698709730960386, 0.43294130862496705, 0.4184377408871969, 0.4105071766041232, 0.40099141289333956, 0.39058257797227947, 0.37593067489340126, 0.35045883357776936, 0.3310390470558041, 0.3200017866218082, 0.31239071733909074, 0.30511198231414594, 0.2977475065640988], 'val_losses': [0.44565789884544676, 0.42185714750148917, 0.41388196080209855, 0.40609109745992333, 0.39579747922512215, 0.3826566480586529, 0.35626581711154315, 0.3541283202662741, 0.3720769891388179, 0.3740361473783188, 0.369791587131571, 0.38985415917487837, 0.38883219057942164], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7188322265380278, 0.7143215135947876, 0.7337426387670718, 0.715699786994111, 0.6786117027941361, 0.6794887858664328, 0.6819947375015663, 0.6603182558576619, 0.660694148602932], 'final_model_size_bytes': 116087, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.828898348222212e-05, 'batch_size': 128, 'epochs': 13, 'base_channels': 7, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.234951472736742, 'weight_decay': 0.0006830158858248096, 'gamma_focal': 2.519200623184112, 'effective_beta': 0.9802518350623161, 'label_smoothing': 0.09888398358414087, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1893068794997887, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}, 'model_parameter_count': 25903, 'model_storage_size_kb': 111.30195312500001, 'model_size_validation': 'PASS'}
2025-10-03 04:08:05,430 - INFO - _models.training_function_executor - BO Objective: base=0.6607, size_penalty=0.0000, final=0.6607
2025-10-03 04:08:05,430 - INFO - _models.training_function_executor - Model: 25,903 parameters, 111.3KB (PASS 256KB limit)
2025-10-03 04:08:05,430 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 77.292s
2025-10-03 04:08:05,552 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6607
2025-10-03 04:08:05,552 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.122s
2025-10-03 04:08:05,553 - INFO - bo.run_bo - Recorded observation #43: hparams={'lr': 3.828898348222212e-05, 'batch_size': np.int64(128), 'epochs': np.int64(13), 'base_channels': np.int64(7), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.234951472736742, 'weight_decay': 0.0006830158858248096, 'gamma_focal': 2.519200623184112, 'effective_beta': 0.9802518350623161, 'label_smoothing': 0.09888398358414087, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1893068794997887, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)}, value=0.6607
2025-10-03 04:08:05,553 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 43: {'lr': 3.828898348222212e-05, 'batch_size': np.int64(128), 'epochs': np.int64(13), 'base_channels': np.int64(7), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.234951472736742, 'weight_decay': 0.0006830158858248096, 'gamma_focal': 2.519200623184112, 'effective_beta': 0.9802518350623161, 'label_smoothing': 0.09888398358414087, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1893068794997887, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)} -> 0.6607
2025-10-03 04:08:05,553 - INFO - bo.run_bo - ðŸ”BO Trial 44: Using RF surrogate + Expected Improvement
2025-10-03 04:08:05,553 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 04:08:05,553 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:08:05,553 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:08:05,553 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 4.250139242691296e-05, 'batch_size': 256, 'epochs': 80, 'base_channels': 7, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.055476395773081735, 'weight_decay': 0.001704600882030296, 'gamma_focal': 1.8216243476898497, 'effective_beta': 0.9474035360113531, 'label_smoothing': 0.0008899417515335075, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.17340129921199726, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 04:08:05,555 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 4.250139242691296e-05, 'batch_size': 256, 'epochs': 80, 'base_channels': 7, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.055476395773081735, 'weight_decay': 0.001704600882030296, 'gamma_focal': 1.8216243476898497, 'effective_beta': 0.9474035360113531, 'label_smoothing': 0.0008899417515335075, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.17340129921199726, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}
2025-10-03 04:13:57,438 - INFO - _models.training_function_executor - Model: 51,057 parameters, 109.7KB storage
2025-10-03 04:13:57,438 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6113421655431658, 0.46652672745487433, 0.4453391365451823, 0.42221751458378315, 0.38544561349818707, 0.3491143076072468, 0.33092991459120585, 0.318324835675815, 0.31005943634921335, 0.3025798653473708, 0.2943484225449166, 0.2885378266153481, 0.28068347146564626, 0.27017387378911484, 0.2619500441650988, 0.25418319940731327, 0.24592740516579248, 0.24049155641381828, 0.23055085602460287, 0.22476722909906577, 0.21839334737484167, 0.21083098663001926, 0.20335720609674565, 0.1984561886465109, 0.1906449019178854, 0.18862435462369478, 0.18227099347675824, 0.17860661021387414, 0.1758834319716949, 0.17335990622335778, 0.1701861666132998, 0.1680449117118022, 0.1627421573187681, 0.16194967678806274, 0.15670744026070962, 0.15519368980086073, 0.15283771630276102, 0.15121476964869454, 0.14977034516764423, 0.148941394409709, 0.1454555649384219, 0.14220513356877265, 0.14179862920033007, 0.1388399723452912, 0.13577284910401183, 0.13739468157618162, 0.13529618256860884, 0.13018095706637162, 0.12899952520134195, 0.13060884506721168, 0.12761439790019333, 0.12449077697859907, 0.12488121495920028, 0.1231400668957148, 0.12249306381715926, 0.1195297916372688, 0.1185503654359102, 0.11839116104788541, 0.11698478413073299, 0.11326882176429591, 0.11456873464335274, 0.11173615170272748, 0.11188090173800376, 0.11067720013728242, 0.10969499092153136, 0.10700723452776129, 0.10687854010297276, 0.10745281546421544, 0.10648048071779039, 0.10432298657628851, 0.10370640294095446, 0.10383117940617964, 0.10180844197574461, 0.10016541267436156, 0.10062601700337859, 0.0996857890557295, 0.09911174730250169, 0.09830642582625371, 0.09898507728002479, 0.09675184756548218], 'val_losses': [0.4810248010141511, 0.4546605256689385, 0.4371766114373835, 0.4092246375239563, 0.3673856436773106, 0.3367430873419644, 0.3210639792185591, 0.309123565220561, 0.3017230622153909, 0.2908402457170477, 0.2824098794771128, 0.27308505959505425, 0.2646756329892378, 0.25675188247317465, 0.2453977493691782, 0.2378495191440384, 0.22657489853535, 0.21623887341992776, 0.21032594685610667, 0.2008654480309224, 0.19534739168787224, 0.18726487447109874, 0.18040329407232625, 0.17651070955738868, 0.1671043683761583, 0.16333406470186682, 0.1596622426637416, 0.15933754477991136, 0.15091130772437267, 0.14918259458163377, 0.14924372993657023, 0.14347378038035166, 0.14292458755422424, 0.14086752734511676, 0.1373476969028472, 0.13894694604426752, 0.1349465671547253, 0.13268126409546113, 0.13391433165920602, 0.12805199421463329, 0.12646486969493623, 0.1258211760962462, 0.12409912805564122, 0.12157387388460529, 0.11907036550965708, 0.1187357700116771, 0.11599843265635076, 0.11949344691276072, 0.11510453814851387, 0.11603477972519605, 0.11177684643964333, 0.10924049941366619, 0.10586390001196384, 0.10992218097892416, 0.10553535282455012, 0.10477704096521565, 0.10193800862541086, 0.10023914101674673, 0.10069572644012044, 0.10339332027940759, 0.10010644202393079, 0.09728480215852922, 0.10081758177895815, 0.09725873902124535, 0.098131127663908, 0.0956750782942984, 0.0970171176204256, 0.09424058980787028, 0.09449921749451005, 0.09356879051870429, 0.09158267874292615, 0.09219643369255441, 0.09487892198332598, 0.08990270817037842, 0.08845845009357227, 0.08997399209921647, 0.08838938921223218, 0.09003781061724543, 0.08754079957987308, 0.09232075003443349], 'val_acc': [0.7200852023555946, 0.7180804410474878, 0.7155744894123544, 0.7190828217015411, 0.737125673474502, 0.7782232802906904, 0.7856158376143341, 0.7873700037589274, 0.7874953013406841, 0.7968926199724345, 0.8040345821325648, 0.8084199974940484, 0.8111765442926951, 0.8184438040345822, 0.8275905275028193, 0.8361107630622729, 0.8426262373136199, 0.8517729607818569, 0.855155995489287, 0.862423255231174, 0.8658062899386042, 0.8721964666081945, 0.8783360481142714, 0.8827214634757549, 0.8874827715825084, 0.8913669966169653, 0.8942488409973688, 0.8962536023054755, 0.8986342563588523, 0.9008896128304724, 0.901766695902769, 0.9052750281919559, 0.9062774088460093, 0.9051497306101992, 0.9081568725723593, 0.9040220523743892, 0.9077809798270894, 0.9085327653176294, 0.9080315749906027, 0.9120410976068162, 0.9139205613331662, 0.9136699661696529, 0.914045858914923, 0.9150482395689763, 0.9173035960405964, 0.9200601428392432, 0.9194336549304598, 0.9171782984588397, 0.9209372259115399, 0.9181806791128931, 0.9223154993108633, 0.9264503195088335, 0.9270768074176169, 0.9234431775466734, 0.9288309735622102, 0.9282044856534268, 0.930083949379777, 0.9310863300338303, 0.928956271143967, 0.9287056759804536, 0.9320887106878837, 0.9337175792507204, 0.9310863300338303, 0.9330910913419371, 0.9319634131061271, 0.9339681744142339, 0.9339681744142339, 0.9347199599047739, 0.9358476381405839, 0.9353464478135572, 0.9372259115399073, 0.9362235308858539, 0.9345946623230171, 0.9378523994486906, 0.9379776970304473, 0.9387294825209873, 0.9389800776845006, 0.9372259115399073, 0.939606565593284, 0.9349705550682872], 'final_model_size_bytes': 112774, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 4.250139242691296e-05, 'batch_size': 256, 'epochs': 80, 'base_channels': 7, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 4, 'dropout': 0.055476395773081735, 'weight_decay': 0.001704600882030296, 'gamma_focal': 1.8216243476898497, 'effective_beta': 0.9474035360113531, 'label_smoothing': 0.0008899417515335075, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.17340129921199726, 'smote_max_multiplier': 2, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 5}, 'model_parameter_count': 51057, 'model_storage_size_kb': 109.69277343750001, 'model_size_validation': 'PASS'}
2025-10-03 04:13:57,438 - INFO - _models.training_function_executor - BO Objective: base=0.9350, size_penalty=0.0000, final=0.9350
2025-10-03 04:13:57,438 - INFO - _models.training_function_executor - Model: 51,057 parameters, 109.7KB (PASS 256KB limit)
2025-10-03 04:13:57,438 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 351.885s
2025-10-03 04:13:57,562 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9350
2025-10-03 04:13:57,562 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.124s
2025-10-03 04:13:57,562 - INFO - bo.run_bo - Recorded observation #44: hparams={'lr': 4.250139242691296e-05, 'batch_size': np.int64(256), 'epochs': np.int64(80), 'base_channels': np.int64(7), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.055476395773081735, 'weight_decay': 0.001704600882030296, 'gamma_focal': 1.8216243476898497, 'effective_beta': 0.9474035360113531, 'label_smoothing': 0.0008899417515335075, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.17340129921199726, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)}, value=0.9350
2025-10-03 04:13:57,562 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 44: {'lr': 4.250139242691296e-05, 'batch_size': np.int64(256), 'epochs': np.int64(80), 'base_channels': np.int64(7), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(4), 'dropout': 0.055476395773081735, 'weight_decay': 0.001704600882030296, 'gamma_focal': 1.8216243476898497, 'effective_beta': 0.9474035360113531, 'label_smoothing': 0.0008899417515335075, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.17340129921199726, 'smote_max_multiplier': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(5)} -> 0.9350
2025-10-03 04:13:57,563 - INFO - bo.run_bo - ðŸ”BO Trial 45: Using RF surrogate + Expected Improvement
2025-10-03 04:13:57,563 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 04:13:57,563 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:13:57,563 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:13:57,563 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.5809675510601566e-05, 'batch_size': 256, 'epochs': 16, 'base_channels': 10, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.00031125550386068573, 'weight_decay': 1.2597448251375742e-05, 'gamma_focal': 1.6883418691331582, 'effective_beta': 0.9379959458284152, 'label_smoothing': 0.07020045762340836, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.21140717780526708, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 04:13:57,564 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.5809675510601566e-05, 'batch_size': 256, 'epochs': 16, 'base_channels': 10, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.00031125550386068573, 'weight_decay': 1.2597448251375742e-05, 'gamma_focal': 1.6883418691331582, 'effective_beta': 0.9379959458284152, 'label_smoothing': 0.07020045762340836, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.21140717780526708, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}
2025-10-03 04:15:28,259 - INFO - _models.training_function_executor - Model: 26,484 parameters, 113.8KB storage
2025-10-03 04:15:28,259 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6501469328876756, 0.49992118120152407, 0.47137221968648607, 0.452750684081793, 0.43213255274987766, 0.41005935159085954, 0.389167730182132, 0.3653898318601133, 0.33123615993535693, 0.2955311675732011, 0.2670518494056019, 0.24311213509656618, 0.22351182103754527, 0.2078237433518749, 0.19506098582924739, 0.1844160061226794], 'val_losses': [0.5227721395602786, 0.4862094461775979, 0.4673013887135341, 0.4482039505117087, 0.4262146420525722, 0.40268629326690125, 0.3809393174090252, 0.3518342335463735, 0.3088925113998817, 0.27685720198973873, 0.2504575870947019, 0.2316156758814434, 0.21425028075857908, 0.20015678834928605, 0.18916487055992276, 0.1794297934911586], 'val_acc': [0.7200852023555946, 0.7281042475880215, 0.7342438290940985, 0.7383786492920686, 0.7428893622353089, 0.762686380152863, 0.7744643528379902, 0.790627740884601, 0.8064152361859416, 0.830347074301466, 0.8586643277784739, 0.8738253351710312, 0.8876080691642652, 0.8982583636135822, 0.9046485402831725, 0.9087833604811427], 'final_model_size_bytes': 203064, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.5809675510601566e-05, 'batch_size': 256, 'epochs': 16, 'base_channels': 10, 'ds_kernel_size': 7, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.00031125550386068573, 'weight_decay': 1.2597448251375742e-05, 'gamma_focal': 1.6883418691331582, 'effective_beta': 0.9379959458284152, 'label_smoothing': 0.07020045762340836, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.21140717780526708, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 5}, 'model_parameter_count': 26484, 'model_storage_size_kb': 113.7984375, 'model_size_validation': 'PASS'}
2025-10-03 04:15:28,259 - INFO - _models.training_function_executor - BO Objective: base=0.9088, size_penalty=0.0000, final=0.9088
2025-10-03 04:15:28,259 - INFO - _models.training_function_executor - Model: 26,484 parameters, 113.8KB (PASS 256KB limit)
2025-10-03 04:15:28,259 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 90.696s
2025-10-03 04:15:28,380 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9088
2025-10-03 04:15:28,380 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.121s
2025-10-03 04:15:28,380 - INFO - bo.run_bo - Recorded observation #45: hparams={'lr': 3.5809675510601566e-05, 'batch_size': np.int64(256), 'epochs': np.int64(16), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.00031125550386068573, 'weight_decay': 1.2597448251375742e-05, 'gamma_focal': 1.6883418691331582, 'effective_beta': 0.9379959458284152, 'label_smoothing': 0.07020045762340836, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.21140717780526708, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)}, value=0.9088
2025-10-03 04:15:28,380 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 45: {'lr': 3.5809675510601566e-05, 'batch_size': np.int64(256), 'epochs': np.int64(16), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.00031125550386068573, 'weight_decay': 1.2597448251375742e-05, 'gamma_focal': 1.6883418691331582, 'effective_beta': 0.9379959458284152, 'label_smoothing': 0.07020045762340836, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.21140717780526708, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(5)} -> 0.9088
2025-10-03 04:15:28,381 - INFO - bo.run_bo - ðŸ”BO Trial 46: Using RF surrogate + Expected Improvement
2025-10-03 04:15:28,381 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 04:15:28,381 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:15:28,381 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:15:28,381 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 4.446318222042707e-05, 'batch_size': 128, 'epochs': 98, 'base_channels': 9, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.23651068536573544, 'weight_decay': 1.634298906591329e-05, 'gamma_focal': 2.0554344741333184, 'effective_beta': 0.9672921609935468, 'label_smoothing': 0.09574446382183206, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.22574235313994173, 'smote_max_multiplier': 0, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 04:15:28,383 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 4.446318222042707e-05, 'batch_size': 128, 'epochs': 98, 'base_channels': 9, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.23651068536573544, 'weight_decay': 1.634298906591329e-05, 'gamma_focal': 2.0554344741333184, 'effective_beta': 0.9672921609935468, 'label_smoothing': 0.09574446382183206, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.22574235313994173, 'smote_max_multiplier': 0, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 04:21:26,953 - INFO - _models.training_function_executor - Model: 34,735 parameters, 149.3KB storage
2025-10-03 04:21:26,953 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8534257716249462, 0.6986802585146044, 0.644646101335762, 0.6169616943269481, 0.5953000682228649, 0.5720011393382548, 0.5442760033611992, 0.5222630639434592, 0.5004382127927658, 0.48388817355013153, 0.4651576739287585, 0.4530814585849725, 0.44214684874671767, 0.43336718590057915, 0.42330054923781013, 0.4157501368795601, 0.4082082664867192, 0.40045020633737377, 0.39019548347590166, 0.3841524766125747, 0.37741764838662306, 0.36916876289828593, 0.3611685699080217, 0.3538910246306754, 0.3511789825132685, 0.3428032851062268, 0.33915153726605807, 0.33464806676762215, 0.3294881728551147, 0.32363909238389843, 0.31839528869886846, 0.31865257853137746, 0.31277692837234206, 0.3091124075549707, 0.30429288433927193, 0.3013950400547671, 0.2969329253005315, 0.2958959973228384, 0.2914124180967181, 0.2860367276085363, 0.28453425297209983, 0.28003320924570474, 0.2776037793753622, 0.2750946119299416, 0.2730229934172403, 0.2696775547519039, 0.2646024063043778, 0.2639694490884876, 0.2606814965400397, 0.2583910472066702, 0.25405320606499676, 0.25245902634998646, 0.249222216670054, 0.24958685571651704, 0.245995271485209, 0.24717517455272425, 0.24269703623538227, 0.24022121737589733, 0.23731963004531967, 0.23551026569270211, 0.23278278742462527, 0.2328186882561653, 0.23062541734851552, 0.22809261536556333, 0.22683508126527793, 0.22419036582665058, 0.22334602533834574, 0.22352831099590312, 0.22163978409819146, 0.2190735568199871, 0.21984275464002595, 0.21571214066280348, 0.21546403160498115, 0.21441453042814235, 0.21175238346827596, 0.21069497653837255, 0.21005685462786744, 0.2116247614939891, 0.2084707022230918, 0.20575138557701297, 0.20395596132539986, 0.20360492072766817, 0.20375511839714938, 0.20020114963920738, 0.19986047944499125, 0.20033627555976982, 0.19712499669093106, 0.19923410208826564, 0.1978077243051705, 0.19355325086609051, 0.19568905406085288, 0.1941100894374095, 0.19327476143793262, 0.18951377630712313, 0.19074008450013877, 0.18751081977242756, 0.18846654688009315, 0.18766503191493059], 'val_losses': [0.5945044956011905, 0.594285243623404, 0.6150462150633477, 0.5741911133852329, 0.5843050118123599, 0.59004886832815, 0.5673760204607347, 0.5410600192093368, 0.518013541672466, 0.5210423647961571, 0.5161666836845832, 0.47733412710829587, 0.47721047448329257, 0.4525613697970844, 0.4565069144448157, 0.43567127506545633, 0.4464604979528698, 0.43555708534435855, 0.4407241145122501, 0.4475412599134678, 0.4211460468524411, 0.42573093968052533, 0.4089944647594049, 0.4267684378289083, 0.4301442336489452, 0.4213120624350211, 0.4137213547536565, 0.4081962316193955, 0.40360672705649375, 0.42169926598875346, 0.4196531797557707, 0.40553423732821614, 0.3894017825388338, 0.3632811409961668, 0.3863989171893984, 0.3817300069841462, 0.3654793191039761, 0.3737171382074173, 0.37212346341657454, 0.34191840112605143, 0.33341264080962446, 0.3456623281636135, 0.3657588056716804, 0.3559988185757758, 0.3298565079067722, 0.3167288017413354, 0.3378601107004665, 0.32823349565525217, 0.3204294082970865, 0.28403990842796867, 0.3201470119084441, 0.3238011686153063, 0.31148401840022, 0.28012156683301404, 0.2763138469964426, 0.27647801490971774, 0.27986362875217397, 0.2727369848849286, 0.2813379674339605, 0.2754357416197226, 0.26643390780462534, 0.28061465939368957, 0.27341207499313497, 0.287565268817932, 0.27186764701463395, 0.2724252519056681, 0.26964487680037386, 0.24910414644596707, 0.28769978246075245, 0.27822622293115123, 0.24783681583066974, 0.23809495900615596, 0.2896600585319303, 0.27937369678734747, 0.241333535960736, 0.23110213486484502, 0.2596924186215904, 0.2356621315176513, 0.24799019394400662, 0.24641681280579428, 0.23157273308553578, 0.25759970082434297, 0.2578631132904591, 0.2526689180416984, 0.23647088406901806, 0.24036612514489994, 0.2441092120489819, 0.22549829333777893, 0.21332923897696443, 0.21936824504982005, 0.24789722738611325, 0.21658193555754718, 0.24212469713025606, 0.23995857765435305, 0.2332017549934759, 0.22920576478436902, 0.22327387046879685, 0.24072832576001651], 'val_acc': [0.6482896880090214, 0.5623355469239444, 0.5461721588773336, 0.5650920937225912, 0.5604560831975943, 0.5647162009773211, 0.577371256734745, 0.5974188698158126, 0.6153364240070167, 0.6076932715198596, 0.6217266006766069, 0.6441548678110512, 0.6484149855907781, 0.6741009898508958, 0.6719709309610324, 0.7020423505826338, 0.6907655682245333, 0.6957774714948002, 0.699285803783987, 0.6986593158752036, 0.7094349079062774, 0.708808419997494, 0.7282295451697782, 0.7098108006515474, 0.7051747901265506, 0.7139456208495176, 0.7204610951008645, 0.7170780603934344, 0.7187069289562712, 0.7163262749028944, 0.7113143716326275, 0.7183310362110011, 0.7321137702042351, 0.7485277534143591, 0.7322390677859917, 0.7363738879839619, 0.7438917428893622, 0.7413857912542288, 0.7428893622353089, 0.7640646535521864, 0.7690765568224533, 0.7583009647913795, 0.7521613832853026, 0.7577997744643529, 0.7705801278035334, 0.777596792381907, 0.7697030447312367, 0.7713319132940735, 0.7794762561082571, 0.7983961909535146, 0.7788497681994737, 0.7754667334920436, 0.7836110763062273, 0.7981455957900012, 0.8019045232427015, 0.8027816063149982, 0.8019045232427015, 0.8059140458589149, 0.7998997619345947, 0.8044104748778349, 0.8121789249467485, 0.7992732740258113, 0.8004009522616213, 0.794010775592031, 0.8037839869690515, 0.8026563087332415, 0.8015286304974314, 0.8184438040345822, 0.7863676231048741, 0.7963914296454079, 0.8193208871068789, 0.8258363613582258, 0.7931336925197344, 0.7956396441548678, 0.8199473750156622, 0.8282170154116025, 0.8039092845508081, 0.8209497556697156, 0.8166896378899887, 0.816188447562962, 0.8264628492670092, 0.807417616839995, 0.8065405337676983, 0.8097982708933718, 0.8222027314872823, 0.8224533266507956, 0.8181932088710688, 0.8308482646284927, 0.8461345696028066, 0.8381155243703796, 0.8159378523994487, 0.8399949880967297, 0.8199473750156622, 0.8234557073048491, 0.8293446936474126, 0.831474752537276, 0.8382408219521363, 0.8232051121413356], 'final_model_size_bytes': 149638, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 4.446318222042707e-05, 'batch_size': 128, 'epochs': 98, 'base_channels': 9, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 1, 'ff_multiplier': 2, 'dropout': 0.23651068536573544, 'weight_decay': 1.634298906591329e-05, 'gamma_focal': 2.0554344741333184, 'effective_beta': 0.9672921609935468, 'label_smoothing': 0.09574446382183206, 'use_amp': False, 'use_smote_tomek': True, 'smote_target_min_frac': 0.22574235313994173, 'smote_max_multiplier': 0, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 7}, 'model_parameter_count': 34735, 'model_storage_size_kb': 149.251953125, 'model_size_validation': 'PASS'}
2025-10-03 04:21:26,953 - INFO - _models.training_function_executor - BO Objective: base=0.8232, size_penalty=0.0000, final=0.8232
2025-10-03 04:21:26,953 - INFO - _models.training_function_executor - Model: 34,735 parameters, 149.3KB (PASS 256KB limit)
2025-10-03 04:21:26,953 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 358.572s
2025-10-03 04:21:27,076 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8232
2025-10-03 04:21:27,076 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.123s
2025-10-03 04:21:27,076 - INFO - bo.run_bo - Recorded observation #46: hparams={'lr': 4.446318222042707e-05, 'batch_size': np.int64(128), 'epochs': np.int64(98), 'base_channels': np.int64(9), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.23651068536573544, 'weight_decay': 1.634298906591329e-05, 'gamma_focal': 2.0554344741333184, 'effective_beta': 0.9672921609935468, 'label_smoothing': 0.09574446382183206, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.22574235313994173, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)}, value=0.8232
2025-10-03 04:21:27,077 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 46: {'lr': 4.446318222042707e-05, 'batch_size': np.int64(128), 'epochs': np.int64(98), 'base_channels': np.int64(9), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(2), 'dropout': 0.23651068536573544, 'weight_decay': 1.634298906591329e-05, 'gamma_focal': 2.0554344741333184, 'effective_beta': 0.9672921609935468, 'label_smoothing': 0.09574446382183206, 'use_amp': np.False_, 'use_smote_tomek': np.True_, 'smote_target_min_frac': 0.22574235313994173, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)} -> 0.8232
2025-10-03 04:21:27,077 - INFO - bo.run_bo - ðŸ”BO Trial 47: Using RF surrogate + Expected Improvement
2025-10-03 04:21:27,077 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 04:21:27,077 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:21:27,077 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:21:27,077 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 9.509275119469011e-05, 'batch_size': 512, 'epochs': 18, 'base_channels': 11, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.23219766677376558, 'weight_decay': 2.70205067218285e-05, 'gamma_focal': 2.7481097265202354, 'effective_beta': 0.965895217448211, 'label_smoothing': 0.09350043580450602, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1796782645104228, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 4}
2025-10-03 04:21:27,079 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 9.509275119469011e-05, 'batch_size': 512, 'epochs': 18, 'base_channels': 11, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.23219766677376558, 'weight_decay': 2.70205067218285e-05, 'gamma_focal': 2.7481097265202354, 'effective_beta': 0.965895217448211, 'label_smoothing': 0.09350043580450602, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1796782645104228, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 4}
2025-10-03 04:22:54,798 - INFO - _models.training_function_executor - Model: 101,187 parameters, 217.4KB storage
2025-10-03 04:22:54,798 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.44848784526420693, 0.37232395027457027, 0.3356734527063688, 0.3050566886407004, 0.2836454543951844, 0.26962808458989856, 0.2595864105991256, 0.2506737598739568, 0.2411187456303012, 0.23216309050914993, 0.22433675844967413, 0.21781090111276977, 0.21007993027856803, 0.20431119359800273, 0.1990576159239704, 0.19184690911378963, 0.18568649479135424, 0.17999470300760775], 'val_losses': [0.3814933363237621, 0.3595269107158404, 0.3212515800395954, 0.29833446008238096, 0.27680290880141706, 0.26414785127400187, 0.2548488492734031, 0.2454724681865092, 0.2380944245695963, 0.23929058708219716, 0.23447668813789807, 0.245547631758972, 0.279781485717408, 0.2743833822171023, 0.2809404129410519, 0.29369851912476486, 0.2566268558550592, 0.30122378474114725], 'val_acc': [0.7198346071920811, 0.7227164515724847, 0.7565467986467861, 0.7599298333542163, 0.7613081067535397, 0.7638140583886731, 0.7698283423129934, 0.7812304222528506, 0.794010775592031, 0.7753414359102869, 0.7681994737501566, 0.7387545420373387, 0.6749780729231926, 0.6824959278285929, 0.677108131813056, 0.6598170655306352, 0.7238441298082947, 0.6725974188698158], 'final_model_size_bytes': 214903, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 9.509275119469011e-05, 'batch_size': 512, 'epochs': 18, 'base_channels': 11, 'ds_kernel_size': 9, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.23219766677376558, 'weight_decay': 2.70205067218285e-05, 'gamma_focal': 2.7481097265202354, 'effective_beta': 0.965895217448211, 'label_smoothing': 0.09350043580450602, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.1796782645104228, 'smote_max_multiplier': 0, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'quant_calibrate_batches': 4}, 'model_parameter_count': 101187, 'model_storage_size_kb': 217.39394531250002, 'model_size_validation': 'PASS'}
2025-10-03 04:22:54,798 - INFO - _models.training_function_executor - BO Objective: base=0.6726, size_penalty=0.0000, final=0.6726
2025-10-03 04:22:54,798 - INFO - _models.training_function_executor - Model: 101,187 parameters, 217.4KB (PASS 256KB limit)
2025-10-03 04:22:54,798 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 87.721s
2025-10-03 04:22:54,923 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6726
2025-10-03 04:22:54,923 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.125s
2025-10-03 04:22:54,923 - INFO - bo.run_bo - Recorded observation #47: hparams={'lr': 9.509275119469011e-05, 'batch_size': np.int64(512), 'epochs': np.int64(18), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.23219766677376558, 'weight_decay': 2.70205067218285e-05, 'gamma_focal': 2.7481097265202354, 'effective_beta': 0.965895217448211, 'label_smoothing': 0.09350043580450602, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1796782645104228, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(4)}, value=0.6726
2025-10-03 04:22:54,923 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 47: {'lr': 9.509275119469011e-05, 'batch_size': np.int64(512), 'epochs': np.int64(18), 'base_channels': np.int64(11), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.23219766677376558, 'weight_decay': 2.70205067218285e-05, 'gamma_focal': 2.7481097265202354, 'effective_beta': 0.965895217448211, 'label_smoothing': 0.09350043580450602, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.1796782645104228, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'quant_calibrate_batches': np.int64(4)} -> 0.6726
2025-10-03 04:22:54,924 - INFO - bo.run_bo - ðŸ”BO Trial 48: Using RF surrogate + Expected Improvement
2025-10-03 04:22:54,924 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 04:22:54,924 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:22:54,924 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:22:54,924 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0012329173097969798, 'batch_size': 256, 'epochs': 13, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.2272908495741376, 'weight_decay': 0.0029166508090732123, 'gamma_focal': 2.908002111552559, 'effective_beta': 0.9257160238595051, 'label_smoothing': 0.09940374338781192, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.39376396611921116, 'smote_max_multiplier': 0, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 04:22:54,925 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0012329173097969798, 'batch_size': 256, 'epochs': 13, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.2272908495741376, 'weight_decay': 0.0029166508090732123, 'gamma_focal': 2.908002111552559, 'effective_beta': 0.9257160238595051, 'label_smoothing': 0.09940374338781192, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.39376396611921116, 'smote_max_multiplier': 0, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 04:23:56,567 - INFO - _models.training_function_executor - Model: 26,290 parameters, 113.0KB storage
2025-10-03 04:23:56,567 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2764955515307606, 0.14943902717952093, 0.11116559591752427, 0.0945255993731084, 0.08495917183609641, 0.07921882221676019, 0.07253950493311563, 0.06826046283438504, 0.06653975020296232, 0.06443126861550875, 0.05815294082911788, 0.05921769760687003, 0.055934001529344295], 'val_losses': [0.21011492177748828, 0.14155091633213865, 0.129489222774603, 0.10837974312228975, 0.10384751758305386, 0.09567752158062273, 0.10299113007464694, 0.09625785889694706, 0.09806459002750378, 0.09438403903347645, 0.09004591072864841, 0.09537053474909739, 0.07496010926698565], 'val_acc': [0.7654429269515098, 0.8787119408595414, 0.8927452700162887, 0.9030196717203358, 0.9023931838115524, 0.9149229419872197, 0.9125422879338428, 0.914045858914923, 0.9176794887858665, 0.9273274025811302, 0.9189324646034331, 0.8960030071419621, 0.9407342438290941], 'final_model_size_bytes': 200077, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0012329173097969798, 'batch_size': 256, 'epochs': 13, 'base_channels': 10, 'ds_kernel_size': 5, 'n_ds_blocks': 1, 'd_model': 64, 'head_dim': 16, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.2272908495741376, 'weight_decay': 0.0029166508090732123, 'gamma_focal': 2.908002111552559, 'effective_beta': 0.9257160238595051, 'label_smoothing': 0.09940374338781192, 'use_amp': True, 'use_smote_tomek': False, 'smote_target_min_frac': 0.39376396611921116, 'smote_max_multiplier': 0, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'quant_calibrate_batches': 4}, 'model_parameter_count': 26290, 'model_storage_size_kb': 112.96484375000001, 'model_size_validation': 'PASS'}
2025-10-03 04:23:56,567 - INFO - _models.training_function_executor - BO Objective: base=0.9407, size_penalty=0.0000, final=0.9407
2025-10-03 04:23:56,567 - INFO - _models.training_function_executor - Model: 26,290 parameters, 113.0KB (PASS 256KB limit)
2025-10-03 04:23:56,567 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 61.644s
2025-10-03 04:23:56,692 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9407
2025-10-03 04:23:56,692 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.125s
2025-10-03 04:23:56,692 - INFO - bo.run_bo - Recorded observation #48: hparams={'lr': 0.0012329173097969798, 'batch_size': np.int64(256), 'epochs': np.int64(13), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.2272908495741376, 'weight_decay': 0.0029166508090732123, 'gamma_focal': 2.908002111552559, 'effective_beta': 0.9257160238595051, 'label_smoothing': 0.09940374338781192, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.39376396611921116, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)}, value=0.9407
2025-10-03 04:23:56,692 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 48: {'lr': 0.0012329173097969798, 'batch_size': np.int64(256), 'epochs': np.int64(13), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(5), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(64), 'head_dim': np.int64(16), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.2272908495741376, 'weight_decay': 0.0029166508090732123, 'gamma_focal': 2.908002111552559, 'effective_beta': 0.9257160238595051, 'label_smoothing': 0.09940374338781192, 'use_amp': np.True_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.39376396611921116, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)} -> 0.9407
2025-10-03 04:23:56,693 - INFO - bo.run_bo - ðŸ”BO Trial 49: Using RF surrogate + Expected Improvement
2025-10-03 04:23:56,693 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 04:23:56,693 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:23:56,693 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:23:56,693 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007068954345007392, 'batch_size': 256, 'epochs': 11, 'base_channels': 7, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.0581458939422186, 'weight_decay': 0.008478291746356701, 'gamma_focal': 1.2257420996426376, 'effective_beta': 0.9167981795634261, 'label_smoothing': 0.09788701344248653, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.21580231349842005, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 04:23:56,694 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007068954345007392, 'batch_size': 256, 'epochs': 11, 'base_channels': 7, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.0581458939422186, 'weight_decay': 0.008478291746356701, 'gamma_focal': 1.2257420996426376, 'effective_beta': 0.9167981795634261, 'label_smoothing': 0.09788701344248653, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.21580231349842005, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}
2025-10-03 04:25:25,975 - INFO - _models.training_function_executor - Model: 68,017 parameters, 146.1KB storage
2025-10-03 04:25:25,975 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5437043954018487, 0.3661886794381216, 0.25980788245351377, 0.19772194381097846, 0.16782787036244867, 0.14921112621416444, 0.13685941004088972, 0.13006002389783083, 0.12103483687357038, 0.11273090800661545, 0.11069508989969411], 'val_losses': [0.43348786029290204, 0.28466783537704565, 0.21740616850537206, 0.1732096728649284, 0.1635885327819176, 0.14525220007387682, 0.13761352858956694, 0.13206206485134306, 0.11973779166742549, 0.12409666546184815, 0.11585462365853519], 'val_acc': [0.7868688134319007, 0.8666833730109009, 0.907906277408846, 0.9253226412730234, 0.9278285929081569, 0.9408595414108508, 0.939230672848014, 0.9427390051372009, 0.9500062648790878, 0.9468738253351711, 0.9495050745520611], 'final_model_size_bytes': 150818, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007068954345007392, 'batch_size': 256, 'epochs': 11, 'base_channels': 7, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 8, 'num_transformer_layers': 2, 'ff_multiplier': 2, 'dropout': 0.0581458939422186, 'weight_decay': 0.008478291746356701, 'gamma_focal': 1.2257420996426376, 'effective_beta': 0.9167981795634261, 'label_smoothing': 0.09788701344248653, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.21580231349842005, 'smote_max_multiplier': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 4}, 'model_parameter_count': 68017, 'model_storage_size_kb': 146.13027343750002, 'model_size_validation': 'PASS'}
2025-10-03 04:25:25,975 - INFO - _models.training_function_executor - BO Objective: base=0.9495, size_penalty=0.0000, final=0.9495
2025-10-03 04:25:25,975 - INFO - _models.training_function_executor - Model: 68,017 parameters, 146.1KB (PASS 256KB limit)
2025-10-03 04:25:25,976 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 89.283s
2025-10-03 04:25:26,101 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9495
2025-10-03 04:25:26,101 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.125s
2025-10-03 04:25:26,101 - INFO - bo.run_bo - Recorded observation #49: hparams={'lr': 0.0007068954345007392, 'batch_size': np.int64(256), 'epochs': np.int64(11), 'base_channels': np.int64(7), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.0581458939422186, 'weight_decay': 0.008478291746356701, 'gamma_focal': 1.2257420996426376, 'effective_beta': 0.9167981795634261, 'label_smoothing': 0.09788701344248653, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.21580231349842005, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)}, value=0.9495
2025-10-03 04:25:26,101 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 49: {'lr': 0.0007068954345007392, 'batch_size': np.int64(256), 'epochs': np.int64(11), 'base_channels': np.int64(7), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(2), 'dropout': 0.0581458939422186, 'weight_decay': 0.008478291746356701, 'gamma_focal': 1.2257420996426376, 'effective_beta': 0.9167981795634261, 'label_smoothing': 0.09788701344248653, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.21580231349842005, 'smote_max_multiplier': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(4)} -> 0.9495
2025-10-03 04:25:26,101 - INFO - bo.run_bo - ðŸ”BO Trial 50: Using RF surrogate + Expected Improvement
2025-10-03 04:25:26,101 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 04:25:26,102 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:25:26,102 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:25:26,102 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0039036847853241596, 'batch_size': 256, 'epochs': 75, 'base_channels': 10, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.2322855877764513, 'weight_decay': 0.0013388638340214366, 'gamma_focal': 2.343994847271148, 'effective_beta': 0.94695820426607, 'label_smoothing': 0.08951255256942843, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.3112877842578682, 'smote_max_multiplier': 0, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 04:25:26,103 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0039036847853241596, 'batch_size': 256, 'epochs': 75, 'base_channels': 10, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.2322855877764513, 'weight_decay': 0.0013388638340214366, 'gamma_focal': 2.343994847271148, 'effective_beta': 0.94695820426607, 'label_smoothing': 0.08951255256942843, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.3112877842578682, 'smote_max_multiplier': 0, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 7}
2025-10-03 04:34:16,950 - INFO - _models.training_function_executor - Model: 3,734 parameters, 4.0KB storage
2025-10-03 04:34:16,950 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3573451354967578, 0.21894069057290194, 0.18294161230748607, 0.15535697415090646, 0.14698852680214394, 0.1378168032843883, 0.1292622747241702, 0.1265147918065613, 0.12376682105365151, 0.11749757899099979, 0.11608350605119855, 0.11512998293165581, 0.10730284866382837, 0.10869072475146334, 0.10416206418282294, 0.10043078847001924, 0.10051891268330312, 0.09592024551576919, 0.09838017436284766, 0.09344826281911855, 0.09292148543489728, 0.0899833492103594, 0.08933459067554345, 0.08888460039047805, 0.08625076678596269, 0.08351075459746973, 0.082398131679735, 0.08380297228637482, 0.08150628705947814, 0.08162121531037099, 0.07810083874550446, 0.07795048291957367, 0.0766368592693887, 0.07670577767787935, 0.07684716745602832, 0.07573393692562642, 0.07317312702705092, 0.07168376882960614, 0.07226872024933684, 0.07044632435083031, 0.06886533343525521, 0.07029802422406074, 0.06906447449480647, 0.07133293871427096, 0.06816406796866034, 0.06723666223495918, 0.06693387386206359, 0.06650261068286328, 0.06621224839356192, 0.0658860277137474, 0.06353631592416205, 0.06532364234123351, 0.06452955382630773, 0.06324380563805267, 0.06786653521099836, 0.06187990267512752, 0.062009819886808995, 0.0607094612558735, 0.06085690629017632, 0.060275963334736546, 0.0629683733171997, 0.06001198249347855, 0.059142388912444166, 0.06083526678297695, 0.06030023609657759, 0.05876250760342992, 0.05906517023333679, 0.058069702918299555, 0.058397766785118564, 0.05663721366194605, 0.05870948329192614, 0.05762762210341747, 0.05434570304998773, 0.055983679207876294, 0.05640265490836758], 'val_losses': [0.2614760612141233, 0.23254444002881114, 0.2402829893354395, 0.1932349528656644, 0.18476027736664116, 0.2222696080189602, 0.18687273102466243, 0.22551577355931568, 0.16080126067761893, 0.22265687324606137, 0.1801622478309363, 0.16717698692700392, 0.18000413993854986, 0.20660572151666654, 0.1941819112057555, 0.20720129222137082, 0.1988220526758045, 0.17059694021630295, 0.21748698228038263, 0.17631368816473955, 0.19749773688900965, 0.18356083422178554, 0.16081782013584894, 0.17278647556473417, 0.22216801010954426, 0.22390921385705567, 0.17008077294332702, 0.19713965612662615, 0.19234994881330256, 0.19782955888152318, 0.17251391158622542, 0.18627920532835976, 0.20246376352992188, 0.17216551399644703, 0.19530125345827987, 0.21224053893011924, 0.17542724667030413, 0.19992843218957135, 0.1861744698236467, 0.18019992259305725, 0.14625733998197912, 0.1879675830049841, 0.157629552031352, 0.18226468830445614, 0.22265282056711325, 0.13222271980886516, 0.14316257879431124, 0.18503329907648397, 0.18245787364231764, 0.20339189666977892, 0.18488681490947662, 0.17731846957491418, 0.14488912535376933, 0.18455235600329598, 0.13419816068405943, 0.18612489300127572, 0.20424621906133839, 0.15752090385437847, 0.16259793687391036, 0.1596269530657613, 0.17524299929202045, 0.1384890916715393, 0.1580392957375979, 0.16588474878831969, 0.1408446239709645, 0.14374039727947555, 0.1512690361427224, 0.1650987129909116, 0.13268335537942724, 0.17614058632477828, 0.17227245078411277, 0.15694884167228662, 0.18790175680333907, 0.14943954796289804, 0.18289386617996956], 'val_acc': [0.784488159378524, 0.800902142588648, 0.8155619596541787, 0.8513970680365869, 0.8688134319007643, 0.8562836737250971, 0.8753289061521112, 0.8520235559453703, 0.8985089587770956, 0.870066407718331, 0.876205989224408, 0.884600927202105, 0.883473248966295, 0.8691893246460344, 0.8673098609196842, 0.8721964666081945, 0.8767071795514346, 0.898383661195339, 0.84488159378524, 0.8792131311865681, 0.8864803909284551, 0.9022678862297958, 0.901766695902769, 0.898383661195339, 0.8799649166771081, 0.860919684250094, 0.9001378273399323, 0.8913669966169653, 0.8941235434156121, 0.8798396190953515, 0.9028943741385791, 0.9015161007392557, 0.8812178924946749, 0.9035208620473625, 0.8788372384412981, 0.8856033078561584, 0.8992607442676356, 0.8798396190953515, 0.8927452700162887, 0.8968800902142589, 0.9116652048615461, 0.8866056885102117, 0.9028943741385791, 0.9092845508081694, 0.884225034456835, 0.9272021049993735, 0.9161759178047864, 0.8958777095602055, 0.8924946748527753, 0.8928705675980454, 0.9035208620473625, 0.9060268136824959, 0.9069038967547927, 0.8973812805412855, 0.9273274025811302, 0.8968800902142589, 0.8859792006014284, 0.9124169903520862, 0.9074050870818193, 0.9109134193710061, 0.9026437789750658, 0.9171782984588397, 0.9021425886480391, 0.9007643152487157, 0.9110387169527628, 0.9154241323142464, 0.9109134193710061, 0.9117905024433028, 0.9195589525122165, 0.8944994361608821, 0.9146723468237064, 0.9091592532264128, 0.9011402079939858, 0.9195589525122165, 0.9028943741385791], 'final_model_size_bytes': 34307, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0039036847853241596, 'batch_size': 256, 'epochs': 75, 'base_channels': 10, 'ds_kernel_size': 7, 'n_ds_blocks': 1, 'd_model': 32, 'head_dim': 8, 'num_transformer_layers': 1, 'ff_multiplier': 3, 'dropout': 0.2322855877764513, 'weight_decay': 0.0013388638340214366, 'gamma_focal': 2.343994847271148, 'effective_beta': 0.94695820426607, 'label_smoothing': 0.08951255256942843, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.3112877842578682, 'smote_max_multiplier': 0, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 7}, 'model_parameter_count': 3734, 'model_storage_size_kb': 4.0111328125000005, 'model_size_validation': 'PASS'}
2025-10-03 04:34:16,950 - INFO - _models.training_function_executor - BO Objective: base=0.9029, size_penalty=0.0000, final=0.9029
2025-10-03 04:34:16,950 - INFO - _models.training_function_executor - Model: 3,734 parameters, 4.0KB (PASS 256KB limit)
2025-10-03 04:34:16,950 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 530.848s
2025-10-03 04:34:17,077 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9029
2025-10-03 04:34:17,077 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.127s
2025-10-03 04:34:17,077 - INFO - bo.run_bo - Recorded observation #50: hparams={'lr': 0.0039036847853241596, 'batch_size': np.int64(256), 'epochs': np.int64(75), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.2322855877764513, 'weight_decay': 0.0013388638340214366, 'gamma_focal': 2.343994847271148, 'effective_beta': 0.94695820426607, 'label_smoothing': 0.08951255256942843, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.3112877842578682, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)}, value=0.9029
2025-10-03 04:34:17,077 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 50: {'lr': 0.0039036847853241596, 'batch_size': np.int64(256), 'epochs': np.int64(75), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(7), 'n_ds_blocks': np.int64(1), 'd_model': np.int64(32), 'head_dim': np.int64(8), 'num_transformer_layers': np.int64(1), 'ff_multiplier': np.int64(3), 'dropout': 0.2322855877764513, 'weight_decay': 0.0013388638340214366, 'gamma_focal': 2.343994847271148, 'effective_beta': 0.94695820426607, 'label_smoothing': 0.08951255256942843, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.3112877842578682, 'smote_max_multiplier': np.int64(0), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(7)} -> 0.9029
2025-10-03 04:34:17,077 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.9698
2025-10-03 04:34:17,077 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.0005656578768263601, 'batch_size': np.int64(512), 'epochs': np.int64(65), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(3)}
2025-10-03 04:34:17,077 - INFO - visualization - Generating BO visualization charts with 50 trials...
2025-10-03 04:34:18,520 - INFO - visualization - BO summary saved to: charts/20251003_043417_BO_CATNet1D-TransTiny/bo_summary.txt
2025-10-03 04:34:18,520 - INFO - visualization - BO charts saved to: charts/20251003_043417_BO_CATNet1D-TransTiny
2025-10-03 04:34:18,520 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“Š BO charts saved to: charts/20251003_043417_BO_CATNet1D-TransTiny
2025-10-03 04:34:18,573 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸš€ STEP 4: Final Training Execution
2025-10-03 04:34:18,573 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-10-03 04:34:18,664 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-03 04:34:18,676 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-03 04:34:18,692 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-03 04:34:18,692 - INFO - _models.training_function_executor - Loaded training function: CATNet1D-TransTiny
2025-10-03 04:34:18,692 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-03 04:34:18,692 - INFO - _models.training_function_executor - Loaded training function: CATNet1D-TransTiny
2025-10-03 04:34:18,692 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-03 04:34:18,692 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.0005656578768263601, 'batch_size': np.int64(512), 'epochs': np.int64(65), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(3)}
2025-10-03 04:34:18,692 - INFO - evaluation.code_generation_pipeline_orchestrator - Using test set for final training evaluation
2025-10-03 04:34:18,692 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 04:34:18,714 - INFO - _models.training_function_executor - Executing training function: CATNet1D-TransTiny
2025-10-03 04:34:18,715 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0005656578768263601, 'batch_size': np.int64(512), 'epochs': np.int64(65), 'base_channels': np.int64(10), 'ds_kernel_size': np.int64(9), 'n_ds_blocks': np.int64(2), 'd_model': np.int64(64), 'head_dim': np.int64(32), 'num_transformer_layers': np.int64(2), 'ff_multiplier': np.int64(4), 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': np.False_, 'use_smote_tomek': np.False_, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': np.int64(3), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'quant_calibrate_batches': np.int64(3)}
2025-10-03 04:34:18,716 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0005656578768263601, 'batch_size': 512, 'epochs': 65, 'base_channels': 10, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 3}
2025-10-03 04:39:55,386 - INFO - _models.training_function_executor - Model: 101,329 parameters, 217.7KB storage
2025-10-03 04:39:55,386 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4685397851486252, 0.262559396475434, 0.16925300293286133, 0.1359482836441317, 0.11670834946340815, 0.10590496932924894, 0.09774934155112185, 0.08691183378159666, 0.08168927007134474, 0.07658799371137653, 0.07235722042434962, 0.06842242245829, 0.06545495504771794, 0.0633921829599711, 0.06040464423196547, 0.05687458696681782, 0.05442804814508273, 0.05191349538092434, 0.05433665063014103, 0.048452543971556704, 0.04790822413855299, 0.04606921951724896, 0.04562264106086281, 0.04461940355246413, 0.04356559768988308, 0.04243923805552477, 0.04370135841734331, 0.04003087878705218, 0.03897276086958294, 0.03762397489718848, 0.03700796635291198, 0.03724556081982833, 0.035706726345737745, 0.033612521823851126, 0.03387125364318789, 0.0337206870183026, 0.03155657307580173, 0.030510563851478295, 0.03014481601655579, 0.02956926608615, 0.03219696512384566, 0.02904594478697039, 0.03001466082780956, 0.0285277198731254, 0.027010693139401836, 0.027360538462865513, 0.026054854781845615, 0.025977407536546326, 0.026106802592802354, 0.025491432260659187, 0.026629229646053618, 0.024424128192562816, 0.023914519777340753, 0.022042653388205585, 0.02311011991421031, 0.02325819374481104, 0.023277861456765397, 0.025002472331178312, 0.021549479952253814, 0.020956280423177417, 0.02027229547094324, 0.019700368688574006, 0.02186645420362281, 0.020224238770514272, 0.01851231010620271], 'val_losses': [0.3649356350244148, 0.19984080982938554, 0.14444362275913583, 0.11871891269758589, 0.10607032004748893, 0.09862382093847882, 0.09135873245871275, 0.08664234616319405, 0.08043090693360486, 0.07594542059261465, 0.07154664844358162, 0.07482038560435361, 0.06894526379778126, 0.07192253808316705, 0.0643637170524502, 0.06659264176206939, 0.06272260335993723, 0.06109297615123841, 0.05953604004492728, 0.05952030862463131, 0.05791450749893644, 0.058648118527762805, 0.05738315668287746, 0.05550823734046969, 0.059760644049152566, 0.05379541484526582, 0.06465611167424722, 0.053294155818688994, 0.053914620915850336, 0.05437843978199482, 0.05281352502211813, 0.05825947587436647, 0.049961056340721496, 0.04886118639847508, 0.053269100176005724, 0.04949185966868254, 0.051438652899314326, 0.04985887655545788, 0.05417789268120615, 0.051615618566002885, 0.04976643889959626, 0.05121502230110705, 0.05241890112882809, 0.0524727825170081, 0.05425700270969974, 0.05025996124531251, 0.04773095025285982, 0.05056400759537384, 0.05005548400507925, 0.05822831920896476, 0.0562760375751383, 0.054414013142134954, 0.05224213053997857, 0.04862361558503992, 0.05447737687814686, 0.04918608071152415, 0.053845553595201406, 0.047571553885948685, 0.05380266203445188, 0.050970896067658325, 0.050701572544166, 0.05123707038797656, 0.05290131603237162, 0.04983790627294703, 0.05133537447483134], 'val_acc': [0.7936813407104483, 0.8927110897281694, 0.9284740598187796, 0.9392189880522813, 0.9438697778846925, 0.9484403816855104, 0.9501242883489696, 0.9533317296127015, 0.9571806591291797, 0.9582230775398926, 0.9597466121401652, 0.9579825194451127, 0.9617512629299976, 0.9611097746772512, 0.9647983321305429, 0.9598267981717585, 0.9650388902253227, 0.9665624248255954, 0.965920936572849, 0.9669633549835619, 0.9687274476786144, 0.9676048432363082, 0.9687274476786144, 0.9692887498997674, 0.9664822387940021, 0.9700906102157004, 0.9671237270467484, 0.9692085638681741, 0.9696896800577339, 0.9707320984684468, 0.9705717264052602, 0.9701707962472937, 0.9715339587843798, 0.9740197257637719, 0.9688878197418009, 0.9732980514794323, 0.9730574933846524, 0.9730574933846524, 0.9714537727527864, 0.9723358191003127, 0.973217865447839, 0.9704113543420736, 0.9722556330687194, 0.9718547029107529, 0.9705717264052602, 0.9724961911634993, 0.9741800978269586, 0.9740197257637719, 0.9730574933846524, 0.9724961911634993, 0.9699302381525138, 0.9710528425948199, 0.9741800978269586, 0.9736187956058054, 0.9737791676689921, 0.9724160051319061, 0.9688076337102077, 0.9740999117953653, 0.9735386095742121, 0.9742602838585518, 0.9725763771950926, 0.9728971213214658, 0.9719348889423463, 0.9732980514794323, 0.9755432603640446], 'final_model_size_bytes': 217442, 'model_name': 'CATNet1D-TransTiny', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0005656578768263601, 'batch_size': 512, 'epochs': 65, 'base_channels': 10, 'ds_kernel_size': 9, 'n_ds_blocks': 2, 'd_model': 64, 'head_dim': 32, 'num_transformer_layers': 2, 'ff_multiplier': 4, 'dropout': 0.016186003247183004, 'weight_decay': 1.8729002784383675e-05, 'gamma_focal': 1.8127093680882647, 'effective_beta': 0.9881155853593956, 'label_smoothing': 0.09952206042048854, 'use_amp': False, 'use_smote_tomek': False, 'smote_target_min_frac': 0.18215786857000849, 'smote_max_multiplier': 3, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'quant_calibrate_batches': 3}, 'model_parameter_count': 101329, 'model_storage_size_kb': 217.69902343750002, 'model_size_validation': 'PASS'}
2025-10-03 04:39:55,386 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final test metrics from training (avoids preprocessing mismatch)
2025-10-03 04:39:55,386 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 0.9755432603640446, 'macro_f1': None}
2025-10-03 04:39:55,545 - INFO - evaluation.code_generation_pipeline_orchestrator - Model and test tensors saved to: trained_models/20251003_043418_CATNet1D-TransTiny
2025-10-03 04:39:55,551 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“Š STEP 5: Performance Analysis
2025-10-03 04:39:55,551 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: CATNet1D-TransTiny
2025-10-03 04:39:55,551 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.9698
2025-10-03 04:39:55,551 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: 0.9755
2025-10-03 04:39:55,551 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-10-03 04:39:55,551 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: CATNet1D-TransTiny
2025-10-03 04:39:55,551 - INFO - evaluation.code_generation_pipeline_orchestrator - Score: 0.9755
2025-10-03 04:39:55,551 - INFO - __main__ - AI-enhanced training completed!
2025-10-03 04:39:55,551 - INFO - __main__ - Final model achieved: {'acc': 0.9755432603640446, 'macro_f1': None}
2025-10-03 04:39:55,551 - INFO - __main__ - Pipeline completed successfully in single attempt
2025-10-03 04:39:55,551 - INFO - __main__ - Pipeline completed: CATNet1D-TransTiny, metrics: {'acc': 0.9755432603640446, 'macro_f1': None}
2025-10-03 04:39:55,552 - INFO - __main__ - Pipeline summary saved to charts/pipeline_summary_20251003_043955.json
2025-10-03 04:39:55,553 - INFO - __main__ - Model saved: trained_models/best_model_CATNet1D-TransTiny_20251003_043955.pth, performance: {'acc': 0.9755432603640446, 'macro_f1': None}
2025-10-03 04:39:55,553 - INFO - __main__ - AI-enhanced processing completed successfully
