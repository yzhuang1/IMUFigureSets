2025-09-20 16:00:06,567 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 16:00:06,910 - INFO - __main__ - Logging system initialized successfully
2025-09-20 16:00:06,911 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-20 16:00:06,911 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-20 16:00:06,912 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-20 16:00:06,912 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-20 16:00:06,914 - INFO - __main__ - Attempting to load: X.npy
2025-09-20 16:00:07,040 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-20 16:00:07,128 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-20 16:00:07,128 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-20 16:00:07,128 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-20 16:00:07,128 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-20 16:00:07,130 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 16:00:07,130 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-20 16:00:07,130 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-20 16:00:07,130 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-20 16:00:07,130 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-20 16:00:07,130 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-20 16:00:07,130 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-20 16:00:07,130 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-20 16:00:07,131 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-20 16:00:07,334 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-20 16:00:07,334 - INFO - class_balancing - Class imbalance analysis:
2025-09-20 16:00:07,334 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-20 16:00:07,334 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-20 16:00:07,334 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-20 16:00:07,334 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-20 16:00:07,334 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-20 16:00:07,334 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-20 16:00:07,334 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-20 16:00:07,334 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-20 16:00:08,025 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-20 16:00:08,034 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-20 16:00:08,034 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-20 16:00:08,034 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-20 16:00:08,034 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-20 16:00:08,034 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-20 16:00:08,034 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-20 16:00:08,034 - INFO - _models.ai_code_generator - Prompt length: 2552 characters
2025-09-20 16:00:08,034 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-20 16:00:08,034 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-20 16:00:08,034 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-20 16:01:10,785 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-20 16:01:10,868 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-20 16:01:10,868 - INFO - _models.ai_code_generator - AI generated training function: MITBIH_MLP_Classifier_PTQ
2025-09-20 16:01:10,868 - INFO - _models.ai_code_generator - Confidence: 0.90
2025-09-20 16:01:10,868 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: MITBIH_MLP_Classifier_PTQ
2025-09-20 16:01:10,868 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'hidden_size1', 'hidden_size2', 'dropout', 'weight_decay', 'step_size', 'gamma', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'n_calibration_batches']
2025-09-20 16:01:10,868 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.90
2025-09-20 16:01:10,871 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-20 16:01:10,873 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions\training_function_torch_tensor_MITBIH_MLP_Classifier_PTQ_1758402070.json
2025-09-20 16:01:10,873 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions\training_function_torch_tensor_MITBIH_MLP_Classifier_PTQ_1758402070.json
2025-09-20 16:01:10,873 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-20 16:01:10,873 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: MITBIH_MLP_Classifier_PTQ
2025-09-20 16:01:10,873 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-20 16:01:10,874 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-20 16:01:10,875 - WARNING - package_installer - Could not parse code for imports due to syntax error: unexpected character after line continuation character (<unknown>, line 1)
2025-09-20 16:01:10,875 - INFO - package_installer - Extracted imports from code: set()
2025-09-20 16:01:10,875 - INFO - package_installer - ‚úÖ No external packages required
2025-09-20 16:01:10,875 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-20 16:01:10,955 - INFO - data_splitting - Created BO subset: 5000 samples
2025-09-20 16:01:10,955 - INFO - data_splitting - BO subset class distribution: [3600  766   96  100  438]
2025-09-20 16:01:10,955 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 5000 samples (using bo_sample_num=5000)
2025-09-20 16:01:10,955 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'hidden_size1', 'hidden_size2', 'dropout', 'weight_decay', 'step_size', 'gamma', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'n_calibration_batches']
2025-09-20 16:01:10,956 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 16:01:11,038 - INFO - data_splitting - Created BO subset: 5000 samples
2025-09-20 16:01:11,038 - INFO - data_splitting - BO subset class distribution: [3600  766   96  100  438]
2025-09-20 16:01:11,038 - INFO - _models.training_function_executor - Using BO subset for optimization: 5000 samples (bo_sample_num=5000)
2025-09-20 16:01:11,049 - INFO - _models.training_function_executor - BO splits - Train: 4000, Val: 1000
2025-09-20 16:01:11,335 - INFO - bo.run_bo - Converted GPT search space: 14 parameters
2025-09-20 16:01:11,335 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-20 16:01:11,336 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-20 16:01:11,336 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-20 16:01:11,336 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-20 16:01:11,337 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 16:01:11,337 - INFO - _models.training_function_executor - Executing training function: MITBIH_MLP_Classifier_PTQ
2025-09-20 16:01:11,337 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': 76, 'hidden_size1': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': 12, 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': 88}
2025-09-20 16:01:11,338 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': 76, 'hidden_size1': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': 12, 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': 88}
2025-09-20 16:01:21,910 - INFO - _models.training_function_executor - Model parameter count: 0
2025-09-20 16:01:21,910 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0900427918434143, 0.8841230268478394, 0.8192270574569702, 0.7424585099220276, 0.7375229744911194, 0.7530126667022705, 0.674357210636139, 0.6840281274318695, 0.7037222514152527, 0.6870566811561585, 0.6686097445487976, 0.6246203808784485, 0.6032735900878906, 0.5594393668174744, 0.5229392852783203, 0.513851175904274, 0.4904493682384491, 0.47401564693450926, 0.4709240484237671, 0.4800464735031128, 0.4716159794330597, 0.4561862509250641, 0.4349824812412262, 0.454972736120224, 0.4285411386489868, 0.4010232272148132, 0.38753927516937253, 0.383709018945694, 0.3800474896430969, 0.37435753631591795, 0.37228647339344023, 0.35890084981918335, 0.34238055181503296, 0.3364320571422577, 0.3350648416876793, 0.3141764358282089, 0.3111643397808075, 0.3115444085597992, 0.3073052866458893, 0.29785026621818544, 0.307002007484436, 0.2940326759815216, 0.2934143384695053, 0.29462033247947694, 0.2924756933450699, 0.2829719969034195, 0.2785914877653122, 0.2768968212604523, 0.26863659834861753, 0.2711626569032669, 0.26980696737766263, 0.25936450219154356, 0.25930681836605074, 0.26215268099308014, 0.25901078844070435, 0.26122137713432314, 0.254492787361145, 0.26081893730163574, 0.25818938302993777, 0.2559838138818741, 0.2594855773448944, 0.24365000140666962, 0.2533256208896637, 0.24145839220285414, 0.2495315045118332, 0.25109984022378923, 0.25036384344100954, 0.24473933494091035, 0.24166409242153167, 0.25150476676225664, 0.24386450517177582, 0.24893325531482696, 0.23460643100738526, 0.24133326226472854, 0.2537126238942146, 0.2414288730621338], 'val_losses': [0.8372917852401733, 0.7903840093612671, 0.7508041934967041, 0.7115057373046875, 0.7293452877998352, 0.7038788905143738, 0.748345627784729, 0.7446442914009094, 0.7539808874130249, 0.7313270492553711, 0.7124885382652283, 0.7097455215454102, 0.7081574621200561, 0.696741470336914, 0.6870060887336731, 0.7176199116706848, 0.705641429901123, 0.7952097592353821, 0.7512475395202637, 0.8268262372016907, 0.8578092288970948, 0.8045783576965332, 0.8205735864639282, 0.7718133358955384, 0.8155861678123474, 0.8920098791122436, 0.8914171018600464, 0.8791445417404175, 0.8671891913414002, 0.9062003784179687, 0.948139349937439, 0.9486617588996887, 0.900393714427948, 0.956202094078064, 0.9525695896148682, 1.0500172100067138, 1.0841395559310913, 1.0904534239768982, 1.069377329826355, 1.1312814855575561, 1.0915221500396728, 1.1260857305526732, 1.1107443389892577, 1.1080889730453491, 1.0846966705322265, 1.179589726448059, 1.1280229148864747, 1.1280467596054078, 1.1714192690849303, 1.1872001276016235, 1.171348388671875, 1.1810557975769043, 1.1696762657165527, 1.1816050682067871, 1.1893007044792174, 1.1879011383056641, 1.2029788780212403, 1.1837098207473755, 1.1949458856582642, 1.21378990983963, 1.1912374477386474, 1.210235026359558, 1.227955379486084, 1.2524073696136475, 1.245672282218933, 1.2436591444015503, 1.2226242861747743, 1.2273655824661256, 1.244237868309021, 1.2635298380851745, 1.2692687954902648, 1.294592308998108, 1.2866905522346497, 1.2795367312431336, 1.2618727116584778, 1.2619208607673644], 'val_acc': [0.752, 0.762, 0.769, 0.778, 0.78, 0.779, 0.775, 0.781, 0.766, 0.782, 0.797, 0.793, 0.8, 0.804, 0.805, 0.804, 0.815, 0.798, 0.809, 0.804, 0.806, 0.812, 0.811, 0.81, 0.81, 0.807, 0.805, 0.81, 0.822, 0.827, 0.816, 0.818, 0.815, 0.815, 0.822, 0.815, 0.822, 0.821, 0.825, 0.826, 0.821, 0.824, 0.822, 0.823, 0.82, 0.823, 0.826, 0.826, 0.828, 0.824, 0.825, 0.827, 0.83, 0.822, 0.826, 0.825, 0.829, 0.832, 0.832, 0.829, 0.831, 0.831, 0.826, 0.826, 0.823, 0.825, 0.829, 0.828, 0.83, 0.827, 0.827, 0.828, 0.827, 0.823, 0.822, 0.821], 'model_name': 'MITBIH_MLP_Classifier_PTQ', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': 76, 'hidden_size1': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': 12, 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': 88}, 'model_parameter_count': 0, 'model_size_validation': 'PASS'}
2025-09-20 16:01:21,910 - INFO - _models.training_function_executor - BO Objective: base=0.8210, size_penalty=0.0000, final=0.8210
2025-09-20 16:01:21,910 - INFO - _models.training_function_executor - Model size: 0 parameters (PASS 256K limit)
2025-09-20 16:01:21,910 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 10.573s
2025-09-20 16:01:21,910 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8210
2025-09-20 16:01:21,911 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 16:01:21,911 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(76), 'hidden_size1': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': np.int64(12), 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': np.int64(88)}, value=0.8210
2025-09-20 16:01:21,911 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(76), 'hidden_size1': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': np.int64(12), 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': np.int64(88)} -> 0.8210
2025-09-20 16:01:21,912 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-09-20 16:01:21,912 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 16:01:21,912 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 16:01:21,912 - INFO - _models.training_function_executor - Executing training function: MITBIH_MLP_Classifier_PTQ
2025-09-20 16:01:21,912 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.014528246637516064, 'batch_size': 64, 'epochs': 68, 'hidden_size1': 91, 'hidden_size2': 36, 'dropout': 0.3087407548138583, 'weight_decay': 4.676478725076043e-05, 'step_size': 30, 'gamma': 0.48443106659148305, 'grad_clip': 1.45614570099021, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'n_calibration_batches': 15}
2025-09-20 16:01:21,913 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.014528246637516064, 'batch_size': 64, 'epochs': 68, 'hidden_size1': 91, 'hidden_size2': 36, 'dropout': 0.3087407548138583, 'weight_decay': 4.676478725076043e-05, 'step_size': 30, 'gamma': 0.48443106659148305, 'grad_clip': 1.45614570099021, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'n_calibration_batches': 15}
2025-09-20 16:01:29,792 - INFO - _models.training_function_executor - Model parameter count: 0
2025-09-20 16:01:29,792 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.470433102607727, 1.0339046831130982, 0.9914230241775512, 0.9124551801681519, 0.9092579431533814, 0.8917678937911987, 0.8924446558952331, 0.8924981937408447, 0.8922464032173156, 0.8919521274566651, 0.8924433040618897, 0.8927633504867554, 0.8923199162483215, 0.8921273493766785, 0.8916066827774047, 0.9955088157653809, 0.9298199310302734, 0.888628698348999, 0.8957532730102539, 0.8922878789901734, 0.8923079404830933, 0.8923771681785584, 0.8927462196350098, 0.9110950193405152, 0.9066101179122925, 0.8920973796844482, 0.8916640019416809, 0.8973974556922912, 0.8921467819213867, 0.9001061573028565, 0.8918860273361207, 0.8948177881240845, 0.8917322511672974, 0.8919268112182617, 0.8917418327331543, 0.891851448059082, 0.8917269759178161, 0.8918038902282714, 0.8917418375015259, 0.8917214212417602, 0.8916770973205567, 0.891864004611969, 0.8918399047851563, 0.9053461666107178, 0.8970460548400879, 0.9064280118942261, 0.8949101648330688, 0.8917498769760132, 0.8917671279907227, 0.8918879780769348, 0.891766447544098, 0.8925412421226502, 0.892163146018982, 0.8925443606376648, 0.8921059684753418, 0.8917459111213684, 0.8898100285530091, 0.8920417833328247, 0.8954008393287659, 0.8918760380744934, 0.8916158771514893, 0.8916245985031128, 0.8915845713615418, 0.8917135028839112, 0.8925871410369873, 0.8912326707839966, 0.8931828341484069, 0.8917733583450317], 'val_losses': [0.9721522331237793, 0.8736673545837402, 0.905918885231018, 0.8951601781845093, 0.8914027829170227, 0.8919396162033081, 0.8912637910842895, 0.891278956413269, 0.8912002859115601, 0.891716579914093, 0.8911882739067077, 0.8918621602058411, 0.8912830767631531, 0.8914616818428039, 0.8818499298095703, 0.8920241656303406, 0.8912057428359985, 0.873922791481018, 0.8913070154190064, 0.8914970817565918, 0.8916745843887329, 0.8911952548027039, 0.8916573481559753, 0.8914020204544068, 0.8912439956665039, 0.891231816291809, 0.9481118316650391, 0.8913942003250122, 0.8912940211296082, 0.8912300066947937, 0.891219337940216, 0.8913090739250183, 0.8912829599380493, 0.8911905364990235, 0.8911748356819152, 0.8911826944351197, 0.8911898584365845, 0.8911906480789185, 0.891181601524353, 0.8911838374137878, 0.8911863331794738, 0.8912310528755188, 0.891207335472107, 0.8609994311332703, 0.8912020754814148, 0.8912285809516907, 0.891227740764618, 0.8912318158149719, 0.8912210955619811, 0.8912016515731811, 0.8912350282669067, 0.8887584075927735, 0.890390410900116, 0.891205352306366, 0.8911958913803101, 0.8911873316764831, 0.8678692789077759, 0.898807101726532, 0.8912096786499023, 0.8911789388656616, 0.8911767497062683, 0.8911885447502136, 0.891184847831726, 0.8911761350631714, 0.8911808204650878, 0.8911962513923645, 0.891222918510437, 0.8911913175582886], 'val_acc': [0.734, 0.731, 0.719, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.712, 0.72, 0.72, 0.727, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.719, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.731, 0.717, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72], 'model_name': 'MITBIH_MLP_Classifier_PTQ', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.014528246637516064, 'batch_size': 64, 'epochs': 68, 'hidden_size1': 91, 'hidden_size2': 36, 'dropout': 0.3087407548138583, 'weight_decay': 4.676478725076043e-05, 'step_size': 30, 'gamma': 0.48443106659148305, 'grad_clip': 1.45614570099021, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'n_calibration_batches': 15}, 'model_parameter_count': 0, 'model_size_validation': 'PASS'}
2025-09-20 16:01:29,792 - INFO - _models.training_function_executor - BO Objective: base=0.7200, size_penalty=0.0000, final=0.7200
2025-09-20 16:01:29,792 - INFO - _models.training_function_executor - Model size: 0 parameters (PASS 256K limit)
2025-09-20 16:01:29,793 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 7.881s
2025-09-20 16:01:29,793 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7200
2025-09-20 16:01:29,793 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 16:01:29,793 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 0.014528246637516064, 'batch_size': 64, 'epochs': np.int64(68), 'hidden_size1': np.int64(91), 'hidden_size2': np.int64(36), 'dropout': 0.3087407548138583, 'weight_decay': 4.676478725076043e-05, 'step_size': np.int64(30), 'gamma': 0.48443106659148305, 'grad_clip': 1.45614570099021, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'n_calibration_batches': np.int64(15)}, value=0.7200
2025-09-20 16:01:29,793 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 0.014528246637516064, 'batch_size': 64, 'epochs': np.int64(68), 'hidden_size1': np.int64(91), 'hidden_size2': np.int64(36), 'dropout': 0.3087407548138583, 'weight_decay': 4.676478725076043e-05, 'step_size': np.int64(30), 'gamma': 0.48443106659148305, 'grad_clip': 1.45614570099021, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'n_calibration_batches': np.int64(15)} -> 0.7200
2025-09-20 16:01:29,793 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-09-20 16:01:29,794 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 16:01:29,794 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 16:01:29,794 - INFO - _models.training_function_executor - Executing training function: MITBIH_MLP_Classifier_PTQ
2025-09-20 16:01:29,794 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00019069966103000437, 'batch_size': 512, 'epochs': 55, 'hidden_size1': 86, 'hidden_size2': 18, 'dropout': 0.4299702033681604, 'weight_decay': 0.00012073834860996068, 'step_size': 10, 'gamma': 0.1578959177568988, 'grad_clip': 4.744427686266667, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': 53}
2025-09-20 16:01:29,795 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00019069966103000437, 'batch_size': 512, 'epochs': 55, 'hidden_size1': 86, 'hidden_size2': 18, 'dropout': 0.4299702033681604, 'weight_decay': 0.00012073834860996068, 'step_size': 10, 'gamma': 0.1578959177568988, 'grad_clip': 4.744427686266667, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': 53}
2025-09-20 16:01:31,897 - INFO - _models.training_function_executor - Model parameter count: 173,747
2025-09-20 16:01:31,897 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.7309035377502442, 1.5587699947357179, 1.4431660137176514, 1.372972713470459, 1.3260898914337158, 1.2982290363311768, 1.2570500831604003, 1.2117815847396851, 1.184682053565979, 1.167987657546997, 1.1663070735931396, 1.1398279199600219, 1.1492954483032227, 1.1408072681427002, 1.1467113971710206, 1.1442442302703857, 1.1297273263931273, 1.1528156452178955, 1.1147600231170653, 1.1164308652877808, 1.1197364025115968, 1.126054783821106, 1.1121823749542237, 1.1306712970733643, 1.1115858001708985, 1.1199120845794677, 1.1239347896575929, 1.117163851737976, 1.1142851362228394, 1.1073039970397949, 1.1083427829742432, 1.1379333248138428, 1.1193584089279174, 1.0943695306777954, 1.1065744915008544, 1.1428751020431518, 1.1377420701980592, 1.125461220741272, 1.1018963146209717, 1.1211294736862183, 1.1201523532867432, 1.1237534370422364, 1.1227412414550781, 1.1234348859786987, 1.1177907829284668, 1.1210918531417846, 1.1229755506515502, 1.123489501953125, 1.1299597454071044, 1.111012951850891, 1.1045896549224854, 1.1288698444366454, 1.1306366596221924, 1.1200076961517333, 1.1271867847442627], 'val_losses': [1.5691559791564942, 1.3743457918167115, 1.2418351230621338, 1.1588995370864867, 1.1091878204345702, 1.077574137687683, 1.045047152519226, 1.0071344928741455, 0.9853234009742737, 0.9698192744255066, 0.9680277605056763, 0.9663567194938659, 0.9647583136558533, 0.9631197504997253, 0.9614745411872864, 0.9598151245117188, 0.9582644882202148, 0.9568506360054017, 0.9549316983222962, 0.9530822386741639, 0.9528454842567444, 0.9526130585670471, 0.952351683139801, 0.9521377696990967, 0.9519400691986084, 0.9517218413352966, 0.9515166144371032, 0.9512843341827393, 0.9510487270355225, 0.9507641530036927, 0.9507199144363403, 0.9506823539733886, 0.9506510643959045, 0.9506042137145996, 0.9505530624389649, 0.950512035369873, 0.9504825201034546, 0.9504521398544311, 0.9504096651077271, 0.9503711447715759, 0.9503657584190368, 0.9503600406646728, 0.9503551659584045, 0.9503508319854737, 0.95034503698349, 0.9503401055335998, 0.9503347277641296, 0.9503282046318055, 0.9503225507736206, 0.9503172488212586, 0.9503161816596984, 0.9503154039382935, 0.9503146929740905, 0.9503137435913086, 0.9503130879402161], 'val_acc': [0.338, 0.588, 0.657, 0.671, 0.679, 0.688, 0.69, 0.695, 0.699, 0.706, 0.708, 0.709, 0.709, 0.709, 0.709, 0.709, 0.71, 0.71, 0.71, 0.71, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712, 0.712], 'model_name': 'MITBIH_MLP_Classifier_PTQ', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00019069966103000437, 'batch_size': 512, 'epochs': 55, 'hidden_size1': 86, 'hidden_size2': 18, 'dropout': 0.4299702033681604, 'weight_decay': 0.00012073834860996068, 'step_size': 10, 'gamma': 0.1578959177568988, 'grad_clip': 4.744427686266667, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': 53}, 'model_parameter_count': 173747, 'model_size_validation': 'PASS'}
2025-09-20 16:01:31,897 - INFO - _models.training_function_executor - BO Objective: base=0.7120, size_penalty=0.0000, final=0.7120
2025-09-20 16:01:31,897 - INFO - _models.training_function_executor - Model size: 173,747 parameters (PASS 256K limit)
2025-09-20 16:01:31,897 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 2.103s
2025-09-20 16:01:32,038 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7120
2025-09-20 16:01:32,039 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.142s
2025-09-20 16:01:32,039 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.00019069966103000437, 'batch_size': 512, 'epochs': np.int64(55), 'hidden_size1': np.int64(86), 'hidden_size2': np.int64(18), 'dropout': 0.4299702033681604, 'weight_decay': 0.00012073834860996068, 'step_size': np.int64(10), 'gamma': 0.1578959177568988, 'grad_clip': 4.744427686266667, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': np.int64(53)}, value=0.7120
2025-09-20 16:01:32,039 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.00019069966103000437, 'batch_size': 512, 'epochs': np.int64(55), 'hidden_size1': np.int64(86), 'hidden_size2': np.int64(18), 'dropout': 0.4299702033681604, 'weight_decay': 0.00012073834860996068, 'step_size': np.int64(10), 'gamma': 0.1578959177568988, 'grad_clip': 4.744427686266667, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': np.int64(53)} -> 0.7120
2025-09-20 16:01:32,039 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.8210
2025-09-20 16:01:32,039 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(76), 'hidden_size1': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': np.int64(12), 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': np.int64(88)}
2025-09-20 16:01:32,039 - INFO - visualization - Generating BO visualization charts with 3 trials...
2025-09-20 16:01:34,012 - INFO - visualization - BO summary saved to: charts\BO_MITBIH_MLP_Classifier_PTQ_20250920_160132\bo_summary.txt
2025-09-20 16:01:34,012 - INFO - visualization - BO charts saved to: charts\BO_MITBIH_MLP_Classifier_PTQ_20250920_160132
2025-09-20 16:01:34,013 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä BO charts saved to: charts\BO_MITBIH_MLP_Classifier_PTQ_20250920_160132
2025-09-20 16:01:34,031 - INFO - evaluation.code_generation_pipeline_orchestrator - üöÄ STEP 4: Final Training Execution
2025-09-20 16:01:34,031 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-20 16:01:34,199 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 16:01:34,206 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 16:01:34,217 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 16:01:34,227 - INFO - _models.training_function_executor - Loaded training function: MITBIH_MLP_Classifier_PTQ
2025-09-20 16:01:34,227 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-20 16:01:34,227 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(76), 'hidden_size1': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': np.int64(12), 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': np.int64(88)}
2025-09-20 16:01:34,227 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 16:01:34,270 - INFO - _models.training_function_executor - Executing training function: MITBIH_MLP_Classifier_PTQ
2025-09-20 16:01:34,270 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(76), 'hidden_size1': np.int64(92), 'hidden_size2': np.int64(36), 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': np.int64(12), 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': np.int64(88)}
2025-09-20 16:01:34,272 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': 76, 'hidden_size1': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': 12, 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': 88}
2025-09-20 16:02:58,342 - INFO - _models.training_function_executor - Model parameter count: 0
2025-09-20 16:02:58,342 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7939580949478945, 0.6706871021577809, 0.640415966319579, 0.6227740959922313, 0.6159537542602398, 0.6048979948976084, 0.5958270903769358, 0.5807965729794888, 0.5880804587672401, 0.5814015060161339, 0.5685103049064122, 0.5622971609007767, 0.523771804743226, 0.5115386926611615, 0.506235111710924, 0.49513651422242116, 0.49394350017942995, 0.49171858481053077, 0.4845741016203246, 0.478885669371751, 0.48596619778188016, 0.4709400992538801, 0.48322037801612544, 0.4684656703577294, 0.4481700575628946, 0.44300318653333065, 0.43764572480055647, 0.43984738115367644, 0.42851607438173883, 0.42346346454803907, 0.4203599273871687, 0.42880176644661755, 0.4212559146840953, 0.4227249864205037, 0.41356705259397303, 0.41258275643911185, 0.4015516250752026, 0.3939262710051625, 0.39110648519723246, 0.38775462944601286, 0.39001270954190204, 0.3903554405512194, 0.3827091579496526, 0.3812225535093734, 0.37921203521125874, 0.3837906544267797, 0.3774903676179092, 0.3740760139350233, 0.37095049189727786, 0.36646830543290165, 0.3644916531723217, 0.36179212459871646, 0.36422260244225346, 0.3579437437421719, 0.3598793414275744, 0.35630204872121407, 0.3546178119366133, 0.35797829413375765, 0.35242606803428295, 0.35418513683720215, 0.35695122172809546, 0.34798643346681724, 0.3455675997156665, 0.34330021833168956, 0.34254173158930123, 0.34430215323124297, 0.3409068359610264, 0.3407712914796476, 0.34291890711667733, 0.34373389725792186, 0.3370206769266029, 0.34343655921261884, 0.34518248351841996, 0.33470120851574847, 0.3360300279179858, 0.3380029016049652], 'val_losses': [0.6753156586318022, 0.6087849210871954, 0.5940127668166144, 0.6356901861723504, 0.5790804347892055, 0.6326740320596067, 0.5785327488212054, 0.5924859771368152, 0.571202600158813, 0.6124411580604889, 0.5550955148286639, 0.6055557606626969, 0.5660350454245515, 0.5371408565698937, 0.5531392099265406, 0.5657260316436487, 0.5830226517654414, 0.5518404150297112, 0.5411972052332656, 0.5642349372302481, 0.5534210027106553, 0.57907132771685, 0.5617060858554301, 0.5544425786419742, 0.5235874172760487, 0.5474947382037597, 0.5339755843355886, 0.5293680785028373, 0.547784434192176, 0.5837987515904735, 0.5716665905251631, 0.5610843695669551, 0.5668744032193822, 0.590326167863369, 0.563469876102387, 0.5964675847210578, 0.5714203381366334, 0.5986087733394841, 0.5788474048938614, 0.5813107644763656, 0.5899001867806755, 0.5866201642392525, 0.6055457291134592, 0.6027950228942328, 0.5995045805698503, 0.6156343785823557, 0.6226651927649861, 0.6131478798055476, 0.599850589535489, 0.6141917344156219, 0.6260210839405893, 0.6089167937664753, 0.572365299534843, 0.6048006539137721, 0.593847324376631, 0.6290834896939306, 0.5862532019394128, 0.5996869783910133, 0.6169594671673175, 0.6122452093948639, 0.5924801590540754, 0.6059368886184248, 0.6115755737201238, 0.6028763851759679, 0.6046727793443367, 0.6002661710086203, 0.6150515832412431, 0.6092658103277058, 0.6171679210212386, 0.6098042616337446, 0.6259568441445261, 0.61922113939715, 0.6111260167854851, 0.6191064608984843, 0.6255173207679516, 0.6267760456567227], 'val_acc': [0.7881126591159667, 0.8231933446927934, 0.835822391500451, 0.7942267214593565, 0.8428385286158164, 0.8052520797835021, 0.8420366843740603, 0.8400320737696703, 0.8476495940663527, 0.8385286158163777, 0.8498546657311817, 0.8470482108850356, 0.8497544352009622, 0.8559687280745715, 0.854665731181718, 0.8520597373960108, 0.8544652701212789, 0.8573719554976446, 0.8594767966322542, 0.8508569710333768, 0.8575724165580836, 0.8541645785306204, 0.8590758745113761, 0.8588754134509372, 0.8625839430690588, 0.8639871704921319, 0.8640874010223514, 0.8637867094316929, 0.8637867094316929, 0.8636864789014734, 0.8623834820086198, 0.8644883231432294, 0.8648892452641075, 0.8630850957201563, 0.8642878620827904, 0.8633857873108148, 0.8650897063245464, 0.8650897063245464, 0.8652901673849854, 0.8664929337476195, 0.8658915505663025, 0.866994086398717, 0.8650897063245464, 0.865189936854766, 0.8670943169289366, 0.8670943169289366, 0.865590858975644, 0.8675954695800341, 0.8676957001102535, 0.8664929337476195, 0.8683973138217901, 0.8675954695800341, 0.8681968527613511, 0.8680966222311316, 0.8687982359426681, 0.8675954695800341, 0.8679963917009121, 0.8681968527613511, 0.8679963917009121, 0.866994086398717, 0.8678961611706926, 0.8688984664728876, 0.8679963917009121, 0.8677959306404731, 0.8684975443520097, 0.8680966222311316, 0.8672947779893756, 0.8677959306404731, 0.8675954695800341, 0.8676957001102535, 0.8687982359426681, 0.8682970832915706, 0.8678961611706926, 0.8686980054124487, 0.8688984664728876, 0.8684975443520097], 'model_name': 'MITBIH_MLP_Classifier_PTQ', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': 76, 'hidden_size1': 92, 'hidden_size2': 36, 'dropout': 0.07800932022121827, 'weight_decay': 8.629132190071852e-08, 'step_size': 12, 'gamma': 0.5087315138496218, 'grad_clip': 1.6685430556951095, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'n_calibration_batches': 88}, 'model_parameter_count': 0, 'model_size_validation': 'PASS'}
2025-09-20 16:02:58,381 - ERROR - __main__ - Unhandled exception: NotImplementedError: Could not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear_dynamic' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

CPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\quantized\cpu\qlinear_dynamic.cpp:791 [kernel]
Meta: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:153 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:497 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:96 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:63 [backend fallback]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:67 [backend fallback]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:75 [backend fallback]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:79 [backend fallback]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:87 [backend fallback]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:71 [backend fallback]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:83 [backend fallback]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:91 [backend fallback]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\autograd\TraceTypeManual.cpp:294 [backend fallback]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\autocast_mode.cpp:321 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\autocast_mode.cpp:463 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:161 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:493 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:165 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:157 [backend fallback]

Traceback (most recent call last):
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 484, in <module>
    processed_real_data = process_real_data()
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 342, in process_real_data
    result = process_data_with_ai_enhanced_evaluation(
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 136, in process_data_with_ai_enhanced_evaluation
    result = train_with_iterative_selection(data, labels, device=device, **kwargs)
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 85, in train_with_iterative_selection
    best_model, pipeline_results = orchestrator.run_complete_pipeline(
  File "D:\_A\GPT_research\ml_pipeline\evaluation\code_generation_pipeline_orchestrator.py", line 98, in run_complete_pipeline
    final_model, training_results = self._execute_final_training(
  File "D:\_A\GPT_research\ml_pipeline\evaluation\code_generation_pipeline_orchestrator.py", line 449, in _execute_final_training
    final_metrics = evaluate_model(trained_model, test_loader, device)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "D:\_A\GPT_research\ml_pipeline\evaluation\evaluate.py", line 28, in evaluate_model
    logits = model(X) if lengths is None else model(X, lengths=lengths)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "<string>", line 33, in forward
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 57, in forward
    Y = torch.ops.quantized.linear_dynamic(
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\_ops.py", line 1116, in __call__
    return self._op(*args, **(kwargs or {}))
NotImplementedError: Could not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear_dynamic' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

CPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\quantized\cpu\qlinear_dynamic.cpp:791 [kernel]
Meta: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:153 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:497 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:96 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:63 [backend fallback]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:67 [backend fallback]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:75 [backend fallback]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:79 [backend fallback]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:87 [backend fallback]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:71 [backend fallback]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:83 [backend fallback]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:91 [backend fallback]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\autograd\TraceTypeManual.cpp:294 [backend fallback]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\autocast_mode.cpp:321 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\autocast_mode.cpp:463 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:161 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:493 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:165 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:157 [backend fallback]


