2025-09-15 02:10:47,845 - INFO - __main__ - Logging system initialized successfully
2025-09-15 02:10:47,848 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'y.npy', 'X.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-15 02:10:47,848 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-15 02:10:47,848 - INFO - __main__ - Attempting to load: y.npy
2025-09-15 02:10:47,849 - INFO - __main__ - Attempting to load: X.npy
2025-09-15 02:10:48,191 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-15 02:10:48,504 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-15 02:10:48,504 - INFO - __main__ - Starting AI-enhanced training with new pipeline flow
2025-09-15 02:10:48,504 - INFO - __main__ - Flow: Template Selection → BO → Evaluation → Feedback Loop
2025-09-15 02:10:48,517 - INFO - __main__ - Data profile: DataProfile(type=numpy_array, shape=(62352, 1000, 2), samples=62352, features=2)
2025-09-15 02:10:48,518 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized with max 4 attempts
2025-09-15 02:10:48,518 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution
2025-09-15 02:10:48,518 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation → JSON Storage → BO → Training Execution → Evaluation
2025-09-15 02:10:48,518 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-15 02:10:48,518 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-15 02:10:48,518 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-15 02:10:48,520 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-15 02:10:49,171 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-15 02:10:49,172 - INFO - class_balancing - Class imbalance analysis:
2025-09-15 02:10:49,172 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-15 02:10:49,172 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-15 02:10:49,172 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-15 02:10:49,172 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-15 02:10:49,173 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-15 02:10:49,173 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-15 02:10:49,173 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-15 02:10:49,174 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-15 02:10:50,502 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-15 02:10:50,504 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-15 02:10:50,506 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-15 02:10:50,507 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE ATTEMPT 1/4
2025-09-15 02:10:50,507 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-15 02:10:50,507 - INFO - evaluation.code_generation_pipeline_orchestrator - 🤖 STEP 1: AI Training Code Generation
2025-09-15 02:10:50,507 - INFO - models.ai_code_generator - Conducting literature review before code generation...
2025-09-15 02:10:50,507 - INFO - models.literature_review - Making GPT-5 literature review call with query: sequence classification time series machine learning multiclass sequence classification 2024 2025 PyTorch implementation state-of-the-art methods
2025-09-15 02:12:27,335 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-15 02:12:27,548 - INFO - models.literature_review - Successfully completed GPT-5 literature review with web search
2025-09-15 02:12:27,549 - INFO - models.literature_review - Literature review completed with confidence: 0.72
2025-09-15 02:12:27,549 - INFO - models.literature_review - Found 5 recommended approaches
2025-09-15 02:12:27,550 - INFO - models.literature_review - Literature review saved to: literature_reviews/literature_review_numpy_array_1757902347.txt
2025-09-15 02:12:27,550 - INFO - models.ai_code_generator - Literature review saved to: literature_reviews/literature_review_numpy_array_1757902347.txt
2025-09-15 02:12:27,550 - WARNING - models.ai_code_generator - AI code generation with literature review failed: 'num_samples', using fallback
2025-09-15 02:12:27,550 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: FallbackLSTM
2025-09-15 02:12:27,550 - INFO - evaluation.code_generation_pipeline_orchestrator - Reasoning: Fallback: LSTM selected for sequence data
2025-09-15 02:12:27,551 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'epochs', 'batch_size', 'hidden_size', 'dropout', 'num_layers']
2025-09-15 02:12:27,551 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.70
2025-09-15 02:12:27,551 - INFO - evaluation.code_generation_pipeline_orchestrator - 💾 STEP 2: Save Training Function to JSON
2025-09-15 02:12:27,551 - INFO - models.ai_code_generator - Training function saved to: generated_training_functions/training_function_numpy_array_FallbackLSTM_1757902347.json
2025-09-15 02:12:27,552 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_numpy_array_FallbackLSTM_1757902347.json
2025-09-15 02:12:27,554 - INFO - models.training_function_executor - Training function validation passed
2025-09-15 02:12:27,554 - INFO - evaluation.code_generation_pipeline_orchestrator - 🔍 STEP 3: Bayesian Optimization
2025-09-15 02:12:27,554 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: FallbackLSTM
2025-09-15 02:12:27,595 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 5000 samples
2025-09-15 02:12:27,595 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'epochs', 'batch_size', 'hidden_size', 'dropout', 'num_layers']
2025-09-15 02:12:27,596 - INFO - models.training_function_executor - GPU available: NVIDIA H100 NVL
2025-09-15 02:12:27,596 - WARNING - models.training_function_executor - Using provided subset instead of centralized splits - this may cause data leakage
2025-09-15 02:12:28,170 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-15 02:12:28,182 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-15 02:12:28,184 - INFO - bo.run_bo - BO Trial 1: Initial random exploration
2025-09-15 02:12:28,185 - INFO - bo.run_bo - [PROFILE] suggest() took 0.002s
2025-09-15 02:12:28,185 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:12:28,185 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:12:28,185 - INFO - models.training_function_executor - Hyperparameters: {'lr': 0.01535224694197351, 'epochs': 17, 'batch_size': 128, 'hidden_size': 204, 'dropout': 0.41779511056254093, 'num_layers': 2}
2025-09-15 02:12:28,187 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 0.01535224694197351, 'epochs': 17, 'batch_size': 128, 'hidden_size': 204, 'dropout': 0.41779511056254093, 'num_layers': 2}
2025-09-15 02:13:40,791 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 0.9050525203347206, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.01535224694197351, 'epochs': 17, 'batch_size': 128, 'hidden_size': 204, 'dropout': 0.41779511056254093, 'num_layers': 2}}
2025-09-15 02:13:40,792 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 72.607s
2025-09-15 02:13:40,794 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:13:40,794 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.001s
2025-09-15 02:13:40,794 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.01535224694197351, 'epochs': np.int64(17), 'batch_size': 128, 'hidden_size': np.int64(204), 'dropout': 0.41779511056254093, 'num_layers': np.int64(2)}, value=0.7180
2025-09-15 02:13:40,794 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.01535224694197351, 'epochs': np.int64(17), 'batch_size': 128, 'hidden_size': np.int64(204), 'dropout': 0.41779511056254093, 'num_layers': np.int64(2)} -> 0.7180
2025-09-15 02:13:40,798 - INFO - bo.run_bo - BO Trial 2: Initial random exploration
2025-09-15 02:13:40,798 - INFO - bo.run_bo - [PROFILE] suggest() took 0.004s
2025-09-15 02:13:40,798 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:13:40,798 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:13:40,798 - INFO - models.training_function_executor - Hyperparameters: {'lr': 4.20705395028794e-05, 'epochs': 13, 'batch_size': 32, 'hidden_size': 388, 'dropout': 0.4207805082202462, 'num_layers': 3}
2025-09-15 02:13:40,801 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 4.20705395028794e-05, 'epochs': 13, 'batch_size': 32, 'hidden_size': 388, 'dropout': 0.4207805082202462, 'num_layers': 3}
2025-09-15 02:19:00,149 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 0.9000931668281555, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 4.20705395028794e-05, 'epochs': 13, 'batch_size': 32, 'hidden_size': 388, 'dropout': 0.4207805082202462, 'num_layers': 3}}
2025-09-15 02:19:00,150 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 319.352s
2025-09-15 02:19:00,151 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:19:00,151 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.001s
2025-09-15 02:19:00,152 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 4.20705395028794e-05, 'epochs': np.int64(13), 'batch_size': 32, 'hidden_size': np.int64(388), 'dropout': 0.4207805082202462, 'num_layers': np.int64(3)}, value=0.7180
2025-09-15 02:19:00,152 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 4.20705395028794e-05, 'epochs': np.int64(13), 'batch_size': 32, 'hidden_size': np.int64(388), 'dropout': 0.4207805082202462, 'num_layers': np.int64(3)} -> 0.7180
2025-09-15 02:19:00,155 - INFO - bo.run_bo - BO Trial 3: Initial random exploration
2025-09-15 02:19:00,155 - INFO - bo.run_bo - [PROFILE] suggest() took 0.003s
2025-09-15 02:19:00,155 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:19:00,155 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:19:00,156 - INFO - models.training_function_executor - Hyperparameters: {'lr': 1.2087541473056955e-05, 'epochs': 4, 'batch_size': 128, 'hidden_size': 429, 'dropout': 0.14863737747479333, 'num_layers': 4}
2025-09-15 02:19:00,158 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 1.2087541473056955e-05, 'epochs': 4, 'batch_size': 128, 'hidden_size': 429, 'dropout': 0.14863737747479333, 'num_layers': 4}
2025-09-15 02:19:45,139 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 1.1864151041954756, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.2087541473056955e-05, 'epochs': 4, 'batch_size': 128, 'hidden_size': 429, 'dropout': 0.14863737747479333, 'num_layers': 4}}
2025-09-15 02:19:45,140 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 44.985s
2025-09-15 02:19:45,559 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:19:45,559 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.419s
2025-09-15 02:19:45,559 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 1.2087541473056955e-05, 'epochs': np.int64(4), 'batch_size': 128, 'hidden_size': np.int64(429), 'dropout': 0.14863737747479333, 'num_layers': np.int64(4)}, value=0.7180
2025-09-15 02:19:45,559 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 1.2087541473056955e-05, 'epochs': np.int64(4), 'batch_size': 128, 'hidden_size': np.int64(429), 'dropout': 0.14863737747479333, 'num_layers': np.int64(4)} -> 0.7180
2025-09-15 02:19:45,560 - INFO - bo.run_bo - BO Trial 4: Using RF surrogate + Expected Improvement
2025-09-15 02:19:45,560 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-15 02:19:45,560 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:19:45,560 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:19:45,560 - INFO - models.training_function_executor - Hyperparameters: {'lr': 5.4152441194025364e-05, 'epochs': 13, 'batch_size': 256, 'hidden_size': 232, 'dropout': 0.42515784524199296, 'num_layers': 3}
2025-09-15 02:19:45,563 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 5.4152441194025364e-05, 'epochs': 13, 'batch_size': 256, 'hidden_size': 232, 'dropout': 0.42515784524199296, 'num_layers': 3}
2025-09-15 02:20:35,269 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 0.8879846632480621, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 5.4152441194025364e-05, 'epochs': 13, 'batch_size': 256, 'hidden_size': 232, 'dropout': 0.42515784524199296, 'num_layers': 3}}
2025-09-15 02:20:35,270 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 49.710s
2025-09-15 02:20:35,683 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:20:35,683 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.414s
2025-09-15 02:20:35,684 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 5.4152441194025364e-05, 'epochs': np.int64(13), 'batch_size': np.int64(256), 'hidden_size': np.int64(232), 'dropout': 0.42515784524199296, 'num_layers': np.int64(3)}, value=0.7180
2025-09-15 02:20:35,684 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 5.4152441194025364e-05, 'epochs': np.int64(13), 'batch_size': np.int64(256), 'hidden_size': np.int64(232), 'dropout': 0.42515784524199296, 'num_layers': np.int64(3)} -> 0.7180
2025-09-15 02:20:35,684 - INFO - bo.run_bo - BO Trial 5: Using RF surrogate + Expected Improvement
2025-09-15 02:20:35,684 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-15 02:20:35,684 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:20:35,685 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:20:35,685 - INFO - models.training_function_executor - Hyperparameters: {'lr': 0.00017967486125283075, 'epochs': 9, 'batch_size': 64, 'hidden_size': 38, 'dropout': 0.4077105489062445, 'num_layers': 4}
2025-09-15 02:20:35,687 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00017967486125283075, 'epochs': 9, 'batch_size': 64, 'hidden_size': 38, 'dropout': 0.4077105489062445, 'num_layers': 4}
2025-09-15 02:20:42,658 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 0.8976124894051325, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00017967486125283075, 'epochs': 9, 'batch_size': 64, 'hidden_size': 38, 'dropout': 0.4077105489062445, 'num_layers': 4}}
2025-09-15 02:20:42,658 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 6.974s
2025-09-15 02:20:43,071 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:20:43,071 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.412s
2025-09-15 02:20:43,071 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 0.00017967486125283075, 'epochs': np.int64(9), 'batch_size': np.int64(64), 'hidden_size': np.int64(38), 'dropout': 0.4077105489062445, 'num_layers': np.int64(4)}, value=0.7180
2025-09-15 02:20:43,071 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 0.00017967486125283075, 'epochs': np.int64(9), 'batch_size': np.int64(64), 'hidden_size': np.int64(38), 'dropout': 0.4077105489062445, 'num_layers': np.int64(4)} -> 0.7180
2025-09-15 02:20:43,072 - INFO - bo.run_bo - BO Trial 6: Using RF surrogate + Expected Improvement
2025-09-15 02:20:43,072 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-15 02:20:43,072 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:20:43,072 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:20:43,072 - INFO - models.training_function_executor - Hyperparameters: {'lr': 0.029054252676174452, 'epochs': 8, 'batch_size': 8, 'hidden_size': 279, 'dropout': 0.6099036726033209, 'num_layers': 3}
2025-09-15 02:20:43,074 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 0.029054252676174452, 'epochs': 8, 'batch_size': 8, 'hidden_size': 279, 'dropout': 0.6099036726033209, 'num_layers': 3}
2025-09-15 02:33:18,232 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 1.3221970355808734, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.029054252676174452, 'epochs': 8, 'batch_size': 8, 'hidden_size': 279, 'dropout': 0.6099036726033209, 'num_layers': 3}}
2025-09-15 02:33:18,232 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 755.161s
2025-09-15 02:33:18,647 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:33:18,648 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.415s
2025-09-15 02:33:18,648 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 0.029054252676174452, 'epochs': np.int64(8), 'batch_size': np.int64(8), 'hidden_size': np.int64(279), 'dropout': 0.6099036726033209, 'num_layers': np.int64(3)}, value=0.7180
2025-09-15 02:33:18,648 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 0.029054252676174452, 'epochs': np.int64(8), 'batch_size': np.int64(8), 'hidden_size': np.int64(279), 'dropout': 0.6099036726033209, 'num_layers': np.int64(3)} -> 0.7180
2025-09-15 02:33:18,648 - INFO - bo.run_bo - BO Trial 7: Using RF surrogate + Expected Improvement
2025-09-15 02:33:18,648 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-15 02:33:18,649 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:33:18,649 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:33:18,649 - INFO - models.training_function_executor - Hyperparameters: {'lr': 0.00016039649319436606, 'epochs': 21, 'batch_size': 16, 'hidden_size': 344, 'dropout': 0.3458688280729451, 'num_layers': 4}
2025-09-15 02:33:18,651 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00016039649319436606, 'epochs': 21, 'batch_size': 16, 'hidden_size': 344, 'dropout': 0.3458688280729451, 'num_layers': 4}
2025-09-15 02:55:32,688 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 0.896402363538742, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00016039649319436606, 'epochs': 21, 'batch_size': 16, 'hidden_size': 344, 'dropout': 0.3458688280729451, 'num_layers': 4}}
2025-09-15 02:55:32,689 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 1334.040s
2025-09-15 02:55:33,101 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:55:33,102 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.412s
2025-09-15 02:55:33,102 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.00016039649319436606, 'epochs': np.int64(21), 'batch_size': np.int64(16), 'hidden_size': np.int64(344), 'dropout': 0.3458688280729451, 'num_layers': np.int64(4)}, value=0.7180
2025-09-15 02:55:33,102 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.00016039649319436606, 'epochs': np.int64(21), 'batch_size': np.int64(16), 'hidden_size': np.int64(344), 'dropout': 0.3458688280729451, 'num_layers': np.int64(4)} -> 0.7180
2025-09-15 02:55:33,102 - INFO - bo.run_bo - BO Trial 8: Using RF surrogate + Expected Improvement
2025-09-15 02:55:33,102 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-15 02:55:33,103 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:55:33,103 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:55:33,103 - INFO - models.training_function_executor - Hyperparameters: {'lr': 5.897972017364043e-05, 'epochs': 7, 'batch_size': 128, 'hidden_size': 108, 'dropout': 0.16612064327417395, 'num_layers': 2}
2025-09-15 02:55:33,105 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 5.897972017364043e-05, 'epochs': 7, 'batch_size': 128, 'hidden_size': 108, 'dropout': 0.16612064327417395, 'num_layers': 2}
2025-09-15 02:55:37,191 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 0.9014743659645319, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 5.897972017364043e-05, 'epochs': 7, 'batch_size': 128, 'hidden_size': 108, 'dropout': 0.16612064327417395, 'num_layers': 2}}
2025-09-15 02:55:37,191 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 4.088s
2025-09-15 02:55:37,593 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:55:37,594 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.402s
2025-09-15 02:55:37,594 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 5.897972017364043e-05, 'epochs': np.int64(7), 'batch_size': np.int64(128), 'hidden_size': np.int64(108), 'dropout': 0.16612064327417395, 'num_layers': np.int64(2)}, value=0.7180
2025-09-15 02:55:37,594 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 5.897972017364043e-05, 'epochs': np.int64(7), 'batch_size': np.int64(128), 'hidden_size': np.int64(108), 'dropout': 0.16612064327417395, 'num_layers': np.int64(2)} -> 0.7180
2025-09-15 02:55:37,594 - INFO - bo.run_bo - BO Trial 9: Using RF surrogate + Expected Improvement
2025-09-15 02:55:37,594 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-15 02:55:37,595 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:55:37,595 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:55:37,595 - INFO - models.training_function_executor - Hyperparameters: {'lr': 2.637986940721531e-05, 'epochs': 9, 'batch_size': 8, 'hidden_size': 77, 'dropout': 0.5037808599967664, 'num_layers': 2}
2025-09-15 02:55:37,597 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 2.637986940721531e-05, 'epochs': 9, 'batch_size': 8, 'hidden_size': 77, 'dropout': 0.5037808599967664, 'num_layers': 2}
2025-09-15 02:56:15,071 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 0.8892301127016544, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.637986940721531e-05, 'epochs': 9, 'batch_size': 8, 'hidden_size': 77, 'dropout': 0.5037808599967664, 'num_layers': 2}}
2025-09-15 02:56:15,072 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 37.477s
2025-09-15 02:56:15,476 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:56:15,476 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.405s
2025-09-15 02:56:15,477 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 2.637986940721531e-05, 'epochs': np.int64(9), 'batch_size': np.int64(8), 'hidden_size': np.int64(77), 'dropout': 0.5037808599967664, 'num_layers': np.int64(2)}, value=0.7180
2025-09-15 02:56:15,477 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 2.637986940721531e-05, 'epochs': np.int64(9), 'batch_size': np.int64(8), 'hidden_size': np.int64(77), 'dropout': 0.5037808599967664, 'num_layers': np.int64(2)} -> 0.7180
2025-09-15 02:56:15,477 - INFO - bo.run_bo - BO Trial 10: Using RF surrogate + Expected Improvement
2025-09-15 02:56:15,477 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-15 02:56:15,477 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 02:56:15,478 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 02:56:15,478 - INFO - models.training_function_executor - Hyperparameters: {'lr': 1.4879901732653896e-05, 'epochs': 24, 'batch_size': 128, 'hidden_size': 132, 'dropout': 0.6242983906049006, 'num_layers': 2}
2025-09-15 02:56:15,480 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 1.4879901732653896e-05, 'epochs': 24, 'batch_size': 128, 'hidden_size': 132, 'dropout': 0.6242983906049006, 'num_layers': 2}
2025-09-15 02:57:10,954 - INFO - models.training_function_executor - Training completed successfully: {'val_accuracy': 0.718, 'final_loss': 0.9240664336830378, 'macro_f1': 0.718, 'model_name': 'FallbackLSTM', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.4879901732653896e-05, 'epochs': 24, 'batch_size': 128, 'hidden_size': 132, 'dropout': 0.6242983906049006, 'num_layers': 2}}
2025-09-15 02:57:10,954 - INFO - models.training_function_executor - [PROFILE] objective(train+eval) took 55.477s
2025-09-15 02:57:11,369 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7180
2025-09-15 02:57:11,369 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.414s
2025-09-15 02:57:11,369 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 1.4879901732653896e-05, 'epochs': np.int64(24), 'batch_size': np.int64(128), 'hidden_size': np.int64(132), 'dropout': 0.6242983906049006, 'num_layers': np.int64(2)}, value=0.7180
2025-09-15 02:57:11,369 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 1.4879901732653896e-05, 'epochs': np.int64(24), 'batch_size': np.int64(128), 'hidden_size': np.int64(132), 'dropout': 0.6242983906049006, 'num_layers': np.int64(2)} -> 0.7180
2025-09-15 02:57:11,370 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.7180
2025-09-15 02:57:11,370 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.01535224694197351, 'epochs': np.int64(17), 'batch_size': 128, 'hidden_size': np.int64(204), 'dropout': 0.41779511056254093, 'num_layers': np.int64(2)}
2025-09-15 02:57:11,371 - INFO - visualization - Generating BO visualization charts with 10 trials...
2025-09-15 02:57:20,946 - INFO - visualization - BO summary saved to: charts/BO_FallbackLSTM_20250915_025711/bo_summary.txt
2025-09-15 02:57:20,946 - INFO - visualization - BO charts saved to: charts/BO_FallbackLSTM_20250915_025711
2025-09-15 02:57:20,947 - INFO - evaluation.code_generation_pipeline_orchestrator - 📊 BO charts saved to: charts/BO_FallbackLSTM_20250915_025711
2025-09-15 02:57:20,947 - INFO - evaluation.code_generation_pipeline_orchestrator - 🚀 STEP 4: Final Training Execution
2025-09-15 02:57:20,947 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-15 02:57:21,245 - ERROR - models.training_function_executor - Failed to load training function from generated_training_functions/training_function_numpy_array_FallbackLSTM_1757902347.json: [Errno 2] No such file or directory: 'generated_training_functions/training_function_numpy_array_FallbackLSTM_1757902347.json'
2025-09-15 02:57:21,245 - ERROR - evaluation.code_generation_pipeline_orchestrator - Pipeline attempt 1 failed: [Errno 2] No such file or directory: 'generated_training_functions/training_function_numpy_array_FallbackLSTM_1757902347.json'
2025-09-15 02:57:21,299 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-15 02:57:21,299 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE ATTEMPT 2/4
2025-09-15 02:57:21,299 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-15 02:57:21,300 - INFO - evaluation.code_generation_pipeline_orchestrator - 🤖 STEP 1: AI Training Code Generation
2025-09-15 02:57:21,300 - INFO - models.ai_code_generator - Conducting literature review before code generation...
2025-09-15 02:57:21,300 - INFO - models.literature_review - Making GPT-5 literature review call with query: sequence classification time series machine learning multiclass sequence classification 2024 2025 PyTorch implementation state-of-the-art methods
2025-09-15 03:00:50,564 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-15 03:00:50,676 - INFO - models.literature_review - Successfully completed GPT-5 literature review with web search
2025-09-15 03:00:50,676 - INFO - models.literature_review - Literature review completed with confidence: 0.78
2025-09-15 03:00:50,676 - INFO - models.literature_review - Found 4 recommended approaches
2025-09-15 03:00:50,677 - INFO - models.literature_review - Literature review saved to: literature_reviews/literature_review_numpy_array_1757905250.txt
2025-09-15 03:00:50,677 - INFO - models.ai_code_generator - Literature review saved to: literature_reviews/literature_review_numpy_array_1757905250.txt
2025-09-15 03:00:50,678 - WARNING - models.ai_code_generator - AI code generation with literature review failed: 'num_samples', using fallback
2025-09-15 03:00:50,678 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: FallbackLSTM
2025-09-15 03:00:50,678 - INFO - evaluation.code_generation_pipeline_orchestrator - Reasoning: Fallback: LSTM selected for sequence data
2025-09-15 03:00:50,678 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'epochs', 'batch_size', 'hidden_size', 'dropout', 'num_layers']
2025-09-15 03:00:50,678 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.70
2025-09-15 03:00:50,678 - INFO - evaluation.code_generation_pipeline_orchestrator - 💾 STEP 2: Save Training Function to JSON
2025-09-15 03:00:50,679 - INFO - models.ai_code_generator - Training function saved to: generated_training_functions/training_function_numpy_array_FallbackLSTM_1757905250.json
2025-09-15 03:00:50,679 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_numpy_array_FallbackLSTM_1757905250.json
2025-09-15 03:00:50,681 - INFO - models.training_function_executor - Training function validation passed
2025-09-15 03:00:50,682 - INFO - evaluation.code_generation_pipeline_orchestrator - 🔍 STEP 3: Bayesian Optimization
2025-09-15 03:00:50,682 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: FallbackLSTM
2025-09-15 03:00:50,723 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 5000 samples
2025-09-15 03:00:50,723 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'epochs', 'batch_size', 'hidden_size', 'dropout', 'num_layers']
2025-09-15 03:00:50,724 - INFO - models.training_function_executor - GPU available: NVIDIA H100 NVL
2025-09-15 03:00:50,724 - WARNING - models.training_function_executor - Using provided subset instead of centralized splits - this may cause data leakage
2025-09-15 03:00:50,785 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-15 03:00:50,787 - INFO - bo.run_bo - BO Trial 1: Initial random exploration
2025-09-15 03:00:50,787 - INFO - bo.run_bo - [PROFILE] suggest() took 0.002s
2025-09-15 03:00:50,787 - INFO - models.training_function_executor - Using device: cuda
2025-09-15 03:00:50,788 - INFO - models.training_function_executor - Executing training function: FallbackLSTM
2025-09-15 03:00:50,788 - INFO - models.training_function_executor - Hyperparameters: {'lr': 0.01535224694197351, 'epochs': 17, 'batch_size': 128, 'hidden_size': 204, 'dropout': 0.41779511056254093, 'num_layers': 2}
2025-09-15 03:00:50,790 - INFO - models.training_function_executor - Starting training with hyperparameters: {'lr': 0.01535224694197351, 'epochs': 17, 'batch_size': 128, 'hidden_size': 204, 'dropout': 0.41779511056254093, 'num_layers': 2}
