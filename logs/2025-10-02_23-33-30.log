2025-10-02 23:33:31,255 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 23:33:31,362 - INFO - __main__ - Logging system initialized successfully
2025-10-02 23:33:31,362 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-10-02 23:33:31,362 - INFO - __main__ - Starting real data processing from data/dataset1/ directory
2025-10-02 23:33:31,362 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-10-02 23:33:31,362 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-10-02 23:33:31,362 - INFO - __main__ - Attempting to load: X.npy
2025-10-02 23:33:31,404 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-10-02 23:33:31,443 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-10-02 23:33:31,443 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-10-02 23:33:31,443 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-10-02 23:33:31,443 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-10-02 23:33:31,445 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-02 23:33:31,445 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-10-02 23:33:31,445 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-10-02 23:33:31,445 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-10-02 23:33:31,445 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-10-02 23:33:31,445 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-10-02 23:33:31,445 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-10-02 23:33:31,445 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-10-02 23:33:31,446 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-10-02 23:33:31,543 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-10-02 23:33:31,544 - INFO - class_balancing - Class imbalance analysis:
2025-10-02 23:33:31,544 - INFO - class_balancing -   Strategy: severe_imbalance
2025-10-02 23:33:31,544 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-10-02 23:33:31,544 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-10-02 23:33:31,544 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-10-02 23:33:31,544 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-10-02 23:33:31,544 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-10-02 23:33:31,544 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-10-02 23:33:31,544 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-10-02 23:33:31,716 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-10-02 23:33:31,717 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-10-02 23:33:31,717 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-10-02 23:33:31,717 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-10-02 23:33:31,717 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-10-02 23:33:31,717 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-10-02 23:33:31,717 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-10-02 23:35:49,094 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 23:35:49,167 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-02 23:35:49,167 - INFO - _models.ai_code_generator - Prompt length: 4379 characters
2025-10-02 23:35:49,167 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-02 23:35:49,167 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-02 23:35:49,167 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-02 23:38:51,123 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 23:38:51,171 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-02 23:38:51,171 - WARNING - _models.ai_code_generator - Initial JSON parse failed: Expecting ',' delimiter: line 21 column 17 (char 21356), attempting to fix common issues
2025-10-02 23:38:51,172 - INFO - _models.ai_code_generator - Successfully fixed JSON formatting issues
2025-10-02 23:38:51,172 - INFO - _models.ai_code_generator - AI generated training function: CNNTransformerLite1D
2025-10-02 23:38:51,172 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-10-02 23:38:51,172 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-10-02 23:38:51,172 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: CNNTransformerLite1D
2025-10-02 23:38:51,172 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'head_dim', 'n_heads', 'n_layers', 'd_ff_factor', 'stem_channels', 'ds_kernel1', 'ds_kernel2', 'patch_size', 'dropout', 'weight_decay', 'grad_clip_norm', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'compute_class_weights', 'aug_prob', 'aug_jitter_std', 'aug_scale_low', 'aug_scale_high', 'aug_drift_max_amp', 'normalize', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'seed']
2025-10-02 23:38:51,172 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-10-02 23:38:51,172 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-10-02 23:38:51,173 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:38:51,173 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:38:51,173 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-10-02 23:38:51,173 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: CNNTransformerLite1D
2025-10-02 23:38:51,173 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-10-02 23:38:51,173 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-02 23:38:51,173 - WARNING - package_installer - Could not parse code for imports due to syntax error: unexpected character after line continuation character (<unknown>, line 1)
2025-10-02 23:38:51,173 - INFO - package_installer - Extracted imports from code: set()
2025-10-02 23:38:51,173 - INFO - package_installer - ‚úÖ No external packages required
2025-10-02 23:38:51,173 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-02 23:38:51,173 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:38:51,173 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-02 23:38:51,173 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'head_dim', 'n_heads', 'n_layers', 'd_ff_factor', 'stem_channels', 'ds_kernel1', 'ds_kernel2', 'patch_size', 'dropout', 'weight_decay', 'grad_clip_norm', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'compute_class_weights', 'aug_prob', 'aug_jitter_std', 'aug_scale_low', 'aug_scale_high', 'aug_drift_max_amp', 'normalize', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'seed']
2025-10-02 23:38:51,173 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 23:38:51,173 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:38:51,173 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-02 23:38:51,206 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-02 23:38:51,337 - INFO - bo.run_bo - Converted GPT search space: 28 parameters
2025-10-02 23:38:51,337 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-02 23:38:51,337 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-02 23:38:51,338 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-02 23:38:51,338 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 23:38:51,338 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:38:51,338 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:38:51,338 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}
2025-10-02 23:38:51,340 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}
2025-10-02 23:39:34,248 - ERROR - _models.training_function_executor - Training execution failed: too many values to unpack (expected 2)
2025-10-02 23:39:34,248 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-02 23:39:34,248 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-02 23:39:34,248 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-02 23:39:34,248 - INFO - _models.ai_code_generator - Prompt length: 24296 characters
2025-10-02 23:39:34,248 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-02 23:39:34,248 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-02 23:39:34,248 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-02 23:41:11,301 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 23:41:11,302 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-02 23:41:11,302 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251002_234111_attempt1.txt
2025-10-02 23:41:11,302 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train: torch.Tensor, y_train: torch.Tensor,\n    X_val: torch.Tensor, y_val: torch.Tensor,\n    device,\n    # Architecture hyperparams\n    seq_len: int = 1000,\n    patch_size: int = 10,\n    head_dim: int = 12,\n    n_heads: int = 4,\n    n_layers: int = 2,\n    d_ff_factor: int = 2,\n    stem_channels: int = 8,\n    ds_kernel1: int = 7,\n    ds_kernel2: int = 5,\n    dropout: float = 0.1,\n    num_classes: int = 5,\n    max_rel_positions: int = 128,\n    # Optimization hyperparams\n    epochs: int = 15,\n    batch_size: int = 128,\n    lr: float = 5e-4,\n    weight_decay: float = 1e-4,\n    grad_clip_norm: float = 1.0,\n    use_focal_loss: bool = False,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    compute_class_weights: bool = True,\n    # Augmentations\n    aug_prob: float = 0.5,\n    aug_jitter_std: float = 0.005,\n    aug_scale_low: float = 0.9,\n    aug_scale_high: float = 1.1,\n    aug_drift_max_amp: float = 0.05,\n    normalize: bool = True,\n    # Quantization params (post-training)\n    quantization_bits: int = 8,\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n    # Reproducibility\n    seed: int = 42\n) -> Dict[str, Any]:\n    \"\"\"\n    Trains a compact CNN-Transformer Lite (2-lead ECG) model and returns a quantized model and metrics.\n\n    Notes:\n    - ALWAYS trains on the provided GPU device.\n    - All tensors and the model are moved to the same device during training.\n    - DataLoader pin_memory is set to False as required.\n    - Inputs X_* expected as float tensors shaped (N, 2, 1000) or (N, 1000, 2).\n    \"\"\"\n    # Normalize device argument\n    device = torch.device(device)\n\n    # Reproducibility\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n    # Datasets & Loaders\n    train_ds = ECGDataset(\n        X_train, y_train, seq_len=seq_len, augment=True, aug_prob=aug_prob,\n        aug_jitter_std=aug_jitter_std, aug_scale_low=aug_scale_low, aug_scale_high=aug_scale_high,\n        aug_drift_max_amp=aug_drift_max_amp, normalize=normalize\n    )\n    val_ds = ECGDataset(\n        X_val, y_val, seq_len=seq_len, augment=False, normalize=normalize\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CNNTransformerLite1D(\n        seq_len=seq_len, in_ch=2, stem_ch=stem_channels, ds_kernel1=ds_kernel1, ds_kernel2=ds_kernel2,\n        patch_size=patch_size, head_dim=head_dim, n_heads=n_heads, n_layers=n_layers,\n        d_ff_factor=d_ff_factor, dropout=dropout, num_classes=num_classes, max_rel_positions=max_rel_positions\n    )\n\n    # Move model to GPU\n    model = model.to(device)\n\n    # Class weights (optional)\n    if compute_class_weights:\n        with torch.no_grad():\n            classes = torch.arange(num_classes)\n            counts = torch.tensor([(y_train == c).sum().item() for c in classes], dtype=torch.float)\n            counts = torch.clamp(counts, min=1.0)\n            weights = counts.sum() / counts\n            weights = weights / weights.mean()\n            class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = None  # optionally add schedulers here if desired\n\n    # Helper to robustly unpack batches that may include extra items\n    def _unpack_batch(batch):\n        if isinstance(batch, (list, tuple)):\n            if len(batch) < 2:\n                raise ValueError(\"Batch must contain at least 2 elements (inputs, targets)\")\n            return batch[0], batch[1]\n        if isinstance(batch, dict):\n            # Try common key names\n            x_keys = [\"x\", \"inputs\", \"data\", \"features\"]\n            y_keys = [\"y\", \"labels\", \"target\", \"targets\"]\n            for xk in x_keys:\n                for yk in y_keys:\n                    if xk in batch and yk in batch:\n                        return batch[xk], batch[yk]\n            # Fallback: first two tensor values\n            tensors = [v for v in batch.values() if torch.is_tensor(v)]\n            if len(tensors) >= 2:\n                return tensors[0], tensors[1]\n            raise ValueError(\"Could not unpack batch dict into (inputs, targets)\")\n        raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        total_train = 0\n\n        for batch in train_loader:\n            xb, yb = _unpack_batch(batch)\n\n            # Move to GPU\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Ensure shape (B, 2, T)\n            if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            # Make sure loss is on the same device\n            loss = loss.to(device)\n\n            loss.backward()\n            if grad_clip_norm is not None and grad_clip_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * xb.size(0)\n            total_train += xb.size(0)\n\n        epoch_train_loss = running_loss / max(1, total_train)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _unpack_batch(batch)\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                loss = loss.to(device)\n\n                val_running_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n\n        epoch_val_loss = val_running_loss / max(1, total)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}: train_loss={epoch_train_loss:.5f} val_loss={epoch_val_loss:.5f} val_acc={epoch_val_acc:.4f}\")\n\n    # Move to CPU for post-training quantization\n    model_cpu = model.to('cpu')\n    model_cpu.eval()\n\n    quantized_model = apply_post_training_quantization(model_cpu, quantization_bits, quantize_weights, quantize_activations)\n\n    # Return artifacts\n    return {\n        'model': quantized_model,\n        'metrics': {\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'val_acc': val_accs\n        },\n        'config': {\n            'seq_len': seq_len,\n            'patch_size': patch_size,\n            'head_dim': head_dim,\n            'n_heads': n_heads,\n            'n_layers': n_layers,\n            'd_ff_factor': d_ff_factor,\n            'stem_channels': stem_channels,\n            'ds_kernel1': ds_kernel1,\n            'ds_kernel2': ds_kernel2,\n            'dropout': dropout,\n            'num_classes': num_classes,\n            'epochs': epochs,\n            'batch_size': batch_size,\n            'lr': lr,\n            'weight_decay': weight_decay,\n            'grad_clip_norm': grad_clip_norm,\n            'use_focal_loss': use_focal_loss,\n            'focal_gamma': focal_gamma,\n            'label_smoothing': label_smoothing,\n            'compute_class_weights': compute_class_weights,\n            'augmentations': {\n                'aug_prob': aug_prob,\n                'aug_jitter_std': aug_jitter_std,\n                'aug_scale_low': aug_scale_low,\n                'aug_scale_high': aug_scale_high,\n                'aug_drift_max_amp': aug_drift_max_amp,\n                'normalize': normalize\n            },\n            'quantization': {\n                'quantization_bits': quantization_bits,\n                'quantize_weights': quantize_weights,\n                'quantize_activations': quantize_activations\n            },\n            'seed': seed\n        }\n    }\n"}
2025-10-02 23:41:11,302 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-02 23:41:11,302 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train: torch.Tensor, y_train: torch.Tensor,\n    X_val: torch.Tensor, y_val: torch.Tensor,\n    device,\n    # Architecture hyperparams\n    seq_len: int = 1000,\n    patch_size: int = 10,\n    head_dim: int = 12,\n    n_heads: int = 4,\n    n_layers: int = 2,\n    d_ff_factor: int = 2,\n    stem_channels: int = 8,\n    ds_kernel1: int = 7,\n    ds_kernel2: int = 5,\n    dropout: float = 0.1,\n    num_classes: int = 5,\n    max_rel_positions: int = 128,\n    # Optimization hyperparams\n    epochs: int = 15,\n    batch_size: int = 128,\n    lr: float = 5e-4,\n    weight_decay: float = 1e-4,\n    grad_clip_norm: float = 1.0,\n    use_focal_loss: bool = False,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    compute_class_weights: bool = True,\n    # Augmentations\n    aug_prob: float = 0.5,\n    aug_jitter_std: float = 0.005,\n    aug_scale_low: float = 0.9,\n    aug_scale_high: float = 1.1,\n    aug_drift_max_amp: float = 0.05,\n    normalize: bool = True,\n    # Quantization params (post-training)\n    quantization_bits: int = 8,\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n    # Reproducibility\n    seed: int = 42\n) -> Dict[str, Any]:\n    \"\"\"\n    Trains a compact CNN-Transformer Lite (2-lead ECG) model and returns a quantized model and metrics.\n\n    Notes:\n    - ALWAYS trains on the provided GPU device.\n    - All tensors and the model are moved to the same device during training.\n    - DataLoader pin_memory is set to False as required.\n    - Inputs X_* expected as float tensors shaped (N, 2, 1000) or (N, 1000, 2).\n    \"\"\"\n    # Normalize device argument\n    device = torch.device(device)\n\n    # Reproducibility\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n    # Datasets & Loaders\n    train_ds = ECGDataset(\n        X_train, y_train, seq_len=seq_len, augment=True, aug_prob=aug_prob,\n        aug_jitter_std=aug_jitter_std, aug_scale_low=aug_scale_low, aug_scale_high=aug_scale_high,\n        aug_drift_max_amp=aug_drift_max_amp, normalize=normalize\n    )\n    val_ds = ECGDataset(\n        X_val, y_val, seq_len=seq_len, augment=False, normalize=normalize\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CNNTransformerLite1D(\n        seq_len=seq_len, in_ch=2, stem_ch=stem_channels, ds_kernel1=ds_kernel1, ds_kernel2=ds_kernel2,\n        patch_size=patch_size, head_dim=head_dim, n_heads=n_heads, n_layers=n_layers,\n        d_ff_factor=d_ff_factor, dropout=dropout, num_classes=num_classes, max_rel_positions=max_rel_positions\n    )\n\n    # Move model to GPU\n    model = model.to(device)\n\n    # Class weights (optional)\n    if compute_class_weights:\n        with torch.no_grad():\n            classes = torch.arange(num_classes)\n            counts = torch.tensor([(y_train == c).sum().item() for c in classes], dtype=torch.float)\n            counts = torch.clamp(counts, min=1.0)\n            weights = counts.sum() / counts\n            weights = weights / weights.mean()\n            class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = None  # optionally add schedulers here if desired\n\n    # Helper to robustly unpack batches that may include extra items\n    def _unpack_batch(batch):\n        if isinstance(batch, (list, tuple)):\n            if len(batch) < 2:\n                raise ValueError(\"Batch must contain at least 2 elements (inputs, targets)\")\n            return batch[0], batch[1]\n        if isinstance(batch, dict):\n            # Try common key names\n            x_keys = [\"x\", \"inputs\", \"data\", \"features\"]\n            y_keys = [\"y\", \"labels\", \"target\", \"targets\"]\n            for xk in x_keys:\n                for yk in y_keys:\n                    if xk in batch and yk in batch:\n                        return batch[xk], batch[yk]\n            # Fallback: first two tensor values\n            tensors = [v for v in batch.values() if torch.is_tensor(v)]\n            if len(tensors) >= 2:\n                return tensors[0], tensors[1]\n            raise ValueError(\"Could not unpack batch dict into (inputs, targets)\")\n        raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        total_train = 0\n\n        for batch in train_loader:\n            xb, yb = _unpack_batch(batch)\n\n            # Move to GPU\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Ensure shape (B, 2, T)\n            if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            # Make sure loss is on the same device\n            loss = loss.to(device)\n\n            loss.backward()\n            if grad_clip_norm is not None and grad_clip_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * xb.size(0)\n            total_train += xb.size(0)\n\n        epoch_train_loss = running_loss / max(1, total_train)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _unpack_batch(batch)\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                loss = loss.to(device)\n\n                val_running_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n\n        epoch_val_loss = val_running_loss / max(1, total)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}: train_loss={epoch_train_loss:.5f} val_loss={epoch_val_loss:.5f} val_acc={epoch_val_acc:.4f}\")\n\n    # Move to CPU for post-training quantization\n    model_cpu = model.to('cpu')\n    model_cpu.eval()\n\n    quantized_model = apply_post_training_quantization(model_cpu, quantization_bits, quantize_weights, quantize_activations)\n\n    # Return artifacts\n    return {\n        'model': quantized_model,\n        'metrics': {\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'val_acc': val_accs\n        },\n        'config': {\n            'seq_len': seq_len,\n            'patch_size': patch_size,\n            'head_dim': head_dim,\n            'n_heads': n_heads,\n            'n_layers': n_layers,\n            'd_ff_factor': d_ff_factor,\n            'stem_channels': stem_channels,\n            'ds_kernel1': ds_kernel1,\n            'ds_kernel2': ds_kernel2,\n            'dropout': dropout,\n            'num_classes': num_classes,\n            'epochs': epochs,\n            'batch_size': batch_size,\n            'lr': lr,\n            'weight_decay': weight_decay,\n            'grad_clip_norm': grad_clip_norm,\n            'use_focal_loss': use_focal_loss,\n            'focal_gamma': focal_gamma,\n            'label_smoothing': label_smoothing,\n            'compute_class_weights': compute_class_weights,\n            'augmentations': {\n                'aug_prob': aug_prob,\n                'aug_jitter_std': aug_jitter_std,\n                'aug_scale_low': aug_scale_low,\n                'aug_scale_high': aug_scale_high,\n                'aug_drift_max_amp': aug_drift_max_amp,\n                'normalize': normalize\n            },\n            'quantization': {\n                'quantization_bits': quantization_bits,\n                'quantize_weights': quantize_weights,\n                'quantize_activations': quantize_activations\n            },\n            'seed': seed\n        }\n    }\n"}
2025-10-02 23:41:11,302 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-02 23:41:11,302 - ERROR - _models.training_function_executor - BO training objective failed: too many values to unpack (expected 2)
2025-10-02 23:41:11,302 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 139.964s
2025-10-02 23:41:11,302 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: too many values to unpack (expected 2)
2025-10-02 23:41:11,303 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-10-02 23:41:14,306 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-10-02 23:41:14,306 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-10-02 23:41:14,306 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:41:14,307 - INFO - _models.training_function_executor - Loaded training function: CNNTransformerLite1D
2025-10-02 23:41:14,307 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-02 23:41:14,307 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-10-02 23:41:14,307 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:41:14,307 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: CNNTransformerLite1D
2025-10-02 23:41:14,307 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-10-02 23:41:14,307 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 1/4
2025-10-02 23:41:14,307 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 1: üì¶ Installing dependencies for GPT-generated training code...
2025-10-02 23:41:14,307 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-02 23:41:14,308 - INFO - package_installer - Extracted imports from code: set()
2025-10-02 23:41:14,308 - INFO - package_installer - ‚úÖ No external packages required
2025-10-02 23:41:14,308 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-02 23:41:14,308 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:41:14,308 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-02 23:41:14,308 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'head_dim', 'n_heads', 'n_layers', 'd_ff_factor', 'stem_channels', 'ds_kernel1', 'ds_kernel2', 'patch_size', 'dropout', 'weight_decay', 'grad_clip_norm', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'compute_class_weights', 'aug_prob', 'aug_jitter_std', 'aug_scale_low', 'aug_scale_high', 'aug_drift_max_amp', 'normalize', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'seed']
2025-10-02 23:41:14,309 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 23:41:14,309 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:41:14,309 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-02 23:41:14,341 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-02 23:41:14,377 - INFO - bo.run_bo - Converted GPT search space: 28 parameters
2025-10-02 23:41:14,377 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-02 23:41:14,377 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-02 23:41:14,378 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-02 23:41:14,378 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 23:41:14,378 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:41:14,378 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:41:14,378 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}
2025-10-02 23:41:14,379 - ERROR - _models.training_function_executor - Training execution failed: name 'torch' is not defined
2025-10-02 23:41:14,379 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-02 23:41:14,379 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-02 23:41:14,379 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-02 23:41:14,379 - INFO - _models.ai_code_generator - Prompt length: 13280 characters
2025-10-02 23:41:14,379 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-02 23:41:14,379 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-02 23:41:14,379 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-02 23:42:22,923 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 23:42:22,924 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-02 23:42:22,924 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251002_234222_attempt1.txt
2025-10-02 23:42:22,924 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Architecture hyperparams\n    seq_len: int = 1000,\n    patch_size: int = 10,\n    head_dim: int = 12,\n    n_heads: int = 4,\n    n_layers: int = 2,\n    d_ff_factor: int = 2,\n    stem_channels: int = 8,\n    ds_kernel1: int = 7,\n    ds_kernel2: int = 5,\n    dropout: float = 0.1,\n    num_classes: int = 5,\n    max_rel_positions: int = 128,\n    # Optimization hyperparams\n    epochs: int = 15,\n    batch_size: int = 128,\n    lr: float = 5e-4,\n    weight_decay: float = 1e-4,\n    grad_clip_norm: float = 1.0,\n    use_focal_loss: bool = False,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    compute_class_weights: bool = True,\n    # Augmentations\n    aug_prob: float = 0.5,\n    aug_jitter_std: float = 0.005,\n    aug_scale_low: float = 0.9,\n    aug_scale_high: float = 1.1,\n    aug_drift_max_amp: float = 0.05,\n    normalize: bool = True,\n    # Quantization params (post-training)\n    quantization_bits: int = 8,\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n    # Reproducibility\n    seed: int = 42\n) -> dict:\n    \"\"\"\n    Trains a compact CNN-Transformer Lite (2-lead ECG) model and returns a quantized model and metrics.\n\n    Notes:\n    - ALWAYS trains on the provided GPU device.\n    - All tensors and the model are moved to the same device during training.\n    - DataLoader pin_memory is set to False as required.\n    - Inputs X_* expected as float tensors shaped (N, 2, 1000) or (N, 1000, 2).\n    \"\"\"\n    import random\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader\n\n    # Normalize device argument\n    device = torch.device(device)\n\n    # Reproducibility\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n    # Datasets & Loaders\n    train_ds = ECGDataset(\n        X_train, y_train, seq_len=seq_len, augment=True, aug_prob=aug_prob,\n        aug_jitter_std=aug_jitter_std, aug_scale_low=aug_scale_low, aug_scale_high=aug_scale_high,\n        aug_drift_max_amp=aug_drift_max_amp, normalize=normalize\n    )\n    val_ds = ECGDataset(\n        X_val, y_val, seq_len=seq_len, augment=False, normalize=normalize\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CNNTransformerLite1D(\n        seq_len=seq_len, in_ch=2, stem_ch=stem_channels, ds_kernel1=ds_kernel1, ds_kernel2=ds_kernel2,\n        patch_size=patch_size, head_dim=head_dim, n_heads=n_heads, n_layers=n_layers,\n        d_ff_factor=d_ff_factor, dropout=dropout, num_classes=num_classes, max_rel_positions=max_rel_positions\n    )\n\n    # Move model to GPU\n    model = model.to(device)\n\n    # Class weights (optional)\n    if compute_class_weights:\n        with torch.no_grad():\n            classes = torch.arange(num_classes)\n            counts = torch.tensor([(y_train == c).sum().item() for c in classes], dtype=torch.float)\n            counts = torch.clamp(counts, min=1.0)\n            weights = counts.sum() / counts\n            weights = weights / weights.mean()\n            class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = None  # optionally add schedulers here if desired\n\n    # Helper to robustly unpack batches that may include extra items\n    def _unpack_batch(batch):\n        if isinstance(batch, (list, tuple)):\n            if len(batch) < 2:\n                raise ValueError(\"Batch must contain at least 2 elements (inputs, targets)\")\n            return batch[0], batch[1]\n        if isinstance(batch, dict):\n            # Try common key names\n            x_keys = [\"x\", \"inputs\", \"data\", \"features\"]\n            y_keys = [\"y\", \"labels\", \"target\", \"targets\"]\n            for xk in x_keys:\n                for yk in y_keys:\n                    if xk in batch and yk in batch:\n                        return batch[xk], batch[yk]\n            # Fallback: first two tensor values\n            tensors = [v for v in batch.values() if torch.is_tensor(v)]\n            if len(tensors) >= 2:\n                return tensors[0], tensors[1]\n            raise ValueError(\"Could not unpack batch dict into (inputs, targets)\")\n        raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        total_train = 0\n\n        for batch in train_loader:\n            xb, yb = _unpack_batch(batch)\n\n            # Move to GPU\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Ensure shape (B, 2, T)\n            if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            # Make sure loss is on the same device\n            loss = loss.to(device)\n\n            loss.backward()\n            if grad_clip_norm is not None and grad_clip_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * xb.size(0)\n            total_train += xb.size(0)\n\n        epoch_train_loss = running_loss / max(1, total_train)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _unpack_batch(batch)\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                loss = loss.to(device)\n\n                val_running_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n\n        epoch_val_loss = val_running_loss / max(1, total)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}: train_loss={epoch_train_loss:.5f} val_loss={epoch_val_loss:.5f} val_acc={epoch_val_acc:.4f}\")\n\n    # Move to CPU for post-training quantization\n    model_cpu = model.to('cpu')\n    model_cpu.eval()\n\n    quantized_model = apply_post_training_quantization(model_cpu, quantization_bits, quantize_weights, quantize_activations)\n\n    # Return artifacts\n    return {\n        'model': quantized_model,\n        'metrics': {\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'val_acc': val_accs\n        },\n        'config': {\n            'seq_len': seq_len,\n            'patch_size': patch_size,\n            'head_dim': head_dim,\n            'n_heads': n_heads,\n            'n_layers': n_layers,\n            'd_ff_factor': d_ff_factor,\n            'stem_channels': stem_channels,\n            'ds_kernel1': ds_kernel1,\n            'ds_kernel2': ds_kernel2,\n            'dropout': dropout,\n            'num_classes': num_classes,\n            'epochs': epochs,\n            'batch_size': batch_size,\n            'lr': lr,\n            'weight_decay': weight_decay,\n            'grad_clip_norm': grad_clip_norm,\n            'use_focal_loss': use_focal_loss,\n            'focal_gamma': focal_gamma,\n            'label_smoothing': label_smoothing,\n            'compute_class_weights': compute_class_weights,\n            'augmentations': {\n                'aug_prob': aug_prob,\n                'aug_jitter_std': aug_jitter_std,\n                'aug_scale_low': aug_scale_low,\n                'aug_scale_high': aug_scale_high,\n                'aug_drift_max_amp': aug_drift_max_amp,\n                'normalize': normalize\n            },\n            'quantization': {\n                'quantization_bits': quantization_bits,\n                'quantize_weights': quantize_weights,\n                'quantize_activations': quantize_activations\n            },\n            'seed': seed\n        }\n    }\n"}
2025-10-02 23:42:22,924 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-02 23:42:22,924 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Architecture hyperparams\n    seq_len: int = 1000,\n    patch_size: int = 10,\n    head_dim: int = 12,\n    n_heads: int = 4,\n    n_layers: int = 2,\n    d_ff_factor: int = 2,\n    stem_channels: int = 8,\n    ds_kernel1: int = 7,\n    ds_kernel2: int = 5,\n    dropout: float = 0.1,\n    num_classes: int = 5,\n    max_rel_positions: int = 128,\n    # Optimization hyperparams\n    epochs: int = 15,\n    batch_size: int = 128,\n    lr: float = 5e-4,\n    weight_decay: float = 1e-4,\n    grad_clip_norm: float = 1.0,\n    use_focal_loss: bool = False,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    compute_class_weights: bool = True,\n    # Augmentations\n    aug_prob: float = 0.5,\n    aug_jitter_std: float = 0.005,\n    aug_scale_low: float = 0.9,\n    aug_scale_high: float = 1.1,\n    aug_drift_max_amp: float = 0.05,\n    normalize: bool = True,\n    # Quantization params (post-training)\n    quantization_bits: int = 8,\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n    # Reproducibility\n    seed: int = 42\n) -> dict:\n    \"\"\"\n    Trains a compact CNN-Transformer Lite (2-lead ECG) model and returns a quantized model and metrics.\n\n    Notes:\n    - ALWAYS trains on the provided GPU device.\n    - All tensors and the model are moved to the same device during training.\n    - DataLoader pin_memory is set to False as required.\n    - Inputs X_* expected as float tensors shaped (N, 2, 1000) or (N, 1000, 2).\n    \"\"\"\n    import random\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader\n\n    # Normalize device argument\n    device = torch.device(device)\n\n    # Reproducibility\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n    # Datasets & Loaders\n    train_ds = ECGDataset(\n        X_train, y_train, seq_len=seq_len, augment=True, aug_prob=aug_prob,\n        aug_jitter_std=aug_jitter_std, aug_scale_low=aug_scale_low, aug_scale_high=aug_scale_high,\n        aug_drift_max_amp=aug_drift_max_amp, normalize=normalize\n    )\n    val_ds = ECGDataset(\n        X_val, y_val, seq_len=seq_len, augment=False, normalize=normalize\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model\n    model = CNNTransformerLite1D(\n        seq_len=seq_len, in_ch=2, stem_ch=stem_channels, ds_kernel1=ds_kernel1, ds_kernel2=ds_kernel2,\n        patch_size=patch_size, head_dim=head_dim, n_heads=n_heads, n_layers=n_layers,\n        d_ff_factor=d_ff_factor, dropout=dropout, num_classes=num_classes, max_rel_positions=max_rel_positions\n    )\n\n    # Move model to GPU\n    model = model.to(device)\n\n    # Class weights (optional)\n    if compute_class_weights:\n        with torch.no_grad():\n            classes = torch.arange(num_classes)\n            counts = torch.tensor([(y_train == c).sum().item() for c in classes], dtype=torch.float)\n            counts = torch.clamp(counts, min=1.0)\n            weights = counts.sum() / counts\n            weights = weights / weights.mean()\n            class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = None  # optionally add schedulers here if desired\n\n    # Helper to robustly unpack batches that may include extra items\n    def _unpack_batch(batch):\n        if isinstance(batch, (list, tuple)):\n            if len(batch) < 2:\n                raise ValueError(\"Batch must contain at least 2 elements (inputs, targets)\")\n            return batch[0], batch[1]\n        if isinstance(batch, dict):\n            # Try common key names\n            x_keys = [\"x\", \"inputs\", \"data\", \"features\"]\n            y_keys = [\"y\", \"labels\", \"target\", \"targets\"]\n            for xk in x_keys:\n                for yk in y_keys:\n                    if xk in batch and yk in batch:\n                        return batch[xk], batch[yk]\n            # Fallback: first two tensor values\n            tensors = [v for v in batch.values() if torch.is_tensor(v)]\n            if len(tensors) >= 2:\n                return tensors[0], tensors[1]\n            raise ValueError(\"Could not unpack batch dict into (inputs, targets)\")\n        raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        total_train = 0\n\n        for batch in train_loader:\n            xb, yb = _unpack_batch(batch)\n\n            # Move to GPU\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Ensure shape (B, 2, T)\n            if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            # Make sure loss is on the same device\n            loss = loss.to(device)\n\n            loss.backward()\n            if grad_clip_norm is not None and grad_clip_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * xb.size(0)\n            total_train += xb.size(0)\n\n        epoch_train_loss = running_loss / max(1, total_train)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _unpack_batch(batch)\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                loss = loss.to(device)\n\n                val_running_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n\n        epoch_val_loss = val_running_loss / max(1, total)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}: train_loss={epoch_train_loss:.5f} val_loss={epoch_val_loss:.5f} val_acc={epoch_val_acc:.4f}\")\n\n    # Move to CPU for post-training quantization\n    model_cpu = model.to('cpu')\n    model_cpu.eval()\n\n    quantized_model = apply_post_training_quantization(model_cpu, quantization_bits, quantize_weights, quantize_activations)\n\n    # Return artifacts\n    return {\n        'model': quantized_model,\n        'metrics': {\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'val_acc': val_accs\n        },\n        'config': {\n            'seq_len': seq_len,\n            'patch_size': patch_size,\n            'head_dim': head_dim,\n            'n_heads': n_heads,\n            'n_layers': n_layers,\n            'd_ff_factor': d_ff_factor,\n            'stem_channels': stem_channels,\n            'ds_kernel1': ds_kernel1,\n            'ds_kernel2': ds_kernel2,\n            'dropout': dropout,\n            'num_classes': num_classes,\n            'epochs': epochs,\n            'batch_size': batch_size,\n            'lr': lr,\n            'weight_decay': weight_decay,\n            'grad_clip_norm': grad_clip_norm,\n            'use_focal_loss': use_focal_loss,\n            'focal_gamma': focal_gamma,\n            'label_smoothing': label_smoothing,\n            'compute_class_weights': compute_class_weights,\n            'augmentations': {\n                'aug_prob': aug_prob,\n                'aug_jitter_std': aug_jitter_std,\n                'aug_scale_low': aug_scale_low,\n                'aug_scale_high': aug_scale_high,\n                'aug_drift_max_amp': aug_drift_max_amp,\n                'normalize': normalize\n            },\n            'quantization': {\n                'quantization_bits': quantization_bits,\n                'quantize_weights': quantize_weights,\n                'quantize_activations': quantize_activations\n            },\n            'seed': seed\n        }\n    }\n"}
2025-10-02 23:42:22,924 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-02 23:42:22,924 - ERROR - _models.training_function_executor - BO training objective failed: name 'torch' is not defined
2025-10-02 23:42:22,924 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 68.546s
2025-10-02 23:42:22,924 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name 'torch' is not defined
2025-10-02 23:42:22,924 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-10-02 23:42:25,927 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-10-02 23:42:25,928 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-10-02 23:42:25,928 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:42:25,929 - INFO - _models.training_function_executor - Loaded training function: CNNTransformerLite1D
2025-10-02 23:42:25,929 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-02 23:42:25,929 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-10-02 23:42:25,929 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:42:25,929 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: CNNTransformerLite1D
2025-10-02 23:42:25,929 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-10-02 23:42:25,929 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 2/4
2025-10-02 23:42:25,929 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 2: üì¶ Installing dependencies for GPT-generated training code...
2025-10-02 23:42:25,929 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-02 23:42:25,930 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-02 23:42:25,930 - INFO - package_installer - Available packages: {'torch'}
2025-10-02 23:42:25,930 - INFO - package_installer - Missing packages: set()
2025-10-02 23:42:25,931 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-02 23:42:25,931 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-02 23:42:25,931 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:42:25,931 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-02 23:42:25,931 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'head_dim', 'n_heads', 'n_layers', 'd_ff_factor', 'stem_channels', 'ds_kernel1', 'ds_kernel2', 'patch_size', 'dropout', 'weight_decay', 'grad_clip_norm', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'compute_class_weights', 'aug_prob', 'aug_jitter_std', 'aug_scale_low', 'aug_scale_high', 'aug_drift_max_amp', 'normalize', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'seed']
2025-10-02 23:42:25,931 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 23:42:25,931 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:42:25,931 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-02 23:42:25,963 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-02 23:42:25,997 - INFO - bo.run_bo - Converted GPT search space: 28 parameters
2025-10-02 23:42:25,997 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-02 23:42:25,997 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-02 23:42:25,998 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-02 23:42:25,998 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 23:42:25,998 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:42:25,998 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:42:25,998 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}
2025-10-02 23:42:25,999 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}
2025-10-02 23:42:26,000 - ERROR - _models.training_function_executor - Training execution failed: name 'ECGDataset' is not defined
2025-10-02 23:42:26,000 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-02 23:42:26,000 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-02 23:42:26,000 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-02 23:42:26,000 - INFO - _models.ai_code_generator - Prompt length: 13333 characters
2025-10-02 23:42:26,000 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-02 23:42:26,000 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-02 23:42:26,000 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-02 23:44:46,135 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 23:44:46,162 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-02 23:44:46,162 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251002_234446_attempt1.txt
2025-10-02 23:44:46,162 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Architecture hyperparams\n    seq_len: int = 1000,\n    patch_size: int = 10,\n    head_dim: int = 12,\n    n_heads: int = 4,\n    n_layers: int = 2,\n    d_ff_factor: int = 2,\n    stem_channels: int = 8,\n    ds_kernel1: int = 7,\n    ds_kernel2: int = 5,\n    dropout: float = 0.1,\n    num_classes: int = 5,\n    max_rel_positions: int = 128,\n    # Optimization hyperparams\n    epochs: int = 15,\n    batch_size: int = 128,\n    lr: float = 5e-4,\n    weight_decay: float = 1e-4,\n    grad_clip_norm: float = 1.0,\n    use_focal_loss: bool = False,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    compute_class_weights: bool = True,\n    # Augmentations\n    aug_prob: float = 0.5,\n    aug_jitter_std: float = 0.005,\n    aug_scale_low: float = 0.9,\n    aug_scale_high: float = 1.1,\n    aug_drift_max_amp: float = 0.05,\n    normalize: bool = True,\n    # Quantization params (post-training)\n    quantization_bits: int = 8,\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n    # Reproducibility\n    seed: int = 42\n) -> dict:\n    \"\"\"\n    Trains a compact CNN-Transformer Lite (2-lead ECG) model and returns a quantized model and metrics.\n\n    Notes:\n    - ALWAYS trains on the provided GPU device.\n    - All tensors and the model are moved to the same device during training.\n    - DataLoader pin_memory is set to False as required.\n    - Inputs X_* expected as float tensors shaped (N, 2, 1000) or (N, 1000, 2).\n    \"\"\"\n    import random\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n\n    # Normalize device argument\n    device = torch.device(device)\n\n    # Reproducibility\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n    # ----------------------\n    # Local dataset (fixes NameError: ECGDataset)\n    # ----------------------\n    class ECGDataset(Dataset):\n        def __init__(\n            self,\n            X,\n            y,\n            seq_len: int,\n            augment: bool = False,\n            aug_prob: float = 0.5,\n            aug_jitter_std: float = 0.005,\n            aug_scale_low: float = 0.9,\n            aug_scale_high: float = 1.1,\n            aug_drift_max_amp: float = 0.05,\n            normalize: bool = True,\n        ):\n            self.x = torch.as_tensor(X, dtype=torch.float32)\n            self.y = torch.as_tensor(y, dtype=torch.long)\n            if self.x.dim() != 3:\n                raise ValueError(f\"Expected X to have 3 dims, got {self.x.shape}\")\n            # Standardize to (N, 2, T)\n            if self.x.shape[1] == seq_len and self.x.shape[2] == 2:\n                self.x = self.x.permute(0, 2, 1).contiguous()\n            elif self.x.shape[1] == 2 and self.x.shape[2] == seq_len:\n                pass\n            else:\n                raise ValueError(f\"X must be (N, 2, {seq_len}) or (N, {seq_len}, 2), got {self.x.shape}\")\n            if self.y.shape[0] != self.x.shape[0]:\n                raise ValueError(\"Mismatch between number of samples in X and y\")\n            self.seq_len = seq_len\n            self.augment = augment\n            self.aug_prob = float(aug_prob)\n            self.aug_jitter_std = float(aug_jitter_std)\n            self.aug_scale_low = float(aug_scale_low)\n            self.aug_scale_high = float(aug_scale_high)\n            self.aug_drift_max_amp = float(aug_drift_max_amp)\n            self.normalize = bool(normalize)\n\n        def __len__(self):\n            return self.x.shape[0]\n\n        def _normalize(self, x: torch.Tensor) -> torch.Tensor:\n            # Channel-wise z-score over time\n            mean = x.mean(dim=1, keepdim=True)\n            std = x.std(dim=1, keepdim=True).clamp_min(1e-6)\n            return (x - mean) / std\n\n        def _augment(self, x: torch.Tensor) -> torch.Tensor:\n            # Jitter\n            if self.aug_jitter_std > 0.0:\n                noise = torch.randn_like(x) * self.aug_jitter_std\n                x = x + noise\n            # Scale\n            if self.aug_scale_high > 0.0:\n                scale = random.uniform(self.aug_scale_low, self.aug_scale_high)\n                x = x * scale\n            # Low-frequency drift (sinusoidal)\n            if self.aug_drift_max_amp > 0.0:\n                T = x.shape[-1]\n                freq = random.uniform(0.25, 2.0)  # cycles over the window\n                t = torch.linspace(0, 2 * math.pi * freq, T, dtype=x.dtype, device=x.device)\n                amp = random.uniform(-self.aug_drift_max_amp, self.aug_drift_max_amp)\n                drift = amp * torch.sin(t)[None, :]\n                x = x + drift\n            return x\n\n        def __getitem__(self, idx):\n            x = self.x[idx]\n            y = self.y[idx]\n            if self.normalize:\n                x = self._normalize(x)\n            if self.augment and random.random() < self.aug_prob:\n                x = self._augment(x)\n            return x, y\n\n    # ----------------------\n    # Local FocalLoss (optional usage)\n    # ----------------------\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma: float = 2.0, weight: torch.Tensor | None = None, label_smoothing: float = 0.0):\n            super().__init__()\n            self.gamma = gamma\n            self.register_buffer('weight', weight if weight is not None else None)\n            self.label_smoothing = float(label_smoothing)\n\n        def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n            ce = F.cross_entropy(\n                logits,\n                targets,\n                weight=self.weight,\n                reduction='none',\n                label_smoothing=self.label_smoothing,\n            )\n            pt = torch.exp(-ce)\n            loss = ((1.0 - pt) ** self.gamma) * ce\n            return loss.mean()\n\n    # ----------------------\n    # Post-training quantization stub\n    # ----------------------\n    def apply_post_training_quantization(model: nn.Module, bits: int, quantize_weights: bool, quantize_activations: bool) -> nn.Module:\n        # Identity stub (no-op). Replace with real PTQ if available in the environment.\n        return model\n\n    # ----------------------\n    # Fallback simple model if CNNTransformerLite1D is not available in globals\n    # ----------------------\n    class _SimpleECGNet(nn.Module):\n        def __init__(self, in_ch: int = 2, stem_ch: int = 8, k1: int = 7, k2: int = 5, dropout: float = 0.1, num_classes: int = 5):\n            super().__init__()\n            p1 = k1 // 2\n            p2 = k2 // 2\n            self.net = nn.Sequential(\n                nn.Conv1d(in_ch, stem_ch, kernel_size=k1, padding=p1),\n                nn.BatchNorm1d(stem_ch),\n                nn.ReLU(inplace=True),\n                nn.MaxPool1d(kernel_size=2),\n                nn.Conv1d(stem_ch, stem_ch * 2, kernel_size=k2, padding=p2),\n                nn.BatchNorm1d(stem_ch * 2),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=dropout),\n                nn.Conv1d(stem_ch * 2, stem_ch * 4, kernel_size=k2, padding=p2),\n                nn.BatchNorm1d(stem_ch * 4),\n                nn.ReLU(inplace=True),\n                nn.AdaptiveAvgPool1d(1),\n            )\n            self.head = nn.Linear(stem_ch * 4, num_classes)\n\n        def forward(self, x):\n            x = self.net(x)\n            x = x.squeeze(-1)\n            return self.head(x)\n\n    # Datasets & Loaders\n    train_ds = ECGDataset(\n        X_train, y_train, seq_len=seq_len, augment=True, aug_prob=aug_prob,\n        aug_jitter_std=aug_jitter_std, aug_scale_low=aug_scale_low, aug_scale_high=aug_scale_high,\n        aug_drift_max_amp=aug_drift_max_amp, normalize=normalize\n    )\n    val_ds = ECGDataset(\n        X_val, y_val, seq_len=seq_len, augment=False, normalize=normalize\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model (use CNNTransformerLite1D if defined, else fallback)\n    if 'CNNTransformerLite1D' in globals() and callable(globals()['CNNTransformerLite1D']):\n        model = globals()['CNNTransformerLite1D'](\n            seq_len=seq_len, in_ch=2, stem_ch=stem_channels, ds_kernel1=ds_kernel1, ds_kernel2=ds_kernel2,\n            patch_size=patch_size, head_dim=head_dim, n_heads=n_heads, n_layers=n_layers,\n            d_ff_factor=d_ff_factor, dropout=dropout, num_classes=num_classes, max_rel_positions=max_rel_positions\n        )\n    else:\n        model = _SimpleECGNet(in_ch=2, stem_ch=stem_channels, k1=ds_kernel1, k2=ds_kernel2, dropout=dropout, num_classes=num_classes)\n\n    # Move model to GPU\n    model = model.to(device)\n\n    # Class weights (optional)\n    if compute_class_weights:\n        with torch.no_grad():\n            yt = train_ds.y\n            counts = torch.bincount(yt, minlength=num_classes).float().clamp_min(1.0)\n            weights = counts.sum() / counts\n            weights = weights / weights.mean()\n            class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = None  # optionally add schedulers here if desired\n\n    # Helper to robustly unpack batches that may include extra items\n    def _unpack_batch(batch):\n        if isinstance(batch, (list, tuple)):\n            if len(batch) < 2:\n                raise ValueError(\"Batch must contain at least 2 elements (inputs, targets)\")\n            return batch[0], batch[1]\n        if isinstance(batch, dict):\n            # Try common key names\n            x_keys = [\"x\", \"inputs\", \"data\", \"features\"]\n            y_keys = [\"y\", \"labels\", \"target\", \"targets\"]\n            for xk in x_keys:\n                for yk in y_keys:\n                    if xk in batch and yk in batch:\n                        return batch[xk], batch[yk]\n            tensors = [v for v in batch.values() if torch.is_tensor(v)]\n            if len(tensors) >= 2:\n                return tensors[0], tensors[1]\n            raise ValueError(\"Could not unpack batch dict into (inputs, targets)\")\n        raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        total_train = 0\n\n        for batch in train_loader:\n            xb, yb = _unpack_batch(batch)\n\n            # Move to GPU\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Ensure shape (B, 2, T)\n            if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss = loss.to(device)\n\n            loss.backward()\n            if grad_clip_norm is not None and grad_clip_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * xb.size(0)\n            total_train += xb.size(0)\n\n        epoch_train_loss = running_loss / max(1, total_train)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _unpack_batch(batch)\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                loss = loss.to(device)\n\n                val_running_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n\n        epoch_val_loss = val_running_loss / max(1, total)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}: train_loss={epoch_train_loss:.5f} val_loss={epoch_val_loss:.5f} val_acc={epoch_val_acc:.4f}\")\n\n    # Move to CPU for post-training quantization\n    model_cpu = model.to('cpu')\n    model_cpu.eval()\n\n    quantized_model = apply_post_training_quantization(model_cpu, quantization_bits, quantize_weights, quantize_activations)\n\n    # Return artifacts\n    return {\n        'model': quantized_model,\n        'metrics': {\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'val_acc': val_accs\n        },\n        'config': {\n            'seq_len': seq_len,\n            'patch_size': patch_size,\n            'head_dim': head_dim,\n            'n_heads': n_heads,\n            'n_layers': n_layers,\n            'd_ff_factor': d_ff_factor,\n            'stem_channels': stem_channels,\n            'ds_kernel1': ds_kernel1,\n            'ds_kernel2': ds_kernel2,\n            'dropout': dropout,\n            'num_classes': num_classes,\n            'epochs': epochs,\n            'batch_size': batch_size,\n            'lr': lr,\n            'weight_decay': weight_decay,\n            'grad_clip_norm': grad_clip_norm,\n            'use_focal_loss': use_focal_loss,\n            'focal_gamma': focal_gamma,\n            'label_smoothing': label_smoothing,\n            'compute_class_weights': compute_class_weights,\n            'augmentations': {\n                'aug_prob': aug_prob,\n                'aug_jitter_std': aug_jitter_std,\n                'aug_scale_low': aug_scale_low,\n                'aug_scale_high': aug_scale_high,\n                'aug_drift_max_amp': aug_drift_max_amp,\n                'normalize': normalize\n            },\n            'quantization': {\n                'quantization_bits': quantization_bits,\n                'quantize_weights': quantize_weights,\n                'quantize_activations': quantize_activations\n            },\n            'seed': seed\n        }\n    }\n"}
2025-10-02 23:44:46,162 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-02 23:44:46,162 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Architecture hyperparams\n    seq_len: int = 1000,\n    patch_size: int = 10,\n    head_dim: int = 12,\n    n_heads: int = 4,\n    n_layers: int = 2,\n    d_ff_factor: int = 2,\n    stem_channels: int = 8,\n    ds_kernel1: int = 7,\n    ds_kernel2: int = 5,\n    dropout: float = 0.1,\n    num_classes: int = 5,\n    max_rel_positions: int = 128,\n    # Optimization hyperparams\n    epochs: int = 15,\n    batch_size: int = 128,\n    lr: float = 5e-4,\n    weight_decay: float = 1e-4,\n    grad_clip_norm: float = 1.0,\n    use_focal_loss: bool = False,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    compute_class_weights: bool = True,\n    # Augmentations\n    aug_prob: float = 0.5,\n    aug_jitter_std: float = 0.005,\n    aug_scale_low: float = 0.9,\n    aug_scale_high: float = 1.1,\n    aug_drift_max_amp: float = 0.05,\n    normalize: bool = True,\n    # Quantization params (post-training)\n    quantization_bits: int = 8,\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n    # Reproducibility\n    seed: int = 42\n) -> dict:\n    \"\"\"\n    Trains a compact CNN-Transformer Lite (2-lead ECG) model and returns a quantized model and metrics.\n\n    Notes:\n    - ALWAYS trains on the provided GPU device.\n    - All tensors and the model are moved to the same device during training.\n    - DataLoader pin_memory is set to False as required.\n    - Inputs X_* expected as float tensors shaped (N, 2, 1000) or (N, 1000, 2).\n    \"\"\"\n    import random\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n\n    # Normalize device argument\n    device = torch.device(device)\n\n    # Reproducibility\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n    # ----------------------\n    # Local dataset (fixes NameError: ECGDataset)\n    # ----------------------\n    class ECGDataset(Dataset):\n        def __init__(\n            self,\n            X,\n            y,\n            seq_len: int,\n            augment: bool = False,\n            aug_prob: float = 0.5,\n            aug_jitter_std: float = 0.005,\n            aug_scale_low: float = 0.9,\n            aug_scale_high: float = 1.1,\n            aug_drift_max_amp: float = 0.05,\n            normalize: bool = True,\n        ):\n            self.x = torch.as_tensor(X, dtype=torch.float32)\n            self.y = torch.as_tensor(y, dtype=torch.long)\n            if self.x.dim() != 3:\n                raise ValueError(f\"Expected X to have 3 dims, got {self.x.shape}\")\n            # Standardize to (N, 2, T)\n            if self.x.shape[1] == seq_len and self.x.shape[2] == 2:\n                self.x = self.x.permute(0, 2, 1).contiguous()\n            elif self.x.shape[1] == 2 and self.x.shape[2] == seq_len:\n                pass\n            else:\n                raise ValueError(f\"X must be (N, 2, {seq_len}) or (N, {seq_len}, 2), got {self.x.shape}\")\n            if self.y.shape[0] != self.x.shape[0]:\n                raise ValueError(\"Mismatch between number of samples in X and y\")\n            self.seq_len = seq_len\n            self.augment = augment\n            self.aug_prob = float(aug_prob)\n            self.aug_jitter_std = float(aug_jitter_std)\n            self.aug_scale_low = float(aug_scale_low)\n            self.aug_scale_high = float(aug_scale_high)\n            self.aug_drift_max_amp = float(aug_drift_max_amp)\n            self.normalize = bool(normalize)\n\n        def __len__(self):\n            return self.x.shape[0]\n\n        def _normalize(self, x: torch.Tensor) -> torch.Tensor:\n            # Channel-wise z-score over time\n            mean = x.mean(dim=1, keepdim=True)\n            std = x.std(dim=1, keepdim=True).clamp_min(1e-6)\n            return (x - mean) / std\n\n        def _augment(self, x: torch.Tensor) -> torch.Tensor:\n            # Jitter\n            if self.aug_jitter_std > 0.0:\n                noise = torch.randn_like(x) * self.aug_jitter_std\n                x = x + noise\n            # Scale\n            if self.aug_scale_high > 0.0:\n                scale = random.uniform(self.aug_scale_low, self.aug_scale_high)\n                x = x * scale\n            # Low-frequency drift (sinusoidal)\n            if self.aug_drift_max_amp > 0.0:\n                T = x.shape[-1]\n                freq = random.uniform(0.25, 2.0)  # cycles over the window\n                t = torch.linspace(0, 2 * math.pi * freq, T, dtype=x.dtype, device=x.device)\n                amp = random.uniform(-self.aug_drift_max_amp, self.aug_drift_max_amp)\n                drift = amp * torch.sin(t)[None, :]\n                x = x + drift\n            return x\n\n        def __getitem__(self, idx):\n            x = self.x[idx]\n            y = self.y[idx]\n            if self.normalize:\n                x = self._normalize(x)\n            if self.augment and random.random() < self.aug_prob:\n                x = self._augment(x)\n            return x, y\n\n    # ----------------------\n    # Local FocalLoss (optional usage)\n    # ----------------------\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma: float = 2.0, weight: torch.Tensor | None = None, label_smoothing: float = 0.0):\n            super().__init__()\n            self.gamma = gamma\n            self.register_buffer('weight', weight if weight is not None else None)\n            self.label_smoothing = float(label_smoothing)\n\n        def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n            ce = F.cross_entropy(\n                logits,\n                targets,\n                weight=self.weight,\n                reduction='none',\n                label_smoothing=self.label_smoothing,\n            )\n            pt = torch.exp(-ce)\n            loss = ((1.0 - pt) ** self.gamma) * ce\n            return loss.mean()\n\n    # ----------------------\n    # Post-training quantization stub\n    # ----------------------\n    def apply_post_training_quantization(model: nn.Module, bits: int, quantize_weights: bool, quantize_activations: bool) -> nn.Module:\n        # Identity stub (no-op). Replace with real PTQ if available in the environment.\n        return model\n\n    # ----------------------\n    # Fallback simple model if CNNTransformerLite1D is not available in globals\n    # ----------------------\n    class _SimpleECGNet(nn.Module):\n        def __init__(self, in_ch: int = 2, stem_ch: int = 8, k1: int = 7, k2: int = 5, dropout: float = 0.1, num_classes: int = 5):\n            super().__init__()\n            p1 = k1 // 2\n            p2 = k2 // 2\n            self.net = nn.Sequential(\n                nn.Conv1d(in_ch, stem_ch, kernel_size=k1, padding=p1),\n                nn.BatchNorm1d(stem_ch),\n                nn.ReLU(inplace=True),\n                nn.MaxPool1d(kernel_size=2),\n                nn.Conv1d(stem_ch, stem_ch * 2, kernel_size=k2, padding=p2),\n                nn.BatchNorm1d(stem_ch * 2),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=dropout),\n                nn.Conv1d(stem_ch * 2, stem_ch * 4, kernel_size=k2, padding=p2),\n                nn.BatchNorm1d(stem_ch * 4),\n                nn.ReLU(inplace=True),\n                nn.AdaptiveAvgPool1d(1),\n            )\n            self.head = nn.Linear(stem_ch * 4, num_classes)\n\n        def forward(self, x):\n            x = self.net(x)\n            x = x.squeeze(-1)\n            return self.head(x)\n\n    # Datasets & Loaders\n    train_ds = ECGDataset(\n        X_train, y_train, seq_len=seq_len, augment=True, aug_prob=aug_prob,\n        aug_jitter_std=aug_jitter_std, aug_scale_low=aug_scale_low, aug_scale_high=aug_scale_high,\n        aug_drift_max_amp=aug_drift_max_amp, normalize=normalize\n    )\n    val_ds = ECGDataset(\n        X_val, y_val, seq_len=seq_len, augment=False, normalize=normalize\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model (use CNNTransformerLite1D if defined, else fallback)\n    if 'CNNTransformerLite1D' in globals() and callable(globals()['CNNTransformerLite1D']):\n        model = globals()['CNNTransformerLite1D'](\n            seq_len=seq_len, in_ch=2, stem_ch=stem_channels, ds_kernel1=ds_kernel1, ds_kernel2=ds_kernel2,\n            patch_size=patch_size, head_dim=head_dim, n_heads=n_heads, n_layers=n_layers,\n            d_ff_factor=d_ff_factor, dropout=dropout, num_classes=num_classes, max_rel_positions=max_rel_positions\n        )\n    else:\n        model = _SimpleECGNet(in_ch=2, stem_ch=stem_channels, k1=ds_kernel1, k2=ds_kernel2, dropout=dropout, num_classes=num_classes)\n\n    # Move model to GPU\n    model = model.to(device)\n\n    # Class weights (optional)\n    if compute_class_weights:\n        with torch.no_grad():\n            yt = train_ds.y\n            counts = torch.bincount(yt, minlength=num_classes).float().clamp_min(1.0)\n            weights = counts.sum() / counts\n            weights = weights / weights.mean()\n            class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = None  # optionally add schedulers here if desired\n\n    # Helper to robustly unpack batches that may include extra items\n    def _unpack_batch(batch):\n        if isinstance(batch, (list, tuple)):\n            if len(batch) < 2:\n                raise ValueError(\"Batch must contain at least 2 elements (inputs, targets)\")\n            return batch[0], batch[1]\n        if isinstance(batch, dict):\n            # Try common key names\n            x_keys = [\"x\", \"inputs\", \"data\", \"features\"]\n            y_keys = [\"y\", \"labels\", \"target\", \"targets\"]\n            for xk in x_keys:\n                for yk in y_keys:\n                    if xk in batch and yk in batch:\n                        return batch[xk], batch[yk]\n            tensors = [v for v in batch.values() if torch.is_tensor(v)]\n            if len(tensors) >= 2:\n                return tensors[0], tensors[1]\n            raise ValueError(\"Could not unpack batch dict into (inputs, targets)\")\n        raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        total_train = 0\n\n        for batch in train_loader:\n            xb, yb = _unpack_batch(batch)\n\n            # Move to GPU\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Ensure shape (B, 2, T)\n            if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss = loss.to(device)\n\n            loss.backward()\n            if grad_clip_norm is not None and grad_clip_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * xb.size(0)\n            total_train += xb.size(0)\n\n        epoch_train_loss = running_loss / max(1, total_train)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _unpack_batch(batch)\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                loss = loss.to(device)\n\n                val_running_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n\n        epoch_val_loss = val_running_loss / max(1, total)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}: train_loss={epoch_train_loss:.5f} val_loss={epoch_val_loss:.5f} val_acc={epoch_val_acc:.4f}\")\n\n    # Move to CPU for post-training quantization\n    model_cpu = model.to('cpu')\n    model_cpu.eval()\n\n    quantized_model = apply_post_training_quantization(model_cpu, quantization_bits, quantize_weights, quantize_activations)\n\n    # Return artifacts\n    return {\n        'model': quantized_model,\n        'metrics': {\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'val_acc': val_accs\n        },\n        'config': {\n            'seq_len': seq_len,\n            'patch_size': patch_size,\n            'head_dim': head_dim,\n            'n_heads': n_heads,\n            'n_layers': n_layers,\n            'd_ff_factor': d_ff_factor,\n            'stem_channels': stem_channels,\n            'ds_kernel1': ds_kernel1,\n            'ds_kernel2': ds_kernel2,\n            'dropout': dropout,\n            'num_classes': num_classes,\n            'epochs': epochs,\n            'batch_size': batch_size,\n            'lr': lr,\n            'weight_decay': weight_decay,\n            'grad_clip_norm': grad_clip_norm,\n            'use_focal_loss': use_focal_loss,\n            'focal_gamma': focal_gamma,\n            'label_smoothing': label_smoothing,\n            'compute_class_weights': compute_class_weights,\n            'augmentations': {\n                'aug_prob': aug_prob,\n                'aug_jitter_std': aug_jitter_std,\n                'aug_scale_low': aug_scale_low,\n                'aug_scale_high': aug_scale_high,\n                'aug_drift_max_amp': aug_drift_max_amp,\n                'normalize': normalize\n            },\n            'quantization': {\n                'quantization_bits': quantization_bits,\n                'quantize_weights': quantize_weights,\n                'quantize_activations': quantize_activations\n            },\n            'seed': seed\n        }\n    }\n"}
2025-10-02 23:44:46,162 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-02 23:44:46,162 - ERROR - _models.training_function_executor - BO training objective failed: name 'ECGDataset' is not defined
2025-10-02 23:44:46,162 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 140.164s
2025-10-02 23:44:46,162 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name 'ECGDataset' is not defined
2025-10-02 23:44:46,162 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-10-02 23:44:49,166 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-10-02 23:44:49,166 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-10-02 23:44:49,166 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:44:49,167 - INFO - _models.training_function_executor - Loaded training function: CNNTransformerLite1D
2025-10-02 23:44:49,167 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-02 23:44:49,167 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-10-02 23:44:49,167 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:44:49,167 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: CNNTransformerLite1D
2025-10-02 23:44:49,167 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-10-02 23:44:49,167 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 3/4
2025-10-02 23:44:49,167 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 3: üì¶ Installing dependencies for GPT-generated training code...
2025-10-02 23:44:49,167 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-02 23:44:49,169 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-02 23:44:49,169 - INFO - package_installer - Available packages: {'torch'}
2025-10-02 23:44:49,169 - INFO - package_installer - Missing packages: set()
2025-10-02 23:44:49,169 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-02 23:44:49,170 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-02 23:44:49,170 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:44:49,170 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-02 23:44:49,170 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'head_dim', 'n_heads', 'n_layers', 'd_ff_factor', 'stem_channels', 'ds_kernel1', 'ds_kernel2', 'patch_size', 'dropout', 'weight_decay', 'grad_clip_norm', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'compute_class_weights', 'aug_prob', 'aug_jitter_std', 'aug_scale_low', 'aug_scale_high', 'aug_drift_max_amp', 'normalize', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'seed']
2025-10-02 23:44:49,170 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 23:44:49,170 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:44:49,170 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-02 23:44:49,202 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-02 23:44:49,237 - INFO - bo.run_bo - Converted GPT search space: 28 parameters
2025-10-02 23:44:49,237 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-02 23:44:49,238 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-02 23:44:49,239 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-02 23:44:49,239 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 23:44:49,239 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:44:49,239 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:44:49,239 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}
2025-10-02 23:44:49,240 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}
2025-10-02 23:45:02,194 - ERROR - _models.training_function_executor - Training execution failed: too many values to unpack (expected 2)
2025-10-02 23:45:02,194 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-02 23:45:02,194 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-02 23:45:02,194 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-02 23:45:02,194 - INFO - _models.ai_code_generator - Prompt length: 19115 characters
2025-10-02 23:45:02,194 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-02 23:45:02,194 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-02 23:45:02,194 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-02 23:46:53,187 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 23:46:53,188 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-02 23:46:53,189 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251002_234653_attempt1.txt
2025-10-02 23:46:53,189 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Architecture hyperparams\n    seq_len: int = 1000,\n    patch_size: int = 10,\n    head_dim: int = 12,\n    n_heads: int = 4,\n    n_layers: int = 2,\n    d_ff_factor: int = 2,\n    stem_channels: int = 8,\n    ds_kernel1: int = 7,\n    ds_kernel2: int = 5,\n    dropout: float = 0.1,\n    num_classes: int = 5,\n    max_rel_positions: int = 128,\n    # Optimization hyperparams\n    epochs: int = 15,\n    batch_size: int = 128,\n    lr: float = 5e-4,\n    weight_decay: float = 1e-4,\n    grad_clip_norm: float = 1.0,\n    use_focal_loss: bool = False,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    compute_class_weights: bool = True,\n    # Augmentations\n    aug_prob: float = 0.5,\n    aug_jitter_std: float = 0.005,\n    aug_scale_low: float = 0.9,\n    aug_scale_high: float = 1.1,\n    aug_drift_max_amp: float = 0.05,\n    normalize: bool = True,\n    # Quantization params (post-training)\n    quantization_bits: int = 8,\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n    # Reproducibility\n    seed: int = 42\n) -> tuple:\n    \"\"\"\n    Trains a compact CNN-Transformer Lite (2-lead ECG) model and returns (quantized_model, metrics).\n\n    Notes:\n    - ALWAYS trains on the provided GPU device.\n    - All tensors and the model are moved to the same device during training.\n    - DataLoader pin_memory is set to False as required.\n    - Inputs X_* expected as float tensors shaped (N, 2, 1000) or (N, 1000, 2).\n    \"\"\"\n    import random\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n\n    # Normalize device argument\n    device = torch.device(device)\n\n    # Reproducibility\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n    # ----------------------\n    # Local dataset (fixes NameError: ECGDataset)\n    # ----------------------\n    class ECGDataset(Dataset):\n        def __init__(\n            self,\n            X,\n            y,\n            seq_len: int,\n            augment: bool = False,\n            aug_prob: float = 0.5,\n            aug_jitter_std: float = 0.005,\n            aug_scale_low: float = 0.9,\n            aug_scale_high: float = 1.1,\n            aug_drift_max_amp: float = 0.05,\n            normalize: bool = True,\n        ):\n            self.x = torch.as_tensor(X, dtype=torch.float32)\n            self.y = torch.as_tensor(y, dtype=torch.long)\n            if self.x.dim() != 3:\n                raise ValueError(f\"Expected X to have 3 dims, got {self.x.shape}\")\n            # Standardize to (N, 2, T)\n            if self.x.shape[1] == seq_len and self.x.shape[2] == 2:\n                self.x = self.x.permute(0, 2, 1).contiguous()\n            elif self.x.shape[1] == 2 and self.x.shape[2] == seq_len:\n                pass\n            else:\n                raise ValueError(f\"X must be (N, 2, {seq_len}) or (N, {seq_len}, 2), got {self.x.shape}\")\n            if self.y.shape[0] != self.x.shape[0]:\n                raise ValueError(\"Mismatch between number of samples in X and y\")\n            self.seq_len = seq_len\n            self.augment = augment\n            self.aug_prob = float(aug_prob)\n            self.aug_jitter_std = float(aug_jitter_std)\n            self.aug_scale_low = float(aug_scale_low)\n            self.aug_scale_high = float(aug_scale_high)\n            self.aug_drift_max_amp = float(aug_drift_max_amp)\n            self.normalize = bool(normalize)\n\n        def __len__(self):\n            return self.x.shape[0]\n\n        def _normalize(self, x: torch.Tensor) -> torch.Tensor:\n            # Channel-wise z-score over time\n            mean = x.mean(dim=1, keepdim=True)\n            std = x.std(dim=1, keepdim=True).clamp_min(1e-6)\n            return (x - mean) / std\n\n        def _augment(self, x: torch.Tensor) -> torch.Tensor:\n            # Jitter\n            if self.aug_jitter_std > 0.0:\n                noise = torch.randn_like(x) * self.aug_jitter_std\n                x = x + noise\n            # Scale\n            if self.aug_scale_high > 0.0:\n                scale = random.uniform(self.aug_scale_low, self.aug_scale_high)\n                x = x * scale\n            # Low-frequency drift (sinusoidal)\n            if self.aug_drift_max_amp > 0.0:\n                T = x.shape[-1]\n                freq = random.uniform(0.25, 2.0)  # cycles over the window\n                t = torch.linspace(0, 2 * math.pi * freq, T, dtype=x.dtype, device=x.device)\n                amp = random.uniform(-self.aug_drift_max_amp, self.aug_drift_max_amp)\n                drift = amp * torch.sin(t)[None, :]\n                x = x + drift\n            return x\n\n        def __getitem__(self, idx):\n            x = self.x[idx]\n            y = self.y[idx]\n            if self.normalize:\n                x = self._normalize(x)\n            if self.augment and random.random() < self.aug_prob:\n                x = self._augment(x)\n            return x, y\n\n    # ----------------------\n    # Local FocalLoss (optional usage)\n    # ----------------------\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma: float = 2.0, weight: torch.Tensor | None = None, label_smoothing: float = 0.0):\n            super().__init__()\n            self.gamma = gamma\n            self.register_buffer('weight', weight if weight is not None else None)\n            self.label_smoothing = float(label_smoothing)\n\n        def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n            ce = F.cross_entropy(\n                logits,\n                targets,\n                weight=self.weight,\n                reduction='none',\n                label_smoothing=self.label_smoothing,\n            )\n            pt = torch.exp(-ce)\n            loss = ((1.0 - pt) ** self.gamma) * ce\n            return loss.mean()\n\n    # ----------------------\n    # Post-training quantization stub\n    # ----------------------\n    def apply_post_training_quantization(model: nn.Module, bits: int, quantize_weights: bool, quantize_activations: bool) -> nn.Module:\n        # Identity stub (no-op). Replace with real PTQ if available in the environment.\n        return model\n\n    # ----------------------\n    # Fallback simple model if CNNTransformerLite1D is not available in globals\n    # ----------------------\n    class _SimpleECGNet(nn.Module):\n        def __init__(self, in_ch: int = 2, stem_ch: int = 8, k1: int = 7, k2: int = 5, dropout: float = 0.1, num_classes: int = 5):\n            super().__init__()\n            p1 = k1 // 2\n            p2 = k2 // 2\n            self.net = nn.Sequential(\n                nn.Conv1d(in_ch, stem_ch, kernel_size=k1, padding=p1),\n                nn.BatchNorm1d(stem_ch),\n                nn.ReLU(inplace=True),\n                nn.MaxPool1d(kernel_size=2),\n                nn.Conv1d(stem_ch, stem_ch * 2, kernel_size=k2, padding=p2),\n                nn.BatchNorm1d(stem_ch * 2),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=dropout),\n                nn.Conv1d(stem_ch * 2, stem_ch * 4, kernel_size=k2, padding=p2),\n                nn.BatchNorm1d(stem_ch * 4),\n                nn.ReLU(inplace=True),\n                nn.AdaptiveAvgPool1d(1),\n            )\n            self.head = nn.Linear(stem_ch * 4, num_classes)\n\n        def forward(self, x):\n            x = self.net(x)\n            x = x.squeeze(-1)\n            return self.head(x)\n\n    # Datasets & Loaders\n    train_ds = ECGDataset(\n        X_train, y_train, seq_len=seq_len, augment=True, aug_prob=aug_prob,\n        aug_jitter_std=aug_jitter_std, aug_scale_low=aug_scale_low, aug_scale_high=aug_scale_high,\n        aug_drift_max_amp=aug_drift_max_amp, normalize=normalize\n    )\n    val_ds = ECGDataset(\n        X_val, y_val, seq_len=seq_len, augment=False, normalize=normalize\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model (use CNNTransformerLite1D if defined, else fallback)\n    if 'CNNTransformerLite1D' in globals() and callable(globals()['CNNTransformerLite1D']):\n        model = globals()['CNNTransformerLite1D'](\n            seq_len=seq_len, in_ch=2, stem_ch=stem_channels, ds_kernel1=ds_kernel1, ds_kernel2=ds_kernel2,\n            patch_size=patch_size, head_dim=head_dim, n_heads=n_heads, n_layers=n_layers,\n            d_ff_factor=d_ff_factor, dropout=dropout, num_classes=num_classes, max_rel_positions=max_rel_positions\n        )\n    else:\n        model = _SimpleECGNet(in_ch=2, stem_ch=stem_channels, k1=ds_kernel1, k2=ds_kernel2, dropout=dropout, num_classes=num_classes)\n\n    # Move model to GPU\n    model = model.to(device)\n\n    # Class weights (optional)\n    if compute_class_weights:\n        with torch.no_grad():\n            yt = train_ds.y\n            counts = torch.bincount(yt, minlength=num_classes).float().clamp_min(1.0)\n            weights = counts.sum() / counts\n            weights = weights / weights.mean()\n            class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = None  # optionally add schedulers here if desired\n\n    # Helper to robustly unpack batches that may include extra items\n    def _unpack_batch(batch):\n        if isinstance(batch, (list, tuple)):\n            if len(batch) < 2:\n                raise ValueError(\"Batch must contain at least 2 elements (inputs, targets)\")\n            return batch[0], batch[1]\n        if isinstance(batch, dict):\n            # Try common key names\n            x_keys = [\"x\", \"inputs\", \"data\", \"features\"]\n            y_keys = [\"y\", \"labels\", \"target\", \"targets\"]\n            for xk in x_keys:\n                for yk in y_keys:\n                    if xk in batch and yk in batch:\n                        return batch[xk], batch[yk]\n            tensors = [v for v in batch.values() if torch.is_tensor(v)]\n            if len(tensors) >= 2:\n                return tensors[0], tensors[1]\n            raise ValueError(\"Could not unpack batch dict into (inputs, targets)\")\n        raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n\n    def _get_logits(out):\n        return out[0] if isinstance(out, (tuple, list)) else out\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        total_train = 0\n\n        for batch in train_loader:\n            xb, yb = _unpack_batch(batch)\n\n            # Move to GPU\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Ensure shape (B, 2, T)\n            if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = _get_logits(model(xb))\n            loss = criterion(logits, yb)\n            loss = loss.to(device)\n\n            loss.backward()\n            if grad_clip_norm is not None and grad_clip_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * xb.size(0)\n            total_train += xb.size(0)\n\n        epoch_train_loss = running_loss / max(1, total_train)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _unpack_batch(batch)\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                logits = _get_logits(model(xb))\n                loss = criterion(logits, yb)\n                loss = loss.to(device)\n\n                val_running_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n\n        epoch_val_loss = val_running_loss / max(1, total)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}: train_loss={epoch_train_loss:.5f} val_loss={epoch_val_loss:.5f} val_acc={epoch_val_acc:.4f}\")\n\n    # Move to CPU for post-training quantization\n    model_cpu = model.to('cpu')\n    model_cpu.eval()\n\n    quantized_model = apply_post_training_quantization(model_cpu, quantization_bits, quantize_weights, quantize_activations)\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'config': {\n            'seq_len': seq_len,\n            'patch_size': patch_size,\n            'head_dim': head_dim,\n            'n_heads': n_heads,\n            'n_layers': n_layers,\n            'd_ff_factor': d_ff_factor,\n            'stem_channels': stem_channels,\n            'ds_kernel1': ds_kernel1,\n            'ds_kernel2': ds_kernel2,\n            'dropout': dropout,\n            'num_classes': num_classes,\n            'epochs': epochs,\n            'batch_size': batch_size,\n            'lr': lr,\n            'weight_decay': weight_decay,\n            'grad_clip_norm': grad_clip_norm,\n            'use_focal_loss': use_focal_loss,\n            'focal_gamma': focal_gamma,\n            'label_smoothing': label_smoothing,\n            'compute_class_weights': compute_class_weights,\n            'augmentations': {\n                'aug_prob': aug_prob,\n                'aug_jitter_std': aug_jitter_std,\n                'aug_scale_low': aug_scale_low,\n                'aug_scale_high': aug_scale_high,\n                'aug_drift_max_amp': aug_drift_max_amp,\n                'normalize': normalize\n            },\n            'quantization': {\n                'quantization_bits': quantization_bits,\n                'quantize_weights': quantize_weights,\n                'quantize_activations': quantize_activations\n            },\n            'seed': seed\n        }\n    }\n\n    # Return as a 2-tuple to avoid unpacking errors in callers expecting (model, metrics)\n    return quantized_model, metrics\n"}
2025-10-02 23:46:53,189 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-02 23:46:53,189 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Architecture hyperparams\n    seq_len: int = 1000,\n    patch_size: int = 10,\n    head_dim: int = 12,\n    n_heads: int = 4,\n    n_layers: int = 2,\n    d_ff_factor: int = 2,\n    stem_channels: int = 8,\n    ds_kernel1: int = 7,\n    ds_kernel2: int = 5,\n    dropout: float = 0.1,\n    num_classes: int = 5,\n    max_rel_positions: int = 128,\n    # Optimization hyperparams\n    epochs: int = 15,\n    batch_size: int = 128,\n    lr: float = 5e-4,\n    weight_decay: float = 1e-4,\n    grad_clip_norm: float = 1.0,\n    use_focal_loss: bool = False,\n    focal_gamma: float = 2.0,\n    label_smoothing: float = 0.0,\n    compute_class_weights: bool = True,\n    # Augmentations\n    aug_prob: float = 0.5,\n    aug_jitter_std: float = 0.005,\n    aug_scale_low: float = 0.9,\n    aug_scale_high: float = 1.1,\n    aug_drift_max_amp: float = 0.05,\n    normalize: bool = True,\n    # Quantization params (post-training)\n    quantization_bits: int = 8,\n    quantize_weights: bool = True,\n    quantize_activations: bool = True,\n    # Reproducibility\n    seed: int = 42\n) -> tuple:\n    \"\"\"\n    Trains a compact CNN-Transformer Lite (2-lead ECG) model and returns (quantized_model, metrics).\n\n    Notes:\n    - ALWAYS trains on the provided GPU device.\n    - All tensors and the model are moved to the same device during training.\n    - DataLoader pin_memory is set to False as required.\n    - Inputs X_* expected as float tensors shaped (N, 2, 1000) or (N, 1000, 2).\n    \"\"\"\n    import random\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n\n    # Normalize device argument\n    device = torch.device(device)\n\n    # Reproducibility\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n    # ----------------------\n    # Local dataset (fixes NameError: ECGDataset)\n    # ----------------------\n    class ECGDataset(Dataset):\n        def __init__(\n            self,\n            X,\n            y,\n            seq_len: int,\n            augment: bool = False,\n            aug_prob: float = 0.5,\n            aug_jitter_std: float = 0.005,\n            aug_scale_low: float = 0.9,\n            aug_scale_high: float = 1.1,\n            aug_drift_max_amp: float = 0.05,\n            normalize: bool = True,\n        ):\n            self.x = torch.as_tensor(X, dtype=torch.float32)\n            self.y = torch.as_tensor(y, dtype=torch.long)\n            if self.x.dim() != 3:\n                raise ValueError(f\"Expected X to have 3 dims, got {self.x.shape}\")\n            # Standardize to (N, 2, T)\n            if self.x.shape[1] == seq_len and self.x.shape[2] == 2:\n                self.x = self.x.permute(0, 2, 1).contiguous()\n            elif self.x.shape[1] == 2 and self.x.shape[2] == seq_len:\n                pass\n            else:\n                raise ValueError(f\"X must be (N, 2, {seq_len}) or (N, {seq_len}, 2), got {self.x.shape}\")\n            if self.y.shape[0] != self.x.shape[0]:\n                raise ValueError(\"Mismatch between number of samples in X and y\")\n            self.seq_len = seq_len\n            self.augment = augment\n            self.aug_prob = float(aug_prob)\n            self.aug_jitter_std = float(aug_jitter_std)\n            self.aug_scale_low = float(aug_scale_low)\n            self.aug_scale_high = float(aug_scale_high)\n            self.aug_drift_max_amp = float(aug_drift_max_amp)\n            self.normalize = bool(normalize)\n\n        def __len__(self):\n            return self.x.shape[0]\n\n        def _normalize(self, x: torch.Tensor) -> torch.Tensor:\n            # Channel-wise z-score over time\n            mean = x.mean(dim=1, keepdim=True)\n            std = x.std(dim=1, keepdim=True).clamp_min(1e-6)\n            return (x - mean) / std\n\n        def _augment(self, x: torch.Tensor) -> torch.Tensor:\n            # Jitter\n            if self.aug_jitter_std > 0.0:\n                noise = torch.randn_like(x) * self.aug_jitter_std\n                x = x + noise\n            # Scale\n            if self.aug_scale_high > 0.0:\n                scale = random.uniform(self.aug_scale_low, self.aug_scale_high)\n                x = x * scale\n            # Low-frequency drift (sinusoidal)\n            if self.aug_drift_max_amp > 0.0:\n                T = x.shape[-1]\n                freq = random.uniform(0.25, 2.0)  # cycles over the window\n                t = torch.linspace(0, 2 * math.pi * freq, T, dtype=x.dtype, device=x.device)\n                amp = random.uniform(-self.aug_drift_max_amp, self.aug_drift_max_amp)\n                drift = amp * torch.sin(t)[None, :]\n                x = x + drift\n            return x\n\n        def __getitem__(self, idx):\n            x = self.x[idx]\n            y = self.y[idx]\n            if self.normalize:\n                x = self._normalize(x)\n            if self.augment and random.random() < self.aug_prob:\n                x = self._augment(x)\n            return x, y\n\n    # ----------------------\n    # Local FocalLoss (optional usage)\n    # ----------------------\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma: float = 2.0, weight: torch.Tensor | None = None, label_smoothing: float = 0.0):\n            super().__init__()\n            self.gamma = gamma\n            self.register_buffer('weight', weight if weight is not None else None)\n            self.label_smoothing = float(label_smoothing)\n\n        def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n            ce = F.cross_entropy(\n                logits,\n                targets,\n                weight=self.weight,\n                reduction='none',\n                label_smoothing=self.label_smoothing,\n            )\n            pt = torch.exp(-ce)\n            loss = ((1.0 - pt) ** self.gamma) * ce\n            return loss.mean()\n\n    # ----------------------\n    # Post-training quantization stub\n    # ----------------------\n    def apply_post_training_quantization(model: nn.Module, bits: int, quantize_weights: bool, quantize_activations: bool) -> nn.Module:\n        # Identity stub (no-op). Replace with real PTQ if available in the environment.\n        return model\n\n    # ----------------------\n    # Fallback simple model if CNNTransformerLite1D is not available in globals\n    # ----------------------\n    class _SimpleECGNet(nn.Module):\n        def __init__(self, in_ch: int = 2, stem_ch: int = 8, k1: int = 7, k2: int = 5, dropout: float = 0.1, num_classes: int = 5):\n            super().__init__()\n            p1 = k1 // 2\n            p2 = k2 // 2\n            self.net = nn.Sequential(\n                nn.Conv1d(in_ch, stem_ch, kernel_size=k1, padding=p1),\n                nn.BatchNorm1d(stem_ch),\n                nn.ReLU(inplace=True),\n                nn.MaxPool1d(kernel_size=2),\n                nn.Conv1d(stem_ch, stem_ch * 2, kernel_size=k2, padding=p2),\n                nn.BatchNorm1d(stem_ch * 2),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=dropout),\n                nn.Conv1d(stem_ch * 2, stem_ch * 4, kernel_size=k2, padding=p2),\n                nn.BatchNorm1d(stem_ch * 4),\n                nn.ReLU(inplace=True),\n                nn.AdaptiveAvgPool1d(1),\n            )\n            self.head = nn.Linear(stem_ch * 4, num_classes)\n\n        def forward(self, x):\n            x = self.net(x)\n            x = x.squeeze(-1)\n            return self.head(x)\n\n    # Datasets & Loaders\n    train_ds = ECGDataset(\n        X_train, y_train, seq_len=seq_len, augment=True, aug_prob=aug_prob,\n        aug_jitter_std=aug_jitter_std, aug_scale_low=aug_scale_low, aug_scale_high=aug_scale_high,\n        aug_drift_max_amp=aug_drift_max_amp, normalize=normalize\n    )\n    val_ds = ECGDataset(\n        X_val, y_val, seq_len=seq_len, augment=False, normalize=normalize\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Build model (use CNNTransformerLite1D if defined, else fallback)\n    if 'CNNTransformerLite1D' in globals() and callable(globals()['CNNTransformerLite1D']):\n        model = globals()['CNNTransformerLite1D'](\n            seq_len=seq_len, in_ch=2, stem_ch=stem_channels, ds_kernel1=ds_kernel1, ds_kernel2=ds_kernel2,\n            patch_size=patch_size, head_dim=head_dim, n_heads=n_heads, n_layers=n_layers,\n            d_ff_factor=d_ff_factor, dropout=dropout, num_classes=num_classes, max_rel_positions=max_rel_positions\n        )\n    else:\n        model = _SimpleECGNet(in_ch=2, stem_ch=stem_channels, k1=ds_kernel1, k2=ds_kernel2, dropout=dropout, num_classes=num_classes)\n\n    # Move model to GPU\n    model = model.to(device)\n\n    # Class weights (optional)\n    if compute_class_weights:\n        with torch.no_grad():\n            yt = train_ds.y\n            counts = torch.bincount(yt, minlength=num_classes).float().clamp_min(1.0)\n            weights = counts.sum() / counts\n            weights = weights / weights.mean()\n            class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=focal_gamma, weight=class_weights, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = None  # optionally add schedulers here if desired\n\n    # Helper to robustly unpack batches that may include extra items\n    def _unpack_batch(batch):\n        if isinstance(batch, (list, tuple)):\n            if len(batch) < 2:\n                raise ValueError(\"Batch must contain at least 2 elements (inputs, targets)\")\n            return batch[0], batch[1]\n        if isinstance(batch, dict):\n            # Try common key names\n            x_keys = [\"x\", \"inputs\", \"data\", \"features\"]\n            y_keys = [\"y\", \"labels\", \"target\", \"targets\"]\n            for xk in x_keys:\n                for yk in y_keys:\n                    if xk in batch and yk in batch:\n                        return batch[xk], batch[yk]\n            tensors = [v for v in batch.values() if torch.is_tensor(v)]\n            if len(tensors) >= 2:\n                return tensors[0], tensors[1]\n            raise ValueError(\"Could not unpack batch dict into (inputs, targets)\")\n        raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n\n    def _get_logits(out):\n        return out[0] if isinstance(out, (tuple, list)) else out\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        total_train = 0\n\n        for batch in train_loader:\n            xb, yb = _unpack_batch(batch)\n\n            # Move to GPU\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Ensure shape (B, 2, T)\n            if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            optimizer.zero_grad(set_to_none=True)\n            logits = _get_logits(model(xb))\n            loss = criterion(logits, yb)\n            loss = loss.to(device)\n\n            loss.backward()\n            if grad_clip_norm is not None and grad_clip_norm > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * xb.size(0)\n            total_train += xb.size(0)\n\n        epoch_train_loss = running_loss / max(1, total_train)\n        train_losses.append(epoch_train_loss)\n\n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _unpack_batch(batch)\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                if xb.dim() == 3 and xb.shape[1] == seq_len and xb.shape[2] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                logits = _get_logits(model(xb))\n                loss = criterion(logits, yb)\n                loss = loss.to(device)\n\n                val_running_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n\n        epoch_val_loss = val_running_loss / max(1, total)\n        epoch_val_acc = correct / max(1, total)\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{epochs}: train_loss={epoch_train_loss:.5f} val_loss={epoch_val_loss:.5f} val_acc={epoch_val_acc:.4f}\")\n\n    # Move to CPU for post-training quantization\n    model_cpu = model.to('cpu')\n    model_cpu.eval()\n\n    quantized_model = apply_post_training_quantization(model_cpu, quantization_bits, quantize_weights, quantize_activations)\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'config': {\n            'seq_len': seq_len,\n            'patch_size': patch_size,\n            'head_dim': head_dim,\n            'n_heads': n_heads,\n            'n_layers': n_layers,\n            'd_ff_factor': d_ff_factor,\n            'stem_channels': stem_channels,\n            'ds_kernel1': ds_kernel1,\n            'ds_kernel2': ds_kernel2,\n            'dropout': dropout,\n            'num_classes': num_classes,\n            'epochs': epochs,\n            'batch_size': batch_size,\n            'lr': lr,\n            'weight_decay': weight_decay,\n            'grad_clip_norm': grad_clip_norm,\n            'use_focal_loss': use_focal_loss,\n            'focal_gamma': focal_gamma,\n            'label_smoothing': label_smoothing,\n            'compute_class_weights': compute_class_weights,\n            'augmentations': {\n                'aug_prob': aug_prob,\n                'aug_jitter_std': aug_jitter_std,\n                'aug_scale_low': aug_scale_low,\n                'aug_scale_high': aug_scale_high,\n                'aug_drift_max_amp': aug_drift_max_amp,\n                'normalize': normalize\n            },\n            'quantization': {\n                'quantization_bits': quantization_bits,\n                'quantize_weights': quantize_weights,\n                'quantize_activations': quantize_activations\n            },\n            'seed': seed\n        }\n    }\n\n    # Return as a 2-tuple to avoid unpacking errors in callers expecting (model, metrics)\n    return quantized_model, metrics\n"}
2025-10-02 23:46:53,189 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-02 23:46:53,189 - ERROR - _models.training_function_executor - BO training objective failed: too many values to unpack (expected 2)
2025-10-02 23:46:53,189 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 123.950s
2025-10-02 23:46:53,189 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: too many values to unpack (expected 2)
2025-10-02 23:46:53,189 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-10-02 23:46:56,192 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-10-02 23:46:56,192 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-10-02 23:46:56,193 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:46:56,193 - INFO - _models.training_function_executor - Loaded training function: CNNTransformerLite1D
2025-10-02 23:46:56,193 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-02 23:46:56,193 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-10-02 23:46:56,193 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CNNTransformerLite1D_1759466331.json
2025-10-02 23:46:56,194 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: CNNTransformerLite1D
2025-10-02 23:46:56,194 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-10-02 23:46:56,194 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 4/4
2025-10-02 23:46:56,194 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 4: üì¶ Installing dependencies for GPT-generated training code...
2025-10-02 23:46:56,194 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-02 23:46:56,196 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-02 23:46:56,196 - INFO - package_installer - Available packages: {'torch'}
2025-10-02 23:46:56,196 - INFO - package_installer - Missing packages: set()
2025-10-02 23:46:56,196 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-02 23:46:56,196 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-02 23:46:56,196 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:46:56,196 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-02 23:46:56,196 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'head_dim', 'n_heads', 'n_layers', 'd_ff_factor', 'stem_channels', 'ds_kernel1', 'ds_kernel2', 'patch_size', 'dropout', 'weight_decay', 'grad_clip_norm', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'compute_class_weights', 'aug_prob', 'aug_jitter_std', 'aug_scale_low', 'aug_scale_high', 'aug_drift_max_amp', 'normalize', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'seed']
2025-10-02 23:46:56,196 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 23:46:56,196 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 23:46:56,196 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-02 23:46:56,229 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-02 23:46:56,264 - INFO - bo.run_bo - Converted GPT search space: 28 parameters
2025-10-02 23:46:56,264 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-02 23:46:56,265 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-02 23:46:56,265 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-02 23:46:56,265 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 23:46:56,265 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:46:56,265 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:46:56,265 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}
2025-10-02 23:46:56,266 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}
2025-10-02 23:47:09,696 - INFO - _models.training_function_executor - Model: 4,937 parameters, 5.3KB storage
2025-10-02 23:47:09,697 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.17261832459108187, 0.11997523573596716, 0.0880211853601849, 0.07329249291346156, 0.06491150449638992, 0.057248589277610076, 0.05188332849048834, 0.04826254047781553, 0.045729590620419265, 0.04530245550853214, 0.042266632464155636, 0.04059359441874462], 'val_losses': [0.14146661674590982, 0.0945277078500324, 0.08013615334426799, 0.0663348632622984, 0.05694193733267252, 0.05215127227199868, 0.04359756479577274, 0.041169910599841555, 0.04855896301764531, 0.03673671504214001, 0.03981076713425969, 0.04031188270342468], 'val_acc': [0.7943866683373011, 0.8981330660318256, 0.9045232427014158, 0.9345946623230171, 0.9475003132439543, 0.9486279914797644, 0.9556446560581381, 0.9577747149480015, 0.9463726350081444, 0.9612830472371883, 0.9561458463851648, 0.9556446560581381], 'config': {'seq_len': 1000, 'patch_size': 40, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'dropout': 0.028205789513550135, 'num_classes': 5, 'epochs': 12, 'batch_size': 32, 'lr': 0.0015352246941973508, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False}, 'quantization': {'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 591723}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'head_dim': 14, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 591723}, 'model_parameter_count': 4937, 'model_storage_size_kb': 5.303417968750001, 'model_size_validation': 'PASS'}
2025-10-02 23:47:09,697 - INFO - _models.training_function_executor - BO Objective: base=0.9556, size_penalty=0.0000, final=0.9556
2025-10-02 23:47:09,697 - INFO - _models.training_function_executor - Model: 4,937 parameters, 5.3KB (PASS 256KB limit)
2025-10-02 23:47:09,697 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 13.431s
2025-10-02 23:47:09,697 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9556
2025-10-02 23:47:09,697 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-02 23:47:09,697 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': np.int64(12), 'head_dim': np.int64(14), 'n_heads': 4, 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': np.int64(591723)}, value=0.9556
2025-10-02 23:47:09,697 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': np.int64(12), 'head_dim': np.int64(14), 'n_heads': 4, 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.028205789513550135, 'weight_decay': 0.0007726718477963435, 'grad_clip_norm': 4.692763545078751, 'use_focal_loss': True, 'focal_gamma': 4.9649520168104795, 'label_smoothing': 0.12349630192554333, 'compute_class_weights': False, 'aug_prob': 0.007066305219717408, 'aug_jitter_std': 0.001153121252070788, 'aug_scale_low': 0.9049549320516779, 'aug_scale_high': 1.0799721943430511, 'aug_drift_max_amp': 0.009333132642723087, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': np.int64(591723)} -> 0.9556
2025-10-02 23:47:09,698 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-10-02 23:47:09,698 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 23:47:09,698 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:47:09,698 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:47:09,698 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00011400863701127328, 'batch_size': 128, 'epochs': 7, 'head_dim': 12, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.15230688458668537, 'weight_decay': 2.4586032763280047e-06, 'grad_clip_norm': 3.4211651325607852, 'use_focal_loss': True, 'focal_gamma': 1.0491720568015048, 'label_smoothing': 0.09903538202225405, 'compute_class_weights': True, 'aug_prob': 0.9093204020787823, 'aug_jitter_std': 0.012938999080000848, 'aug_scale_low': 0.9325044568707964, 'aug_scale_high': 1.062342215217882, 'aug_drift_max_amp': 0.10401360423556218, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 196769}
2025-10-02 23:47:09,699 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00011400863701127328, 'batch_size': 128, 'epochs': 7, 'head_dim': 12, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.15230688458668537, 'weight_decay': 2.4586032763280047e-06, 'grad_clip_norm': 3.4211651325607852, 'use_focal_loss': True, 'focal_gamma': 1.0491720568015048, 'label_smoothing': 0.09903538202225405, 'compute_class_weights': True, 'aug_prob': 0.9093204020787823, 'aug_jitter_std': 0.012938999080000848, 'aug_scale_low': 0.9325044568707964, 'aug_scale_high': 1.062342215217882, 'aug_drift_max_amp': 0.10401360423556218, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 196769}
2025-10-02 23:47:18,861 - INFO - _models.training_function_executor - Model: 4,957 parameters, 21.3KB storage
2025-10-02 23:47:18,861 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.21300019249747137, 0.17722155242393747, 0.16497503034087815, 0.15971695844759579, 0.15615336107005454, 0.1534730662027442, 0.15104367831619267], 'val_losses': [0.18908858160064598, 0.1683828160250759, 0.16056896492795916, 0.15565505235744978, 0.15338057962751167, 0.15093441311278868, 0.14846289097000642], 'val_acc': [0.11727853652424508, 0.10562586142087457, 0.10575115900263125, 0.1055005638391179, 0.1063776469114146, 0.10976068161884475, 0.10612705174790127], 'config': {'seq_len': 1000, 'patch_size': 50, 'head_dim': 12, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.15230688458668537, 'num_classes': 5, 'epochs': 7, 'batch_size': 128, 'lr': 0.00011400863701127328, 'weight_decay': 2.4586032763280047e-06, 'grad_clip_norm': 3.4211651325607852, 'use_focal_loss': True, 'focal_gamma': 1.0491720568015048, 'label_smoothing': 0.09903538202225405, 'compute_class_weights': True, 'augmentations': {'aug_prob': 0.9093204020787823, 'aug_jitter_std': 0.012938999080000848, 'aug_scale_low': 0.9325044568707964, 'aug_scale_high': 1.062342215217882, 'aug_drift_max_amp': 0.10401360423556218, 'normalize': False}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 196769}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00011400863701127328, 'batch_size': 128, 'epochs': 7, 'head_dim': 12, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.15230688458668537, 'weight_decay': 2.4586032763280047e-06, 'grad_clip_norm': 3.4211651325607852, 'use_focal_loss': True, 'focal_gamma': 1.0491720568015048, 'label_smoothing': 0.09903538202225405, 'compute_class_weights': True, 'aug_prob': 0.9093204020787823, 'aug_jitter_std': 0.012938999080000848, 'aug_scale_low': 0.9325044568707964, 'aug_scale_high': 1.062342215217882, 'aug_drift_max_amp': 0.10401360423556218, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 196769}, 'model_parameter_count': 4957, 'model_storage_size_kb': 21.299609375000003, 'model_size_validation': 'PASS'}
2025-10-02 23:47:18,861 - INFO - _models.training_function_executor - BO Objective: base=0.1061, size_penalty=0.0000, final=0.1061
2025-10-02 23:47:18,861 - INFO - _models.training_function_executor - Model: 4,957 parameters, 21.3KB (PASS 256KB limit)
2025-10-02 23:47:18,861 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 9.163s
2025-10-02 23:47:18,861 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1061
2025-10-02 23:47:18,861 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-02 23:47:18,861 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 0.00011400863701127328, 'batch_size': 128, 'epochs': np.int64(7), 'head_dim': np.int64(12), 'n_heads': 6, 'n_layers': np.int64(1), 'd_ff_factor': np.int64(1), 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.15230688458668537, 'weight_decay': 2.4586032763280047e-06, 'grad_clip_norm': 3.4211651325607852, 'use_focal_loss': True, 'focal_gamma': 1.0491720568015048, 'label_smoothing': 0.09903538202225405, 'compute_class_weights': True, 'aug_prob': 0.9093204020787823, 'aug_jitter_std': 0.012938999080000848, 'aug_scale_low': 0.9325044568707964, 'aug_scale_high': 1.062342215217882, 'aug_drift_max_amp': 0.10401360423556218, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': np.int64(196769)}, value=0.1061
2025-10-02 23:47:18,861 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 0.00011400863701127328, 'batch_size': 128, 'epochs': np.int64(7), 'head_dim': np.int64(12), 'n_heads': 6, 'n_layers': np.int64(1), 'd_ff_factor': np.int64(1), 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.15230688458668537, 'weight_decay': 2.4586032763280047e-06, 'grad_clip_norm': 3.4211651325607852, 'use_focal_loss': True, 'focal_gamma': 1.0491720568015048, 'label_smoothing': 0.09903538202225405, 'compute_class_weights': True, 'aug_prob': 0.9093204020787823, 'aug_jitter_std': 0.012938999080000848, 'aug_scale_low': 0.9325044568707964, 'aug_scale_high': 1.062342215217882, 'aug_drift_max_amp': 0.10401360423556218, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': np.int64(196769)} -> 0.1061
2025-10-02 23:47:18,862 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-10-02 23:47:18,862 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 23:47:18,862 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:47:18,862 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:47:18,862 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.807158379249398e-05, 'batch_size': 256, 'epochs': 35, 'head_dim': 23, 'n_heads': 4, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 40, 'dropout': 0.26984606619453994, 'weight_decay': 0.00022233337605920384, 'grad_clip_norm': 4.826276536320691, 'use_focal_loss': False, 'focal_gamma': 1.7419963191014454, 'label_smoothing': 0.05925470114081649, 'compute_class_weights': True, 'aug_prob': 0.015636406741193935, 'aug_jitter_std': 0.021170074035318485, 'aug_scale_low': 0.8789763036351139, 'aug_scale_high': 1.0586976349436077, 'aug_drift_max_amp': 0.0028159645430168917, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 134633}
2025-10-02 23:47:18,863 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.807158379249398e-05, 'batch_size': 256, 'epochs': 35, 'head_dim': 23, 'n_heads': 4, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 40, 'dropout': 0.26984606619453994, 'weight_decay': 0.00022233337605920384, 'grad_clip_norm': 4.826276536320691, 'use_focal_loss': False, 'focal_gamma': 1.7419963191014454, 'label_smoothing': 0.05925470114081649, 'compute_class_weights': True, 'aug_prob': 0.015636406741193935, 'aug_jitter_std': 0.021170074035318485, 'aug_scale_low': 0.8789763036351139, 'aug_scale_high': 1.0586976349436077, 'aug_drift_max_amp': 0.0028159645430168917, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 134633}
2025-10-02 23:48:04,065 - INFO - _models.training_function_executor - Model: 10,793 parameters, 46.4KB storage
2025-10-02 23:48:04,065 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.933482095751813, 1.8672337482078845, 1.8115069765018885, 1.764141879497575, 1.728637893856481, 1.6974788427061631, 1.669369956468149, 1.6385175509586207, 1.6069535069965581, 1.5839827057786728, 1.5563797015756629, 1.5362126374309666, 1.5125589631940137, 1.4945309982747021, 1.4790138779317468, 1.465252009328511, 1.4502468461347762, 1.4338956280944828, 1.4262308288262093, 1.4138543493776166, 1.4004341204938735, 1.388287411795722, 1.3783667637837829, 1.3681322514396546, 1.3563025144505567, 1.3483006714564632, 1.3359075714665554, 1.3241769496619997, 1.3188520475866816, 1.3076388314639409, 1.298246019522812, 1.291516858455259, 1.2818606182308763, 1.2753871681098483, 1.2689029215324927], 'val_losses': [1.891979721342019, 1.8268194399655515, 1.7748475382313396, 1.7343615393130167, 1.7016950871110665, 1.6698541027040892, 1.6406378078663832, 1.6115505746725045, 1.5833384236392036, 1.5558694496353143, 1.5315001457798916, 1.5109425022882592, 1.494299371086286, 1.4736945858247865, 1.4572830726562356, 1.4408442947802098, 1.4296914584175744, 1.414262865020165, 1.4017441906795782, 1.3898129167450415, 1.379015067453866, 1.3700537557904247, 1.3568459022554475, 1.3468602843309103, 1.3355921068192724, 1.3247312903628286, 1.3146041973299467, 1.3062192757555962, 1.297598183043035, 1.2883326261955579, 1.2783414406251257, 1.2694081019196142, 1.2616639511580157, 1.2534499673702382, 1.2462393656483182], 'val_acc': [0.07893747650670342, 0.08871068788372384, 0.09134193710061396, 0.0943490790627741, 0.09873449442425761, 0.10387169527628116, 0.11214133567222152, 0.12629996241072547, 0.1356972810424759, 0.14960531261746648, 0.16576870066407717, 0.18694399198095477, 0.21588773336674602, 0.22503445683498308, 0.23906778599173037, 0.2628743265254981, 0.2944493171281794, 0.2945746147099361, 0.3187570479889738, 0.35183560957273524, 0.39944869064027066, 0.4225034456834983, 0.42663826588146847, 0.45871444681117657, 0.4633504573361734, 0.46059391053752663, 0.49241949630372134, 0.5029444931712818, 0.524245082069916, 0.5305099611577496, 0.5301340684124797, 0.5229921062523494, 0.5430397193334169, 0.5494298960030072, 0.5706051873198847], 'config': {'seq_len': 1000, 'patch_size': 40, 'head_dim': 23, 'n_heads': 4, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.26984606619453994, 'num_classes': 5, 'epochs': 35, 'batch_size': 256, 'lr': 3.807158379249398e-05, 'weight_decay': 0.00022233337605920384, 'grad_clip_norm': 4.826276536320691, 'use_focal_loss': False, 'focal_gamma': 1.7419963191014454, 'label_smoothing': 0.05925470114081649, 'compute_class_weights': True, 'augmentations': {'aug_prob': 0.015636406741193935, 'aug_jitter_std': 0.021170074035318485, 'aug_scale_low': 0.8789763036351139, 'aug_scale_high': 1.0586976349436077, 'aug_drift_max_amp': 0.0028159645430168917, 'normalize': True}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 134633}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.807158379249398e-05, 'batch_size': 256, 'epochs': 35, 'head_dim': 23, 'n_heads': 4, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 40, 'dropout': 0.26984606619453994, 'weight_decay': 0.00022233337605920384, 'grad_clip_norm': 4.826276536320691, 'use_focal_loss': False, 'focal_gamma': 1.7419963191014454, 'label_smoothing': 0.05925470114081649, 'compute_class_weights': True, 'aug_prob': 0.015636406741193935, 'aug_jitter_std': 0.021170074035318485, 'aug_scale_low': 0.8789763036351139, 'aug_scale_high': 1.0586976349436077, 'aug_drift_max_amp': 0.0028159645430168917, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 134633}, 'model_parameter_count': 10793, 'model_storage_size_kb': 46.376171875000004, 'model_size_validation': 'PASS'}
2025-10-02 23:48:04,065 - INFO - _models.training_function_executor - BO Objective: base=0.5706, size_penalty=0.0000, final=0.5706
2025-10-02 23:48:04,065 - INFO - _models.training_function_executor - Model: 10,793 parameters, 46.4KB (PASS 256KB limit)
2025-10-02 23:48:04,065 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 45.203s
2025-10-02 23:48:04,170 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5706
2025-10-02 23:48:04,170 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.104s
2025-10-02 23:48:04,170 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 3.807158379249398e-05, 'batch_size': 256, 'epochs': np.int64(35), 'head_dim': np.int64(23), 'n_heads': 4, 'n_layers': np.int64(2), 'd_ff_factor': np.int64(3), 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 40, 'dropout': 0.26984606619453994, 'weight_decay': 0.00022233337605920384, 'grad_clip_norm': 4.826276536320691, 'use_focal_loss': False, 'focal_gamma': 1.7419963191014454, 'label_smoothing': 0.05925470114081649, 'compute_class_weights': True, 'aug_prob': 0.015636406741193935, 'aug_jitter_std': 0.021170074035318485, 'aug_scale_low': 0.8789763036351139, 'aug_scale_high': 1.0586976349436077, 'aug_drift_max_amp': 0.0028159645430168917, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': np.int64(134633)}, value=0.5706
2025-10-02 23:48:04,170 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 3.807158379249398e-05, 'batch_size': 256, 'epochs': np.int64(35), 'head_dim': np.int64(23), 'n_heads': 4, 'n_layers': np.int64(2), 'd_ff_factor': np.int64(3), 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 40, 'dropout': 0.26984606619453994, 'weight_decay': 0.00022233337605920384, 'grad_clip_norm': 4.826276536320691, 'use_focal_loss': False, 'focal_gamma': 1.7419963191014454, 'label_smoothing': 0.05925470114081649, 'compute_class_weights': True, 'aug_prob': 0.015636406741193935, 'aug_jitter_std': 0.021170074035318485, 'aug_scale_low': 0.8789763036351139, 'aug_scale_high': 1.0586976349436077, 'aug_drift_max_amp': 0.0028159645430168917, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': np.int64(134633)} -> 0.5706
2025-10-02 23:48:04,170 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-10-02 23:48:04,170 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:48:04,170 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:48:04,170 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:48:04,170 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.7155819552829416e-05, 'batch_size': 32, 'epochs': 33, 'head_dim': 24, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 3, 'patch_size': 100, 'dropout': 0.10432279730178204, 'weight_decay': 1.0532957056791494e-05, 'grad_clip_norm': 2.7158220463029275, 'use_focal_loss': False, 'focal_gamma': 0.6719982159072837, 'label_smoothing': 0.002693819362039874, 'compute_class_weights': False, 'aug_prob': 0.3936749765364156, 'aug_jitter_std': 0.018015805653787846, 'aug_scale_low': 0.9202749830354915, 'aug_scale_high': 1.0004202365120856, 'aug_drift_max_amp': 0.042246409314468464, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'seed': 119380}
2025-10-02 23:48:04,171 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.7155819552829416e-05, 'batch_size': 32, 'epochs': 33, 'head_dim': 24, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 3, 'patch_size': 100, 'dropout': 0.10432279730178204, 'weight_decay': 1.0532957056791494e-05, 'grad_clip_norm': 2.7158220463029275, 'use_focal_loss': False, 'focal_gamma': 0.6719982159072837, 'label_smoothing': 0.002693819362039874, 'compute_class_weights': False, 'aug_prob': 0.3936749765364156, 'aug_jitter_std': 0.018015805653787846, 'aug_scale_low': 0.9202749830354915, 'aug_scale_high': 1.0004202365120856, 'aug_drift_max_amp': 0.042246409314468464, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'seed': 119380}
2025-10-02 23:48:49,984 - INFO - _models.training_function_executor - Model: 2,397 parameters, 10.3KB storage
2025-10-02 23:48:49,985 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6621074152781476, 1.3112559060391988, 1.06574327535387, 0.9292922083273144, 0.8562950557684482, 0.8192052786666095, 0.7980958843624794, 0.7847011436593938, 0.7757783564444126, 0.7666948738804072, 0.7598154779604248, 0.7546543655410296, 0.7475205569086086, 0.7421661059356695, 0.7373660465524045, 0.7304721791388804, 0.7257101657907941, 0.719303715747352, 0.7148850730807359, 0.708259141482876, 0.704277414197563, 0.6995707229725031, 0.694001765029045, 0.6888601014149786, 0.682470847290082, 0.678316767525375, 0.6705782022989435, 0.6678462874503623, 0.6604006846861572, 0.6554217068377814, 0.6483444269802906, 0.6420702253077198, 0.638843350012313], 'val_losses': [1.4883694129694585, 1.181281712222198, 1.0027694546732744, 0.8991898069508975, 0.850548014569411, 0.8156807307177746, 0.7996467214272354, 0.7867393046972253, 0.7768719862344166, 0.7740474654355304, 0.7653122403585527, 0.7580194344886217, 0.7504969400102491, 0.7454641568916153, 0.737904640430646, 0.7347991290120549, 0.7246956805390622, 0.719891928861709, 0.7163626809359762, 0.7083716696851042, 0.700997081671931, 0.6956122413925503, 0.6865313216687264, 0.6896206649409367, 0.6739141618140493, 0.6673116550202388, 0.6638831219910948, 0.6569502985562586, 0.6513753779407074, 0.6408762007059272, 0.6337396235390425, 0.6355853828776228, 0.6210557835925986], 'val_acc': [0.5692269139205614, 0.722967046735998, 0.728354842751535, 0.7215887733366746, 0.7169527628116777, 0.7210875830096479, 0.7244706177170781, 0.728354842751535, 0.730234306477885, 0.7271018669339682, 0.7301090088961283, 0.7331161508582884, 0.7367497807292319, 0.7387545420373387, 0.7385039468738254, 0.7400075178549054, 0.7425134694900388, 0.7472747775967924, 0.746648289688009, 0.7536649542663827, 0.7481518606690891, 0.7594286430271896, 0.7633128680616464, 0.7599298333542163, 0.7663200100238066, 0.7666959027690765, 0.7717078060393434, 0.7754667334920436, 0.7813557198346072, 0.784488159378524, 0.7901265505575742, 0.7930083949379777, 0.8014033329156748], 'config': {'seq_len': 1000, 'patch_size': 100, 'head_dim': 24, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 3, 'dropout': 0.10432279730178204, 'num_classes': 5, 'epochs': 33, 'batch_size': 32, 'lr': 2.7155819552829416e-05, 'weight_decay': 1.0532957056791494e-05, 'grad_clip_norm': 2.7158220463029275, 'use_focal_loss': False, 'focal_gamma': 0.6719982159072837, 'label_smoothing': 0.002693819362039874, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.3936749765364156, 'aug_jitter_std': 0.018015805653787846, 'aug_scale_low': 0.9202749830354915, 'aug_scale_high': 1.0004202365120856, 'aug_drift_max_amp': 0.042246409314468464, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 119380}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.7155819552829416e-05, 'batch_size': 32, 'epochs': 33, 'head_dim': 24, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 3, 'patch_size': 100, 'dropout': 0.10432279730178204, 'weight_decay': 1.0532957056791494e-05, 'grad_clip_norm': 2.7158220463029275, 'use_focal_loss': False, 'focal_gamma': 0.6719982159072837, 'label_smoothing': 0.002693819362039874, 'compute_class_weights': False, 'aug_prob': 0.3936749765364156, 'aug_jitter_std': 0.018015805653787846, 'aug_scale_low': 0.9202749830354915, 'aug_scale_high': 1.0004202365120856, 'aug_drift_max_amp': 0.042246409314468464, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'seed': 119380}, 'model_parameter_count': 2397, 'model_storage_size_kb': 10.299609375000001, 'model_size_validation': 'PASS'}
2025-10-02 23:48:49,985 - INFO - _models.training_function_executor - BO Objective: base=0.8014, size_penalty=0.0000, final=0.8014
2025-10-02 23:48:49,985 - INFO - _models.training_function_executor - Model: 2,397 parameters, 10.3KB (PASS 256KB limit)
2025-10-02 23:48:49,985 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 45.814s
2025-10-02 23:48:50,088 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8014
2025-10-02 23:48:50,088 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.103s
2025-10-02 23:48:50,088 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 2.7155819552829416e-05, 'batch_size': np.int64(32), 'epochs': np.int64(33), 'head_dim': np.int64(24), 'n_heads': np.int64(2), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(100), 'dropout': 0.10432279730178204, 'weight_decay': 1.0532957056791494e-05, 'grad_clip_norm': 2.7158220463029275, 'use_focal_loss': np.False_, 'focal_gamma': 0.6719982159072837, 'label_smoothing': 0.002693819362039874, 'compute_class_weights': np.False_, 'aug_prob': 0.3936749765364156, 'aug_jitter_std': 0.018015805653787846, 'aug_scale_low': 0.9202749830354915, 'aug_scale_high': 1.0004202365120856, 'aug_drift_max_amp': 0.042246409314468464, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(119380)}, value=0.8014
2025-10-02 23:48:50,088 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 2.7155819552829416e-05, 'batch_size': np.int64(32), 'epochs': np.int64(33), 'head_dim': np.int64(24), 'n_heads': np.int64(2), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(100), 'dropout': 0.10432279730178204, 'weight_decay': 1.0532957056791494e-05, 'grad_clip_norm': 2.7158220463029275, 'use_focal_loss': np.False_, 'focal_gamma': 0.6719982159072837, 'label_smoothing': 0.002693819362039874, 'compute_class_weights': np.False_, 'aug_prob': 0.3936749765364156, 'aug_jitter_std': 0.018015805653787846, 'aug_scale_low': 0.9202749830354915, 'aug_scale_high': 1.0004202365120856, 'aug_drift_max_amp': 0.042246409314468464, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(119380)} -> 0.8014
2025-10-02 23:48:50,088 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-10-02 23:48:50,088 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:48:50,088 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:48:50,088 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:48:50,088 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.090167027384619e-05, 'batch_size': 128, 'epochs': 5, 'head_dim': 17, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.18603456268129628, 'weight_decay': 0.009813640664149168, 'grad_clip_norm': 3.2207870326214265, 'use_focal_loss': True, 'focal_gamma': 1.643135742020218, 'label_smoothing': 0.18435404794866356, 'compute_class_weights': False, 'aug_prob': 0.09965236993158247, 'aug_jitter_std': 0.02840412519846917, 'aug_scale_low': 0.9915947843978845, 'aug_scale_high': 1.1097636496677725, 'aug_drift_max_amp': 0.0635118200783714, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 380273}
2025-10-02 23:48:50,090 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.090167027384619e-05, 'batch_size': 128, 'epochs': 5, 'head_dim': 17, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.18603456268129628, 'weight_decay': 0.009813640664149168, 'grad_clip_norm': 3.2207870326214265, 'use_focal_loss': True, 'focal_gamma': 1.643135742020218, 'label_smoothing': 0.18435404794866356, 'compute_class_weights': False, 'aug_prob': 0.09965236993158247, 'aug_jitter_std': 0.02840412519846917, 'aug_scale_low': 0.9915947843978845, 'aug_scale_high': 1.1097636496677725, 'aug_drift_max_amp': 0.0635118200783714, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 380273}
2025-10-02 23:48:54,234 - INFO - _models.training_function_executor - Model: 8,565 parameters, 18.4KB storage
2025-10-02 23:48:54,234 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1071112412197661, 0.8853919795927493, 0.7550234069233606, 0.6792520670179354, 0.6353088184391553], 'val_losses': [0.9751996369667182, 0.8195257926327799, 0.725281269990058, 0.6663419963004759, 0.6307615087438894], 'val_acc': [0.6048114271394562, 0.7353715073299085, 0.7416363864177421, 0.7372509710562586, 0.7338679363488285], 'config': {'seq_len': 1000, 'patch_size': 40, 'head_dim': 17, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'dropout': 0.18603456268129628, 'num_classes': 5, 'epochs': 5, 'batch_size': 128, 'lr': 3.090167027384619e-05, 'weight_decay': 0.009813640664149168, 'grad_clip_norm': 3.2207870326214265, 'use_focal_loss': True, 'focal_gamma': 1.643135742020218, 'label_smoothing': 0.18435404794866356, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.09965236993158247, 'aug_jitter_std': 0.02840412519846917, 'aug_scale_low': 0.9915947843978845, 'aug_scale_high': 1.1097636496677725, 'aug_drift_max_amp': 0.0635118200783714, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'seed': 380273}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.090167027384619e-05, 'batch_size': 128, 'epochs': 5, 'head_dim': 17, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.18603456268129628, 'weight_decay': 0.009813640664149168, 'grad_clip_norm': 3.2207870326214265, 'use_focal_loss': True, 'focal_gamma': 1.643135742020218, 'label_smoothing': 0.18435404794866356, 'compute_class_weights': False, 'aug_prob': 0.09965236993158247, 'aug_jitter_std': 0.02840412519846917, 'aug_scale_low': 0.9915947843978845, 'aug_scale_high': 1.1097636496677725, 'aug_drift_max_amp': 0.0635118200783714, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 380273}, 'model_parameter_count': 8565, 'model_storage_size_kb': 18.4013671875, 'model_size_validation': 'PASS'}
2025-10-02 23:48:54,234 - INFO - _models.training_function_executor - BO Objective: base=0.7339, size_penalty=0.0000, final=0.7339
2025-10-02 23:48:54,234 - INFO - _models.training_function_executor - Model: 8,565 parameters, 18.4KB (PASS 256KB limit)
2025-10-02 23:48:54,234 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.146s
2025-10-02 23:48:54,338 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7339
2025-10-02 23:48:54,338 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.104s
2025-10-02 23:48:54,338 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 3.090167027384619e-05, 'batch_size': np.int64(128), 'epochs': np.int64(5), 'head_dim': np.int64(17), 'n_heads': np.int64(4), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(40), 'dropout': 0.18603456268129628, 'weight_decay': 0.009813640664149168, 'grad_clip_norm': 3.2207870326214265, 'use_focal_loss': np.True_, 'focal_gamma': 1.643135742020218, 'label_smoothing': 0.18435404794866356, 'compute_class_weights': np.False_, 'aug_prob': 0.09965236993158247, 'aug_jitter_std': 0.02840412519846917, 'aug_scale_low': 0.9915947843978845, 'aug_scale_high': 1.1097636496677725, 'aug_drift_max_amp': 0.0635118200783714, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(380273)}, value=0.7339
2025-10-02 23:48:54,338 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 3.090167027384619e-05, 'batch_size': np.int64(128), 'epochs': np.int64(5), 'head_dim': np.int64(17), 'n_heads': np.int64(4), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(40), 'dropout': 0.18603456268129628, 'weight_decay': 0.009813640664149168, 'grad_clip_norm': 3.2207870326214265, 'use_focal_loss': np.True_, 'focal_gamma': 1.643135742020218, 'label_smoothing': 0.18435404794866356, 'compute_class_weights': np.False_, 'aug_prob': 0.09965236993158247, 'aug_jitter_std': 0.02840412519846917, 'aug_scale_low': 0.9915947843978845, 'aug_scale_high': 1.1097636496677725, 'aug_drift_max_amp': 0.0635118200783714, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(380273)} -> 0.7339
2025-10-02 23:48:54,338 - INFO - bo.run_bo - üîçBO Trial 6: Using RF surrogate + Expected Improvement
2025-10-02 23:48:54,338 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:48:54,339 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:48:54,339 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:48:54,339 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00013649773625610433, 'batch_size': 64, 'epochs': 12, 'head_dim': 13, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 25, 'dropout': 0.4426896348955101, 'weight_decay': 7.142139220022951e-06, 'grad_clip_norm': 4.210966627083761, 'use_focal_loss': False, 'focal_gamma': 2.1151072427772766, 'label_smoothing': 0.1039437551983422, 'compute_class_weights': True, 'aug_prob': 0.4564857100240757, 'aug_jitter_std': 0.015341643079761573, 'aug_scale_low': 0.9878695728869109, 'aug_scale_high': 1.107783760267984, 'aug_drift_max_amp': 0.1423145978833826, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 49339}
2025-10-02 23:48:54,340 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00013649773625610433, 'batch_size': 64, 'epochs': 12, 'head_dim': 13, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 25, 'dropout': 0.4426896348955101, 'weight_decay': 7.142139220022951e-06, 'grad_clip_norm': 4.210966627083761, 'use_focal_loss': False, 'focal_gamma': 2.1151072427772766, 'label_smoothing': 0.1039437551983422, 'compute_class_weights': True, 'aug_prob': 0.4564857100240757, 'aug_jitter_std': 0.015341643079761573, 'aug_scale_low': 0.9878695728869109, 'aug_scale_high': 1.107783760267984, 'aug_drift_max_amp': 0.1423145978833826, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 49339}
2025-10-02 23:49:16,726 - INFO - _models.training_function_executor - Model: 10,793 parameters, 46.4KB storage
2025-10-02 23:49:16,726 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.1105073196481756, 1.9316047180924907, 1.828952658360522, 1.7532126198390618, 1.6883956615038243, 1.6435036818208848, 1.6088734044432098, 1.5911701773916036, 1.5593981892229922, 1.5501530466494067, 1.5422119092467779, 1.5385588682172473], 'val_losses': [1.9776019180437898, 1.8467910360564057, 1.7625634562415058, 1.6851772490696058, 1.6385356830394502, 1.594623170062095, 1.5623477391619505, 1.5451279732087937, 1.5315883533949541, 1.514350187657486, 1.5089958368305454, 1.5045871423190287], 'val_acc': [0.10023806540533768, 0.1756672096228543, 0.3000877083072297, 0.369878461345696, 0.484149855907781, 0.508958777095602, 0.49580253101115146, 0.576619471244205, 0.4813933091091342, 0.5959152988347325, 0.5538153113644907, 0.5994236311239193], 'config': {'seq_len': 1000, 'patch_size': 25, 'head_dim': 13, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.4426896348955101, 'num_classes': 5, 'epochs': 12, 'batch_size': 64, 'lr': 0.00013649773625610433, 'weight_decay': 7.142139220022951e-06, 'grad_clip_norm': 4.210966627083761, 'use_focal_loss': False, 'focal_gamma': 2.1151072427772766, 'label_smoothing': 0.1039437551983422, 'compute_class_weights': True, 'augmentations': {'aug_prob': 0.4564857100240757, 'aug_jitter_std': 0.015341643079761573, 'aug_scale_low': 0.9878695728869109, 'aug_scale_high': 1.107783760267984, 'aug_drift_max_amp': 0.1423145978833826, 'normalize': True}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 49339}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00013649773625610433, 'batch_size': 64, 'epochs': 12, 'head_dim': 13, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 25, 'dropout': 0.4426896348955101, 'weight_decay': 7.142139220022951e-06, 'grad_clip_norm': 4.210966627083761, 'use_focal_loss': False, 'focal_gamma': 2.1151072427772766, 'label_smoothing': 0.1039437551983422, 'compute_class_weights': True, 'aug_prob': 0.4564857100240757, 'aug_jitter_std': 0.015341643079761573, 'aug_scale_low': 0.9878695728869109, 'aug_scale_high': 1.107783760267984, 'aug_drift_max_amp': 0.1423145978833826, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 49339}, 'model_parameter_count': 10793, 'model_storage_size_kb': 46.376171875000004, 'model_size_validation': 'PASS'}
2025-10-02 23:49:16,726 - INFO - _models.training_function_executor - BO Objective: base=0.5994, size_penalty=0.0000, final=0.5994
2025-10-02 23:49:16,726 - INFO - _models.training_function_executor - Model: 10,793 parameters, 46.4KB (PASS 256KB limit)
2025-10-02 23:49:16,726 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 22.388s
2025-10-02 23:49:16,831 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5994
2025-10-02 23:49:16,831 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-10-02 23:49:16,831 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 0.00013649773625610433, 'batch_size': np.int64(64), 'epochs': np.int64(12), 'head_dim': np.int64(13), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(25), 'dropout': 0.4426896348955101, 'weight_decay': 7.142139220022951e-06, 'grad_clip_norm': 4.210966627083761, 'use_focal_loss': np.False_, 'focal_gamma': 2.1151072427772766, 'label_smoothing': 0.1039437551983422, 'compute_class_weights': np.True_, 'aug_prob': 0.4564857100240757, 'aug_jitter_std': 0.015341643079761573, 'aug_scale_low': 0.9878695728869109, 'aug_scale_high': 1.107783760267984, 'aug_drift_max_amp': 0.1423145978833826, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(49339)}, value=0.5994
2025-10-02 23:49:16,831 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 0.00013649773625610433, 'batch_size': np.int64(64), 'epochs': np.int64(12), 'head_dim': np.int64(13), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(25), 'dropout': 0.4426896348955101, 'weight_decay': 7.142139220022951e-06, 'grad_clip_norm': 4.210966627083761, 'use_focal_loss': np.False_, 'focal_gamma': 2.1151072427772766, 'label_smoothing': 0.1039437551983422, 'compute_class_weights': np.True_, 'aug_prob': 0.4564857100240757, 'aug_jitter_std': 0.015341643079761573, 'aug_scale_low': 0.9878695728869109, 'aug_scale_high': 1.107783760267984, 'aug_drift_max_amp': 0.1423145978833826, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(49339)} -> 0.5994
2025-10-02 23:49:16,831 - INFO - bo.run_bo - üîçBO Trial 7: Using RF surrogate + Expected Improvement
2025-10-02 23:49:16,831 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:49:16,831 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:49:16,831 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:49:16,831 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007791271812370822, 'batch_size': 128, 'epochs': 9, 'head_dim': 18, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.3061371021023827, 'weight_decay': 4.659424672436356e-06, 'grad_clip_norm': 4.402435612870453, 'use_focal_loss': True, 'focal_gamma': 1.6758149021078321, 'label_smoothing': 0.06277543015852995, 'compute_class_weights': False, 'aug_prob': 0.12783726450404012, 'aug_jitter_std': 0.02621663801393214, 'aug_scale_low': 0.890531290012331, 'aug_scale_high': 1.0416943169682404, 'aug_drift_max_amp': 0.04616022485797878, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 724664}
2025-10-02 23:49:16,833 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007791271812370822, 'batch_size': 128, 'epochs': 9, 'head_dim': 18, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.3061371021023827, 'weight_decay': 4.659424672436356e-06, 'grad_clip_norm': 4.402435612870453, 'use_focal_loss': True, 'focal_gamma': 1.6758149021078321, 'label_smoothing': 0.06277543015852995, 'compute_class_weights': False, 'aug_prob': 0.12783726450404012, 'aug_jitter_std': 0.02621663801393214, 'aug_scale_low': 0.890531290012331, 'aug_scale_high': 1.0416943169682404, 'aug_drift_max_amp': 0.04616022485797878, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 724664}
2025-10-02 23:49:24,288 - INFO - _models.training_function_executor - Model: 8,437 parameters, 36.3KB storage
2025-10-02 23:49:24,289 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5002500473812544, 0.3823041894122726, 0.32086077663726914, 0.2795259850160656, 0.2538890440913328, 0.23380366888622986, 0.21937554330450584, 0.20350491606520368, 0.19549649005896225], 'val_losses': [0.42189167346023493, 0.36002459831590894, 0.2859625589868476, 0.2637762702425505, 0.2262457360967092, 0.23504881471369354, 0.20402731458624865, 0.20025840929472838, 0.17979031247447985], 'val_acc': [0.7792256609447438, 0.8111765442926951, 0.8649292068663075, 0.8936223530885854, 0.8981330660318256, 0.8988848515223656, 0.9209372259115399, 0.9107881217892495, 0.9251973436912667], 'config': {'seq_len': 1000, 'patch_size': 40, 'head_dim': 18, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 3, 'dropout': 0.3061371021023827, 'num_classes': 5, 'epochs': 9, 'batch_size': 128, 'lr': 0.0007791271812370822, 'weight_decay': 4.659424672436356e-06, 'grad_clip_norm': 4.402435612870453, 'use_focal_loss': True, 'focal_gamma': 1.6758149021078321, 'label_smoothing': 0.06277543015852995, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.12783726450404012, 'aug_jitter_std': 0.02621663801393214, 'aug_scale_low': 0.890531290012331, 'aug_scale_high': 1.0416943169682404, 'aug_drift_max_amp': 0.04616022485797878, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 724664}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007791271812370822, 'batch_size': 128, 'epochs': 9, 'head_dim': 18, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.3061371021023827, 'weight_decay': 4.659424672436356e-06, 'grad_clip_norm': 4.402435612870453, 'use_focal_loss': True, 'focal_gamma': 1.6758149021078321, 'label_smoothing': 0.06277543015852995, 'compute_class_weights': False, 'aug_prob': 0.12783726450404012, 'aug_jitter_std': 0.02621663801393214, 'aug_scale_low': 0.890531290012331, 'aug_scale_high': 1.0416943169682404, 'aug_drift_max_amp': 0.04616022485797878, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 724664}, 'model_parameter_count': 8437, 'model_storage_size_kb': 36.252734375, 'model_size_validation': 'PASS'}
2025-10-02 23:49:24,289 - INFO - _models.training_function_executor - BO Objective: base=0.9252, size_penalty=0.0000, final=0.9252
2025-10-02 23:49:24,289 - INFO - _models.training_function_executor - Model: 8,437 parameters, 36.3KB (PASS 256KB limit)
2025-10-02 23:49:24,289 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 7.457s
2025-10-02 23:49:24,392 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9252
2025-10-02 23:49:24,393 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.104s
2025-10-02 23:49:24,393 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.0007791271812370822, 'batch_size': np.int64(128), 'epochs': np.int64(9), 'head_dim': np.int64(18), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(40), 'dropout': 0.3061371021023827, 'weight_decay': 4.659424672436356e-06, 'grad_clip_norm': 4.402435612870453, 'use_focal_loss': np.True_, 'focal_gamma': 1.6758149021078321, 'label_smoothing': 0.06277543015852995, 'compute_class_weights': np.False_, 'aug_prob': 0.12783726450404012, 'aug_jitter_std': 0.02621663801393214, 'aug_scale_low': 0.890531290012331, 'aug_scale_high': 1.0416943169682404, 'aug_drift_max_amp': 0.04616022485797878, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(724664)}, value=0.9252
2025-10-02 23:49:24,393 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.0007791271812370822, 'batch_size': np.int64(128), 'epochs': np.int64(9), 'head_dim': np.int64(18), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(40), 'dropout': 0.3061371021023827, 'weight_decay': 4.659424672436356e-06, 'grad_clip_norm': 4.402435612870453, 'use_focal_loss': np.True_, 'focal_gamma': 1.6758149021078321, 'label_smoothing': 0.06277543015852995, 'compute_class_weights': np.False_, 'aug_prob': 0.12783726450404012, 'aug_jitter_std': 0.02621663801393214, 'aug_scale_low': 0.890531290012331, 'aug_scale_high': 1.0416943169682404, 'aug_drift_max_amp': 0.04616022485797878, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(724664)} -> 0.9252
2025-10-02 23:49:24,393 - INFO - bo.run_bo - üîçBO Trial 8: Using RF surrogate + Expected Improvement
2025-10-02 23:49:24,393 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:49:24,393 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:49:24,393 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:49:24,393 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 8.247104089163682e-06, 'batch_size': 64, 'epochs': 46, 'head_dim': 23, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.03107492017457187, 'weight_decay': 0.003954452932761458, 'grad_clip_norm': 2.8638722538266435, 'use_focal_loss': False, 'focal_gamma': 3.3150936167015677, 'label_smoothing': 0.19363447467420938, 'compute_class_weights': False, 'aug_prob': 0.5899735913397084, 'aug_jitter_std': 0.009720636217972969, 'aug_scale_low': 0.8780517740138271, 'aug_scale_high': 1.0522903362510554, 'aug_drift_max_amp': 0.13503889122471732, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 378151}
2025-10-02 23:49:24,394 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 8.247104089163682e-06, 'batch_size': 64, 'epochs': 46, 'head_dim': 23, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.03107492017457187, 'weight_decay': 0.003954452932761458, 'grad_clip_norm': 2.8638722538266435, 'use_focal_loss': False, 'focal_gamma': 3.3150936167015677, 'label_smoothing': 0.19363447467420938, 'compute_class_weights': False, 'aug_prob': 0.5899735913397084, 'aug_jitter_std': 0.009720636217972969, 'aug_scale_low': 0.8780517740138271, 'aug_scale_high': 1.0522903362510554, 'aug_drift_max_amp': 0.13503889122471732, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 378151}
2025-10-02 23:50:52,832 - INFO - _models.training_function_executor - Model: 13,685 parameters, 29.4KB storage
2025-10-02 23:50:52,832 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.547754318938273, 1.4150076503051319, 1.3123415564062735, 1.2346651648286104, 1.1802080730572906, 1.1377746463064689, 1.1055992544285507, 1.0808816373570425, 1.0615014309005264, 1.045554227836896, 1.0320592500420884, 1.0201227569424731, 1.0103834575867812, 1.0011878383752266, 0.9931646370948999, 0.985609776481248, 0.9782878014354469, 0.9710965529758709, 0.9639881072522211, 0.957963749075042, 0.9515612327253012, 0.9455708077003703, 0.939765867966479, 0.9341243565823804, 0.928523165525113, 0.9231924914859633, 0.9188517337323684, 0.9129396960896866, 0.9084524479672055, 0.9041362802654386, 0.899169955076447, 0.8950536405275038, 0.8908379094566248, 0.8867485136352894, 0.8831603116410771, 0.8790511477655648, 0.8751693155010986, 0.8715049915884617, 0.8683214096972522, 0.8651038626249103, 0.8618188366325181, 0.8589327112197338, 0.85599129417802, 0.853024097459939, 0.8500993573134978, 0.8478894614095628], 'val_losses': [1.4766375047344116, 1.3567118459500074, 1.2692987641331068, 1.2059425846285545, 1.155913372010519, 1.1213966019992139, 1.0907355548893056, 1.0673984540851622, 1.0510863702767117, 1.0355830053655206, 1.021894835369761, 1.0104736422942764, 1.0011750068480532, 0.9914169992783392, 0.984264928481137, 0.9757634855395136, 0.9684542798111723, 0.9616256848625515, 0.9547258261943607, 0.9493587223782365, 0.9417915155588572, 0.9358392494272401, 0.9300505268649938, 0.9237494866750307, 0.9180774322773129, 0.913441632418626, 0.907746395115087, 0.9033317246423331, 0.8981354168043625, 0.8946306007261817, 0.8891202960302919, 0.8854760690420108, 0.8814179413809452, 0.876841187880401, 0.874073750601423, 0.8688012753015711, 0.865021907145062, 0.8616569521671816, 0.8600087368970765, 0.8556697516195787, 0.8523987356305884, 0.848971881890055, 0.8464000304679407, 0.8427208842766373, 0.8409115498622488, 0.8383572283969339], 'val_acc': [0.729858413732615, 0.7194587144468112, 0.7199599047738379, 0.7197093096103245, 0.7189575241197845, 0.7188322265380278, 0.722215261245458, 0.7235935346447814, 0.7272271645157249, 0.7324896629495051, 0.7442676356346323, 0.7561709059015161, 0.7714572108758301, 0.7822328029069039, 0.7981455957900012, 0.8022804159879714, 0.8129307104372886, 0.8225786242325523, 0.830347074301466, 0.8423756421501065, 0.8432527252224032, 0.8466357599298333, 0.8500187946372635, 0.8498934970555069, 0.8525247462723969, 0.8554065906528004, 0.8560330785615837, 0.8570354592156372, 0.8604184939230672, 0.8627991479764441, 0.8638015286304974, 0.8668086705926575, 0.8728229545169778, 0.8777095602054881, 0.8790878336048115, 0.8799649166771081, 0.8778348577872447, 0.8839744392933216, 0.8894875328906152, 0.8887357474000752, 0.8917428893622353, 0.8972559829595289, 0.8988848515223656, 0.9028943741385791, 0.9087833604811427, 0.9107881217892495], 'config': {'seq_len': 1000, 'patch_size': 25, 'head_dim': 23, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 5, 'dropout': 0.03107492017457187, 'num_classes': 5, 'epochs': 46, 'batch_size': 64, 'lr': 8.247104089163682e-06, 'weight_decay': 0.003954452932761458, 'grad_clip_norm': 2.8638722538266435, 'use_focal_loss': False, 'focal_gamma': 3.3150936167015677, 'label_smoothing': 0.19363447467420938, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.5899735913397084, 'aug_jitter_std': 0.009720636217972969, 'aug_scale_low': 0.8780517740138271, 'aug_scale_high': 1.0522903362510554, 'aug_drift_max_amp': 0.13503889122471732, 'normalize': True}, 'quantization': {'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 378151}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 8.247104089163682e-06, 'batch_size': 64, 'epochs': 46, 'head_dim': 23, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.03107492017457187, 'weight_decay': 0.003954452932761458, 'grad_clip_norm': 2.8638722538266435, 'use_focal_loss': False, 'focal_gamma': 3.3150936167015677, 'label_smoothing': 0.19363447467420938, 'compute_class_weights': False, 'aug_prob': 0.5899735913397084, 'aug_jitter_std': 0.009720636217972969, 'aug_scale_low': 0.8780517740138271, 'aug_scale_high': 1.0522903362510554, 'aug_drift_max_amp': 0.13503889122471732, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 378151}, 'model_parameter_count': 13685, 'model_storage_size_kb': 29.401367187500004, 'model_size_validation': 'PASS'}
2025-10-02 23:50:52,832 - INFO - _models.training_function_executor - BO Objective: base=0.9108, size_penalty=0.0000, final=0.9108
2025-10-02 23:50:52,832 - INFO - _models.training_function_executor - Model: 13,685 parameters, 29.4KB (PASS 256KB limit)
2025-10-02 23:50:52,832 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 88.439s
2025-10-02 23:50:52,940 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9108
2025-10-02 23:50:52,940 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.108s
2025-10-02 23:50:52,940 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 8.247104089163682e-06, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'head_dim': np.int64(23), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(25), 'dropout': 0.03107492017457187, 'weight_decay': 0.003954452932761458, 'grad_clip_norm': 2.8638722538266435, 'use_focal_loss': np.False_, 'focal_gamma': 3.3150936167015677, 'label_smoothing': 0.19363447467420938, 'compute_class_weights': np.False_, 'aug_prob': 0.5899735913397084, 'aug_jitter_std': 0.009720636217972969, 'aug_scale_low': 0.8780517740138271, 'aug_scale_high': 1.0522903362510554, 'aug_drift_max_amp': 0.13503889122471732, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(378151)}, value=0.9108
2025-10-02 23:50:52,940 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 8.247104089163682e-06, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'head_dim': np.int64(23), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(25), 'dropout': 0.03107492017457187, 'weight_decay': 0.003954452932761458, 'grad_clip_norm': 2.8638722538266435, 'use_focal_loss': np.False_, 'focal_gamma': 3.3150936167015677, 'label_smoothing': 0.19363447467420938, 'compute_class_weights': np.False_, 'aug_prob': 0.5899735913397084, 'aug_jitter_std': 0.009720636217972969, 'aug_scale_low': 0.8780517740138271, 'aug_scale_high': 1.0522903362510554, 'aug_drift_max_amp': 0.13503889122471732, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(378151)} -> 0.9108
2025-10-02 23:50:52,940 - INFO - bo.run_bo - üîçBO Trial 9: Using RF surrogate + Expected Improvement
2025-10-02 23:50:52,940 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:50:52,940 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:50:52,940 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:50:52,940 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.770693624717543e-06, 'batch_size': 128, 'epochs': 49, 'head_dim': 23, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'patch_size': 100, 'dropout': 0.10270732044606593, 'weight_decay': 2.933491467540985e-05, 'grad_clip_norm': 4.037708113332819, 'use_focal_loss': True, 'focal_gamma': 1.3265177760216438, 'label_smoothing': 0.1357560157362425, 'compute_class_weights': False, 'aug_prob': 0.4952337251494917, 'aug_jitter_std': 0.049118305117722146, 'aug_scale_low': 0.8079293539022002, 'aug_scale_high': 1.0122868410227832, 'aug_drift_max_amp': 0.16056015071396912, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 836951}
2025-10-02 23:50:52,942 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.770693624717543e-06, 'batch_size': 128, 'epochs': 49, 'head_dim': 23, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'patch_size': 100, 'dropout': 0.10270732044606593, 'weight_decay': 2.933491467540985e-05, 'grad_clip_norm': 4.037708113332819, 'use_focal_loss': True, 'focal_gamma': 1.3265177760216438, 'label_smoothing': 0.1357560157362425, 'compute_class_weights': False, 'aug_prob': 0.4952337251494917, 'aug_jitter_std': 0.049118305117722146, 'aug_scale_low': 0.8079293539022002, 'aug_scale_high': 1.0122868410227832, 'aug_drift_max_amp': 0.16056015071396912, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 836951}
2025-10-02 23:52:16,457 - INFO - _models.training_function_executor - Model: 8,565 parameters, 18.4KB storage
2025-10-02 23:52:16,457 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1293546867128625, 1.1134893053012567, 1.0981667701625115, 1.0828794176572623, 1.0681252940810981, 1.0534992958300433, 1.0392685832413568, 1.0255605107959798, 1.0120387846443006, 0.9988333773904996, 0.9859802612676413, 0.9734096237991969, 0.9612049297260558, 0.9491531948818023, 0.9376690346462081, 0.9262672647088369, 0.9152117355072314, 0.9043698941029781, 0.8938678812794685, 0.8837195744181517, 0.873733908329169, 0.8641560073858275, 0.8548226138891419, 0.8458809588269378, 0.8370997008310017, 0.8287675948447424, 0.8204816600114515, 0.8124179612054333, 0.8047558019059129, 0.7971294071390318, 0.789854259065334, 0.7826646542790368, 0.7758268206873786, 0.7690685455655631, 0.7624202420626899, 0.7560672505444177, 0.7497824499549802, 0.7435297672657375, 0.7374097238364302, 0.731737267435917, 0.7258965249509399, 0.7206058974209739, 0.7151192970578804, 0.70990405892331, 0.7047373650665738, 0.6995517197092334, 0.6951856201922046, 0.6902294265474438, 0.6858210531040768], 'val_losses': [1.1261048949970187, 1.1104846683126273, 1.094993155248451, 1.079324373937821, 1.0653077021172106, 1.0489173509351384, 1.0358392413345707, 1.0208908949649926, 1.0079835741281837, 0.9969892970393317, 0.9818113220244359, 0.9702662674500342, 0.9582634427810041, 0.9453857782012224, 0.9327809252008252, 0.9216601514320388, 0.9110056977420803, 0.8993701371031139, 0.8889251356061547, 0.8776232209395024, 0.870120628475172, 0.858381979097446, 0.8500494632471804, 0.8408830797757411, 0.8325327988585618, 0.8246675432213804, 0.8154738705090422, 0.8070788890881402, 0.8015998365615875, 0.790617087956524, 0.7856046177811856, 0.7806835092931833, 0.7735105563623089, 0.7675902092087883, 0.7573728907184484, 0.7533975212052119, 0.7463744427234423, 0.7409485462255279, 0.7343153490441856, 0.728748708631054, 0.7218954214594475, 0.7177723106279242, 0.7114171655296667, 0.7069942924465815, 0.7004251085999406, 0.6961225789824967, 0.691009516151996, 0.6863105772895154, 0.6819310192625081], 'val_acc': [0.4223781481017416, 0.4904147349956146, 0.5351459716827465, 0.6233554692394436, 0.6954015787495301, 0.7195840120285678, 0.7199599047738379, 0.7199599047738379, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946], 'config': {'seq_len': 1000, 'patch_size': 100, 'head_dim': 23, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'dropout': 0.10270732044606593, 'num_classes': 5, 'epochs': 49, 'batch_size': 128, 'lr': 1.770693624717543e-06, 'weight_decay': 2.933491467540985e-05, 'grad_clip_norm': 4.037708113332819, 'use_focal_loss': True, 'focal_gamma': 1.3265177760216438, 'label_smoothing': 0.1357560157362425, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.4952337251494917, 'aug_jitter_std': 0.049118305117722146, 'aug_scale_low': 0.8079293539022002, 'aug_scale_high': 1.0122868410227832, 'aug_drift_max_amp': 0.16056015071396912, 'normalize': True}, 'quantization': {'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 836951}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.770693624717543e-06, 'batch_size': 128, 'epochs': 49, 'head_dim': 23, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'patch_size': 100, 'dropout': 0.10270732044606593, 'weight_decay': 2.933491467540985e-05, 'grad_clip_norm': 4.037708113332819, 'use_focal_loss': True, 'focal_gamma': 1.3265177760216438, 'label_smoothing': 0.1357560157362425, 'compute_class_weights': False, 'aug_prob': 0.4952337251494917, 'aug_jitter_std': 0.049118305117722146, 'aug_scale_low': 0.8079293539022002, 'aug_scale_high': 1.0122868410227832, 'aug_drift_max_amp': 0.16056015071396912, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 836951}, 'model_parameter_count': 8565, 'model_storage_size_kb': 18.4013671875, 'model_size_validation': 'PASS'}
2025-10-02 23:52:16,457 - INFO - _models.training_function_executor - BO Objective: base=0.7201, size_penalty=0.0000, final=0.7201
2025-10-02 23:52:16,457 - INFO - _models.training_function_executor - Model: 8,565 parameters, 18.4KB (PASS 256KB limit)
2025-10-02 23:52:16,457 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 83.517s
2025-10-02 23:52:16,568 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7201
2025-10-02 23:52:16,568 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.110s
2025-10-02 23:52:16,568 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 1.770693624717543e-06, 'batch_size': np.int64(128), 'epochs': np.int64(49), 'head_dim': np.int64(23), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(100), 'dropout': 0.10270732044606593, 'weight_decay': 2.933491467540985e-05, 'grad_clip_norm': 4.037708113332819, 'use_focal_loss': np.True_, 'focal_gamma': 1.3265177760216438, 'label_smoothing': 0.1357560157362425, 'compute_class_weights': np.False_, 'aug_prob': 0.4952337251494917, 'aug_jitter_std': 0.049118305117722146, 'aug_scale_low': 0.8079293539022002, 'aug_scale_high': 1.0122868410227832, 'aug_drift_max_amp': 0.16056015071396912, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(836951)}, value=0.7201
2025-10-02 23:52:16,568 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 1.770693624717543e-06, 'batch_size': np.int64(128), 'epochs': np.int64(49), 'head_dim': np.int64(23), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(100), 'dropout': 0.10270732044606593, 'weight_decay': 2.933491467540985e-05, 'grad_clip_norm': 4.037708113332819, 'use_focal_loss': np.True_, 'focal_gamma': 1.3265177760216438, 'label_smoothing': 0.1357560157362425, 'compute_class_weights': np.False_, 'aug_prob': 0.4952337251494917, 'aug_jitter_std': 0.049118305117722146, 'aug_scale_low': 0.8079293539022002, 'aug_scale_high': 1.0122868410227832, 'aug_drift_max_amp': 0.16056015071396912, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(836951)} -> 0.7201
2025-10-02 23:52:16,568 - INFO - bo.run_bo - üîçBO Trial 10: Using RF surrogate + Expected Improvement
2025-10-02 23:52:16,568 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:52:16,568 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:52:16,568 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:52:16,568 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0005191952996528251, 'batch_size': 64, 'epochs': 21, 'head_dim': 13, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.13199600147716709, 'weight_decay': 0.00043705175258975634, 'grad_clip_norm': 3.6951054866179827, 'use_focal_loss': False, 'focal_gamma': 4.591153172941608, 'label_smoothing': 0.14497840764987335, 'compute_class_weights': False, 'aug_prob': 0.32459597001483015, 'aug_jitter_std': 0.022462377229431445, 'aug_scale_low': 0.8422129670730308, 'aug_scale_high': 1.044822828135245, 'aug_drift_max_amp': 0.04221702388542557, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 272180}
2025-10-02 23:52:16,570 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0005191952996528251, 'batch_size': 64, 'epochs': 21, 'head_dim': 13, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.13199600147716709, 'weight_decay': 0.00043705175258975634, 'grad_clip_norm': 3.6951054866179827, 'use_focal_loss': False, 'focal_gamma': 4.591153172941608, 'label_smoothing': 0.14497840764987335, 'compute_class_weights': False, 'aug_prob': 0.32459597001483015, 'aug_jitter_std': 0.022462377229431445, 'aug_scale_low': 0.8422129670730308, 'aug_scale_high': 1.044822828135245, 'aug_drift_max_amp': 0.04221702388542557, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 272180}
2025-10-02 23:52:40,597 - INFO - _models.training_function_executor - Model: 13,749 parameters, 14.8KB storage
2025-10-02 23:52:40,597 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9839854087797625, 0.7817954614124132, 0.7002632916207056, 0.6728980353283142, 0.6585946367097872, 0.6489228641176883, 0.6406857901018627, 0.6344776733609528, 0.6297695664102838, 0.6267905478391889, 0.6247209341227364, 0.621107197732468, 0.6185294621249837, 0.6164371890538961, 0.6151769033456115, 0.6126658273001637, 0.6117975029439603, 0.6107604646915759, 0.6085778155769878, 0.6080907511146498, 0.6068811492674505], 'val_losses': [0.8427340355661486, 0.717956647973299, 0.6795078886808451, 0.654162744758213, 0.6580439666326954, 0.6319414635765689, 0.6420676404858364, 0.6267390751387598, 0.6255457058181113, 0.6304945279864527, 0.6117446135196033, 0.6097791230315598, 0.6170706345225139, 0.6117819960203076, 0.6138889539392651, 0.6042871859763412, 0.6032933062172342, 0.6020992559652373, 0.6096069230619282, 0.604608616767977, 0.6006879678660954], 'val_acc': [0.8357348703170029, 0.9150482395689763, 0.9349705550682872, 0.9443678737000376, 0.9426137075554442, 0.9553940608946248, 0.9547675729858414, 0.9605312617466483, 0.9577747149480015, 0.9572735246209748, 0.9615336424007017, 0.9626613206365117, 0.960656559328405, 0.9604059641648917, 0.9622854278912417, 0.9624107254729983, 0.9635384037088084, 0.9646660819446184, 0.9619095351459717, 0.9656684625986719, 0.9662949505074552], 'config': {'seq_len': 1000, 'patch_size': 25, 'head_dim': 13, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 5, 'dropout': 0.13199600147716709, 'num_classes': 5, 'epochs': 21, 'batch_size': 64, 'lr': 0.0005191952996528251, 'weight_decay': 0.00043705175258975634, 'grad_clip_norm': 3.6951054866179827, 'use_focal_loss': False, 'focal_gamma': 4.591153172941608, 'label_smoothing': 0.14497840764987335, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.32459597001483015, 'aug_jitter_std': 0.022462377229431445, 'aug_scale_low': 0.8422129670730308, 'aug_scale_high': 1.044822828135245, 'aug_drift_max_amp': 0.04221702388542557, 'normalize': False}, 'quantization': {'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 272180}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0005191952996528251, 'batch_size': 64, 'epochs': 21, 'head_dim': 13, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.13199600147716709, 'weight_decay': 0.00043705175258975634, 'grad_clip_norm': 3.6951054866179827, 'use_focal_loss': False, 'focal_gamma': 4.591153172941608, 'label_smoothing': 0.14497840764987335, 'compute_class_weights': False, 'aug_prob': 0.32459597001483015, 'aug_jitter_std': 0.022462377229431445, 'aug_scale_low': 0.8422129670730308, 'aug_scale_high': 1.044822828135245, 'aug_drift_max_amp': 0.04221702388542557, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 272180}, 'model_parameter_count': 13749, 'model_storage_size_kb': 14.769433593750001, 'model_size_validation': 'PASS'}
2025-10-02 23:52:40,597 - INFO - _models.training_function_executor - BO Objective: base=0.9663, size_penalty=0.0000, final=0.9663
2025-10-02 23:52:40,597 - INFO - _models.training_function_executor - Model: 13,749 parameters, 14.8KB (PASS 256KB limit)
2025-10-02 23:52:40,597 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 24.029s
2025-10-02 23:52:40,710 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9663
2025-10-02 23:52:40,711 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.114s
2025-10-02 23:52:40,711 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 0.0005191952996528251, 'batch_size': np.int64(64), 'epochs': np.int64(21), 'head_dim': np.int64(13), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(25), 'dropout': 0.13199600147716709, 'weight_decay': 0.00043705175258975634, 'grad_clip_norm': 3.6951054866179827, 'use_focal_loss': np.False_, 'focal_gamma': 4.591153172941608, 'label_smoothing': 0.14497840764987335, 'compute_class_weights': np.False_, 'aug_prob': 0.32459597001483015, 'aug_jitter_std': 0.022462377229431445, 'aug_scale_low': 0.8422129670730308, 'aug_scale_high': 1.044822828135245, 'aug_drift_max_amp': 0.04221702388542557, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(272180)}, value=0.9663
2025-10-02 23:52:40,711 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 0.0005191952996528251, 'batch_size': np.int64(64), 'epochs': np.int64(21), 'head_dim': np.int64(13), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(25), 'dropout': 0.13199600147716709, 'weight_decay': 0.00043705175258975634, 'grad_clip_norm': 3.6951054866179827, 'use_focal_loss': np.False_, 'focal_gamma': 4.591153172941608, 'label_smoothing': 0.14497840764987335, 'compute_class_weights': np.False_, 'aug_prob': 0.32459597001483015, 'aug_jitter_std': 0.022462377229431445, 'aug_scale_low': 0.8422129670730308, 'aug_scale_high': 1.044822828135245, 'aug_drift_max_amp': 0.04221702388542557, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(272180)} -> 0.9663
2025-10-02 23:52:40,711 - INFO - bo.run_bo - üîçBO Trial 11: Using RF surrogate + Expected Improvement
2025-10-02 23:52:40,711 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:52:40,711 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:52:40,711 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:52:40,711 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0021659510579135097, 'batch_size': 128, 'epochs': 6, 'head_dim': 21, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.12251390803481352, 'weight_decay': 0.00016743164786577907, 'grad_clip_norm': 1.4672437697071392, 'use_focal_loss': False, 'focal_gamma': 3.6836662283353196, 'label_smoothing': 0.1453135600631924, 'compute_class_weights': True, 'aug_prob': 0.463182649955553, 'aug_jitter_std': 0.042424041217718214, 'aug_scale_low': 0.8048408417834968, 'aug_scale_high': 1.0143670965085672, 'aug_drift_max_amp': 0.15246790043703037, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'seed': 253369}
2025-10-02 23:52:40,712 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0021659510579135097, 'batch_size': 128, 'epochs': 6, 'head_dim': 21, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.12251390803481352, 'weight_decay': 0.00016743164786577907, 'grad_clip_norm': 1.4672437697071392, 'use_focal_loss': False, 'focal_gamma': 3.6836662283353196, 'label_smoothing': 0.1453135600631924, 'compute_class_weights': True, 'aug_prob': 0.463182649955553, 'aug_jitter_std': 0.042424041217718214, 'aug_scale_low': 0.8048408417834968, 'aug_scale_high': 1.0143670965085672, 'aug_drift_max_amp': 0.15246790043703037, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'seed': 253369}
2025-10-02 23:52:46,621 - INFO - _models.training_function_executor - Model: 3,677 parameters, 15.8KB storage
2025-10-02 23:52:46,621 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.2214860957588156, 2.0689666292271034, 1.9595046338926592, 1.9145362238027628, 1.8695826043805708, 1.8466833200243502], 'val_losses': [2.137473277737362, 2.0121650580489954, 1.9074969383397955, 1.876373295526489, 1.8279393123752894, 1.843015242244289], 'val_acc': [0.11702794136073173, 0.1680240571356973, 0.26375140959779475, 0.30597669464979327, 0.24821450945996743, 0.35622102493421876], 'config': {'seq_len': 1000, 'patch_size': 20, 'head_dim': 21, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'dropout': 0.12251390803481352, 'num_classes': 5, 'epochs': 6, 'batch_size': 128, 'lr': 0.0021659510579135097, 'weight_decay': 0.00016743164786577907, 'grad_clip_norm': 1.4672437697071392, 'use_focal_loss': False, 'focal_gamma': 3.6836662283353196, 'label_smoothing': 0.1453135600631924, 'compute_class_weights': True, 'augmentations': {'aug_prob': 0.463182649955553, 'aug_jitter_std': 0.042424041217718214, 'aug_scale_low': 0.8048408417834968, 'aug_scale_high': 1.0143670965085672, 'aug_drift_max_amp': 0.15246790043703037, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 253369}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0021659510579135097, 'batch_size': 128, 'epochs': 6, 'head_dim': 21, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.12251390803481352, 'weight_decay': 0.00016743164786577907, 'grad_clip_norm': 1.4672437697071392, 'use_focal_loss': False, 'focal_gamma': 3.6836662283353196, 'label_smoothing': 0.1453135600631924, 'compute_class_weights': True, 'aug_prob': 0.463182649955553, 'aug_jitter_std': 0.042424041217718214, 'aug_scale_low': 0.8048408417834968, 'aug_scale_high': 1.0143670965085672, 'aug_drift_max_amp': 0.15246790043703037, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'seed': 253369}, 'model_parameter_count': 3677, 'model_storage_size_kb': 15.799609375000001, 'model_size_validation': 'PASS'}
2025-10-02 23:52:46,621 - INFO - _models.training_function_executor - BO Objective: base=0.3562, size_penalty=0.0000, final=0.3562
2025-10-02 23:52:46,621 - INFO - _models.training_function_executor - Model: 3,677 parameters, 15.8KB (PASS 256KB limit)
2025-10-02 23:52:46,621 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.910s
2025-10-02 23:52:46,733 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.3562
2025-10-02 23:52:46,733 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.112s
2025-10-02 23:52:46,733 - INFO - bo.run_bo - Recorded observation #11: hparams={'lr': 0.0021659510579135097, 'batch_size': np.int64(128), 'epochs': np.int64(6), 'head_dim': np.int64(21), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(20), 'dropout': 0.12251390803481352, 'weight_decay': 0.00016743164786577907, 'grad_clip_norm': 1.4672437697071392, 'use_focal_loss': np.False_, 'focal_gamma': 3.6836662283353196, 'label_smoothing': 0.1453135600631924, 'compute_class_weights': np.True_, 'aug_prob': 0.463182649955553, 'aug_jitter_std': 0.042424041217718214, 'aug_scale_low': 0.8048408417834968, 'aug_scale_high': 1.0143670965085672, 'aug_drift_max_amp': 0.15246790043703037, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(253369)}, value=0.3562
2025-10-02 23:52:46,733 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 11: {'lr': 0.0021659510579135097, 'batch_size': np.int64(128), 'epochs': np.int64(6), 'head_dim': np.int64(21), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(20), 'dropout': 0.12251390803481352, 'weight_decay': 0.00016743164786577907, 'grad_clip_norm': 1.4672437697071392, 'use_focal_loss': np.False_, 'focal_gamma': 3.6836662283353196, 'label_smoothing': 0.1453135600631924, 'compute_class_weights': np.True_, 'aug_prob': 0.463182649955553, 'aug_jitter_std': 0.042424041217718214, 'aug_scale_low': 0.8048408417834968, 'aug_scale_high': 1.0143670965085672, 'aug_drift_max_amp': 0.15246790043703037, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(253369)} -> 0.3562
2025-10-02 23:52:46,733 - INFO - bo.run_bo - üîçBO Trial 12: Using RF surrogate + Expected Improvement
2025-10-02 23:52:46,733 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:52:46,733 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:52:46,733 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:52:46,733 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004009118597859599, 'batch_size': 128, 'epochs': 21, 'head_dim': 11, 'n_heads': 8, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 5, 'ds_kernel2': 5, 'patch_size': 10, 'dropout': 0.15601415871088545, 'weight_decay': 0.00013852948273858327, 'grad_clip_norm': 4.87314243014512, 'use_focal_loss': True, 'focal_gamma': 3.152694170336613, 'label_smoothing': 0.051124368689232924, 'compute_class_weights': False, 'aug_prob': 0.2521279814031883, 'aug_jitter_std': 0.004088080185435934, 'aug_scale_low': 0.8601398005490682, 'aug_scale_high': 1.0549558111866364, 'aug_drift_max_amp': 0.08579746804273171, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 790594}
2025-10-02 23:52:46,734 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004009118597859599, 'batch_size': 128, 'epochs': 21, 'head_dim': 11, 'n_heads': 8, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 5, 'ds_kernel2': 5, 'patch_size': 10, 'dropout': 0.15601415871088545, 'weight_decay': 0.00013852948273858327, 'grad_clip_norm': 4.87314243014512, 'use_focal_loss': True, 'focal_gamma': 3.152694170336613, 'label_smoothing': 0.051124368689232924, 'compute_class_weights': False, 'aug_prob': 0.2521279814031883, 'aug_jitter_std': 0.004088080185435934, 'aug_scale_low': 0.8601398005490682, 'aug_scale_high': 1.0549558111866364, 'aug_drift_max_amp': 0.08579746804273171, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 790594}
2025-10-02 23:53:02,458 - INFO - _models.training_function_executor - Model: 3,613 parameters, 15.5KB storage
2025-10-02 23:53:02,458 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2732134164976072, 0.1817910137512748, 0.1302534193794773, 0.10722362099551648, 0.09209763639965171, 0.08240443004903625, 0.07409046887856649, 0.06870260373440817, 0.06330686423912259, 0.0605924844473887, 0.056671963910870475, 0.05403253484820863, 0.052749205918997386, 0.050070019820634165, 0.04830179649808921, 0.04927722406814717, 0.04675150266197194, 0.046047876679060455, 0.04437049029269127, 0.04273376564142814, 0.043923257762325876], 'val_losses': [0.23664103368106543, 0.14328720015629448, 0.10697671451592203, 0.08650975408892093, 0.09768252169846174, 0.0825884334239845, 0.09563897321202107, 0.058387828433260465, 0.09814686004887875, 0.0732142891542996, 0.05778890968119645, 0.055193621277353576, 0.046643093213403514, 0.07405176251293977, 0.042938771803394594, 0.04940256644906204, 0.04159246676688639, 0.05022941325959101, 0.052340653744922847, 0.043893437062423636, 0.03930499866864329], 'val_acc': [0.7644405462974565, 0.8927452700162887, 0.92319258238316, 0.9408595414108508, 0.930835734870317, 0.9421125172284175, 0.9245708557824834, 0.9546422754040848, 0.9015161007392557, 0.9424884099736875, 0.9553940608946248, 0.953765192331788, 0.9592782859290816, 0.9243202606189701, 0.9630372133817817, 0.9595288810925949, 0.963287808545295, 0.9548928705675981, 0.955268763312868, 0.9630372133817817, 0.9635384037088084], 'config': {'seq_len': 1000, 'patch_size': 10, 'head_dim': 11, 'n_heads': 8, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 5, 'ds_kernel2': 5, 'dropout': 0.15601415871088545, 'num_classes': 5, 'epochs': 21, 'batch_size': 128, 'lr': 0.004009118597859599, 'weight_decay': 0.00013852948273858327, 'grad_clip_norm': 4.87314243014512, 'use_focal_loss': True, 'focal_gamma': 3.152694170336613, 'label_smoothing': 0.051124368689232924, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.2521279814031883, 'aug_jitter_std': 0.004088080185435934, 'aug_scale_low': 0.8601398005490682, 'aug_scale_high': 1.0549558111866364, 'aug_drift_max_amp': 0.08579746804273171, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 790594}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004009118597859599, 'batch_size': 128, 'epochs': 21, 'head_dim': 11, 'n_heads': 8, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 5, 'ds_kernel2': 5, 'patch_size': 10, 'dropout': 0.15601415871088545, 'weight_decay': 0.00013852948273858327, 'grad_clip_norm': 4.87314243014512, 'use_focal_loss': True, 'focal_gamma': 3.152694170336613, 'label_smoothing': 0.051124368689232924, 'compute_class_weights': False, 'aug_prob': 0.2521279814031883, 'aug_jitter_std': 0.004088080185435934, 'aug_scale_low': 0.8601398005490682, 'aug_scale_high': 1.0549558111866364, 'aug_drift_max_amp': 0.08579746804273171, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 790594}, 'model_parameter_count': 3613, 'model_storage_size_kb': 15.524609375, 'model_size_validation': 'PASS'}
2025-10-02 23:53:02,459 - INFO - _models.training_function_executor - BO Objective: base=0.9635, size_penalty=0.0000, final=0.9635
2025-10-02 23:53:02,459 - INFO - _models.training_function_executor - Model: 3,613 parameters, 15.5KB (PASS 256KB limit)
2025-10-02 23:53:02,459 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 15.725s
2025-10-02 23:53:02,574 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9635
2025-10-02 23:53:02,574 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.115s
2025-10-02 23:53:02,575 - INFO - bo.run_bo - Recorded observation #12: hparams={'lr': 0.004009118597859599, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'head_dim': np.int64(11), 'n_heads': np.int64(8), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(10), 'dropout': 0.15601415871088545, 'weight_decay': 0.00013852948273858327, 'grad_clip_norm': 4.87314243014512, 'use_focal_loss': np.True_, 'focal_gamma': 3.152694170336613, 'label_smoothing': 0.051124368689232924, 'compute_class_weights': np.False_, 'aug_prob': 0.2521279814031883, 'aug_jitter_std': 0.004088080185435934, 'aug_scale_low': 0.8601398005490682, 'aug_scale_high': 1.0549558111866364, 'aug_drift_max_amp': 0.08579746804273171, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(790594)}, value=0.9635
2025-10-02 23:53:02,575 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 12: {'lr': 0.004009118597859599, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'head_dim': np.int64(11), 'n_heads': np.int64(8), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(10), 'dropout': 0.15601415871088545, 'weight_decay': 0.00013852948273858327, 'grad_clip_norm': 4.87314243014512, 'use_focal_loss': np.True_, 'focal_gamma': 3.152694170336613, 'label_smoothing': 0.051124368689232924, 'compute_class_weights': np.False_, 'aug_prob': 0.2521279814031883, 'aug_jitter_std': 0.004088080185435934, 'aug_scale_low': 0.8601398005490682, 'aug_scale_high': 1.0549558111866364, 'aug_drift_max_amp': 0.08579746804273171, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(790594)} -> 0.9635
2025-10-02 23:53:02,575 - INFO - bo.run_bo - üîçBO Trial 13: Using RF surrogate + Expected Improvement
2025-10-02 23:53:02,575 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:53:02,575 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:53:02,575 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:53:02,575 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0009145273316685421, 'batch_size': 128, 'epochs': 22, 'head_dim': 9, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.03419949211496954, 'weight_decay': 5.649553011739201e-05, 'grad_clip_norm': 1.486908922981838, 'use_focal_loss': True, 'focal_gamma': 3.787649746642418, 'label_smoothing': 0.07390582713531672, 'compute_class_weights': False, 'aug_prob': 0.45175473765150964, 'aug_jitter_std': 0.03444632266654618, 'aug_scale_low': 0.8265460878719129, 'aug_scale_high': 1.0225248308432384, 'aug_drift_max_amp': 0.01872313787501836, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 785650}
2025-10-02 23:53:02,576 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0009145273316685421, 'batch_size': 128, 'epochs': 22, 'head_dim': 9, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.03419949211496954, 'weight_decay': 5.649553011739201e-05, 'grad_clip_norm': 1.486908922981838, 'use_focal_loss': True, 'focal_gamma': 3.787649746642418, 'label_smoothing': 0.07390582713531672, 'compute_class_weights': False, 'aug_prob': 0.45175473765150964, 'aug_jitter_std': 0.03444632266654618, 'aug_scale_low': 0.8265460878719129, 'aug_scale_high': 1.0225248308432384, 'aug_drift_max_amp': 0.01872313787501836, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 785650}
2025-10-02 23:53:29,660 - INFO - _models.training_function_executor - Model: 18,869 parameters, 81.1KB storage
2025-10-02 23:53:29,660 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.24235421763490594, 0.12919484560559039, 0.09156152206209524, 0.07306848906153997, 0.06180227694127567, 0.05397289731613217, 0.04801121491186023, 0.04493015442028722, 0.041486902977788474, 0.03896848159128217, 0.03707068147347172, 0.03439180259051507, 0.03482605459105129, 0.03254411198764246, 0.03249751759830553, 0.031006715249330765, 0.030464597692054916, 0.02945180365553951, 0.02920409149808848, 0.028523918207012125, 0.027556677136437297, 0.027757462303493494], 'val_losses': [0.16734553624097154, 0.10052745998915821, 0.07729382880658468, 0.0654393669025049, 0.05308553925193188, 0.0578966396548626, 0.05137378237548739, 0.04004414747434703, 0.054997902736487865, 0.039257589017683184, 0.041370448343294286, 0.03473529547673242, 0.034725807519421184, 0.05242872713190872, 0.032080132953935574, 0.03283778066929981, 0.04172474529947528, 0.030197790579172757, 0.03895541296658279, 0.03181454003540559, 0.027259321286555536, 0.036230582213165795], 'val_acc': [0.8369878461345696, 0.9219396065655933, 0.9404836486655808, 0.9498809672973312, 0.9591529883473249, 0.9490038842250345, 0.9560205488034081, 0.9620348327277284, 0.9525122165142212, 0.962536023054755, 0.9635384037088084, 0.9671720335797519, 0.9662949505074552, 0.9521363237689513, 0.9660443553439418, 0.9684250093973187, 0.9540157874953014, 0.9721839368500188, 0.9536398947500313, 0.969427390051372, 0.9708056634506954, 0.9647913795263752], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 9, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.03419949211496954, 'num_classes': 5, 'epochs': 22, 'batch_size': 128, 'lr': 0.0009145273316685421, 'weight_decay': 5.649553011739201e-05, 'grad_clip_norm': 1.486908922981838, 'use_focal_loss': True, 'focal_gamma': 3.787649746642418, 'label_smoothing': 0.07390582713531672, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.45175473765150964, 'aug_jitter_std': 0.03444632266654618, 'aug_scale_low': 0.8265460878719129, 'aug_scale_high': 1.0225248308432384, 'aug_drift_max_amp': 0.01872313787501836, 'normalize': False}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 785650}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0009145273316685421, 'batch_size': 128, 'epochs': 22, 'head_dim': 9, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.03419949211496954, 'weight_decay': 5.649553011739201e-05, 'grad_clip_norm': 1.486908922981838, 'use_focal_loss': True, 'focal_gamma': 3.787649746642418, 'label_smoothing': 0.07390582713531672, 'compute_class_weights': False, 'aug_prob': 0.45175473765150964, 'aug_jitter_std': 0.03444632266654618, 'aug_scale_low': 0.8265460878719129, 'aug_scale_high': 1.0225248308432384, 'aug_drift_max_amp': 0.01872313787501836, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 785650}, 'model_parameter_count': 18869, 'model_storage_size_kb': 81.077734375, 'model_size_validation': 'PASS'}
2025-10-02 23:53:29,660 - INFO - _models.training_function_executor - BO Objective: base=0.9648, size_penalty=0.0000, final=0.9648
2025-10-02 23:53:29,660 - INFO - _models.training_function_executor - Model: 18,869 parameters, 81.1KB (PASS 256KB limit)
2025-10-02 23:53:29,660 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 27.085s
2025-10-02 23:53:29,779 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9648
2025-10-02 23:53:29,779 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.119s
2025-10-02 23:53:29,779 - INFO - bo.run_bo - Recorded observation #13: hparams={'lr': 0.0009145273316685421, 'batch_size': np.int64(128), 'epochs': np.int64(22), 'head_dim': np.int64(9), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.03419949211496954, 'weight_decay': 5.649553011739201e-05, 'grad_clip_norm': 1.486908922981838, 'use_focal_loss': np.True_, 'focal_gamma': 3.787649746642418, 'label_smoothing': 0.07390582713531672, 'compute_class_weights': np.False_, 'aug_prob': 0.45175473765150964, 'aug_jitter_std': 0.03444632266654618, 'aug_scale_low': 0.8265460878719129, 'aug_scale_high': 1.0225248308432384, 'aug_drift_max_amp': 0.01872313787501836, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(785650)}, value=0.9648
2025-10-02 23:53:29,779 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 13: {'lr': 0.0009145273316685421, 'batch_size': np.int64(128), 'epochs': np.int64(22), 'head_dim': np.int64(9), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.03419949211496954, 'weight_decay': 5.649553011739201e-05, 'grad_clip_norm': 1.486908922981838, 'use_focal_loss': np.True_, 'focal_gamma': 3.787649746642418, 'label_smoothing': 0.07390582713531672, 'compute_class_weights': np.False_, 'aug_prob': 0.45175473765150964, 'aug_jitter_std': 0.03444632266654618, 'aug_scale_low': 0.8265460878719129, 'aug_scale_high': 1.0225248308432384, 'aug_drift_max_amp': 0.01872313787501836, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(785650)} -> 0.9648
2025-10-02 23:53:29,779 - INFO - bo.run_bo - üîçBO Trial 14: Using RF surrogate + Expected Improvement
2025-10-02 23:53:29,779 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:53:29,779 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:53:29,780 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:53:29,780 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0013916351204073811, 'batch_size': 64, 'epochs': 6, 'head_dim': 13, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 40, 'dropout': 0.34929504671433204, 'weight_decay': 1.9575211968143368e-06, 'grad_clip_norm': 3.9359981038054928, 'use_focal_loss': False, 'focal_gamma': 4.689879303369969, 'label_smoothing': 0.02634260857382516, 'compute_class_weights': False, 'aug_prob': 0.6462752914825526, 'aug_jitter_std': 0.046449784564075614, 'aug_scale_low': 0.8182084849497018, 'aug_scale_high': 1.0142067246830118, 'aug_drift_max_amp': 0.14646277299029917, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 857619}
2025-10-02 23:53:29,781 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0013916351204073811, 'batch_size': 64, 'epochs': 6, 'head_dim': 13, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 40, 'dropout': 0.34929504671433204, 'weight_decay': 1.9575211968143368e-06, 'grad_clip_norm': 3.9359981038054928, 'use_focal_loss': False, 'focal_gamma': 4.689879303369969, 'label_smoothing': 0.02634260857382516, 'compute_class_weights': False, 'aug_prob': 0.6462752914825526, 'aug_jitter_std': 0.046449784564075614, 'aug_scale_low': 0.8182084849497018, 'aug_scale_high': 1.0142067246830118, 'aug_drift_max_amp': 0.14646277299029917, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 857619}
2025-10-02 23:53:37,410 - INFO - _models.training_function_executor - Model: 7,913 parameters, 34.0KB storage
2025-10-02 23:53:37,410 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6899308688058495, 0.43491669503369024, 0.36949383184821166, 0.3356274741085364, 0.31250801249576293, 0.2990294706626139], 'val_losses': [0.4764086095936659, 0.38856126106467975, 0.32466727292988773, 0.30460404417685377, 0.29341140635523516, 0.2791769018807726], 'val_acc': [0.9052750281919559, 0.9259491291818068, 0.9387294825209873, 0.9446184688635509, 0.9527628116777346, 0.9543916802405713], 'config': {'seq_len': 1000, 'patch_size': 40, 'head_dim': 13, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'dropout': 0.34929504671433204, 'num_classes': 5, 'epochs': 6, 'batch_size': 64, 'lr': 0.0013916351204073811, 'weight_decay': 1.9575211968143368e-06, 'grad_clip_norm': 3.9359981038054928, 'use_focal_loss': False, 'focal_gamma': 4.689879303369969, 'label_smoothing': 0.02634260857382516, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.6462752914825526, 'aug_jitter_std': 0.046449784564075614, 'aug_scale_low': 0.8182084849497018, 'aug_scale_high': 1.0142067246830118, 'aug_drift_max_amp': 0.14646277299029917, 'normalize': False}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 857619}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0013916351204073811, 'batch_size': 64, 'epochs': 6, 'head_dim': 13, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 40, 'dropout': 0.34929504671433204, 'weight_decay': 1.9575211968143368e-06, 'grad_clip_norm': 3.9359981038054928, 'use_focal_loss': False, 'focal_gamma': 4.689879303369969, 'label_smoothing': 0.02634260857382516, 'compute_class_weights': False, 'aug_prob': 0.6462752914825526, 'aug_jitter_std': 0.046449784564075614, 'aug_scale_low': 0.8182084849497018, 'aug_scale_high': 1.0142067246830118, 'aug_drift_max_amp': 0.14646277299029917, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 857619}, 'model_parameter_count': 7913, 'model_storage_size_kb': 34.001171875000004, 'model_size_validation': 'PASS'}
2025-10-02 23:53:37,410 - INFO - _models.training_function_executor - BO Objective: base=0.9544, size_penalty=0.0000, final=0.9544
2025-10-02 23:53:37,410 - INFO - _models.training_function_executor - Model: 7,913 parameters, 34.0KB (PASS 256KB limit)
2025-10-02 23:53:37,410 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 7.631s
2025-10-02 23:53:37,527 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9544
2025-10-02 23:53:37,527 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.117s
2025-10-02 23:53:37,527 - INFO - bo.run_bo - Recorded observation #14: hparams={'lr': 0.0013916351204073811, 'batch_size': np.int64(64), 'epochs': np.int64(6), 'head_dim': np.int64(13), 'n_heads': np.int64(2), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(40), 'dropout': 0.34929504671433204, 'weight_decay': 1.9575211968143368e-06, 'grad_clip_norm': 3.9359981038054928, 'use_focal_loss': np.False_, 'focal_gamma': 4.689879303369969, 'label_smoothing': 0.02634260857382516, 'compute_class_weights': np.False_, 'aug_prob': 0.6462752914825526, 'aug_jitter_std': 0.046449784564075614, 'aug_scale_low': 0.8182084849497018, 'aug_scale_high': 1.0142067246830118, 'aug_drift_max_amp': 0.14646277299029917, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(857619)}, value=0.9544
2025-10-02 23:53:37,527 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 14: {'lr': 0.0013916351204073811, 'batch_size': np.int64(64), 'epochs': np.int64(6), 'head_dim': np.int64(13), 'n_heads': np.int64(2), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(40), 'dropout': 0.34929504671433204, 'weight_decay': 1.9575211968143368e-06, 'grad_clip_norm': 3.9359981038054928, 'use_focal_loss': np.False_, 'focal_gamma': 4.689879303369969, 'label_smoothing': 0.02634260857382516, 'compute_class_weights': np.False_, 'aug_prob': 0.6462752914825526, 'aug_jitter_std': 0.046449784564075614, 'aug_scale_low': 0.8182084849497018, 'aug_scale_high': 1.0142067246830118, 'aug_drift_max_amp': 0.14646277299029917, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(857619)} -> 0.9544
2025-10-02 23:53:37,528 - INFO - bo.run_bo - üîçBO Trial 15: Using RF surrogate + Expected Improvement
2025-10-02 23:53:37,528 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:53:37,528 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:53:37,528 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:53:37,528 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002358086334798522, 'batch_size': 128, 'epochs': 47, 'head_dim': 8, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 10, 'dropout': 0.15172392030640833, 'weight_decay': 7.068256778749376e-05, 'grad_clip_norm': 4.770044801412329, 'use_focal_loss': True, 'focal_gamma': 3.8634120634279125, 'label_smoothing': 0.11930554540666867, 'compute_class_weights': False, 'aug_prob': 0.9734988346562418, 'aug_jitter_std': 0.016684191161168486, 'aug_scale_low': 0.8394515902488175, 'aug_scale_high': 1.1205808615484991, 'aug_drift_max_amp': 0.11290353190012256, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 281464}
2025-10-02 23:53:37,529 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002358086334798522, 'batch_size': 128, 'epochs': 47, 'head_dim': 8, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 10, 'dropout': 0.15172392030640833, 'weight_decay': 7.068256778749376e-05, 'grad_clip_norm': 4.770044801412329, 'use_focal_loss': True, 'focal_gamma': 3.8634120634279125, 'label_smoothing': 0.11930554540666867, 'compute_class_weights': False, 'aug_prob': 0.9734988346562418, 'aug_jitter_std': 0.016684191161168486, 'aug_scale_low': 0.8394515902488175, 'aug_scale_high': 1.1205808615484991, 'aug_drift_max_amp': 0.11290353190012256, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 281464}
2025-10-02 23:54:38,273 - INFO - _models.training_function_executor - Model: 3,581 parameters, 15.4KB storage
2025-10-02 23:54:38,274 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2740554748598923, 0.19924253525142213, 0.1601186766177234, 0.13407070481571431, 0.11450917218842648, 0.10156212419390966, 0.09599240996704171, 0.09040426801120757, 0.08621463847883669, 0.08172973115172776, 0.07775647935357431, 0.07667883768893832, 0.07166667941587669, 0.06976784005048367, 0.06734380075400012, 0.06544108182206786, 0.06455818319221347, 0.06364511968262515, 0.06079926233548096, 0.0599341417547241, 0.05866431074209785, 0.05800912489600115, 0.05632420648786186, 0.055817021544274356, 0.055248866187795356, 0.05398325297946609, 0.0541445990477712, 0.05322983762478017, 0.05240501941843999, 0.05214925762414708, 0.05182697485198931, 0.05115049097961125, 0.05058763225903652, 0.05016791312512896, 0.04897605044819715, 0.048687985111535, 0.049096892727029856, 0.047676150401687704, 0.047467736643378014, 0.048204844562713206, 0.04794236254765643, 0.04754676084815046, 0.046701112361184016, 0.04603098180861889, 0.04601605460050056, 0.0462067844388468, 0.04546388656113809], 'val_losses': [0.22067137391902408, 0.17777556999855032, 0.1376154053666406, 0.11679402057512289, 0.1069642156619257, 0.09791626310861211, 0.09108214503880686, 0.11294543455696662, 0.08758483295463228, 0.07425387291586155, 0.06957919345747378, 0.0746682598088121, 0.06612253659590939, 0.06180938921683027, 0.060348973002122795, 0.060219038145301815, 0.059012536642366505, 0.0647713385943392, 0.06092542869198399, 0.054947540892154825, 0.051368895622198776, 0.050463132552784826, 0.054382597780899065, 0.04924313150754236, 0.054444957397576633, 0.049841071441330834, 0.04950840864598923, 0.052062218179119935, 0.048366930919689216, 0.052382102974815006, 0.04737441749913533, 0.04567759769622462, 0.04993401035991338, 0.048288602046803894, 0.046291981824559236, 0.047905090072783534, 0.042971917330404674, 0.044354390297418785, 0.043144037284945626, 0.04504968009048006, 0.04721939025238826, 0.045122594872620034, 0.04674764036900746, 0.04251462102119314, 0.050647489561773666, 0.04292140487717321, 0.041748169714358796], 'val_acc': [0.754291442175166, 0.8164390427264754, 0.8878586643277785, 0.9204360355845133, 0.9186818694399198, 0.9307104372885603, 0.932339305851397, 0.9221902017291066, 0.9379776970304473, 0.9498809672973312, 0.9505074552061146, 0.9451196591905776, 0.9566470367121914, 0.9584012028567849, 0.9586517980202982, 0.9570229294574615, 0.9568976318757048, 0.9566470367121914, 0.9557699536398947, 0.9584012028567849, 0.9635384037088084, 0.961784237564215, 0.9596541786743515, 0.9601553690013783, 0.9641648916175918, 0.9619095351459717, 0.9629119158000251, 0.9597794762561083, 0.9645407843628618, 0.962160130309485, 0.9650419746898885, 0.9640395940358351, 0.960656559328405, 0.962536023054755, 0.9631625109635384, 0.9630372133817817, 0.9664202480892119, 0.9644154867811051, 0.967547926325022, 0.9644154867811051, 0.9650419746898885, 0.9649166771081318, 0.9681744142338052, 0.9679238190702919, 0.9615336424007017, 0.968299711815562, 0.9695526876331286], 'config': {'seq_len': 1000, 'patch_size': 10, 'head_dim': 8, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 5, 'dropout': 0.15172392030640833, 'num_classes': 5, 'epochs': 47, 'batch_size': 128, 'lr': 0.002358086334798522, 'weight_decay': 7.068256778749376e-05, 'grad_clip_norm': 4.770044801412329, 'use_focal_loss': True, 'focal_gamma': 3.8634120634279125, 'label_smoothing': 0.11930554540666867, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.9734988346562418, 'aug_jitter_std': 0.016684191161168486, 'aug_scale_low': 0.8394515902488175, 'aug_scale_high': 1.1205808615484991, 'aug_drift_max_amp': 0.11290353190012256, 'normalize': False}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 281464}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002358086334798522, 'batch_size': 128, 'epochs': 47, 'head_dim': 8, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 10, 'dropout': 0.15172392030640833, 'weight_decay': 7.068256778749376e-05, 'grad_clip_norm': 4.770044801412329, 'use_focal_loss': True, 'focal_gamma': 3.8634120634279125, 'label_smoothing': 0.11930554540666867, 'compute_class_weights': False, 'aug_prob': 0.9734988346562418, 'aug_jitter_std': 0.016684191161168486, 'aug_scale_low': 0.8394515902488175, 'aug_scale_high': 1.1205808615484991, 'aug_drift_max_amp': 0.11290353190012256, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 281464}, 'model_parameter_count': 3581, 'model_storage_size_kb': 15.387109375000001, 'model_size_validation': 'PASS'}
2025-10-02 23:54:38,274 - INFO - _models.training_function_executor - BO Objective: base=0.9696, size_penalty=0.0000, final=0.9696
2025-10-02 23:54:38,274 - INFO - _models.training_function_executor - Model: 3,581 parameters, 15.4KB (PASS 256KB limit)
2025-10-02 23:54:38,274 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 60.746s
2025-10-02 23:54:38,512 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9696
2025-10-02 23:54:38,512 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.238s
2025-10-02 23:54:38,512 - INFO - bo.run_bo - Recorded observation #15: hparams={'lr': 0.002358086334798522, 'batch_size': np.int64(128), 'epochs': np.int64(47), 'head_dim': np.int64(8), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(10), 'dropout': 0.15172392030640833, 'weight_decay': 7.068256778749376e-05, 'grad_clip_norm': 4.770044801412329, 'use_focal_loss': np.True_, 'focal_gamma': 3.8634120634279125, 'label_smoothing': 0.11930554540666867, 'compute_class_weights': np.False_, 'aug_prob': 0.9734988346562418, 'aug_jitter_std': 0.016684191161168486, 'aug_scale_low': 0.8394515902488175, 'aug_scale_high': 1.1205808615484991, 'aug_drift_max_amp': 0.11290353190012256, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(281464)}, value=0.9696
2025-10-02 23:54:38,512 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 15: {'lr': 0.002358086334798522, 'batch_size': np.int64(128), 'epochs': np.int64(47), 'head_dim': np.int64(8), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(10), 'dropout': 0.15172392030640833, 'weight_decay': 7.068256778749376e-05, 'grad_clip_norm': 4.770044801412329, 'use_focal_loss': np.True_, 'focal_gamma': 3.8634120634279125, 'label_smoothing': 0.11930554540666867, 'compute_class_weights': np.False_, 'aug_prob': 0.9734988346562418, 'aug_jitter_std': 0.016684191161168486, 'aug_scale_low': 0.8394515902488175, 'aug_scale_high': 1.1205808615484991, 'aug_drift_max_amp': 0.11290353190012256, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(281464)} -> 0.9696
2025-10-02 23:54:38,512 - INFO - bo.run_bo - üîçBO Trial 16: Using RF surrogate + Expected Improvement
2025-10-02 23:54:38,512 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:54:38,513 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:54:38,513 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:54:38,513 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001177420136263278, 'batch_size': 64, 'epochs': 21, 'head_dim': 10, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.07574477766211575, 'weight_decay': 0.008552448956757953, 'grad_clip_norm': 4.48551707290015, 'use_focal_loss': True, 'focal_gamma': 4.320215257854417, 'label_smoothing': 0.13644578755008815, 'compute_class_weights': False, 'aug_prob': 0.9713417637548024, 'aug_jitter_std': 0.04538985706497825, 'aug_scale_low': 0.8550033367756249, 'aug_scale_high': 1.1187514220166974, 'aug_drift_max_amp': 0.056679612578434624, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'seed': 205208}
2025-10-02 23:54:38,514 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001177420136263278, 'batch_size': 64, 'epochs': 21, 'head_dim': 10, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.07574477766211575, 'weight_decay': 0.008552448956757953, 'grad_clip_norm': 4.48551707290015, 'use_focal_loss': True, 'focal_gamma': 4.320215257854417, 'label_smoothing': 0.13644578755008815, 'compute_class_weights': False, 'aug_prob': 0.9713417637548024, 'aug_jitter_std': 0.04538985706497825, 'aug_scale_low': 0.8550033367756249, 'aug_scale_high': 1.1187514220166974, 'aug_drift_max_amp': 0.056679612578434624, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'seed': 205208}
2025-10-02 23:55:09,530 - INFO - _models.training_function_executor - Model: 3,677 parameters, 15.8KB storage
2025-10-02 23:55:09,530 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2712276399264422, 0.18389905425247735, 0.1427642819642763, 0.11713303568635941, 0.09926390231656457, 0.08947223631265251, 0.0812726555026829, 0.07400469408088062, 0.07005500033041762, 0.06673319778370909, 0.06301450734271355, 0.06135720426206044, 0.0581402189063844, 0.0573682767240355, 0.055152700515004455, 0.05357813107240313, 0.05203474893934001, 0.05161382239290345, 0.04987600096242491, 0.048341959705320445, 0.047588622308195085], 'val_losses': [0.20545916880466125, 0.15728506646969162, 0.13374546544210697, 0.10113159469749497, 0.08916370744484735, 0.08317103897999842, 0.07154244375869663, 0.06930031817111884, 0.08841376339690217, 0.07611797881018487, 0.0625336757677754, 0.05878281492640494, 0.05060955318867929, 0.05004141246952474, 0.05538320123016005, 0.04847212226205101, 0.0517891057250143, 0.061181494511282096, 0.048189452311259554, 0.04673607969956953, 0.04806291549005032], 'val_acc': [0.7640646535521864, 0.8501440922190202, 0.8938729482520987, 0.9122916927703295, 0.9270768074176169, 0.9307104372885603, 0.9453702543540909, 0.9467485277534143, 0.9250720461095101, 0.9359729357223405, 0.9512592406966546, 0.9560205488034081, 0.9605312617466483, 0.9596541786743515, 0.9512592406966546, 0.9612830472371883, 0.9605312617466483, 0.9513845382784112, 0.9611577496554317, 0.9627866182182684, 0.9607818569101616], 'config': {'seq_len': 1000, 'patch_size': 25, 'head_dim': 10, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'dropout': 0.07574477766211575, 'num_classes': 5, 'epochs': 21, 'batch_size': 64, 'lr': 0.001177420136263278, 'weight_decay': 0.008552448956757953, 'grad_clip_norm': 4.48551707290015, 'use_focal_loss': True, 'focal_gamma': 4.320215257854417, 'label_smoothing': 0.13644578755008815, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.9713417637548024, 'aug_jitter_std': 0.04538985706497825, 'aug_scale_low': 0.8550033367756249, 'aug_scale_high': 1.1187514220166974, 'aug_drift_max_amp': 0.056679612578434624, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 205208}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001177420136263278, 'batch_size': 64, 'epochs': 21, 'head_dim': 10, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.07574477766211575, 'weight_decay': 0.008552448956757953, 'grad_clip_norm': 4.48551707290015, 'use_focal_loss': True, 'focal_gamma': 4.320215257854417, 'label_smoothing': 0.13644578755008815, 'compute_class_weights': False, 'aug_prob': 0.9713417637548024, 'aug_jitter_std': 0.04538985706497825, 'aug_scale_low': 0.8550033367756249, 'aug_scale_high': 1.1187514220166974, 'aug_drift_max_amp': 0.056679612578434624, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'seed': 205208}, 'model_parameter_count': 3677, 'model_storage_size_kb': 15.799609375000001, 'model_size_validation': 'PASS'}
2025-10-02 23:55:09,530 - INFO - _models.training_function_executor - BO Objective: base=0.9608, size_penalty=0.0000, final=0.9608
2025-10-02 23:55:09,530 - INFO - _models.training_function_executor - Model: 3,677 parameters, 15.8KB (PASS 256KB limit)
2025-10-02 23:55:09,530 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 31.017s
2025-10-02 23:55:09,650 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9608
2025-10-02 23:55:09,650 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.120s
2025-10-02 23:55:09,650 - INFO - bo.run_bo - Recorded observation #16: hparams={'lr': 0.001177420136263278, 'batch_size': np.int64(64), 'epochs': np.int64(21), 'head_dim': np.int64(10), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(25), 'dropout': 0.07574477766211575, 'weight_decay': 0.008552448956757953, 'grad_clip_norm': 4.48551707290015, 'use_focal_loss': np.True_, 'focal_gamma': 4.320215257854417, 'label_smoothing': 0.13644578755008815, 'compute_class_weights': np.False_, 'aug_prob': 0.9713417637548024, 'aug_jitter_std': 0.04538985706497825, 'aug_scale_low': 0.8550033367756249, 'aug_scale_high': 1.1187514220166974, 'aug_drift_max_amp': 0.056679612578434624, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(205208)}, value=0.9608
2025-10-02 23:55:09,650 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 16: {'lr': 0.001177420136263278, 'batch_size': np.int64(64), 'epochs': np.int64(21), 'head_dim': np.int64(10), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(25), 'dropout': 0.07574477766211575, 'weight_decay': 0.008552448956757953, 'grad_clip_norm': 4.48551707290015, 'use_focal_loss': np.True_, 'focal_gamma': 4.320215257854417, 'label_smoothing': 0.13644578755008815, 'compute_class_weights': np.False_, 'aug_prob': 0.9713417637548024, 'aug_jitter_std': 0.04538985706497825, 'aug_scale_low': 0.8550033367756249, 'aug_scale_high': 1.1187514220166974, 'aug_drift_max_amp': 0.056679612578434624, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(205208)} -> 0.9608
2025-10-02 23:55:09,650 - INFO - bo.run_bo - üîçBO Trial 17: Using RF surrogate + Expected Improvement
2025-10-02 23:55:09,650 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:55:09,650 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:55:09,650 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:55:09,650 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.007339859354988528, 'batch_size': 128, 'epochs': 41, 'head_dim': 8, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 100, 'dropout': 0.037328835792335154, 'weight_decay': 0.00012200706749352956, 'grad_clip_norm': 3.402506195697881, 'use_focal_loss': False, 'focal_gamma': 3.620022460414162, 'label_smoothing': 0.15006509632652576, 'compute_class_weights': False, 'aug_prob': 0.6045682315060944, 'aug_jitter_std': 0.016049729648360676, 'aug_scale_low': 0.8275856968275304, 'aug_scale_high': 1.1889668364829, 'aug_drift_max_amp': 0.07886585602547536, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 204317}
2025-10-02 23:55:09,651 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.007339859354988528, 'batch_size': 128, 'epochs': 41, 'head_dim': 8, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 100, 'dropout': 0.037328835792335154, 'weight_decay': 0.00012200706749352956, 'grad_clip_norm': 3.402506195697881, 'use_focal_loss': False, 'focal_gamma': 3.620022460414162, 'label_smoothing': 0.15006509632652576, 'compute_class_weights': False, 'aug_prob': 0.6045682315060944, 'aug_jitter_std': 0.016049729648360676, 'aug_scale_low': 0.8275856968275304, 'aug_scale_high': 1.1889668364829, 'aug_drift_max_amp': 0.07886585602547536, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 204317}
2025-10-02 23:56:22,758 - INFO - _models.training_function_executor - Model: 10,793 parameters, 46.4KB storage
2025-10-02 23:56:22,759 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7617119459924709, 0.6538555755601193, 0.6372192470317787, 0.6319332130682056, 0.6213028393521276, 0.619101663906652, 0.6165926191411184, 0.6117730569324894, 0.6114036450987863, 0.6093012896116017, 0.6065853689019589, 0.606645667436812, 0.6060172523266353, 0.6039824121910965, 0.6020010990673478, 0.6007822960110097, 0.601846242406197, 0.5990559540531919, 0.5990377760985044, 0.5979693652457791, 0.5977499497413665, 0.5973375480534181, 0.5976377135697633, 0.5970518537897538, 0.5955582620341523, 0.5968304717503025, 0.5948134666247553, 0.5943083803490081, 0.5936088607905969, 0.5931850744101113, 0.5926350463031547, 0.5932141230585312, 0.5922460533475247, 0.5913540643808167, 0.5909486939927014, 0.5922676595336636, 0.5906076287243393, 0.5897643982634054, 0.5894018988955406, 0.5894667923597974, 0.5890267813515291], 'val_losses': [0.6878601797181791, 0.6524766297541141, 0.6455249425657201, 0.6208758629767941, 0.6171416439757343, 0.6506586974089255, 0.6225286558851116, 0.6148408652920275, 0.6070927131476221, 0.6011856803692697, 0.6067489968805799, 0.6186555259330218, 0.6169060444700406, 0.6000987246750202, 0.6006799007459864, 0.6213462876953916, 0.6031365173978176, 0.6003523540965836, 0.600009525200132, 0.6204667129980832, 0.6618204910479427, 0.5927421339710409, 0.5989893262449952, 0.6035397101533008, 0.6050761709713933, 0.5992718580193036, 0.5938916316144136, 0.5980124580592772, 0.5960255095929136, 0.6344466431071295, 0.5963920371124073, 0.5896741143295811, 0.5984727683453882, 0.5929872093381716, 0.5903966688688589, 0.5986534997479178, 0.5891186999521851, 0.5916246710945648, 0.5897082404196524, 0.6121706656981882, 0.5923039765902263], 'val_acc': [0.9428643027189575, 0.9538904899135446, 0.9592782859290816, 0.9635384037088084, 0.9629119158000251, 0.9670467359979953, 0.970930961032452, 0.9715574489412354, 0.9674226287432652, 0.9713068537777221, 0.9719333416865055, 0.9713068537777221, 0.967547926325022, 0.9719333416865055, 0.9719333416865055, 0.9769452449567724, 0.9730610199223155, 0.9723092344317754, 0.9719333416865055, 0.9664202480892119, 0.9681744142338052, 0.976694649793259, 0.9784488159378524, 0.9705550682871821, 0.9739381029946123, 0.9706803658689387, 0.9773211377020423, 0.9708056634506954, 0.9736875078310988, 0.9792006014283924, 0.9719333416865055, 0.9797017917554192, 0.9749404836486656, 0.9775717328655557, 0.9749404836486656, 0.9724345320135321, 0.9809547675729858, 0.9807041724094725, 0.9769452449567724, 0.9775717328655557, 0.9768199473750157], 'config': {'seq_len': 1000, 'patch_size': 100, 'head_dim': 8, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.037328835792335154, 'num_classes': 5, 'epochs': 41, 'batch_size': 128, 'lr': 0.007339859354988528, 'weight_decay': 0.00012200706749352956, 'grad_clip_norm': 3.402506195697881, 'use_focal_loss': False, 'focal_gamma': 3.620022460414162, 'label_smoothing': 0.15006509632652576, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.6045682315060944, 'aug_jitter_std': 0.016049729648360676, 'aug_scale_low': 0.8275856968275304, 'aug_scale_high': 1.1889668364829, 'aug_drift_max_amp': 0.07886585602547536, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 204317}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.007339859354988528, 'batch_size': 128, 'epochs': 41, 'head_dim': 8, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 100, 'dropout': 0.037328835792335154, 'weight_decay': 0.00012200706749352956, 'grad_clip_norm': 3.402506195697881, 'use_focal_loss': False, 'focal_gamma': 3.620022460414162, 'label_smoothing': 0.15006509632652576, 'compute_class_weights': False, 'aug_prob': 0.6045682315060944, 'aug_jitter_std': 0.016049729648360676, 'aug_scale_low': 0.8275856968275304, 'aug_scale_high': 1.1889668364829, 'aug_drift_max_amp': 0.07886585602547536, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 204317}, 'model_parameter_count': 10793, 'model_storage_size_kb': 46.376171875000004, 'model_size_validation': 'PASS'}
2025-10-02 23:56:22,759 - INFO - _models.training_function_executor - BO Objective: base=0.9768, size_penalty=0.0000, final=0.9768
2025-10-02 23:56:22,759 - INFO - _models.training_function_executor - Model: 10,793 parameters, 46.4KB (PASS 256KB limit)
2025-10-02 23:56:22,759 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 73.108s
2025-10-02 23:56:22,879 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9768
2025-10-02 23:56:22,879 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.121s
2025-10-02 23:56:22,879 - INFO - bo.run_bo - Recorded observation #17: hparams={'lr': 0.007339859354988528, 'batch_size': np.int64(128), 'epochs': np.int64(41), 'head_dim': np.int64(8), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(100), 'dropout': 0.037328835792335154, 'weight_decay': 0.00012200706749352956, 'grad_clip_norm': 3.402506195697881, 'use_focal_loss': np.False_, 'focal_gamma': 3.620022460414162, 'label_smoothing': 0.15006509632652576, 'compute_class_weights': np.False_, 'aug_prob': 0.6045682315060944, 'aug_jitter_std': 0.016049729648360676, 'aug_scale_low': 0.8275856968275304, 'aug_scale_high': 1.1889668364829, 'aug_drift_max_amp': 0.07886585602547536, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(204317)}, value=0.9768
2025-10-02 23:56:22,879 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 17: {'lr': 0.007339859354988528, 'batch_size': np.int64(128), 'epochs': np.int64(41), 'head_dim': np.int64(8), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(100), 'dropout': 0.037328835792335154, 'weight_decay': 0.00012200706749352956, 'grad_clip_norm': 3.402506195697881, 'use_focal_loss': np.False_, 'focal_gamma': 3.620022460414162, 'label_smoothing': 0.15006509632652576, 'compute_class_weights': np.False_, 'aug_prob': 0.6045682315060944, 'aug_jitter_std': 0.016049729648360676, 'aug_scale_low': 0.8275856968275304, 'aug_scale_high': 1.1889668364829, 'aug_drift_max_amp': 0.07886585602547536, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(204317)} -> 0.9768
2025-10-02 23:56:22,880 - INFO - bo.run_bo - üîçBO Trial 18: Using RF surrogate + Expected Improvement
2025-10-02 23:56:22,880 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:56:22,880 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:56:22,880 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:56:22,880 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006139120306445673, 'batch_size': 64, 'epochs': 5, 'head_dim': 11, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 40, 'dropout': 0.3344500390481808, 'weight_decay': 0.00035708286782019986, 'grad_clip_norm': 4.399701082354561, 'use_focal_loss': False, 'focal_gamma': 3.8946784883897534, 'label_smoothing': 0.018156309310940835, 'compute_class_weights': False, 'aug_prob': 0.34537131488980205, 'aug_jitter_std': 0.000657088683334678, 'aug_scale_low': 0.9944475227527488, 'aug_scale_high': 1.1234666616391713, 'aug_drift_max_amp': 0.1832527998948744, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 89370}
2025-10-02 23:56:22,881 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006139120306445673, 'batch_size': 64, 'epochs': 5, 'head_dim': 11, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 40, 'dropout': 0.3344500390481808, 'weight_decay': 0.00035708286782019986, 'grad_clip_norm': 4.399701082354561, 'use_focal_loss': False, 'focal_gamma': 3.8946784883897534, 'label_smoothing': 0.018156309310940835, 'compute_class_weights': False, 'aug_prob': 0.34537131488980205, 'aug_jitter_std': 0.000657088683334678, 'aug_scale_low': 0.9944475227527488, 'aug_scale_high': 1.1234666616391713, 'aug_drift_max_amp': 0.1832527998948744, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 89370}
2025-10-02 23:56:30,994 - INFO - _models.training_function_executor - Model: 7,913 parameters, 8.5KB storage
2025-10-02 23:56:30,995 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.43432137094146567, 0.2687801958060267, 0.24735633194394283, 0.23142880695944565, 0.22096304691581353], 'val_losses': [0.32483122501811235, 0.27114451096972314, 0.22777511338709053, 0.23333052386243963, 0.2054112669654156], 'val_acc': [0.9345946623230171, 0.9492544793885478, 0.961408344818945, 0.9558952512216514, 0.9652925698534018], 'config': {'seq_len': 1000, 'patch_size': 40, 'head_dim': 11, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'dropout': 0.3344500390481808, 'num_classes': 5, 'epochs': 5, 'batch_size': 64, 'lr': 0.006139120306445673, 'weight_decay': 0.00035708286782019986, 'grad_clip_norm': 4.399701082354561, 'use_focal_loss': False, 'focal_gamma': 3.8946784883897534, 'label_smoothing': 0.018156309310940835, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.34537131488980205, 'aug_jitter_std': 0.000657088683334678, 'aug_scale_low': 0.9944475227527488, 'aug_scale_high': 1.1234666616391713, 'aug_drift_max_amp': 0.1832527998948744, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'seed': 89370}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006139120306445673, 'batch_size': 64, 'epochs': 5, 'head_dim': 11, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 40, 'dropout': 0.3344500390481808, 'weight_decay': 0.00035708286782019986, 'grad_clip_norm': 4.399701082354561, 'use_focal_loss': False, 'focal_gamma': 3.8946784883897534, 'label_smoothing': 0.018156309310940835, 'compute_class_weights': False, 'aug_prob': 0.34537131488980205, 'aug_jitter_std': 0.000657088683334678, 'aug_scale_low': 0.9944475227527488, 'aug_scale_high': 1.1234666616391713, 'aug_drift_max_amp': 0.1832527998948744, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 89370}, 'model_parameter_count': 7913, 'model_storage_size_kb': 8.500292968750001, 'model_size_validation': 'PASS'}
2025-10-02 23:56:30,995 - INFO - _models.training_function_executor - BO Objective: base=0.9653, size_penalty=0.0000, final=0.9653
2025-10-02 23:56:30,995 - INFO - _models.training_function_executor - Model: 7,913 parameters, 8.5KB (PASS 256KB limit)
2025-10-02 23:56:30,995 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 8.115s
2025-10-02 23:56:31,116 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9653
2025-10-02 23:56:31,116 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.121s
2025-10-02 23:56:31,116 - INFO - bo.run_bo - Recorded observation #18: hparams={'lr': 0.006139120306445673, 'batch_size': np.int64(64), 'epochs': np.int64(5), 'head_dim': np.int64(11), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(40), 'dropout': 0.3344500390481808, 'weight_decay': 0.00035708286782019986, 'grad_clip_norm': 4.399701082354561, 'use_focal_loss': np.False_, 'focal_gamma': 3.8946784883897534, 'label_smoothing': 0.018156309310940835, 'compute_class_weights': np.False_, 'aug_prob': 0.34537131488980205, 'aug_jitter_std': 0.000657088683334678, 'aug_scale_low': 0.9944475227527488, 'aug_scale_high': 1.1234666616391713, 'aug_drift_max_amp': 0.1832527998948744, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(89370)}, value=0.9653
2025-10-02 23:56:31,116 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 18: {'lr': 0.006139120306445673, 'batch_size': np.int64(64), 'epochs': np.int64(5), 'head_dim': np.int64(11), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(40), 'dropout': 0.3344500390481808, 'weight_decay': 0.00035708286782019986, 'grad_clip_norm': 4.399701082354561, 'use_focal_loss': np.False_, 'focal_gamma': 3.8946784883897534, 'label_smoothing': 0.018156309310940835, 'compute_class_weights': np.False_, 'aug_prob': 0.34537131488980205, 'aug_jitter_std': 0.000657088683334678, 'aug_scale_low': 0.9944475227527488, 'aug_scale_high': 1.1234666616391713, 'aug_drift_max_amp': 0.1832527998948744, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(89370)} -> 0.9653
2025-10-02 23:56:31,116 - INFO - bo.run_bo - üîçBO Trial 19: Using RF surrogate + Expected Improvement
2025-10-02 23:56:31,116 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:56:31,116 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:56:31,116 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:56:31,116 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006132316206546906, 'batch_size': 32, 'epochs': 50, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.34698977749547977, 'weight_decay': 4.448015676125961e-06, 'grad_clip_norm': 3.756329768589258, 'use_focal_loss': True, 'focal_gamma': 4.724414476629648, 'label_smoothing': 0.1149686330094148, 'compute_class_weights': False, 'aug_prob': 0.8178382434880991, 'aug_jitter_std': 0.03315100497303667, 'aug_scale_low': 0.9280723971112907, 'aug_scale_high': 1.145994655002148, 'aug_drift_max_amp': 0.17365594834936995, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 135566}
2025-10-02 23:56:31,118 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006132316206546906, 'batch_size': 32, 'epochs': 50, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.34698977749547977, 'weight_decay': 4.448015676125961e-06, 'grad_clip_norm': 3.756329768589258, 'use_focal_loss': True, 'focal_gamma': 4.724414476629648, 'label_smoothing': 0.1149686330094148, 'compute_class_weights': False, 'aug_prob': 0.8178382434880991, 'aug_jitter_std': 0.03315100497303667, 'aug_scale_low': 0.9280723971112907, 'aug_scale_high': 1.145994655002148, 'aug_drift_max_amp': 0.17365594834936995, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 135566}
2025-10-02 23:58:35,050 - INFO - _models.training_function_executor - Model: 13,557 parameters, 58.3KB storage
2025-10-02 23:58:35,050 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.11213452218296653, 0.055095419247538115, 0.045990957939066136, 0.04248354085972118, 0.040310340872323776, 0.037179004679988976, 0.035427301885310486, 0.033474197111957545, 0.03337947452146842, 0.032240821679768335, 0.031300052463706325, 0.03047523514069744, 0.02957533693037169, 0.029304489736506425, 0.028467045526802064, 0.027414711583240606, 0.026390495294490838, 0.027666648280484554, 0.02646459359748581, 0.025639946843917766, 0.02619970748396187, 0.02516271260554202, 0.02448828527072847, 0.024332349742815694, 0.023831256232696878, 0.024430650094090442, 0.02384636047154528, 0.023862612622239553, 0.02310209300083002, 0.023395875434103864, 0.022479941395762808, 0.02334245192725191, 0.022098897138534454, 0.02178909450991614, 0.02180386134617177, 0.021703559918710657, 0.022051032926561458, 0.02106570200528118, 0.021509279030737613, 0.021580415837814705, 0.02127573535434921, 0.020787789508375024, 0.0200924407171174, 0.02100967817575405, 0.020717089000178053, 0.02051189246155417, 0.020106450267426905, 0.020320297220872204, 0.01977245531242438, 0.020141431742394134], 'val_losses': [0.060088808179768, 0.04831390455878628, 0.042249268076052685, 0.04200047573552904, 0.03312872906418884, 0.030892045568880647, 0.02934260526152724, 0.033140679575150926, 0.02811163046607665, 0.026817579311082394, 0.027673517434000445, 0.026936462879285465, 0.02554026849697652, 0.027830996576175254, 0.027876933032357144, 0.025618151915042027, 0.03016523123739686, 0.026173904444800628, 0.02406211017203083, 0.023009872870806122, 0.024681442379279328, 0.026255470572550364, 0.02550222284138733, 0.021864524952504377, 0.022593724941971337, 0.022361945450985556, 0.023822255435406364, 0.022410012823629314, 0.024392554437273235, 0.02204037097078146, 0.022069670012564515, 0.02123060966625412, 0.020906654697076345, 0.021633161406680736, 0.023968242298330434, 0.0204726592529658, 0.02138940099560965, 0.02154561002523943, 0.0206786710785275, 0.02215346977975401, 0.020823141522850253, 0.022871196854384886, 0.020569090619300515, 0.02002924175198074, 0.020636090066607413, 0.020947390746956795, 0.02017035032742126, 0.02011801990451796, 0.019561438148757476, 0.021178182824566735], 'val_acc': [0.930083949379777, 0.9459967422628743, 0.9453702543540909, 0.9498809672973312, 0.9601553690013783, 0.9667961408344818, 0.9635384037088084, 0.9616589399824583, 0.9671720335797519, 0.9688009021425886, 0.9677985214885353, 0.9680491166520486, 0.9728104247588022, 0.968299711815562, 0.9662949505074552, 0.9713068537777221, 0.9634131061270518, 0.9724345320135321, 0.9724345320135321, 0.9736875078310988, 0.9723092344317754, 0.9721839368500188, 0.970179175541912, 0.9768199473750157, 0.9718080441047487, 0.9756922691392056, 0.9715574489412354, 0.9714321513594788, 0.9662949505074552, 0.9740634005763689, 0.9759428643027189, 0.9771958401202857, 0.977822328029069, 0.9779476256108257, 0.9724345320135321, 0.9769452449567724, 0.9758175667209623, 0.977822328029069, 0.976318757047989, 0.9740634005763689, 0.9771958401202857, 0.9736875078310988, 0.9779476256108257, 0.9807041724094725, 0.9790753038466358, 0.9775717328655557, 0.9794511965919058, 0.9776970304473124, 0.9797017917554192, 0.9746898884851523], 'config': {'seq_len': 1000, 'patch_size': 20, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 5, 'dropout': 0.34698977749547977, 'num_classes': 5, 'epochs': 50, 'batch_size': 32, 'lr': 0.006132316206546906, 'weight_decay': 4.448015676125961e-06, 'grad_clip_norm': 3.756329768589258, 'use_focal_loss': True, 'focal_gamma': 4.724414476629648, 'label_smoothing': 0.1149686330094148, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.8178382434880991, 'aug_jitter_std': 0.03315100497303667, 'aug_scale_low': 0.9280723971112907, 'aug_scale_high': 1.145994655002148, 'aug_drift_max_amp': 0.17365594834936995, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 135566}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006132316206546906, 'batch_size': 32, 'epochs': 50, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.34698977749547977, 'weight_decay': 4.448015676125961e-06, 'grad_clip_norm': 3.756329768589258, 'use_focal_loss': True, 'focal_gamma': 4.724414476629648, 'label_smoothing': 0.1149686330094148, 'compute_class_weights': False, 'aug_prob': 0.8178382434880991, 'aug_jitter_std': 0.03315100497303667, 'aug_scale_low': 0.9280723971112907, 'aug_scale_high': 1.145994655002148, 'aug_drift_max_amp': 0.17365594834936995, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 135566}, 'model_parameter_count': 13557, 'model_storage_size_kb': 58.252734375, 'model_size_validation': 'PASS'}
2025-10-02 23:58:35,050 - INFO - _models.training_function_executor - BO Objective: base=0.9747, size_penalty=0.0000, final=0.9747
2025-10-02 23:58:35,050 - INFO - _models.training_function_executor - Model: 13,557 parameters, 58.3KB (PASS 256KB limit)
2025-10-02 23:58:35,050 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 123.934s
2025-10-02 23:58:35,172 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9747
2025-10-02 23:58:35,172 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.122s
2025-10-02 23:58:35,172 - INFO - bo.run_bo - Recorded observation #19: hparams={'lr': 0.006132316206546906, 'batch_size': np.int64(32), 'epochs': np.int64(50), 'head_dim': np.int64(8), 'n_heads': np.int64(6), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(20), 'dropout': 0.34698977749547977, 'weight_decay': 4.448015676125961e-06, 'grad_clip_norm': 3.756329768589258, 'use_focal_loss': np.True_, 'focal_gamma': 4.724414476629648, 'label_smoothing': 0.1149686330094148, 'compute_class_weights': np.False_, 'aug_prob': 0.8178382434880991, 'aug_jitter_std': 0.03315100497303667, 'aug_scale_low': 0.9280723971112907, 'aug_scale_high': 1.145994655002148, 'aug_drift_max_amp': 0.17365594834936995, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(135566)}, value=0.9747
2025-10-02 23:58:35,172 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 19: {'lr': 0.006132316206546906, 'batch_size': np.int64(32), 'epochs': np.int64(50), 'head_dim': np.int64(8), 'n_heads': np.int64(6), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(20), 'dropout': 0.34698977749547977, 'weight_decay': 4.448015676125961e-06, 'grad_clip_norm': 3.756329768589258, 'use_focal_loss': np.True_, 'focal_gamma': 4.724414476629648, 'label_smoothing': 0.1149686330094148, 'compute_class_weights': np.False_, 'aug_prob': 0.8178382434880991, 'aug_jitter_std': 0.03315100497303667, 'aug_scale_low': 0.9280723971112907, 'aug_scale_high': 1.145994655002148, 'aug_drift_max_amp': 0.17365594834936995, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(135566)} -> 0.9747
2025-10-02 23:58:35,172 - INFO - bo.run_bo - üîçBO Trial 20: Using RF surrogate + Expected Improvement
2025-10-02 23:58:35,172 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:58:35,172 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:58:35,172 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:58:35,172 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0039042707043039425, 'batch_size': 256, 'epochs': 24, 'head_dim': 10, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 3, 'patch_size': 100, 'dropout': 0.22554235554116314, 'weight_decay': 3.803246699078484e-05, 'grad_clip_norm': 0.13351603243845214, 'use_focal_loss': True, 'focal_gamma': 3.7888946962390193, 'label_smoothing': 0.08577799680382911, 'compute_class_weights': False, 'aug_prob': 0.33938635096994313, 'aug_jitter_std': 0.047936254622275735, 'aug_scale_low': 0.9708815940412696, 'aug_scale_high': 1.0789981058269578, 'aug_drift_max_amp': 0.09328494569176245, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'seed': 142362}
2025-10-02 23:58:35,174 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0039042707043039425, 'batch_size': 256, 'epochs': 24, 'head_dim': 10, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 3, 'patch_size': 100, 'dropout': 0.22554235554116314, 'weight_decay': 3.803246699078484e-05, 'grad_clip_norm': 0.13351603243845214, 'use_focal_loss': True, 'focal_gamma': 3.7888946962390193, 'label_smoothing': 0.08577799680382911, 'compute_class_weights': False, 'aug_prob': 0.33938635096994313, 'aug_jitter_std': 0.047936254622275735, 'aug_scale_low': 0.9708815940412696, 'aug_scale_high': 1.0789981058269578, 'aug_drift_max_amp': 0.09328494569176245, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'seed': 142362}
2025-10-02 23:59:06,790 - INFO - _models.training_function_executor - Model: 2,301 parameters, 9.9KB storage
2025-10-02 23:59:06,790 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2492186000340005, 0.16800052339597196, 0.14224013509968972, 0.1229325095785873, 0.11043320472776952, 0.10279994931676366, 0.09392916188458544, 0.08680836242426906, 0.08194602131484142, 0.07521935268670354, 0.07235588854797217, 0.07005213788821017, 0.06638522826386523, 0.06536488347270135, 0.06236676796367547, 0.06173008335681632, 0.05865341302387749, 0.05791182223404844, 0.05674953841010688, 0.0560416674965348, 0.054414629992699186, 0.0531612348388056, 0.05265322957341113, 0.0523742289613711], 'val_losses': [0.19277353341205872, 0.15474269632471785, 0.13026169198708135, 0.11385219325630636, 0.10104892863542525, 0.10349903382289052, 0.08429948153372636, 0.08670585720614672, 0.0691956119860881, 0.0777325906823817, 0.06565129750672774, 0.06808891107699669, 0.06725035496770237, 0.07327247688367304, 0.06018498168749224, 0.0560077429097811, 0.06110525086275648, 0.05381360865056761, 0.059394212384810796, 0.052445646809980456, 0.055618716244440064, 0.05055034491553848, 0.056113496428835646, 0.04560821549657696], 'val_acc': [0.8170655306352588, 0.8724470617717078, 0.8931211627615587, 0.9131687758426262, 0.9245708557824834, 0.9091592532264128, 0.9353464478135572, 0.9303345445432903, 0.946122039844631, 0.9328404961784238, 0.9436160882094976, 0.9418619220649042, 0.9433654930459842, 0.939606565593284, 0.9452449567723343, 0.9459967422628743, 0.9447437664453076, 0.9497556697155745, 0.9422378148101742, 0.9561458463851648, 0.9488785866432777, 0.9562711439669215, 0.9513845382784112, 0.9591529883473249], 'config': {'seq_len': 1000, 'patch_size': 100, 'head_dim': 10, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 3, 'dropout': 0.22554235554116314, 'num_classes': 5, 'epochs': 24, 'batch_size': 256, 'lr': 0.0039042707043039425, 'weight_decay': 3.803246699078484e-05, 'grad_clip_norm': 0.13351603243845214, 'use_focal_loss': True, 'focal_gamma': 3.7888946962390193, 'label_smoothing': 0.08577799680382911, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.33938635096994313, 'aug_jitter_std': 0.047936254622275735, 'aug_scale_low': 0.9708815940412696, 'aug_scale_high': 1.0789981058269578, 'aug_drift_max_amp': 0.09328494569176245, 'normalize': True}, 'quantization': {'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 142362}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0039042707043039425, 'batch_size': 256, 'epochs': 24, 'head_dim': 10, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 3, 'patch_size': 100, 'dropout': 0.22554235554116314, 'weight_decay': 3.803246699078484e-05, 'grad_clip_norm': 0.13351603243845214, 'use_focal_loss': True, 'focal_gamma': 3.7888946962390193, 'label_smoothing': 0.08577799680382911, 'compute_class_weights': False, 'aug_prob': 0.33938635096994313, 'aug_jitter_std': 0.047936254622275735, 'aug_scale_low': 0.9708815940412696, 'aug_scale_high': 1.0789981058269578, 'aug_drift_max_amp': 0.09328494569176245, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'seed': 142362}, 'model_parameter_count': 2301, 'model_storage_size_kb': 9.887109375000001, 'model_size_validation': 'PASS'}
2025-10-02 23:59:06,790 - INFO - _models.training_function_executor - BO Objective: base=0.9592, size_penalty=0.0000, final=0.9592
2025-10-02 23:59:06,790 - INFO - _models.training_function_executor - Model: 2,301 parameters, 9.9KB (PASS 256KB limit)
2025-10-02 23:59:06,790 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 31.618s
2025-10-02 23:59:06,913 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9592
2025-10-02 23:59:06,913 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.123s
2025-10-02 23:59:06,913 - INFO - bo.run_bo - Recorded observation #20: hparams={'lr': 0.0039042707043039425, 'batch_size': np.int64(256), 'epochs': np.int64(24), 'head_dim': np.int64(10), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(100), 'dropout': 0.22554235554116314, 'weight_decay': 3.803246699078484e-05, 'grad_clip_norm': 0.13351603243845214, 'use_focal_loss': np.True_, 'focal_gamma': 3.7888946962390193, 'label_smoothing': 0.08577799680382911, 'compute_class_weights': np.False_, 'aug_prob': 0.33938635096994313, 'aug_jitter_std': 0.047936254622275735, 'aug_scale_low': 0.9708815940412696, 'aug_scale_high': 1.0789981058269578, 'aug_drift_max_amp': 0.09328494569176245, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(142362)}, value=0.9592
2025-10-02 23:59:06,913 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 20: {'lr': 0.0039042707043039425, 'batch_size': np.int64(256), 'epochs': np.int64(24), 'head_dim': np.int64(10), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(100), 'dropout': 0.22554235554116314, 'weight_decay': 3.803246699078484e-05, 'grad_clip_norm': 0.13351603243845214, 'use_focal_loss': np.True_, 'focal_gamma': 3.7888946962390193, 'label_smoothing': 0.08577799680382911, 'compute_class_weights': np.False_, 'aug_prob': 0.33938635096994313, 'aug_jitter_std': 0.047936254622275735, 'aug_scale_low': 0.9708815940412696, 'aug_scale_high': 1.0789981058269578, 'aug_drift_max_amp': 0.09328494569176245, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(142362)} -> 0.9592
2025-10-02 23:59:06,914 - INFO - bo.run_bo - üîçBO Trial 21: Using RF surrogate + Expected Improvement
2025-10-02 23:59:06,914 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:59:06,914 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:59:06,914 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:59:06,914 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0016105027986591158, 'batch_size': 128, 'epochs': 7, 'head_dim': 11, 'n_heads': 8, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.22105567555548616, 'weight_decay': 1.251285428979249e-06, 'grad_clip_norm': 1.3874678860554588, 'use_focal_loss': False, 'focal_gamma': 2.4121033067476567, 'label_smoothing': 0.18452014556835683, 'compute_class_weights': False, 'aug_prob': 0.46782804189205207, 'aug_jitter_std': 0.01540944632667883, 'aug_scale_low': 0.9872980659446808, 'aug_scale_high': 1.1252168302835368, 'aug_drift_max_amp': 0.07752043360604652, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'seed': 87497}
2025-10-02 23:59:06,915 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0016105027986591158, 'batch_size': 128, 'epochs': 7, 'head_dim': 11, 'n_heads': 8, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.22105567555548616, 'weight_decay': 1.251285428979249e-06, 'grad_clip_norm': 1.3874678860554588, 'use_focal_loss': False, 'focal_gamma': 2.4121033067476567, 'label_smoothing': 0.18452014556835683, 'compute_class_weights': False, 'aug_prob': 0.46782804189205207, 'aug_jitter_std': 0.01540944632667883, 'aug_scale_low': 0.9872980659446808, 'aug_scale_high': 1.1252168302835368, 'aug_drift_max_amp': 0.07752043360604652, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'seed': 87497}
2025-10-02 23:59:19,454 - INFO - _models.training_function_executor - Model: 18,677 parameters, 80.3KB storage
2025-10-02 23:59:19,454 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.907420298412241, 0.7723174439752789, 0.7433202081622402, 0.729394061245073, 0.7218683280805311, 0.7157727076763595, 0.7115907407948141], 'val_losses': [0.7960290442192978, 0.7534935631155744, 0.738803640648381, 0.7298846986042559, 0.7158255841465855, 0.7142451821062413, 0.7101534362156549], 'val_acc': [0.9258238316000501, 0.9442425761182809, 0.9508833479513845, 0.9538904899135446, 0.960656559328405, 0.9597794762561083, 0.9571482270392181], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 11, 'n_heads': 8, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'dropout': 0.22105567555548616, 'num_classes': 5, 'epochs': 7, 'batch_size': 128, 'lr': 0.0016105027986591158, 'weight_decay': 1.251285428979249e-06, 'grad_clip_norm': 1.3874678860554588, 'use_focal_loss': False, 'focal_gamma': 2.4121033067476567, 'label_smoothing': 0.18452014556835683, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.46782804189205207, 'aug_jitter_std': 0.01540944632667883, 'aug_scale_low': 0.9872980659446808, 'aug_scale_high': 1.1252168302835368, 'aug_drift_max_amp': 0.07752043360604652, 'normalize': True}, 'quantization': {'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 87497}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0016105027986591158, 'batch_size': 128, 'epochs': 7, 'head_dim': 11, 'n_heads': 8, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.22105567555548616, 'weight_decay': 1.251285428979249e-06, 'grad_clip_norm': 1.3874678860554588, 'use_focal_loss': False, 'focal_gamma': 2.4121033067476567, 'label_smoothing': 0.18452014556835683, 'compute_class_weights': False, 'aug_prob': 0.46782804189205207, 'aug_jitter_std': 0.01540944632667883, 'aug_scale_low': 0.9872980659446808, 'aug_scale_high': 1.1252168302835368, 'aug_drift_max_amp': 0.07752043360604652, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'seed': 87497}, 'model_parameter_count': 18677, 'model_storage_size_kb': 80.252734375, 'model_size_validation': 'PASS'}
2025-10-02 23:59:19,454 - INFO - _models.training_function_executor - BO Objective: base=0.9571, size_penalty=0.0000, final=0.9571
2025-10-02 23:59:19,454 - INFO - _models.training_function_executor - Model: 18,677 parameters, 80.3KB (PASS 256KB limit)
2025-10-02 23:59:19,455 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 12.541s
2025-10-02 23:59:19,585 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9571
2025-10-02 23:59:19,585 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.130s
2025-10-02 23:59:19,585 - INFO - bo.run_bo - Recorded observation #21: hparams={'lr': 0.0016105027986591158, 'batch_size': np.int64(128), 'epochs': np.int64(7), 'head_dim': np.int64(11), 'n_heads': np.int64(8), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.22105567555548616, 'weight_decay': 1.251285428979249e-06, 'grad_clip_norm': 1.3874678860554588, 'use_focal_loss': np.False_, 'focal_gamma': 2.4121033067476567, 'label_smoothing': 0.18452014556835683, 'compute_class_weights': np.False_, 'aug_prob': 0.46782804189205207, 'aug_jitter_std': 0.01540944632667883, 'aug_scale_low': 0.9872980659446808, 'aug_scale_high': 1.1252168302835368, 'aug_drift_max_amp': 0.07752043360604652, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(87497)}, value=0.9571
2025-10-02 23:59:19,585 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 21: {'lr': 0.0016105027986591158, 'batch_size': np.int64(128), 'epochs': np.int64(7), 'head_dim': np.int64(11), 'n_heads': np.int64(8), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.22105567555548616, 'weight_decay': 1.251285428979249e-06, 'grad_clip_norm': 1.3874678860554588, 'use_focal_loss': np.False_, 'focal_gamma': 2.4121033067476567, 'label_smoothing': 0.18452014556835683, 'compute_class_weights': np.False_, 'aug_prob': 0.46782804189205207, 'aug_jitter_std': 0.01540944632667883, 'aug_scale_low': 0.9872980659446808, 'aug_scale_high': 1.1252168302835368, 'aug_drift_max_amp': 0.07752043360604652, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(87497)} -> 0.9571
2025-10-02 23:59:19,585 - INFO - bo.run_bo - üîçBO Trial 22: Using RF surrogate + Expected Improvement
2025-10-02 23:59:19,585 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:59:19,585 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:59:19,586 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:59:19,586 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00025932675354219136, 'batch_size': 256, 'epochs': 32, 'head_dim': 12, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 20, 'dropout': 0.07716883280650967, 'weight_decay': 5.010983171768417e-05, 'grad_clip_norm': 2.122203511487365, 'use_focal_loss': True, 'focal_gamma': 4.756690581127159, 'label_smoothing': 0.14859531882420227, 'compute_class_weights': False, 'aug_prob': 0.8776673284196131, 'aug_jitter_std': 0.030701136369334303, 'aug_scale_low': 0.9766585588736356, 'aug_scale_high': 1.1701866352111554, 'aug_drift_max_amp': 0.07703549425007233, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 16028}
2025-10-02 23:59:19,587 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00025932675354219136, 'batch_size': 256, 'epochs': 32, 'head_dim': 12, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 20, 'dropout': 0.07716883280650967, 'weight_decay': 5.010983171768417e-05, 'grad_clip_norm': 2.122203511487365, 'use_focal_loss': True, 'focal_gamma': 4.756690581127159, 'label_smoothing': 0.14859531882420227, 'compute_class_weights': False, 'aug_prob': 0.8776673284196131, 'aug_jitter_std': 0.030701136369334303, 'aug_scale_low': 0.9766585588736356, 'aug_scale_high': 1.1701866352111554, 'aug_drift_max_amp': 0.07703549425007233, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 16028}
2025-10-02 23:59:58,643 - INFO - _models.training_function_executor - Model: 4,861 parameters, 20.9KB storage
2025-10-02 23:59:58,643 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3494190678980231, 0.23979712463641112, 0.20921765682353594, 0.1922973643666173, 0.181635215609152, 0.17335987665769514, 0.16642456244780815, 0.15979223858974811, 0.15449335533289205, 0.14937354348821866, 0.14466005664047948, 0.14039250923426785, 0.13640103810378282, 0.13222328231358996, 0.1285550508511506, 0.12484289778011565, 0.12076513942818749, 0.11720947968468004, 0.11432053887082853, 0.11121227422250272, 0.1086958270517988, 0.10588938126440459, 0.10268644803550142, 0.09989201045362504, 0.09856747311115437, 0.09591389708056418, 0.09418735616145993, 0.09315609819357383, 0.09119521993194402, 0.08916134116146285, 0.08755173316283232, 0.08585728218446559], 'val_losses': [0.2632039608908781, 0.22498159314617897, 0.2020093962380259, 0.1874633671746429, 0.17861336124800692, 0.1710998145397811, 0.16807151617090918, 0.15648352963619552, 0.15242813887743703, 0.14774763930666912, 0.1419129852854792, 0.1382768861281963, 0.1331438448360847, 0.12989780056907516, 0.12936497887167292, 0.12255529384341714, 0.11748521685947558, 0.1163983302345634, 0.11264325456259439, 0.10895911635299581, 0.10569594579643436, 0.10474570950288072, 0.0992525660930802, 0.09776303718699024, 0.09503149349686348, 0.09524553743992857, 0.09109630394629437, 0.09096116837790445, 0.08832541694160975, 0.08605161441898244, 0.08537460151159394, 0.0824995258380306], 'val_acc': [0.7193334168650545, 0.7243453201353214, 0.7521613832853026, 0.7738378649292069, 0.7849893497055507, 0.8017792256609447, 0.814308983836612, 0.8213256484149856, 0.8398696905149731, 0.8317253477007894, 0.8549054003257737, 0.8570354592156372, 0.8591655181055006, 0.8783360481142714, 0.8739506327527878, 0.8879839619095351, 0.8883598546548052, 0.8953765192331788, 0.8876080691642652, 0.8888610449818318, 0.8924946748527753, 0.913294073424383, 0.914045858914923, 0.9160506202230297, 0.9171782984588397, 0.9203107380027565, 0.9189324646034331, 0.9243202606189701, 0.92356847512843, 0.9230672848014033, 0.9261997243453202, 0.9277032953264002], 'config': {'seq_len': 1000, 'patch_size': 20, 'head_dim': 12, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 7, 'dropout': 0.07716883280650967, 'num_classes': 5, 'epochs': 32, 'batch_size': 256, 'lr': 0.00025932675354219136, 'weight_decay': 5.010983171768417e-05, 'grad_clip_norm': 2.122203511487365, 'use_focal_loss': True, 'focal_gamma': 4.756690581127159, 'label_smoothing': 0.14859531882420227, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.8776673284196131, 'aug_jitter_std': 0.030701136369334303, 'aug_scale_low': 0.9766585588736356, 'aug_scale_high': 1.1701866352111554, 'aug_drift_max_amp': 0.07703549425007233, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 16028}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00025932675354219136, 'batch_size': 256, 'epochs': 32, 'head_dim': 12, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 20, 'dropout': 0.07716883280650967, 'weight_decay': 5.010983171768417e-05, 'grad_clip_norm': 2.122203511487365, 'use_focal_loss': True, 'focal_gamma': 4.756690581127159, 'label_smoothing': 0.14859531882420227, 'compute_class_weights': False, 'aug_prob': 0.8776673284196131, 'aug_jitter_std': 0.030701136369334303, 'aug_scale_low': 0.9766585588736356, 'aug_scale_high': 1.1701866352111554, 'aug_drift_max_amp': 0.07703549425007233, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 16028}, 'model_parameter_count': 4861, 'model_storage_size_kb': 20.887109375, 'model_size_validation': 'PASS'}
2025-10-02 23:59:58,643 - INFO - _models.training_function_executor - BO Objective: base=0.9277, size_penalty=0.0000, final=0.9277
2025-10-02 23:59:58,643 - INFO - _models.training_function_executor - Model: 4,861 parameters, 20.9KB (PASS 256KB limit)
2025-10-02 23:59:58,643 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 39.058s
2025-10-02 23:59:58,768 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9277
2025-10-02 23:59:58,768 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.124s
2025-10-02 23:59:58,768 - INFO - bo.run_bo - Recorded observation #22: hparams={'lr': 0.00025932675354219136, 'batch_size': np.int64(256), 'epochs': np.int64(32), 'head_dim': np.int64(12), 'n_heads': np.int64(2), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(20), 'dropout': 0.07716883280650967, 'weight_decay': 5.010983171768417e-05, 'grad_clip_norm': 2.122203511487365, 'use_focal_loss': np.True_, 'focal_gamma': 4.756690581127159, 'label_smoothing': 0.14859531882420227, 'compute_class_weights': np.False_, 'aug_prob': 0.8776673284196131, 'aug_jitter_std': 0.030701136369334303, 'aug_scale_low': 0.9766585588736356, 'aug_scale_high': 1.1701866352111554, 'aug_drift_max_amp': 0.07703549425007233, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(16028)}, value=0.9277
2025-10-02 23:59:58,768 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 22: {'lr': 0.00025932675354219136, 'batch_size': np.int64(256), 'epochs': np.int64(32), 'head_dim': np.int64(12), 'n_heads': np.int64(2), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(20), 'dropout': 0.07716883280650967, 'weight_decay': 5.010983171768417e-05, 'grad_clip_norm': 2.122203511487365, 'use_focal_loss': np.True_, 'focal_gamma': 4.756690581127159, 'label_smoothing': 0.14859531882420227, 'compute_class_weights': np.False_, 'aug_prob': 0.8776673284196131, 'aug_jitter_std': 0.030701136369334303, 'aug_scale_low': 0.9766585588736356, 'aug_scale_high': 1.1701866352111554, 'aug_drift_max_amp': 0.07703549425007233, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(16028)} -> 0.9277
2025-10-02 23:59:58,768 - INFO - bo.run_bo - üîçBO Trial 23: Using RF surrogate + Expected Improvement
2025-10-02 23:59:58,768 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 23:59:58,768 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 23:59:58,768 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-02 23:59:58,768 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0020847889147527126, 'batch_size': 32, 'epochs': 49, 'head_dim': 12, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 20, 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': True, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': False, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 937702}
2025-10-02 23:59:58,769 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0020847889147527126, 'batch_size': 32, 'epochs': 49, 'head_dim': 12, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 20, 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': True, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': False, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 937702}
2025-10-03 00:01:25,661 - INFO - _models.training_function_executor - Model: 10,697 parameters, 46.0KB storage
2025-10-03 00:01:25,661 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.27210005898198636, 0.17797824141151927, 0.15786634036711059, 0.1502217680058996, 0.1432611514392254, 0.13938471926002505, 0.13678869803354443, 0.1342062975149258, 0.1331870065539253, 0.13052447291641714, 0.12922938477106868, 0.12707352152438672, 0.12623102026227623, 0.12439319195631762, 0.12374707663230639, 0.1237448569007585, 0.12249061408689087, 0.12139325952403315, 0.12079838976645363, 0.1200981813379985, 0.11949549868380341, 0.11852072278975963, 0.11887984527923505, 0.11767872385124035, 0.11746975734442004, 0.11596609958159655, 0.11610898988352523, 0.11707458405201475, 0.11624095007098977, 0.11578759426743569, 0.11445162272993284, 0.11473553079911142, 0.11421603463790629, 0.1139979190790455, 0.11298211642372628, 0.11354182046804759, 0.11348198518412336, 0.11250182405906307, 0.11300583899322691, 0.11208611519951997, 0.11194780964595843, 0.11204700815199842, 0.11229975775775315, 0.1113989452829084, 0.11150354069354476, 0.1108416691107383, 0.110366708379263, 0.11013243119918967, 0.1104629192153202], 'val_losses': [0.18195663868649056, 0.15804766970101403, 0.1502851009286745, 0.1405127224819416, 0.13359501257365158, 0.12767824298710231, 0.12806173958152237, 0.13081148596891948, 0.12463579785702836, 0.1262732637942756, 0.11798540138239372, 0.1195273156184598, 0.1190077453877839, 0.11885879053692817, 0.12558750349124143, 0.11748195619843625, 0.11386495925716672, 0.11259112736662294, 0.11409809258054064, 0.11462662958543172, 0.1144881752585576, 0.12394023063322937, 0.114934249567875, 0.11321993523739794, 0.11149575368695773, 0.11084514231613712, 0.11515038349495214, 0.11130375441132505, 0.11159210360195086, 0.11327961577335227, 0.10980480721145924, 0.10923100835233535, 0.10680286948311345, 0.10856703603391643, 0.1179476744596738, 0.10875296049392279, 0.1082256351470529, 0.10930698906146051, 0.10733267325287608, 0.10823516181919034, 0.10641289810730452, 0.10792083914283522, 0.1054712172820296, 0.10546676513396204, 0.10693670044326495, 0.10598513250242811, 0.11027481002611052, 0.1056812001998346, 0.10528682447319236], 'val_acc': [0.9476256108257111, 0.9572735246209748, 0.9604059641648917, 0.9615336424007017, 0.9635384037088084, 0.9646660819446184, 0.9660443553439418, 0.9657937601804285, 0.9655431650169152, 0.9715574489412354, 0.9674226287432652, 0.969427390051372, 0.9676732239067786, 0.9720586392682621, 0.9641648916175918, 0.9684250093973187, 0.9730610199223155, 0.9753163763939356, 0.9721839368500188, 0.9750657812304222, 0.9756922691392056, 0.9740634005763689, 0.9773211377020423, 0.9734369126675855, 0.9754416739756923, 0.9723092344317754, 0.9735622102493422, 0.9725598295952889, 0.9779476256108257, 0.9738128054128555, 0.976318757047989, 0.9740634005763689, 0.975566971557449, 0.9760681618844756, 0.9743139957398822, 0.9773211377020423, 0.9771958401202857, 0.977070542538529, 0.977446435283799, 0.978198220774339, 0.977822328029069, 0.977446435283799, 0.9771958401202857, 0.9769452449567724, 0.9798270893371758, 0.9785741135196091, 0.977070542538529, 0.9786994111013657, 0.9812053627364992], 'config': {'seq_len': 1000, 'patch_size': 20, 'head_dim': 12, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 7, 'dropout': 0.3830472741727743, 'num_classes': 5, 'epochs': 49, 'batch_size': 32, 'lr': 0.0020847889147527126, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': True, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 937702}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0020847889147527126, 'batch_size': 32, 'epochs': 49, 'head_dim': 12, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 20, 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': True, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': False, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 937702}, 'model_parameter_count': 10697, 'model_storage_size_kb': 45.963671875, 'model_size_validation': 'PASS'}
2025-10-03 00:01:25,661 - INFO - _models.training_function_executor - BO Objective: base=0.9812, size_penalty=0.0000, final=0.9812
2025-10-03 00:01:25,661 - INFO - _models.training_function_executor - Model: 10,697 parameters, 46.0KB (PASS 256KB limit)
2025-10-03 00:01:25,661 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 86.893s
2025-10-03 00:01:25,786 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9812
2025-10-03 00:01:25,786 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.124s
2025-10-03 00:01:25,786 - INFO - bo.run_bo - Recorded observation #23: hparams={'lr': 0.0020847889147527126, 'batch_size': np.int64(32), 'epochs': np.int64(49), 'head_dim': np.int64(12), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(20), 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': np.True_, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': np.False_, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(937702)}, value=0.9812
2025-10-03 00:01:25,786 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 23: {'lr': 0.0020847889147527126, 'batch_size': np.int64(32), 'epochs': np.int64(49), 'head_dim': np.int64(12), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(20), 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': np.True_, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': np.False_, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(937702)} -> 0.9812
2025-10-03 00:01:25,786 - INFO - bo.run_bo - üîçBO Trial 24: Using RF surrogate + Expected Improvement
2025-10-03 00:01:25,786 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 00:01:25,786 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:01:25,786 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:01:25,786 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0036659088884624736, 'batch_size': 32, 'epochs': 47, 'head_dim': 11, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 100, 'dropout': 0.027930136509064292, 'weight_decay': 0.00011191377768535846, 'grad_clip_norm': 3.908794684368598, 'use_focal_loss': True, 'focal_gamma': 1.3902777260216557, 'label_smoothing': 0.07824119689336445, 'compute_class_weights': False, 'aug_prob': 0.5725214244063545, 'aug_jitter_std': 0.042333490410273004, 'aug_scale_low': 0.9141315023763079, 'aug_scale_high': 1.170197444854336, 'aug_drift_max_amp': 0.1397013915909125, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 705264}
2025-10-03 00:01:25,787 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0036659088884624736, 'batch_size': 32, 'epochs': 47, 'head_dim': 11, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 100, 'dropout': 0.027930136509064292, 'weight_decay': 0.00011191377768535846, 'grad_clip_norm': 3.908794684368598, 'use_focal_loss': True, 'focal_gamma': 1.3902777260216557, 'label_smoothing': 0.07824119689336445, 'compute_class_weights': False, 'aug_prob': 0.5725214244063545, 'aug_jitter_std': 0.042333490410273004, 'aug_scale_low': 0.9141315023763079, 'aug_scale_high': 1.170197444854336, 'aug_drift_max_amp': 0.1397013915909125, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 705264}
2025-10-03 00:03:11,384 - INFO - _models.training_function_executor - Model: 7,913 parameters, 34.0KB storage
2025-10-03 00:03:11,384 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.26364688582642987, 0.16071701492456417, 0.14367970456228749, 0.13712461186830283, 0.13347195568111006, 0.12938540061820894, 0.12619490209044182, 0.12347896506481405, 0.12189079159676153, 0.11943873179160357, 0.11793662772164684, 0.11616688150555084, 0.11559430436847264, 0.11421384133242861, 0.11315757788478986, 0.11185326826065303, 0.11205417058653122, 0.10970488416878042, 0.10918501865641424, 0.1086928289704214, 0.10796513005564118, 0.10687455144609809, 0.10486011109172344, 0.10514108538282203, 0.10402808270789969, 0.10405016416770206, 0.10355675331724246, 0.10199301208958486, 0.10174193224370996, 0.10143947614995145, 0.10107707862652651, 0.10151123001630513, 0.1002273562595076, 0.10071829201579673, 0.09951514071781034, 0.09953001410719431, 0.09944989240982104, 0.09901260811800168, 0.09741230059326529, 0.09830079493799765, 0.0984506761306932, 0.09709272424474275, 0.09678643621245007, 0.0968582497298936, 0.0961582241777982, 0.09528801287237768, 0.09625693356954779], 'val_losses': [0.1885609493647254, 0.15167426015556704, 0.13521742699192874, 0.13340270572655064, 0.1267037866834321, 0.12903111559184716, 0.13377481197679034, 0.11745431426622682, 0.1274054803158642, 0.12904979408631242, 0.13166178784010749, 0.1134260455672715, 0.11716063593790475, 0.11066662647598385, 0.11111719051767839, 0.11446629849431389, 0.10638545857771135, 0.11140491873261146, 0.12031825201401748, 0.11075879998440702, 0.11564875318955962, 0.10482499340978306, 0.10626457790476444, 0.10721532220216701, 0.10410526522738998, 0.11187432411170785, 0.1126524970848419, 0.10639112305034747, 0.10778029316231326, 0.10396115150820297, 0.10420079389814595, 0.10365794639945285, 0.10313854384371518, 0.10491189211252235, 0.10270916533983974, 0.10750395701114075, 0.1002240063268549, 0.09710413157051181, 0.12573432466795892, 0.09881260080708551, 0.10101060585903354, 0.09840057765820648, 0.09894757284778742, 0.09992248608928293, 0.09851963894810238, 0.10361844214964684, 0.10131742970385717], 'val_acc': [0.9485026938980078, 0.9540157874953014, 0.9645407843628618, 0.9651672722716451, 0.9650419746898885, 0.9705550682871821, 0.9609071544919183, 0.970179175541912, 0.9657937601804285, 0.9677985214885353, 0.9670467359979953, 0.9738128054128555, 0.9738128054128555, 0.9731863175040721, 0.977070542538529, 0.9703044731236687, 0.9756922691392056, 0.9720586392682621, 0.9672973311615086, 0.9723092344317754, 0.9750657812304222, 0.9773211377020423, 0.9764440546297456, 0.9750657812304222, 0.9776970304473124, 0.9710562586142087, 0.9719333416865055, 0.9758175667209623, 0.9711815561959655, 0.9751910788121789, 0.9768199473750157, 0.978198220774339, 0.9786994111013657, 0.9812053627364992, 0.978198220774339, 0.975566971557449, 0.9768199473750157, 0.9786994111013657, 0.9649166771081318, 0.9798270893371758, 0.9784488159378524, 0.9776970304473124, 0.9786994111013657, 0.9786994111013657, 0.9783235183560958, 0.9784488159378524, 0.9803282796642024], 'config': {'seq_len': 1000, 'patch_size': 100, 'head_dim': 11, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'dropout': 0.027930136509064292, 'num_classes': 5, 'epochs': 47, 'batch_size': 32, 'lr': 0.0036659088884624736, 'weight_decay': 0.00011191377768535846, 'grad_clip_norm': 3.908794684368598, 'use_focal_loss': True, 'focal_gamma': 1.3902777260216557, 'label_smoothing': 0.07824119689336445, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.5725214244063545, 'aug_jitter_std': 0.042333490410273004, 'aug_scale_low': 0.9141315023763079, 'aug_scale_high': 1.170197444854336, 'aug_drift_max_amp': 0.1397013915909125, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 705264}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0036659088884624736, 'batch_size': 32, 'epochs': 47, 'head_dim': 11, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 100, 'dropout': 0.027930136509064292, 'weight_decay': 0.00011191377768535846, 'grad_clip_norm': 3.908794684368598, 'use_focal_loss': True, 'focal_gamma': 1.3902777260216557, 'label_smoothing': 0.07824119689336445, 'compute_class_weights': False, 'aug_prob': 0.5725214244063545, 'aug_jitter_std': 0.042333490410273004, 'aug_scale_low': 0.9141315023763079, 'aug_scale_high': 1.170197444854336, 'aug_drift_max_amp': 0.1397013915909125, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 705264}, 'model_parameter_count': 7913, 'model_storage_size_kb': 34.001171875000004, 'model_size_validation': 'PASS'}
2025-10-03 00:03:11,385 - INFO - _models.training_function_executor - BO Objective: base=0.9803, size_penalty=0.0000, final=0.9803
2025-10-03 00:03:11,385 - INFO - _models.training_function_executor - Model: 7,913 parameters, 34.0KB (PASS 256KB limit)
2025-10-03 00:03:11,385 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 105.598s
2025-10-03 00:03:11,510 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9803
2025-10-03 00:03:11,511 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.126s
2025-10-03 00:03:11,511 - INFO - bo.run_bo - Recorded observation #24: hparams={'lr': 0.0036659088884624736, 'batch_size': np.int64(32), 'epochs': np.int64(47), 'head_dim': np.int64(11), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(100), 'dropout': 0.027930136509064292, 'weight_decay': 0.00011191377768535846, 'grad_clip_norm': 3.908794684368598, 'use_focal_loss': np.True_, 'focal_gamma': 1.3902777260216557, 'label_smoothing': 0.07824119689336445, 'compute_class_weights': np.False_, 'aug_prob': 0.5725214244063545, 'aug_jitter_std': 0.042333490410273004, 'aug_scale_low': 0.9141315023763079, 'aug_scale_high': 1.170197444854336, 'aug_drift_max_amp': 0.1397013915909125, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(705264)}, value=0.9803
2025-10-03 00:03:11,511 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 24: {'lr': 0.0036659088884624736, 'batch_size': np.int64(32), 'epochs': np.int64(47), 'head_dim': np.int64(11), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(100), 'dropout': 0.027930136509064292, 'weight_decay': 0.00011191377768535846, 'grad_clip_norm': 3.908794684368598, 'use_focal_loss': np.True_, 'focal_gamma': 1.3902777260216557, 'label_smoothing': 0.07824119689336445, 'compute_class_weights': np.False_, 'aug_prob': 0.5725214244063545, 'aug_jitter_std': 0.042333490410273004, 'aug_scale_low': 0.9141315023763079, 'aug_scale_high': 1.170197444854336, 'aug_drift_max_amp': 0.1397013915909125, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(705264)} -> 0.9803
2025-10-03 00:03:11,511 - INFO - bo.run_bo - üîçBO Trial 25: Using RF surrogate + Expected Improvement
2025-10-03 00:03:11,511 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 00:03:11,511 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:03:11,511 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:03:11,511 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004254515624305729, 'batch_size': 256, 'epochs': 8, 'head_dim': 10, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.4129738127741927, 'weight_decay': 7.036506015382984e-05, 'grad_clip_norm': 0.3804946643074753, 'use_focal_loss': False, 'focal_gamma': 1.043098646288619, 'label_smoothing': 0.021239834187238876, 'compute_class_weights': False, 'aug_prob': 0.31595517641007836, 'aug_jitter_std': 0.0154912753772568, 'aug_scale_low': 0.9630156474019712, 'aug_scale_high': 1.0326919845216394, 'aug_drift_max_amp': 0.03350237216979362, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 53868}
2025-10-03 00:03:11,513 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004254515624305729, 'batch_size': 256, 'epochs': 8, 'head_dim': 10, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.4129738127741927, 'weight_decay': 7.036506015382984e-05, 'grad_clip_norm': 0.3804946643074753, 'use_focal_loss': False, 'focal_gamma': 1.043098646288619, 'label_smoothing': 0.021239834187238876, 'compute_class_weights': False, 'aug_prob': 0.31595517641007836, 'aug_jitter_std': 0.0154912753772568, 'aug_scale_low': 0.9630156474019712, 'aug_scale_high': 1.0326919845216394, 'aug_drift_max_amp': 0.03350237216979362, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 53868}
2025-10-03 00:03:18,144 - INFO - _models.training_function_executor - Model: 4,889 parameters, 5.3KB storage
2025-10-03 00:03:18,144 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8200556397815211, 0.654361408900166, 0.5862860179961678, 0.5420828172520722, 0.5015111153167139, 0.4728357427827406, 0.44157911763675783, 0.4234258968416799], 'val_losses': [0.7197988460183726, 0.7526298815587785, 0.5724722833699741, 0.5095784335660092, 0.5993100336068197, 0.5232968112532469, 0.39746666260044045, 0.4152371448176111], 'val_acc': [0.7719584012028567, 0.7976444054629745, 0.8285929081568726, 0.8589149229419872, 0.8465104623480767, 0.8814684876581882, 0.914045858914923, 0.9116652048615461], 'config': {'seq_len': 1000, 'patch_size': 40, 'head_dim': 10, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 3, 'dropout': 0.4129738127741927, 'num_classes': 5, 'epochs': 8, 'batch_size': 256, 'lr': 0.004254515624305729, 'weight_decay': 7.036506015382984e-05, 'grad_clip_norm': 0.3804946643074753, 'use_focal_loss': False, 'focal_gamma': 1.043098646288619, 'label_smoothing': 0.021239834187238876, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.31595517641007836, 'aug_jitter_std': 0.0154912753772568, 'aug_scale_low': 0.9630156474019712, 'aug_scale_high': 1.0326919845216394, 'aug_drift_max_amp': 0.03350237216979362, 'normalize': False}, 'quantization': {'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 53868}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004254515624305729, 'batch_size': 256, 'epochs': 8, 'head_dim': 10, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 3, 'patch_size': 40, 'dropout': 0.4129738127741927, 'weight_decay': 7.036506015382984e-05, 'grad_clip_norm': 0.3804946643074753, 'use_focal_loss': False, 'focal_gamma': 1.043098646288619, 'label_smoothing': 0.021239834187238876, 'compute_class_weights': False, 'aug_prob': 0.31595517641007836, 'aug_jitter_std': 0.0154912753772568, 'aug_scale_low': 0.9630156474019712, 'aug_scale_high': 1.0326919845216394, 'aug_drift_max_amp': 0.03350237216979362, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 53868}, 'model_parameter_count': 4889, 'model_storage_size_kb': 5.2518554687500005, 'model_size_validation': 'PASS'}
2025-10-03 00:03:18,144 - INFO - _models.training_function_executor - BO Objective: base=0.9117, size_penalty=0.0000, final=0.9117
2025-10-03 00:03:18,144 - INFO - _models.training_function_executor - Model: 4,889 parameters, 5.3KB (PASS 256KB limit)
2025-10-03 00:03:18,144 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 6.633s
2025-10-03 00:03:18,273 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9117
2025-10-03 00:03:18,273 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.129s
2025-10-03 00:03:18,273 - INFO - bo.run_bo - Recorded observation #25: hparams={'lr': 0.004254515624305729, 'batch_size': np.int64(256), 'epochs': np.int64(8), 'head_dim': np.int64(10), 'n_heads': np.int64(6), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(40), 'dropout': 0.4129738127741927, 'weight_decay': 7.036506015382984e-05, 'grad_clip_norm': 0.3804946643074753, 'use_focal_loss': np.False_, 'focal_gamma': 1.043098646288619, 'label_smoothing': 0.021239834187238876, 'compute_class_weights': np.False_, 'aug_prob': 0.31595517641007836, 'aug_jitter_std': 0.0154912753772568, 'aug_scale_low': 0.9630156474019712, 'aug_scale_high': 1.0326919845216394, 'aug_drift_max_amp': 0.03350237216979362, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(53868)}, value=0.9117
2025-10-03 00:03:18,273 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 25: {'lr': 0.004254515624305729, 'batch_size': np.int64(256), 'epochs': np.int64(8), 'head_dim': np.int64(10), 'n_heads': np.int64(6), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(40), 'dropout': 0.4129738127741927, 'weight_decay': 7.036506015382984e-05, 'grad_clip_norm': 0.3804946643074753, 'use_focal_loss': np.False_, 'focal_gamma': 1.043098646288619, 'label_smoothing': 0.021239834187238876, 'compute_class_weights': np.False_, 'aug_prob': 0.31595517641007836, 'aug_jitter_std': 0.0154912753772568, 'aug_scale_low': 0.9630156474019712, 'aug_scale_high': 1.0326919845216394, 'aug_drift_max_amp': 0.03350237216979362, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(53868)} -> 0.9117
2025-10-03 00:03:18,273 - INFO - bo.run_bo - üîçBO Trial 26: Using RF surrogate + Expected Improvement
2025-10-03 00:03:18,273 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 00:03:18,273 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:03:18,274 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:03:18,274 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0001988015263673152, 'batch_size': 64, 'epochs': 7, 'head_dim': 12, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 50, 'dropout': 0.21770913615881476, 'weight_decay': 0.0014186393071905932, 'grad_clip_norm': 1.62071037954698, 'use_focal_loss': False, 'focal_gamma': 3.390391446874432, 'label_smoothing': 0.020124744850412074, 'compute_class_weights': False, 'aug_prob': 0.6108967038642815, 'aug_jitter_std': 0.016315785807219138, 'aug_scale_low': 0.8222040908229624, 'aug_scale_high': 1.1010409423723209, 'aug_drift_max_amp': 0.03359313345500317, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'seed': 704739}
2025-10-03 00:03:18,275 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0001988015263673152, 'batch_size': 64, 'epochs': 7, 'head_dim': 12, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 50, 'dropout': 0.21770913615881476, 'weight_decay': 0.0014186393071905932, 'grad_clip_norm': 1.62071037954698, 'use_focal_loss': False, 'focal_gamma': 3.390391446874432, 'label_smoothing': 0.020124744850412074, 'compute_class_weights': False, 'aug_prob': 0.6108967038642815, 'aug_jitter_std': 0.016315785807219138, 'aug_scale_low': 0.8222040908229624, 'aug_scale_high': 1.1010409423723209, 'aug_drift_max_amp': 0.03359313345500317, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'seed': 704739}
2025-10-03 00:03:26,475 - INFO - _models.training_function_executor - Model: 3,677 parameters, 15.8KB storage
2025-10-03 00:03:26,475 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.150887488310593, 0.8163059653858827, 0.7581875174896903, 0.7100398676947267, 0.6612975721098152, 0.6135892036194692, 0.5731623491480458], 'val_losses': [0.8700028838347768, 0.7863029197797547, 0.7371631435711063, 0.6856459384125256, 0.6452301634297516, 0.5847591996573817, 0.541100063877825], 'val_acc': [0.714196216013031, 0.7244706177170781, 0.7416363864177421, 0.7728354842751535, 0.8184438040345822, 0.821952136323769, 0.8577872447061772], 'config': {'seq_len': 1000, 'patch_size': 50, 'head_dim': 12, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'dropout': 0.21770913615881476, 'num_classes': 5, 'epochs': 7, 'batch_size': 64, 'lr': 0.0001988015263673152, 'weight_decay': 0.0014186393071905932, 'grad_clip_norm': 1.62071037954698, 'use_focal_loss': False, 'focal_gamma': 3.390391446874432, 'label_smoothing': 0.020124744850412074, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.6108967038642815, 'aug_jitter_std': 0.016315785807219138, 'aug_scale_low': 0.8222040908229624, 'aug_scale_high': 1.1010409423723209, 'aug_drift_max_amp': 0.03359313345500317, 'normalize': False}, 'quantization': {'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 704739}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0001988015263673152, 'batch_size': 64, 'epochs': 7, 'head_dim': 12, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 50, 'dropout': 0.21770913615881476, 'weight_decay': 0.0014186393071905932, 'grad_clip_norm': 1.62071037954698, 'use_focal_loss': False, 'focal_gamma': 3.390391446874432, 'label_smoothing': 0.020124744850412074, 'compute_class_weights': False, 'aug_prob': 0.6108967038642815, 'aug_jitter_std': 0.016315785807219138, 'aug_scale_low': 0.8222040908229624, 'aug_scale_high': 1.1010409423723209, 'aug_drift_max_amp': 0.03359313345500317, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'seed': 704739}, 'model_parameter_count': 3677, 'model_storage_size_kb': 15.799609375000001, 'model_size_validation': 'PASS'}
2025-10-03 00:03:26,475 - INFO - _models.training_function_executor - BO Objective: base=0.8578, size_penalty=0.0000, final=0.8578
2025-10-03 00:03:26,475 - INFO - _models.training_function_executor - Model: 3,677 parameters, 15.8KB (PASS 256KB limit)
2025-10-03 00:03:26,475 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 8.202s
2025-10-03 00:03:26,603 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8578
2025-10-03 00:03:26,603 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.128s
2025-10-03 00:03:26,603 - INFO - bo.run_bo - Recorded observation #26: hparams={'lr': 0.0001988015263673152, 'batch_size': np.int64(64), 'epochs': np.int64(7), 'head_dim': np.int64(12), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(50), 'dropout': 0.21770913615881476, 'weight_decay': 0.0014186393071905932, 'grad_clip_norm': 1.62071037954698, 'use_focal_loss': np.False_, 'focal_gamma': 3.390391446874432, 'label_smoothing': 0.020124744850412074, 'compute_class_weights': np.False_, 'aug_prob': 0.6108967038642815, 'aug_jitter_std': 0.016315785807219138, 'aug_scale_low': 0.8222040908229624, 'aug_scale_high': 1.1010409423723209, 'aug_drift_max_amp': 0.03359313345500317, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(704739)}, value=0.8578
2025-10-03 00:03:26,603 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 26: {'lr': 0.0001988015263673152, 'batch_size': np.int64(64), 'epochs': np.int64(7), 'head_dim': np.int64(12), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(50), 'dropout': 0.21770913615881476, 'weight_decay': 0.0014186393071905932, 'grad_clip_norm': 1.62071037954698, 'use_focal_loss': np.False_, 'focal_gamma': 3.390391446874432, 'label_smoothing': 0.020124744850412074, 'compute_class_weights': np.False_, 'aug_prob': 0.6108967038642815, 'aug_jitter_std': 0.016315785807219138, 'aug_scale_low': 0.8222040908229624, 'aug_scale_high': 1.1010409423723209, 'aug_drift_max_amp': 0.03359313345500317, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(704739)} -> 0.8578
2025-10-03 00:03:26,603 - INFO - bo.run_bo - üîçBO Trial 27: Using RF surrogate + Expected Improvement
2025-10-03 00:03:26,604 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 00:03:26,604 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:03:26,604 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:03:26,604 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0011511863494203117, 'batch_size': 128, 'epochs': 49, 'head_dim': 24, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.17396578796944964, 'weight_decay': 5.370242919423116e-05, 'grad_clip_norm': 3.7194331887440506, 'use_focal_loss': False, 'focal_gamma': 1.0542714549198147, 'label_smoothing': 0.13873822020139126, 'compute_class_weights': False, 'aug_prob': 0.10762618321440579, 'aug_jitter_std': 0.030860966367053867, 'aug_scale_low': 0.9116886243451519, 'aug_scale_high': 1.039265368817714, 'aug_drift_max_amp': 0.17493849130571967, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 263257}
2025-10-03 00:03:26,605 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0011511863494203117, 'batch_size': 128, 'epochs': 49, 'head_dim': 24, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.17396578796944964, 'weight_decay': 5.370242919423116e-05, 'grad_clip_norm': 3.7194331887440506, 'use_focal_loss': False, 'focal_gamma': 1.0542714549198147, 'label_smoothing': 0.13873822020139126, 'compute_class_weights': False, 'aug_prob': 0.10762618321440579, 'aug_jitter_std': 0.030860966367053867, 'aug_scale_low': 0.9116886243451519, 'aug_scale_high': 1.039265368817714, 'aug_drift_max_amp': 0.17493849130571967, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 263257}
2025-10-03 00:04:28,649 - INFO - _models.training_function_executor - Model: 4,957 parameters, 21.3KB storage
2025-10-03 00:04:28,649 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9818029199571002, 0.7278040061897567, 0.673557938103075, 0.6494031323416581, 0.6368669718405927, 0.6279198596849667, 0.6218630707985036, 0.617216687421581, 0.6145843279427822, 0.6109383279576777, 0.6075344309731242, 0.6051260469006338, 0.6034569959590762, 0.6013930128342383, 0.5998238725694794, 0.5977807963913524, 0.5967251434663153, 0.5950576365507512, 0.5943155544830273, 0.592704754410406, 0.5920552495121676, 0.5902161115046279, 0.5898462406051856, 0.5896907605608204, 0.587899309273326, 0.5884681733984048, 0.5870929727226303, 0.5867634892885657, 0.5864938105407023, 0.5855787947610092, 0.5849041593547931, 0.5845043512546519, 0.5841007283545525, 0.5833409435187493, 0.5823355632904825, 0.5812125690030494, 0.5819847218819968, 0.5820136839997756, 0.5809844102909686, 0.5812590623437411, 0.5792333510228014, 0.5795619231460425, 0.5793249081038736, 0.5796183857449358, 0.5776744495786843, 0.5783336643299146, 0.5780894996673807, 0.577866574563512, 0.5771279022799165], 'val_losses': [0.7967284438216048, 0.6854114232991158, 0.6875190262641544, 0.6335559766433873, 0.6620999751660388, 0.6205096159822554, 0.6128194473348694, 0.6170790267758405, 0.6076947415536723, 0.6056601266681125, 0.6043921696887311, 0.6054657073624973, 0.5957161713596454, 0.5951430519755481, 0.5925838655733013, 0.5940447062899962, 0.5906505942449222, 0.5915653755634653, 0.5893121479507856, 0.5848942015601525, 0.5865938591039983, 0.5852015173953512, 0.5981216561508991, 0.5910519992867563, 0.5937244038640997, 0.5820681877549079, 0.5812795512166302, 0.5839467291888132, 0.5858036539127707, 0.5793626379166132, 0.5799234725526514, 0.5756198326726707, 0.5981841707585106, 0.5912470080517629, 0.5780463864698459, 0.5952484487755348, 0.5895122158588615, 0.5780424117115921, 0.5754179965880651, 0.578016688793528, 0.5842703852852459, 0.5756183429598405, 0.5830276138560543, 0.5813154551995128, 0.5808961114225016, 0.6048096576036928, 0.5714317755641801, 0.5767609002804969, 0.5721421249527304], 'val_acc': [0.8976318757047989, 0.9307104372885603, 0.9295827590527502, 0.9525122165142212, 0.9394812680115274, 0.9548928705675981, 0.9556446560581381, 0.9577747149480015, 0.9571482270392181, 0.9557699536398947, 0.9589023931838115, 0.9570229294574615, 0.9597794762561083, 0.9612830472371883, 0.9626613206365117, 0.9600300714196216, 0.9642901891993485, 0.9650419746898885, 0.9662949505074552, 0.963287808545295, 0.9659190577621852, 0.9639142964540784, 0.9647913795263752, 0.967547926325022, 0.9674226287432652, 0.9695526876331286, 0.9656684625986719, 0.9672973311615086, 0.9698032827966421, 0.967547926325022, 0.9650419746898885, 0.9679238190702919, 0.9689261997243453, 0.9715574489412354, 0.9699285803783987, 0.9670467359979953, 0.9724345320135321, 0.9716827465229921, 0.9715574489412354, 0.9726851271770455, 0.9718080441047487, 0.968675604560832, 0.9718080441047487, 0.9699285803783987, 0.9713068537777221, 0.9710562586142087, 0.9704297707054254, 0.9695526876331286, 0.9693020924696153], 'config': {'seq_len': 1000, 'patch_size': 50, 'head_dim': 24, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.17396578796944964, 'num_classes': 5, 'epochs': 49, 'batch_size': 128, 'lr': 0.0011511863494203117, 'weight_decay': 5.370242919423116e-05, 'grad_clip_norm': 3.7194331887440506, 'use_focal_loss': False, 'focal_gamma': 1.0542714549198147, 'label_smoothing': 0.13873822020139126, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.10762618321440579, 'aug_jitter_std': 0.030860966367053867, 'aug_scale_low': 0.9116886243451519, 'aug_scale_high': 1.039265368817714, 'aug_drift_max_amp': 0.17493849130571967, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 263257}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0011511863494203117, 'batch_size': 128, 'epochs': 49, 'head_dim': 24, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.17396578796944964, 'weight_decay': 5.370242919423116e-05, 'grad_clip_norm': 3.7194331887440506, 'use_focal_loss': False, 'focal_gamma': 1.0542714549198147, 'label_smoothing': 0.13873822020139126, 'compute_class_weights': False, 'aug_prob': 0.10762618321440579, 'aug_jitter_std': 0.030860966367053867, 'aug_scale_low': 0.9116886243451519, 'aug_scale_high': 1.039265368817714, 'aug_drift_max_amp': 0.17493849130571967, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'seed': 263257}, 'model_parameter_count': 4957, 'model_storage_size_kb': 21.299609375000003, 'model_size_validation': 'PASS'}
2025-10-03 00:04:28,649 - INFO - _models.training_function_executor - BO Objective: base=0.9693, size_penalty=0.0000, final=0.9693
2025-10-03 00:04:28,649 - INFO - _models.training_function_executor - Model: 4,957 parameters, 21.3KB (PASS 256KB limit)
2025-10-03 00:04:28,649 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 62.045s
2025-10-03 00:04:28,778 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9693
2025-10-03 00:04:28,778 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.129s
2025-10-03 00:04:28,778 - INFO - bo.run_bo - Recorded observation #27: hparams={'lr': 0.0011511863494203117, 'batch_size': np.int64(128), 'epochs': np.int64(49), 'head_dim': np.int64(24), 'n_heads': np.int64(4), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(50), 'dropout': 0.17396578796944964, 'weight_decay': 5.370242919423116e-05, 'grad_clip_norm': 3.7194331887440506, 'use_focal_loss': np.False_, 'focal_gamma': 1.0542714549198147, 'label_smoothing': 0.13873822020139126, 'compute_class_weights': np.False_, 'aug_prob': 0.10762618321440579, 'aug_jitter_std': 0.030860966367053867, 'aug_scale_low': 0.9116886243451519, 'aug_scale_high': 1.039265368817714, 'aug_drift_max_amp': 0.17493849130571967, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(263257)}, value=0.9693
2025-10-03 00:04:28,778 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 27: {'lr': 0.0011511863494203117, 'batch_size': np.int64(128), 'epochs': np.int64(49), 'head_dim': np.int64(24), 'n_heads': np.int64(4), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(50), 'dropout': 0.17396578796944964, 'weight_decay': 5.370242919423116e-05, 'grad_clip_norm': 3.7194331887440506, 'use_focal_loss': np.False_, 'focal_gamma': 1.0542714549198147, 'label_smoothing': 0.13873822020139126, 'compute_class_weights': np.False_, 'aug_prob': 0.10762618321440579, 'aug_jitter_std': 0.030860966367053867, 'aug_scale_low': 0.9116886243451519, 'aug_scale_high': 1.039265368817714, 'aug_drift_max_amp': 0.17493849130571967, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(263257)} -> 0.9693
2025-10-03 00:04:28,779 - INFO - bo.run_bo - üîçBO Trial 28: Using RF surrogate + Expected Improvement
2025-10-03 00:04:28,779 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 00:04:28,779 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:04:28,779 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:04:28,779 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0021972399911256903, 'batch_size': 64, 'epochs': 7, 'head_dim': 19, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 5, 'dropout': 0.11133369944897739, 'weight_decay': 0.00044224864608341296, 'grad_clip_norm': 4.388008488769935, 'use_focal_loss': True, 'focal_gamma': 1.031504920863039, 'label_smoothing': 0.09667318238882174, 'compute_class_weights': False, 'aug_prob': 0.8504912634575, 'aug_jitter_std': 0.027479032255379086, 'aug_scale_low': 0.9143063646807144, 'aug_scale_high': 1.1585414569494485, 'aug_drift_max_amp': 0.08034384675993983, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 795006}
2025-10-03 00:04:28,780 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0021972399911256903, 'batch_size': 64, 'epochs': 7, 'head_dim': 19, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 5, 'dropout': 0.11133369944897739, 'weight_decay': 0.00044224864608341296, 'grad_clip_norm': 4.388008488769935, 'use_focal_loss': True, 'focal_gamma': 1.031504920863039, 'label_smoothing': 0.09667318238882174, 'compute_class_weights': False, 'aug_prob': 0.8504912634575, 'aug_jitter_std': 0.027479032255379086, 'aug_scale_low': 0.9143063646807144, 'aug_scale_high': 1.1585414569494485, 'aug_drift_max_amp': 0.08034384675993983, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 795006}
2025-10-03 00:04:43,447 - INFO - _models.training_function_executor - Model: 10,793 parameters, 11.6KB storage
2025-10-03 00:04:43,448 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.36249777805841427, 0.24052516212981845, 0.21890279759168887, 0.20941987316758473, 0.20293080462805368, 0.1987511150904647, 0.1958973235721274], 'val_losses': [0.30152865957345987, 0.223656357832084, 0.20596408641976144, 0.20082067037200796, 0.1935080807059051, 0.19985414672325075, 0.1893452122693611], 'val_acc': [0.9378523994486906, 0.9567723342939481, 0.9654178674351584, 0.9657937601804285, 0.9669214384162386, 0.9688009021425886, 0.9695526876331286], 'config': {'seq_len': 1000, 'patch_size': 5, 'head_dim': 19, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.11133369944897739, 'num_classes': 5, 'epochs': 7, 'batch_size': 64, 'lr': 0.0021972399911256903, 'weight_decay': 0.00044224864608341296, 'grad_clip_norm': 4.388008488769935, 'use_focal_loss': True, 'focal_gamma': 1.031504920863039, 'label_smoothing': 0.09667318238882174, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.8504912634575, 'aug_jitter_std': 0.027479032255379086, 'aug_scale_low': 0.9143063646807144, 'aug_scale_high': 1.1585414569494485, 'aug_drift_max_amp': 0.08034384675993983, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 795006}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0021972399911256903, 'batch_size': 64, 'epochs': 7, 'head_dim': 19, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 5, 'dropout': 0.11133369944897739, 'weight_decay': 0.00044224864608341296, 'grad_clip_norm': 4.388008488769935, 'use_focal_loss': True, 'focal_gamma': 1.031504920863039, 'label_smoothing': 0.09667318238882174, 'compute_class_weights': False, 'aug_prob': 0.8504912634575, 'aug_jitter_std': 0.027479032255379086, 'aug_scale_low': 0.9143063646807144, 'aug_scale_high': 1.1585414569494485, 'aug_drift_max_amp': 0.08034384675993983, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 795006}, 'model_parameter_count': 10793, 'model_storage_size_kb': 11.594042968750001, 'model_size_validation': 'PASS'}
2025-10-03 00:04:43,448 - INFO - _models.training_function_executor - BO Objective: base=0.9696, size_penalty=0.0000, final=0.9696
2025-10-03 00:04:43,448 - INFO - _models.training_function_executor - Model: 10,793 parameters, 11.6KB (PASS 256KB limit)
2025-10-03 00:04:43,448 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 14.669s
2025-10-03 00:04:43,577 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9696
2025-10-03 00:04:43,577 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.129s
2025-10-03 00:04:43,577 - INFO - bo.run_bo - Recorded observation #28: hparams={'lr': 0.0021972399911256903, 'batch_size': np.int64(64), 'epochs': np.int64(7), 'head_dim': np.int64(19), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(5), 'dropout': 0.11133369944897739, 'weight_decay': 0.00044224864608341296, 'grad_clip_norm': 4.388008488769935, 'use_focal_loss': np.True_, 'focal_gamma': 1.031504920863039, 'label_smoothing': 0.09667318238882174, 'compute_class_weights': np.False_, 'aug_prob': 0.8504912634575, 'aug_jitter_std': 0.027479032255379086, 'aug_scale_low': 0.9143063646807144, 'aug_scale_high': 1.1585414569494485, 'aug_drift_max_amp': 0.08034384675993983, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(795006)}, value=0.9696
2025-10-03 00:04:43,577 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 28: {'lr': 0.0021972399911256903, 'batch_size': np.int64(64), 'epochs': np.int64(7), 'head_dim': np.int64(19), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(5), 'dropout': 0.11133369944897739, 'weight_decay': 0.00044224864608341296, 'grad_clip_norm': 4.388008488769935, 'use_focal_loss': np.True_, 'focal_gamma': 1.031504920863039, 'label_smoothing': 0.09667318238882174, 'compute_class_weights': np.False_, 'aug_prob': 0.8504912634575, 'aug_jitter_std': 0.027479032255379086, 'aug_scale_low': 0.9143063646807144, 'aug_scale_high': 1.1585414569494485, 'aug_drift_max_amp': 0.08034384675993983, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(795006)} -> 0.9696
2025-10-03 00:04:43,578 - INFO - bo.run_bo - üîçBO Trial 29: Using RF surrogate + Expected Improvement
2025-10-03 00:04:43,578 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 00:04:43,578 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:04:43,578 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:04:43,578 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0016658274496447242, 'batch_size': 128, 'epochs': 34, 'head_dim': 10, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.3848866607601345, 'weight_decay': 0.005298118994438062, 'grad_clip_norm': 4.361682368126166, 'use_focal_loss': False, 'focal_gamma': 2.035021028094691, 'label_smoothing': 0.15312498010270212, 'compute_class_weights': True, 'aug_prob': 0.6427709210238178, 'aug_jitter_std': 0.04023754380125276, 'aug_scale_low': 0.9206354322485091, 'aug_scale_high': 1.1604867929371985, 'aug_drift_max_amp': 0.09332313378193598, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 229390}
2025-10-03 00:04:43,579 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0016658274496447242, 'batch_size': 128, 'epochs': 34, 'head_dim': 10, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.3848866607601345, 'weight_decay': 0.005298118994438062, 'grad_clip_norm': 4.361682368126166, 'use_focal_loss': False, 'focal_gamma': 2.035021028094691, 'label_smoothing': 0.15312498010270212, 'compute_class_weights': True, 'aug_prob': 0.6427709210238178, 'aug_jitter_std': 0.04023754380125276, 'aug_scale_low': 0.9206354322485091, 'aug_scale_high': 1.1604867929371985, 'aug_drift_max_amp': 0.09332313378193598, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 229390}
2025-10-03 00:05:51,890 - INFO - _models.training_function_executor - Model: 18,869 parameters, 81.1KB storage
2025-10-03 00:05:51,890 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.064942556242669, 1.8615462092481243, 1.8262518028364076, 1.8086632192685663, 1.799991748989687, 1.7865411546062049, 1.7775766994333042, 1.7778100743500043, 1.7722398383730251, 1.769139372756882, 1.7659447444425178, 1.758215313047685, 1.76218556739392, 1.75932678434554, 1.7573821471679465, 1.758222733673788, 1.7516852171876707, 1.7512302727671347, 1.747197747409775, 1.7445362443647314, 1.7452604817264163, 1.7497164573122137, 1.7465783825412982, 1.7365888492380606, 1.7416124015554533, 1.7367766611715667, 1.7434100852784733, 1.7393922841112384, 1.7366119385945655, 1.7356093647231832, 1.739428375745908, 1.734216182063786, 1.735924401946621, 1.7294053917850083], 'val_losses': [1.9067866241494271, 1.9008652892063143, 1.8250213789022771, 1.8212746967425129, 1.7936816965931714, 1.78516634346145, 1.7793924357714532, 1.769803891876798, 1.782525479591904, 1.7777377164329506, 1.7688931365491691, 1.7477049248631684, 1.747203717299264, 1.7782229578087256, 1.7499035304836306, 1.7454768152049334, 1.7427977084097834, 1.7504448777466337, 1.736016163086924, 1.730878201392551, 1.7309840102793803, 1.7711974309360363, 1.7295139527473216, 1.7370797621697476, 1.7313700331704776, 1.7415589296553402, 1.7182607336490021, 1.7487771540951151, 1.719017685602339, 1.7184112005672305, 1.7537003133757314, 1.7292870633689672, 1.714490760643819, 1.717925480442408], 'val_acc': [0.2406966545545671, 0.3825335171031199, 0.3871695276281168, 0.28668086705926576, 0.45821325648414984, 0.44706177170780603, 0.34732489662949506, 0.3919308357348703, 0.2851772960781857, 0.26412730234306475, 0.43979451196591907, 0.4867811051246711, 0.4197468988848515, 0.6902643778975066, 0.41473499561458466, 0.27415110888359856, 0.3336674602180178, 0.47976444054629747, 0.35145971682746524, 0.35759929833354215, 0.35246209748151863, 0.3269013908031575, 0.3279037714572109, 0.2713945620849518, 0.3891742889362235, 0.3020924696153364, 0.3256484149855908, 0.276657060518732, 0.27991479764440547, 0.3664954266382659, 0.2970805663450695, 0.284300213005889, 0.3601052499686756, 0.3710061395815061], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 10, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.3848866607601345, 'num_classes': 5, 'epochs': 34, 'batch_size': 128, 'lr': 0.0016658274496447242, 'weight_decay': 0.005298118994438062, 'grad_clip_norm': 4.361682368126166, 'use_focal_loss': False, 'focal_gamma': 2.035021028094691, 'label_smoothing': 0.15312498010270212, 'compute_class_weights': True, 'augmentations': {'aug_prob': 0.6427709210238178, 'aug_jitter_std': 0.04023754380125276, 'aug_scale_low': 0.9206354322485091, 'aug_scale_high': 1.1604867929371985, 'aug_drift_max_amp': 0.09332313378193598, 'normalize': True}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 229390}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0016658274496447242, 'batch_size': 128, 'epochs': 34, 'head_dim': 10, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.3848866607601345, 'weight_decay': 0.005298118994438062, 'grad_clip_norm': 4.361682368126166, 'use_focal_loss': False, 'focal_gamma': 2.035021028094691, 'label_smoothing': 0.15312498010270212, 'compute_class_weights': True, 'aug_prob': 0.6427709210238178, 'aug_jitter_std': 0.04023754380125276, 'aug_scale_low': 0.9206354322485091, 'aug_scale_high': 1.1604867929371985, 'aug_drift_max_amp': 0.09332313378193598, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 229390}, 'model_parameter_count': 18869, 'model_storage_size_kb': 81.077734375, 'model_size_validation': 'PASS'}
2025-10-03 00:05:51,890 - INFO - _models.training_function_executor - BO Objective: base=0.3710, size_penalty=0.0000, final=0.3710
2025-10-03 00:05:51,890 - INFO - _models.training_function_executor - Model: 18,869 parameters, 81.1KB (PASS 256KB limit)
2025-10-03 00:05:51,890 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 68.312s
2025-10-03 00:05:52,019 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.3710
2025-10-03 00:05:52,019 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.129s
2025-10-03 00:05:52,019 - INFO - bo.run_bo - Recorded observation #29: hparams={'lr': 0.0016658274496447242, 'batch_size': np.int64(128), 'epochs': np.int64(34), 'head_dim': np.int64(10), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.3848866607601345, 'weight_decay': 0.005298118994438062, 'grad_clip_norm': 4.361682368126166, 'use_focal_loss': np.False_, 'focal_gamma': 2.035021028094691, 'label_smoothing': 0.15312498010270212, 'compute_class_weights': np.True_, 'aug_prob': 0.6427709210238178, 'aug_jitter_std': 0.04023754380125276, 'aug_scale_low': 0.9206354322485091, 'aug_scale_high': 1.1604867929371985, 'aug_drift_max_amp': 0.09332313378193598, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(229390)}, value=0.3710
2025-10-03 00:05:52,019 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 29: {'lr': 0.0016658274496447242, 'batch_size': np.int64(128), 'epochs': np.int64(34), 'head_dim': np.int64(10), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.3848866607601345, 'weight_decay': 0.005298118994438062, 'grad_clip_norm': 4.361682368126166, 'use_focal_loss': np.False_, 'focal_gamma': 2.035021028094691, 'label_smoothing': 0.15312498010270212, 'compute_class_weights': np.True_, 'aug_prob': 0.6427709210238178, 'aug_jitter_std': 0.04023754380125276, 'aug_scale_low': 0.9206354322485091, 'aug_scale_high': 1.1604867929371985, 'aug_drift_max_amp': 0.09332313378193598, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(229390)} -> 0.3710
2025-10-03 00:05:52,020 - INFO - bo.run_bo - üîçBO Trial 30: Using RF surrogate + Expected Improvement
2025-10-03 00:05:52,020 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 00:05:52,020 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:05:52,020 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:05:52,020 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001678129365530074, 'batch_size': 32, 'epochs': 50, 'head_dim': 22, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 10, 'dropout': 0.38070925935070815, 'weight_decay': 0.0001456142955398164, 'grad_clip_norm': 2.952605214804773, 'use_focal_loss': False, 'focal_gamma': 0.9886417479480991, 'label_smoothing': 0.19794824299140248, 'compute_class_weights': False, 'aug_prob': 0.06132724930592494, 'aug_jitter_std': 0.0488152374851007, 'aug_scale_low': 0.9339990831879076, 'aug_scale_high': 1.1646232966391867, 'aug_drift_max_amp': 0.14409058908905234, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 272005}
2025-10-03 00:05:52,021 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001678129365530074, 'batch_size': 32, 'epochs': 50, 'head_dim': 22, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 10, 'dropout': 0.38070925935070815, 'weight_decay': 0.0001456142955398164, 'grad_clip_norm': 2.952605214804773, 'use_focal_loss': False, 'focal_gamma': 0.9886417479480991, 'label_smoothing': 0.19794824299140248, 'compute_class_weights': False, 'aug_prob': 0.06132724930592494, 'aug_jitter_std': 0.0488152374851007, 'aug_scale_low': 0.9339990831879076, 'aug_scale_high': 1.1646232966391867, 'aug_drift_max_amp': 0.14409058908905234, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 272005}
2025-10-03 00:07:17,652 - INFO - _models.training_function_executor - Model: 7,769 parameters, 33.4KB storage
2025-10-03 00:07:17,652 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9259947534678922, 0.8112904320637602, 0.7845240514185727, 0.770809964951733, 0.7639860815823095, 0.7593174620095611, 0.7559241425351377, 0.7522968755096091, 0.750518206256421, 0.7483784008738623, 0.7448163905661458, 0.7440620053009787, 0.742524642310704, 0.7408496750382086, 0.7402031243144318, 0.7399623430832412, 0.7383304693546301, 0.7374839733795829, 0.7363011071889826, 0.736274455575997, 0.7355604394875914, 0.7347054172719073, 0.7339285425205815, 0.7332397679377703, 0.7332867187758053, 0.733116892104616, 0.7326002322211808, 0.7315510960071565, 0.7300922233624532, 0.7301832784653547, 0.7300986361784418, 0.7292788855579082, 0.7292489858624093, 0.7282630927214007, 0.7279590491912749, 0.7277002647246976, 0.727930415861552, 0.7273013098225783, 0.7268665198118724, 0.7262889829747857, 0.7273038367062543, 0.725677540462253, 0.7261719333775617, 0.7258906879154836, 0.7256748993564129, 0.7252580989883294, 0.7250518335217772, 0.7252174747287826, 0.7247413327004368, 0.7240677414519093], 'val_losses': [0.8384235651178507, 0.7850239291150474, 0.7636246162090605, 0.7490487533499313, 0.7456208178011521, 0.7434273172091039, 0.7431727356965075, 0.744016346231228, 0.7467339687920381, 0.7315898597457029, 0.7369367625088699, 0.7317147437170803, 0.7301811602850214, 0.734507520236761, 0.7298936158877957, 0.7341442241090355, 0.7298217743892834, 0.7281335265605675, 0.7278262213453167, 0.7285497599026864, 0.7249639773784073, 0.72708197338153, 0.7254341189656068, 0.7418924748875148, 0.7242334002750049, 0.7569978139093086, 0.7246317212376405, 0.7431399243141741, 0.7256652976378898, 0.722328356096222, 0.7234715311363845, 0.7311694130176382, 0.7288515945975157, 0.7254987105497545, 0.7328184576444408, 0.7348580464296899, 0.7289711182061599, 0.7227134501750331, 0.7379070371034644, 0.7209065124708046, 0.7384807953050659, 0.722339548333936, 0.7178862065080562, 0.7182932827813541, 0.7248525415024293, 0.7280875389594779, 0.7443809209050967, 0.7294566380904441, 0.7328115708396901, 0.7233944372072687], 'val_acc': [0.9121663951885729, 0.9503821576243578, 0.9535145971682747, 0.9565217391304348, 0.9576494173662449, 0.9562711439669215, 0.9591529883473249, 0.9631625109635384, 0.963287808545295, 0.9641648916175918, 0.9615336424007017, 0.962536023054755, 0.9620348327277284, 0.962160130309485, 0.9644154867811051, 0.9636637012905651, 0.9656684625986719, 0.9635384037088084, 0.9660443553439418, 0.9656684625986719, 0.9670467359979953, 0.9680491166520486, 0.9688009021425886, 0.9691767948878587, 0.967547926325022, 0.9688009021425886, 0.970930961032452, 0.967547926325022, 0.9699285803783987, 0.9716827465229921, 0.9708056634506954, 0.969427390051372, 0.9700538779601554, 0.9693020924696153, 0.9719333416865055, 0.9726851271770455, 0.9713068537777221, 0.9714321513594788, 0.9719333416865055, 0.970930961032452, 0.9743139957398822, 0.9720586392682621, 0.9704297707054254, 0.969427390051372, 0.9708056634506954, 0.9733116150858289, 0.9743139957398822, 0.9736875078310988, 0.9748151860669089, 0.9721839368500188], 'config': {'seq_len': 1000, 'patch_size': 10, 'head_dim': 22, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 5, 'dropout': 0.38070925935070815, 'num_classes': 5, 'epochs': 50, 'batch_size': 32, 'lr': 0.001678129365530074, 'weight_decay': 0.0001456142955398164, 'grad_clip_norm': 2.952605214804773, 'use_focal_loss': False, 'focal_gamma': 0.9886417479480991, 'label_smoothing': 0.19794824299140248, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.06132724930592494, 'aug_jitter_std': 0.0488152374851007, 'aug_scale_low': 0.9339990831879076, 'aug_scale_high': 1.1646232966391867, 'aug_drift_max_amp': 0.14409058908905234, 'normalize': True}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'seed': 272005}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001678129365530074, 'batch_size': 32, 'epochs': 50, 'head_dim': 22, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 10, 'dropout': 0.38070925935070815, 'weight_decay': 0.0001456142955398164, 'grad_clip_norm': 2.952605214804773, 'use_focal_loss': False, 'focal_gamma': 0.9886417479480991, 'label_smoothing': 0.19794824299140248, 'compute_class_weights': False, 'aug_prob': 0.06132724930592494, 'aug_jitter_std': 0.0488152374851007, 'aug_scale_low': 0.9339990831879076, 'aug_scale_high': 1.1646232966391867, 'aug_drift_max_amp': 0.14409058908905234, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'seed': 272005}, 'model_parameter_count': 7769, 'model_storage_size_kb': 33.382421875000006, 'model_size_validation': 'PASS'}
2025-10-03 00:07:17,652 - INFO - _models.training_function_executor - BO Objective: base=0.9722, size_penalty=0.0000, final=0.9722
2025-10-03 00:07:17,652 - INFO - _models.training_function_executor - Model: 7,769 parameters, 33.4KB (PASS 256KB limit)
2025-10-03 00:07:17,652 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 85.632s
2025-10-03 00:07:17,782 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9722
2025-10-03 00:07:17,782 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.130s
2025-10-03 00:07:17,782 - INFO - bo.run_bo - Recorded observation #30: hparams={'lr': 0.001678129365530074, 'batch_size': np.int64(32), 'epochs': np.int64(50), 'head_dim': np.int64(22), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(10), 'dropout': 0.38070925935070815, 'weight_decay': 0.0001456142955398164, 'grad_clip_norm': 2.952605214804773, 'use_focal_loss': np.False_, 'focal_gamma': 0.9886417479480991, 'label_smoothing': 0.19794824299140248, 'compute_class_weights': np.False_, 'aug_prob': 0.06132724930592494, 'aug_jitter_std': 0.0488152374851007, 'aug_scale_low': 0.9339990831879076, 'aug_scale_high': 1.1646232966391867, 'aug_drift_max_amp': 0.14409058908905234, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(272005)}, value=0.9722
2025-10-03 00:07:17,782 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 30: {'lr': 0.001678129365530074, 'batch_size': np.int64(32), 'epochs': np.int64(50), 'head_dim': np.int64(22), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(10), 'dropout': 0.38070925935070815, 'weight_decay': 0.0001456142955398164, 'grad_clip_norm': 2.952605214804773, 'use_focal_loss': np.False_, 'focal_gamma': 0.9886417479480991, 'label_smoothing': 0.19794824299140248, 'compute_class_weights': np.False_, 'aug_prob': 0.06132724930592494, 'aug_jitter_std': 0.0488152374851007, 'aug_scale_low': 0.9339990831879076, 'aug_scale_high': 1.1646232966391867, 'aug_drift_max_amp': 0.14409058908905234, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'seed': np.int64(272005)} -> 0.9722
2025-10-03 00:07:17,782 - INFO - bo.run_bo - üîçBO Trial 31: Using RF surrogate + Expected Improvement
2025-10-03 00:07:17,782 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-03 00:07:17,782 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:07:17,782 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:07:17,782 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002394383496927743, 'batch_size': 32, 'epochs': 37, 'head_dim': 15, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 4, 'stem_channels': 12, 'ds_kernel1': 7, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.4853182690985613, 'weight_decay': 0.00984203285367288, 'grad_clip_norm': 3.851221523387038, 'use_focal_loss': False, 'focal_gamma': 1.30852544811032, 'label_smoothing': 0.0502818902023458, 'compute_class_weights': False, 'aug_prob': 0.14055884648906294, 'aug_jitter_std': 0.019783663580625582, 'aug_scale_low': 0.9631163507850752, 'aug_scale_high': 1.1355515275645396, 'aug_drift_max_amp': 0.1325660930444326, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'seed': 56824}
2025-10-03 00:07:17,784 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002394383496927743, 'batch_size': 32, 'epochs': 37, 'head_dim': 15, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 4, 'stem_channels': 12, 'ds_kernel1': 7, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.4853182690985613, 'weight_decay': 0.00984203285367288, 'grad_clip_norm': 3.851221523387038, 'use_focal_loss': False, 'focal_gamma': 1.30852544811032, 'label_smoothing': 0.0502818902023458, 'compute_class_weights': False, 'aug_prob': 0.14055884648906294, 'aug_jitter_std': 0.019783663580625582, 'aug_scale_low': 0.9631163507850752, 'aug_scale_high': 1.1355515275645396, 'aug_drift_max_amp': 0.1325660930444326, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'seed': 56824}
2025-10-03 00:08:03,387 - INFO - _models.training_function_executor - Model: 10,745 parameters, 46.2KB storage
2025-10-03 00:08:03,387 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6302940595909172, 0.44263550244906347, 0.40874622080651185, 0.38876476614086813, 0.37602544438905056, 0.36534274165236674, 0.36007060062256807, 0.35282525134890225, 0.3482618792112006, 0.3496553710789115, 0.34375849581188983, 0.3429533040800239, 0.34218301269000057, 0.3392692473339668, 0.33813043850479396, 0.33699502786108865, 0.33632045259445237, 0.3346198174936881, 0.3334025623964695, 0.33244685654808537, 0.33268502530039495, 0.3322298461992901, 0.32974483258304554, 0.3299560956767794, 0.32765910555850863, 0.3276671778699029, 0.3274093120193263, 0.3255314194752422, 0.3266440350420548, 0.3259533157258413, 0.32572052410070274, 0.3240547884702944, 0.32402062364123546, 0.3250440253753006, 0.3242213763778559, 0.3240723589749891, 0.3237138989126832], 'val_losses': [0.4656732109290229, 0.4036435652400539, 0.3864197629411602, 0.3633074054755658, 0.3513340955800153, 0.35546624767662305, 0.34584898905950656, 0.3331610508064267, 0.33498480311836343, 0.32965706499607816, 0.3327358688549087, 0.3320894928007493, 0.3247557652727014, 0.3332207626215633, 0.3225893195918771, 0.3160325969360389, 0.3187465684304275, 0.31513471654402164, 0.3152223796589663, 0.3181578791271459, 0.32272910150933753, 0.3121306807747492, 0.31389722540599335, 0.3245041140554364, 0.3141931660632982, 0.3243045608376877, 0.3123321942317577, 0.32233849186868, 0.31056689916958324, 0.3173781571576745, 0.32274545593050213, 0.31021968445119485, 0.313016345328876, 0.31211920542430793, 0.3118254610088412, 0.31160325783563575, 0.3095018653706222], 'val_acc': [0.9272021049993735, 0.9419872196466608, 0.9510086455331412, 0.9520110261871946, 0.9597794762561083, 0.9547675729858414, 0.9644154867811051, 0.9615336424007017, 0.9611577496554317, 0.9644154867811051, 0.9654178674351584, 0.9615336424007017, 0.963287808545295, 0.9644154867811051, 0.9627866182182684, 0.9676732239067786, 0.9674226287432652, 0.9695526876331286, 0.9672973311615086, 0.9669214384162386, 0.9629119158000251, 0.9656684625986719, 0.967547926325022, 0.9634131061270518, 0.9657937601804285, 0.9634131061270518, 0.9679238190702919, 0.9641648916175918, 0.9693020924696153, 0.9634131061270518, 0.963287808545295, 0.9695526876331286, 0.9672973311615086, 0.968299711815562, 0.9654178674351584, 0.9703044731236687, 0.9685503069790753], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 15, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 4, 'stem_channels': 12, 'ds_kernel1': 7, 'ds_kernel2': 7, 'dropout': 0.4853182690985613, 'num_classes': 5, 'epochs': 37, 'batch_size': 32, 'lr': 0.002394383496927743, 'weight_decay': 0.00984203285367288, 'grad_clip_norm': 3.851221523387038, 'use_focal_loss': False, 'focal_gamma': 1.30852544811032, 'label_smoothing': 0.0502818902023458, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.14055884648906294, 'aug_jitter_std': 0.019783663580625582, 'aug_scale_low': 0.9631163507850752, 'aug_scale_high': 1.1355515275645396, 'aug_drift_max_amp': 0.1325660930444326, 'normalize': False}, 'quantization': {'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'seed': 56824}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002394383496927743, 'batch_size': 32, 'epochs': 37, 'head_dim': 15, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 4, 'stem_channels': 12, 'ds_kernel1': 7, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.4853182690985613, 'weight_decay': 0.00984203285367288, 'grad_clip_norm': 3.851221523387038, 'use_focal_loss': False, 'focal_gamma': 1.30852544811032, 'label_smoothing': 0.0502818902023458, 'compute_class_weights': False, 'aug_prob': 0.14055884648906294, 'aug_jitter_std': 0.019783663580625582, 'aug_scale_low': 0.9631163507850752, 'aug_scale_high': 1.1355515275645396, 'aug_drift_max_amp': 0.1325660930444326, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'seed': 56824}, 'model_parameter_count': 10745, 'model_storage_size_kb': 46.16992187500001, 'model_size_validation': 'PASS'}
2025-10-03 00:08:03,387 - INFO - _models.training_function_executor - BO Objective: base=0.9686, size_penalty=0.0000, final=0.9686
2025-10-03 00:08:03,387 - INFO - _models.training_function_executor - Model: 10,745 parameters, 46.2KB (PASS 256KB limit)
2025-10-03 00:08:03,387 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 45.604s
2025-10-03 00:08:03,518 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9686
2025-10-03 00:08:03,518 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.131s
2025-10-03 00:08:03,518 - INFO - bo.run_bo - Recorded observation #31: hparams={'lr': 0.002394383496927743, 'batch_size': np.int64(32), 'epochs': np.int64(37), 'head_dim': np.int64(15), 'n_heads': np.int64(4), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.4853182690985613, 'weight_decay': 0.00984203285367288, 'grad_clip_norm': 3.851221523387038, 'use_focal_loss': np.False_, 'focal_gamma': 1.30852544811032, 'label_smoothing': 0.0502818902023458, 'compute_class_weights': np.False_, 'aug_prob': 0.14055884648906294, 'aug_jitter_std': 0.019783663580625582, 'aug_scale_low': 0.9631163507850752, 'aug_scale_high': 1.1355515275645396, 'aug_drift_max_amp': 0.1325660930444326, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(56824)}, value=0.9686
2025-10-03 00:08:03,518 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 31: {'lr': 0.002394383496927743, 'batch_size': np.int64(32), 'epochs': np.int64(37), 'head_dim': np.int64(15), 'n_heads': np.int64(4), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.4853182690985613, 'weight_decay': 0.00984203285367288, 'grad_clip_norm': 3.851221523387038, 'use_focal_loss': np.False_, 'focal_gamma': 1.30852544811032, 'label_smoothing': 0.0502818902023458, 'compute_class_weights': np.False_, 'aug_prob': 0.14055884648906294, 'aug_jitter_std': 0.019783663580625582, 'aug_scale_low': 0.9631163507850752, 'aug_scale_high': 1.1355515275645396, 'aug_drift_max_amp': 0.1325660930444326, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(56824)} -> 0.9686
2025-10-03 00:08:03,518 - INFO - bo.run_bo - üîçBO Trial 32: Using RF surrogate + Expected Improvement
2025-10-03 00:08:03,518 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:08:03,518 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:08:03,519 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:08:03,519 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006764006998859417, 'batch_size': 128, 'epochs': 32, 'head_dim': 12, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 7, 'patch_size': 100, 'dropout': 0.0005261042337958434, 'weight_decay': 0.0027507942526819253, 'grad_clip_norm': 4.057896095554695, 'use_focal_loss': True, 'focal_gamma': 3.772579138491524, 'label_smoothing': 0.18021234165053757, 'compute_class_weights': False, 'aug_prob': 0.9138734451126277, 'aug_jitter_std': 0.016309026320120865, 'aug_scale_low': 0.8089866995935535, 'aug_scale_high': 1.1533621499481375, 'aug_drift_max_amp': 0.12328745497026568, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 640527}
2025-10-03 00:08:03,520 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006764006998859417, 'batch_size': 128, 'epochs': 32, 'head_dim': 12, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 7, 'patch_size': 100, 'dropout': 0.0005261042337958434, 'weight_decay': 0.0027507942526819253, 'grad_clip_norm': 4.057896095554695, 'use_focal_loss': True, 'focal_gamma': 3.772579138491524, 'label_smoothing': 0.18021234165053757, 'compute_class_weights': False, 'aug_prob': 0.9138734451126277, 'aug_jitter_std': 0.016309026320120865, 'aug_scale_low': 0.8089866995935535, 'aug_scale_high': 1.1533621499481375, 'aug_drift_max_amp': 0.12328745497026568, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 640527}
2025-10-03 00:08:53,950 - INFO - _models.training_function_executor - Model: 18,805 parameters, 80.8KB storage
2025-10-03 00:08:53,951 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2509212007973833, 0.13647577987654566, 0.10613841510343136, 0.09218408910786527, 0.08563482341720463, 0.0798361705608473, 0.07764093073232746, 0.07499212926486544, 0.07146002863016628, 0.07129812632792912, 0.07066343453689218, 0.06789564510025939, 0.069272946608704, 0.06609096714896344, 0.06670780866755634, 0.0657035422748321, 0.06520103393311309, 0.06452573645705101, 0.0639058453392438, 0.06370443420438743, 0.06321923937647951, 0.06404131245942461, 0.06242681977807069, 0.06176597291172901, 0.06113498179172639, 0.061238312639891906, 0.060593950673944796, 0.060539386061854605, 0.06015201714783146, 0.05997348695487429, 0.05964752542435482, 0.05873115273891593], 'val_losses': [0.19363230453586386, 0.12545798080388115, 0.09815245022762241, 0.11258149576928737, 0.08821913198079102, 0.08667201452660181, 0.07356544236759094, 0.07143488757955507, 0.08256739130680789, 0.09406149206346265, 0.07833803269221899, 0.10057463149179896, 0.06892678784989516, 0.07658576128973786, 0.06804455229642621, 0.06851161017130826, 0.06611225646385467, 0.06999504570393555, 0.07250173640660423, 0.0715505545936705, 0.06426796642345767, 0.06516627442241536, 0.06631021444330895, 0.06536149985919305, 0.06299406505328584, 0.06529116217462869, 0.06641648626222212, 0.06305742961801603, 0.06918417100391208, 0.0620047034939165, 0.06655229960052819, 0.0618507873452243], 'val_acc': [0.8564089713068538, 0.9339681744142339, 0.9582759052750282, 0.9407342438290941, 0.9568976318757048, 0.9597794762561083, 0.9654178674351584, 0.9679238190702919, 0.9666708432527252, 0.9522616213507079, 0.9660443553439418, 0.9391053752662574, 0.970930961032452, 0.9706803658689387, 0.9695526876331286, 0.9704297707054254, 0.9745645909033955, 0.9720586392682621, 0.9660443553439418, 0.9741886981581256, 0.9750657812304222, 0.9728104247588022, 0.9741886981581256, 0.9743139957398822, 0.9746898884851523, 0.9715574489412354, 0.9749404836486656, 0.9773211377020423, 0.9693020924696153, 0.9754416739756923, 0.9726851271770455, 0.9751910788121789], 'config': {'seq_len': 1000, 'patch_size': 100, 'head_dim': 12, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 7, 'dropout': 0.0005261042337958434, 'num_classes': 5, 'epochs': 32, 'batch_size': 128, 'lr': 0.006764006998859417, 'weight_decay': 0.0027507942526819253, 'grad_clip_norm': 4.057896095554695, 'use_focal_loss': True, 'focal_gamma': 3.772579138491524, 'label_smoothing': 0.18021234165053757, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.9138734451126277, 'aug_jitter_std': 0.016309026320120865, 'aug_scale_low': 0.8089866995935535, 'aug_scale_high': 1.1533621499481375, 'aug_drift_max_amp': 0.12328745497026568, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 640527}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006764006998859417, 'batch_size': 128, 'epochs': 32, 'head_dim': 12, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 7, 'patch_size': 100, 'dropout': 0.0005261042337958434, 'weight_decay': 0.0027507942526819253, 'grad_clip_norm': 4.057896095554695, 'use_focal_loss': True, 'focal_gamma': 3.772579138491524, 'label_smoothing': 0.18021234165053757, 'compute_class_weights': False, 'aug_prob': 0.9138734451126277, 'aug_jitter_std': 0.016309026320120865, 'aug_scale_low': 0.8089866995935535, 'aug_scale_high': 1.1533621499481375, 'aug_drift_max_amp': 0.12328745497026568, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 640527}, 'model_parameter_count': 18805, 'model_storage_size_kb': 80.802734375, 'model_size_validation': 'PASS'}
2025-10-03 00:08:53,951 - INFO - _models.training_function_executor - BO Objective: base=0.9752, size_penalty=0.0000, final=0.9752
2025-10-03 00:08:53,951 - INFO - _models.training_function_executor - Model: 18,805 parameters, 80.8KB (PASS 256KB limit)
2025-10-03 00:08:53,951 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 50.432s
2025-10-03 00:08:54,081 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9752
2025-10-03 00:08:54,081 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.131s
2025-10-03 00:08:54,081 - INFO - bo.run_bo - Recorded observation #32: hparams={'lr': 0.006764006998859417, 'batch_size': np.int64(128), 'epochs': np.int64(32), 'head_dim': np.int64(12), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(100), 'dropout': 0.0005261042337958434, 'weight_decay': 0.0027507942526819253, 'grad_clip_norm': 4.057896095554695, 'use_focal_loss': np.True_, 'focal_gamma': 3.772579138491524, 'label_smoothing': 0.18021234165053757, 'compute_class_weights': np.False_, 'aug_prob': 0.9138734451126277, 'aug_jitter_std': 0.016309026320120865, 'aug_scale_low': 0.8089866995935535, 'aug_scale_high': 1.1533621499481375, 'aug_drift_max_amp': 0.12328745497026568, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(640527)}, value=0.9752
2025-10-03 00:08:54,081 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 32: {'lr': 0.006764006998859417, 'batch_size': np.int64(128), 'epochs': np.int64(32), 'head_dim': np.int64(12), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(100), 'dropout': 0.0005261042337958434, 'weight_decay': 0.0027507942526819253, 'grad_clip_norm': 4.057896095554695, 'use_focal_loss': np.True_, 'focal_gamma': 3.772579138491524, 'label_smoothing': 0.18021234165053757, 'compute_class_weights': np.False_, 'aug_prob': 0.9138734451126277, 'aug_jitter_std': 0.016309026320120865, 'aug_scale_low': 0.8089866995935535, 'aug_scale_high': 1.1533621499481375, 'aug_drift_max_amp': 0.12328745497026568, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(640527)} -> 0.9752
2025-10-03 00:08:54,082 - INFO - bo.run_bo - üîçBO Trial 33: Using RF surrogate + Expected Improvement
2025-10-03 00:08:54,082 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:08:54,082 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:08:54,082 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:08:54,082 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0022904153738246636, 'batch_size': 128, 'epochs': 40, 'head_dim': 21, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 5, 'dropout': 0.42106797891653835, 'weight_decay': 0.008719296102961903, 'grad_clip_norm': 4.549438111072757, 'use_focal_loss': False, 'focal_gamma': 3.0545864626079533, 'label_smoothing': 0.01265269053425531, 'compute_class_weights': False, 'aug_prob': 0.049144112039310586, 'aug_jitter_std': 0.04566359295828633, 'aug_scale_low': 0.8087129638848505, 'aug_scale_high': 1.116155018143367, 'aug_drift_max_amp': 0.19540562665896088, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 166323}
2025-10-03 00:08:54,083 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0022904153738246636, 'batch_size': 128, 'epochs': 40, 'head_dim': 21, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 5, 'dropout': 0.42106797891653835, 'weight_decay': 0.008719296102961903, 'grad_clip_norm': 4.549438111072757, 'use_focal_loss': False, 'focal_gamma': 3.0545864626079533, 'label_smoothing': 0.01265269053425531, 'compute_class_weights': False, 'aug_prob': 0.049144112039310586, 'aug_jitter_std': 0.04566359295828633, 'aug_scale_low': 0.8087129638848505, 'aug_scale_high': 1.116155018143367, 'aug_drift_max_amp': 0.19540562665896088, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 166323}
2025-10-03 00:09:30,013 - INFO - _models.training_function_executor - Model: 18,741 parameters, 20.1KB storage
2025-10-03 00:09:30,014 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6798023516677792, 0.3652009290960712, 0.28657614401332854, 0.25609770172151586, 0.23374940529972135, 0.21395925805614477, 0.2047633024493476, 0.19985473503169643, 0.19524239822585, 0.18986399136515178, 0.1869495555997003, 0.18516704554665794, 0.18215396746662116, 0.17861596837136148, 0.1760584367770276, 0.17224281856287985, 0.17175998189180966, 0.16987741345922044, 0.1692449955292275, 0.17034693006127938, 0.1677032670462613, 0.16629867091753, 0.1661895164902548, 0.16600823488853272, 0.16386081940644637, 0.16168287725514033, 0.16087613943969495, 0.1596627217193999, 0.15930503272391858, 0.15879991740670765, 0.160008359889139, 0.15949544850872807, 0.15957359994588907, 0.15740668647899383, 0.1570569910927083, 0.15564209097181875, 0.15624306662501733, 0.1549818137047326, 0.15547833888131088, 0.15624015820850554], 'val_losses': [0.46133722499892343, 0.301462446697791, 0.2702898773136122, 0.23438516719615216, 0.2473211785599397, 0.3081073741718068, 0.18796000730392404, 0.18152043539664303, 0.2050116920871493, 0.18565253680951163, 0.1729354323330805, 0.1876411415900731, 0.1988561482092534, 0.19311581314162732, 0.18284390541884613, 0.16695702678278088, 0.164226619955472, 0.1581468152613019, 0.18913114210549878, 0.17022412569133608, 0.1548011034387288, 0.18548530154607004, 0.16028234665442523, 0.15453707410076062, 0.15719582488656028, 0.17433939422712397, 0.15472341822795202, 0.15049649004671242, 0.16773081494033404, 0.1612521107465518, 0.14753542344024376, 0.16537007451423383, 0.14890611903057438, 0.1538859179646968, 0.1446398159134524, 0.14968189739774693, 0.14954104760605474, 0.1486119505826908, 0.15692307582819853, 0.1438333901855943], 'val_acc': [0.8701917053000877, 0.9310863300338303, 0.939230672848014, 0.9495050745520611, 0.9493797769703045, 0.915173537150733, 0.9641648916175918, 0.9646660819446184, 0.9591529883473249, 0.9626613206365117, 0.9656684625986719, 0.9635384037088084, 0.9610324520736749, 0.9594035835108382, 0.9661696529256986, 0.9713068537777221, 0.9699285803783987, 0.9735622102493422, 0.9666708432527252, 0.9716827465229921, 0.9736875078310988, 0.9654178674351584, 0.9740634005763689, 0.9728104247588022, 0.9715574489412354, 0.9706803658689387, 0.9731863175040721, 0.9736875078310988, 0.9735622102493422, 0.9735622102493422, 0.9750657812304222, 0.9736875078310988, 0.9748151860669089, 0.9743139957398822, 0.9760681618844756, 0.9751910788121789, 0.9756922691392056, 0.9751910788121789, 0.9724345320135321, 0.9754416739756923], 'config': {'seq_len': 1000, 'patch_size': 5, 'head_dim': 21, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 5, 'ds_kernel2': 7, 'dropout': 0.42106797891653835, 'num_classes': 5, 'epochs': 40, 'batch_size': 128, 'lr': 0.0022904153738246636, 'weight_decay': 0.008719296102961903, 'grad_clip_norm': 4.549438111072757, 'use_focal_loss': False, 'focal_gamma': 3.0545864626079533, 'label_smoothing': 0.01265269053425531, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.049144112039310586, 'aug_jitter_std': 0.04566359295828633, 'aug_scale_low': 0.8087129638848505, 'aug_scale_high': 1.116155018143367, 'aug_drift_max_amp': 0.19540562665896088, 'normalize': False}, 'quantization': {'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'seed': 166323}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0022904153738246636, 'batch_size': 128, 'epochs': 40, 'head_dim': 21, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 5, 'dropout': 0.42106797891653835, 'weight_decay': 0.008719296102961903, 'grad_clip_norm': 4.549438111072757, 'use_focal_loss': False, 'focal_gamma': 3.0545864626079533, 'label_smoothing': 0.01265269053425531, 'compute_class_weights': False, 'aug_prob': 0.049144112039310586, 'aug_jitter_std': 0.04566359295828633, 'aug_scale_low': 0.8087129638848505, 'aug_scale_high': 1.116155018143367, 'aug_drift_max_amp': 0.19540562665896088, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 166323}, 'model_parameter_count': 18741, 'model_storage_size_kb': 20.131933593750002, 'model_size_validation': 'PASS'}
2025-10-03 00:09:30,014 - INFO - _models.training_function_executor - BO Objective: base=0.9754, size_penalty=0.0000, final=0.9754
2025-10-03 00:09:30,014 - INFO - _models.training_function_executor - Model: 18,741 parameters, 20.1KB (PASS 256KB limit)
2025-10-03 00:09:30,014 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 35.932s
2025-10-03 00:09:30,264 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9754
2025-10-03 00:09:30,264 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.250s
2025-10-03 00:09:30,264 - INFO - bo.run_bo - Recorded observation #33: hparams={'lr': 0.0022904153738246636, 'batch_size': np.int64(128), 'epochs': np.int64(40), 'head_dim': np.int64(21), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(5), 'dropout': 0.42106797891653835, 'weight_decay': 0.008719296102961903, 'grad_clip_norm': 4.549438111072757, 'use_focal_loss': np.False_, 'focal_gamma': 3.0545864626079533, 'label_smoothing': 0.01265269053425531, 'compute_class_weights': np.False_, 'aug_prob': 0.049144112039310586, 'aug_jitter_std': 0.04566359295828633, 'aug_scale_low': 0.8087129638848505, 'aug_scale_high': 1.116155018143367, 'aug_drift_max_amp': 0.19540562665896088, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(166323)}, value=0.9754
2025-10-03 00:09:30,264 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 33: {'lr': 0.0022904153738246636, 'batch_size': np.int64(128), 'epochs': np.int64(40), 'head_dim': np.int64(21), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(5), 'dropout': 0.42106797891653835, 'weight_decay': 0.008719296102961903, 'grad_clip_norm': 4.549438111072757, 'use_focal_loss': np.False_, 'focal_gamma': 3.0545864626079533, 'label_smoothing': 0.01265269053425531, 'compute_class_weights': np.False_, 'aug_prob': 0.049144112039310586, 'aug_jitter_std': 0.04566359295828633, 'aug_scale_low': 0.8087129638848505, 'aug_scale_high': 1.116155018143367, 'aug_drift_max_amp': 0.19540562665896088, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(166323)} -> 0.9754
2025-10-03 00:09:30,265 - INFO - bo.run_bo - üîçBO Trial 34: Using RF surrogate + Expected Improvement
2025-10-03 00:09:30,265 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:09:30,265 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:09:30,265 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:09:30,265 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.009533478351327748, 'batch_size': 64, 'epochs': 39, 'head_dim': 20, 'n_heads': 8, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.4265517749424385, 'weight_decay': 0.007431837365539624, 'grad_clip_norm': 4.934528108100691, 'use_focal_loss': False, 'focal_gamma': 3.6592065077835314, 'label_smoothing': 0.19256067307165128, 'compute_class_weights': False, 'aug_prob': 0.46237484782698, 'aug_jitter_std': 0.007277308305673437, 'aug_scale_low': 0.939858936588452, 'aug_scale_high': 1.1333569292887051, 'aug_drift_max_amp': 0.17860176103891107, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 583986}
2025-10-03 00:09:30,266 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.009533478351327748, 'batch_size': 64, 'epochs': 39, 'head_dim': 20, 'n_heads': 8, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.4265517749424385, 'weight_decay': 0.007431837365539624, 'grad_clip_norm': 4.934528108100691, 'use_focal_loss': False, 'focal_gamma': 3.6592065077835314, 'label_smoothing': 0.19256067307165128, 'compute_class_weights': False, 'aug_prob': 0.46237484782698, 'aug_jitter_std': 0.007277308305673437, 'aug_scale_low': 0.939858936588452, 'aug_scale_high': 1.1333569292887051, 'aug_drift_max_amp': 0.17860176103891107, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 583986}
2025-10-03 00:10:22,707 - INFO - _models.training_function_executor - Model: 18,869 parameters, 40.5KB storage
2025-10-03 00:10:22,707 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9566157934279033, 0.7939284254060713, 0.7612607106452067, 0.7474418598353906, 0.7388455251761181, 0.7361307095376725, 0.7323450720318053, 0.7291734237324806, 0.7266467962670408, 0.727873796383861, 0.7257331649794733, 0.7238797002973456, 0.7208695618542342, 0.7198247489738602, 0.7205516034766695, 0.7188663725618005, 0.7173690751199155, 0.7164288657603248, 0.7173376621016396, 0.7168096071182403, 0.7168908741332025, 0.7141508437545848, 0.7145295370003374, 0.7145371248043708, 0.7124336889799655, 0.7143071182400079, 0.7120501077369696, 0.71253993959089, 0.7124248827377201, 0.7126189912186677, 0.7115798283257371, 0.7104621927634303, 0.710595916795039, 0.7102977200991094, 0.7094564512252987, 0.7100993563192842, 0.7093939380445147, 0.7095053077875565, 0.710093462829045], 'val_losses': [0.8087937933149052, 0.811077302648764, 0.736955303362237, 0.7374004536800673, 0.7291398261247938, 0.7204239147824611, 0.7220763989355821, 0.7300884618360048, 0.7229395494875103, 0.7278362394215663, 0.7149178200893691, 0.7112944381022839, 0.7166781645716768, 0.712596985417311, 0.723176714546802, 0.7114711623191236, 0.7089666525622446, 0.7108563257628941, 0.7139405395927592, 0.7068051410384562, 0.7101737265672888, 0.708587930108682, 0.7063312458373627, 0.7046627810001075, 0.7068517508773842, 0.7068354978386344, 0.70799940583282, 0.709439941350865, 0.705561742367653, 0.7117349473534782, 0.7086512349166005, 0.7035359549217692, 0.7109869242670422, 0.7048328519390274, 0.7074041074043138, 0.7099032663952481, 0.7078903911182747, 0.7052297420541142, 0.7027342451827118], 'val_acc': [0.9200601428392432, 0.9312116276155871, 0.9547675729858414, 0.9627866182182684, 0.9585265004385415, 0.9634131061270518, 0.9650419746898885, 0.9651672722716451, 0.9635384037088084, 0.9639142964540784, 0.961784237564215, 0.9620348327277284, 0.9672973311615086, 0.9622854278912417, 0.9655431650169152, 0.9700538779601554, 0.9620348327277284, 0.9674226287432652, 0.9713068537777221, 0.968675604560832, 0.9636637012905651, 0.9681744142338052, 0.9691767948878587, 0.969427390051372, 0.968299711815562, 0.9665455456709685, 0.9642901891993485, 0.9664202480892119, 0.9657937601804285, 0.9671720335797519, 0.9666708432527252, 0.968675604560832, 0.969427390051372, 0.9671720335797519, 0.9684250093973187, 0.9654178674351584, 0.9696779852148854, 0.969427390051372, 0.970179175541912], 'config': {'seq_len': 1000, 'patch_size': 50, 'head_dim': 20, 'n_heads': 8, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.4265517749424385, 'num_classes': 5, 'epochs': 39, 'batch_size': 64, 'lr': 0.009533478351327748, 'weight_decay': 0.007431837365539624, 'grad_clip_norm': 4.934528108100691, 'use_focal_loss': False, 'focal_gamma': 3.6592065077835314, 'label_smoothing': 0.19256067307165128, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.46237484782698, 'aug_jitter_std': 0.007277308305673437, 'aug_scale_low': 0.939858936588452, 'aug_scale_high': 1.1333569292887051, 'aug_drift_max_amp': 0.17860176103891107, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'seed': 583986}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.009533478351327748, 'batch_size': 64, 'epochs': 39, 'head_dim': 20, 'n_heads': 8, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.4265517749424385, 'weight_decay': 0.007431837365539624, 'grad_clip_norm': 4.934528108100691, 'use_focal_loss': False, 'focal_gamma': 3.6592065077835314, 'label_smoothing': 0.19256067307165128, 'compute_class_weights': False, 'aug_prob': 0.46237484782698, 'aug_jitter_std': 0.007277308305673437, 'aug_scale_low': 0.939858936588452, 'aug_scale_high': 1.1333569292887051, 'aug_drift_max_amp': 0.17860176103891107, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 583986}, 'model_parameter_count': 18869, 'model_storage_size_kb': 40.5388671875, 'model_size_validation': 'PASS'}
2025-10-03 00:10:22,707 - INFO - _models.training_function_executor - BO Objective: base=0.9702, size_penalty=0.0000, final=0.9702
2025-10-03 00:10:22,707 - INFO - _models.training_function_executor - Model: 18,869 parameters, 40.5KB (PASS 256KB limit)
2025-10-03 00:10:22,707 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 52.443s
2025-10-03 00:10:22,841 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9702
2025-10-03 00:10:22,841 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.133s
2025-10-03 00:10:22,841 - INFO - bo.run_bo - Recorded observation #34: hparams={'lr': 0.009533478351327748, 'batch_size': np.int64(64), 'epochs': np.int64(39), 'head_dim': np.int64(20), 'n_heads': np.int64(8), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(50), 'dropout': 0.4265517749424385, 'weight_decay': 0.007431837365539624, 'grad_clip_norm': 4.934528108100691, 'use_focal_loss': np.False_, 'focal_gamma': 3.6592065077835314, 'label_smoothing': 0.19256067307165128, 'compute_class_weights': np.False_, 'aug_prob': 0.46237484782698, 'aug_jitter_std': 0.007277308305673437, 'aug_scale_low': 0.939858936588452, 'aug_scale_high': 1.1333569292887051, 'aug_drift_max_amp': 0.17860176103891107, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(583986)}, value=0.9702
2025-10-03 00:10:22,841 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 34: {'lr': 0.009533478351327748, 'batch_size': np.int64(64), 'epochs': np.int64(39), 'head_dim': np.int64(20), 'n_heads': np.int64(8), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(50), 'dropout': 0.4265517749424385, 'weight_decay': 0.007431837365539624, 'grad_clip_norm': 4.934528108100691, 'use_focal_loss': np.False_, 'focal_gamma': 3.6592065077835314, 'label_smoothing': 0.19256067307165128, 'compute_class_weights': np.False_, 'aug_prob': 0.46237484782698, 'aug_jitter_std': 0.007277308305673437, 'aug_scale_low': 0.939858936588452, 'aug_scale_high': 1.1333569292887051, 'aug_drift_max_amp': 0.17860176103891107, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(583986)} -> 0.9702
2025-10-03 00:10:22,842 - INFO - bo.run_bo - üîçBO Trial 35: Using RF surrogate + Expected Improvement
2025-10-03 00:10:22,842 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:10:22,842 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:10:22,842 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:10:22,842 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0032104084054294037, 'batch_size': 128, 'epochs': 39, 'head_dim': 20, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.49326958134332427, 'weight_decay': 0.00023188231456276387, 'grad_clip_norm': 1.9869230919923084, 'use_focal_loss': True, 'focal_gamma': 2.9726742401578945, 'label_smoothing': 0.18730944812855407, 'compute_class_weights': False, 'aug_prob': 0.45123209194602454, 'aug_jitter_std': 0.03197078521537058, 'aug_scale_low': 0.9095890689566217, 'aug_scale_high': 1.0993502661517567, 'aug_drift_max_amp': 0.10102790201349181, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 944197}
2025-10-03 00:10:22,843 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0032104084054294037, 'batch_size': 128, 'epochs': 39, 'head_dim': 20, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.49326958134332427, 'weight_decay': 0.00023188231456276387, 'grad_clip_norm': 1.9869230919923084, 'use_focal_loss': True, 'focal_gamma': 2.9726742401578945, 'label_smoothing': 0.18730944812855407, 'compute_class_weights': False, 'aug_prob': 0.45123209194602454, 'aug_jitter_std': 0.03197078521537058, 'aug_scale_low': 0.9095890689566217, 'aug_scale_high': 1.0993502661517567, 'aug_drift_max_amp': 0.10102790201349181, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 944197}
2025-10-03 00:11:32,678 - INFO - _models.training_function_executor - Model: 18,677 parameters, 80.3KB storage
2025-10-03 00:11:32,678 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2454583080552505, 0.1552741959869826, 0.13966308597347976, 0.13239365114399212, 0.12810128911266805, 0.12518418720492377, 0.1228724177723144, 0.12099660719090401, 0.11960839618236518, 0.11795223578336905, 0.11692356252302089, 0.11562062565254672, 0.11505093902589399, 0.1134748348988238, 0.11387272739781587, 0.11271917089202352, 0.11122767880889918, 0.11156062723556057, 0.11086906681034316, 0.10991412088220343, 0.11009012125295957, 0.10994510189587059, 0.10920977335176653, 0.10942048399182253, 0.108905782364303, 0.1083908123259007, 0.10762460443215707, 0.10728413503449823, 0.10712750269808358, 0.1067131996389111, 0.10736668596353252, 0.10638550841198326, 0.10668061147901836, 0.10649461101824503, 0.10557170006346853, 0.10562715211712402, 0.10569657659631838, 0.10442227723515542, 0.10497027428718796], 'val_losses': [0.16210756599791082, 0.14285828814782411, 0.1575305469239838, 0.12752124071289253, 0.12378171381589957, 0.11939220382312593, 0.11811092847302629, 0.12253854442338809, 0.11307975191745732, 0.11302376118795628, 0.11520184066985398, 0.11287500276973023, 0.11165206446494992, 0.11205927879197035, 0.12031082275920219, 0.10613128393533729, 0.10704063235697718, 0.10879504637317301, 0.10734920923258186, 0.10952691168298759, 0.10492716571405976, 0.10440219620324633, 0.12254234310111192, 0.10412282507053042, 0.10418454000800792, 0.11062330818892481, 0.10426909802230644, 0.10206813791008496, 0.10399504090300958, 0.10188631199196667, 0.10291977629836617, 0.10179912391618266, 0.10122021611337897, 0.10558800117001022, 0.11057244990870045, 0.10170650841030407, 0.09926529834927988, 0.10303936150943019, 0.10108080040488442], 'val_acc': [0.9497556697155745, 0.9501315624608445, 0.9513845382784112, 0.9619095351459717, 0.9631625109635384, 0.9626613206365117, 0.9644154867811051, 0.9679238190702919, 0.9680491166520486, 0.9651672722716451, 0.9645407843628618, 0.9695526876331286, 0.9647913795263752, 0.9719333416865055, 0.9685503069790753, 0.9713068537777221, 0.9730610199223155, 0.9738128054128555, 0.9728104247588022, 0.9735622102493422, 0.9761934594662323, 0.9745645909033955, 0.9738128054128555, 0.9748151860669089, 0.9721839368500188, 0.9708056634506954, 0.9721839368500188, 0.976694649793259, 0.9759428643027189, 0.9723092344317754, 0.9730610199223155, 0.9775717328655557, 0.9760681618844756, 0.9769452449567724, 0.9731863175040721, 0.9744392933216389, 0.9788247086831224, 0.9743139957398822, 0.9793258990101491], 'config': {'seq_len': 1000, 'patch_size': 50, 'head_dim': 20, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'dropout': 0.49326958134332427, 'num_classes': 5, 'epochs': 39, 'batch_size': 128, 'lr': 0.0032104084054294037, 'weight_decay': 0.00023188231456276387, 'grad_clip_norm': 1.9869230919923084, 'use_focal_loss': True, 'focal_gamma': 2.9726742401578945, 'label_smoothing': 0.18730944812855407, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.45123209194602454, 'aug_jitter_std': 0.03197078521537058, 'aug_scale_low': 0.9095890689566217, 'aug_scale_high': 1.0993502661517567, 'aug_drift_max_amp': 0.10102790201349181, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 944197}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0032104084054294037, 'batch_size': 128, 'epochs': 39, 'head_dim': 20, 'n_heads': 4, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.49326958134332427, 'weight_decay': 0.00023188231456276387, 'grad_clip_norm': 1.9869230919923084, 'use_focal_loss': True, 'focal_gamma': 2.9726742401578945, 'label_smoothing': 0.18730944812855407, 'compute_class_weights': False, 'aug_prob': 0.45123209194602454, 'aug_jitter_std': 0.03197078521537058, 'aug_scale_low': 0.9095890689566217, 'aug_scale_high': 1.0993502661517567, 'aug_drift_max_amp': 0.10102790201349181, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 944197}, 'model_parameter_count': 18677, 'model_storage_size_kb': 80.252734375, 'model_size_validation': 'PASS'}
2025-10-03 00:11:32,678 - INFO - _models.training_function_executor - BO Objective: base=0.9793, size_penalty=0.0000, final=0.9793
2025-10-03 00:11:32,678 - INFO - _models.training_function_executor - Model: 18,677 parameters, 80.3KB (PASS 256KB limit)
2025-10-03 00:11:32,678 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 69.836s
2025-10-03 00:11:32,810 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9793
2025-10-03 00:11:32,810 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.132s
2025-10-03 00:11:32,810 - INFO - bo.run_bo - Recorded observation #35: hparams={'lr': 0.0032104084054294037, 'batch_size': np.int64(128), 'epochs': np.int64(39), 'head_dim': np.int64(20), 'n_heads': np.int64(4), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(50), 'dropout': 0.49326958134332427, 'weight_decay': 0.00023188231456276387, 'grad_clip_norm': 1.9869230919923084, 'use_focal_loss': np.True_, 'focal_gamma': 2.9726742401578945, 'label_smoothing': 0.18730944812855407, 'compute_class_weights': np.False_, 'aug_prob': 0.45123209194602454, 'aug_jitter_std': 0.03197078521537058, 'aug_scale_low': 0.9095890689566217, 'aug_scale_high': 1.0993502661517567, 'aug_drift_max_amp': 0.10102790201349181, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(944197)}, value=0.9793
2025-10-03 00:11:32,810 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 35: {'lr': 0.0032104084054294037, 'batch_size': np.int64(128), 'epochs': np.int64(39), 'head_dim': np.int64(20), 'n_heads': np.int64(4), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(50), 'dropout': 0.49326958134332427, 'weight_decay': 0.00023188231456276387, 'grad_clip_norm': 1.9869230919923084, 'use_focal_loss': np.True_, 'focal_gamma': 2.9726742401578945, 'label_smoothing': 0.18730944812855407, 'compute_class_weights': np.False_, 'aug_prob': 0.45123209194602454, 'aug_jitter_std': 0.03197078521537058, 'aug_scale_low': 0.9095890689566217, 'aug_scale_high': 1.0993502661517567, 'aug_drift_max_amp': 0.10102790201349181, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(944197)} -> 0.9793
2025-10-03 00:11:32,810 - INFO - bo.run_bo - üîçBO Trial 36: Using RF surrogate + Expected Improvement
2025-10-03 00:11:32,811 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:11:32,811 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:11:32,811 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:11:32,811 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0033905380994907427, 'batch_size': 256, 'epochs': 46, 'head_dim': 8, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.034156059780401433, 'weight_decay': 0.0003061841354388759, 'grad_clip_norm': 2.5418362846544227, 'use_focal_loss': False, 'focal_gamma': 4.564209074951612, 'label_smoothing': 0.15047766483213051, 'compute_class_weights': False, 'aug_prob': 0.28053917025421166, 'aug_jitter_std': 0.024890791207223793, 'aug_scale_low': 0.9492749677069587, 'aug_scale_high': 1.0126903118840267, 'aug_drift_max_amp': 0.16993711718808296, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 696875}
2025-10-03 00:11:32,812 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0033905380994907427, 'batch_size': 256, 'epochs': 46, 'head_dim': 8, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.034156059780401433, 'weight_decay': 0.0003061841354388759, 'grad_clip_norm': 2.5418362846544227, 'use_focal_loss': False, 'focal_gamma': 4.564209074951612, 'label_smoothing': 0.15047766483213051, 'compute_class_weights': False, 'aug_prob': 0.28053917025421166, 'aug_jitter_std': 0.024890791207223793, 'aug_scale_low': 0.9492749677069587, 'aug_scale_high': 1.0126903118840267, 'aug_drift_max_amp': 0.16993711718808296, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 696875}
2025-10-03 00:12:20,817 - INFO - _models.training_function_executor - Model: 18,869 parameters, 81.1KB storage
2025-10-03 00:12:20,817 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9343266118343165, 0.7129583496129988, 0.6710090489180335, 0.6544379728149547, 0.6433017048901328, 0.6352190921428572, 0.6282747308924783, 0.6265659274095372, 0.6234288519644609, 0.6232918288046543, 0.6190382529872702, 0.6169405085242183, 0.6168900767977075, 0.6166075778811123, 0.6131332898836023, 0.6120928691838561, 0.6116824826474843, 0.6095720943575443, 0.6088884412333044, 0.6094165480988659, 0.6078526503579029, 0.607839474771994, 0.6057910538909467, 0.6055374626855976, 0.6057608912986902, 0.604044607551393, 0.6043829658058126, 0.6042865010403946, 0.6022530446183848, 0.6025811650678858, 0.6025353091326147, 0.6028040244092888, 0.6003281595384075, 0.6000253880968355, 0.6010388283171204, 0.5994120321469331, 0.5997499141598115, 0.6002151059711525, 0.5981685031279285, 0.5993114750800461, 0.5981526212058479, 0.5978477312628796, 0.5975285378762536, 0.5958749289441384, 0.5960323833468504, 0.596897871550382], 'val_losses': [0.7786521231556071, 0.6993497926151374, 0.6865826014990855, 0.6431959032444798, 0.7205461533412257, 0.6341311035710698, 0.6508429294286134, 0.6549977940166375, 0.7084624333842299, 0.6200836124971029, 0.6163525920197445, 0.6243569168690795, 0.6216721412846057, 0.6216069679633073, 0.6113877557017364, 0.6145672579783125, 0.6094197863072265, 0.6190850109836539, 0.6326577706961924, 0.618710715607016, 0.6133883235717743, 0.6117180292356315, 0.6162807111407922, 0.6279562087513693, 0.6051912114085776, 0.6227501668498087, 0.6135843871918676, 0.6071683466621187, 0.6089842630170548, 0.61404272053956, 0.6032502970709237, 0.6045088766586391, 0.6094896177005206, 0.6078751109518991, 0.6021637593634878, 0.6031572352359653, 0.612182431286132, 0.5976223050166365, 0.602440282891021, 0.6099859940618848, 0.60983640100406, 0.6151056250389499, 0.5989933401671078, 0.6010302967736034, 0.6111714266891274, 0.5985039788116385], 'val_acc': [0.876957774714948, 0.9355970429770706, 0.9379776970304473, 0.9530134068412479, 0.9335922816689638, 0.9577747149480015, 0.9572735246209748, 0.9517604310236812, 0.940358351083824, 0.9647913795263752, 0.9656684625986719, 0.9636637012905651, 0.9619095351459717, 0.9670467359979953, 0.9664202480892119, 0.9640395940358351, 0.9666708432527252, 0.9634131061270518, 0.9674226287432652, 0.9670467359979953, 0.9666708432527252, 0.9699285803783987, 0.9677985214885353, 0.9626613206365117, 0.970179175541912, 0.9651672722716451, 0.968675604560832, 0.970179175541912, 0.9679238190702919, 0.9681744142338052, 0.9714321513594788, 0.9714321513594788, 0.9710562586142087, 0.970179175541912, 0.9710562586142087, 0.970179175541912, 0.9730610199223155, 0.9723092344317754, 0.9698032827966421, 0.970930961032452, 0.9708056634506954, 0.9671720335797519, 0.9733116150858289, 0.9734369126675855, 0.9688009021425886, 0.9705550682871821], 'config': {'seq_len': 1000, 'patch_size': 50, 'head_dim': 8, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.034156059780401433, 'num_classes': 5, 'epochs': 46, 'batch_size': 256, 'lr': 0.0033905380994907427, 'weight_decay': 0.0003061841354388759, 'grad_clip_norm': 2.5418362846544227, 'use_focal_loss': False, 'focal_gamma': 4.564209074951612, 'label_smoothing': 0.15047766483213051, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.28053917025421166, 'aug_jitter_std': 0.024890791207223793, 'aug_scale_low': 0.9492749677069587, 'aug_scale_high': 1.0126903118840267, 'aug_drift_max_amp': 0.16993711718808296, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 696875}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0033905380994907427, 'batch_size': 256, 'epochs': 46, 'head_dim': 8, 'n_heads': 6, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 50, 'dropout': 0.034156059780401433, 'weight_decay': 0.0003061841354388759, 'grad_clip_norm': 2.5418362846544227, 'use_focal_loss': False, 'focal_gamma': 4.564209074951612, 'label_smoothing': 0.15047766483213051, 'compute_class_weights': False, 'aug_prob': 0.28053917025421166, 'aug_jitter_std': 0.024890791207223793, 'aug_scale_low': 0.9492749677069587, 'aug_scale_high': 1.0126903118840267, 'aug_drift_max_amp': 0.16993711718808296, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 696875}, 'model_parameter_count': 18869, 'model_storage_size_kb': 81.077734375, 'model_size_validation': 'PASS'}
2025-10-03 00:12:20,817 - INFO - _models.training_function_executor - BO Objective: base=0.9706, size_penalty=0.0000, final=0.9706
2025-10-03 00:12:20,818 - INFO - _models.training_function_executor - Model: 18,869 parameters, 81.1KB (PASS 256KB limit)
2025-10-03 00:12:20,818 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 48.007s
2025-10-03 00:12:20,950 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9706
2025-10-03 00:12:20,951 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.133s
2025-10-03 00:12:20,951 - INFO - bo.run_bo - Recorded observation #36: hparams={'lr': 0.0033905380994907427, 'batch_size': np.int64(256), 'epochs': np.int64(46), 'head_dim': np.int64(8), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(50), 'dropout': 0.034156059780401433, 'weight_decay': 0.0003061841354388759, 'grad_clip_norm': 2.5418362846544227, 'use_focal_loss': np.False_, 'focal_gamma': 4.564209074951612, 'label_smoothing': 0.15047766483213051, 'compute_class_weights': np.False_, 'aug_prob': 0.28053917025421166, 'aug_jitter_std': 0.024890791207223793, 'aug_scale_low': 0.9492749677069587, 'aug_scale_high': 1.0126903118840267, 'aug_drift_max_amp': 0.16993711718808296, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(696875)}, value=0.9706
2025-10-03 00:12:20,951 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 36: {'lr': 0.0033905380994907427, 'batch_size': np.int64(256), 'epochs': np.int64(46), 'head_dim': np.int64(8), 'n_heads': np.int64(6), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(50), 'dropout': 0.034156059780401433, 'weight_decay': 0.0003061841354388759, 'grad_clip_norm': 2.5418362846544227, 'use_focal_loss': np.False_, 'focal_gamma': 4.564209074951612, 'label_smoothing': 0.15047766483213051, 'compute_class_weights': np.False_, 'aug_prob': 0.28053917025421166, 'aug_jitter_std': 0.024890791207223793, 'aug_scale_low': 0.9492749677069587, 'aug_scale_high': 1.0126903118840267, 'aug_drift_max_amp': 0.16993711718808296, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(696875)} -> 0.9706
2025-10-03 00:12:20,951 - INFO - bo.run_bo - üîçBO Trial 37: Using RF surrogate + Expected Improvement
2025-10-03 00:12:20,951 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:12:20,951 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:12:20,951 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:12:20,951 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0019155240536609778, 'batch_size': 32, 'epochs': 40, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 50, 'dropout': 0.3207746594676903, 'weight_decay': 0.004096421944227174, 'grad_clip_norm': 4.924923796410516, 'use_focal_loss': False, 'focal_gamma': 1.0027955614540474, 'label_smoothing': 0.10869595049577248, 'compute_class_weights': False, 'aug_prob': 0.5910474870405912, 'aug_jitter_std': 0.021150557310300775, 'aug_scale_low': 0.8566369524141854, 'aug_scale_high': 1.1649403483082161, 'aug_drift_max_amp': 0.12684123631179833, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 107208}
2025-10-03 00:12:20,953 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0019155240536609778, 'batch_size': 32, 'epochs': 40, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 50, 'dropout': 0.3207746594676903, 'weight_decay': 0.004096421944227174, 'grad_clip_norm': 4.924923796410516, 'use_focal_loss': False, 'focal_gamma': 1.0027955614540474, 'label_smoothing': 0.10869595049577248, 'compute_class_weights': False, 'aug_prob': 0.5910474870405912, 'aug_jitter_std': 0.021150557310300775, 'aug_scale_low': 0.8566369524141854, 'aug_scale_high': 1.1649403483082161, 'aug_drift_max_amp': 0.12684123631179833, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 107208}
2025-10-03 00:13:49,663 - INFO - _models.training_function_executor - Model: 3,677 parameters, 7.9KB storage
2025-10-03 00:13:49,663 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7480854347585807, 0.6020568393284496, 0.5745906120340235, 0.5622443162072621, 0.552695850704904, 0.5462705862610627, 0.541980803419884, 0.5372707407635764, 0.5348384780692296, 0.5335724282876384, 0.529642060408858, 0.529067705311139, 0.5262703964092811, 0.5256854915022906, 0.5245660475814036, 0.524007493721133, 0.521782686816187, 0.5213523281063833, 0.5197834683436258, 0.520617642697246, 0.51797840159306, 0.5190058113595236, 0.5186071111564928, 0.5167179855342736, 0.5169726491993883, 0.5153966565794303, 0.515677522680154, 0.5147303569937874, 0.5143515124715846, 0.5146448162821037, 0.5126180827488813, 0.5134932753001487, 0.5136963592467219, 0.5119453638384068, 0.5120818345979824, 0.5116086412117684, 0.5123100182018129, 0.511288729929272, 0.5104751287686534, 0.5112845383856868], 'val_losses': [0.6165318795873205, 0.5750189096448775, 0.5551262056524331, 0.5435700438671163, 0.5393418699051111, 0.529309643553158, 0.527567135303365, 0.5360718479193537, 0.5193924546585302, 0.5201919144996201, 0.5182569142855314, 0.5151375130205402, 0.5172275022865312, 0.5137289937031536, 0.5164149550025796, 0.5293591384256772, 0.5206449499047798, 0.5154467835609753, 0.5165461675816947, 0.516944997114371, 0.5150058633954644, 0.522596309237605, 0.5244518027032561, 0.5109120082652087, 0.5289622101549546, 0.5125547630747165, 0.5135795178755141, 0.5100191013210474, 0.508340056715357, 0.5153766480204366, 0.5034586743086057, 0.5059593317577705, 0.5072018366800396, 0.5060952016471129, 0.5213636226364879, 0.5075876784345558, 0.5060368500889849, 0.508264147253984, 0.5016230000781619, 0.5108866751559947], 'val_acc': [0.9305851397068037, 0.9441172785365243, 0.9510086455331412, 0.9582759052750282, 0.9551434657311114, 0.9573988222027315, 0.9595288810925949, 0.9616589399824583, 0.962536023054755, 0.9600300714196216, 0.9595288810925949, 0.9586517980202982, 0.9647913795263752, 0.9665455456709685, 0.9635384037088084, 0.961408344818945, 0.9641648916175918, 0.9631625109635384, 0.9670467359979953, 0.9649166771081318, 0.9669214384162386, 0.9655431650169152, 0.967547926325022, 0.9644154867811051, 0.9664202480892119, 0.9664202480892119, 0.9661696529256986, 0.9647913795263752, 0.9649166771081318, 0.9657937601804285, 0.9665455456709685, 0.9631625109635384, 0.9661696529256986, 0.9672973311615086, 0.9680491166520486, 0.9680491166520486, 0.9680491166520486, 0.9688009021425886, 0.9667961408344818, 0.968299711815562], 'config': {'seq_len': 1000, 'patch_size': 50, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'dropout': 0.3207746594676903, 'num_classes': 5, 'epochs': 40, 'batch_size': 32, 'lr': 0.0019155240536609778, 'weight_decay': 0.004096421944227174, 'grad_clip_norm': 4.924923796410516, 'use_focal_loss': False, 'focal_gamma': 1.0027955614540474, 'label_smoothing': 0.10869595049577248, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.5910474870405912, 'aug_jitter_std': 0.021150557310300775, 'aug_scale_low': 0.8566369524141854, 'aug_scale_high': 1.1649403483082161, 'aug_drift_max_amp': 0.12684123631179833, 'normalize': True}, 'quantization': {'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 107208}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0019155240536609778, 'batch_size': 32, 'epochs': 40, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 5, 'patch_size': 50, 'dropout': 0.3207746594676903, 'weight_decay': 0.004096421944227174, 'grad_clip_norm': 4.924923796410516, 'use_focal_loss': False, 'focal_gamma': 1.0027955614540474, 'label_smoothing': 0.10869595049577248, 'compute_class_weights': False, 'aug_prob': 0.5910474870405912, 'aug_jitter_std': 0.021150557310300775, 'aug_scale_low': 0.8566369524141854, 'aug_scale_high': 1.1649403483082161, 'aug_drift_max_amp': 0.12684123631179833, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 107208}, 'model_parameter_count': 3677, 'model_storage_size_kb': 7.8998046875000005, 'model_size_validation': 'PASS'}
2025-10-03 00:13:49,664 - INFO - _models.training_function_executor - BO Objective: base=0.9683, size_penalty=0.0000, final=0.9683
2025-10-03 00:13:49,664 - INFO - _models.training_function_executor - Model: 3,677 parameters, 7.9KB (PASS 256KB limit)
2025-10-03 00:13:49,664 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 88.712s
2025-10-03 00:13:49,796 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9683
2025-10-03 00:13:49,796 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.133s
2025-10-03 00:13:49,796 - INFO - bo.run_bo - Recorded observation #37: hparams={'lr': 0.0019155240536609778, 'batch_size': np.int64(32), 'epochs': np.int64(40), 'head_dim': np.int64(8), 'n_heads': np.int64(6), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(50), 'dropout': 0.3207746594676903, 'weight_decay': 0.004096421944227174, 'grad_clip_norm': 4.924923796410516, 'use_focal_loss': np.False_, 'focal_gamma': 1.0027955614540474, 'label_smoothing': 0.10869595049577248, 'compute_class_weights': np.False_, 'aug_prob': 0.5910474870405912, 'aug_jitter_std': 0.021150557310300775, 'aug_scale_low': 0.8566369524141854, 'aug_scale_high': 1.1649403483082161, 'aug_drift_max_amp': 0.12684123631179833, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(107208)}, value=0.9683
2025-10-03 00:13:49,796 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 37: {'lr': 0.0019155240536609778, 'batch_size': np.int64(32), 'epochs': np.int64(40), 'head_dim': np.int64(8), 'n_heads': np.int64(6), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(50), 'dropout': 0.3207746594676903, 'weight_decay': 0.004096421944227174, 'grad_clip_norm': 4.924923796410516, 'use_focal_loss': np.False_, 'focal_gamma': 1.0027955614540474, 'label_smoothing': 0.10869595049577248, 'compute_class_weights': np.False_, 'aug_prob': 0.5910474870405912, 'aug_jitter_std': 0.021150557310300775, 'aug_scale_low': 0.8566369524141854, 'aug_scale_high': 1.1649403483082161, 'aug_drift_max_amp': 0.12684123631179833, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(107208)} -> 0.9683
2025-10-03 00:13:49,797 - INFO - bo.run_bo - üîçBO Trial 38: Using RF surrogate + Expected Improvement
2025-10-03 00:13:49,797 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:13:49,797 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:13:49,797 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:13:49,797 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006964895571723717, 'batch_size': 128, 'epochs': 6, 'head_dim': 9, 'n_heads': 8, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.18490444379832507, 'weight_decay': 0.0016219869701475595, 'grad_clip_norm': 2.7118009012876017, 'use_focal_loss': False, 'focal_gamma': 4.848319634115852, 'label_smoothing': 0.10980653243089937, 'compute_class_weights': False, 'aug_prob': 0.7031101794861123, 'aug_jitter_std': 0.04314516763660711, 'aug_scale_low': 0.9517118582456405, 'aug_scale_high': 1.0299784082339651, 'aug_drift_max_amp': 0.14705727852336223, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 434979}
2025-10-03 00:13:49,798 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006964895571723717, 'batch_size': 128, 'epochs': 6, 'head_dim': 9, 'n_heads': 8, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.18490444379832507, 'weight_decay': 0.0016219869701475595, 'grad_clip_norm': 2.7118009012876017, 'use_focal_loss': False, 'focal_gamma': 4.848319634115852, 'label_smoothing': 0.10980653243089937, 'compute_class_weights': False, 'aug_prob': 0.7031101794861123, 'aug_jitter_std': 0.04314516763660711, 'aug_scale_low': 0.9517118582456405, 'aug_scale_high': 1.0299784082339651, 'aug_drift_max_amp': 0.14705727852336223, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 434979}
2025-10-03 00:13:57,751 - INFO - _models.training_function_executor - Model: 13,557 parameters, 29.1KB storage
2025-10-03 00:13:57,752 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9057435549855609, 0.7060235488744501, 0.6209300840251619, 0.5872978460810012, 0.5677007179367353, 0.555523334318611], 'val_losses': [0.827570461088696, 0.6460739897092382, 0.5839282914206373, 0.5655182527578679, 0.5645627260148384, 0.5443807426901214], 'val_acc': [0.8277158250845759, 0.9205613331662699, 0.9340934719959905, 0.9498809672973312, 0.9507580503696279, 0.9557699536398947], 'config': {'seq_len': 1000, 'patch_size': 20, 'head_dim': 9, 'n_heads': 8, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 5, 'dropout': 0.18490444379832507, 'num_classes': 5, 'epochs': 6, 'batch_size': 128, 'lr': 0.006964895571723717, 'weight_decay': 0.0016219869701475595, 'grad_clip_norm': 2.7118009012876017, 'use_focal_loss': False, 'focal_gamma': 4.848319634115852, 'label_smoothing': 0.10980653243089937, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.7031101794861123, 'aug_jitter_std': 0.04314516763660711, 'aug_scale_low': 0.9517118582456405, 'aug_scale_high': 1.0299784082339651, 'aug_drift_max_amp': 0.14705727852336223, 'normalize': False}, 'quantization': {'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 434979}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006964895571723717, 'batch_size': 128, 'epochs': 6, 'head_dim': 9, 'n_heads': 8, 'n_layers': 1, 'd_ff_factor': 2, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.18490444379832507, 'weight_decay': 0.0016219869701475595, 'grad_clip_norm': 2.7118009012876017, 'use_focal_loss': False, 'focal_gamma': 4.848319634115852, 'label_smoothing': 0.10980653243089937, 'compute_class_weights': False, 'aug_prob': 0.7031101794861123, 'aug_jitter_std': 0.04314516763660711, 'aug_scale_low': 0.9517118582456405, 'aug_scale_high': 1.0299784082339651, 'aug_drift_max_amp': 0.14705727852336223, 'normalize': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 434979}, 'model_parameter_count': 13557, 'model_storage_size_kb': 29.1263671875, 'model_size_validation': 'PASS'}
2025-10-03 00:13:57,752 - INFO - _models.training_function_executor - BO Objective: base=0.9558, size_penalty=0.0000, final=0.9558
2025-10-03 00:13:57,752 - INFO - _models.training_function_executor - Model: 13,557 parameters, 29.1KB (PASS 256KB limit)
2025-10-03 00:13:57,752 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 7.955s
2025-10-03 00:13:57,886 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9558
2025-10-03 00:13:57,886 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.134s
2025-10-03 00:13:57,886 - INFO - bo.run_bo - Recorded observation #38: hparams={'lr': 0.006964895571723717, 'batch_size': np.int64(128), 'epochs': np.int64(6), 'head_dim': np.int64(9), 'n_heads': np.int64(8), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(20), 'dropout': 0.18490444379832507, 'weight_decay': 0.0016219869701475595, 'grad_clip_norm': 2.7118009012876017, 'use_focal_loss': np.False_, 'focal_gamma': 4.848319634115852, 'label_smoothing': 0.10980653243089937, 'compute_class_weights': np.False_, 'aug_prob': 0.7031101794861123, 'aug_jitter_std': 0.04314516763660711, 'aug_scale_low': 0.9517118582456405, 'aug_scale_high': 1.0299784082339651, 'aug_drift_max_amp': 0.14705727852336223, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(434979)}, value=0.9558
2025-10-03 00:13:57,886 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 38: {'lr': 0.006964895571723717, 'batch_size': np.int64(128), 'epochs': np.int64(6), 'head_dim': np.int64(9), 'n_heads': np.int64(8), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(20), 'dropout': 0.18490444379832507, 'weight_decay': 0.0016219869701475595, 'grad_clip_norm': 2.7118009012876017, 'use_focal_loss': np.False_, 'focal_gamma': 4.848319634115852, 'label_smoothing': 0.10980653243089937, 'compute_class_weights': np.False_, 'aug_prob': 0.7031101794861123, 'aug_jitter_std': 0.04314516763660711, 'aug_scale_low': 0.9517118582456405, 'aug_scale_high': 1.0299784082339651, 'aug_drift_max_amp': 0.14705727852336223, 'normalize': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(434979)} -> 0.9558
2025-10-03 00:13:57,887 - INFO - bo.run_bo - üîçBO Trial 39: Using RF surrogate + Expected Improvement
2025-10-03 00:13:57,887 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:13:57,887 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:13:57,887 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:13:57,887 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0026353278981139866, 'batch_size': 128, 'epochs': 29, 'head_dim': 15, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 5, 'patch_size': 8, 'dropout': 0.42184755493835036, 'weight_decay': 0.004084364262997973, 'grad_clip_norm': 4.2901494152884165, 'use_focal_loss': False, 'focal_gamma': 3.681020924923425, 'label_smoothing': 0.1737213462847832, 'compute_class_weights': False, 'aug_prob': 0.6896320790069125, 'aug_jitter_std': 0.0326431020266987, 'aug_scale_low': 0.8121305904232855, 'aug_scale_high': 1.0822341067270116, 'aug_drift_max_amp': 0.08060068215679939, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 794004}
2025-10-03 00:13:57,888 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0026353278981139866, 'batch_size': 128, 'epochs': 29, 'head_dim': 15, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 5, 'patch_size': 8, 'dropout': 0.42184755493835036, 'weight_decay': 0.004084364262997973, 'grad_clip_norm': 4.2901494152884165, 'use_focal_loss': False, 'focal_gamma': 3.681020924923425, 'label_smoothing': 0.1737213462847832, 'compute_class_weights': False, 'aug_prob': 0.6896320790069125, 'aug_jitter_std': 0.0326431020266987, 'aug_scale_low': 0.8121305904232855, 'aug_scale_high': 1.0822341067270116, 'aug_drift_max_amp': 0.08060068215679939, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 794004}
2025-10-03 00:14:53,561 - INFO - _models.training_function_executor - Model: 13,685 parameters, 29.4KB storage
2025-10-03 00:14:53,561 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8931198597581704, 0.7508372904341483, 0.7196482537746713, 0.7086534899652249, 0.7008819824579839, 0.6951266529677869, 0.6921233461919861, 0.6886786399574591, 0.6891283786834954, 0.684232933477281, 0.6845121714934608, 0.6819820865385179, 0.6803658572955228, 0.6794353121149955, 0.6786577710474851, 0.6773474498249641, 0.6764397067342998, 0.6763482945300626, 0.6753875691557694, 0.6745613268793471, 0.6738608796147741, 0.6733169328582745, 0.6724458508681516, 0.6715406787424231, 0.6718430597803868, 0.6705653578760898, 0.669968200959243, 0.6695777925282005, 0.6687680001796287], 'val_losses': [0.8185593788075037, 0.7312141649706025, 0.7054652512603767, 0.7615208737967591, 0.696823327953005, 0.6809144482966907, 0.7163486945540993, 0.6806147579799662, 0.6787469702665556, 0.6916805580510426, 0.7166493125180525, 0.6800704987824912, 0.6763177453657542, 0.6685345916577172, 0.6700450881144977, 0.6709632510798959, 0.6725632921524057, 0.6656884856651846, 0.6765324742016435, 0.6690404745528997, 0.6679934055012668, 0.676656459465524, 0.6689286057908139, 0.673615982756252, 0.6611418004861049, 0.6781990724224716, 0.6654373743662244, 0.6669211606530674, 0.6637610027984544], 'val_acc': [0.8924946748527753, 0.9497556697155745, 0.9572735246209748, 0.945746147099361, 0.960656559328405, 0.9610324520736749, 0.9612830472371883, 0.9615336424007017, 0.9645407843628618, 0.9605312617466483, 0.9610324520736749, 0.9652925698534018, 0.9679238190702919, 0.9654178674351584, 0.9652925698534018, 0.9669214384162386, 0.9652925698534018, 0.9691767948878587, 0.9644154867811051, 0.9708056634506954, 0.9662949505074552, 0.970930961032452, 0.9705550682871821, 0.9710562586142087, 0.969051497306102, 0.9730610199223155, 0.9706803658689387, 0.9739381029946123, 0.9719333416865055], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 15, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 5, 'dropout': 0.42184755493835036, 'num_classes': 5, 'epochs': 29, 'batch_size': 128, 'lr': 0.0026353278981139866, 'weight_decay': 0.004084364262997973, 'grad_clip_norm': 4.2901494152884165, 'use_focal_loss': False, 'focal_gamma': 3.681020924923425, 'label_smoothing': 0.1737213462847832, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.6896320790069125, 'aug_jitter_std': 0.0326431020266987, 'aug_scale_low': 0.8121305904232855, 'aug_scale_high': 1.0822341067270116, 'aug_drift_max_amp': 0.08060068215679939, 'normalize': True}, 'quantization': {'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 794004}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0026353278981139866, 'batch_size': 128, 'epochs': 29, 'head_dim': 15, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 5, 'patch_size': 8, 'dropout': 0.42184755493835036, 'weight_decay': 0.004084364262997973, 'grad_clip_norm': 4.2901494152884165, 'use_focal_loss': False, 'focal_gamma': 3.681020924923425, 'label_smoothing': 0.1737213462847832, 'compute_class_weights': False, 'aug_prob': 0.6896320790069125, 'aug_jitter_std': 0.0326431020266987, 'aug_scale_low': 0.8121305904232855, 'aug_scale_high': 1.0822341067270116, 'aug_drift_max_amp': 0.08060068215679939, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'seed': 794004}, 'model_parameter_count': 13685, 'model_storage_size_kb': 29.401367187500004, 'model_size_validation': 'PASS'}
2025-10-03 00:14:53,561 - INFO - _models.training_function_executor - BO Objective: base=0.9719, size_penalty=0.0000, final=0.9719
2025-10-03 00:14:53,561 - INFO - _models.training_function_executor - Model: 13,685 parameters, 29.4KB (PASS 256KB limit)
2025-10-03 00:14:53,561 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 55.675s
2025-10-03 00:14:53,697 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9719
2025-10-03 00:14:53,697 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.135s
2025-10-03 00:14:53,697 - INFO - bo.run_bo - Recorded observation #39: hparams={'lr': 0.0026353278981139866, 'batch_size': np.int64(128), 'epochs': np.int64(29), 'head_dim': np.int64(15), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(8), 'dropout': 0.42184755493835036, 'weight_decay': 0.004084364262997973, 'grad_clip_norm': 4.2901494152884165, 'use_focal_loss': np.False_, 'focal_gamma': 3.681020924923425, 'label_smoothing': 0.1737213462847832, 'compute_class_weights': np.False_, 'aug_prob': 0.6896320790069125, 'aug_jitter_std': 0.0326431020266987, 'aug_scale_low': 0.8121305904232855, 'aug_scale_high': 1.0822341067270116, 'aug_drift_max_amp': 0.08060068215679939, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(794004)}, value=0.9719
2025-10-03 00:14:53,697 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 39: {'lr': 0.0026353278981139866, 'batch_size': np.int64(128), 'epochs': np.int64(29), 'head_dim': np.int64(15), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(8), 'dropout': 0.42184755493835036, 'weight_decay': 0.004084364262997973, 'grad_clip_norm': 4.2901494152884165, 'use_focal_loss': np.False_, 'focal_gamma': 3.681020924923425, 'label_smoothing': 0.1737213462847832, 'compute_class_weights': np.False_, 'aug_prob': 0.6896320790069125, 'aug_jitter_std': 0.0326431020266987, 'aug_scale_low': 0.8121305904232855, 'aug_scale_high': 1.0822341067270116, 'aug_drift_max_amp': 0.08060068215679939, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(794004)} -> 0.9719
2025-10-03 00:14:53,697 - INFO - bo.run_bo - üîçBO Trial 40: Using RF surrogate + Expected Improvement
2025-10-03 00:14:53,697 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:14:53,697 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:14:53,698 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:14:53,698 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00734993160033751, 'batch_size': 32, 'epochs': 50, 'head_dim': 20, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.3962968296549265, 'weight_decay': 0.004721869063977617, 'grad_clip_norm': 4.331087186534746, 'use_focal_loss': True, 'focal_gamma': 1.335789589702404, 'label_smoothing': 0.14245437515032514, 'compute_class_weights': False, 'aug_prob': 0.976292946860036, 'aug_jitter_std': 0.030338378498503227, 'aug_scale_low': 0.8073072391031795, 'aug_scale_high': 1.136058403381243, 'aug_drift_max_amp': 0.14943870104872636, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 671131}
2025-10-03 00:14:53,699 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00734993160033751, 'batch_size': 32, 'epochs': 50, 'head_dim': 20, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.3962968296549265, 'weight_decay': 0.004721869063977617, 'grad_clip_norm': 4.331087186534746, 'use_focal_loss': True, 'focal_gamma': 1.335789589702404, 'label_smoothing': 0.14245437515032514, 'compute_class_weights': False, 'aug_prob': 0.976292946860036, 'aug_jitter_std': 0.030338378498503227, 'aug_scale_low': 0.8073072391031795, 'aug_scale_high': 1.136058403381243, 'aug_drift_max_amp': 0.14943870104872636, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 671131}
2025-10-03 00:16:37,479 - INFO - _models.training_function_executor - Model: 18,677 parameters, 80.3KB storage
2025-10-03 00:16:37,480 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5080260018724227, 0.32890696850903844, 0.28700323360684304, 0.26983096785415095, 0.2629031418803655, 0.2530852704834147, 0.24874688241775145, 0.24440992519217908, 0.24049382813115452, 0.23789716217633838, 0.2357980876188247, 0.2328084595885353, 0.23130549918166438, 0.2300583633651367, 0.2286011318679412, 0.22850918636942882, 0.2265029695809599, 0.22536636717204175, 0.22670076880688633, 0.2240418960284803, 0.22280360149897696, 0.22376347171101912, 0.22199032835295238, 0.22198902544775348, 0.22087479617709344, 0.22052691692648438, 0.22049995579197268, 0.22059930458954768, 0.21883845349169687, 0.22003437012841975, 0.21970792648922818, 0.21902380320658246, 0.21809653250009678, 0.21788438436430327, 0.21753960070845157, 0.21781473751177977, 0.21728059467888303, 0.21689832964304362, 0.21668850843865625, 0.21734799882519296, 0.21726256071413996, 0.2166901686865185, 0.21608146022641972, 0.21629007355919555, 0.2156275658259187, 0.21648754993036376, 0.21584699373466235, 0.2152735466093242, 0.21405552751006435, 0.2145829971181761], 'val_losses': [0.417946532086731, 0.27958115318069154, 0.25693299680436676, 0.24837257313617803, 0.23878293633162265, 0.236795200319939, 0.23037440156058372, 0.2564011955788494, 0.23012527734948615, 0.22181617110583018, 0.22383841023440948, 0.22461722439623852, 0.22009129340734387, 0.2212936478611999, 0.22376448266668406, 0.22083369117006593, 0.21665592840583892, 0.22324356796702352, 0.22492999945602804, 0.21429261430778715, 0.21498874261883802, 0.21370378341266735, 0.21264445780587515, 0.2114634943806039, 0.21414198700369877, 0.21545246101578827, 0.22124962636946796, 0.2134960569724181, 0.21091768785222642, 0.21056758295084177, 0.2130584731434778, 0.21702042264965837, 0.2104693777712645, 0.21566873600632008, 0.21332260625372254, 0.21361416805703126, 0.21425833593871477, 0.21266014724428411, 0.21060149569347708, 0.211751473840952, 0.21100261673758822, 0.21143337973316367, 0.21113886640956417, 0.21023973721771397, 0.20972322189498704, 0.21030278231861327, 0.21813101285517536, 0.2072251380206857, 0.20915490687976368, 0.2073995060280531], 'val_acc': [0.8923693772710186, 0.9473750156621977, 0.9556446560581381, 0.9573988222027315, 0.9604059641648917, 0.9627866182182684, 0.9642901891993485, 0.9511339431148979, 0.9680491166520486, 0.9681744142338052, 0.969051497306102, 0.9677985214885353, 0.9708056634506954, 0.9680491166520486, 0.9671720335797519, 0.967547926325022, 0.9700538779601554, 0.9666708432527252, 0.9689261997243453, 0.9720586392682621, 0.9708056634506954, 0.9705550682871821, 0.970930961032452, 0.9724345320135321, 0.9725598295952889, 0.9681744142338052, 0.9665455456709685, 0.9748151860669089, 0.9731863175040721, 0.9726851271770455, 0.9738128054128555, 0.9721839368500188, 0.9736875078310988, 0.9720586392682621, 0.9716827465229921, 0.9733116150858289, 0.9723092344317754, 0.9738128054128555, 0.9739381029946123, 0.9723092344317754, 0.9739381029946123, 0.9739381029946123, 0.9748151860669089, 0.976318757047989, 0.9743139957398822, 0.9729357223405588, 0.9749404836486656, 0.9738128054128555, 0.9741886981581256, 0.9749404836486656], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 20, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'dropout': 0.3962968296549265, 'num_classes': 5, 'epochs': 50, 'batch_size': 32, 'lr': 0.00734993160033751, 'weight_decay': 0.004721869063977617, 'grad_clip_norm': 4.331087186534746, 'use_focal_loss': True, 'focal_gamma': 1.335789589702404, 'label_smoothing': 0.14245437515032514, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.976292946860036, 'aug_jitter_std': 0.030338378498503227, 'aug_scale_low': 0.8073072391031795, 'aug_scale_high': 1.136058403381243, 'aug_drift_max_amp': 0.14943870104872636, 'normalize': False}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 671131}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00734993160033751, 'batch_size': 32, 'epochs': 50, 'head_dim': 20, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 16, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.3962968296549265, 'weight_decay': 0.004721869063977617, 'grad_clip_norm': 4.331087186534746, 'use_focal_loss': True, 'focal_gamma': 1.335789589702404, 'label_smoothing': 0.14245437515032514, 'compute_class_weights': False, 'aug_prob': 0.976292946860036, 'aug_jitter_std': 0.030338378498503227, 'aug_scale_low': 0.8073072391031795, 'aug_scale_high': 1.136058403381243, 'aug_drift_max_amp': 0.14943870104872636, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 671131}, 'model_parameter_count': 18677, 'model_storage_size_kb': 80.252734375, 'model_size_validation': 'PASS'}
2025-10-03 00:16:37,480 - INFO - _models.training_function_executor - BO Objective: base=0.9749, size_penalty=0.0000, final=0.9749
2025-10-03 00:16:37,480 - INFO - _models.training_function_executor - Model: 18,677 parameters, 80.3KB (PASS 256KB limit)
2025-10-03 00:16:37,480 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 103.782s
2025-10-03 00:16:37,616 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9749
2025-10-03 00:16:37,616 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.136s
2025-10-03 00:16:37,616 - INFO - bo.run_bo - Recorded observation #40: hparams={'lr': 0.00734993160033751, 'batch_size': np.int64(32), 'epochs': np.int64(50), 'head_dim': np.int64(20), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.3962968296549265, 'weight_decay': 0.004721869063977617, 'grad_clip_norm': 4.331087186534746, 'use_focal_loss': np.True_, 'focal_gamma': 1.335789589702404, 'label_smoothing': 0.14245437515032514, 'compute_class_weights': np.False_, 'aug_prob': 0.976292946860036, 'aug_jitter_std': 0.030338378498503227, 'aug_scale_low': 0.8073072391031795, 'aug_scale_high': 1.136058403381243, 'aug_drift_max_amp': 0.14943870104872636, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(671131)}, value=0.9749
2025-10-03 00:16:37,616 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 40: {'lr': 0.00734993160033751, 'batch_size': np.int64(32), 'epochs': np.int64(50), 'head_dim': np.int64(20), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.3962968296549265, 'weight_decay': 0.004721869063977617, 'grad_clip_norm': 4.331087186534746, 'use_focal_loss': np.True_, 'focal_gamma': 1.335789589702404, 'label_smoothing': 0.14245437515032514, 'compute_class_weights': np.False_, 'aug_prob': 0.976292946860036, 'aug_jitter_std': 0.030338378498503227, 'aug_scale_low': 0.8073072391031795, 'aug_scale_high': 1.136058403381243, 'aug_drift_max_amp': 0.14943870104872636, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(671131)} -> 0.9749
2025-10-03 00:16:37,617 - INFO - bo.run_bo - üîçBO Trial 41: Using RF surrogate + Expected Improvement
2025-10-03 00:16:37,617 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:16:37,617 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:16:37,617 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:16:37,617 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0031832639036643014, 'batch_size': 128, 'epochs': 50, 'head_dim': 16, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 100, 'dropout': 0.46080224401545555, 'weight_decay': 0.0002818784557581532, 'grad_clip_norm': 4.946468134503963, 'use_focal_loss': False, 'focal_gamma': 2.2183086574741493, 'label_smoothing': 0.08617980204331693, 'compute_class_weights': False, 'aug_prob': 0.18149460312594648, 'aug_jitter_std': 0.044010818401682195, 'aug_scale_low': 0.8069641746272354, 'aug_scale_high': 1.1733048803353368, 'aug_drift_max_amp': 0.18040003960924256, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 169087}
2025-10-03 00:16:37,618 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0031832639036643014, 'batch_size': 128, 'epochs': 50, 'head_dim': 16, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 100, 'dropout': 0.46080224401545555, 'weight_decay': 0.0002818784557581532, 'grad_clip_norm': 4.946468134503963, 'use_focal_loss': False, 'focal_gamma': 2.2183086574741493, 'label_smoothing': 0.08617980204331693, 'compute_class_weights': False, 'aug_prob': 0.18149460312594648, 'aug_jitter_std': 0.044010818401682195, 'aug_scale_low': 0.8069641746272354, 'aug_scale_high': 1.1733048803353368, 'aug_drift_max_amp': 0.18040003960924256, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 169087}
2025-10-03 00:17:47,025 - INFO - _models.training_function_executor - Model: 7,769 parameters, 16.7KB storage
2025-10-03 00:17:47,025 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7414978711418283, 0.5468951069061063, 0.5099879988239204, 0.4979538017581012, 0.4868818324535251, 0.47981683112205026, 0.4747071154220725, 0.47073247331845947, 0.4671842212295807, 0.4639118757309316, 0.4632565518412581, 0.4613505608819352, 0.45752282066908045, 0.4572863222509123, 0.45563054477343584, 0.45389939256319817, 0.453671676796921, 0.4520269088040758, 0.4512918463544365, 0.4500141459727905, 0.4499241046221697, 0.45006030665204866, 0.4474086131278209, 0.4475909931406589, 0.44754657486239924, 0.44515232141407846, 0.4461243959296688, 0.4449204154046777, 0.4451021281873672, 0.4441346453990389, 0.44402548639307465, 0.44350985254302, 0.44201550067182477, 0.44174293281552307, 0.4424507574884903, 0.4410285559624571, 0.43938205674250047, 0.44151518189010475, 0.44064692584034254, 0.4394295788621991, 0.43908991868329467, 0.43821296354913475, 0.43809260511272846, 0.43767438731247166, 0.4381648060694653, 0.4368085623295337, 0.43723688360440294, 0.43625053307341144, 0.43790776589156294, 0.43564608996812854], 'val_losses': [0.5658408107240462, 0.5060610572970043, 0.5147226818513637, 0.4907668334449567, 0.4941083220159361, 0.4649270624015702, 0.4577565217597669, 0.45973688785242056, 0.4604623832136239, 0.45195516345345904, 0.45194178461177115, 0.44807482467022114, 0.45159903484214714, 0.44492894730357624, 0.4516717939168614, 0.4482115523406787, 0.4418810391139303, 0.43747830877117666, 0.4373372356637112, 0.44080099846587534, 0.44295567109790634, 0.45893834370925096, 0.4399342048236891, 0.44810358746251044, 0.4417355001801731, 0.43913215436130043, 0.4411312036086497, 0.4406054874602274, 0.4379338410217023, 0.43616899946939525, 0.4401974981626671, 0.4391696730691857, 0.4502429063047417, 0.44349542990571467, 0.4382335856704392, 0.4352721169753386, 0.44200038823279625, 0.4313802541743662, 0.43790068273227056, 0.4285946083373555, 0.4306485228349714, 0.4315649522618981, 0.43313724735359427, 0.43813614354009933, 0.43350044021688894, 0.4361546924875697, 0.4318335043086009, 0.43442527738269926, 0.43424811456275453, 0.4409052613436944], 'val_acc': [0.9225660944743767, 0.9483773963162511, 0.945746147099361, 0.9506327527878712, 0.9523869189324646, 0.9595288810925949, 0.9572735246209748, 0.9631625109635384, 0.9602806665831349, 0.9634131061270518, 0.9666708432527252, 0.9635384037088084, 0.9635384037088084, 0.9646660819446184, 0.9605312617466483, 0.9624107254729983, 0.9639142964540784, 0.9659190577621852, 0.9674226287432652, 0.9650419746898885, 0.9661696529256986, 0.961408344818945, 0.9685503069790753, 0.9649166771081318, 0.9705550682871821, 0.9640395940358351, 0.9634131061270518, 0.9664202480892119, 0.9656684625986719, 0.9645407843628618, 0.9679238190702919, 0.9654178674351584, 0.9604059641648917, 0.9680491166520486, 0.9640395940358351, 0.9706803658689387, 0.9714321513594788, 0.9666708432527252, 0.9696779852148854, 0.9689261997243453, 0.9666708432527252, 0.9703044731236687, 0.9644154867811051, 0.968675604560832, 0.9713068537777221, 0.9646660819446184, 0.9671720335797519, 0.9660443553439418, 0.9671720335797519, 0.9672973311615086], 'config': {'seq_len': 1000, 'patch_size': 100, 'head_dim': 16, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 5, 'dropout': 0.46080224401545555, 'num_classes': 5, 'epochs': 50, 'batch_size': 128, 'lr': 0.0031832639036643014, 'weight_decay': 0.0002818784557581532, 'grad_clip_norm': 4.946468134503963, 'use_focal_loss': False, 'focal_gamma': 2.2183086574741493, 'label_smoothing': 0.08617980204331693, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.18149460312594648, 'aug_jitter_std': 0.044010818401682195, 'aug_scale_low': 0.8069641746272354, 'aug_scale_high': 1.1733048803353368, 'aug_drift_max_amp': 0.18040003960924256, 'normalize': True}, 'quantization': {'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'seed': 169087}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0031832639036643014, 'batch_size': 128, 'epochs': 50, 'head_dim': 16, 'n_heads': 2, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 100, 'dropout': 0.46080224401545555, 'weight_decay': 0.0002818784557581532, 'grad_clip_norm': 4.946468134503963, 'use_focal_loss': False, 'focal_gamma': 2.2183086574741493, 'label_smoothing': 0.08617980204331693, 'compute_class_weights': False, 'aug_prob': 0.18149460312594648, 'aug_jitter_std': 0.044010818401682195, 'aug_scale_low': 0.8069641746272354, 'aug_scale_high': 1.1733048803353368, 'aug_drift_max_amp': 0.18040003960924256, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 169087}, 'model_parameter_count': 7769, 'model_storage_size_kb': 16.691210937500003, 'model_size_validation': 'PASS'}
2025-10-03 00:17:47,025 - INFO - _models.training_function_executor - BO Objective: base=0.9673, size_penalty=0.0000, final=0.9673
2025-10-03 00:17:47,025 - INFO - _models.training_function_executor - Model: 7,769 parameters, 16.7KB (PASS 256KB limit)
2025-10-03 00:17:47,025 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 69.409s
2025-10-03 00:17:47,162 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9673
2025-10-03 00:17:47,162 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.137s
2025-10-03 00:17:47,163 - INFO - bo.run_bo - Recorded observation #41: hparams={'lr': 0.0031832639036643014, 'batch_size': np.int64(128), 'epochs': np.int64(50), 'head_dim': np.int64(16), 'n_heads': np.int64(2), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(100), 'dropout': 0.46080224401545555, 'weight_decay': 0.0002818784557581532, 'grad_clip_norm': 4.946468134503963, 'use_focal_loss': np.False_, 'focal_gamma': 2.2183086574741493, 'label_smoothing': 0.08617980204331693, 'compute_class_weights': np.False_, 'aug_prob': 0.18149460312594648, 'aug_jitter_std': 0.044010818401682195, 'aug_scale_low': 0.8069641746272354, 'aug_scale_high': 1.1733048803353368, 'aug_drift_max_amp': 0.18040003960924256, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(169087)}, value=0.9673
2025-10-03 00:17:47,163 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 41: {'lr': 0.0031832639036643014, 'batch_size': np.int64(128), 'epochs': np.int64(50), 'head_dim': np.int64(16), 'n_heads': np.int64(2), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(100), 'dropout': 0.46080224401545555, 'weight_decay': 0.0002818784557581532, 'grad_clip_norm': 4.946468134503963, 'use_focal_loss': np.False_, 'focal_gamma': 2.2183086574741493, 'label_smoothing': 0.08617980204331693, 'compute_class_weights': np.False_, 'aug_prob': 0.18149460312594648, 'aug_jitter_std': 0.044010818401682195, 'aug_scale_low': 0.8069641746272354, 'aug_scale_high': 1.1733048803353368, 'aug_drift_max_amp': 0.18040003960924256, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(169087)} -> 0.9673
2025-10-03 00:17:47,163 - INFO - bo.run_bo - üîçBO Trial 42: Using RF surrogate + Expected Improvement
2025-10-03 00:17:47,163 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:17:47,163 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:17:47,163 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:17:47,163 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008709149419452727, 'batch_size': 128, 'epochs': 48, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 8, 'dropout': 0.46973035167160526, 'weight_decay': 0.009131490678934056, 'grad_clip_norm': 3.9939133974354557, 'use_focal_loss': False, 'focal_gamma': 4.460799121840654, 'label_smoothing': 0.1470203170133538, 'compute_class_weights': False, 'aug_prob': 0.7875530382697488, 'aug_jitter_std': 0.008430538251383131, 'aug_scale_low': 0.8071845932014201, 'aug_scale_high': 1.1786783246243595, 'aug_drift_max_amp': 0.19306120657154827, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 806840}
2025-10-03 00:17:47,165 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008709149419452727, 'batch_size': 128, 'epochs': 48, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 8, 'dropout': 0.46973035167160526, 'weight_decay': 0.009131490678934056, 'grad_clip_norm': 3.9939133974354557, 'use_focal_loss': False, 'focal_gamma': 4.460799121840654, 'label_smoothing': 0.1470203170133538, 'compute_class_weights': False, 'aug_prob': 0.7875530382697488, 'aug_jitter_std': 0.008430538251383131, 'aug_scale_low': 0.8071845932014201, 'aug_scale_high': 1.1786783246243595, 'aug_drift_max_amp': 0.19306120657154827, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 806840}
2025-10-03 00:19:14,665 - INFO - _models.training_function_executor - Model: 4,937 parameters, 21.2KB storage
2025-10-03 00:19:14,665 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0659524573795225, 0.8676991625596634, 0.7867055484511644, 0.743969066842167, 0.7222985364129115, 0.7075309108023065, 0.6959602109726347, 0.6864111195720273, 0.6806262754536592, 0.673387939640257, 0.6681574108352436, 0.6641637331295315, 0.658961714391395, 0.6552442365005919, 0.6524677274128035, 0.64968143953729, 0.6475422351733674, 0.6462942221012121, 0.6436853509713611, 0.6433612726664023, 0.6419223253374369, 0.6402344666555614, 0.6394268522711284, 0.6393450410508036, 0.6373250249263185, 0.6370225559536259, 0.6361873582413003, 0.634648504683059, 0.6347461542184575, 0.6326571192481638, 0.6331582264214146, 0.6323789635184968, 0.6314460409859034, 0.6302487219563366, 0.6301778434915426, 0.630107403374725, 0.6293189467816328, 0.62902631295249, 0.6282481774536062, 0.6269996230197394, 0.6274446655894881, 0.6267836841990079, 0.6258479229202882, 0.6253203370160889, 0.6251524547375492, 0.6252891319951135, 0.6248213719392568, 0.6231990688871748], 'val_losses': [0.9180328907815455, 0.8436690525981122, 0.7581228051115705, 0.7131020001802897, 0.7164954930495355, 0.6792114677621583, 0.6751257567429002, 0.6647081808583238, 0.6579657082583966, 0.6653708258512817, 0.6604763036711654, 0.6594746503565876, 0.6432745475814451, 0.6410378000369094, 0.6414790420122365, 0.6340379490468185, 0.6447041635898073, 0.642216594770967, 0.6299658429053564, 0.6300892310324147, 0.6366214772660457, 0.6281091108191896, 0.630818425331957, 0.6280101201331432, 0.6481041121252825, 0.6219043853124301, 0.6269540781664708, 0.6423628954133986, 0.6208467878370473, 0.621123401955276, 0.6211514401116208, 0.6173438496001755, 0.6151738557103373, 0.6216164312182236, 0.6262574209006055, 0.6187531681993435, 0.6176805369926566, 0.6294042173893943, 0.6278290093475349, 0.612909552858611, 0.6119345939408836, 0.6149439812528058, 0.6131616058676423, 0.6148027619866258, 0.6330137538918874, 0.6134818345438999, 0.6155517097479, 0.6211939103814567], 'val_acc': [0.8071670216764817, 0.8775842626237313, 0.9115399072797895, 0.9243202606189701, 0.9298333542162637, 0.9383535897757174, 0.9424884099736875, 0.9475003132439543, 0.9512592406966546, 0.9525122165142212, 0.9497556697155745, 0.9521363237689513, 0.953765192331788, 0.9550181681493547, 0.9561458463851648, 0.955268763312868, 0.9575241197844881, 0.9579000125297582, 0.9586517980202982, 0.9594035835108382, 0.9597794762561083, 0.9579000125297582, 0.9567723342939481, 0.9605312617466483, 0.9587770956020549, 0.9627866182182684, 0.9611577496554317, 0.9594035835108382, 0.9616589399824583, 0.9626613206365117, 0.9612830472371883, 0.9626613206365117, 0.9607818569101616, 0.9639142964540784, 0.9616589399824583, 0.9630372133817817, 0.9649166771081318, 0.9629119158000251, 0.9637889988723217, 0.9616589399824583, 0.9637889988723217, 0.9635384037088084, 0.9627866182182684, 0.9656684625986719, 0.9652925698534018, 0.9627866182182684, 0.9630372133817817, 0.963287808545295], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'dropout': 0.46973035167160526, 'num_classes': 5, 'epochs': 48, 'batch_size': 128, 'lr': 0.0008709149419452727, 'weight_decay': 0.009131490678934056, 'grad_clip_norm': 3.9939133974354557, 'use_focal_loss': False, 'focal_gamma': 4.460799121840654, 'label_smoothing': 0.1470203170133538, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.7875530382697488, 'aug_jitter_std': 0.008430538251383131, 'aug_scale_low': 0.8071845932014201, 'aug_scale_high': 1.1786783246243595, 'aug_drift_max_amp': 0.19306120657154827, 'normalize': True}, 'quantization': {'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 806840}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008709149419452727, 'batch_size': 128, 'epochs': 48, 'head_dim': 8, 'n_heads': 6, 'n_layers': 2, 'd_ff_factor': 4, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 3, 'patch_size': 8, 'dropout': 0.46973035167160526, 'weight_decay': 0.009131490678934056, 'grad_clip_norm': 3.9939133974354557, 'use_focal_loss': False, 'focal_gamma': 4.460799121840654, 'label_smoothing': 0.1470203170133538, 'compute_class_weights': False, 'aug_prob': 0.7875530382697488, 'aug_jitter_std': 0.008430538251383131, 'aug_scale_low': 0.8071845932014201, 'aug_scale_high': 1.1786783246243595, 'aug_drift_max_amp': 0.19306120657154827, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'seed': 806840}, 'model_parameter_count': 4937, 'model_storage_size_kb': 21.213671875000003, 'model_size_validation': 'PASS'}
2025-10-03 00:19:14,665 - INFO - _models.training_function_executor - BO Objective: base=0.9633, size_penalty=0.0000, final=0.9633
2025-10-03 00:19:14,665 - INFO - _models.training_function_executor - Model: 4,937 parameters, 21.2KB (PASS 256KB limit)
2025-10-03 00:19:14,665 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 87.502s
2025-10-03 00:19:14,801 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9633
2025-10-03 00:19:14,801 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.136s
2025-10-03 00:19:14,802 - INFO - bo.run_bo - Recorded observation #42: hparams={'lr': 0.0008709149419452727, 'batch_size': np.int64(128), 'epochs': np.int64(48), 'head_dim': np.int64(8), 'n_heads': np.int64(6), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(8), 'dropout': 0.46973035167160526, 'weight_decay': 0.009131490678934056, 'grad_clip_norm': 3.9939133974354557, 'use_focal_loss': np.False_, 'focal_gamma': 4.460799121840654, 'label_smoothing': 0.1470203170133538, 'compute_class_weights': np.False_, 'aug_prob': 0.7875530382697488, 'aug_jitter_std': 0.008430538251383131, 'aug_scale_low': 0.8071845932014201, 'aug_scale_high': 1.1786783246243595, 'aug_drift_max_amp': 0.19306120657154827, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(806840)}, value=0.9633
2025-10-03 00:19:14,802 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 42: {'lr': 0.0008709149419452727, 'batch_size': np.int64(128), 'epochs': np.int64(48), 'head_dim': np.int64(8), 'n_heads': np.int64(6), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(8), 'dropout': 0.46973035167160526, 'weight_decay': 0.009131490678934056, 'grad_clip_norm': 3.9939133974354557, 'use_focal_loss': np.False_, 'focal_gamma': 4.460799121840654, 'label_smoothing': 0.1470203170133538, 'compute_class_weights': np.False_, 'aug_prob': 0.7875530382697488, 'aug_jitter_std': 0.008430538251383131, 'aug_scale_low': 0.8071845932014201, 'aug_scale_high': 1.1786783246243595, 'aug_drift_max_amp': 0.19306120657154827, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(806840)} -> 0.9633
2025-10-03 00:19:14,802 - INFO - bo.run_bo - üîçBO Trial 43: Using RF surrogate + Expected Improvement
2025-10-03 00:19:14,802 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:19:14,802 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:19:14,803 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:19:14,803 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005238910918086435, 'batch_size': 32, 'epochs': 49, 'head_dim': 18, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.19027602593063425, 'weight_decay': 0.0007027088434687595, 'grad_clip_norm': 0.438619661382137, 'use_focal_loss': False, 'focal_gamma': 0.5094076904226219, 'label_smoothing': 0.17621418896942562, 'compute_class_weights': False, 'aug_prob': 0.49095910576511426, 'aug_jitter_std': 0.04965848778553738, 'aug_scale_low': 0.804390673136416, 'aug_scale_high': 1.0086941894914072, 'aug_drift_max_amp': 0.10716137957120409, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 478102}
2025-10-03 00:19:14,804 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005238910918086435, 'batch_size': 32, 'epochs': 49, 'head_dim': 18, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.19027602593063425, 'weight_decay': 0.0007027088434687595, 'grad_clip_norm': 0.438619661382137, 'use_focal_loss': False, 'focal_gamma': 0.5094076904226219, 'label_smoothing': 0.17621418896942562, 'compute_class_weights': False, 'aug_prob': 0.49095910576511426, 'aug_jitter_std': 0.04965848778553738, 'aug_scale_low': 0.804390673136416, 'aug_scale_high': 1.0086941894914072, 'aug_drift_max_amp': 0.10716137957120409, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 478102}
2025-10-03 00:20:58,762 - INFO - _models.training_function_executor - Model: 7,817 parameters, 8.4KB storage
2025-10-03 00:20:58,762 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8129845368131354, 0.7359356289649598, 0.7161325893989725, 0.707321457335873, 0.7009828947049217, 0.6978989900049103, 0.6919259407431194, 0.689731450226583, 0.6873029514190768, 0.68527599368273, 0.6836728605079728, 0.6819582243250698, 0.6794800198313788, 0.6787583332658057, 0.678325526445009, 0.6769027857464835, 0.6751427533960923, 0.6737668971401765, 0.6736124585237441, 0.6726835575747698, 0.6726782891332321, 0.6711702957822652, 0.6711970059430954, 0.6698414292064552, 0.6694467932035334, 0.6694255532807762, 0.6684582183987142, 0.6677957005590864, 0.667339551446208, 0.6674437769593121, 0.6671373258462896, 0.6662662634779195, 0.6669663861181542, 0.6653462410742406, 0.6661419381818403, 0.6643376367039031, 0.6649138957445953, 0.6641622296596416, 0.6632322553397383, 0.6642527738463829, 0.6635504424099605, 0.6632491446107498, 0.6634165045257067, 0.6622425114544988, 0.6630084652057945, 0.6625418669597228, 0.6626786765096122, 0.6621501646151509, 0.6616822854807042], 'val_losses': [0.7518737388723641, 0.7016948858996469, 0.7025906604687502, 0.7047871748257842, 0.6889520723588812, 0.6930373091100825, 0.6853864735237206, 0.6812527593910925, 0.6764388691137764, 0.7315723477128663, 0.7033300392760055, 0.6742290582159172, 0.703241873558206, 0.682072803962501, 0.6791378293564977, 0.6731813713252851, 0.6676125757647343, 0.6689616554050185, 0.6723672171879377, 0.6675489758088944, 0.6705727945248177, 0.6729681782518737, 0.6720151346199734, 0.6650052975030968, 0.6665409908693785, 0.6707000634736278, 0.6751770437876543, 0.6722154586988199, 0.661629138457687, 0.6666346302293563, 0.6696176627587143, 0.6738613484318342, 0.6629814132683977, 0.6640666339155755, 0.6651471524500874, 0.6660175224008335, 0.6624449394711455, 0.6632972506708467, 0.6601869687895445, 0.670976373489422, 0.6738872346596048, 0.6710383913642765, 0.6692273167347403, 0.6628922557953291, 0.6583476232983477, 0.6573303852399748, 0.6635614854622389, 0.6597809944889507, 0.6605508811879227], 'val_acc': [0.9347199599047739, 0.9538904899135446, 0.9531387044230046, 0.9570229294574615, 0.9624107254729983, 0.9661696529256986, 0.9642901891993485, 0.9672973311615086, 0.9667961408344818, 0.9684250093973187, 0.9699285803783987, 0.9665455456709685, 0.9693020924696153, 0.9703044731236687, 0.9729357223405588, 0.9696779852148854, 0.9719333416865055, 0.9720586392682621, 0.9743139957398822, 0.970930961032452, 0.9761934594662323, 0.9746898884851523, 0.9745645909033955, 0.9759428643027189, 0.9704297707054254, 0.9760681618844756, 0.977446435283799, 0.9751910788121789, 0.9740634005763689, 0.9761934594662323, 0.976694649793259, 0.9764440546297456, 0.9769452449567724, 0.9738128054128555, 0.9729357223405588, 0.977822328029069, 0.977822328029069, 0.9779476256108257, 0.978198220774339, 0.9771958401202857, 0.9775717328655557, 0.9748151860669089, 0.9771958401202857, 0.9744392933216389, 0.9775717328655557, 0.9769452449567724, 0.9768199473750157, 0.9743139957398822, 0.9773211377020423], 'config': {'seq_len': 1000, 'patch_size': 25, 'head_dim': 18, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 5, 'dropout': 0.19027602593063425, 'num_classes': 5, 'epochs': 49, 'batch_size': 32, 'lr': 0.005238910918086435, 'weight_decay': 0.0007027088434687595, 'grad_clip_norm': 0.438619661382137, 'use_focal_loss': False, 'focal_gamma': 0.5094076904226219, 'label_smoothing': 0.17621418896942562, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.49095910576511426, 'aug_jitter_std': 0.04965848778553738, 'aug_scale_low': 0.804390673136416, 'aug_scale_high': 1.0086941894914072, 'aug_drift_max_amp': 0.10716137957120409, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'seed': 478102}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005238910918086435, 'batch_size': 32, 'epochs': 49, 'head_dim': 18, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 2, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 5, 'patch_size': 25, 'dropout': 0.19027602593063425, 'weight_decay': 0.0007027088434687595, 'grad_clip_norm': 0.438619661382137, 'use_focal_loss': False, 'focal_gamma': 0.5094076904226219, 'label_smoothing': 0.17621418896942562, 'compute_class_weights': False, 'aug_prob': 0.49095910576511426, 'aug_jitter_std': 0.04965848778553738, 'aug_scale_low': 0.804390673136416, 'aug_scale_high': 1.0086941894914072, 'aug_drift_max_amp': 0.10716137957120409, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'seed': 478102}, 'model_parameter_count': 7817, 'model_storage_size_kb': 8.39716796875, 'model_size_validation': 'PASS'}
2025-10-03 00:20:58,762 - INFO - _models.training_function_executor - BO Objective: base=0.9773, size_penalty=0.0000, final=0.9773
2025-10-03 00:20:58,762 - INFO - _models.training_function_executor - Model: 7,817 parameters, 8.4KB (PASS 256KB limit)
2025-10-03 00:20:58,762 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 103.960s
2025-10-03 00:20:58,899 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9773
2025-10-03 00:20:58,899 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.137s
2025-10-03 00:20:58,899 - INFO - bo.run_bo - Recorded observation #43: hparams={'lr': 0.005238910918086435, 'batch_size': np.int64(32), 'epochs': np.int64(49), 'head_dim': np.int64(18), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(25), 'dropout': 0.19027602593063425, 'weight_decay': 0.0007027088434687595, 'grad_clip_norm': 0.438619661382137, 'use_focal_loss': np.False_, 'focal_gamma': 0.5094076904226219, 'label_smoothing': 0.17621418896942562, 'compute_class_weights': np.False_, 'aug_prob': 0.49095910576511426, 'aug_jitter_std': 0.04965848778553738, 'aug_scale_low': 0.804390673136416, 'aug_scale_high': 1.0086941894914072, 'aug_drift_max_amp': 0.10716137957120409, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(478102)}, value=0.9773
2025-10-03 00:20:58,899 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 43: {'lr': 0.005238910918086435, 'batch_size': np.int64(32), 'epochs': np.int64(49), 'head_dim': np.int64(18), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(2), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(25), 'dropout': 0.19027602593063425, 'weight_decay': 0.0007027088434687595, 'grad_clip_norm': 0.438619661382137, 'use_focal_loss': np.False_, 'focal_gamma': 0.5094076904226219, 'label_smoothing': 0.17621418896942562, 'compute_class_weights': np.False_, 'aug_prob': 0.49095910576511426, 'aug_jitter_std': 0.04965848778553738, 'aug_scale_low': 0.804390673136416, 'aug_scale_high': 1.0086941894914072, 'aug_drift_max_amp': 0.10716137957120409, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'seed': np.int64(478102)} -> 0.9773
2025-10-03 00:20:58,900 - INFO - bo.run_bo - üîçBO Trial 44: Using RF surrogate + Expected Improvement
2025-10-03 00:20:58,900 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:20:58,900 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:20:58,900 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:20:58,900 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002149971226704266, 'batch_size': 64, 'epochs': 30, 'head_dim': 24, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 40, 'dropout': 0.2654497846754071, 'weight_decay': 0.008267466876312423, 'grad_clip_norm': 0.9211494687991579, 'use_focal_loss': True, 'focal_gamma': 2.9626461404329634, 'label_smoothing': 0.1896872751034155, 'compute_class_weights': False, 'aug_prob': 0.42622428735572415, 'aug_jitter_std': 0.024860159220028266, 'aug_scale_low': 0.8004707075067758, 'aug_scale_high': 1.037455650239263, 'aug_drift_max_amp': 0.11575530301366074, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 649967}
2025-10-03 00:20:58,901 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002149971226704266, 'batch_size': 64, 'epochs': 30, 'head_dim': 24, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 40, 'dropout': 0.2654497846754071, 'weight_decay': 0.008267466876312423, 'grad_clip_norm': 0.9211494687991579, 'use_focal_loss': True, 'focal_gamma': 2.9626461404329634, 'label_smoothing': 0.1896872751034155, 'compute_class_weights': False, 'aug_prob': 0.42622428735572415, 'aug_jitter_std': 0.024860159220028266, 'aug_scale_low': 0.8004707075067758, 'aug_scale_high': 1.037455650239263, 'aug_drift_max_amp': 0.11575530301366074, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 649967}
2025-10-03 00:21:51,560 - INFO - _models.training_function_executor - Model: 10,649 parameters, 45.8KB storage
2025-10-03 00:21:51,560 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.244188480489711, 0.16342724782835102, 0.1459286533165642, 0.13948291138757893, 0.134146099690693, 0.13050705330937712, 0.12820154953287854, 0.12483591621243535, 0.12319655136812922, 0.12193329887566694, 0.12107602808255397, 0.12000932876983764, 0.11854475058333504, 0.11759269082429165, 0.1168094419046321, 0.11646992758794472, 0.11609030078021901, 0.11502513574423529, 0.11489695645883949, 0.11323823431273419, 0.1135649145885551, 0.1129487603534057, 0.11247870977206535, 0.11143237211075552, 0.11205734712568638, 0.11161590894788696, 0.11125977858431906, 0.11074867390096887, 0.10993495600641892, 0.1099295952034016], 'val_losses': [0.16790451177557733, 0.14229971752954207, 0.1369872669362735, 0.13070545479227705, 0.1347088068116878, 0.12419254494843335, 0.13606677515519291, 0.11898050876211783, 0.13813590875523027, 0.11773532412711905, 0.1184447388547567, 0.12130655772688095, 0.1265212468352029, 0.12263523458374008, 0.11441177331390065, 0.11453626628243047, 0.11128976175606124, 0.12063078425825575, 0.11384956530462066, 0.1110973072584493, 0.11385907244804046, 0.11100428556756647, 0.11251537326119804, 0.11157063395158344, 0.1106380530354254, 0.1106321366461592, 0.10916772266891185, 0.10904828560966968, 0.10745691307530399, 0.10709985309474047], 'val_acc': [0.939230672848014, 0.9570229294574615, 0.9587770956020549, 0.9597794762561083, 0.960656559328405, 0.9619095351459717, 0.9581506076932715, 0.9662949505074552, 0.9586517980202982, 0.9685503069790753, 0.9679238190702919, 0.9662949505074552, 0.9637889988723217, 0.9654178674351584, 0.9696779852148854, 0.9693020924696153, 0.9720586392682621, 0.9708056634506954, 0.9660443553439418, 0.9731863175040721, 0.9714321513594788, 0.9698032827966421, 0.9716827465229921, 0.9751910788121789, 0.970930961032452, 0.9750657812304222, 0.9761934594662323, 0.9734369126675855, 0.976694649793259, 0.9754416739756923], 'config': {'seq_len': 1000, 'patch_size': 40, 'head_dim': 24, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 7, 'dropout': 0.2654497846754071, 'num_classes': 5, 'epochs': 30, 'batch_size': 64, 'lr': 0.002149971226704266, 'weight_decay': 0.008267466876312423, 'grad_clip_norm': 0.9211494687991579, 'use_focal_loss': True, 'focal_gamma': 2.9626461404329634, 'label_smoothing': 0.1896872751034155, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.42622428735572415, 'aug_jitter_std': 0.024860159220028266, 'aug_scale_low': 0.8004707075067758, 'aug_scale_high': 1.037455650239263, 'aug_drift_max_amp': 0.11575530301366074, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 649967}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002149971226704266, 'batch_size': 64, 'epochs': 30, 'head_dim': 24, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 3, 'stem_channels': 12, 'ds_kernel1': 3, 'ds_kernel2': 7, 'patch_size': 40, 'dropout': 0.2654497846754071, 'weight_decay': 0.008267466876312423, 'grad_clip_norm': 0.9211494687991579, 'use_focal_loss': True, 'focal_gamma': 2.9626461404329634, 'label_smoothing': 0.1896872751034155, 'compute_class_weights': False, 'aug_prob': 0.42622428735572415, 'aug_jitter_std': 0.024860159220028266, 'aug_scale_low': 0.8004707075067758, 'aug_scale_high': 1.037455650239263, 'aug_drift_max_amp': 0.11575530301366074, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 649967}, 'model_parameter_count': 10649, 'model_storage_size_kb': 45.757421875000006, 'model_size_validation': 'PASS'}
2025-10-03 00:21:51,560 - INFO - _models.training_function_executor - BO Objective: base=0.9754, size_penalty=0.0000, final=0.9754
2025-10-03 00:21:51,560 - INFO - _models.training_function_executor - Model: 10,649 parameters, 45.8KB (PASS 256KB limit)
2025-10-03 00:21:51,560 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 52.661s
2025-10-03 00:21:51,698 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9754
2025-10-03 00:21:51,699 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.138s
2025-10-03 00:21:51,699 - INFO - bo.run_bo - Recorded observation #44: hparams={'lr': 0.002149971226704266, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'head_dim': np.int64(24), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(40), 'dropout': 0.2654497846754071, 'weight_decay': 0.008267466876312423, 'grad_clip_norm': 0.9211494687991579, 'use_focal_loss': np.True_, 'focal_gamma': 2.9626461404329634, 'label_smoothing': 0.1896872751034155, 'compute_class_weights': np.False_, 'aug_prob': 0.42622428735572415, 'aug_jitter_std': 0.024860159220028266, 'aug_scale_low': 0.8004707075067758, 'aug_scale_high': 1.037455650239263, 'aug_drift_max_amp': 0.11575530301366074, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(649967)}, value=0.9754
2025-10-03 00:21:51,699 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 44: {'lr': 0.002149971226704266, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'head_dim': np.int64(24), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(40), 'dropout': 0.2654497846754071, 'weight_decay': 0.008267466876312423, 'grad_clip_norm': 0.9211494687991579, 'use_focal_loss': np.True_, 'focal_gamma': 2.9626461404329634, 'label_smoothing': 0.1896872751034155, 'compute_class_weights': np.False_, 'aug_prob': 0.42622428735572415, 'aug_jitter_std': 0.024860159220028266, 'aug_scale_low': 0.8004707075067758, 'aug_scale_high': 1.037455650239263, 'aug_drift_max_amp': 0.11575530301366074, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(649967)} -> 0.9754
2025-10-03 00:21:51,699 - INFO - bo.run_bo - üîçBO Trial 45: Using RF surrogate + Expected Improvement
2025-10-03 00:21:51,699 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:21:51,699 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:21:51,700 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:21:51,700 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004613381261077666, 'batch_size': 32, 'epochs': 36, 'head_dim': 12, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 8, 'dropout': 0.2642784420400201, 'weight_decay': 0.005439795674719284, 'grad_clip_norm': 3.8079343503383942, 'use_focal_loss': False, 'focal_gamma': 2.886674987268719, 'label_smoothing': 0.024184190601223482, 'compute_class_weights': False, 'aug_prob': 0.013622605197992414, 'aug_jitter_std': 0.04416584558905961, 'aug_scale_low': 0.8748847568864547, 'aug_scale_high': 1.002371776970302, 'aug_drift_max_amp': 0.14733799457937205, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 317737}
2025-10-03 00:21:51,701 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004613381261077666, 'batch_size': 32, 'epochs': 36, 'head_dim': 12, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 8, 'dropout': 0.2642784420400201, 'weight_decay': 0.005439795674719284, 'grad_clip_norm': 3.8079343503383942, 'use_focal_loss': False, 'focal_gamma': 2.886674987268719, 'label_smoothing': 0.024184190601223482, 'compute_class_weights': False, 'aug_prob': 0.013622605197992414, 'aug_jitter_std': 0.04416584558905961, 'aug_scale_low': 0.8748847568864547, 'aug_scale_high': 1.002371776970302, 'aug_drift_max_amp': 0.14733799457937205, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 317737}
2025-10-03 00:22:29,925 - INFO - _models.training_function_executor - Model: 3,581 parameters, 15.4KB storage
2025-10-03 00:22:29,926 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7395275775955251, 0.5735308334398952, 0.45683203738192, 0.406298162197927, 0.3753514774840372, 0.35935145805672447, 0.3437966902677313, 0.32870023897235057, 0.3190930156427618, 0.30510894063953187, 0.30166349298179596, 0.2947722729579647, 0.28984491840152893, 0.2840093051620217, 0.27866718603215623, 0.272685273126698, 0.2727582633497198, 0.2704319435890368, 0.26721932872869847, 0.26534707273053176, 0.26347157245974673, 0.2626598396404485, 0.2608792845181825, 0.25893650634808674, 0.2565299818675082, 0.25703942468675484, 0.2540785974815324, 0.25238576259461576, 0.25339397732220365, 0.25491513987346776, 0.2505307800301602, 0.25101174713880187, 0.2507132541318139, 0.25087003661225965, 0.24828728762268368, 0.2473684029725845], 'val_losses': [0.633664428679929, 0.49752119520734894, 0.4062618989608981, 0.3650965535794564, 0.3550608621707403, 0.33308023673268344, 0.30611248752584197, 0.3016311776673879, 0.2930293715944148, 0.2818159103214315, 0.2720257480436363, 0.2698035582633055, 0.27741431794493976, 0.2756036572842621, 0.25878669873258997, 0.2568400880400511, 0.28101712293514947, 0.24305056143622547, 0.2399801909326088, 0.25252063788801277, 0.24168703394297025, 0.24548421164769424, 0.24051932873858528, 0.24683927300728856, 0.23895888633408982, 0.23817981156157514, 0.24719426219975077, 0.24209528502747613, 0.2641742483533843, 0.24015132773491601, 0.23359983009348656, 0.23508598587593926, 0.25672887678866907, 0.22863624669782293, 0.22821461957598252, 0.23535181010461007], 'val_acc': [0.8168149354717454, 0.8813431900764315, 0.9114146096980328, 0.9305851397068037, 0.9344693647412605, 0.9343440671595038, 0.9408595414108508, 0.9498809672973312, 0.9501315624608445, 0.9491291818067912, 0.9582759052750282, 0.9530134068412479, 0.9553940608946248, 0.9551434657311114, 0.9592782859290816, 0.9584012028567849, 0.954516977822328, 0.9619095351459717, 0.9605312617466483, 0.9616589399824583, 0.9611577496554317, 0.9604059641648917, 0.960656559328405, 0.9570229294574615, 0.962160130309485, 0.9630372133817817, 0.9622854278912417, 0.9612830472371883, 0.9594035835108382, 0.9601553690013783, 0.9666708432527252, 0.9627866182182684, 0.9596541786743515, 0.9630372133817817, 0.9674226287432652, 0.9630372133817817], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 12, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 5, 'dropout': 0.2642784420400201, 'num_classes': 5, 'epochs': 36, 'batch_size': 32, 'lr': 0.004613381261077666, 'weight_decay': 0.005439795674719284, 'grad_clip_norm': 3.8079343503383942, 'use_focal_loss': False, 'focal_gamma': 2.886674987268719, 'label_smoothing': 0.024184190601223482, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.013622605197992414, 'aug_jitter_std': 0.04416584558905961, 'aug_scale_low': 0.8748847568864547, 'aug_scale_high': 1.002371776970302, 'aug_drift_max_amp': 0.14733799457937205, 'normalize': False}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 317737}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004613381261077666, 'batch_size': 32, 'epochs': 36, 'head_dim': 12, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 3, 'ds_kernel2': 5, 'patch_size': 8, 'dropout': 0.2642784420400201, 'weight_decay': 0.005439795674719284, 'grad_clip_norm': 3.8079343503383942, 'use_focal_loss': False, 'focal_gamma': 2.886674987268719, 'label_smoothing': 0.024184190601223482, 'compute_class_weights': False, 'aug_prob': 0.013622605197992414, 'aug_jitter_std': 0.04416584558905961, 'aug_scale_low': 0.8748847568864547, 'aug_scale_high': 1.002371776970302, 'aug_drift_max_amp': 0.14733799457937205, 'normalize': False, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 317737}, 'model_parameter_count': 3581, 'model_storage_size_kb': 15.387109375000001, 'model_size_validation': 'PASS'}
2025-10-03 00:22:29,926 - INFO - _models.training_function_executor - BO Objective: base=0.9630, size_penalty=0.0000, final=0.9630
2025-10-03 00:22:29,926 - INFO - _models.training_function_executor - Model: 3,581 parameters, 15.4KB (PASS 256KB limit)
2025-10-03 00:22:29,926 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 38.226s
2025-10-03 00:22:30,063 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9630
2025-10-03 00:22:30,063 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.138s
2025-10-03 00:22:30,063 - INFO - bo.run_bo - Recorded observation #45: hparams={'lr': 0.004613381261077666, 'batch_size': np.int64(32), 'epochs': np.int64(36), 'head_dim': np.int64(12), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(8), 'dropout': 0.2642784420400201, 'weight_decay': 0.005439795674719284, 'grad_clip_norm': 3.8079343503383942, 'use_focal_loss': np.False_, 'focal_gamma': 2.886674987268719, 'label_smoothing': 0.024184190601223482, 'compute_class_weights': np.False_, 'aug_prob': 0.013622605197992414, 'aug_jitter_std': 0.04416584558905961, 'aug_scale_low': 0.8748847568864547, 'aug_scale_high': 1.002371776970302, 'aug_drift_max_amp': 0.14733799457937205, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(317737)}, value=0.9630
2025-10-03 00:22:30,063 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 45: {'lr': 0.004613381261077666, 'batch_size': np.int64(32), 'epochs': np.int64(36), 'head_dim': np.int64(12), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(3), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(8), 'dropout': 0.2642784420400201, 'weight_decay': 0.005439795674719284, 'grad_clip_norm': 3.8079343503383942, 'use_focal_loss': np.False_, 'focal_gamma': 2.886674987268719, 'label_smoothing': 0.024184190601223482, 'compute_class_weights': np.False_, 'aug_prob': 0.013622605197992414, 'aug_jitter_std': 0.04416584558905961, 'aug_scale_low': 0.8748847568864547, 'aug_scale_high': 1.002371776970302, 'aug_drift_max_amp': 0.14733799457937205, 'normalize': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(317737)} -> 0.9630
2025-10-03 00:22:30,064 - INFO - bo.run_bo - üîçBO Trial 46: Using RF surrogate + Expected Improvement
2025-10-03 00:22:30,064 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:22:30,064 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:22:30,064 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:22:30,064 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0010124957778835345, 'batch_size': 64, 'epochs': 38, 'head_dim': 24, 'n_heads': 4, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 10, 'dropout': 0.3344193751722666, 'weight_decay': 0.006104049034894057, 'grad_clip_norm': 1.9263527067957769, 'use_focal_loss': True, 'focal_gamma': 1.6716524074150254, 'label_smoothing': 0.07255804760984687, 'compute_class_weights': False, 'aug_prob': 0.5764342820942837, 'aug_jitter_std': 0.04128338123777596, 'aug_scale_low': 0.9638711294064497, 'aug_scale_high': 1.1932107374318641, 'aug_drift_max_amp': 0.09728011920573307, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 100211}
2025-10-03 00:22:30,065 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0010124957778835345, 'batch_size': 64, 'epochs': 38, 'head_dim': 24, 'n_heads': 4, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 10, 'dropout': 0.3344193751722666, 'weight_decay': 0.006104049034894057, 'grad_clip_norm': 1.9263527067957769, 'use_focal_loss': True, 'focal_gamma': 1.6716524074150254, 'label_smoothing': 0.07255804760984687, 'compute_class_weights': False, 'aug_prob': 0.5764342820942837, 'aug_jitter_std': 0.04128338123777596, 'aug_scale_low': 0.9638711294064497, 'aug_scale_high': 1.1932107374318641, 'aug_drift_max_amp': 0.09728011920573307, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 100211}
2025-10-03 00:23:15,605 - INFO - _models.training_function_executor - Model: 4,957 parameters, 5.3KB storage
2025-10-03 00:23:15,605 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4544156687301797, 0.2901175180150464, 0.22039927508321072, 0.18464584433888298, 0.16698944760622492, 0.1559426978179597, 0.14726091280551093, 0.14098596437275082, 0.13680143582648369, 0.13140714147403246, 0.12688025955710724, 0.12349816772908125, 0.12287976597376987, 0.12049590564823814, 0.11760853881464078, 0.1168259219480174, 0.11647920509137423, 0.11311409648874142, 0.1125556987478101, 0.11122596671365016, 0.10978204460847095, 0.10978114754601058, 0.10828587401444625, 0.10661766144233445, 0.1046399340870009, 0.10535232695499033, 0.10417496911868399, 0.10494314232145684, 0.10372826271546805, 0.10303482048056409, 0.10279608799963073, 0.10337231195843452, 0.10145101556217477, 0.10161563990589756, 0.100621529750887, 0.10099691112802593, 0.10007533048873363, 0.1000647406769651], 'val_losses': [0.33056885406399206, 0.23906540999868403, 0.24276933239794155, 0.1587547413718206, 0.14819181154745442, 0.1388824789747157, 0.12932091146285574, 0.1362518062942265, 0.13723043930880066, 0.11656891329797672, 0.11105653702607804, 0.1311984848468663, 0.11434107008225008, 0.11238668719534138, 0.10404755510534504, 0.11207029041681892, 0.10365341424609677, 0.10698257390231869, 0.10293100245251212, 0.10162143792376872, 0.10037627759347686, 0.09587584495499624, 0.09897884997064167, 0.09666857606295041, 0.0996430504779113, 0.09688147080506974, 0.09438641556890937, 0.09601954514688903, 0.0976257251496005, 0.10997223010798501, 0.10101690206920842, 0.10503316739924852, 0.09630080130263508, 0.09236676145292227, 0.09423950583194524, 0.09501717155599726, 0.09002191441098845, 0.09133339793240572], 'val_acc': [0.8324771331913294, 0.9198095476757299, 0.9142964540784363, 0.9357223405588272, 0.9433654930459842, 0.9502568600426011, 0.955268763312868, 0.9507580503696279, 0.9500062648790878, 0.9590276907655683, 0.9577747149480015, 0.9536398947500313, 0.9562711439669215, 0.9604059641648917, 0.9656684625986719, 0.962536023054755, 0.9649166771081318, 0.9642901891993485, 0.962536023054755, 0.9644154867811051, 0.9681744142338052, 0.968299711815562, 0.9679238190702919, 0.9693020924696153, 0.968299711815562, 0.9670467359979953, 0.9693020924696153, 0.9700538779601554, 0.9642901891993485, 0.9586517980202982, 0.9669214384162386, 0.9644154867811051, 0.9695526876331286, 0.9670467359979953, 0.9664202480892119, 0.9698032827966421, 0.970179175541912, 0.9705550682871821], 'config': {'seq_len': 1000, 'patch_size': 10, 'head_dim': 24, 'n_heads': 4, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.3344193751722666, 'num_classes': 5, 'epochs': 38, 'batch_size': 64, 'lr': 0.0010124957778835345, 'weight_decay': 0.006104049034894057, 'grad_clip_norm': 1.9263527067957769, 'use_focal_loss': True, 'focal_gamma': 1.6716524074150254, 'label_smoothing': 0.07255804760984687, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.5764342820942837, 'aug_jitter_std': 0.04128338123777596, 'aug_scale_low': 0.9638711294064497, 'aug_scale_high': 1.1932107374318641, 'aug_drift_max_amp': 0.09728011920573307, 'normalize': False}, 'quantization': {'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'seed': 100211}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0010124957778835345, 'batch_size': 64, 'epochs': 38, 'head_dim': 24, 'n_heads': 4, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 10, 'dropout': 0.3344193751722666, 'weight_decay': 0.006104049034894057, 'grad_clip_norm': 1.9263527067957769, 'use_focal_loss': True, 'focal_gamma': 1.6716524074150254, 'label_smoothing': 0.07255804760984687, 'compute_class_weights': False, 'aug_prob': 0.5764342820942837, 'aug_jitter_std': 0.04128338123777596, 'aug_scale_low': 0.9638711294064497, 'aug_scale_high': 1.1932107374318641, 'aug_drift_max_amp': 0.09728011920573307, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 100211}, 'model_parameter_count': 4957, 'model_storage_size_kb': 5.324902343750001, 'model_size_validation': 'PASS'}
2025-10-03 00:23:15,605 - INFO - _models.training_function_executor - BO Objective: base=0.9706, size_penalty=0.0000, final=0.9706
2025-10-03 00:23:15,606 - INFO - _models.training_function_executor - Model: 4,957 parameters, 5.3KB (PASS 256KB limit)
2025-10-03 00:23:15,606 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 45.541s
2025-10-03 00:23:15,744 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9706
2025-10-03 00:23:15,744 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.138s
2025-10-03 00:23:15,744 - INFO - bo.run_bo - Recorded observation #46: hparams={'lr': 0.0010124957778835345, 'batch_size': np.int64(64), 'epochs': np.int64(38), 'head_dim': np.int64(24), 'n_heads': np.int64(4), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(10), 'dropout': 0.3344193751722666, 'weight_decay': 0.006104049034894057, 'grad_clip_norm': 1.9263527067957769, 'use_focal_loss': np.True_, 'focal_gamma': 1.6716524074150254, 'label_smoothing': 0.07255804760984687, 'compute_class_weights': np.False_, 'aug_prob': 0.5764342820942837, 'aug_jitter_std': 0.04128338123777596, 'aug_scale_low': 0.9638711294064497, 'aug_scale_high': 1.1932107374318641, 'aug_drift_max_amp': 0.09728011920573307, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(100211)}, value=0.9706
2025-10-03 00:23:15,744 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 46: {'lr': 0.0010124957778835345, 'batch_size': np.int64(64), 'epochs': np.int64(38), 'head_dim': np.int64(24), 'n_heads': np.int64(4), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(10), 'dropout': 0.3344193751722666, 'weight_decay': 0.006104049034894057, 'grad_clip_norm': 1.9263527067957769, 'use_focal_loss': np.True_, 'focal_gamma': 1.6716524074150254, 'label_smoothing': 0.07255804760984687, 'compute_class_weights': np.False_, 'aug_prob': 0.5764342820942837, 'aug_jitter_std': 0.04128338123777596, 'aug_scale_low': 0.9638711294064497, 'aug_scale_high': 1.1932107374318641, 'aug_drift_max_amp': 0.09728011920573307, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(100211)} -> 0.9706
2025-10-03 00:23:15,745 - INFO - bo.run_bo - üîçBO Trial 47: Using RF surrogate + Expected Improvement
2025-10-03 00:23:15,745 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:23:15,745 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:23:15,745 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:23:15,745 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007068248786729482, 'batch_size': 256, 'epochs': 42, 'head_dim': 21, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 7, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.40726774257833753, 'weight_decay': 5.425553402840583e-06, 'grad_clip_norm': 0.21663753482495443, 'use_focal_loss': True, 'focal_gamma': 1.5726592770695356, 'label_smoothing': 0.1609362142362512, 'compute_class_weights': False, 'aug_prob': 0.08560140099398829, 'aug_jitter_std': 0.02267873236212574, 'aug_scale_low': 0.9683914967269723, 'aug_scale_high': 1.190512431530519, 'aug_drift_max_amp': 0.08702569843377687, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 492250}
2025-10-03 00:23:15,746 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007068248786729482, 'batch_size': 256, 'epochs': 42, 'head_dim': 21, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 7, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.40726774257833753, 'weight_decay': 5.425553402840583e-06, 'grad_clip_norm': 0.21663753482495443, 'use_focal_loss': True, 'focal_gamma': 1.5726592770695356, 'label_smoothing': 0.1609362142362512, 'compute_class_weights': False, 'aug_prob': 0.08560140099398829, 'aug_jitter_std': 0.02267873236212574, 'aug_scale_low': 0.9683914967269723, 'aug_scale_high': 1.190512431530519, 'aug_drift_max_amp': 0.08702569843377687, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 492250}
2025-10-03 00:23:40,981 - INFO - _models.training_function_executor - Model: 3,645 parameters, 3.9KB storage
2025-10-03 00:23:40,981 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7467913586659505, 0.5567343772695584, 0.5286562261349746, 0.5060346820440873, 0.4853454666932275, 0.46758115915126175, 0.45260508812333106, 0.4366804586193471, 0.41813558635453857, 0.401517993846013, 0.38447483020371315, 0.370238878266299, 0.36198800363733485, 0.350913993212094, 0.3418640531054551, 0.33655003215276824, 0.33081986749507925, 0.3255720387598, 0.3188229647406965, 0.31500484355484165, 0.3109832735677876, 0.30612973318461306, 0.30161799038371034, 0.29780085082696134, 0.29695047895453447, 0.29132101049258313, 0.29076636607469264, 0.28879066037604434, 0.28604548896284765, 0.2855061909802758, 0.2813175339856826, 0.2813792037363769, 0.27942907753636903, 0.27750985921633536, 0.27572003040450244, 0.2762255913169858, 0.2735385795616362, 0.2734933966217687, 0.27019594934479363, 0.26911636564033514, 0.2681925692159244, 0.26861081373212453], 'val_losses': [0.5864938015388734, 0.5492764770529083, 0.5257926476605179, 0.5017403013618679, 0.48151942161506406, 0.47485587503091237, 0.44374673094475814, 0.4251652075531578, 0.4041569712966804, 0.39184415339303336, 0.3830311874663108, 0.3664769100606658, 0.35043653841306055, 0.3377520432300159, 0.33137936725262157, 0.31625848392423955, 0.3186983267099035, 0.32624871816914147, 0.3129287439713187, 0.30937378891181144, 0.29264877296684527, 0.290926421206557, 0.28342163196640374, 0.28242878618283374, 0.28303451349548253, 0.2841057492917887, 0.27641782209010163, 0.2811160068223835, 0.2801215358768366, 0.2667249946229187, 0.278745335850586, 0.27555347220847787, 0.26720636773305106, 0.26393071435930554, 0.2619950780536489, 0.2690027991384212, 0.2632583158776629, 0.2566889247067843, 0.28206580848177637, 0.26979520376605987, 0.27074259539741485, 0.25648686955774536], 'val_acc': [0.713444430522491, 0.7532890615211126, 0.7803533391805538, 0.7881217892494675, 0.8186943991980955, 0.8240821952136324, 0.8252098734494424, 0.8462598671845634, 0.8739506327527878, 0.8859792006014284, 0.8921187821075053, 0.8939982458338555, 0.9048991354466859, 0.9101616338804661, 0.9136699661696529, 0.9156747274777597, 0.9194336549304598, 0.9188071670216765, 0.9218143089838366, 0.9171782984588397, 0.9270768074176169, 0.9309610324520736, 0.936975316376394, 0.9348452574865305, 0.9448690640270643, 0.937727101866934, 0.9454955519358477, 0.9365994236311239, 0.9459967422628743, 0.9502568600426011, 0.9417366244831474, 0.9429896003007142, 0.9478762059892244, 0.9475003132439543, 0.9493797769703045, 0.9493797769703045, 0.9507580503696279, 0.9522616213507079, 0.945746147099361, 0.9522616213507079, 0.9473750156621977, 0.9531387044230046], 'config': {'seq_len': 1000, 'patch_size': 20, 'head_dim': 21, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 7, 'ds_kernel2': 5, 'dropout': 0.40726774257833753, 'num_classes': 5, 'epochs': 42, 'batch_size': 256, 'lr': 0.0007068248786729482, 'weight_decay': 5.425553402840583e-06, 'grad_clip_norm': 0.21663753482495443, 'use_focal_loss': True, 'focal_gamma': 1.5726592770695356, 'label_smoothing': 0.1609362142362512, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.08560140099398829, 'aug_jitter_std': 0.02267873236212574, 'aug_scale_low': 0.9683914967269723, 'aug_scale_high': 1.190512431530519, 'aug_drift_max_amp': 0.08702569843377687, 'normalize': False}, 'quantization': {'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'seed': 492250}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007068248786729482, 'batch_size': 256, 'epochs': 42, 'head_dim': 21, 'n_heads': 4, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 8, 'ds_kernel1': 7, 'ds_kernel2': 5, 'patch_size': 20, 'dropout': 0.40726774257833753, 'weight_decay': 5.425553402840583e-06, 'grad_clip_norm': 0.21663753482495443, 'use_focal_loss': True, 'focal_gamma': 1.5726592770695356, 'label_smoothing': 0.1609362142362512, 'compute_class_weights': False, 'aug_prob': 0.08560140099398829, 'aug_jitter_std': 0.02267873236212574, 'aug_scale_low': 0.9683914967269723, 'aug_scale_high': 1.190512431530519, 'aug_drift_max_amp': 0.08702569843377687, 'normalize': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'seed': 492250}, 'model_parameter_count': 3645, 'model_storage_size_kb': 3.9155273437500004, 'model_size_validation': 'PASS'}
2025-10-03 00:23:40,981 - INFO - _models.training_function_executor - BO Objective: base=0.9531, size_penalty=0.0000, final=0.9531
2025-10-03 00:23:40,981 - INFO - _models.training_function_executor - Model: 3,645 parameters, 3.9KB (PASS 256KB limit)
2025-10-03 00:23:40,981 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 25.236s
2025-10-03 00:23:41,120 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9531
2025-10-03 00:23:41,120 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.139s
2025-10-03 00:23:41,120 - INFO - bo.run_bo - Recorded observation #47: hparams={'lr': 0.0007068248786729482, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'head_dim': np.int64(21), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(20), 'dropout': 0.40726774257833753, 'weight_decay': 5.425553402840583e-06, 'grad_clip_norm': 0.21663753482495443, 'use_focal_loss': np.True_, 'focal_gamma': 1.5726592770695356, 'label_smoothing': 0.1609362142362512, 'compute_class_weights': np.False_, 'aug_prob': 0.08560140099398829, 'aug_jitter_std': 0.02267873236212574, 'aug_scale_low': 0.9683914967269723, 'aug_scale_high': 1.190512431530519, 'aug_drift_max_amp': 0.08702569843377687, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(492250)}, value=0.9531
2025-10-03 00:23:41,120 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 47: {'lr': 0.0007068248786729482, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'head_dim': np.int64(21), 'n_heads': np.int64(4), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(5), 'patch_size': np.int64(20), 'dropout': 0.40726774257833753, 'weight_decay': 5.425553402840583e-06, 'grad_clip_norm': 0.21663753482495443, 'use_focal_loss': np.True_, 'focal_gamma': 1.5726592770695356, 'label_smoothing': 0.1609362142362512, 'compute_class_weights': np.False_, 'aug_prob': 0.08560140099398829, 'aug_jitter_std': 0.02267873236212574, 'aug_scale_low': 0.9683914967269723, 'aug_scale_high': 1.190512431530519, 'aug_drift_max_amp': 0.08702569843377687, 'normalize': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(492250)} -> 0.9531
2025-10-03 00:23:41,121 - INFO - bo.run_bo - üîçBO Trial 48: Using RF surrogate + Expected Improvement
2025-10-03 00:23:41,121 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:23:41,121 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:23:41,121 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:23:41,121 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0011417090561947967, 'batch_size': 32, 'epochs': 35, 'head_dim': 24, 'n_heads': 8, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.4495946162630533, 'weight_decay': 0.0003991966703527362, 'grad_clip_norm': 1.2738517565408067, 'use_focal_loss': True, 'focal_gamma': 4.728209075982194, 'label_smoothing': 0.1657181659916812, 'compute_class_weights': False, 'aug_prob': 0.029112729494307217, 'aug_jitter_std': 0.03934222689137367, 'aug_scale_low': 0.9828464767538027, 'aug_scale_high': 1.1718655251025405, 'aug_drift_max_amp': 0.16551015259966428, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 710142}
2025-10-03 00:23:41,122 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0011417090561947967, 'batch_size': 32, 'epochs': 35, 'head_dim': 24, 'n_heads': 8, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.4495946162630533, 'weight_decay': 0.0003991966703527362, 'grad_clip_norm': 1.2738517565408067, 'use_focal_loss': True, 'focal_gamma': 4.728209075982194, 'label_smoothing': 0.1657181659916812, 'compute_class_weights': False, 'aug_prob': 0.029112729494307217, 'aug_jitter_std': 0.03934222689137367, 'aug_scale_low': 0.9828464767538027, 'aug_scale_high': 1.1718655251025405, 'aug_drift_max_amp': 0.16551015259966428, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 710142}
2025-10-03 00:24:42,985 - INFO - _models.training_function_executor - Model: 4,957 parameters, 21.3KB storage
2025-10-03 00:24:42,985 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.14719524811595633, 0.0805333053954983, 0.06805073629109101, 0.061627772938166854, 0.05911055523183913, 0.05551181402471331, 0.0541322436950968, 0.052318367654999684, 0.052279096592012415, 0.0504972686366713, 0.050457530984487506, 0.04892701084562155, 0.04862654310265822, 0.04763702856345784, 0.04700539298993907, 0.04706049474097488, 0.045964574614549135, 0.04555852766141248, 0.04537554209396004, 0.04519067139115662, 0.04479238623544596, 0.044942185747041084, 0.04409109496068686, 0.04412372105230392, 0.04406443264771721, 0.0439613266741455, 0.043538349590017185, 0.04335229434391552, 0.042422730771897246, 0.04238201203085236, 0.04242589662069572, 0.04231587046965585, 0.041912196011167586, 0.04197034749841896, 0.04140235928836263], 'val_losses': [0.08730221728030758, 0.07177294655661583, 0.07105385032554211, 0.05474351359562324, 0.051327044406364083, 0.04849081280337974, 0.04684948855109, 0.04720824865967151, 0.04567743312530598, 0.04690304606622688, 0.04312551446616567, 0.04383418176828687, 0.045316976455761784, 0.04300363993561816, 0.042151839474570615, 0.04336347760950738, 0.039644021848481543, 0.039876406076236444, 0.04129441565304319, 0.039957981418956866, 0.03949029792588899, 0.041569237365652156, 0.041010097496313284, 0.040222756362039036, 0.03901625794771993, 0.03772318833226952, 0.03931724703105795, 0.038679379525003596, 0.038764889761030694, 0.04221667562559006, 0.03837594415494455, 0.039982828753303126, 0.03771444163724543, 0.04325922913247221, 0.03770235196164849], 'val_acc': [0.9307104372885603, 0.9477509084074678, 0.9414860293196341, 0.9561458463851648, 0.9594035835108382, 0.9612830472371883, 0.9592782859290816, 0.9600300714196216, 0.9630372133817817, 0.9630372133817817, 0.9642901891993485, 0.9639142964540784, 0.9636637012905651, 0.9672973311615086, 0.9646660819446184, 0.963287808545295, 0.9693020924696153, 0.9681744142338052, 0.9662949505074552, 0.9652925698534018, 0.9665455456709685, 0.9662949505074552, 0.9669214384162386, 0.9656684625986719, 0.9693020924696153, 0.9718080441047487, 0.968675604560832, 0.9688009021425886, 0.970179175541912, 0.9669214384162386, 0.9698032827966421, 0.9676732239067786, 0.9689261997243453, 0.9660443553439418, 0.9699285803783987], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 24, 'n_heads': 8, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'dropout': 0.4495946162630533, 'num_classes': 5, 'epochs': 35, 'batch_size': 32, 'lr': 0.0011417090561947967, 'weight_decay': 0.0003991966703527362, 'grad_clip_norm': 1.2738517565408067, 'use_focal_loss': True, 'focal_gamma': 4.728209075982194, 'label_smoothing': 0.1657181659916812, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.029112729494307217, 'aug_jitter_std': 0.03934222689137367, 'aug_scale_low': 0.9828464767538027, 'aug_scale_high': 1.1718655251025405, 'aug_drift_max_amp': 0.16551015259966428, 'normalize': True}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 710142}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0011417090561947967, 'batch_size': 32, 'epochs': 35, 'head_dim': 24, 'n_heads': 8, 'n_layers': 3, 'd_ff_factor': 3, 'stem_channels': 8, 'ds_kernel1': 9, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.4495946162630533, 'weight_decay': 0.0003991966703527362, 'grad_clip_norm': 1.2738517565408067, 'use_focal_loss': True, 'focal_gamma': 4.728209075982194, 'label_smoothing': 0.1657181659916812, 'compute_class_weights': False, 'aug_prob': 0.029112729494307217, 'aug_jitter_std': 0.03934222689137367, 'aug_scale_low': 0.9828464767538027, 'aug_scale_high': 1.1718655251025405, 'aug_drift_max_amp': 0.16551015259966428, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 710142}, 'model_parameter_count': 4957, 'model_storage_size_kb': 21.299609375000003, 'model_size_validation': 'PASS'}
2025-10-03 00:24:42,986 - INFO - _models.training_function_executor - BO Objective: base=0.9699, size_penalty=0.0000, final=0.9699
2025-10-03 00:24:42,986 - INFO - _models.training_function_executor - Model: 4,957 parameters, 21.3KB (PASS 256KB limit)
2025-10-03 00:24:42,986 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 61.864s
2025-10-03 00:24:43,123 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9699
2025-10-03 00:24:43,124 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.138s
2025-10-03 00:24:43,124 - INFO - bo.run_bo - Recorded observation #48: hparams={'lr': 0.0011417090561947967, 'batch_size': np.int64(32), 'epochs': np.int64(35), 'head_dim': np.int64(24), 'n_heads': np.int64(8), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.4495946162630533, 'weight_decay': 0.0003991966703527362, 'grad_clip_norm': 1.2738517565408067, 'use_focal_loss': np.True_, 'focal_gamma': 4.728209075982194, 'label_smoothing': 0.1657181659916812, 'compute_class_weights': np.False_, 'aug_prob': 0.029112729494307217, 'aug_jitter_std': 0.03934222689137367, 'aug_scale_low': 0.9828464767538027, 'aug_scale_high': 1.1718655251025405, 'aug_drift_max_amp': 0.16551015259966428, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(710142)}, value=0.9699
2025-10-03 00:24:43,124 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 48: {'lr': 0.0011417090561947967, 'batch_size': np.int64(32), 'epochs': np.int64(35), 'head_dim': np.int64(24), 'n_heads': np.int64(8), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(3), 'stem_channels': np.int64(8), 'ds_kernel1': np.int64(9), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.4495946162630533, 'weight_decay': 0.0003991966703527362, 'grad_clip_norm': 1.2738517565408067, 'use_focal_loss': np.True_, 'focal_gamma': 4.728209075982194, 'label_smoothing': 0.1657181659916812, 'compute_class_weights': np.False_, 'aug_prob': 0.029112729494307217, 'aug_jitter_std': 0.03934222689137367, 'aug_scale_low': 0.9828464767538027, 'aug_scale_high': 1.1718655251025405, 'aug_drift_max_amp': 0.16551015259966428, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(710142)} -> 0.9699
2025-10-03 00:24:43,124 - INFO - bo.run_bo - üîçBO Trial 49: Using RF surrogate + Expected Improvement
2025-10-03 00:24:43,124 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:24:43,125 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:24:43,125 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:24:43,125 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008388025036636128, 'batch_size': 64, 'epochs': 32, 'head_dim': 9, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.2510384004022538, 'weight_decay': 0.0013523811364288803, 'grad_clip_norm': 1.837699375835483, 'use_focal_loss': True, 'focal_gamma': 2.145826074269606, 'label_smoothing': 0.15873609096258753, 'compute_class_weights': False, 'aug_prob': 0.395226417907234, 'aug_jitter_std': 0.04845499315860017, 'aug_scale_low': 0.8048352413449764, 'aug_scale_high': 1.0206767549116373, 'aug_drift_max_amp': 0.06562934452923656, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 764755}
2025-10-03 00:24:43,126 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008388025036636128, 'batch_size': 64, 'epochs': 32, 'head_dim': 9, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.2510384004022538, 'weight_decay': 0.0013523811364288803, 'grad_clip_norm': 1.837699375835483, 'use_focal_loss': True, 'focal_gamma': 2.145826074269606, 'label_smoothing': 0.15873609096258753, 'compute_class_weights': False, 'aug_prob': 0.395226417907234, 'aug_jitter_std': 0.04845499315860017, 'aug_scale_low': 0.8048352413449764, 'aug_scale_high': 1.0206767549116373, 'aug_drift_max_amp': 0.06562934452923656, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 764755}
2025-10-03 00:25:42,632 - INFO - _models.training_function_executor - Model: 18,741 parameters, 80.5KB storage
2025-10-03 00:25:42,632 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.31802250197773363, 0.1961096660410391, 0.17839149470995555, 0.16915405023831032, 0.16434189246256348, 0.15967604165374383, 0.15616320127424835, 0.15500562394452871, 0.15218805606611369, 0.1510923853694797, 0.14972273593289032, 0.14895445778622862, 0.14754809195940885, 0.14671464947438592, 0.14604107393595717, 0.14493734935564687, 0.14443115752511382, 0.14252080263449346, 0.14300810432767738, 0.14195288997023894, 0.14046030978189988, 0.14059322082056824, 0.13982326185765268, 0.1396141226580764, 0.13875865538298843, 0.13813423098843935, 0.1377346375456797, 0.13733173108881727, 0.13691986934728978, 0.13638056094795525, 0.13635556635641757, 0.1366542651019359], 'val_losses': [0.21619001022706785, 0.1791339246305274, 0.17077897350346888, 0.16313217354401835, 0.15753756267484337, 0.16197197971980593, 0.15125317373433786, 0.15641653033078665, 0.14910678016207388, 0.14550793737214157, 0.1585567937509232, 0.14375513748161597, 0.1425649110475678, 0.14336901799342847, 0.14026822175892684, 0.14893443616779956, 0.13938423548705475, 0.1464996475886468, 0.14538495274149316, 0.14372814662988853, 0.14229372062527465, 0.1375160771154279, 0.1471335056834525, 0.14008377392725782, 0.13551628782499436, 0.13643255908228433, 0.13566434962560384, 0.13532486363784246, 0.13732787826883958, 0.13740683071685905, 0.1388103139010636, 0.13617103230854635], 'val_acc': [0.9320887106878837, 0.9561458463851648, 0.9605312617466483, 0.9602806665831349, 0.9626613206365117, 0.9639142964540784, 0.9651672722716451, 0.9688009021425886, 0.9676732239067786, 0.9714321513594788, 0.9685503069790753, 0.9726851271770455, 0.9716827465229921, 0.9724345320135321, 0.9746898884851523, 0.9704297707054254, 0.9734369126675855, 0.9716827465229921, 0.9711815561959655, 0.9741886981581256, 0.9744392933216389, 0.9751910788121789, 0.9715574489412354, 0.9734369126675855, 0.9758175667209623, 0.9776970304473124, 0.9748151860669089, 0.9745645909033955, 0.9754416739756923, 0.9788247086831224, 0.9739381029946123, 0.9769452449567724], 'config': {'seq_len': 1000, 'patch_size': 8, 'head_dim': 9, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 5, 'ds_kernel2': 7, 'dropout': 0.2510384004022538, 'num_classes': 5, 'epochs': 32, 'batch_size': 64, 'lr': 0.0008388025036636128, 'weight_decay': 0.0013523811364288803, 'grad_clip_norm': 1.837699375835483, 'use_focal_loss': True, 'focal_gamma': 2.145826074269606, 'label_smoothing': 0.15873609096258753, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.395226417907234, 'aug_jitter_std': 0.04845499315860017, 'aug_scale_low': 0.8048352413449764, 'aug_scale_high': 1.0206767549116373, 'aug_drift_max_amp': 0.06562934452923656, 'normalize': True}, 'quantization': {'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 764755}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008388025036636128, 'batch_size': 64, 'epochs': 32, 'head_dim': 9, 'n_heads': 6, 'n_layers': 3, 'd_ff_factor': 1, 'stem_channels': 16, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 8, 'dropout': 0.2510384004022538, 'weight_decay': 0.0013523811364288803, 'grad_clip_norm': 1.837699375835483, 'use_focal_loss': True, 'focal_gamma': 2.145826074269606, 'label_smoothing': 0.15873609096258753, 'compute_class_weights': False, 'aug_prob': 0.395226417907234, 'aug_jitter_std': 0.04845499315860017, 'aug_scale_low': 0.8048352413449764, 'aug_scale_high': 1.0206767549116373, 'aug_drift_max_amp': 0.06562934452923656, 'normalize': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'seed': 764755}, 'model_parameter_count': 18741, 'model_storage_size_kb': 80.52773437500001, 'model_size_validation': 'PASS'}
2025-10-03 00:25:42,632 - INFO - _models.training_function_executor - BO Objective: base=0.9769, size_penalty=0.0000, final=0.9769
2025-10-03 00:25:42,632 - INFO - _models.training_function_executor - Model: 18,741 parameters, 80.5KB (PASS 256KB limit)
2025-10-03 00:25:42,632 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 59.508s
2025-10-03 00:25:42,771 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9769
2025-10-03 00:25:42,771 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.139s
2025-10-03 00:25:42,771 - INFO - bo.run_bo - Recorded observation #49: hparams={'lr': 0.0008388025036636128, 'batch_size': np.int64(64), 'epochs': np.int64(32), 'head_dim': np.int64(9), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.2510384004022538, 'weight_decay': 0.0013523811364288803, 'grad_clip_norm': 1.837699375835483, 'use_focal_loss': np.True_, 'focal_gamma': 2.145826074269606, 'label_smoothing': 0.15873609096258753, 'compute_class_weights': np.False_, 'aug_prob': 0.395226417907234, 'aug_jitter_std': 0.04845499315860017, 'aug_scale_low': 0.8048352413449764, 'aug_scale_high': 1.0206767549116373, 'aug_drift_max_amp': 0.06562934452923656, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(764755)}, value=0.9769
2025-10-03 00:25:42,771 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 49: {'lr': 0.0008388025036636128, 'batch_size': np.int64(64), 'epochs': np.int64(32), 'head_dim': np.int64(9), 'n_heads': np.int64(6), 'n_layers': np.int64(3), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(8), 'dropout': 0.2510384004022538, 'weight_decay': 0.0013523811364288803, 'grad_clip_norm': 1.837699375835483, 'use_focal_loss': np.True_, 'focal_gamma': 2.145826074269606, 'label_smoothing': 0.15873609096258753, 'compute_class_weights': np.False_, 'aug_prob': 0.395226417907234, 'aug_jitter_std': 0.04845499315860017, 'aug_scale_low': 0.8048352413449764, 'aug_scale_high': 1.0206767549116373, 'aug_drift_max_amp': 0.06562934452923656, 'normalize': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(764755)} -> 0.9769
2025-10-03 00:25:42,772 - INFO - bo.run_bo - üîçBO Trial 50: Using RF surrogate + Expected Improvement
2025-10-03 00:25:42,772 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-03 00:25:42,772 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:25:42,772 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:25:42,773 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008644847148868506, 'batch_size': 128, 'epochs': 37, 'head_dim': 11, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'patch_size': 5, 'dropout': 0.4114951921965436, 'weight_decay': 0.0006424434510541514, 'grad_clip_norm': 3.6618702663420524, 'use_focal_loss': True, 'focal_gamma': 1.468107762297165, 'label_smoothing': 0.19176275787223926, 'compute_class_weights': False, 'aug_prob': 0.8439216395938745, 'aug_jitter_std': 0.04562363098495353, 'aug_scale_low': 0.8014127998750344, 'aug_scale_high': 1.0218437832153726, 'aug_drift_max_amp': 0.12251936699576238, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 138116}
2025-10-03 00:25:42,774 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008644847148868506, 'batch_size': 128, 'epochs': 37, 'head_dim': 11, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'patch_size': 5, 'dropout': 0.4114951921965436, 'weight_decay': 0.0006424434510541514, 'grad_clip_norm': 3.6618702663420524, 'use_focal_loss': True, 'focal_gamma': 1.468107762297165, 'label_smoothing': 0.19176275787223926, 'compute_class_weights': False, 'aug_prob': 0.8439216395938745, 'aug_jitter_std': 0.04562363098495353, 'aug_scale_low': 0.8014127998750344, 'aug_scale_high': 1.0218437832153726, 'aug_drift_max_amp': 0.12251936699576238, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 138116}
2025-10-03 00:26:56,089 - INFO - _models.training_function_executor - Model: 8,565 parameters, 18.4KB storage
2025-10-03 00:26:56,089 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5612363622861709, 0.409124358520843, 0.36392181827281644, 0.344499800391235, 0.33412793132590607, 0.3264696980853601, 0.32145005873923826, 0.3165753821686904, 0.312718310607793, 0.30977993074975974, 0.3074296559032535, 0.3048573788735658, 0.30263810731566043, 0.3001029044665128, 0.29873219844643173, 0.297508754326512, 0.29669727275303065, 0.2958732518713094, 0.29446124044482025, 0.29364973082692686, 0.2930961803651613, 0.29170443799917445, 0.2915170038913017, 0.2905642676894178, 0.2902113129995866, 0.28937962058156375, 0.28853300916324437, 0.2877111040036847, 0.28831987718111873, 0.28686448305269246, 0.2870553632670617, 0.28619063919066967, 0.2854040218171472, 0.28649962328730777, 0.28563417469697483, 0.28562368112263087, 0.2845860757065556], 'val_losses': [0.4311262036554348, 0.3869710922271084, 0.3428046143961616, 0.3357088548835993, 0.37550332503365985, 0.3121141129127945, 0.3265218803972517, 0.31338954750808345, 0.30305791143530997, 0.301585559139364, 0.30078015554415793, 0.2929605636257917, 0.29662530648276675, 0.2937062487702608, 0.2942080060845105, 0.2867536992898876, 0.2991147934905032, 0.2913428520278037, 0.2855089699403488, 0.2938356295539807, 0.28566622746765963, 0.2866329641856581, 0.2903733595005179, 0.287918132489792, 0.2830797023581108, 0.2960036563859848, 0.2835325419701157, 0.28211754755393675, 0.28519197625558845, 0.2985426904847188, 0.2909205306025917, 0.2826412608264727, 0.28159918439760195, 0.2903796281899415, 0.28174910491129135, 0.28611631017626865, 0.28226174410237], 'val_acc': [0.8813431900764315, 0.9163012153865431, 0.9421125172284175, 0.9452449567723343, 0.9270768074176169, 0.9556446560581381, 0.9568976318757048, 0.9582759052750282, 0.9584012028567849, 0.9605312617466483, 0.9557699536398947, 0.9596541786743515, 0.9626613206365117, 0.9641648916175918, 0.9616589399824583, 0.9624107254729983, 0.9640395940358351, 0.9660443553439418, 0.9661696529256986, 0.9666708432527252, 0.963287808545295, 0.9651672722716451, 0.9650419746898885, 0.9661696529256986, 0.9671720335797519, 0.9637889988723217, 0.9706803658689387, 0.9679238190702919, 0.9689261997243453, 0.9674226287432652, 0.9615336424007017, 0.9713068537777221, 0.9680491166520486, 0.9674226287432652, 0.967547926325022, 0.9715574489412354, 0.9704297707054254], 'config': {'seq_len': 1000, 'patch_size': 5, 'head_dim': 11, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'dropout': 0.4114951921965436, 'num_classes': 5, 'epochs': 37, 'batch_size': 128, 'lr': 0.0008644847148868506, 'weight_decay': 0.0006424434510541514, 'grad_clip_norm': 3.6618702663420524, 'use_focal_loss': True, 'focal_gamma': 1.468107762297165, 'label_smoothing': 0.19176275787223926, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.8439216395938745, 'aug_jitter_std': 0.04562363098495353, 'aug_scale_low': 0.8014127998750344, 'aug_scale_high': 1.0218437832153726, 'aug_drift_max_amp': 0.12251936699576238, 'normalize': True}, 'quantization': {'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'seed': 138116}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008644847148868506, 'batch_size': 128, 'epochs': 37, 'head_dim': 11, 'n_heads': 2, 'n_layers': 1, 'd_ff_factor': 4, 'stem_channels': 16, 'ds_kernel1': 7, 'ds_kernel2': 3, 'patch_size': 5, 'dropout': 0.4114951921965436, 'weight_decay': 0.0006424434510541514, 'grad_clip_norm': 3.6618702663420524, 'use_focal_loss': True, 'focal_gamma': 1.468107762297165, 'label_smoothing': 0.19176275787223926, 'compute_class_weights': False, 'aug_prob': 0.8439216395938745, 'aug_jitter_std': 0.04562363098495353, 'aug_scale_low': 0.8014127998750344, 'aug_scale_high': 1.0218437832153726, 'aug_drift_max_amp': 0.12251936699576238, 'normalize': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 138116}, 'model_parameter_count': 8565, 'model_storage_size_kb': 18.4013671875, 'model_size_validation': 'PASS'}
2025-10-03 00:26:56,089 - INFO - _models.training_function_executor - BO Objective: base=0.9704, size_penalty=0.0000, final=0.9704
2025-10-03 00:26:56,089 - INFO - _models.training_function_executor - Model: 8,565 parameters, 18.4KB (PASS 256KB limit)
2025-10-03 00:26:56,089 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 73.317s
2025-10-03 00:26:56,231 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9704
2025-10-03 00:26:56,231 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.142s
2025-10-03 00:26:56,231 - INFO - bo.run_bo - Recorded observation #50: hparams={'lr': 0.0008644847148868506, 'batch_size': np.int64(128), 'epochs': np.int64(37), 'head_dim': np.int64(11), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(5), 'dropout': 0.4114951921965436, 'weight_decay': 0.0006424434510541514, 'grad_clip_norm': 3.6618702663420524, 'use_focal_loss': np.True_, 'focal_gamma': 1.468107762297165, 'label_smoothing': 0.19176275787223926, 'compute_class_weights': np.False_, 'aug_prob': 0.8439216395938745, 'aug_jitter_std': 0.04562363098495353, 'aug_scale_low': 0.8014127998750344, 'aug_scale_high': 1.0218437832153726, 'aug_drift_max_amp': 0.12251936699576238, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(138116)}, value=0.9704
2025-10-03 00:26:56,231 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 50: {'lr': 0.0008644847148868506, 'batch_size': np.int64(128), 'epochs': np.int64(37), 'head_dim': np.int64(11), 'n_heads': np.int64(2), 'n_layers': np.int64(1), 'd_ff_factor': np.int64(4), 'stem_channels': np.int64(16), 'ds_kernel1': np.int64(7), 'ds_kernel2': np.int64(3), 'patch_size': np.int64(5), 'dropout': 0.4114951921965436, 'weight_decay': 0.0006424434510541514, 'grad_clip_norm': 3.6618702663420524, 'use_focal_loss': np.True_, 'focal_gamma': 1.468107762297165, 'label_smoothing': 0.19176275787223926, 'compute_class_weights': np.False_, 'aug_prob': 0.8439216395938745, 'aug_jitter_std': 0.04562363098495353, 'aug_scale_low': 0.8014127998750344, 'aug_scale_high': 1.0218437832153726, 'aug_drift_max_amp': 0.12251936699576238, 'normalize': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'seed': np.int64(138116)} -> 0.9704
2025-10-03 00:26:56,231 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.9812
2025-10-03 00:26:56,231 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.0020847889147527126, 'batch_size': np.int64(32), 'epochs': np.int64(49), 'head_dim': np.int64(12), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(20), 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': np.True_, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': np.False_, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(937702)}
2025-10-03 00:26:56,231 - INFO - visualization - Generating BO visualization charts with 50 trials...
2025-10-03 00:26:57,644 - INFO - visualization - BO summary saved to: charts/20251003_002656_BO_CNNTransformerLite1D/bo_summary.txt
2025-10-03 00:26:57,644 - INFO - visualization - BO charts saved to: charts/20251003_002656_BO_CNNTransformerLite1D
2025-10-03 00:26:57,644 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä BO charts saved to: charts/20251003_002656_BO_CNNTransformerLite1D
2025-10-03 00:26:57,738 - INFO - evaluation.code_generation_pipeline_orchestrator - üöÄ STEP 4: Final Training Execution
2025-10-03 00:26:57,738 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-10-03 00:26:57,827 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-03 00:26:57,839 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-03 00:26:57,853 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-03 00:26:57,854 - INFO - _models.training_function_executor - Loaded training function: CNNTransformerLite1D
2025-10-03 00:26:57,854 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-03 00:26:57,854 - INFO - _models.training_function_executor - Loaded training function: CNNTransformerLite1D
2025-10-03 00:26:57,854 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-03 00:26:57,854 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.0020847889147527126, 'batch_size': np.int64(32), 'epochs': np.int64(49), 'head_dim': np.int64(12), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(20), 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': np.True_, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': np.False_, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(937702)}
2025-10-03 00:26:57,854 - INFO - evaluation.code_generation_pipeline_orchestrator - Using test set for final training evaluation
2025-10-03 00:26:57,854 - INFO - _models.training_function_executor - Using device: cuda
2025-10-03 00:26:57,876 - INFO - _models.training_function_executor - Executing training function: CNNTransformerLite1D
2025-10-03 00:26:57,876 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0020847889147527126, 'batch_size': np.int64(32), 'epochs': np.int64(49), 'head_dim': np.int64(12), 'n_heads': np.int64(2), 'n_layers': np.int64(2), 'd_ff_factor': np.int64(1), 'stem_channels': np.int64(12), 'ds_kernel1': np.int64(5), 'ds_kernel2': np.int64(7), 'patch_size': np.int64(20), 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': np.True_, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': np.False_, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'seed': np.int64(937702)}
2025-10-03 00:26:57,877 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0020847889147527126, 'batch_size': 32, 'epochs': 49, 'head_dim': 12, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 20, 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': True, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': False, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 937702}
2025-10-03 00:28:50,195 - INFO - _models.training_function_executor - Model: 10,697 parameters, 46.0KB storage
2025-10-03 00:28:50,195 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2513840381465344, 0.167829799396378, 0.153417198673046, 0.14498622459154084, 0.14003079029661997, 0.1352431394978342, 0.13300933994191497, 0.13048251290486731, 0.12948515791365312, 0.12738792135200216, 0.1266252647819477, 0.12453133361331921, 0.12301000541056646, 0.1228168896476364, 0.12090972826518148, 0.11990806978396061, 0.1198645977851574, 0.11905446126399848, 0.11823349196311465, 0.11770936684815905, 0.11642785205445294, 0.11671926963061643, 0.11506214716382857, 0.11595247033460673, 0.11488894497454405, 0.11447259784939773, 0.11423847028491205, 0.11394767058713014, 0.11331183463932518, 0.11344175197859238, 0.11250988913829936, 0.11228304447153614, 0.11213468371292258, 0.11122373496305493, 0.11124416582328181, 0.1112997048648721, 0.11054370573213221, 0.11037806750730027, 0.1113313147805649, 0.11000890582919408, 0.10938341073757568, 0.10959485852560619, 0.10932730111415231, 0.10913319996862098, 0.1086361789187146, 0.10870366421645417, 0.10813952881451502, 0.10892104757792871, 0.10783686865279458], 'val_losses': [0.16983275989414207, 0.15925308339230262, 0.14122366778915174, 0.1340558445964568, 0.1341546146498869, 0.1254679713938596, 0.12705961633666307, 0.13326793793170974, 0.11938558162338855, 0.11696638548902784, 0.11611672105192516, 0.12067113149465779, 0.1227648978605275, 0.11550072121449073, 0.11249895044828358, 0.11414807971091061, 0.11242053441678825, 0.11719590707837928, 0.11390732233535411, 0.11746987480427955, 0.10958758066578231, 0.10881729181740911, 0.10956040022128682, 0.11120975195164287, 0.1072916621928532, 0.11044426716785845, 0.10766904121371607, 0.1081291000999432, 0.10482777330659078, 0.11566767638450483, 0.10651878583926874, 0.10740600306417955, 0.10953309453343363, 0.10502807135181357, 0.10494274121916253, 0.10326375624160608, 0.10816174222627033, 0.11129525955878458, 0.11058073367488269, 0.10507569048996104, 0.10459145916671113, 0.10719025203983992, 0.10812138797116828, 0.10996473551683099, 0.1036057723240265, 0.1040448958458796, 0.1032439679822661, 0.10496331746746358, 0.10336108573594191], 'val_acc': [0.9516478229492422, 0.9545345200866009, 0.9619116349931842, 0.9595060540453852, 0.9647181460989496, 0.9656001924464758, 0.9687274476786144, 0.9674444711731216, 0.9675246572047149, 0.9695293079945474, 0.9720150749739396, 0.9661614946676289, 0.9720150749739396, 0.9720952610055328, 0.9724961911634993, 0.9695293079945474, 0.9716943308475664, 0.9724160051319061, 0.9708122845000401, 0.9740197257637719, 0.9738593537005854, 0.975382888300858, 0.9737791676689921, 0.9731376794162457, 0.9750621441744848, 0.9754630743324513, 0.9757838184588244, 0.973217865447839, 0.9769064229011306, 0.9708122845000401, 0.9773073530590971, 0.9776280971854703, 0.9745810279849251, 0.9773073530590971, 0.9760243765536044, 0.9761045625851976, 0.9771469809959105, 0.9760243765536044, 0.9767460508379441, 0.9757036324272311, 0.9766658648063508, 0.9739395397321786, 0.9768262368695373, 0.9741800978269586, 0.9779488413118435, 0.9789110736909631, 0.9780290273434368, 0.9793120038489295, 0.9781092133750301], 'config': {'seq_len': 1000, 'patch_size': 20, 'head_dim': 12, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 7, 'dropout': 0.3830472741727743, 'num_classes': 5, 'epochs': 49, 'batch_size': 32, 'lr': 0.0020847889147527126, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': True, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': False, 'augmentations': {'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': True}, 'quantization': {'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'seed': 937702}, 'model_name': 'CNNTransformerLite1D', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0020847889147527126, 'batch_size': 32, 'epochs': 49, 'head_dim': 12, 'n_heads': 2, 'n_layers': 2, 'd_ff_factor': 1, 'stem_channels': 12, 'ds_kernel1': 5, 'ds_kernel2': 7, 'patch_size': 20, 'dropout': 0.3830472741727743, 'weight_decay': 0.0020806453573445224, 'grad_clip_norm': 3.6166822759233823, 'use_focal_loss': True, 'focal_gamma': 1.8941695899522633, 'label_smoothing': 0.11669916000953029, 'compute_class_weights': False, 'aug_prob': 0.034110543115853716, 'aug_jitter_std': 0.0012926873678561826, 'aug_scale_low': 0.9640076779414234, 'aug_scale_high': 1.0197493600216034, 'aug_drift_max_amp': 0.11813635222586436, 'normalize': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'seed': 937702}, 'model_parameter_count': 10697, 'model_storage_size_kb': 45.963671875, 'model_size_validation': 'PASS'}
2025-10-03 00:28:50,196 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final test metrics from training (avoids preprocessing mismatch)
2025-10-03 00:28:50,196 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 0.9781092133750301, 'macro_f1': None}
2025-10-03 00:28:50,349 - INFO - evaluation.code_generation_pipeline_orchestrator - Model and test tensors saved to: trained_models/20251003_002657_CNNTransformerLite1D
2025-10-03 00:28:50,355 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä STEP 5: Performance Analysis
2025-10-03 00:28:50,355 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: CNNTransformerLite1D
2025-10-03 00:28:50,355 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.9812
2025-10-03 00:28:50,355 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: 0.9781
2025-10-03 00:28:50,355 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-10-03 00:28:50,355 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: CNNTransformerLite1D
2025-10-03 00:28:50,355 - INFO - evaluation.code_generation_pipeline_orchestrator - Score: 0.9781
2025-10-03 00:28:50,355 - INFO - __main__ - AI-enhanced training completed!
2025-10-03 00:28:50,355 - INFO - __main__ - Final model achieved: {'acc': 0.9781092133750301, 'macro_f1': None}
2025-10-03 00:28:50,355 - INFO - __main__ - Pipeline completed successfully in single attempt
2025-10-03 00:28:50,355 - INFO - __main__ - Pipeline completed: CNNTransformerLite1D, metrics: {'acc': 0.9781092133750301, 'macro_f1': None}
2025-10-03 00:28:50,355 - INFO - __main__ - Pipeline summary saved to charts/pipeline_summary_20251003_002850.json
2025-10-03 00:28:50,356 - INFO - __main__ - Model saved: trained_models/best_model_CNNTransformerLite1D_20251003_002850.pth, performance: {'acc': 0.9781092133750301, 'macro_f1': None}
2025-10-03 00:28:50,356 - INFO - __main__ - AI-enhanced processing completed successfully
