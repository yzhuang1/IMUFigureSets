2025-09-23 19:00:32,487 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-23 19:00:32,596 - INFO - __main__ - Logging system initialized successfully
2025-09-23 19:00:32,596 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-23 19:00:32,596 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-23 19:00:32,596 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-23 19:00:32,596 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-23 19:00:32,596 - INFO - __main__ - Attempting to load: X.npy
2025-09-23 19:00:32,637 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-23 19:00:32,674 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-23 19:00:32,674 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-23 19:00:32,674 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-23 19:00:32,674 - INFO - __main__ - Flow: Code Generation â†’ BO â†’ Evaluation
2025-09-23 19:00:32,676 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-23 19:00:32,676 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-23 19:00:32,676 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-23 19:00:32,676 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-23 19:00:32,676 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation â†’ JSON Storage â†’ BO â†’ Training Execution â†’ Evaluation
2025-09-23 19:00:32,676 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-23 19:00:32,676 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-23 19:00:32,676 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-23 19:00:32,676 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-23 19:00:32,775 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-23 19:00:32,776 - INFO - class_balancing - Class imbalance analysis:
2025-09-23 19:00:32,776 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-23 19:00:32,776 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-23 19:00:32,776 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-23 19:00:32,776 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-23 19:00:32,776 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-23 19:00:32,776 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-23 19:00:32,776 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-23 19:00:32,776 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-23 19:00:32,947 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-23 19:00:32,948 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ¤– STEP 1: AI Training Code Generation
2025-09-23 19:00:32,948 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-23 19:03:55,882 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Prompt length: 3998 characters
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-23 19:03:55,950 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-23 19:09:42,551 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-23 19:09:42,561 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-23 19:09:42,561 - WARNING - _models.ai_code_generator - Initial JSON parse failed: Expecting ',' delimiter: line 21 column 17 (char 16862), attempting to fix common issues
2025-09-23 19:09:42,562 - INFO - _models.ai_code_generator - Successfully fixed JSON formatting issues
2025-09-23 19:09:42,562 - INFO - _models.ai_code_generator - AI generated training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:09:42,562 - INFO - _models.ai_code_generator - Confidence: 0.90
2025-09-23 19:09:42,562 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-09-23 19:09:42,562 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:09:42,562 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'd_model', 'n_heads', 'num_layers', 'mlp_ratio', 'dropout', 'weight_decay', 'patch_size', 'label_smoothing', 'use_focal_loss', 'focal_gamma', 'grad_clip_norm', 'scheduler', 'class_weights', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-23 19:09:42,562 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.90
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ’¾ STEP 2: Save Training Function to JSON
2025-09-23 19:09:42,563 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_Tiny1D-ViT-SE-Transformer-MITBIH_1758672582.json
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_Tiny1D-ViT-SE-Transformer-MITBIH_1758672582.json
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ” STEP 3: Bayesian Optimization
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:09:42,563 - INFO - evaluation.code_generation_pipeline_orchestrator - ğŸ“¦ Installing dependencies for GPT-generated training code...
2025-09-23 19:09:42,564 - INFO - package_installer - ğŸ” Analyzing GPT-generated code for package dependencies...
2025-09-23 19:09:42,566 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-23 19:09:42,567 - INFO - package_installer - Available packages: {'torch'}
2025-09-23 19:09:42,567 - INFO - package_installer - Missing packages: set()
2025-09-23 19:09:42,567 - INFO - package_installer - âœ… All required packages are already available
2025-09-23 19:09:42,567 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-09-23 19:09:42,567 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-23 19:09:42,567 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-23 19:09:42,567 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'd_model', 'n_heads', 'num_layers', 'mlp_ratio', 'dropout', 'weight_decay', 'patch_size', 'label_smoothing', 'use_focal_loss', 'focal_gamma', 'grad_clip_norm', 'scheduler', 'class_weights', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-23 19:09:42,567 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-23 19:09:42,567 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-23 19:09:42,567 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-23 19:09:42,602 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-23 19:09:42,735 - INFO - bo.run_bo - Converted GPT search space: 20 parameters
2025-09-23 19:09:42,735 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-23 19:09:42,735 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-23 19:09:42,736 - INFO - bo.run_bo - ğŸ”BO Trial 1: Initial random exploration
2025-09-23 19:09:42,736 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:09:42,736 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:09:42,736 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:09:42,736 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': 12, 'd_model': 92, 'n_heads': 3, 'num_layers': 2, 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': 39188, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-23 19:09:42,737 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': 12, 'd_model': 92, 'n_heads': 3, 'num_layers': 2, 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': 39188, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - Model: 10,856 parameters, 46.6KB storage
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.25489993971720476, 0.19067650278050832, 0.17220858917052453, 0.1592238353429326, 0.13939441082300041, 0.1419021197516895, 0.12744052242158002, 0.12366793638876132, 0.12623549760726707, 0.10909633015226468, 0.11421404015274607, 0.10665167654676813], 'val_losses': [0.21870903324514587, 0.17624613041399625, 0.15134156606704008, 0.1344638961919148, 0.13005695043183288, 0.12962597129917125, 0.113186163550749, 0.12376264798459724, 0.10881336247591637, 0.10537854445307733, 0.09983899139891014, 0.10109018617717003], 'val_acc': [0.7403834106001754, 0.8061646410224282, 0.8486405212379401, 0.8585390301967172, 0.8621726600676607, 0.8735747400075179, 0.8887357474000752, 0.8728229545169778, 0.8901140207993986, 0.8986342563588523, 0.8966294950507455, 0.8990101491041222], 'model_size_bytes': 199413, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': 12, 'd_model': 92, 'n_heads': 3, 'num_layers': 2, 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': 39188, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 10856, 'model_storage_size_kb': 46.646875, 'model_size_validation': 'PASS'}
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - BO Objective: base=0.8990, size_penalty=0.0000, final=0.8990
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - Model: 10,856 parameters, 46.6KB (PASS 256KB limit)
2025-09-23 19:10:07,610 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 24.874s
2025-09-23 19:10:07,610 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8990
2025-09-23 19:10:07,610 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-23 19:10:07,610 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': np.int64(12), 'd_model': np.int64(92), 'n_heads': 3, 'num_layers': np.int64(2), 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': np.int64(39188), 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, value=0.8990
2025-09-23 19:10:07,610 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': np.int64(12), 'd_model': np.int64(92), 'n_heads': 3, 'num_layers': np.int64(2), 'mlp_ratio': 1.8899863008405067, 'dropout': 0.029041806084099737, 'weight_decay': 0.0029154431891537584, 'patch_size': 40, 'label_smoothing': 0.14161451555920912, 'use_focal_loss': True, 'focal_gamma': 2.9398197043239893, 'grad_clip_norm': 1.664885281600844, 'scheduler': 'none', 'class_weights': 'none', 'seed': np.int64(39188), 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True} -> 0.8990
2025-09-23 19:10:07,611 - INFO - bo.run_bo - ğŸ”BO Trial 2: Initial random exploration
2025-09-23 19:10:07,611 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:10:07,611 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:10:07,611 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:10:07,611 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': 46, 'd_model': 91, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': 43021, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:10:07,612 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': 46, 'd_model': 91, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': 43021, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - Model: 20,293 parameters, 87.2KB storage
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.1500932466258225, 2.112802879174685, 2.0973590202649404, 2.0528743468934283, 2.0106371819097304, 1.9918745874451151, 1.984209218725907, 1.9709511053195086, 1.9556792418401021, 1.9403461090578784, 1.9300434519032228, 1.9193303539270268, 1.9192023465116792, 1.9031998253412363, 1.8967574969294883, 1.8873678382706778, 1.8853790417495155, 1.8813880054595884, 1.8779873931982634, 1.8740331361613671, 1.8628751037600524, 1.8585746137136072, 1.8497283426594944, 1.854211062864153, 1.8471998022944207, 1.8466184386498117, 1.8376032263138566, 1.8377672057027368, 1.839443708397454, 1.826800062024339, 1.8281496797846108, 1.823567141787665, 1.8192270049451942, 1.8226270880820297, 1.8179370276638094, 1.8157651135225634, 1.8118047988030508, 1.8076471641197154, 1.8067987621217034, 1.7976774092706131, 1.8038279405868176, 1.7961453804750063, 1.7989135244841699, 1.79160667988597, 1.798327983756959, 1.7876412005209854], 'val_losses': [2.095532409918665, 2.069051769439655, 2.0258972375647732, 1.9879878547134562, 1.980980456375774, 1.972381000067115, 1.956298968887855, 1.9238942562077461, 1.9087476630868685, 1.893755261662938, 1.8862539637194826, 1.867320267154286, 1.8567437609400221, 1.8453974532029516, 1.8366496933087597, 1.8249355011335249, 1.8179281500852313, 1.8116441185858865, 1.8050985577381011, 1.7996862507033746, 1.7949920958287284, 1.7888045010747744, 1.7846533663364568, 1.7763447950633453, 1.768478030923046, 1.766736144903623, 1.7645365375128774, 1.7557303018997732, 1.757565603595586, 1.7508552980040595, 1.7510244126099421, 1.7477209429859801, 1.742604725821815, 1.7363454076135805, 1.7450731862922193, 1.73276772049193, 1.730522380722389, 1.727269389160055, 1.724862581147181, 1.724137035143314, 1.7215917497451347, 1.717771629016959, 1.7085448488408261, 1.7128316829294479, 1.7052855918527232, 1.7058867993263385], 'val_acc': [0.10349580253101115, 0.08181932088710688, 0.09986217266006767, 0.1055005638391179, 0.10161633880466107, 0.1032452073674978, 0.1032452073674978, 0.1086330033830347, 0.10499937351209121, 0.10662824207492795, 0.11013657436411477, 0.10951008645533142, 0.1252975817566721, 0.12680115273775217, 0.13030948502693898, 0.14208745771206616, 0.16551810550056384, 0.15912792883097357, 0.14985590778097982, 0.15073299085327654, 0.16990352086204735, 0.15524370379651672, 0.17053000877083072, 0.18957524119784488, 0.17253477007893747, 0.192206490414735, 0.25297581756672094, 0.20448565342688887, 0.19295827590527503, 0.21024934218769578, 0.2181430898383661, 0.21212880591404587, 0.22165142212755293, 0.22754040847011653, 0.23330409723092344, 0.23505826337551686, 0.2240320761809297, 0.2118782107505325, 0.2411978448815938, 0.2554817692018544, 0.24633504573361734, 0.2698909911038717, 0.2361859416113269, 0.25209873449442427, 0.2557323643653677, 0.24270141586267385], 'model_size_bytes': 229237, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': 46, 'd_model': 91, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': 43021, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 20293, 'model_storage_size_kb': 87.19648437500001, 'model_size_validation': 'PASS'}
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - BO Objective: base=0.2427, size_penalty=0.0000, final=0.2427
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - Model: 20,293 parameters, 87.2KB (PASS 256KB limit)
2025-09-23 19:10:38,158 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 30.548s
2025-09-23 19:10:38,159 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2427
2025-09-23 19:10:38,159 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-23 19:10:38,159 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': np.int64(46), 'd_model': np.int64(91), 'n_heads': 2, 'num_layers': np.int64(2), 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': np.int64(43021), 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, value=0.2427
2025-09-23 19:10:38,159 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 1.1727009450102244e-05, 'batch_size': 128, 'epochs': np.int64(46), 'd_model': np.int64(91), 'n_heads': 2, 'num_layers': np.int64(2), 'mlp_ratio': 1.7265160863320521, 'dropout': 0.3091930046665437, 'weight_decay': 3.387255565852147e-05, 'patch_size': 100, 'label_smoothing': 0.09335257864959601, 'use_focal_loss': False, 'focal_gamma': 2.3606150771755594, 'grad_clip_norm': 0.9009985039390862, 'scheduler': 'none', 'class_weights': 'auto', 'seed': np.int64(43021), 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True} -> 0.2427
2025-09-23 19:10:38,159 - INFO - bo.run_bo - ğŸ”BO Trial 3: Initial random exploration
2025-09-23 19:10:38,159 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-23 19:10:38,159 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:10:38,159 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:10:38,159 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': 11, 'd_model': 75, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': 37065, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:10:38,161 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': 11, 'd_model': 75, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': 37065, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - Model: 10,275 parameters, 44.2KB storage
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4018279313434318, 0.23614479681016642, 0.19631894869842548, 0.16708861271570197, 0.1472443863569792, 0.12918425497158098, 0.11135982659004746, 0.09408311714894849, 0.08428363786484071, 0.07432211902915276, 0.07300470031512447], 'val_losses': [0.2602476288506507, 0.16657838058354427, 0.16082730029343573, 0.1483654274737427, 0.11402542149287957, 0.10026475431204081, 0.0957042981833481, 0.08266359348120345, 0.0698509370599373, 0.06935254825763858, 0.06816789332267083], 'val_acc': [0.7893747650670342, 0.8602931963413106, 0.8759553940608946, 0.884225034456835, 0.915549429896003, 0.9196842500939731, 0.9233178799649167, 0.9360982333040972, 0.9485026938980078, 0.9473750156621977, 0.9483773963162511], 'model_size_bytes': 205881, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': 11, 'd_model': 75, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': 37065, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 10275, 'model_storage_size_kb': 44.15039062500001, 'model_size_validation': 'PASS'}
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - BO Objective: base=0.9484, size_penalty=0.0000, final=0.9484
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - Model: 10,275 parameters, 44.2KB (PASS 256KB limit)
2025-09-23 19:10:53,118 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 14.959s
2025-09-23 19:10:53,203 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9484
2025-09-23 19:10:53,203 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-09-23 19:10:53,203 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': np.int64(11), 'd_model': np.int64(75), 'n_heads': 2, 'num_layers': np.int64(3), 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': np.int64(37065), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, value=0.9484
2025-09-23 19:10:53,203 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.001129013355909267, 'batch_size': 64, 'epochs': np.int64(11), 'd_model': np.int64(75), 'n_heads': 2, 'num_layers': np.int64(3), 'mlp_ratio': 1.9334116337694303, 'dropout': 0.19553030378662045, 'weight_decay': 5.357280069601835e-06, 'patch_size': 50, 'label_smoothing': 0.08503117489824896, 'use_focal_loss': True, 'focal_gamma': 2.135400655639983, 'grad_clip_norm': 0.06262658491111718, 'scheduler': 'onecycle', 'class_weights': 'none', 'seed': np.int64(37065), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False} -> 0.9484
2025-09-23 19:10:53,203 - INFO - bo.run_bo - ğŸ”BO Trial 4: Using RF surrogate + Expected Improvement
2025-09-23 19:10:53,203 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:10:53,203 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:10:53,203 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:10:53,203 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.842797040686455e-05, 'batch_size': 64, 'epochs': 35, 'd_model': 36, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': 100, 'label_smoothing': 0.04065048351171404, 'use_focal_loss': True, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 34569, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:10:53,204 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.842797040686455e-05, 'batch_size': 64, 'epochs': 35, 'd_model': 36, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': 100, 'label_smoothing': 0.04065048351171404, 'use_focal_loss': True, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 34569, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - Model: 46,321 parameters, 199.0KB storage
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.15767041525849573, 0.14616018050106266, 0.1389311932112995, 0.1333474838427089, 0.12963859027103572, 0.1252417524818101, 0.1215065193470157, 0.1193185193184731, 0.11662642792214077, 0.11290705654118621, 0.11056507623388337, 0.10704908485044473, 0.1034095215018081, 0.1022642092663808, 0.10037222819368563, 0.09870833570827119, 0.09567042898371662, 0.09546559434628586, 0.09299017942857687, 0.09258078671500534, 0.09045722395162051, 0.08824532288669722, 0.08699650520995211, 0.08537494567063983, 0.08436709075010394, 0.08327295449147662, 0.08190273695085908, 0.08266386755644721, 0.07967210366259567, 0.07892636184671677, 0.07929298308317176, 0.07824130221679482, 0.07677836464239822, 0.07556479323354408, 0.07567030588841822], 'val_losses': [0.14489994995700223, 0.1372329736017125, 0.1296484014529316, 0.1236207095768208, 0.11950206941666049, 0.11542725467174786, 0.11310341750672924, 0.11072972302005682, 0.10862877832784223, 0.10650023672197147, 0.10397165430301997, 0.10099560703978161, 0.09875633772063518, 0.09628058232151392, 0.09497356113028203, 0.093659577856923, 0.09215870446072397, 0.0906902638082777, 0.08925786165758835, 0.08759404240296742, 0.08547395726045284, 0.08420684236471604, 0.08323384095471113, 0.08098939327387049, 0.08055997367020822, 0.07787574320922347, 0.07680459707707461, 0.07642832227716873, 0.07430307711584871, 0.072861900637826, 0.07287679730565973, 0.07220860632842921, 0.07101894565003175, 0.06987220781966492, 0.0697316282709213], 'val_acc': [0.024057135697281042, 0.02806665831349455, 0.029570229294574615, 0.031324395439168026, 0.0323267760932214, 0.03295326400200476, 0.03320385916551811, 0.03658689387294825, 0.044480641523618594, 0.059015161007392555, 0.06227289813306603, 0.07179551434657311, 0.07730860794386668, 0.08269640395940359, 0.0875830096479138, 0.08958777095602055, 0.0943490790627741, 0.09497556697155744, 0.0969803282796642, 0.09998747024182433, 0.09948627991479765, 0.1006139581506077, 0.10211752913168776, 0.10274401704047112, 0.10424758802155118, 0.10186693396817441, 0.10487407593033454, 0.10499937351209121, 0.10449818318506453, 0.1055005638391179, 0.10662824207492795, 0.10362110011276783, 0.11176544292695151, 0.12604936724721214, 0.115399072797895], 'model_size_bytes': 204975, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.842797040686455e-05, 'batch_size': 64, 'epochs': 35, 'd_model': 36, 'n_heads': 3, 'num_layers': 4, 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': 100, 'label_smoothing': 0.04065048351171404, 'use_focal_loss': True, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 34569, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 46321, 'model_storage_size_kb': 199.03554687500002, 'model_size_validation': 'PASS'}
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - BO Objective: base=0.1154, size_penalty=0.0000, final=0.1154
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - Model: 46,321 parameters, 199.0KB (PASS 256KB limit)
2025-09-23 19:11:49,561 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 56.358s
2025-09-23 19:11:49,646 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1154
2025-09-23 19:11:49,646 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.085s
2025-09-23 19:11:49,646 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 1.842797040686455e-05, 'batch_size': np.int64(64), 'epochs': np.int64(35), 'd_model': np.int64(36), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': np.int64(100), 'label_smoothing': 0.04065048351171404, 'use_focal_loss': np.True_, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': np.int64(34569), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.1154
2025-09-23 19:11:49,646 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 1.842797040686455e-05, 'batch_size': np.int64(64), 'epochs': np.int64(35), 'd_model': np.int64(36), 'n_heads': np.int64(3), 'num_layers': np.int64(4), 'mlp_ratio': 1.5804658997023913, 'dropout': 0.37942700336447116, 'weight_decay': 1.768371089113777e-05, 'patch_size': np.int64(100), 'label_smoothing': 0.04065048351171404, 'use_focal_loss': np.True_, 'focal_gamma': 1.356527613017854, 'grad_clip_norm': 1.716059736220723, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': np.int64(34569), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.1154
2025-09-23 19:11:49,646 - INFO - bo.run_bo - ğŸ”BO Trial 5: Using RF surrogate + Expected Improvement
2025-09-23 19:11:49,646 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:11:49,647 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:11:49,647 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:11:49,647 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 6.218414569281548e-05, 'batch_size': 32, 'epochs': 6, 'd_model': 60, 'n_heads': 3, 'num_layers': 1, 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': 100, 'label_smoothing': 0.16645169790002662, 'use_focal_loss': True, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 44158, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:11:49,648 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 6.218414569281548e-05, 'batch_size': 32, 'epochs': 6, 'd_model': 60, 'n_heads': 3, 'num_layers': 1, 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': 100, 'label_smoothing': 0.16645169790002662, 'use_focal_loss': True, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 44158, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-23 19:11:59,145 - INFO - _models.training_function_executor - Model: 13,140 parameters, 14.1KB storage
2025-09-23 19:11:59,146 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4184733569282221, 0.334049597420355, 0.3070350216934844, 0.2863255955977416, 0.2730059827656404, 0.26300052871038804], 'val_losses': [0.34017408296210083, 0.31310684578566755, 0.28217635433030985, 0.25060428626431125, 0.22825787404253137, 0.2178835974614836], 'val_acc': [0.7675729858413732, 0.7804786367623104, 0.7869941110136575, 0.8031574990602681, 0.823079814559579, 0.8341060017541662], 'model_size_bytes': 107569, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 6.218414569281548e-05, 'batch_size': 32, 'epochs': 6, 'd_model': 60, 'n_heads': 3, 'num_layers': 1, 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': 100, 'label_smoothing': 0.16645169790002662, 'use_focal_loss': True, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 44158, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 13140, 'model_storage_size_kb': 14.115234375000002, 'model_size_validation': 'PASS'}
2025-09-23 19:11:59,146 - INFO - _models.training_function_executor - BO Objective: base=0.8341, size_penalty=0.0000, final=0.8341
2025-09-23 19:11:59,146 - INFO - _models.training_function_executor - Model: 13,140 parameters, 14.1KB (PASS 256KB limit)
2025-09-23 19:11:59,146 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 9.499s
2025-09-23 19:11:59,230 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8341
2025-09-23 19:11:59,230 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-09-23 19:11:59,230 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 6.218414569281548e-05, 'batch_size': np.int64(32), 'epochs': np.int64(6), 'd_model': np.int64(60), 'n_heads': np.int64(3), 'num_layers': np.int64(1), 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': np.int64(100), 'label_smoothing': 0.16645169790002662, 'use_focal_loss': np.True_, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(44158), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.8341
2025-09-23 19:11:59,230 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 6.218414569281548e-05, 'batch_size': np.int64(32), 'epochs': np.int64(6), 'd_model': np.int64(60), 'n_heads': np.int64(3), 'num_layers': np.int64(1), 'mlp_ratio': 3.563234320245162, 'dropout': 0.24110522535203832, 'weight_decay': 0.006526789909744681, 'patch_size': np.int64(100), 'label_smoothing': 0.16645169790002662, 'use_focal_loss': np.True_, 'focal_gamma': 1.988780323532577, 'grad_clip_norm': 0.04514990837413558, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(44158), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.8341
2025-09-23 19:11:59,230 - INFO - bo.run_bo - ğŸ”BO Trial 6: Using RF surrogate + Expected Improvement
2025-09-23 19:11:59,230 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:11:59,230 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:11:59,230 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:11:59,230 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00022687383616628082, 'batch_size': 256, 'epochs': 15, 'd_model': 43, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': 50, 'label_smoothing': 0.13151121598021862, 'use_focal_loss': True, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42992, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:11:59,232 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00022687383616628082, 'batch_size': 256, 'epochs': 15, 'd_model': 43, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': 50, 'label_smoothing': 0.13151121598021862, 'use_focal_loss': True, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42992, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - Model: 5,891 parameters, 25.3KB storage
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8403377209253278, 0.6258667548083013, 0.4839695855959747, 0.3857827268561164, 0.33141840789299637, 0.2924884982782621, 0.2661674457498314, 0.2460892591710057, 0.22924609782757535, 0.22009057536801982, 0.20791382570437827, 0.20424145248955483, 0.1996998912715353, 0.19642767790370397, 0.19749426222639574], 'val_losses': [0.6704454867556914, 0.5590170607682144, 0.41072977507059144, 0.33516997183688135, 0.28310740360930187, 0.2431544594773821, 0.22633619276208547, 0.19714266706672084, 0.18249125922626905, 0.1765057689738653, 0.16564461565707772, 0.16262220376312486, 0.15773755699945055, 0.1576868868302827, 0.15757392703471157], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7675729858413732, 0.800150357098108, 0.8240821952136324, 0.8531512341811803, 0.8646786117027941, 0.8893622353088585, 0.8982583636135822, 0.9013908031574991, 0.909033955644656, 0.907906277408846, 0.9125422879338428, 0.9122916927703295, 0.9124169903520862], 'model_size_bytes': 118137, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00022687383616628082, 'batch_size': 256, 'epochs': 15, 'd_model': 43, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': 50, 'label_smoothing': 0.13151121598021862, 'use_focal_loss': True, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': 42992, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 5891, 'model_storage_size_kb': 25.312890625, 'model_size_validation': 'PASS'}
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - BO Objective: base=0.9124, size_penalty=0.0000, final=0.9124
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - Model: 5,891 parameters, 25.3KB (PASS 256KB limit)
2025-09-23 19:12:09,123 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 9.893s
2025-09-23 19:12:09,208 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9124
2025-09-23 19:12:09,209 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.085s
2025-09-23 19:12:09,209 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 0.00022687383616628082, 'batch_size': np.int64(256), 'epochs': np.int64(15), 'd_model': np.int64(43), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.13151121598021862, 'use_focal_loss': np.True_, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(42992), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9124
2025-09-23 19:12:09,209 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 0.00022687383616628082, 'batch_size': np.int64(256), 'epochs': np.int64(15), 'd_model': np.int64(43), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': 3.8039113964502476, 'dropout': 0.15985791576652456, 'weight_decay': 1.4893451274300488e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.13151121598021862, 'use_focal_loss': np.True_, 'focal_gamma': 1.4791000617655423, 'grad_clip_norm': 1.7370939864789101, 'scheduler': np.str_('onecycle'), 'class_weights': np.str_('none'), 'seed': np.int64(42992), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9124
2025-09-23 19:12:09,209 - INFO - bo.run_bo - ğŸ”BO Trial 7: Using RF surrogate + Expected Improvement
2025-09-23 19:12:09,209 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:12:09,209 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:12:09,209 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:12:09,209 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00015566751798448127, 'batch_size': 256, 'epochs': 5, 'd_model': 70, 'n_heads': 2, 'num_layers': 1, 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': 40, 'label_smoothing': 0.08349945433375607, 'use_focal_loss': False, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 54321, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:12:09,210 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00015566751798448127, 'batch_size': 256, 'epochs': 5, 'd_model': 70, 'n_heads': 2, 'num_layers': 1, 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': 40, 'label_smoothing': 0.08349945433375607, 'use_focal_loss': False, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 54321, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:12:11,592 - INFO - _models.training_function_executor - Model: 45,922 parameters, 197.3KB storage
2025-09-23 19:12:11,593 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0648087401798307, 0.905266115729516, 0.8634229612295197, 0.8423048275509555, 0.8254659642918689], 'val_losses': [0.9437481293387318, 0.862746339105093, 0.8478108550534035, 0.8129762783039782, 0.7909928219477614], 'val_acc': [0.7406340057636888, 0.7572985841373262, 0.7668212003508332, 0.7767197093096103, 0.7862423255231175], 'model_size_bytes': 191387, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00015566751798448127, 'batch_size': 256, 'epochs': 5, 'd_model': 70, 'n_heads': 2, 'num_layers': 1, 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': 40, 'label_smoothing': 0.08349945433375607, 'use_focal_loss': False, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 54321, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 45922, 'model_storage_size_kb': 197.32109375000002, 'model_size_validation': 'PASS'}
2025-09-23 19:12:11,593 - INFO - _models.training_function_executor - BO Objective: base=0.7862, size_penalty=0.0000, final=0.7862
2025-09-23 19:12:11,593 - INFO - _models.training_function_executor - Model: 45,922 parameters, 197.3KB (PASS 256KB limit)
2025-09-23 19:12:11,593 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 2.384s
2025-09-23 19:12:11,678 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7862
2025-09-23 19:12:11,678 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.086s
2025-09-23 19:12:11,678 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.00015566751798448127, 'batch_size': np.int64(256), 'epochs': np.int64(5), 'd_model': np.int64(70), 'n_heads': np.int64(2), 'num_layers': np.int64(1), 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': np.int64(40), 'label_smoothing': 0.08349945433375607, 'use_focal_loss': np.False_, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(54321), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7862
2025-09-23 19:12:11,678 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.00015566751798448127, 'batch_size': np.int64(256), 'epochs': np.int64(5), 'd_model': np.int64(70), 'n_heads': np.int64(2), 'num_layers': np.int64(1), 'mlp_ratio': 1.7940276341385704, 'dropout': 0.2948397861084151, 'weight_decay': 0.00017833844049553005, 'patch_size': np.int64(40), 'label_smoothing': 0.08349945433375607, 'use_focal_loss': np.False_, 'focal_gamma': 2.6807981325538437, 'grad_clip_norm': 0.3231966864339298, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(54321), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7862
2025-09-23 19:12:11,679 - INFO - bo.run_bo - ğŸ”BO Trial 8: Using RF surrogate + Expected Improvement
2025-09-23 19:12:11,679 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:12:11,679 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:12:11,679 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:12:11,679 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008105504003066627, 'batch_size': 64, 'epochs': 46, 'd_model': 90, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': 50, 'label_smoothing': 0.1210834125192326, 'use_focal_loss': True, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 38713, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:12:11,680 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008105504003066627, 'batch_size': 64, 'epochs': 46, 'd_model': 90, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': 50, 'label_smoothing': 0.1210834125192326, 'use_focal_loss': True, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 38713, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - Model: 240,581 parameters, 1033.7KB storage
2025-09-23 19:13:16,288 - WARNING - _models.training_function_executor - Model storage 1033.7KB exceeds 256KB limit!
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.19402191426316864, 0.10380616666932858, 0.08323842161195162, 0.0740543108293333, 0.06209955993977108, 0.058803237813369665, 0.0540743937219171, 0.0524321887224373, 0.048090012885446454, 0.04609093761623944, 0.04341533979228224, 0.041409051960848056, 0.040509571708418825, 0.03769778325030905, 0.03750676702865196, 0.03478473435507507, 0.03529374467422127, 0.03312316440543017, 0.031596071257608715, 0.03194808653796992, 0.029779182051760097, 0.0294667512974697, 0.028883320897296214, 0.02682374300175478, 0.025242142850794586, 0.026583471257430144, 0.025032384885494353, 0.02498025796611749, 0.022101100295090315, 0.024823000523156495, 0.023228524828667694, 0.02181378414188539, 0.021685687679880935, 0.019311544874425882, 0.020380833995059196, 0.02047515753484499, 0.019194364171267804, 0.01843690606994611, 0.019655034668590504, 0.01965944219712151, 0.01846216628736621, 0.016063565371459285, 0.01734960416275698, 0.017008920280368496, 0.017059166247323117, 0.015128577917764542], 'val_losses': [0.10498044782003922, 0.09484021347065107, 0.06796667309422613, 0.05482126874532964, 0.062149272068201665, 0.05053795231189992, 0.0484423271302531, 0.048714250493260225, 0.04929099869892765, 0.05149665904506647, 0.04379175399598017, 0.044720863265518315, 0.03989090675822088, 0.048944122162308555, 0.0400333539652221, 0.04094765249902541, 0.04079150805260779, 0.038296187481117225, 0.037811013179238524, 0.037465563073125104, 0.03897783275408458, 0.0372100897770958, 0.03508899121162154, 0.03584909024781981, 0.03972865535627106, 0.03851599444267281, 0.039223804722646624, 0.033822349304058634, 0.039980545445815495, 0.03335312195005549, 0.03549564303334323, 0.03550892116383134, 0.033364330069147366, 0.03884592070725609, 0.03892813040422715, 0.03810818213864446, 0.0359345229272331, 0.03369371062421198, 0.03765217835906753, 0.03537922469898358, 0.03821704734680184, 0.036669804860369005, 0.03603055560255376, 0.03475600990465957, 0.03330556340324238, 0.03676727135002785], 'val_acc': [0.8986342563588523, 0.9142964540784363, 0.9383535897757174, 0.9473750156621977, 0.9451196591905776, 0.9517604310236812, 0.9493797769703045, 0.9515098358601679, 0.9408595414108508, 0.9515098358601679, 0.9561458463851648, 0.9582759052750282, 0.9594035835108382, 0.9517604310236812, 0.9597794762561083, 0.9607818569101616, 0.9566470367121914, 0.9610324520736749, 0.960656559328405, 0.9602806665831349, 0.9641648916175918, 0.9624107254729983, 0.9637889988723217, 0.9660443553439418, 0.953765192331788, 0.9601553690013783, 0.9609071544919183, 0.9645407843628618, 0.9570229294574615, 0.9666708432527252, 0.9672973311615086, 0.9650419746898885, 0.967547926325022, 0.9647913795263752, 0.9680491166520486, 0.9669214384162386, 0.9679238190702919, 0.9674226287432652, 0.9679238190702919, 0.9676732239067786, 0.9670467359979953, 0.9695526876331286, 0.9684250093973187, 0.9666708432527252, 0.970930961032452, 0.968299711815562], 'model_size_bytes': 496147, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008105504003066627, 'batch_size': 64, 'epochs': 46, 'd_model': 90, 'n_heads': 3, 'num_layers': 3, 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': 50, 'label_smoothing': 0.1210834125192326, 'use_focal_loss': True, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': 38713, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 240581, 'model_storage_size_kb': 1033.7464843750001, 'model_size_validation': 'FAIL'}
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - BO Objective: base=0.9683, size_penalty=0.8000, final=0.1683
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - Model: 240,581 parameters, 1033.7KB (FAIL 256KB limit)
2025-09-23 19:13:16,288 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 64.609s
2025-09-23 19:13:16,378 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1683
2025-09-23 19:13:16,378 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.089s
2025-09-23 19:13:16,378 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 0.0008105504003066627, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'd_model': np.int64(90), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.1210834125192326, 'use_focal_loss': np.True_, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(38713), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.1683
2025-09-23 19:13:16,378 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 0.0008105504003066627, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'd_model': np.int64(90), 'n_heads': np.int64(3), 'num_layers': np.int64(3), 'mlp_ratio': 2.647383152291282, 'dropout': 0.1018655373068372, 'weight_decay': 1.3340406579970343e-06, 'patch_size': np.int64(50), 'label_smoothing': 0.1210834125192326, 'use_focal_loss': np.True_, 'focal_gamma': 2.5803587801738423, 'grad_clip_norm': 0.8495328980000474, 'scheduler': np.str_('none'), 'class_weights': np.str_('none'), 'seed': np.int64(38713), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.1683
2025-09-23 19:13:16,378 - INFO - bo.run_bo - ğŸ”BO Trial 9: Using RF surrogate + Expected Improvement
2025-09-23 19:13:16,378 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-23 19:13:16,378 - INFO - _models.training_function_executor - Using device: cuda
2025-09-23 19:13:16,378 - INFO - _models.training_function_executor - Executing training function: Tiny1D-ViT-SE-Transformer-MITBIH
2025-09-23 19:13:16,378 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005127798466822118, 'batch_size': 32, 'epochs': 12, 'd_model': 66, 'n_heads': 4, 'num_layers': 1, 'mlp_ratio': 2.8893049947637337, 'dropout': 0.14583261727127086, 'weight_decay': 1.698922866678694e-06, 'patch_size': 100, 'label_smoothing': 0.14154242924170948, 'use_focal_loss': True, 'focal_gamma': 1.3669523491593882, 'grad_clip_norm': 0.07563102648852206, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 11383, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:13:16,379 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005127798466822118, 'batch_size': 32, 'epochs': 12, 'd_model': 66, 'n_heads': 4, 'num_layers': 1, 'mlp_ratio': 2.8893049947637337, 'dropout': 0.14583261727127086, 'weight_decay': 1.698922866678694e-06, 'patch_size': 100, 'label_smoothing': 0.14154242924170948, 'use_focal_loss': True, 'focal_gamma': 1.3669523491593882, 'grad_clip_norm': 0.07563102648852206, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 11383, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - Model: 57,825 parameters, 248.5KB storage
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.12990208620046406, 0.11292482934619343, 0.11742104445212587, 0.1131646870545448, 0.10649996532210523, 0.1053118714544315, 0.09977679508686144, 0.09466852618867012, 0.09271588285360523, 0.09158203387515394, 0.0916983009147197, 0.08971676959688007], 'val_losses': [0.10005818082930121, 0.10181149605639693, 0.10308305690126407, 0.08786725732006012, 0.08330204460922526, 0.08928685805886002, 0.0813044332539157, 0.08669461998558742, 0.07528372267964803, 0.0722673459228489, 0.07112939409606366, 0.07200135766389566], 'val_acc': [0.08206991605062022, 0.1136449066533016, 0.10938478887357474, 0.11489788247086831, 0.13218894875328907, 0.0799398571607568, 0.17403834106001753, 0.19458714446811176, 0.14897882470868312, 0.15474251346949003, 0.17128179426137075, 0.1458463851647663], 'model_size_bytes': 239195, 'model_name': 'Tiny1D-ViT-SE-Transformer-MITBIH', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005127798466822118, 'batch_size': 32, 'epochs': 12, 'd_model': 66, 'n_heads': 4, 'num_layers': 1, 'mlp_ratio': 2.8893049947637337, 'dropout': 0.14583261727127086, 'weight_decay': 1.698922866678694e-06, 'patch_size': 100, 'label_smoothing': 0.14154242924170948, 'use_focal_loss': True, 'focal_gamma': 1.3669523491593882, 'grad_clip_norm': 0.07563102648852206, 'scheduler': np.str_('none'), 'class_weights': np.str_('auto'), 'seed': 11383, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 57825, 'model_storage_size_kb': 248.46679687500003, 'model_size_validation': 'PASS'}
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - BO Objective: base=0.1458, size_penalty=0.0000, final=0.1458
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - Model: 57,825 parameters, 248.5KB (PASS 256KB limit)
2025-09-23 19:13:35,458 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 19.080s
