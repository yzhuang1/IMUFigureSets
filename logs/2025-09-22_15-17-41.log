2025-09-22 15:17:41,679 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 15:17:41,789 - INFO - __main__ - Logging system initialized successfully
2025-09-22 15:17:41,790 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-22 15:17:41,790 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-22 15:17:41,790 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-22 15:17:41,790 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-22 15:17:41,790 - INFO - __main__ - Attempting to load: X.npy
2025-09-22 15:17:41,832 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-22 15:17:41,872 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-22 15:17:41,872 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-22 15:17:41,872 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-22 15:17:41,873 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-22 15:17:41,874 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-22 15:17:41,874 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-22 15:17:41,875 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-22 15:17:41,875 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-22 15:17:41,875 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-22 15:17:41,875 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-22 15:17:41,875 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-22 15:17:41,875 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-22 15:17:41,875 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-22 15:17:41,976 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-22 15:17:41,976 - INFO - class_balancing - Class imbalance analysis:
2025-09-22 15:17:41,976 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-22 15:17:41,976 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-22 15:17:41,976 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-22 15:17:41,976 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-22 15:17:41,976 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-22 15:17:41,976 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-22 15:17:41,976 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-22 15:17:41,976 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-22 15:17:42,149 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-22 15:17:42,149 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-22 15:17:42,149 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-22 15:17:42,149 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-22 15:17:42,149 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-22 15:17:42,149 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-22 15:17:42,149 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-22 15:20:13,339 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 15:20:13,375 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 15:20:13,375 - INFO - _models.ai_code_generator - Prompt length: 3244 characters
2025-09-22 15:20:13,375 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 15:20:13,376 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 15:20:13,376 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 15:22:28,413 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 15:22:28,413 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 15:22:28,413 - INFO - _models.ai_code_generator - AI generated training function: Tiny1DTransformerRR
2025-09-22 15:22:28,413 - INFO - _models.ai_code_generator - Confidence: 0.90
2025-09-22 15:22:28,414 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.72)
2025-09-22 15:22:28,414 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: Tiny1DTransformerRR
2025-09-22 15:22:28,414 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'weight_decay', 'optimizer', 'scheduler', 'warmup_epochs', 'grad_clip', 'amp', 'patch_size', 'd_model', 'n_heads', 'num_layers', 'mlp_ratio', 'dropout', 'head_hidden', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'class_weight_power', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calib_batches']
2025-09-22 15:22:28,414 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.90
2025-09-22 15:22:28,414 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-22 15:22:28,415 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_Tiny1DTransformerRR_1758572548.json
2025-09-22 15:22:28,415 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_Tiny1DTransformerRR_1758572548.json
2025-09-22 15:22:28,415 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-22 15:22:28,415 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: Tiny1DTransformerRR
2025-09-22 15:22:28,415 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 15:22:28,415 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 15:22:28,415 - WARNING - package_installer - Could not parse code for imports due to syntax error: unexpected character after line continuation character (<unknown>, line 1)
2025-09-22 15:22:28,415 - INFO - package_installer - Extracted imports from code: set()
2025-09-22 15:22:28,415 - INFO - package_installer - ‚úÖ No external packages required
2025-09-22 15:22:28,415 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 15:22:28,415 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 15:22:28,415 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 15:22:28,415 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'optimizer', 'scheduler', 'warmup_epochs', 'grad_clip', 'amp', 'patch_size', 'd_model', 'n_heads', 'num_layers', 'mlp_ratio', 'dropout', 'head_hidden', 'use_focal_loss', 'focal_gamma', 'label_smoothing', 'class_weight_power', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calib_batches']
2025-09-22 15:22:28,415 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 15:22:28,416 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 15:22:28,416 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 15:22:28,451 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 15:22:28,590 - INFO - bo.run_bo - Converted GPT search space: 24 parameters
2025-09-22 15:22:28,590 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 15:22:28,590 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 15:22:28,591 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 15:22:28,591 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:22:28,591 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:22:28,591 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:22:28,591 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'optimizer': 'adamw', 'scheduler': 'none', 'warmup_epochs': 2, 'grad_clip': 0.45924889196586727, 'amp': True, 'patch_size': 25, 'd_model': 34, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.1061695553391381, 'head_hidden': 95, 'use_focal_loss': False, 'focal_gamma': 2.2349630192554333, 'label_smoothing': 0.1223306320976562, 'class_weight_power': 0.007066305219717408, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 3}
2025-09-22 15:22:28,593 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'optimizer': 'adamw', 'scheduler': 'none', 'warmup_epochs': 2, 'grad_clip': 0.45924889196586727, 'amp': True, 'patch_size': 25, 'd_model': 34, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.1061695553391381, 'head_hidden': 95, 'use_focal_loss': False, 'focal_gamma': 2.2349630192554333, 'label_smoothing': 0.1223306320976562, 'class_weight_power': 0.007066305219717408, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 3}
2025-09-22 15:23:08,121 - INFO - _models.training_function_executor - Model parameter count: 39,066
2025-09-22 15:23:08,122 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8232452911836657, 0.7040380469742147, 0.6687816063070104, 0.6447515836302896, 0.6238686657768217, 0.606615052569597, 0.6000849088716412, 0.5972213286772163, 0.593405989581054, 0.5837805096496598, 0.5757737947399819, 0.5679708888967044], 'val_losses': [0.7236211073382159, 0.6958979027355812, 0.6513361322926094, 0.6554565920355983, 0.6061948581665564, 0.6270844186999837, 0.5917333290102249, 0.6048972333243819, 0.5979555688874044, 0.5780475034912104, 0.5876388424785046, 0.6098940316081242], 'val_acc': [0.884600927202105, 0.892244079689262, 0.9186818694399198, 0.9161759178047864, 0.9350958526500438, 0.9280791880716702, 0.9429896003007142, 0.9394812680115274, 0.9386041849392307, 0.9523869189324646, 0.9488785866432777, 0.9354717453953139], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'optimizer': 'adamw', 'scheduler': 'none', 'warmup_epochs': 2, 'grad_clip': 0.45924889196586727, 'amp': True, 'patch_size': 25, 'd_model': 34, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.1061695553391381, 'head_hidden': 95, 'use_focal_loss': False, 'focal_gamma': 2.2349630192554333, 'label_smoothing': 0.1223306320976562, 'class_weight_power': 0.007066305219717408, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 3}, 'model_parameter_count': 39066, 'model_size_validation': 'PASS'}
2025-09-22 15:23:08,122 - INFO - _models.training_function_executor - BO Objective: base=0.9355, size_penalty=0.0000, final=0.9355
2025-09-22 15:23:08,122 - INFO - _models.training_function_executor - Model size: 39,066 parameters (PASS 256K limit)
2025-09-22 15:23:08,122 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 39.531s
2025-09-22 15:23:08,122 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9355
2025-09-22 15:23:08,122 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-22 15:23:08,122 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'optimizer': 'adamw', 'scheduler': 'none', 'warmup_epochs': np.int64(2), 'grad_clip': 0.45924889196586727, 'amp': True, 'patch_size': 25, 'd_model': np.int64(34), 'n_heads': 1, 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.1061695553391381, 'head_hidden': np.int64(95), 'use_focal_loss': False, 'focal_gamma': 2.2349630192554333, 'label_smoothing': 0.1223306320976562, 'class_weight_power': 0.007066305219717408, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': np.int64(3)}, value=0.9355
2025-09-22 15:23:08,122 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'optimizer': 'adamw', 'scheduler': 'none', 'warmup_epochs': np.int64(2), 'grad_clip': 0.45924889196586727, 'amp': True, 'patch_size': 25, 'd_model': np.int64(34), 'n_heads': 1, 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.1061695553391381, 'head_hidden': np.int64(95), 'use_focal_loss': False, 'focal_gamma': 2.2349630192554333, 'label_smoothing': 0.1223306320976562, 'class_weight_power': 0.007066305219717408, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': np.int64(3)} -> 0.9355
2025-09-22 15:23:08,123 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-09-22 15:23:08,123 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:23:08,123 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:23:08,123 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:23:08,123 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00010770935922423236, 'batch_size': 256, 'epochs': 7, 'weight_decay': 0.0027527173929429443, 'optimizer': 'adam', 'scheduler': 'none', 'warmup_epochs': 1, 'grad_clip': 0.9488855372533335, 'amp': False, 'patch_size': 125, 'd_model': 40, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.34163175941272916, 'head_hidden': 75, 'use_focal_loss': True, 'focal_gamma': 1.0687770422304368, 'label_smoothing': 0.18186408041575644, 'class_weight_power': 0.258779981600017, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 10}
2025-09-22 15:23:08,124 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00010770935922423236, 'batch_size': 256, 'epochs': 7, 'weight_decay': 0.0027527173929429443, 'optimizer': 'adam', 'scheduler': 'none', 'warmup_epochs': 1, 'grad_clip': 0.9488855372533335, 'amp': False, 'patch_size': 125, 'd_model': 40, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.34163175941272916, 'head_hidden': 75, 'use_focal_loss': True, 'focal_gamma': 1.0687770422304368, 'label_smoothing': 0.18186408041575644, 'class_weight_power': 0.258779981600017, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 10}
2025-09-22 15:23:10,969 - INFO - _models.training_function_executor - Model parameter count: 20,560
2025-09-22 15:23:10,969 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8352515609910478, 0.6769907127921004, 0.616254146520765, 0.5929746609416309, 0.5788320052925479, 0.5678503856460123, 0.5577407387404335], 'val_losses': [0.5718211962411672, 0.4808858038376573, 0.4456136650630457, 0.43087487771141847, 0.435823536444959, 0.4341515565057429, 0.41144558183532387], 'val_acc': [0.7468988848515223, 0.7427640646535522, 0.7577997744643529, 0.760806916426513, 0.7501566219771958, 0.7448941235434157, 0.7541661445934094], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00010770935922423236, 'batch_size': 256, 'epochs': 7, 'weight_decay': 0.0027527173929429443, 'optimizer': 'adam', 'scheduler': 'none', 'warmup_epochs': 1, 'grad_clip': 0.9488855372533335, 'amp': False, 'patch_size': 125, 'd_model': 40, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.34163175941272916, 'head_hidden': 75, 'use_focal_loss': True, 'focal_gamma': 1.0687770422304368, 'label_smoothing': 0.18186408041575644, 'class_weight_power': 0.258779981600017, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 10}, 'model_parameter_count': 20560, 'model_size_validation': 'PASS'}
2025-09-22 15:23:10,969 - INFO - _models.training_function_executor - BO Objective: base=0.7542, size_penalty=0.0000, final=0.7542
2025-09-22 15:23:10,969 - INFO - _models.training_function_executor - Model size: 20,560 parameters (PASS 256K limit)
2025-09-22 15:23:10,969 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 2.846s
2025-09-22 15:23:10,969 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7542
2025-09-22 15:23:10,969 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-22 15:23:10,969 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 0.00010770935922423236, 'batch_size': 256, 'epochs': np.int64(7), 'weight_decay': 0.0027527173929429443, 'optimizer': 'adam', 'scheduler': 'none', 'warmup_epochs': np.int64(1), 'grad_clip': 0.9488855372533335, 'amp': False, 'patch_size': 125, 'd_model': np.int64(40), 'n_heads': 1, 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.34163175941272916, 'head_hidden': np.int64(75), 'use_focal_loss': True, 'focal_gamma': 1.0687770422304368, 'label_smoothing': 0.18186408041575644, 'class_weight_power': 0.258779981600017, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': np.int64(10)}, value=0.7542
2025-09-22 15:23:10,969 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 0.00010770935922423236, 'batch_size': 256, 'epochs': np.int64(7), 'weight_decay': 0.0027527173929429443, 'optimizer': 'adam', 'scheduler': 'none', 'warmup_epochs': np.int64(1), 'grad_clip': 0.9488855372533335, 'amp': False, 'patch_size': 125, 'd_model': np.int64(40), 'n_heads': 1, 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.34163175941272916, 'head_hidden': np.int64(75), 'use_focal_loss': True, 'focal_gamma': 1.0687770422304368, 'label_smoothing': 0.18186408041575644, 'class_weight_power': 0.258779981600017, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': np.int64(10)} -> 0.7542
2025-09-22 15:23:10,970 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-09-22 15:23:10,970 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:23:10,970 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:23:10,970 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:23:10,970 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003405698171993879, 'batch_size': 32, 'epochs': 22, 'weight_decay': 0.0012604664585649477, 'optimizer': 'adam', 'scheduler': 'cosine', 'warmup_epochs': 5, 'grad_clip': 0.727271995856421, 'amp': True, 'patch_size': 50, 'd_model': 93, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.13567451588694798, 'head_hidden': 84, 'use_focal_loss': False, 'focal_gamma': 2.930510614528276, 'label_smoothing': 0.12140684953733696, 'class_weight_power': 0.2759991820225434, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 9}
2025-09-22 15:23:10,971 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003405698171993879, 'batch_size': 32, 'epochs': 22, 'weight_decay': 0.0012604664585649477, 'optimizer': 'adam', 'scheduler': 'cosine', 'warmup_epochs': 5, 'grad_clip': 0.727271995856421, 'amp': True, 'patch_size': 50, 'd_model': 93, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.13567451588694798, 'head_hidden': 84, 'use_focal_loss': False, 'focal_gamma': 2.930510614528276, 'label_smoothing': 0.12140684953733696, 'class_weight_power': 0.2759991820225434, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 9}
2025-09-22 15:24:24,285 - INFO - _models.training_function_executor - Model parameter count: 108,066
2025-09-22 15:24:24,285 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.3823694142393685, 1.1619181721497944, 1.0875977490796178, 1.071674150680161, 1.0780621753189903, 1.0688575093236756, 1.0722851880386957, 1.0696332060939346, 1.0632992836854191, 1.0561544768916251, 1.0568415592088924, 1.0511097426783884, 1.0536501188645353, 1.0470828165073829, 1.0470815310519885, 1.0423064855421948, 1.0518702259115402, 1.0360269634413644, 1.0409296870183082, 1.0443956825557899, 1.033824414708458, 1.0405175824760289], 'val_losses': [1.1838921248322813, 1.0846904664988983, 1.0446796566677368, 1.0926943650118406, 1.100883489861373, 1.122704419549314, 1.0812648734626966, 1.0643580568686657, 1.1199568206020323, 1.0815963147726442, 1.0796126286094163, 1.1550668160608577, 1.0905513433477452, 1.1030400241321974, 1.1007012668266554, 1.0533194973898656, 1.0496413626627223, 1.0661580266174018, 1.042206118301317, 1.0572530838155967, 1.0557125264907328, 1.0209690863708851], 'val_acc': [0.7710813181305601, 0.8129307104372886, 0.8455080816940233, 0.8269640395940359, 0.807041724094725, 0.7983961909535146, 0.8100488660568851, 0.8346071920811928, 0.8031574990602681, 0.8160631499812053, 0.8099235684751285, 0.7774714948001503, 0.8116777346197218, 0.8086705926575617, 0.8017792256609447, 0.829219396065656, 0.8218268387420122, 0.8262122541034959, 0.8413732614960531, 0.8297205863926826, 0.8295952888109259, 0.8541536148352337], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003405698171993879, 'batch_size': 32, 'epochs': 22, 'weight_decay': 0.0012604664585649477, 'optimizer': 'adam', 'scheduler': 'cosine', 'warmup_epochs': 5, 'grad_clip': 0.727271995856421, 'amp': True, 'patch_size': 50, 'd_model': 93, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.13567451588694798, 'head_hidden': 84, 'use_focal_loss': False, 'focal_gamma': 2.930510614528276, 'label_smoothing': 0.12140684953733696, 'class_weight_power': 0.2759991820225434, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 9}, 'model_parameter_count': 108066, 'model_size_validation': 'PASS'}
2025-09-22 15:24:24,285 - INFO - _models.training_function_executor - BO Objective: base=0.8542, size_penalty=0.0000, final=0.8542
2025-09-22 15:24:24,285 - INFO - _models.training_function_executor - Model size: 108,066 parameters (PASS 256K limit)
2025-09-22 15:24:24,285 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 73.315s
2025-09-22 15:24:24,377 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8542
2025-09-22 15:24:24,377 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-09-22 15:24:24,377 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.0003405698171993879, 'batch_size': 32, 'epochs': np.int64(22), 'weight_decay': 0.0012604664585649477, 'optimizer': 'adam', 'scheduler': 'cosine', 'warmup_epochs': np.int64(5), 'grad_clip': 0.727271995856421, 'amp': True, 'patch_size': 50, 'd_model': np.int64(93), 'n_heads': 2, 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.13567451588694798, 'head_hidden': np.int64(84), 'use_focal_loss': False, 'focal_gamma': 2.930510614528276, 'label_smoothing': 0.12140684953733696, 'class_weight_power': 0.2759991820225434, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': np.int64(9)}, value=0.8542
2025-09-22 15:24:24,377 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.0003405698171993879, 'batch_size': 32, 'epochs': np.int64(22), 'weight_decay': 0.0012604664585649477, 'optimizer': 'adam', 'scheduler': 'cosine', 'warmup_epochs': np.int64(5), 'grad_clip': 0.727271995856421, 'amp': True, 'patch_size': 50, 'd_model': np.int64(93), 'n_heads': 2, 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.13567451588694798, 'head_hidden': np.int64(84), 'use_focal_loss': False, 'focal_gamma': 2.930510614528276, 'label_smoothing': 0.12140684953733696, 'class_weight_power': 0.2759991820225434, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': np.int64(9)} -> 0.8542
2025-09-22 15:24:24,377 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-09-22 15:24:24,377 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:24:24,377 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:24:24,377 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:24:24,377 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0012141307774357368, 'batch_size': 256, 'epochs': 15, 'weight_decay': 0.0009628338128266344, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.4790359851022804, 'amp': False, 'patch_size': 40, 'd_model': 52, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.01322688414000173, 'head_hidden': 101, 'use_focal_loss': True, 'focal_gamma': 1.3391002888687478, 'label_smoothing': 0.024944759861619085, 'class_weight_power': 0.012313961929444676, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 2}
2025-09-22 15:24:24,379 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0012141307774357368, 'batch_size': 256, 'epochs': 15, 'weight_decay': 0.0009628338128266344, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.4790359851022804, 'amp': False, 'patch_size': 40, 'd_model': 52, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.01322688414000173, 'head_hidden': 101, 'use_focal_loss': True, 'focal_gamma': 1.3391002888687478, 'label_smoothing': 0.024944759861619085, 'class_weight_power': 0.012313961929444676, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 2}
2025-09-22 15:24:32,079 - INFO - _models.training_function_executor - Model parameter count: 77,929
2025-09-22 15:24:32,079 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5084555147304588, 0.3141437253162638, 0.24977077926405428, 0.2203084569112462, 0.20421245478319314, 0.189663692215621, 0.17552894978004116, 0.17304025827919717, 0.16852528607136166, 0.16102769405011025, 0.1583994188132794, 0.15351478468308274, 0.1553597029790885, 0.14724389272771585, 0.14675188404300946], 'val_losses': [0.3625893598342357, 0.28514924678657905, 0.24351544256437962, 0.2534775634208369, 0.19521117838473714, 0.18903148832244693, 0.1959069503095272, 0.1995853078996772, 0.17523907078972675, 0.18594022748307795, 0.17468817095632705, 0.15552077680646947, 0.1614674216706029, 0.16578310362312254, 0.1483231823279466], 'val_acc': [0.7955143465731112, 0.8571607567973938, 0.8783360481142714, 0.860919684250094, 0.8986342563588523, 0.9000125297581757, 0.8779601553690014, 0.8927452700162887, 0.9055256233554693, 0.9026437789750658, 0.906402706427766, 0.9161759178047864, 0.9096604435534394, 0.9072797895000626, 0.9189324646034331], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0012141307774357368, 'batch_size': 256, 'epochs': 15, 'weight_decay': 0.0009628338128266344, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.4790359851022804, 'amp': False, 'patch_size': 40, 'd_model': 52, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.01322688414000173, 'head_hidden': 101, 'use_focal_loss': True, 'focal_gamma': 1.3391002888687478, 'label_smoothing': 0.024944759861619085, 'class_weight_power': 0.012313961929444676, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 2}, 'model_parameter_count': 77929, 'model_size_validation': 'PASS'}
2025-09-22 15:24:32,079 - INFO - _models.training_function_executor - BO Objective: base=0.9189, size_penalty=0.0000, final=0.9189
2025-09-22 15:24:32,079 - INFO - _models.training_function_executor - Model size: 77,929 parameters (PASS 256K limit)
2025-09-22 15:24:32,079 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 7.702s
2025-09-22 15:24:32,170 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9189
2025-09-22 15:24:32,170 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.091s
2025-09-22 15:24:32,170 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 0.0012141307774357368, 'batch_size': np.int64(256), 'epochs': np.int64(15), 'weight_decay': 0.0009628338128266344, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.4790359851022804, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(52), 'n_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.01322688414000173, 'head_hidden': np.int64(101), 'use_focal_loss': np.True_, 'focal_gamma': 1.3391002888687478, 'label_smoothing': 0.024944759861619085, 'class_weight_power': 0.012313961929444676, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(2)}, value=0.9189
2025-09-22 15:24:32,170 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 0.0012141307774357368, 'batch_size': np.int64(256), 'epochs': np.int64(15), 'weight_decay': 0.0009628338128266344, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.4790359851022804, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(52), 'n_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.01322688414000173, 'head_hidden': np.int64(101), 'use_focal_loss': np.True_, 'focal_gamma': 1.3391002888687478, 'label_smoothing': 0.024944759861619085, 'class_weight_power': 0.012313961929444676, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(2)} -> 0.9189
2025-09-22 15:24:32,171 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-09-22 15:24:32,171 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:24:32,171 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:24:32,171 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:24:32,171 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 7.175782601869225e-05, 'batch_size': 256, 'epochs': 29, 'weight_decay': 0.00010960056834837765, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.930334384222875, 'amp': False, 'patch_size': 100, 'd_model': 105, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.4704554334023616, 'head_hidden': 119, 'use_focal_loss': True, 'focal_gamma': 2.787862636882167, 'label_smoothing': 0.12022814098653747, 'class_weight_power': 0.9301761223423054, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 3}
2025-09-22 15:24:32,172 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 7.175782601869225e-05, 'batch_size': 256, 'epochs': 29, 'weight_decay': 0.00010960056834837765, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.930334384222875, 'amp': False, 'patch_size': 100, 'd_model': 105, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.4704554334023616, 'head_hidden': 119, 'use_focal_loss': True, 'focal_gamma': 2.787862636882167, 'label_smoothing': 0.12022814098653747, 'class_weight_power': 0.9301761223423054, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 3}
2025-09-22 15:24:42,887 - INFO - _models.training_function_executor - Model parameter count: 84,469
2025-09-22 15:24:42,888 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6794483505521536, 0.5003030645981008, 0.4261227509052544, 0.3951788607698759, 0.3586343466456969, 0.31883070436265015, 0.28673824310000257, 0.27193424376846015, 0.2574607452305734, 0.24931467968121107, 0.2398835032841311, 0.23236727755942357, 0.2240213009656919, 0.2228278554510369, 0.2132011247211294, 0.20946750494242797, 0.20297560205808152, 0.20120490689969686, 0.1976067023809529, 0.19194828009785025, 0.18736996205999557, 0.18571545065468767, 0.18485207209531682, 0.17833418149039085, 0.17344581689348915, 0.17537034041247182, 0.1741085263047381, 0.1702855761610087, 0.16558518884318127], 'val_losses': [0.21889882014293954, 0.2602248208637211, 0.26128641021287524, 0.24966152239571746, 0.25636301512012294, 0.2438521627203711, 0.2345682053137447, 0.2274933539992839, 0.2223066104961777, 0.21545624911837405, 0.21273625139910213, 0.20979480234031583, 0.21041321215616435, 0.20259860541464736, 0.2015993704030036, 0.19161318250765047, 0.1879461393826379, 0.19159488275398842, 0.18942033972317168, 0.1878717653794092, 0.18799551751974786, 0.18780213687977326, 0.1767681854557773, 0.1881454255445427, 0.1841894172868578, 0.17917562275844892, 0.18180460809854737, 0.1794513085311613, 0.1884560547904433], 'val_acc': [0.2626237313619847, 0.0754291442175166, 0.1392056133316627, 0.10988597920060143, 0.10888359854654805, 0.10612705174790127, 0.10600175416614459, 0.1068788372384413, 0.10888359854654805, 0.10988597920060143, 0.11727853652424508, 0.12141335672221526, 0.1251722841749154, 0.13557198346071922, 0.138704423004636, 0.14897882470868312, 0.15837614334043354, 0.15825084575867684, 0.16188447562962036, 0.17666959027690765, 0.1810550056383912, 0.18757047988973813, 0.19195589525122164, 0.1890740508708182, 0.19458714446811176, 0.2029820824458088, 0.20147851146472873, 0.20110261871945873, 0.20135321388297206], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 7.175782601869225e-05, 'batch_size': 256, 'epochs': 29, 'weight_decay': 0.00010960056834837765, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.930334384222875, 'amp': False, 'patch_size': 100, 'd_model': 105, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.4704554334023616, 'head_hidden': 119, 'use_focal_loss': True, 'focal_gamma': 2.787862636882167, 'label_smoothing': 0.12022814098653747, 'class_weight_power': 0.9301761223423054, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 3}, 'model_parameter_count': 84469, 'model_size_validation': 'PASS'}
2025-09-22 15:24:42,888 - INFO - _models.training_function_executor - BO Objective: base=0.2014, size_penalty=0.0000, final=0.2014
2025-09-22 15:24:42,888 - INFO - _models.training_function_executor - Model size: 84,469 parameters (PASS 256K limit)
2025-09-22 15:24:42,888 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 10.717s
2025-09-22 15:24:42,978 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2014
2025-09-22 15:24:42,979 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.091s
2025-09-22 15:24:42,979 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 7.175782601869225e-05, 'batch_size': np.int64(256), 'epochs': np.int64(29), 'weight_decay': 0.00010960056834837765, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.930334384222875, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(105), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.4704554334023616, 'head_hidden': np.int64(119), 'use_focal_loss': np.True_, 'focal_gamma': 2.787862636882167, 'label_smoothing': 0.12022814098653747, 'class_weight_power': 0.9301761223423054, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(3)}, value=0.2014
2025-09-22 15:24:42,979 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 7.175782601869225e-05, 'batch_size': np.int64(256), 'epochs': np.int64(29), 'weight_decay': 0.00010960056834837765, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.930334384222875, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(105), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.4704554334023616, 'head_hidden': np.int64(119), 'use_focal_loss': np.True_, 'focal_gamma': 2.787862636882167, 'label_smoothing': 0.12022814098653747, 'class_weight_power': 0.9301761223423054, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(3)} -> 0.2014
2025-09-22 15:24:42,979 - INFO - bo.run_bo - üîçBO Trial 6: Using RF surrogate + Expected Improvement
2025-09-22 15:24:42,979 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:24:42,979 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:24:42,979 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:24:42,979 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00030650561619712603, 'batch_size': 128, 'epochs': 27, 'weight_decay': 1.8563043531433583e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 1, 'grad_clip': 0.2655352249112632, 'amp': False, 'patch_size': 200, 'd_model': 37, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.02958488928731235, 'head_hidden': 109, 'use_focal_loss': True, 'focal_gamma': 2.0221378906751255, 'label_smoothing': 0.07784834227821896, 'class_weight_power': 0.4235315081452776, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 7}
2025-09-22 15:24:42,980 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00030650561619712603, 'batch_size': 128, 'epochs': 27, 'weight_decay': 1.8563043531433583e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 1, 'grad_clip': 0.2655352249112632, 'amp': False, 'patch_size': 200, 'd_model': 37, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.02958488928731235, 'head_hidden': 109, 'use_focal_loss': True, 'focal_gamma': 2.0221378906751255, 'label_smoothing': 0.07784834227821896, 'class_weight_power': 0.4235315081452776, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 7}
2025-09-22 15:24:59,641 - INFO - _models.training_function_executor - Model parameter count: 53,824
2025-09-22 15:24:59,641 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5015292461149653, 0.31392214635630783, 0.25169429282011274, 0.22285492377093655, 0.19747178255204692, 0.18065396252907193, 0.17369793790601434, 0.15392815385446645, 0.1452085755342298, 0.13697257853538677, 0.12396948127508914, 0.11921955197461397, 0.1121432916218224, 0.10438400129157903, 0.10008690192610706, 0.0922703220215006, 0.09017847926246117, 0.08657770975012194, 0.08154010253710746, 0.07819908371609643, 0.07331653126453512, 0.07048551969157123, 0.06764525387889983, 0.06463732957467472, 0.05995791386791337, 0.06073760064685494, 0.057040537494391315], 'val_losses': [0.2629088054151706, 0.19949935189936188, 0.1922554634012027, 0.15733435583837557, 0.16814415388919254, 0.14235058126443909, 0.13070753470666277, 0.13175333539146988, 0.12142161844241531, 0.11175388363075053, 0.11171770124391511, 0.11720036404307003, 0.10828564215118672, 0.11154327217558097, 0.09980915186586214, 0.11209140142922772, 0.09884529319297639, 0.1021313815552808, 0.09990107237568267, 0.10489940209961404, 0.10047506935200869, 0.09950052189238462, 0.09973848058087473, 0.09504714305725571, 0.08969778675049667, 0.09587196460561545, 0.09632208468568697], 'val_acc': [0.6034331537401328, 0.6337551685252475, 0.5582007267259742, 0.6818694399198095, 0.6712191454704924, 0.7252224032076181, 0.7519107881217892, 0.7516601929582759, 0.7842375642150107, 0.8076682120035084, 0.7962661320636512, 0.7734619721839369, 0.8170655306352588, 0.8105500563839118, 0.8388673098609197, 0.8031574990602681, 0.8569101616338805, 0.8377396316251097, 0.8401202856784864, 0.8473875454203734, 0.8309735622102493, 0.8585390301967172, 0.8686881343190076, 0.8683122415737377, 0.8759553940608946, 0.8654303971933341, 0.8654303971933341], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00030650561619712603, 'batch_size': 128, 'epochs': 27, 'weight_decay': 1.8563043531433583e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 1, 'grad_clip': 0.2655352249112632, 'amp': False, 'patch_size': 200, 'd_model': 37, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.02958488928731235, 'head_hidden': 109, 'use_focal_loss': True, 'focal_gamma': 2.0221378906751255, 'label_smoothing': 0.07784834227821896, 'class_weight_power': 0.4235315081452776, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 7}, 'model_parameter_count': 53824, 'model_size_validation': 'PASS'}
2025-09-22 15:24:59,641 - INFO - _models.training_function_executor - BO Objective: base=0.8654, size_penalty=0.0000, final=0.8654
2025-09-22 15:24:59,641 - INFO - _models.training_function_executor - Model size: 53,824 parameters (PASS 256K limit)
2025-09-22 15:24:59,641 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 16.662s
2025-09-22 15:24:59,733 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8654
2025-09-22 15:24:59,733 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-09-22 15:24:59,733 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 0.00030650561619712603, 'batch_size': np.int64(128), 'epochs': np.int64(27), 'weight_decay': 1.8563043531433583e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(1), 'grad_clip': 0.2655352249112632, 'amp': np.False_, 'patch_size': np.int64(200), 'd_model': np.int64(37), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.02958488928731235, 'head_hidden': np.int64(109), 'use_focal_loss': np.True_, 'focal_gamma': 2.0221378906751255, 'label_smoothing': 0.07784834227821896, 'class_weight_power': 0.4235315081452776, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(7)}, value=0.8654
2025-09-22 15:24:59,733 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 0.00030650561619712603, 'batch_size': np.int64(128), 'epochs': np.int64(27), 'weight_decay': 1.8563043531433583e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(1), 'grad_clip': 0.2655352249112632, 'amp': np.False_, 'patch_size': np.int64(200), 'd_model': np.int64(37), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.02958488928731235, 'head_hidden': np.int64(109), 'use_focal_loss': np.True_, 'focal_gamma': 2.0221378906751255, 'label_smoothing': 0.07784834227821896, 'class_weight_power': 0.4235315081452776, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(7)} -> 0.8654
2025-09-22 15:24:59,733 - INFO - bo.run_bo - üîçBO Trial 7: Using RF surrogate + Expected Improvement
2025-09-22 15:24:59,733 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:24:59,733 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:24:59,734 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:24:59,734 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003644905026186252, 'batch_size': 32, 'epochs': 8, 'weight_decay': 0.00037431743061223054, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.49364985681039353, 'amp': False, 'patch_size': 125, 'd_model': 74, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.026831222978520926, 'head_hidden': 89, 'use_focal_loss': True, 'focal_gamma': 2.6940845201871975, 'label_smoothing': 0.11306276049585415, 'class_weight_power': 0.0870425080523295, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:24:59,735 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003644905026186252, 'batch_size': 32, 'epochs': 8, 'weight_decay': 0.00037431743061223054, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.49364985681039353, 'amp': False, 'patch_size': 125, 'd_model': 74, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.026831222978520926, 'head_hidden': 89, 'use_focal_loss': True, 'focal_gamma': 2.6940845201871975, 'label_smoothing': 0.11306276049585415, 'class_weight_power': 0.0870425080523295, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:25:17,193 - INFO - _models.training_function_executor - Model parameter count: 159,965
2025-09-22 15:25:17,193 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2650531164775967, 0.1391861277190108, 0.09926554502966753, 0.08270479965674599, 0.07279216034552424, 0.061326403664098555, 0.058319005733018334, 0.050125336290608724], 'val_losses': [0.15523716720359515, 0.10697461979652971, 0.08620303651826033, 0.08745041578660825, 0.07656282676149768, 0.07004014619302516, 0.060800846500394336, 0.06169015004934038], 'val_acc': [0.8351083824082195, 0.8658062899386042, 0.9047738378649292, 0.9035208620473625, 0.9359729357223405, 0.9379776970304473, 0.9349705550682872, 0.9495050745520611], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003644905026186252, 'batch_size': 32, 'epochs': 8, 'weight_decay': 0.00037431743061223054, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.49364985681039353, 'amp': False, 'patch_size': 125, 'd_model': 74, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.026831222978520926, 'head_hidden': 89, 'use_focal_loss': True, 'focal_gamma': 2.6940845201871975, 'label_smoothing': 0.11306276049585415, 'class_weight_power': 0.0870425080523295, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 6}, 'model_parameter_count': 159965, 'model_size_validation': 'PASS'}
2025-09-22 15:25:17,193 - INFO - _models.training_function_executor - BO Objective: base=0.9495, size_penalty=0.0000, final=0.9495
2025-09-22 15:25:17,193 - INFO - _models.training_function_executor - Model size: 159,965 parameters (PASS 256K limit)
2025-09-22 15:25:17,193 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 17.460s
2025-09-22 15:25:17,285 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9495
2025-09-22 15:25:17,285 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-09-22 15:25:17,285 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.0003644905026186252, 'batch_size': np.int64(32), 'epochs': np.int64(8), 'weight_decay': 0.00037431743061223054, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.49364985681039353, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(74), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.026831222978520926, 'head_hidden': np.int64(89), 'use_focal_loss': np.True_, 'focal_gamma': 2.6940845201871975, 'label_smoothing': 0.11306276049585415, 'class_weight_power': 0.0870425080523295, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)}, value=0.9495
2025-09-22 15:25:17,285 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.0003644905026186252, 'batch_size': np.int64(32), 'epochs': np.int64(8), 'weight_decay': 0.00037431743061223054, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.49364985681039353, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(74), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.026831222978520926, 'head_hidden': np.int64(89), 'use_focal_loss': np.True_, 'focal_gamma': 2.6940845201871975, 'label_smoothing': 0.11306276049585415, 'class_weight_power': 0.0870425080523295, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)} -> 0.9495
2025-09-22 15:25:17,285 - INFO - bo.run_bo - üîçBO Trial 8: Using RF surrogate + Expected Improvement
2025-09-22 15:25:17,285 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:25:17,286 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:25:17,286 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:25:17,286 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0009085960789705392, 'batch_size': 64, 'epochs': 6, 'weight_decay': 1.0421223600501382e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.15933554625895, 'amp': False, 'patch_size': 20, 'd_model': 42, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.11578455054139716, 'head_hidden': 87, 'use_focal_loss': False, 'focal_gamma': 2.018243853818549, 'label_smoothing': 0.0206156064004204, 'class_weight_power': 0.05342169854597158, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 3}
2025-09-22 15:25:17,287 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0009085960789705392, 'batch_size': 64, 'epochs': 6, 'weight_decay': 1.0421223600501382e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.15933554625895, 'amp': False, 'patch_size': 20, 'd_model': 42, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.11578455054139716, 'head_hidden': 87, 'use_focal_loss': False, 'focal_gamma': 2.018243853818549, 'label_smoothing': 0.0206156064004204, 'class_weight_power': 0.05342169854597158, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 3}
2025-09-22 15:25:27,935 - INFO - _models.training_function_executor - Model parameter count: 73,403
2025-09-22 15:25:27,935 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.917509386881743, 0.588804342050815, 0.43681986912134146, 0.4018954133470603, 0.35871482709145497, 0.3196082538724248], 'val_losses': [0.6979685934948273, 0.4254037847966721, 0.39169598426336094, 0.3780352875278522, 0.34846509974965656, 0.306487791644438], 'val_acc': [0.8014033329156748, 0.907154491918306, 0.914797644405463, 0.9234431775466734, 0.9354717453953139, 0.9427390051372009], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0009085960789705392, 'batch_size': 64, 'epochs': 6, 'weight_decay': 1.0421223600501382e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.15933554625895, 'amp': False, 'patch_size': 20, 'd_model': 42, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.11578455054139716, 'head_hidden': 87, 'use_focal_loss': False, 'focal_gamma': 2.018243853818549, 'label_smoothing': 0.0206156064004204, 'class_weight_power': 0.05342169854597158, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 3}, 'model_parameter_count': 73403, 'model_size_validation': 'PASS'}
2025-09-22 15:25:27,935 - INFO - _models.training_function_executor - BO Objective: base=0.9427, size_penalty=0.0000, final=0.9427
2025-09-22 15:25:27,935 - INFO - _models.training_function_executor - Model size: 73,403 parameters (PASS 256K limit)
2025-09-22 15:25:27,935 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 10.650s
2025-09-22 15:25:28,030 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9427
2025-09-22 15:25:28,030 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-09-22 15:25:28,030 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 0.0009085960789705392, 'batch_size': np.int64(64), 'epochs': np.int64(6), 'weight_decay': 1.0421223600501382e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.15933554625895, 'amp': np.False_, 'patch_size': np.int64(20), 'd_model': np.int64(42), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.11578455054139716, 'head_hidden': np.int64(87), 'use_focal_loss': np.False_, 'focal_gamma': 2.018243853818549, 'label_smoothing': 0.0206156064004204, 'class_weight_power': 0.05342169854597158, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(3)}, value=0.9427
2025-09-22 15:25:28,030 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 0.0009085960789705392, 'batch_size': np.int64(64), 'epochs': np.int64(6), 'weight_decay': 1.0421223600501382e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.15933554625895, 'amp': np.False_, 'patch_size': np.int64(20), 'd_model': np.int64(42), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.11578455054139716, 'head_hidden': np.int64(87), 'use_focal_loss': np.False_, 'focal_gamma': 2.018243853818549, 'label_smoothing': 0.0206156064004204, 'class_weight_power': 0.05342169854597158, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(3)} -> 0.9427
2025-09-22 15:25:28,030 - INFO - bo.run_bo - üîçBO Trial 9: Using RF surrogate + Expected Improvement
2025-09-22 15:25:28,030 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:25:28,031 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:25:28,031 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:25:28,031 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003751817462925709, 'batch_size': 32, 'epochs': 7, 'weight_decay': 1.2222497039955728e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 5, 'grad_clip': 0.41880103949462555, 'amp': False, 'patch_size': 125, 'd_model': 87, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.14478102442863092, 'head_hidden': 121, 'use_focal_loss': True, 'focal_gamma': 1.6157754315993906, 'label_smoothing': 0.07956898423558538, 'class_weight_power': 0.12671247051748205, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:25:28,032 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003751817462925709, 'batch_size': 32, 'epochs': 7, 'weight_decay': 1.2222497039955728e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 5, 'grad_clip': 0.41880103949462555, 'amp': False, 'patch_size': 125, 'd_model': 87, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.14478102442863092, 'head_hidden': 121, 'use_focal_loss': True, 'focal_gamma': 1.6157754315993906, 'label_smoothing': 0.07956898423558538, 'class_weight_power': 0.12671247051748205, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:25:43,945 - INFO - _models.training_function_executor - Model parameter count: 145,725
2025-09-22 15:25:43,945 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.35965199451269825, 0.25160514027733544, 0.20267622673132102, 0.17218642817951438, 0.1531913863991009, 0.12262276907565842, 0.0898987807069036], 'val_losses': [0.22602492726591503, 0.17016301745386175, 0.1721309167189999, 0.12205926050982698, 0.12870871081580465, 0.1098143544998881, 0.06923344189877827], 'val_acc': [0.8218268387420122, 0.8622979576494174, 0.8893622353088585, 0.9119158000250596, 0.908658062899386, 0.9243202606189701, 0.9561458463851648], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003751817462925709, 'batch_size': 32, 'epochs': 7, 'weight_decay': 1.2222497039955728e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 5, 'grad_clip': 0.41880103949462555, 'amp': False, 'patch_size': 125, 'd_model': 87, 'n_heads': 2, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.14478102442863092, 'head_hidden': 121, 'use_focal_loss': True, 'focal_gamma': 1.6157754315993906, 'label_smoothing': 0.07956898423558538, 'class_weight_power': 0.12671247051748205, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 6}, 'model_parameter_count': 145725, 'model_size_validation': 'PASS'}
2025-09-22 15:25:43,945 - INFO - _models.training_function_executor - BO Objective: base=0.9561, size_penalty=0.0000, final=0.9561
2025-09-22 15:25:43,945 - INFO - _models.training_function_executor - Model size: 145,725 parameters (PASS 256K limit)
2025-09-22 15:25:43,945 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 15.914s
2025-09-22 15:25:44,156 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9561
2025-09-22 15:25:44,157 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.212s
2025-09-22 15:25:44,157 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 0.003751817462925709, 'batch_size': np.int64(32), 'epochs': np.int64(7), 'weight_decay': 1.2222497039955728e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.41880103949462555, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(87), 'n_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.14478102442863092, 'head_hidden': np.int64(121), 'use_focal_loss': np.True_, 'focal_gamma': 1.6157754315993906, 'label_smoothing': 0.07956898423558538, 'class_weight_power': 0.12671247051748205, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)}, value=0.9561
2025-09-22 15:25:44,157 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 0.003751817462925709, 'batch_size': np.int64(32), 'epochs': np.int64(7), 'weight_decay': 1.2222497039955728e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.41880103949462555, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(87), 'n_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.14478102442863092, 'head_hidden': np.int64(121), 'use_focal_loss': np.True_, 'focal_gamma': 1.6157754315993906, 'label_smoothing': 0.07956898423558538, 'class_weight_power': 0.12671247051748205, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)} -> 0.9561
2025-09-22 15:25:44,157 - INFO - bo.run_bo - üîçBO Trial 10: Using RF surrogate + Expected Improvement
2025-09-22 15:25:44,157 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:25:44,157 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:25:44,157 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:25:44,157 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0017619398915591968, 'batch_size': 64, 'epochs': 5, 'weight_decay': 0.00010625721161407022, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 1, 'grad_clip': 0.9205131062013047, 'amp': False, 'patch_size': 40, 'd_model': 103, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4144269367509193, 'head_hidden': 69, 'use_focal_loss': False, 'focal_gamma': 2.6710683735387004, 'label_smoothing': 0.07810748386631995, 'class_weight_power': 0.0021144989297475774, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}
2025-09-22 15:25:44,158 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0017619398915591968, 'batch_size': 64, 'epochs': 5, 'weight_decay': 0.00010625721161407022, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 1, 'grad_clip': 0.9205131062013047, 'amp': False, 'patch_size': 40, 'd_model': 103, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4144269367509193, 'head_hidden': 69, 'use_focal_loss': False, 'focal_gamma': 2.6710683735387004, 'label_smoothing': 0.07810748386631995, 'class_weight_power': 0.0021144989297475774, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}
2025-09-22 15:25:51,744 - INFO - _models.training_function_executor - Model parameter count: 105,209
2025-09-22 15:25:51,744 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8780550573869438, 0.7434681614953257, 0.6828507506055341, 0.652223813342588, 0.6339956357829146], 'val_losses': [0.806943942887818, 0.7621176505563791, 0.7616794958828297, 0.7352530753839556, 0.7667599321830663], 'val_acc': [0.7872447061771708, 0.8399949880967297, 0.8592908156872572, 0.8699411101365744, 0.8646786117027941], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0017619398915591968, 'batch_size': 64, 'epochs': 5, 'weight_decay': 0.00010625721161407022, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 1, 'grad_clip': 0.9205131062013047, 'amp': False, 'patch_size': 40, 'd_model': 103, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4144269367509193, 'head_hidden': 69, 'use_focal_loss': False, 'focal_gamma': 2.6710683735387004, 'label_smoothing': 0.07810748386631995, 'class_weight_power': 0.0021144989297475774, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}, 'model_parameter_count': 105209, 'model_size_validation': 'PASS'}
2025-09-22 15:25:51,744 - INFO - _models.training_function_executor - BO Objective: base=0.8647, size_penalty=0.0000, final=0.8647
2025-09-22 15:25:51,744 - INFO - _models.training_function_executor - Model size: 105,209 parameters (PASS 256K limit)
2025-09-22 15:25:51,744 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 7.587s
2025-09-22 15:25:51,844 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8647
2025-09-22 15:25:51,845 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-22 15:25:51,845 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 0.0017619398915591968, 'batch_size': np.int64(64), 'epochs': np.int64(5), 'weight_decay': 0.00010625721161407022, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(1), 'grad_clip': 0.9205131062013047, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(103), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.4144269367509193, 'head_hidden': np.int64(69), 'use_focal_loss': np.False_, 'focal_gamma': 2.6710683735387004, 'label_smoothing': 0.07810748386631995, 'class_weight_power': 0.0021144989297475774, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(6)}, value=0.8647
2025-09-22 15:25:51,845 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 0.0017619398915591968, 'batch_size': np.int64(64), 'epochs': np.int64(5), 'weight_decay': 0.00010625721161407022, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(1), 'grad_clip': 0.9205131062013047, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(103), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.4144269367509193, 'head_hidden': np.int64(69), 'use_focal_loss': np.False_, 'focal_gamma': 2.6710683735387004, 'label_smoothing': 0.07810748386631995, 'class_weight_power': 0.0021144989297475774, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(6)} -> 0.8647
2025-09-22 15:25:51,845 - INFO - bo.run_bo - üîçBO Trial 11: Using RF surrogate + Expected Improvement
2025-09-22 15:25:51,845 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:25:51,845 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:25:51,845 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:25:51,845 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004222130589582689, 'batch_size': 32, 'epochs': 17, 'weight_decay': 1.2067670398949835e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.9048986052839371, 'amp': False, 'patch_size': 20, 'd_model': 38, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.15166407536154805, 'head_hidden': 112, 'use_focal_loss': False, 'focal_gamma': 2.1053037033746005, 'label_smoothing': 0.19369250437704055, 'class_weight_power': 0.1589363336487664, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}
2025-09-22 15:25:51,846 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004222130589582689, 'batch_size': 32, 'epochs': 17, 'weight_decay': 1.2067670398949835e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.9048986052839371, 'amp': False, 'patch_size': 20, 'd_model': 38, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.15166407536154805, 'head_hidden': 112, 'use_focal_loss': False, 'focal_gamma': 2.1053037033746005, 'label_smoothing': 0.19369250437704055, 'class_weight_power': 0.1589363336487664, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}
2025-09-22 15:26:40,739 - INFO - _models.training_function_executor - Model parameter count: 62,157
2025-09-22 15:26:40,739 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.227505871344639, 1.0852071959224854, 1.0224799921413048, 1.0005695208927199, 0.9806312381207366, 0.9652889896909705, 0.9629602307750218, 0.950631831689567, 0.9465150463025799, 0.9433097051964724, 0.9345766520905725, 0.9281891077393724, 0.9269147121760271, 0.9213345301829794, 0.9183324118946309, 0.9187191584517819, 0.9104356231540232], 'val_losses': [1.198957081120977, 1.0378351762186926, 1.0133842238506388, 0.9991123947997452, 1.0221670979829738, 0.9894285813084134, 0.960789226063689, 0.9735096106551939, 0.9577517459796135, 0.9681520196910849, 0.9617249822126656, 0.9515191261787161, 0.95952092703535, 0.9370604567085049, 0.9324918588052311, 0.9389324074116525, 0.9340867835348253], 'val_acc': [0.7710813181305601, 0.9060268136824959, 0.9141711564966796, 0.9102869314622227, 0.8958777095602055, 0.9209372259115399, 0.9357223405588272, 0.9274527001628868, 0.9249467485277534, 0.9270768074176169, 0.9279538904899135, 0.9345946623230171, 0.9164265129682997, 0.9352211502318005, 0.9454955519358477, 0.9411101365743642, 0.9433654930459842], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004222130589582689, 'batch_size': 32, 'epochs': 17, 'weight_decay': 1.2067670398949835e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.9048986052839371, 'amp': False, 'patch_size': 20, 'd_model': 38, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.15166407536154805, 'head_hidden': 112, 'use_focal_loss': False, 'focal_gamma': 2.1053037033746005, 'label_smoothing': 0.19369250437704055, 'class_weight_power': 0.1589363336487664, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}, 'model_parameter_count': 62157, 'model_size_validation': 'PASS'}
2025-09-22 15:26:40,739 - INFO - _models.training_function_executor - BO Objective: base=0.9434, size_penalty=0.0000, final=0.9434
2025-09-22 15:26:40,739 - INFO - _models.training_function_executor - Model size: 62,157 parameters (PASS 256K limit)
2025-09-22 15:26:40,739 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 48.894s
2025-09-22 15:26:40,840 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9434
2025-09-22 15:26:40,840 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.101s
2025-09-22 15:26:40,840 - INFO - bo.run_bo - Recorded observation #11: hparams={'lr': 0.004222130589582689, 'batch_size': np.int64(32), 'epochs': np.int64(17), 'weight_decay': 1.2067670398949835e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.9048986052839371, 'amp': np.False_, 'patch_size': np.int64(20), 'd_model': np.int64(38), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.15166407536154805, 'head_hidden': np.int64(112), 'use_focal_loss': np.False_, 'focal_gamma': 2.1053037033746005, 'label_smoothing': 0.19369250437704055, 'class_weight_power': 0.1589363336487664, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(6)}, value=0.9434
2025-09-22 15:26:40,840 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 11: {'lr': 0.004222130589582689, 'batch_size': np.int64(32), 'epochs': np.int64(17), 'weight_decay': 1.2067670398949835e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.9048986052839371, 'amp': np.False_, 'patch_size': np.int64(20), 'd_model': np.int64(38), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.15166407536154805, 'head_hidden': np.int64(112), 'use_focal_loss': np.False_, 'focal_gamma': 2.1053037033746005, 'label_smoothing': 0.19369250437704055, 'class_weight_power': 0.1589363336487664, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(6)} -> 0.9434
2025-09-22 15:26:40,840 - INFO - bo.run_bo - üîçBO Trial 12: Using RF surrogate + Expected Improvement
2025-09-22 15:26:40,840 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:26:40,840 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:26:40,840 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:26:40,840 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0005708003577056149, 'batch_size': 128, 'epochs': 31, 'weight_decay': 1.7291718164029704e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.09924544779839975, 'amp': True, 'patch_size': 20, 'd_model': 57, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.25172507259400645, 'head_hidden': 112, 'use_focal_loss': False, 'focal_gamma': 2.2865706313585656, 'label_smoothing': 0.18656610844708488, 'class_weight_power': 0.13770887226440168, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 3}
2025-09-22 15:26:40,842 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0005708003577056149, 'batch_size': 128, 'epochs': 31, 'weight_decay': 1.7291718164029704e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.09924544779839975, 'amp': True, 'patch_size': 20, 'd_model': 57, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.25172507259400645, 'head_hidden': 112, 'use_focal_loss': False, 'focal_gamma': 2.2865706313585656, 'label_smoothing': 0.18656610844708488, 'class_weight_power': 0.13770887226440168, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 3}
2025-09-22 15:27:10,756 - INFO - _models.training_function_executor - Model parameter count: 65,824
2025-09-22 15:27:10,756 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.2380143738934164, 1.0531126341231092, 0.9939763347755954, 0.9709538407139479, 0.9516332346488668, 0.943059794721241, 0.9361400982156768, 0.9280077971111237, 0.9184311029046407, 0.9169282867948584, 0.9106518813281624, 0.9091602891932842, 0.9008081927319067, 0.8934927115116084, 0.8904160660161554, 0.88661523663945, 0.8866276865603276, 0.8822806022898602, 0.8824695778349395, 0.8763466126148113, 0.8757375497583929, 0.8733156651993187, 0.870407929341454, 0.869400573165906, 0.8668634927265702, 0.8664316832113694, 0.8641611081975872, 0.8637822957854894, 0.8660071743530569, 0.858028185461688, 0.8590640070197674], 'val_losses': [1.062885450795545, 1.028910284964845, 1.013524051627305, 1.0050695697820033, 0.9829819802427752, 1.0593581036537933, 1.0111179630374896, 0.9935129003228438, 0.9894338186231416, 1.0053916552389135, 1.0031864686637217, 0.9782840712613262, 0.9821093314468614, 0.9882050545468634, 0.9962650162655972, 0.9750363474396473, 0.9643654309034856, 0.9586209831209712, 0.9895953615272483, 0.9664478576089218, 0.9492893714815763, 0.9513347803717372, 0.9561302815593733, 1.007284986660633, 0.9815380259603356, 1.0132665340052915, 0.948295774726906, 0.9620766513046802, 0.9960317588841283, 0.9829707022463673, 1.0140180203897018], 'val_acc': [0.8501440922190202, 0.8946247337426387, 0.9032702668838491, 0.9026437789750658, 0.9170530008770831, 0.8950006264879088, 0.9099110387169528, 0.9062774088460093, 0.907154491918306, 0.8926199724345321, 0.908658062899386, 0.9218143089838366, 0.9199348452574865, 0.9122916927703295, 0.9011402079939858, 0.9213131186568099, 0.9256985340182935, 0.931587520360857, 0.9181806791128931, 0.9275779977446436, 0.9358476381405839, 0.9352211502318005, 0.9332163889236937, 0.9109134193710061, 0.92356847512843, 0.9094098483899261, 0.9354717453953139, 0.9329657937601804, 0.915925322641273, 0.9194336549304598, 0.9115399072797895], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0005708003577056149, 'batch_size': 128, 'epochs': 31, 'weight_decay': 1.7291718164029704e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.09924544779839975, 'amp': True, 'patch_size': 20, 'd_model': 57, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.25172507259400645, 'head_hidden': 112, 'use_focal_loss': False, 'focal_gamma': 2.2865706313585656, 'label_smoothing': 0.18656610844708488, 'class_weight_power': 0.13770887226440168, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 3}, 'model_parameter_count': 65824, 'model_size_validation': 'PASS'}
2025-09-22 15:27:10,756 - INFO - _models.training_function_executor - BO Objective: base=0.9115, size_penalty=0.0000, final=0.9115
2025-09-22 15:27:10,756 - INFO - _models.training_function_executor - Model size: 65,824 parameters (PASS 256K limit)
2025-09-22 15:27:10,756 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 29.916s
2025-09-22 15:27:10,859 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9115
2025-09-22 15:27:10,859 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.103s
2025-09-22 15:27:10,859 - INFO - bo.run_bo - Recorded observation #12: hparams={'lr': 0.0005708003577056149, 'batch_size': np.int64(128), 'epochs': np.int64(31), 'weight_decay': 1.7291718164029704e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.09924544779839975, 'amp': np.True_, 'patch_size': np.int64(20), 'd_model': np.int64(57), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.25172507259400645, 'head_hidden': np.int64(112), 'use_focal_loss': np.False_, 'focal_gamma': 2.2865706313585656, 'label_smoothing': 0.18656610844708488, 'class_weight_power': 0.13770887226440168, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(3)}, value=0.9115
2025-09-22 15:27:10,859 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 12: {'lr': 0.0005708003577056149, 'batch_size': np.int64(128), 'epochs': np.int64(31), 'weight_decay': 1.7291718164029704e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.09924544779839975, 'amp': np.True_, 'patch_size': np.int64(20), 'd_model': np.int64(57), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.25172507259400645, 'head_hidden': np.int64(112), 'use_focal_loss': np.False_, 'focal_gamma': 2.2865706313585656, 'label_smoothing': 0.18656610844708488, 'class_weight_power': 0.13770887226440168, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(3)} -> 0.9115
2025-09-22 15:27:10,860 - INFO - bo.run_bo - üîçBO Trial 13: Using RF surrogate + Expected Improvement
2025-09-22 15:27:10,860 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:27:10,860 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:27:10,860 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:27:10,860 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.3347107220023736e-05, 'batch_size': 32, 'epochs': 7, 'weight_decay': 4.185675996421134e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.6089352977878104, 'amp': False, 'patch_size': 40, 'd_model': 66, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.3421626504706663, 'head_hidden': 108, 'use_focal_loss': False, 'focal_gamma': 1.8082314169566291, 'label_smoothing': 0.0020086741006174473, 'class_weight_power': 0.046537030330614744, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 4}
2025-09-22 15:27:10,861 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.3347107220023736e-05, 'batch_size': 32, 'epochs': 7, 'weight_decay': 4.185675996421134e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.6089352977878104, 'amp': False, 'patch_size': 40, 'd_model': 66, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.3421626504706663, 'head_hidden': 108, 'use_focal_loss': False, 'focal_gamma': 1.8082314169566291, 'label_smoothing': 0.0020086741006174473, 'class_weight_power': 0.046537030330614744, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 4}
2025-09-22 15:27:31,100 - INFO - _models.training_function_executor - Model parameter count: 174,515
2025-09-22 15:27:31,100 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.2782286765491537, 0.9304450196539165, 0.8641285802346887, 0.8113926099837657, 0.7774213267073604, 0.7536226795758796, 0.7258558258677068], 'val_losses': [0.9476394960268176, 0.9650702043712446, 0.8913445751546932, 0.8543551378404866, 0.8734109813001816, 0.8320172336720357, 0.7460770864744949], 'val_acc': [0.7130685377772209, 0.7126926450319508, 0.7356221024934219, 0.7495301340684125, 0.7436411477258489, 0.7594286430271896, 0.7933842876832478], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.3347107220023736e-05, 'batch_size': 32, 'epochs': 7, 'weight_decay': 4.185675996421134e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.6089352977878104, 'amp': False, 'patch_size': 40, 'd_model': 66, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.3421626504706663, 'head_hidden': 108, 'use_focal_loss': False, 'focal_gamma': 1.8082314169566291, 'label_smoothing': 0.0020086741006174473, 'class_weight_power': 0.046537030330614744, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 4}, 'model_parameter_count': 174515, 'model_size_validation': 'PASS'}
2025-09-22 15:27:31,100 - INFO - _models.training_function_executor - BO Objective: base=0.7934, size_penalty=0.0000, final=0.7934
2025-09-22 15:27:31,100 - INFO - _models.training_function_executor - Model size: 174,515 parameters (PASS 256K limit)
2025-09-22 15:27:31,100 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 20.240s
2025-09-22 15:27:31,205 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7934
2025-09-22 15:27:31,205 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-09-22 15:27:31,205 - INFO - bo.run_bo - Recorded observation #13: hparams={'lr': 1.3347107220023736e-05, 'batch_size': np.int64(32), 'epochs': np.int64(7), 'weight_decay': 4.185675996421134e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.6089352977878104, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(66), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.3421626504706663, 'head_hidden': np.int64(108), 'use_focal_loss': np.False_, 'focal_gamma': 1.8082314169566291, 'label_smoothing': 0.0020086741006174473, 'class_weight_power': 0.046537030330614744, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(4)}, value=0.7934
2025-09-22 15:27:31,205 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 13: {'lr': 1.3347107220023736e-05, 'batch_size': np.int64(32), 'epochs': np.int64(7), 'weight_decay': 4.185675996421134e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.6089352977878104, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(66), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.3421626504706663, 'head_hidden': np.int64(108), 'use_focal_loss': np.False_, 'focal_gamma': 1.8082314169566291, 'label_smoothing': 0.0020086741006174473, 'class_weight_power': 0.046537030330614744, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(4)} -> 0.7934
2025-09-22 15:27:31,205 - INFO - bo.run_bo - üîçBO Trial 14: Using RF surrogate + Expected Improvement
2025-09-22 15:27:31,205 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:27:31,205 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:27:31,205 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:27:31,205 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0016244660602971772, 'batch_size': 32, 'epochs': 7, 'weight_decay': 0.00025242740749875756, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.3607770414363837, 'amp': False, 'patch_size': 20, 'd_model': 92, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4664285864879719, 'head_hidden': 105, 'use_focal_loss': False, 'focal_gamma': 1.295564785479071, 'label_smoothing': 0.08134597223589925, 'class_weight_power': 0.6580416719799681, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 2}
2025-09-22 15:27:31,207 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0016244660602971772, 'batch_size': 32, 'epochs': 7, 'weight_decay': 0.00025242740749875756, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.3607770414363837, 'amp': False, 'patch_size': 20, 'd_model': 92, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4664285864879719, 'head_hidden': 105, 'use_focal_loss': False, 'focal_gamma': 1.295564785479071, 'label_smoothing': 0.08134597223589925, 'class_weight_power': 0.6580416719799681, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 2}
2025-09-22 15:27:51,446 - INFO - _models.training_function_executor - Model parameter count: 28,888
2025-09-22 15:27:51,446 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0933072095869836, 0.903227957564742, 0.8442216656472649, 0.8069728997380827, 0.7790700127911688, 0.7542776825213438, 0.7401701908297091], 'val_losses': [1.978592377320789, 1.9419346606278836, 2.199936477690767, 2.2168143932838187, 2.1852440037922727, 2.6824281877061953, 2.3683422826731837], 'val_acc': [0.37288560330785614, 0.37426387670717953, 0.36900137827339935, 0.516226036837489, 0.5092093722591154, 0.4045858914922942, 0.5157248465104624], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0016244660602971772, 'batch_size': 32, 'epochs': 7, 'weight_decay': 0.00025242740749875756, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.3607770414363837, 'amp': False, 'patch_size': 20, 'd_model': 92, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4664285864879719, 'head_hidden': 105, 'use_focal_loss': False, 'focal_gamma': 1.295564785479071, 'label_smoothing': 0.08134597223589925, 'class_weight_power': 0.6580416719799681, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 2}, 'model_parameter_count': 28888, 'model_size_validation': 'PASS'}
2025-09-22 15:27:51,446 - INFO - _models.training_function_executor - BO Objective: base=0.5157, size_penalty=0.0000, final=0.5157
2025-09-22 15:27:51,446 - INFO - _models.training_function_executor - Model size: 28,888 parameters (PASS 256K limit)
2025-09-22 15:27:51,446 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 20.241s
2025-09-22 15:27:51,552 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5157
2025-09-22 15:27:51,552 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.106s
2025-09-22 15:27:51,552 - INFO - bo.run_bo - Recorded observation #14: hparams={'lr': 0.0016244660602971772, 'batch_size': np.int64(32), 'epochs': np.int64(7), 'weight_decay': 0.00025242740749875756, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.3607770414363837, 'amp': np.False_, 'patch_size': np.int64(20), 'd_model': np.int64(92), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.4664285864879719, 'head_hidden': np.int64(105), 'use_focal_loss': np.False_, 'focal_gamma': 1.295564785479071, 'label_smoothing': 0.08134597223589925, 'class_weight_power': 0.6580416719799681, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(2)}, value=0.5157
2025-09-22 15:27:51,552 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 14: {'lr': 0.0016244660602971772, 'batch_size': np.int64(32), 'epochs': np.int64(7), 'weight_decay': 0.00025242740749875756, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.3607770414363837, 'amp': np.False_, 'patch_size': np.int64(20), 'd_model': np.int64(92), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.4664285864879719, 'head_hidden': np.int64(105), 'use_focal_loss': np.False_, 'focal_gamma': 1.295564785479071, 'label_smoothing': 0.08134597223589925, 'class_weight_power': 0.6580416719799681, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(2)} -> 0.5157
2025-09-22 15:27:51,552 - INFO - bo.run_bo - üîçBO Trial 15: Using RF surrogate + Expected Improvement
2025-09-22 15:27:51,553 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:27:51,553 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:27:51,553 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:27:51,553 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 6.865543832902657e-05, 'batch_size': 32, 'epochs': 7, 'weight_decay': 2.1807095646150714e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 1, 'grad_clip': 0.08671924348508687, 'amp': True, 'patch_size': 50, 'd_model': 85, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.04439796049320439, 'head_hidden': 48, 'use_focal_loss': False, 'focal_gamma': 2.812363182083862, 'label_smoothing': 0.036114885108245016, 'class_weight_power': 0.7206662227906225, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}
2025-09-22 15:27:51,554 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 6.865543832902657e-05, 'batch_size': 32, 'epochs': 7, 'weight_decay': 2.1807095646150714e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 1, 'grad_clip': 0.08671924348508687, 'amp': True, 'patch_size': 50, 'd_model': 85, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.04439796049320439, 'head_hidden': 48, 'use_focal_loss': False, 'focal_gamma': 2.812363182083862, 'label_smoothing': 0.036114885108245016, 'class_weight_power': 0.7206662227906225, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}
2025-09-22 15:28:09,731 - INFO - _models.training_function_executor - Model parameter count: 161,464
2025-09-22 15:28:09,732 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1261156867501416, 0.7440035763350891, 0.6229695797552834, 0.5678360570071952, 0.5288366306859875, 0.49319167261291996, 0.47316468360504477], 'val_losses': [1.6169814758171728, 1.5629004210013373, 1.3615736597508301, 1.3109368737546623, 1.2617978744676517, 1.218135962703147, 1.1804315843852207], 'val_acc': [0.19483773963162512, 0.21651422127552938, 0.24132314246335046, 0.31813056008019047, 0.40258113018418745, 0.4345320135321388, 0.47675729858413735], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 6.865543832902657e-05, 'batch_size': 32, 'epochs': 7, 'weight_decay': 2.1807095646150714e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 1, 'grad_clip': 0.08671924348508687, 'amp': True, 'patch_size': 50, 'd_model': 85, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.04439796049320439, 'head_hidden': 48, 'use_focal_loss': False, 'focal_gamma': 2.812363182083862, 'label_smoothing': 0.036114885108245016, 'class_weight_power': 0.7206662227906225, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}, 'model_parameter_count': 161464, 'model_size_validation': 'PASS'}
2025-09-22 15:28:09,732 - INFO - _models.training_function_executor - BO Objective: base=0.4768, size_penalty=0.0000, final=0.4768
2025-09-22 15:28:09,732 - INFO - _models.training_function_executor - Model size: 161,464 parameters (PASS 256K limit)
2025-09-22 15:28:09,732 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 18.179s
2025-09-22 15:28:09,837 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.4768
2025-09-22 15:28:09,837 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-09-22 15:28:09,837 - INFO - bo.run_bo - Recorded observation #15: hparams={'lr': 6.865543832902657e-05, 'batch_size': np.int64(32), 'epochs': np.int64(7), 'weight_decay': 2.1807095646150714e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(1), 'grad_clip': 0.08671924348508687, 'amp': np.True_, 'patch_size': np.int64(50), 'd_model': np.int64(85), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.04439796049320439, 'head_hidden': np.int64(48), 'use_focal_loss': np.False_, 'focal_gamma': 2.812363182083862, 'label_smoothing': 0.036114885108245016, 'class_weight_power': 0.7206662227906225, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)}, value=0.4768
2025-09-22 15:28:09,837 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 15: {'lr': 6.865543832902657e-05, 'batch_size': np.int64(32), 'epochs': np.int64(7), 'weight_decay': 2.1807095646150714e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(1), 'grad_clip': 0.08671924348508687, 'amp': np.True_, 'patch_size': np.int64(50), 'd_model': np.int64(85), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.04439796049320439, 'head_hidden': np.int64(48), 'use_focal_loss': np.False_, 'focal_gamma': 2.812363182083862, 'label_smoothing': 0.036114885108245016, 'class_weight_power': 0.7206662227906225, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)} -> 0.4768
2025-09-22 15:28:09,837 - INFO - bo.run_bo - üîçBO Trial 16: Using RF surrogate + Expected Improvement
2025-09-22 15:28:09,838 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:28:09,838 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:28:09,838 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:28:09,838 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.000579539049730513, 'batch_size': 32, 'epochs': 11, 'weight_decay': 1.192183721852831e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.34319095267119865, 'amp': True, 'patch_size': 100, 'd_model': 117, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.11981765777712292, 'head_hidden': 127, 'use_focal_loss': False, 'focal_gamma': 1.8060485550957943, 'label_smoothing': 0.076318155349116, 'class_weight_power': 0.5005442335708183, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 7}
2025-09-22 15:28:09,839 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.000579539049730513, 'batch_size': 32, 'epochs': 11, 'weight_decay': 1.192183721852831e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.34319095267119865, 'amp': True, 'patch_size': 100, 'd_model': 117, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.11981765777712292, 'head_hidden': 127, 'use_focal_loss': False, 'focal_gamma': 1.8060485550957943, 'label_smoothing': 0.076318155349116, 'class_weight_power': 0.5005442335708183, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 7}
2025-09-22 15:28:38,658 - INFO - _models.training_function_executor - Model parameter count: 89,413
2025-09-22 15:28:38,659 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1582878091966913, 0.9081473205074856, 0.8252283857082569, 0.7751216002134692, 0.746169359857709, 0.7167119054448816, 0.6969880040242432, 0.6832962277927177, 0.6589306973818755, 0.6490803723303824, 0.6392779116888518], 'val_losses': [1.235731677817247, 1.1452661361883731, 1.076512961590175, 1.0309717503873765, 1.0897732127535809, 0.9623696212257125, 0.9658100512702578, 1.0374904017806308, 0.9087938824896077, 0.9707778845794098, 0.8751415632494319], 'val_acc': [0.6719709309610324, 0.7276030572609948, 0.7641899511339432, 0.793258990101491, 0.7793509585265005, 0.8569101616338805, 0.8289688009021426, 0.8035333918055382, 0.8824708683122415, 0.8425009397318631, 0.9008896128304724], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.000579539049730513, 'batch_size': 32, 'epochs': 11, 'weight_decay': 1.192183721852831e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.34319095267119865, 'amp': True, 'patch_size': 100, 'd_model': 117, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.11981765777712292, 'head_hidden': 127, 'use_focal_loss': False, 'focal_gamma': 1.8060485550957943, 'label_smoothing': 0.076318155349116, 'class_weight_power': 0.5005442335708183, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 7}, 'model_parameter_count': 89413, 'model_size_validation': 'PASS'}
2025-09-22 15:28:38,659 - INFO - _models.training_function_executor - BO Objective: base=0.9009, size_penalty=0.0000, final=0.9009
2025-09-22 15:28:38,659 - INFO - _models.training_function_executor - Model size: 89,413 parameters (PASS 256K limit)
2025-09-22 15:28:38,659 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 28.821s
2025-09-22 15:28:38,766 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9009
2025-09-22 15:28:38,766 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.107s
2025-09-22 15:28:38,766 - INFO - bo.run_bo - Recorded observation #16: hparams={'lr': 0.000579539049730513, 'batch_size': np.int64(32), 'epochs': np.int64(11), 'weight_decay': 1.192183721852831e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.34319095267119865, 'amp': np.True_, 'patch_size': np.int64(100), 'd_model': np.int64(117), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.11981765777712292, 'head_hidden': np.int64(127), 'use_focal_loss': np.False_, 'focal_gamma': 1.8060485550957943, 'label_smoothing': 0.076318155349116, 'class_weight_power': 0.5005442335708183, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(7)}, value=0.9009
2025-09-22 15:28:38,766 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 16: {'lr': 0.000579539049730513, 'batch_size': np.int64(32), 'epochs': np.int64(11), 'weight_decay': 1.192183721852831e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.34319095267119865, 'amp': np.True_, 'patch_size': np.int64(100), 'd_model': np.int64(117), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.11981765777712292, 'head_hidden': np.int64(127), 'use_focal_loss': np.False_, 'focal_gamma': 1.8060485550957943, 'label_smoothing': 0.076318155349116, 'class_weight_power': 0.5005442335708183, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(7)} -> 0.9009
2025-09-22 15:28:38,766 - INFO - bo.run_bo - üîçBO Trial 17: Using RF surrogate + Expected Improvement
2025-09-22 15:28:38,766 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:28:38,766 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:28:38,766 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:28:38,766 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0011435887339007592, 'batch_size': 128, 'epochs': 39, 'weight_decay': 2.0669045373468117e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.33405219421740334, 'amp': False, 'patch_size': 100, 'd_model': 72, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.3835497713138072, 'head_hidden': 117, 'use_focal_loss': True, 'focal_gamma': 1.2969750694797475, 'label_smoothing': 0.17408380958289338, 'class_weight_power': 0.1333677322479138, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 3}
2025-09-22 15:28:38,768 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0011435887339007592, 'batch_size': 128, 'epochs': 39, 'weight_decay': 2.0669045373468117e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.33405219421740334, 'amp': False, 'patch_size': 100, 'd_model': 72, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.3835497713138072, 'head_hidden': 117, 'use_focal_loss': True, 'focal_gamma': 1.2969750694797475, 'label_smoothing': 0.17408380958289338, 'class_weight_power': 0.1333677322479138, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 3}
2025-09-22 15:29:06,981 - INFO - _models.training_function_executor - Model parameter count: 64,800
2025-09-22 15:29:06,981 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5105924671561042, 0.3970270262652388, 0.3195829682169532, 0.28389320940902363, 0.2646107047350937, 0.2348839671901267, 0.22449510433887235, 0.20715632676589057, 0.1924739276332537, 0.18789181609901798, 0.17569717010577376, 0.16926443532001434, 0.16343815891667945, 0.15087360344845613, 0.14775090275910385, 0.14531446820547364, 0.13678075589714084, 0.13486991694867304, 0.1297075082132087, 0.1284939419450521, 0.12561440587121248, 0.11985152072326262, 0.1207691053080797, 0.11554380378063055, 0.11173083600439501, 0.11624460052027265, 0.10889611512266656, 0.11036475537303911, 0.10927998985034072, 0.10540735046921176, 0.10397091141275806, 0.10417097816126196, 0.10076872235829155, 0.10415186313671587, 0.09943529125965281, 0.09913932439485364, 0.10269198947547989, 0.09585103065408426, 0.09446193928976866], 'val_losses': [0.3645135533457157, 0.28058190203436695, 0.24332451640667138, 0.21639994512732325, 0.21117768137284113, 0.18538044284211858, 0.1984992337059204, 0.1820350741962191, 0.20075703515823945, 0.16892430799420563, 0.18773579616517355, 0.14785749676253784, 0.12852987516174308, 0.1192336403484139, 0.13345131396877633, 0.11184751242212508, 0.09969669754097985, 0.12449118808470129, 0.10136261029345636, 0.10475291179551649, 0.09988782863773181, 0.1099354341876356, 0.12226821682845915, 0.09578931546675398, 0.10955204885061366, 0.0883586311766773, 0.09109463783646418, 0.08969318745742987, 0.08327875467587452, 0.0932858892129621, 0.10342545029816361, 0.09717523484494196, 0.09244149341262563, 0.07692911764267259, 0.07464102898649577, 0.09406630759093713, 0.09246346186762476, 0.08190575509128259, 0.07356071791811973], 'val_acc': [0.7749655431650169, 0.7849893497055507, 0.8005262498433781, 0.8399949880967297, 0.8561583761433404, 0.8812178924946749, 0.8690640270642777, 0.8748277158250846, 0.8782107505325147, 0.9001378273399323, 0.885352712692645, 0.9092845508081694, 0.9214384162385666, 0.9325899010149105, 0.9234431775466734, 0.936975316376394, 0.9421125172284175, 0.9294574614709936, 0.9329657937601804, 0.9442425761182809, 0.9447437664453076, 0.9389800776845006, 0.9350958526500438, 0.9497556697155745, 0.9397318631750408, 0.9506327527878712, 0.9468738253351711, 0.9513845382784112, 0.9501315624608445, 0.9355970429770706, 0.9423631123919308, 0.9476256108257111, 0.9540157874953014, 0.9555193584763814, 0.9566470367121914, 0.9505074552061146, 0.9510086455331412, 0.9563964415486781, 0.954516977822328], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0011435887339007592, 'batch_size': 128, 'epochs': 39, 'weight_decay': 2.0669045373468117e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.33405219421740334, 'amp': False, 'patch_size': 100, 'd_model': 72, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.3835497713138072, 'head_hidden': 117, 'use_focal_loss': True, 'focal_gamma': 1.2969750694797475, 'label_smoothing': 0.17408380958289338, 'class_weight_power': 0.1333677322479138, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 3}, 'model_parameter_count': 64800, 'model_size_validation': 'PASS'}
2025-09-22 15:29:06,981 - INFO - _models.training_function_executor - BO Objective: base=0.9545, size_penalty=0.0000, final=0.9545
2025-09-22 15:29:06,982 - INFO - _models.training_function_executor - Model size: 64,800 parameters (PASS 256K limit)
2025-09-22 15:29:06,982 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 28.215s
2025-09-22 15:29:07,090 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9545
2025-09-22 15:29:07,090 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.108s
2025-09-22 15:29:07,090 - INFO - bo.run_bo - Recorded observation #17: hparams={'lr': 0.0011435887339007592, 'batch_size': np.int64(128), 'epochs': np.int64(39), 'weight_decay': 2.0669045373468117e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.33405219421740334, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(72), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.3835497713138072, 'head_hidden': np.int64(117), 'use_focal_loss': np.True_, 'focal_gamma': 1.2969750694797475, 'label_smoothing': 0.17408380958289338, 'class_weight_power': 0.1333677322479138, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(3)}, value=0.9545
2025-09-22 15:29:07,090 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 17: {'lr': 0.0011435887339007592, 'batch_size': np.int64(128), 'epochs': np.int64(39), 'weight_decay': 2.0669045373468117e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.33405219421740334, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(72), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.3835497713138072, 'head_hidden': np.int64(117), 'use_focal_loss': np.True_, 'focal_gamma': 1.2969750694797475, 'label_smoothing': 0.17408380958289338, 'class_weight_power': 0.1333677322479138, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(3)} -> 0.9545
2025-09-22 15:29:07,090 - INFO - bo.run_bo - üîçBO Trial 18: Using RF surrogate + Expected Improvement
2025-09-22 15:29:07,090 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:29:07,090 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:29:07,091 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:29:07,091 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0036509409530276664, 'batch_size': 256, 'epochs': 32, 'weight_decay': 4.3809101409253757e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.2894741315745419, 'amp': True, 'patch_size': 125, 'd_model': 114, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.4440350792784875, 'head_hidden': 83, 'use_focal_loss': False, 'focal_gamma': 2.6382916620802668, 'label_smoothing': 0.15237627501443413, 'class_weight_power': 0.05406331448628633, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 5}
2025-09-22 15:29:07,092 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0036509409530276664, 'batch_size': 256, 'epochs': 32, 'weight_decay': 4.3809101409253757e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.2894741315745419, 'amp': True, 'patch_size': 125, 'd_model': 114, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.4440350792784875, 'head_hidden': 83, 'use_focal_loss': False, 'focal_gamma': 2.6382916620802668, 'label_smoothing': 0.15237627501443413, 'class_weight_power': 0.05406331448628633, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 5}
2025-09-22 15:29:20,693 - INFO - _models.training_function_executor - Model parameter count: 86,625
2025-09-22 15:29:20,693 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1345243547037647, 1.0278556767708862, 0.9811037096116707, 0.9397382236709311, 0.9199351459145042, 0.8971333191070895, 0.8815551608426527, 0.8721351203989259, 0.8624799842710943, 0.8599729121480076, 0.8495729205916296, 0.8392353878953913, 0.8386246507166247, 0.8333660302908876, 0.827450181182956, 0.8238772994477218, 0.8195910709594286, 0.816023223509829, 0.8169559246838827, 0.8117828150162686, 0.8124367436404306, 0.8053274196076206, 0.8055645875926604, 0.8004613976145031, 0.7921857834161047, 0.7962092856799503, 0.7898409888696779, 0.7906570145070574, 0.7897011048493632, 0.7918874401461117, 0.7897689877583173, 0.7879373089986289], 'val_losses': [1.1085518490938058, 1.0196363390828864, 0.9492911674999949, 0.9453963418007495, 0.920637765589132, 0.9946709352094657, 0.9581053420096587, 1.0386405593395054, 0.9718969228303219, 0.9602572582087978, 0.9339898039771566, 1.0029733560464986, 1.070097154325866, 0.9029177807429248, 0.9647500877048181, 0.9421958356046419, 0.9327147546051708, 0.9767413288560253, 1.022993141883986, 0.9745607721284284, 0.9226084963269527, 0.9374340408622972, 0.9973239774110099, 0.9681729657535579, 0.9148030632675299, 1.0580271815106528, 0.9355820338912154, 1.030625478947406, 0.9491651693078722, 0.836335051494379, 0.902876199031142, 0.9860099819859321], 'val_acc': [0.7739631625109635, 0.7842375642150107, 0.836862548552813, 0.8516476632001002, 0.8622979576494174, 0.839493797769703, 0.8539030196717203, 0.8391179050244331, 0.8505199849642902, 0.8584137326149606, 0.8815937852399449, 0.8695652173913043, 0.8403708808419997, 0.8968800902142589, 0.8699411101365744, 0.8792131311865681, 0.886104498183185, 0.8635509334669841, 0.8520235559453703, 0.8622979576494174, 0.901014910412229, 0.8789625360230547, 0.8570354592156372, 0.8744518230798145, 0.8917428893622353, 0.8342312993359228, 0.8839744392933216, 0.8336048114271395, 0.8852274151108883, 0.9161759178047864, 0.8881092594912918, 0.8666833730109009], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0036509409530276664, 'batch_size': 256, 'epochs': 32, 'weight_decay': 4.3809101409253757e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.2894741315745419, 'amp': True, 'patch_size': 125, 'd_model': 114, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.4440350792784875, 'head_hidden': 83, 'use_focal_loss': False, 'focal_gamma': 2.6382916620802668, 'label_smoothing': 0.15237627501443413, 'class_weight_power': 0.05406331448628633, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 5}, 'model_parameter_count': 86625, 'model_size_validation': 'PASS'}
2025-09-22 15:29:20,693 - INFO - _models.training_function_executor - BO Objective: base=0.8667, size_penalty=0.0000, final=0.8667
2025-09-22 15:29:20,693 - INFO - _models.training_function_executor - Model size: 86,625 parameters (PASS 256K limit)
2025-09-22 15:29:20,693 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 13.603s
2025-09-22 15:29:20,802 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8667
2025-09-22 15:29:20,802 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.109s
2025-09-22 15:29:20,802 - INFO - bo.run_bo - Recorded observation #18: hparams={'lr': 0.0036509409530276664, 'batch_size': np.int64(256), 'epochs': np.int64(32), 'weight_decay': 4.3809101409253757e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.2894741315745419, 'amp': np.True_, 'patch_size': np.int64(125), 'd_model': np.int64(114), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.4440350792784875, 'head_hidden': np.int64(83), 'use_focal_loss': np.False_, 'focal_gamma': 2.6382916620802668, 'label_smoothing': 0.15237627501443413, 'class_weight_power': 0.05406331448628633, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(5)}, value=0.8667
2025-09-22 15:29:20,802 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 18: {'lr': 0.0036509409530276664, 'batch_size': np.int64(256), 'epochs': np.int64(32), 'weight_decay': 4.3809101409253757e-05, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.2894741315745419, 'amp': np.True_, 'patch_size': np.int64(125), 'd_model': np.int64(114), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.4440350792784875, 'head_hidden': np.int64(83), 'use_focal_loss': np.False_, 'focal_gamma': 2.6382916620802668, 'label_smoothing': 0.15237627501443413, 'class_weight_power': 0.05406331448628633, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(5)} -> 0.8667
2025-09-22 15:29:20,802 - INFO - bo.run_bo - üîçBO Trial 19: Using RF surrogate + Expected Improvement
2025-09-22 15:29:20,802 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:29:20,802 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:29:20,802 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:29:20,802 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0038271824893595353, 'batch_size': 256, 'epochs': 60, 'weight_decay': 0.006479825986824844, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.48725076792655697, 'amp': False, 'patch_size': 125, 'd_model': 64, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.11401617484965013, 'head_hidden': 94, 'use_focal_loss': True, 'focal_gamma': 2.9291662058949077, 'label_smoothing': 0.13186371704982924, 'class_weight_power': 0.5694455465232893, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}
2025-09-22 15:29:20,804 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0038271824893595353, 'batch_size': 256, 'epochs': 60, 'weight_decay': 0.006479825986824844, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.48725076792655697, 'amp': False, 'patch_size': 125, 'd_model': 64, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.11401617484965013, 'head_hidden': 94, 'use_focal_loss': True, 'focal_gamma': 2.9291662058949077, 'label_smoothing': 0.13186371704982924, 'class_weight_power': 0.5694455465232893, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}
2025-09-22 15:29:44,458 - INFO - _models.training_function_executor - Model parameter count: 90,421
2025-09-22 15:29:44,458 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.36238965158160347, 0.30210070858394406, 0.29181120357794843, 0.288570753478266, 0.28633888189466183, 0.28407342746096237, 0.27538016790019587, 0.28325186903078714, 0.27370905267846124, 0.27417960243446465, 0.2738293074326972, 0.27257543419124647, 0.27377764108642333, 0.27325191437830265, 0.2681221368822714, 0.26873101036328967, 0.2718773342308886, 0.2682274686251316, 0.26969127360133527, 0.27178251721191066, 0.26509513573658794, 0.2665948148520299, 0.2714301519686613, 0.2705088755125144, 0.26598178278241125, 0.2651671670208979, 0.2689775958462616, 0.2674816204533408, 0.2681560621981416, 0.27018295469108694, 0.2663168111195284, 0.2679757004055213, 0.26387873710068166, 0.2663985454968573, 0.27109926037929843, 0.27214002769079937, 0.26640342619732554, 0.26552672197103044, 0.26631051453054316, 0.2659536124286812, 0.2717012679492292, 0.26709197290341846, 0.26433035600242527, 0.2631415119852984, 0.2667044622354653, 0.26347503682311807, 0.2631173366630854, 0.2696602900602635, 0.267531409903535, 0.26143838934005437, 0.2622292163422602, 0.2672742407862205, 0.2693989909094228, 0.26552794479608305, 0.2663466067155516, 0.2653624542652431, 0.2636294777090566, 0.27221379725673467, 0.2649115900704613, 0.26360070550581405], 'val_losses': [0.22678861180884008, 0.22202437007401823, 0.2050539219406311, 0.1965873440922001, 0.20489730198921646, 0.20083900232532242, 0.18847343841115569, 0.19887871549976693, 0.19695155331140293, 0.19884818724568154, 0.1990893441451371, 0.19665832719346094, 0.19631120135178917, 0.20468075640099104, 0.18165087321993253, 0.19250052411362664, 0.18770699545996872, 0.2032354179125145, 0.19652485478351533, 0.18637649293546912, 0.18278012589290468, 0.19671855658729667, 0.19162231759347798, 0.18533286757934303, 0.18436663665130404, 0.18509225611167807, 0.1871936294447284, 0.18897218406088384, 0.18752482469787485, 0.1883409303850943, 0.18872515759011316, 0.1914773149801636, 0.185585670039672, 0.20444187632839028, 0.19199569003912997, 0.19140524503887782, 0.18154005160936362, 0.18177166883247325, 0.19685339004402547, 0.1888197930537882, 0.1872570638503665, 0.1990342763652928, 0.1845356141071304, 0.18261060063783097, 0.18018845126617225, 0.19836710326960774, 0.18586837713460908, 0.20845126699996525, 0.1915940090205463, 0.1959007344509841, 0.19018897388068345, 0.189004399667496, 0.19173778195292143, 0.19401959623255058, 0.17604407442943926, 0.1818689942610113, 0.1917318492425801, 0.17901004047273408, 0.18630300164446767, 0.1920487554976104], 'val_acc': [0.25924069665455457, 0.3414359102869315, 0.3256484149855908, 0.33291567472747774, 0.36837489036461596, 0.3364240070166646, 0.3777722089963664, 0.35108382408219524, 0.3488284676105751, 0.3938102994612204, 0.4013281543666207, 0.36975316376393935, 0.3025936599423631, 0.38829720586392685, 0.3712567347450194, 0.3445683498308483, 0.35496804911665203, 0.30722967046736, 0.3231424633504573, 0.322891868186944, 0.38265881468487656, 0.37789750657812304, 0.31412103746397696, 0.39581506076932715, 0.4113519609071545, 0.3889236937727102, 0.35246209748151863, 0.3955644656058138, 0.3831600050119033, 0.3790251848139331, 0.3965668462598672, 0.3587269765693522, 0.3434406715950382, 0.3716326274902894, 0.3539656684625987, 0.392056133316627, 0.38190702919433656, 0.3834106001754166, 0.36060644029570227, 0.37250971056258614, 0.31813056008019047, 0.39080315749906025, 0.36975316376393935, 0.4038341060017542, 0.4257611828091718, 0.3713820323267761, 0.4169903520862047, 0.2696403959403583, 0.4221275529382283, 0.33116150858288435, 0.3187570479889738, 0.3390552562335547, 0.3569728104247588, 0.31549931086330035, 0.3906778599173036, 0.3919308357348703, 0.3780228041598797, 0.399699285803784, 0.3678737000375893, 0.3471995990477384], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0038271824893595353, 'batch_size': 256, 'epochs': 60, 'weight_decay': 0.006479825986824844, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.48725076792655697, 'amp': False, 'patch_size': 125, 'd_model': 64, 'n_heads': 4, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.11401617484965013, 'head_hidden': 94, 'use_focal_loss': True, 'focal_gamma': 2.9291662058949077, 'label_smoothing': 0.13186371704982924, 'class_weight_power': 0.5694455465232893, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}, 'model_parameter_count': 90421, 'model_size_validation': 'PASS'}
2025-09-22 15:29:44,458 - INFO - _models.training_function_executor - BO Objective: base=0.3472, size_penalty=0.0000, final=0.3472
2025-09-22 15:29:44,458 - INFO - _models.training_function_executor - Model size: 90,421 parameters (PASS 256K limit)
2025-09-22 15:29:44,458 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 23.656s
2025-09-22 15:29:44,570 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.3472
2025-09-22 15:29:44,570 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.112s
2025-09-22 15:29:44,570 - INFO - bo.run_bo - Recorded observation #19: hparams={'lr': 0.0038271824893595353, 'batch_size': np.int64(256), 'epochs': np.int64(60), 'weight_decay': 0.006479825986824844, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.48725076792655697, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(64), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.11401617484965013, 'head_hidden': np.int64(94), 'use_focal_loss': np.True_, 'focal_gamma': 2.9291662058949077, 'label_smoothing': 0.13186371704982924, 'class_weight_power': 0.5694455465232893, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)}, value=0.3472
2025-09-22 15:29:44,570 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 19: {'lr': 0.0038271824893595353, 'batch_size': np.int64(256), 'epochs': np.int64(60), 'weight_decay': 0.006479825986824844, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.48725076792655697, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(64), 'n_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.11401617484965013, 'head_hidden': np.int64(94), 'use_focal_loss': np.True_, 'focal_gamma': 2.9291662058949077, 'label_smoothing': 0.13186371704982924, 'class_weight_power': 0.5694455465232893, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)} -> 0.3472
2025-09-22 15:29:44,570 - INFO - bo.run_bo - üîçBO Trial 20: Using RF surrogate + Expected Improvement
2025-09-22 15:29:44,570 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:29:44,570 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:29:44,570 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:29:44,570 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0030874913648953014, 'batch_size': 32, 'epochs': 27, 'weight_decay': 1.611698913231634e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 2, 'grad_clip': 0.47521982939348184, 'amp': False, 'patch_size': 125, 'd_model': 76, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.22969524165226063, 'head_hidden': 78, 'use_focal_loss': True, 'focal_gamma': 2.833604226658792, 'label_smoothing': 0.162653409633169, 'class_weight_power': 0.4499822309984408, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:29:44,572 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0030874913648953014, 'batch_size': 32, 'epochs': 27, 'weight_decay': 1.611698913231634e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 2, 'grad_clip': 0.47521982939348184, 'amp': False, 'patch_size': 125, 'd_model': 76, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.22969524165226063, 'head_hidden': 78, 'use_focal_loss': True, 'focal_gamma': 2.833604226658792, 'label_smoothing': 0.162653409633169, 'class_weight_power': 0.4499822309984408, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:31:00,897 - INFO - _models.training_function_executor - Model parameter count: 202,409
2025-09-22 15:31:00,897 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2629495065043012, 0.1980176659063622, 0.15916484268121084, 0.13759603823553787, 0.11338700493434443, 0.10587344090230662, 0.09360345811152639, 0.0832232496142294, 0.07730041624661296, 0.07147869563962912, 0.06565412266428891, 0.0601125825913894, 0.051763464928376315, 0.048696096385634706, 0.04562026891485475, 0.04287243632746924, 0.039387502176748676, 0.03351526956049405, 0.03332874973376951, 0.029077989394501326, 0.02784072908814151, 0.026155193902138323, 0.024673231187814598, 0.023157469015905163, 0.023033418206638238, 0.021988302169548724, 0.021731778144676202], 'val_losses': [0.11745953633438419, 0.1361942602501912, 0.09307043805163899, 0.08259638158872662, 0.0842789665402259, 0.07131270021844323, 0.060948772177841413, 0.07196158153522943, 0.06117012163082948, 0.054603749084944656, 0.053286240619460586, 0.04811627017781036, 0.04890531491567102, 0.0525927268694015, 0.04622447314719899, 0.04632769011044141, 0.045668283559343686, 0.05223035778117022, 0.04887788226113121, 0.0510749179048842, 0.04909404464670029, 0.047946600711386005, 0.0501231257688796, 0.04927840529275992, 0.04495868453625979, 0.04999443765670386, 0.048417701365727484], 'val_acc': [0.6881343190076431, 0.6140834481894499, 0.7491542413231425, 0.7730860794386668, 0.7960155369001378, 0.7991479764440547, 0.8640521237940108, 0.8121789249467485, 0.8518982583636135, 0.8654303971933341, 0.871194085954141, 0.8819696779852149, 0.9048991354466859, 0.8706928956271144, 0.886104498183185, 0.8955018168149355, 0.9045232427014158, 0.8863550933466984, 0.9126675855155996, 0.9099110387169528, 0.9168024057135697, 0.9260744267635634, 0.9290815687257236, 0.929708056634507, 0.9258238316000501, 0.9324646034331537, 0.9294574614709936], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0030874913648953014, 'batch_size': 32, 'epochs': 27, 'weight_decay': 1.611698913231634e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 2, 'grad_clip': 0.47521982939348184, 'amp': False, 'patch_size': 125, 'd_model': 76, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.22969524165226063, 'head_hidden': 78, 'use_focal_loss': True, 'focal_gamma': 2.833604226658792, 'label_smoothing': 0.162653409633169, 'class_weight_power': 0.4499822309984408, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 6}, 'model_parameter_count': 202409, 'model_size_validation': 'PASS'}
2025-09-22 15:31:00,897 - INFO - _models.training_function_executor - BO Objective: base=0.9295, size_penalty=0.0000, final=0.9295
2025-09-22 15:31:00,898 - INFO - _models.training_function_executor - Model size: 202,409 parameters (PASS 256K limit)
2025-09-22 15:31:00,898 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 76.327s
2025-09-22 15:31:01,008 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9295
2025-09-22 15:31:01,008 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-09-22 15:31:01,008 - INFO - bo.run_bo - Recorded observation #20: hparams={'lr': 0.0030874913648953014, 'batch_size': np.int64(32), 'epochs': np.int64(27), 'weight_decay': 1.611698913231634e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.47521982939348184, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(76), 'n_heads': np.int64(8), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.22969524165226063, 'head_hidden': np.int64(78), 'use_focal_loss': np.True_, 'focal_gamma': 2.833604226658792, 'label_smoothing': 0.162653409633169, 'class_weight_power': 0.4499822309984408, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)}, value=0.9295
2025-09-22 15:31:01,008 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 20: {'lr': 0.0030874913648953014, 'batch_size': np.int64(32), 'epochs': np.int64(27), 'weight_decay': 1.611698913231634e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.47521982939348184, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(76), 'n_heads': np.int64(8), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.22969524165226063, 'head_hidden': np.int64(78), 'use_focal_loss': np.True_, 'focal_gamma': 2.833604226658792, 'label_smoothing': 0.162653409633169, 'class_weight_power': 0.4499822309984408, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)} -> 0.9295
2025-09-22 15:31:01,009 - INFO - bo.run_bo - üîçBO Trial 21: Using RF surrogate + Expected Improvement
2025-09-22 15:31:01,009 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:31:01,009 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:31:01,009 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:31:01,009 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0038208458557001243, 'batch_size': 128, 'epochs': 47, 'weight_decay': 0.00020700001951418645, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.030732715534022577, 'amp': False, 'patch_size': 25, 'd_model': 59, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.08164235001837063, 'head_hidden': 105, 'use_focal_loss': True, 'focal_gamma': 1.5245261778090198, 'label_smoothing': 0.1739871061730748, 'class_weight_power': 0.49455014870411307, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 1}
2025-09-22 15:31:01,010 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0038208458557001243, 'batch_size': 128, 'epochs': 47, 'weight_decay': 0.00020700001951418645, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.030732715534022577, 'amp': False, 'patch_size': 25, 'd_model': 59, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.08164235001837063, 'head_hidden': 105, 'use_focal_loss': True, 'focal_gamma': 1.5245261778090198, 'label_smoothing': 0.1739871061730748, 'class_weight_power': 0.49455014870411307, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 1}
2025-09-22 15:31:38,187 - INFO - _models.training_function_executor - Model parameter count: 62,481
2025-09-22 15:31:38,187 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3694438176940334, 0.24459298333505025, 0.20138434057585172, 0.17308145269639053, 0.1554917709013463, 0.1352838309483432, 0.12020926651614683, 0.1123336793531176, 0.10099164062680163, 0.09245220332435956, 0.08394869532633, 0.07792912100595822, 0.07671176066144092, 0.06860134924365155, 0.06739409642240193, 0.0628371007371271, 0.06111807597793096, 0.06010298257637576, 0.05551151440812058, 0.05290394848080675, 0.05080297012753203, 0.050306726254424075, 0.04989945143893003, 0.04602853990387574, 0.04621038070034384, 0.04432174853316073, 0.04427257791602415, 0.04351834557768217, 0.04027398695238431, 0.03898739490588144, 0.03788864228703077, 0.03810778135617064, 0.03697778481952873, 0.035300177722341716, 0.036884715929695104, 0.03273282456943285, 0.033284697823509554, 0.0327036083885303, 0.033660576137605064, 0.03105157436807943, 0.03282077927135963, 0.02973723030803952, 0.03024441216612321, 0.03030327697633237, 0.030568870961179717, 0.028005776528066118, 0.028130277216753527], 'val_losses': [0.20605424711570122, 0.21007113809193334, 0.15842703951396914, 0.12633068316443763, 0.12657336905307376, 0.12472141640513572, 0.09937296820742163, 0.09222775884696138, 0.09584658328840986, 0.08155252748409275, 0.0792561408121579, 0.0867962310497451, 0.06436473493299871, 0.07915190206540734, 0.062031053395935086, 0.06298071606433148, 0.08450627547654124, 0.06741532161850526, 0.05993279593704622, 0.06023724929717471, 0.06460387069653066, 0.07359013691369028, 0.06843733098864362, 0.06274292817113618, 0.05847797994830378, 0.0678191456508254, 0.06895613837452733, 0.06492925657207035, 0.05836516530135717, 0.06269931894501951, 0.06731212086571352, 0.066029911566992, 0.06118794609794281, 0.0657887415036927, 0.06350585555685984, 0.06851235802158528, 0.0694910300366488, 0.06064597526400658, 0.07415017148330687, 0.0742572068628038, 0.07067893768636568, 0.06771881050839712, 0.06964121073346256, 0.06695681120493167, 0.0664191447350558, 0.08397221697435808, 0.0723454543919047], 'val_acc': [0.5400325773712568, 0.6350081443428142, 0.7245959152988347, 0.7942613707555444, 0.7947625610825712, 0.8106753539656685, 0.8164390427264754, 0.7935095852650044, 0.8401202856784864, 0.8535271269264503, 0.8575366495426638, 0.8539030196717203, 0.8948753289061521, 0.8680616464102243, 0.900639017666959, 0.8967547926325022, 0.8726976569352212, 0.901014910412229, 0.899511339431149, 0.9035208620473625, 0.9259491291818068, 0.9011402079939858, 0.8881092594912918, 0.9066533015912793, 0.9124169903520862, 0.900263124921689, 0.9026437789750658, 0.9055256233554693, 0.9293321638892369, 0.9210625234932965, 0.9144217516601929, 0.9261997243453202, 0.9397318631750408, 0.9194336549304598, 0.9280791880716702, 0.9348452574865305, 0.9468738253351711, 0.9397318631750408, 0.9244455582007267, 0.9292068663074803, 0.9303345445432903, 0.9423631123919308, 0.9298333542162637, 0.9379776970304473, 0.930459842125047, 0.9546422754040848, 0.9284550808169403], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0038208458557001243, 'batch_size': 128, 'epochs': 47, 'weight_decay': 0.00020700001951418645, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.030732715534022577, 'amp': False, 'patch_size': 25, 'd_model': 59, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.08164235001837063, 'head_hidden': 105, 'use_focal_loss': True, 'focal_gamma': 1.5245261778090198, 'label_smoothing': 0.1739871061730748, 'class_weight_power': 0.49455014870411307, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 1}, 'model_parameter_count': 62481, 'model_size_validation': 'PASS'}
2025-09-22 15:31:38,187 - INFO - _models.training_function_executor - BO Objective: base=0.9285, size_penalty=0.0000, final=0.9285
2025-09-22 15:31:38,187 - INFO - _models.training_function_executor - Model size: 62,481 parameters (PASS 256K limit)
2025-09-22 15:31:38,187 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 37.178s
2025-09-22 15:31:38,300 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9285
2025-09-22 15:31:38,300 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.113s
2025-09-22 15:31:38,300 - INFO - bo.run_bo - Recorded observation #21: hparams={'lr': 0.0038208458557001243, 'batch_size': np.int64(128), 'epochs': np.int64(47), 'weight_decay': 0.00020700001951418645, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.030732715534022577, 'amp': np.False_, 'patch_size': np.int64(25), 'd_model': np.int64(59), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.08164235001837063, 'head_hidden': np.int64(105), 'use_focal_loss': np.True_, 'focal_gamma': 1.5245261778090198, 'label_smoothing': 0.1739871061730748, 'class_weight_power': 0.49455014870411307, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(1)}, value=0.9285
2025-09-22 15:31:38,300 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 21: {'lr': 0.0038208458557001243, 'batch_size': np.int64(128), 'epochs': np.int64(47), 'weight_decay': 0.00020700001951418645, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.030732715534022577, 'amp': np.False_, 'patch_size': np.int64(25), 'd_model': np.int64(59), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.08164235001837063, 'head_hidden': np.int64(105), 'use_focal_loss': np.True_, 'focal_gamma': 1.5245261778090198, 'label_smoothing': 0.1739871061730748, 'class_weight_power': 0.49455014870411307, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(1)} -> 0.9285
2025-09-22 15:31:38,300 - INFO - bo.run_bo - üîçBO Trial 22: Using RF surrogate + Expected Improvement
2025-09-22 15:31:38,300 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:31:38,300 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:31:38,300 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:31:38,301 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00176973184470036, 'batch_size': 256, 'epochs': 55, 'weight_decay': 0.00010919944917939177, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 3, 'grad_clip': 0.2163187606349855, 'amp': False, 'patch_size': 25, 'd_model': 51, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.10813275759040283, 'head_hidden': 124, 'use_focal_loss': False, 'focal_gamma': 1.5629793546591646, 'label_smoothing': 0.13204161349105678, 'class_weight_power': 0.51004800602721, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 5}
2025-09-22 15:31:38,302 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00176973184470036, 'batch_size': 256, 'epochs': 55, 'weight_decay': 0.00010919944917939177, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 3, 'grad_clip': 0.2163187606349855, 'amp': False, 'patch_size': 25, 'd_model': 51, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.10813275759040283, 'head_hidden': 124, 'use_focal_loss': False, 'focal_gamma': 1.5629793546591646, 'label_smoothing': 0.13204161349105678, 'class_weight_power': 0.51004800602721, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 5}
2025-09-22 15:32:41,353 - INFO - _models.training_function_executor - Model parameter count: 76,171
2025-09-22 15:32:41,354 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.3985402946263348, 1.0766282656421093, 0.9793287651325173, 0.9253285877896488, 0.8946396874943813, 0.8702163157395707, 0.8517670364688035, 0.8310818536999388, 0.818217869008151, 0.8062347001690048, 0.8043458299731243, 0.7935531252223597, 0.7864918885637905, 0.7799918779871859, 0.7760163154088364, 0.7720635117390504, 0.7623891305554515, 0.7546483009238629, 0.7614888251546638, 0.7568068282386808, 0.750234421731326, 0.7459314384190237, 0.7397503104626048, 0.7359946602284749, 0.7360411610097114, 0.7286353893355163, 0.7294564638483462, 0.7226687026487661, 0.7205715503989277, 0.7151884817615531, 0.7180697951360002, 0.7126733256089143, 0.7123041209472367, 0.7144066432183859, 0.7138607637985971, 0.7080811625494454, 0.7097112340716959, 0.7044454106049098, 0.7002775540694823, 0.7036668519136082, 0.6999528874181526, 0.7004661918586423, 0.6931568859954624, 0.6878272464235553, 0.6907263312524854, 0.6894831224812477, 0.6917235573009601, 0.6929000818732761, 0.6867333254923786, 0.6929512051447395, 0.68932430484751, 0.6862232122781846, 0.6798638826817217, 0.6902205792785763, 0.6872277097254959], 'val_losses': [1.624441007850254, 1.4683088109451732, 1.2936560576787466, 1.3535082456447505, 1.2736383209759066, 1.2320611514837247, 1.2893608012842872, 1.2151889554869635, 1.281305943242202, 1.2417828082112259, 1.2481893010641933, 1.2633407655746294, 1.2136955663772802, 1.2143273693235757, 1.2260905838359226, 1.2015545396574188, 1.169154386206119, 1.1616057984558, 1.1857788110194596, 1.1734519480702992, 1.2322823039691648, 1.2136805398827761, 1.2221968831491417, 1.1765738439326925, 1.1845431684058585, 1.1686772614429952, 1.1710714800825932, 1.1606258923600243, 1.1452365721576032, 1.141885857820481, 1.1501356925212745, 1.1638007701122886, 1.1623487264317478, 1.1371144005983729, 1.1632873592273945, 1.1874506855022935, 1.1529130617966703, 1.1500234155932132, 1.1640802334012461, 1.1438546533528433, 1.1419416936533355, 1.169391566567994, 1.1493148673463474, 1.1562888307419654, 1.16698316654874, 1.1554414663917065, 1.157494285817344, 1.1396241158474778, 1.1518393874452193, 1.1356840361301321, 1.1399785721306512, 1.1465784529817535, 1.1557307356268134, 1.140629932659638, 1.1404787936525496], 'val_acc': [0.4491918305976695, 0.6406465355218645, 0.7900012529758176, 0.7480265630873324, 0.7886229795764942, 0.8416238566595665, 0.8042851772960782, 0.8575366495426638, 0.8114271394562085, 0.8501440922190202, 0.838366119533893, 0.8378649292068663, 0.8595414108507706, 0.863175040721714, 0.8701917053000877, 0.8748277158250846, 0.8988848515223656, 0.9107881217892495, 0.8913669966169653, 0.8953765192331788, 0.863175040721714, 0.8830973562210249, 0.8833479513845383, 0.8968800902142589, 0.8951259240696654, 0.9066533015912793, 0.9057762185189826, 0.9134193710061396, 0.9312116276155871, 0.9283297832351836, 0.9244455582007267, 0.92281668963789, 0.9218143089838366, 0.9339681744142339, 0.9240696654554567, 0.9061521112642525, 0.9248214509459968, 0.9280791880716702, 0.9239443678737, 0.9329657937601804, 0.9330910913419371, 0.9214384162385666, 0.9328404961784238, 0.9305851397068037, 0.9267009146723468, 0.9313369251973437, 0.928956271143967, 0.9378523994486906, 0.9312116276155871, 0.9428643027189575, 0.9414860293196341, 0.9363488284676106, 0.9325899010149105, 0.9421125172284175, 0.9428643027189575], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00176973184470036, 'batch_size': 256, 'epochs': 55, 'weight_decay': 0.00010919944917939177, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 3, 'grad_clip': 0.2163187606349855, 'amp': False, 'patch_size': 25, 'd_model': 51, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.10813275759040283, 'head_hidden': 124, 'use_focal_loss': False, 'focal_gamma': 1.5629793546591646, 'label_smoothing': 0.13204161349105678, 'class_weight_power': 0.51004800602721, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 5}, 'model_parameter_count': 76171, 'model_size_validation': 'PASS'}
2025-09-22 15:32:41,354 - INFO - _models.training_function_executor - BO Objective: base=0.9429, size_penalty=0.0000, final=0.9429
2025-09-22 15:32:41,354 - INFO - _models.training_function_executor - Model size: 76,171 parameters (PASS 256K limit)
2025-09-22 15:32:41,354 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 63.053s
2025-09-22 15:32:41,468 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9429
2025-09-22 15:32:41,469 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.115s
2025-09-22 15:32:41,469 - INFO - bo.run_bo - Recorded observation #22: hparams={'lr': 0.00176973184470036, 'batch_size': np.int64(256), 'epochs': np.int64(55), 'weight_decay': 0.00010919944917939177, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.2163187606349855, 'amp': np.False_, 'patch_size': np.int64(25), 'd_model': np.int64(51), 'n_heads': np.int64(8), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.10813275759040283, 'head_hidden': np.int64(124), 'use_focal_loss': np.False_, 'focal_gamma': 1.5629793546591646, 'label_smoothing': 0.13204161349105678, 'class_weight_power': 0.51004800602721, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(5)}, value=0.9429
2025-09-22 15:32:41,469 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 22: {'lr': 0.00176973184470036, 'batch_size': np.int64(256), 'epochs': np.int64(55), 'weight_decay': 0.00010919944917939177, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.2163187606349855, 'amp': np.False_, 'patch_size': np.int64(25), 'd_model': np.int64(51), 'n_heads': np.int64(8), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.10813275759040283, 'head_hidden': np.int64(124), 'use_focal_loss': np.False_, 'focal_gamma': 1.5629793546591646, 'label_smoothing': 0.13204161349105678, 'class_weight_power': 0.51004800602721, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(5)} -> 0.9429
2025-09-22 15:32:41,469 - INFO - bo.run_bo - üîçBO Trial 23: Using RF surrogate + Expected Improvement
2025-09-22 15:32:41,469 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:32:41,469 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:32:41,469 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:32:41,469 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003242595760388738, 'batch_size': 256, 'epochs': 36, 'weight_decay': 3.703563852833072e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 0, 'grad_clip': 0.433445580208184, 'amp': False, 'patch_size': 25, 'd_model': 35, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.2461727039614559, 'head_hidden': 118, 'use_focal_loss': True, 'focal_gamma': 1.078808653816326, 'label_smoothing': 0.11964491382891776, 'class_weight_power': 0.5327239602148742, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 5}
2025-09-22 15:32:41,471 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003242595760388738, 'batch_size': 256, 'epochs': 36, 'weight_decay': 3.703563852833072e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 0, 'grad_clip': 0.433445580208184, 'amp': False, 'patch_size': 25, 'd_model': 35, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.2461727039614559, 'head_hidden': 118, 'use_focal_loss': True, 'focal_gamma': 1.078808653816326, 'label_smoothing': 0.11964491382891776, 'class_weight_power': 0.5327239602148742, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 5}
2025-09-22 15:33:03,820 - INFO - _models.training_function_executor - Model parameter count: 28,595
2025-09-22 15:33:03,821 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5046933967976874, 0.3559665362247454, 0.30065431759212785, 0.2730632086911379, 0.2509050612928866, 0.22888459271346964, 0.21347269249540096, 0.20720385927947935, 0.18941856025239787, 0.18261898821218342, 0.17536414446642318, 0.16337144330222816, 0.15731966693803967, 0.15102659310515734, 0.1450875609099288, 0.1384340351529092, 0.12931257111328726, 0.13390400499213748, 0.12746564753306217, 0.11831793832511184, 0.12045317595679371, 0.11362319047433916, 0.1113971621974408, 0.10922331819081185, 0.1074704048399567, 0.11007898937650212, 0.1026518826194639, 0.10337619228699943, 0.10073104450404523, 0.10001396985416315, 0.0940292731727536, 0.09666198328733948, 0.09335878298783054, 0.09165408331959767, 0.09522595044250078, 0.09325822539604446], 'val_losses': [0.3113653067116181, 0.22843275281282432, 0.23532197101942773, 0.2129580443413421, 0.18432488812810718, 0.18572113559848683, 0.17629626151438033, 0.16910672797313267, 0.15802313453395211, 0.14616751954726834, 0.16507715547671817, 0.1687273406451051, 0.16652812428894742, 0.1364288967676426, 0.1275874581208968, 0.1305603390358143, 0.12379044216873503, 0.12936287175020528, 0.13342164063077272, 0.10614213015654977, 0.11762524416065503, 0.11164439543512229, 0.1078230689143555, 0.10162470456226295, 0.11430989912340118, 0.10392137924380775, 0.11903784806304386, 0.13225516722366157, 0.111183871396419, 0.10017297167576968, 0.11863793894441306, 0.10860902497953542, 0.0890301718891074, 0.1040043113838385, 0.10182644734890774, 0.095358967525277], 'val_acc': [0.3905525623355469, 0.6367623104874076, 0.6081944618468863, 0.6673349204360356, 0.7089337175792507, 0.7252224032076181, 0.761934594662323, 0.7482771582508457, 0.762310487407593, 0.7628116777346198, 0.7200852023555946, 0.7083072296704673, 0.729858413732615, 0.7809798270893372, 0.8151860669089087, 0.7866182182683874, 0.8199473750156622, 0.8151860669089087, 0.8095476757298584, 0.837614334043353, 0.7997744643528379, 0.8507705801278035, 0.8255857661947125, 0.8690640270642777, 0.8327277283548428, 0.8526500438541537, 0.8398696905149731, 0.8051622603683749, 0.8466357599298333, 0.870066407718331, 0.853652424508207, 0.8731988472622478, 0.877333667460218, 0.8481393309109134, 0.8818443804034583, 0.8820949755669716], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003242595760388738, 'batch_size': 256, 'epochs': 36, 'weight_decay': 3.703563852833072e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 0, 'grad_clip': 0.433445580208184, 'amp': False, 'patch_size': 25, 'd_model': 35, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.2461727039614559, 'head_hidden': 118, 'use_focal_loss': True, 'focal_gamma': 1.078808653816326, 'label_smoothing': 0.11964491382891776, 'class_weight_power': 0.5327239602148742, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 5}, 'model_parameter_count': 28595, 'model_size_validation': 'PASS'}
2025-09-22 15:33:03,821 - INFO - _models.training_function_executor - BO Objective: base=0.8821, size_penalty=0.0000, final=0.8821
2025-09-22 15:33:03,821 - INFO - _models.training_function_executor - Model size: 28,595 parameters (PASS 256K limit)
2025-09-22 15:33:03,821 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 22.352s
2025-09-22 15:33:03,935 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8821
2025-09-22 15:33:03,935 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.114s
2025-09-22 15:33:03,935 - INFO - bo.run_bo - Recorded observation #23: hparams={'lr': 0.003242595760388738, 'batch_size': np.int64(256), 'epochs': np.int64(36), 'weight_decay': 3.703563852833072e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(0), 'grad_clip': 0.433445580208184, 'amp': np.False_, 'patch_size': np.int64(25), 'd_model': np.int64(35), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.2461727039614559, 'head_hidden': np.int64(118), 'use_focal_loss': np.True_, 'focal_gamma': 1.078808653816326, 'label_smoothing': 0.11964491382891776, 'class_weight_power': 0.5327239602148742, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(5)}, value=0.8821
2025-09-22 15:33:03,935 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 23: {'lr': 0.003242595760388738, 'batch_size': np.int64(256), 'epochs': np.int64(36), 'weight_decay': 3.703563852833072e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(0), 'grad_clip': 0.433445580208184, 'amp': np.False_, 'patch_size': np.int64(25), 'd_model': np.int64(35), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.2461727039614559, 'head_hidden': np.int64(118), 'use_focal_loss': np.True_, 'focal_gamma': 1.078808653816326, 'label_smoothing': 0.11964491382891776, 'class_weight_power': 0.5327239602148742, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(5)} -> 0.8821
2025-09-22 15:33:03,935 - INFO - bo.run_bo - üîçBO Trial 24: Using RF surrogate + Expected Improvement
2025-09-22 15:33:03,935 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:33:03,935 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:33:03,935 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:33:03,936 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0023924115852001432, 'batch_size': 128, 'epochs': 54, 'weight_decay': 1.09117462618666e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.4900438311556613, 'amp': False, 'patch_size': 50, 'd_model': 40, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.2647716320686718, 'head_hidden': 125, 'use_focal_loss': True, 'focal_gamma': 1.0793364238610748, 'label_smoothing': 0.19059718782795843, 'class_weight_power': 0.5322823772746492, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 1}
2025-09-22 15:33:03,937 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0023924115852001432, 'batch_size': 128, 'epochs': 54, 'weight_decay': 1.09117462618666e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.4900438311556613, 'amp': False, 'patch_size': 50, 'd_model': 40, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.2647716320686718, 'head_hidden': 125, 'use_focal_loss': True, 'focal_gamma': 1.0793364238610748, 'label_smoothing': 0.19059718782795843, 'class_weight_power': 0.5322823772746492, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 1}
2025-09-22 15:33:44,126 - INFO - _models.training_function_executor - Model parameter count: 50,645
2025-09-22 15:33:44,126 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4535132558151687, 0.2877540390611655, 0.24529533256926497, 0.22537022842312465, 0.2091999255633984, 0.19043197135630024, 0.1782749294361793, 0.17316815452188364, 0.1563195494546727, 0.14945177806623186, 0.14161232507804303, 0.14222479215125125, 0.12822681485475113, 0.12753565617390444, 0.12157066950972276, 0.11624667594662152, 0.11396484544414762, 0.10972002306168, 0.10734859540467125, 0.10810327501054111, 0.10428165106578186, 0.10459530852772152, 0.1001114879729242, 0.09903033318134177, 0.09566274223140774, 0.0974443978705713, 0.08969731089688705, 0.09013364764186996, 0.09158991855190417, 0.08874782409384874, 0.08449439601235471, 0.08886545646917146, 0.08437313606829937, 0.08344534623083011, 0.08189758591460554, 0.08317399559536644, 0.08296189752582679, 0.07887081472154765, 0.08059407970114048, 0.07948824657127904, 0.07842301609864515, 0.07454138561204289, 0.07695462402124713, 0.07355771605122027, 0.07444099717245244, 0.07266645647004945, 0.07627509035273004, 0.0726766488794182, 0.06853160026054934, 0.071862689993198, 0.07037549807464248, 0.0691395084704574, 0.07205003409381872, 0.06839723533243888], 'val_losses': [0.2553882552143954, 0.1843423324279686, 0.16054524586786528, 0.16623256461014052, 0.15300525608008675, 0.13675279047887676, 0.12474414902460215, 0.13113073711425285, 0.12658679340809095, 0.13964647577641198, 0.09691982901069458, 0.0954847864768377, 0.10866253064120791, 0.11021375973531439, 0.09127074065518445, 0.1018455300679438, 0.0929879090710102, 0.0930865842981112, 0.09603026514466119, 0.10824892694841663, 0.09444359728692842, 0.07951737888313269, 0.08593369537469454, 0.08150608775430776, 0.08072599688214163, 0.08263598546402569, 0.0818774204231148, 0.08989302101389252, 0.07843106737330718, 0.07658677557692195, 0.0718577257918245, 0.0941387716810994, 0.07075614210270936, 0.08711058760612415, 0.08153319430428181, 0.07903202792379645, 0.08349487642025816, 0.08944173939147998, 0.07260994452881851, 0.08344576227184734, 0.072119752976466, 0.07607567263562912, 0.07984329693419878, 0.08298248851146932, 0.07136667144244782, 0.07779169384174935, 0.07515666103920547, 0.07464671257055727, 0.07038767388340914, 0.08036772509652441, 0.07518038508673536, 0.07489947298378996, 0.07638692558969326, 0.06746874600947612], 'val_acc': [0.4930459842125047, 0.6965292569853402, 0.7101866933968174, 0.6492920686630748, 0.7299837113143717, 0.7406340057636888, 0.7796015536900138, 0.7557950131562461, 0.760806916426513, 0.7432652549805788, 0.8550306979075304, 0.8297205863926826, 0.8308482646284927, 0.7898759553940609, 0.852900639017667, 0.8066658313494549, 0.8458839744392933, 0.831850645282546, 0.8401202856784864, 0.8198220774339056, 0.8569101616338805, 0.8729482520987345, 0.8585390301967172, 0.8782107505325147, 0.859792006014284, 0.8552812930710437, 0.869690514973061, 0.853652424508207, 0.8681869439919809, 0.8784613456960281, 0.870442300463601, 0.8554065906528004, 0.8893622353088585, 0.84751284300213, 0.8767071795514346, 0.8742012279163012, 0.8709434907906277, 0.8607943866683373, 0.8857286054379151, 0.8669339681744143, 0.8931211627615587, 0.8783360481142714, 0.8705675980453578, 0.8695652173913043, 0.8976318757047989, 0.8827214634757549, 0.8847262247838616, 0.8802155118406215, 0.8872321764189951, 0.8720711690264378, 0.8986342563588523, 0.8763312868061647, 0.8753289061521112, 0.8902393183811552], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0023924115852001432, 'batch_size': 128, 'epochs': 54, 'weight_decay': 1.09117462618666e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.4900438311556613, 'amp': False, 'patch_size': 50, 'd_model': 40, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.2647716320686718, 'head_hidden': 125, 'use_focal_loss': True, 'focal_gamma': 1.0793364238610748, 'label_smoothing': 0.19059718782795843, 'class_weight_power': 0.5322823772746492, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 1}, 'model_parameter_count': 50645, 'model_size_validation': 'PASS'}
2025-09-22 15:33:44,126 - INFO - _models.training_function_executor - BO Objective: base=0.8902, size_penalty=0.0000, final=0.8902
2025-09-22 15:33:44,126 - INFO - _models.training_function_executor - Model size: 50,645 parameters (PASS 256K limit)
2025-09-22 15:33:44,126 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 40.191s
2025-09-22 15:33:44,242 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8902
2025-09-22 15:33:44,242 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.116s
2025-09-22 15:33:44,242 - INFO - bo.run_bo - Recorded observation #24: hparams={'lr': 0.0023924115852001432, 'batch_size': np.int64(128), 'epochs': np.int64(54), 'weight_decay': 1.09117462618666e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.4900438311556613, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(40), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.2647716320686718, 'head_hidden': np.int64(125), 'use_focal_loss': np.True_, 'focal_gamma': 1.0793364238610748, 'label_smoothing': 0.19059718782795843, 'class_weight_power': 0.5322823772746492, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)}, value=0.8902
2025-09-22 15:33:44,242 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 24: {'lr': 0.0023924115852001432, 'batch_size': np.int64(128), 'epochs': np.int64(54), 'weight_decay': 1.09117462618666e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.4900438311556613, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(40), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.2647716320686718, 'head_hidden': np.int64(125), 'use_focal_loss': np.True_, 'focal_gamma': 1.0793364238610748, 'label_smoothing': 0.19059718782795843, 'class_weight_power': 0.5322823772746492, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)} -> 0.8902
2025-09-22 15:33:44,242 - INFO - bo.run_bo - üîçBO Trial 25: Using RF surrogate + Expected Improvement
2025-09-22 15:33:44,242 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:33:44,242 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:33:44,242 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:33:44,242 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0039732940635894675, 'batch_size': 64, 'epochs': 46, 'weight_decay': 0.007034598734157888, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 2, 'grad_clip': 0.3789540121715964, 'amp': False, 'patch_size': 50, 'd_model': 104, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.0565089484978062, 'head_hidden': 104, 'use_focal_loss': True, 'focal_gamma': 1.1868826617711636, 'label_smoothing': 0.15711242207551043, 'class_weight_power': 0.5459400242250578, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 5}
2025-09-22 15:33:44,244 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0039732940635894675, 'batch_size': 64, 'epochs': 46, 'weight_decay': 0.007034598734157888, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 2, 'grad_clip': 0.3789540121715964, 'amp': False, 'patch_size': 50, 'd_model': 104, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.0565089484978062, 'head_hidden': 104, 'use_focal_loss': True, 'focal_gamma': 1.1868826617711636, 'label_smoothing': 0.15711242207551043, 'class_weight_power': 0.5459400242250578, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 5}
2025-09-22 15:34:46,529 - INFO - _models.training_function_executor - Model parameter count: 112,013
2025-09-22 15:34:46,529 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5111192190938025, 0.469238802350464, 0.43983950637017444, 0.4433236557529246, 0.45173687817320485, 0.45199037349915305, 0.4523930696742717, 0.4427297618622298, 0.44785301097110114, 0.4432474258523109, 0.44717575115779323, 0.4469770427052361, 0.4419270935542332, 0.4453661380663546, 0.4389979027418685, 0.4360240042803375, 0.44065245601914527, 0.44258262554950123, 0.4486975683817696, 0.44157156511976453, 0.4459178833707481, 0.4400572524021298, 0.4480909918687944, 0.448594232767151, 0.4381008956047936, 0.4453074123366257, 0.4371536089913706, 0.4397949051665426, 0.44376357234936253, 0.4430054919677798, 0.4464725515909132, 0.44479463744539927, 0.44672464959656455, 0.44854719951364685, 0.4486633520827192, 0.45347644892569544, 0.4560416866838045, 0.4582668263659465, 0.4599854277327661, 0.47250926099045215, 0.4718377696821027, 0.4827638441741166, 0.4907167963196982, 0.5072917539486441, 0.5210123893813046, 0.5429097481289704], 'val_losses': [0.3526338349008841, 0.3078672076982598, 0.3257580107446004, 0.3187966708300661, 0.3295327105695122, 0.3320468669658525, 0.31235748839929517, 0.33799647957239665, 0.3255878785374678, 0.34802488619343797, 0.30821028175344745, 0.2945554377425958, 0.32293546647897653, 0.33239748177037265, 0.3033503109822371, 0.34106057293432096, 0.30830006601442717, 0.36008053448561395, 0.3166610082802117, 0.33883182158327957, 0.31972902153625987, 0.32904797066514196, 0.34596010443262404, 0.336828844183714, 0.32082262806255496, 0.3325140803189762, 0.3271958674797213, 0.3302090691901765, 0.3325911980309503, 0.3263330908739362, 0.3247518828527295, 0.31405024232160567, 0.3118200054511945, 0.332050274352265, 0.32684931282246715, 0.33619973682592486, 0.3268220467520244, 0.35262275810842747, 0.3340633509638853, 0.3469401506564474, 0.34146988518837623, 0.3535542803970648, 0.3666385306912905, 0.37498224669568686, 0.37672601115015003, 0.39736023211057975], 'val_acc': [0.3688760806916426, 0.3555945370254354, 0.424132314246335, 0.28029069038967547, 0.3093597293572234, 0.28041598797143213, 0.4042099987470242, 0.26099486279914796, 0.39456208495176043, 0.25360230547550433, 0.4112266633253978, 0.46059391053752663, 0.34293948126801155, 0.35459215637138203, 0.42939481268011526, 0.3515850144092219, 0.3177546673349204, 0.24708683122415737, 0.2717704548302218, 0.2447061771707806, 0.29244455582007267, 0.25924069665455457, 0.23819070291943364, 0.2594912918180679, 0.2896880090214259, 0.3028442551058765, 0.26913920561333166, 0.26688384914171154, 0.24370379651672722, 0.25999248214509457, 0.26638265881468487, 0.2889362235308858, 0.3093597293572234, 0.2507204610951009, 0.27314872822954517, 0.25159754416739755, 0.2886856283673725, 0.21663951885728605, 0.24345320135321388, 0.2578624232552312, 0.261746648289688, 0.24107254729983713, 0.215135947876206, 0.21927076807417617, 0.22215261245457962, 0.22027314872822953], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0039732940635894675, 'batch_size': 64, 'epochs': 46, 'weight_decay': 0.007034598734157888, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 2, 'grad_clip': 0.3789540121715964, 'amp': False, 'patch_size': 50, 'd_model': 104, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.0565089484978062, 'head_hidden': 104, 'use_focal_loss': True, 'focal_gamma': 1.1868826617711636, 'label_smoothing': 0.15711242207551043, 'class_weight_power': 0.5459400242250578, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 5}, 'model_parameter_count': 112013, 'model_size_validation': 'PASS'}
2025-09-22 15:34:46,529 - INFO - _models.training_function_executor - BO Objective: base=0.2203, size_penalty=0.0000, final=0.2203
2025-09-22 15:34:46,529 - INFO - _models.training_function_executor - Model size: 112,013 parameters (PASS 256K limit)
2025-09-22 15:34:46,529 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 62.287s
2025-09-22 15:34:46,644 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2203
2025-09-22 15:34:46,644 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.115s
2025-09-22 15:34:46,644 - INFO - bo.run_bo - Recorded observation #25: hparams={'lr': 0.0039732940635894675, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'weight_decay': 0.007034598734157888, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.3789540121715964, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(104), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.0565089484978062, 'head_hidden': np.int64(104), 'use_focal_loss': np.True_, 'focal_gamma': 1.1868826617711636, 'label_smoothing': 0.15711242207551043, 'class_weight_power': 0.5459400242250578, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(5)}, value=0.2203
2025-09-22 15:34:46,644 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 25: {'lr': 0.0039732940635894675, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'weight_decay': 0.007034598734157888, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.3789540121715964, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(104), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.0565089484978062, 'head_hidden': np.int64(104), 'use_focal_loss': np.True_, 'focal_gamma': 1.1868826617711636, 'label_smoothing': 0.15711242207551043, 'class_weight_power': 0.5459400242250578, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(5)} -> 0.2203
2025-09-22 15:34:46,644 - INFO - bo.run_bo - üîçBO Trial 26: Using RF surrogate + Expected Improvement
2025-09-22 15:34:46,645 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:34:46,645 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:34:46,645 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:34:46,645 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0020964767000418052, 'batch_size': 64, 'epochs': 5, 'weight_decay': 0.001909815386648446, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.42976773243031996, 'amp': False, 'patch_size': 40, 'd_model': 111, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.15316287193768088, 'head_hidden': 126, 'use_focal_loss': True, 'focal_gamma': 2.814491191282081, 'label_smoothing': 0.13552992300402442, 'class_weight_power': 0.5302568916066629, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 5}
2025-09-22 15:34:46,646 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0020964767000418052, 'batch_size': 64, 'epochs': 5, 'weight_decay': 0.001909815386648446, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.42976773243031996, 'amp': False, 'patch_size': 40, 'd_model': 111, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.15316287193768088, 'head_hidden': 126, 'use_focal_loss': True, 'focal_gamma': 2.814491191282081, 'label_smoothing': 0.13552992300402442, 'class_weight_power': 0.5302568916066629, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 5}
2025-09-22 15:34:52,743 - INFO - _models.training_function_executor - Model parameter count: 211,566
2025-09-22 15:34:52,743 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.27345904410520233, 0.21582967419664734, 0.20733056820587598, 0.20272814896464986, 0.1852969985458892], 'val_losses': [0.15604275536147325, 0.15800842530385936, 0.17437845987662562, 0.12686998998218127, 0.11957903580137727], 'val_acc': [0.4220022553564716, 0.5206114521989725, 0.5379025184813933, 0.5327653176293697, 0.5989224407968926], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0020964767000418052, 'batch_size': 64, 'epochs': 5, 'weight_decay': 0.001909815386648446, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.42976773243031996, 'amp': False, 'patch_size': 40, 'd_model': 111, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.15316287193768088, 'head_hidden': 126, 'use_focal_loss': True, 'focal_gamma': 2.814491191282081, 'label_smoothing': 0.13552992300402442, 'class_weight_power': 0.5302568916066629, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 5}, 'model_parameter_count': 211566, 'model_size_validation': 'PASS'}
2025-09-22 15:34:52,743 - INFO - _models.training_function_executor - BO Objective: base=0.5989, size_penalty=0.0000, final=0.5989
2025-09-22 15:34:52,743 - INFO - _models.training_function_executor - Model size: 211,566 parameters (PASS 256K limit)
2025-09-22 15:34:52,743 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 6.099s
2025-09-22 15:34:52,859 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5989
2025-09-22 15:34:52,859 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.115s
2025-09-22 15:34:52,859 - INFO - bo.run_bo - Recorded observation #26: hparams={'lr': 0.0020964767000418052, 'batch_size': np.int64(64), 'epochs': np.int64(5), 'weight_decay': 0.001909815386648446, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.42976773243031996, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(111), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.15316287193768088, 'head_hidden': np.int64(126), 'use_focal_loss': np.True_, 'focal_gamma': 2.814491191282081, 'label_smoothing': 0.13552992300402442, 'class_weight_power': 0.5302568916066629, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(5)}, value=0.5989
2025-09-22 15:34:52,859 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 26: {'lr': 0.0020964767000418052, 'batch_size': np.int64(64), 'epochs': np.int64(5), 'weight_decay': 0.001909815386648446, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.42976773243031996, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(111), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.15316287193768088, 'head_hidden': np.int64(126), 'use_focal_loss': np.True_, 'focal_gamma': 2.814491191282081, 'label_smoothing': 0.13552992300402442, 'class_weight_power': 0.5302568916066629, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(5)} -> 0.5989
2025-09-22 15:34:52,859 - INFO - bo.run_bo - üîçBO Trial 27: Using RF surrogate + Expected Improvement
2025-09-22 15:34:52,859 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:34:52,860 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:34:52,860 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:34:52,860 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015893851932705669, 'batch_size': 64, 'epochs': 39, 'weight_decay': 3.4871293537150408e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.0019662353989415853, 'amp': False, 'patch_size': 100, 'd_model': 61, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.1311998378701322, 'head_hidden': 52, 'use_focal_loss': False, 'focal_gamma': 1.7569689187984043, 'label_smoothing': 0.17436627964636883, 'class_weight_power': 0.5362579015973241, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 1}
2025-09-22 15:34:52,861 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015893851932705669, 'batch_size': 64, 'epochs': 39, 'weight_decay': 3.4871293537150408e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.0019662353989415853, 'amp': False, 'patch_size': 100, 'd_model': 61, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.1311998378701322, 'head_hidden': 52, 'use_focal_loss': False, 'focal_gamma': 1.7569689187984043, 'label_smoothing': 0.17436627964636883, 'class_weight_power': 0.5362579015973241, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 1}
2025-09-22 15:35:51,316 - INFO - _models.training_function_executor - Model parameter count: 46,848
2025-09-22 15:35:51,316 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.3823664833145086, 1.178138989720044, 1.1047233536298482, 1.054493128706316, 1.0169772451519836, 0.9922162402159019, 0.9742694451494279, 0.9624690091841853, 0.9466889066564421, 0.9382036925762536, 0.9323067775298579, 0.9210807518922748, 0.9219737672209796, 0.9139819780402967, 0.9093804433449384, 0.9006440296876036, 0.9017043799078823, 0.8895354080076218, 0.888947774257979, 0.8883584291681111, 0.8874166848549504, 0.8922091429833201, 0.8781347170071178, 0.8774070658936364, 0.8847290089647001, 0.8722086068838756, 0.8763331717179352, 0.8753198385219976, 0.8782714674284825, 0.8678041236037558, 0.8721999773456985, 0.8634934768160532, 0.8686939557516623, 0.8673283166799935, 0.8667510997994002, 0.8619724471433955, 0.8572508377101724, 0.8592727441886006, 0.8548102059713997], 'val_losses': [1.767199289259524, 1.6372000573646632, 1.5947654284625525, 1.5660565020730062, 1.4922613812604206, 1.4719088956267445, 1.4375452487454679, 1.4523930226691517, 1.4556521459146186, 1.4366639732433253, 1.4846645777214442, 1.4153893051134316, 1.4141909889284046, 1.4242385696937618, 1.3892366674337422, 1.396501564065431, 1.3763921404076076, 1.391474214446767, 1.4071709415605231, 1.3770960317251304, 1.3863477697051, 1.391352939315942, 1.370569195399053, 1.3684906497244815, 1.3698301832264579, 1.3811703460166633, 1.3627964085675828, 1.3909185323152398, 1.3680132210456313, 1.3775436436677395, 1.3601457579454643, 1.3842456331827813, 1.3820121419592588, 1.3572986670386415, 1.3684585954951487, 1.3712974936423512, 1.3650986178598523, 1.3558134833100355, 1.3657211725939755], 'val_acc': [0.47713319132940735, 0.678235810048866, 0.698534018293447, 0.739756922691392, 0.8183185064528254, 0.8297205863926826, 0.8440045107129432, 0.8392432026061897, 0.8465104623480767, 0.84826462849267, 0.8198220774339056, 0.8881092594912918, 0.8719458714446812, 0.8733241448440046, 0.8958777095602055, 0.8941235434156121, 0.9259491291818068, 0.9107881217892495, 0.9007643152487157, 0.916677108131813, 0.9060268136824959, 0.9126675855155996, 0.9292068663074803, 0.9330910913419371, 0.9265756170905901, 0.9136699661696529, 0.9310863300338303, 0.9081568725723593, 0.9330910913419371, 0.9279538904899135, 0.9345946623230171, 0.9160506202230297, 0.9270768074176169, 0.9441172785365243, 0.9318381155243703, 0.9383535897757174, 0.9348452574865305, 0.9401077559203107, 0.9365994236311239], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0015893851932705669, 'batch_size': 64, 'epochs': 39, 'weight_decay': 3.4871293537150408e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.0019662353989415853, 'amp': False, 'patch_size': 100, 'd_model': 61, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.1311998378701322, 'head_hidden': 52, 'use_focal_loss': False, 'focal_gamma': 1.7569689187984043, 'label_smoothing': 0.17436627964636883, 'class_weight_power': 0.5362579015973241, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 1}, 'model_parameter_count': 46848, 'model_size_validation': 'PASS'}
2025-09-22 15:35:51,316 - INFO - _models.training_function_executor - BO Objective: base=0.9366, size_penalty=0.0000, final=0.9366
2025-09-22 15:35:51,316 - INFO - _models.training_function_executor - Model size: 46,848 parameters (PASS 256K limit)
2025-09-22 15:35:51,316 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 58.457s
2025-09-22 15:35:51,431 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9366
2025-09-22 15:35:51,431 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.115s
2025-09-22 15:35:51,431 - INFO - bo.run_bo - Recorded observation #27: hparams={'lr': 0.0015893851932705669, 'batch_size': np.int64(64), 'epochs': np.int64(39), 'weight_decay': 3.4871293537150408e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.0019662353989415853, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(61), 'n_heads': np.int64(8), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.1311998378701322, 'head_hidden': np.int64(52), 'use_focal_loss': np.False_, 'focal_gamma': 1.7569689187984043, 'label_smoothing': 0.17436627964636883, 'class_weight_power': 0.5362579015973241, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(1)}, value=0.9366
2025-09-22 15:35:51,431 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 27: {'lr': 0.0015893851932705669, 'batch_size': np.int64(64), 'epochs': np.int64(39), 'weight_decay': 3.4871293537150408e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.0019662353989415853, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(61), 'n_heads': np.int64(8), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.1311998378701322, 'head_hidden': np.int64(52), 'use_focal_loss': np.False_, 'focal_gamma': 1.7569689187984043, 'label_smoothing': 0.17436627964636883, 'class_weight_power': 0.5362579015973241, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(1)} -> 0.9366
2025-09-22 15:35:51,432 - INFO - bo.run_bo - üîçBO Trial 28: Using RF surrogate + Expected Improvement
2025-09-22 15:35:51,432 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:35:51,432 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:35:51,432 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:35:51,432 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007770714106483508, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.00013590196054092594, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.030045535045079903, 'amp': True, 'patch_size': 200, 'd_model': 109, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.10698213221684547, 'head_hidden': 127, 'use_focal_loss': True, 'focal_gamma': 1.288260600609262, 'label_smoothing': 0.12336982813980304, 'class_weight_power': 0.5392201731304813, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 4}
2025-09-22 15:35:51,433 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007770714106483508, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.00013590196054092594, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.030045535045079903, 'amp': True, 'patch_size': 200, 'd_model': 109, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.10698213221684547, 'head_hidden': 127, 'use_focal_loss': True, 'focal_gamma': 1.288260600609262, 'label_smoothing': 0.12336982813980304, 'class_weight_power': 0.5392201731304813, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 4}
2025-09-22 15:36:07,276 - INFO - _models.training_function_executor - Model parameter count: 89,527
2025-09-22 15:36:07,277 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4841983031872553, 0.29518044669873933, 0.2313239626194105, 0.187246113691164, 0.1675761982201559, 0.1466602567542829, 0.1309026663704175, 0.11645038101507175, 0.112043671988886, 0.10393954977835196, 0.08929509431832312, 0.09030901462972095], 'val_losses': [0.2413434761909745, 0.19035533387220946, 0.17509255563200096, 0.13807875101331213, 0.13150653388697078, 0.11509427569210044, 0.11392951563286342, 0.11763896868930486, 0.09505737020019496, 0.09081523431038635, 0.10317891348391872, 0.08531038318557065], 'val_acc': [0.5099611577496554, 0.6231048740759303, 0.6609447437664453, 0.7321137702042351, 0.7467735872697657, 0.7803533391805538, 0.7988973812805413, 0.7802280415987971, 0.8496429018919934, 0.8462598671845634, 0.8312241573737627, 0.8513970680365869], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007770714106483508, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.00013590196054092594, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.030045535045079903, 'amp': True, 'patch_size': 200, 'd_model': 109, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.10698213221684547, 'head_hidden': 127, 'use_focal_loss': True, 'focal_gamma': 1.288260600609262, 'label_smoothing': 0.12336982813980304, 'class_weight_power': 0.5392201731304813, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 4}, 'model_parameter_count': 89527, 'model_size_validation': 'PASS'}
2025-09-22 15:36:07,277 - INFO - _models.training_function_executor - BO Objective: base=0.8514, size_penalty=0.0000, final=0.8514
2025-09-22 15:36:07,277 - INFO - _models.training_function_executor - Model size: 89,527 parameters (PASS 256K limit)
2025-09-22 15:36:07,277 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 15.845s
2025-09-22 15:36:07,393 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8514
2025-09-22 15:36:07,394 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.117s
2025-09-22 15:36:07,394 - INFO - bo.run_bo - Recorded observation #28: hparams={'lr': 0.0007770714106483508, 'batch_size': np.int64(64), 'epochs': np.int64(12), 'weight_decay': 0.00013590196054092594, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.030045535045079903, 'amp': np.True_, 'patch_size': np.int64(200), 'd_model': np.int64(109), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.10698213221684547, 'head_hidden': np.int64(127), 'use_focal_loss': np.True_, 'focal_gamma': 1.288260600609262, 'label_smoothing': 0.12336982813980304, 'class_weight_power': 0.5392201731304813, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(4)}, value=0.8514
2025-09-22 15:36:07,394 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 28: {'lr': 0.0007770714106483508, 'batch_size': np.int64(64), 'epochs': np.int64(12), 'weight_decay': 0.00013590196054092594, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.030045535045079903, 'amp': np.True_, 'patch_size': np.int64(200), 'd_model': np.int64(109), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.10698213221684547, 'head_hidden': np.int64(127), 'use_focal_loss': np.True_, 'focal_gamma': 1.288260600609262, 'label_smoothing': 0.12336982813980304, 'class_weight_power': 0.5392201731304813, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(4)} -> 0.8514
2025-09-22 15:36:07,394 - INFO - bo.run_bo - üîçBO Trial 29: Using RF surrogate + Expected Improvement
2025-09-22 15:36:07,394 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:36:07,394 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:36:07,394 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:36:07,394 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003980416451442493, 'batch_size': 32, 'epochs': 15, 'weight_decay': 0.00026302534754921544, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 5, 'grad_clip': 0.151584773162071, 'amp': False, 'patch_size': 100, 'd_model': 45, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.08889795245565187, 'head_hidden': 85, 'use_focal_loss': False, 'focal_gamma': 1.7191346984062488, 'label_smoothing': 0.0016744978280744953, 'class_weight_power': 0.5404410335504044, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 10}
2025-09-22 15:36:07,396 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003980416451442493, 'batch_size': 32, 'epochs': 15, 'weight_decay': 0.00026302534754921544, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 5, 'grad_clip': 0.151584773162071, 'amp': False, 'patch_size': 100, 'd_model': 45, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.08889795245565187, 'head_hidden': 85, 'use_focal_loss': False, 'focal_gamma': 1.7191346984062488, 'label_smoothing': 0.0016744978280744953, 'class_weight_power': 0.5404410335504044, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 10}
2025-09-22 15:36:41,359 - INFO - _models.training_function_executor - Model parameter count: 42,975
2025-09-22 15:36:41,359 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9149319262513989, 0.6100721197381405, 0.49338660960739217, 0.45470504658348343, 0.4245147258888169, 0.37391542855952314, 0.330003442746227, 0.28595660187830785, 0.2548534186453134, 0.22975633170485918, 0.21107116018249267, 0.17474573743317962, 0.15006411493830943, 0.13873519021712558, 0.12663606521384824], 'val_losses': [0.8642101102851564, 0.7066200840048975, 0.5294629855347731, 0.5940536791602258, 0.5971260408966214, 0.465268558830624, 0.4193626421090281, 0.4232916735404865, 0.368776123672481, 0.33147410912675646, 0.35922869868541984, 0.3049119204122192, 0.30210298118309303, 0.3106313414359179, 0.2999069233493927], 'val_acc': [0.5634632251597544, 0.6892619972434532, 0.8397443929332163, 0.7331161508582884, 0.7477759679238191, 0.84488159378524, 0.8675604560831975, 0.8435033203859166, 0.9107881217892495, 0.9112893121162762, 0.900639017666959, 0.9256985340182935, 0.939606565593284, 0.936975316376394, 0.9401077559203107], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003980416451442493, 'batch_size': 32, 'epochs': 15, 'weight_decay': 0.00026302534754921544, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 5, 'grad_clip': 0.151584773162071, 'amp': False, 'patch_size': 100, 'd_model': 45, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.08889795245565187, 'head_hidden': 85, 'use_focal_loss': False, 'focal_gamma': 1.7191346984062488, 'label_smoothing': 0.0016744978280744953, 'class_weight_power': 0.5404410335504044, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 10}, 'model_parameter_count': 42975, 'model_size_validation': 'PASS'}
2025-09-22 15:36:41,359 - INFO - _models.training_function_executor - BO Objective: base=0.9401, size_penalty=0.0000, final=0.9401
2025-09-22 15:36:41,359 - INFO - _models.training_function_executor - Model size: 42,975 parameters (PASS 256K limit)
2025-09-22 15:36:41,359 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 33.965s
2025-09-22 15:36:41,476 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9401
2025-09-22 15:36:41,476 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.117s
2025-09-22 15:36:41,476 - INFO - bo.run_bo - Recorded observation #29: hparams={'lr': 0.003980416451442493, 'batch_size': np.int64(32), 'epochs': np.int64(15), 'weight_decay': 0.00026302534754921544, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.151584773162071, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(45), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.08889795245565187, 'head_hidden': np.int64(85), 'use_focal_loss': np.False_, 'focal_gamma': 1.7191346984062488, 'label_smoothing': 0.0016744978280744953, 'class_weight_power': 0.5404410335504044, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(10)}, value=0.9401
2025-09-22 15:36:41,476 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 29: {'lr': 0.003980416451442493, 'batch_size': np.int64(32), 'epochs': np.int64(15), 'weight_decay': 0.00026302534754921544, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.151584773162071, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(45), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.08889795245565187, 'head_hidden': np.int64(85), 'use_focal_loss': np.False_, 'focal_gamma': 1.7191346984062488, 'label_smoothing': 0.0016744978280744953, 'class_weight_power': 0.5404410335504044, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(10)} -> 0.9401
2025-09-22 15:36:41,477 - INFO - bo.run_bo - üîçBO Trial 30: Using RF surrogate + Expected Improvement
2025-09-22 15:36:41,477 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:36:41,477 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:36:41,477 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:36:41,477 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0031209410061989427, 'batch_size': 64, 'epochs': 30, 'weight_decay': 1.9957360729812688e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.7550673940259013, 'amp': True, 'patch_size': 25, 'd_model': 48, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.18667849100462428, 'head_hidden': 96, 'use_focal_loss': True, 'focal_gamma': 1.6469024275888038, 'label_smoothing': 0.10014304483924408, 'class_weight_power': 0.5412896443225593, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 8}
2025-09-22 15:36:41,478 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0031209410061989427, 'batch_size': 64, 'epochs': 30, 'weight_decay': 1.9957360729812688e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.7550673940259013, 'amp': True, 'patch_size': 25, 'd_model': 48, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.18667849100462428, 'head_hidden': 96, 'use_focal_loss': True, 'focal_gamma': 1.6469024275888038, 'label_smoothing': 0.10014304483924408, 'class_weight_power': 0.5412896443225593, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 8}
2025-09-22 15:37:29,466 - INFO - _models.training_function_executor - Model parameter count: 80,693
2025-09-22 15:37:29,467 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3508524321540348, 0.22452188598074507, 0.1863298536848164, 0.162418317394112, 0.1439998489751615, 0.12343577548749009, 0.11038804053875156, 0.10361681793658643, 0.09339794599300966, 0.08696100743694554, 0.08260806812667736, 0.08092342247156684, 0.07401936111079419, 0.07358585110157276, 0.06670474812691363, 0.06840639610333733, 0.06668693276294747, 0.06362603138604211, 0.05975221069205529, 0.061575398879743094, 0.057988367358836015, 0.05500522313452049, 0.05453464358436342, 0.05534809902326709, 0.052128637991573704, 0.05268307285794639, 0.04913033358644783, 0.04858959261277219, 0.048057360153271736, 0.047850876826846286], 'val_losses': [0.1890426505982032, 0.16779103022184966, 0.14801589051272512, 0.12203302101755244, 0.12130519909236444, 0.10396770541641949, 0.08994813322042586, 0.09147409765105127, 0.10398350327986701, 0.08729724226970599, 0.08557671964235763, 0.07782786314595898, 0.0791904306569698, 0.08091286249816336, 0.06971567057193542, 0.0727224472343974, 0.07101052364129319, 0.060517568081942225, 0.06969176089609794, 0.06605322712880898, 0.06126027694491273, 0.07100537611628338, 0.06496079958010968, 0.0696308349938698, 0.07090251080438825, 0.06609221656599404, 0.06191622018590037, 0.07005494691747992, 0.05512377297485432, 0.06985930878892606], 'val_acc': [0.5634632251597544, 0.5875203608570354, 0.6014283924320261, 0.6791128931211627, 0.6893872948252099, 0.7803533391805538, 0.7853652424508207, 0.7898759553940609, 0.7266006766069415, 0.7997744643528379, 0.8184438040345822, 0.8255857661947125, 0.8359854654805162, 0.8461345696028066, 0.8462598671845634, 0.8346071920811928, 0.8738253351710312, 0.8812178924946749, 0.8622979576494174, 0.8377396316251097, 0.8695652173913043, 0.839493797769703, 0.8888610449818318, 0.8664327778473876, 0.8634256358852274, 0.8701917053000877, 0.8907405087081819, 0.8793384287683248, 0.8975065781230422, 0.8462598671845634], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0031209410061989427, 'batch_size': 64, 'epochs': 30, 'weight_decay': 1.9957360729812688e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.7550673940259013, 'amp': True, 'patch_size': 25, 'd_model': 48, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.18667849100462428, 'head_hidden': 96, 'use_focal_loss': True, 'focal_gamma': 1.6469024275888038, 'label_smoothing': 0.10014304483924408, 'class_weight_power': 0.5412896443225593, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 8}, 'model_parameter_count': 80693, 'model_size_validation': 'PASS'}
2025-09-22 15:37:29,467 - INFO - _models.training_function_executor - BO Objective: base=0.8463, size_penalty=0.0000, final=0.8463
2025-09-22 15:37:29,467 - INFO - _models.training_function_executor - Model size: 80,693 parameters (PASS 256K limit)
2025-09-22 15:37:29,467 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 47.990s
2025-09-22 15:37:29,584 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8463
2025-09-22 15:37:29,584 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.118s
2025-09-22 15:37:29,584 - INFO - bo.run_bo - Recorded observation #30: hparams={'lr': 0.0031209410061989427, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'weight_decay': 1.9957360729812688e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.7550673940259013, 'amp': np.True_, 'patch_size': np.int64(25), 'd_model': np.int64(48), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.18667849100462428, 'head_hidden': np.int64(96), 'use_focal_loss': np.True_, 'focal_gamma': 1.6469024275888038, 'label_smoothing': 0.10014304483924408, 'class_weight_power': 0.5412896443225593, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(8)}, value=0.8463
2025-09-22 15:37:29,584 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 30: {'lr': 0.0031209410061989427, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'weight_decay': 1.9957360729812688e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.7550673940259013, 'amp': np.True_, 'patch_size': np.int64(25), 'd_model': np.int64(48), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.18667849100462428, 'head_hidden': np.int64(96), 'use_focal_loss': np.True_, 'focal_gamma': 1.6469024275888038, 'label_smoothing': 0.10014304483924408, 'class_weight_power': 0.5412896443225593, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(8)} -> 0.8463
2025-09-22 15:37:29,585 - INFO - bo.run_bo - üîçBO Trial 31: Using RF surrogate + Expected Improvement
2025-09-22 15:37:29,585 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:37:29,585 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:37:29,585 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:37:29,585 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0010003687239755732, 'batch_size': 256, 'epochs': 45, 'weight_decay': 2.6912951930279187e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.020943207242356435, 'amp': False, 'patch_size': 40, 'd_model': 96, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.25190247990879977, 'head_hidden': 50, 'use_focal_loss': False, 'focal_gamma': 1.8943990768285084, 'label_smoothing': 0.1704077924153783, 'class_weight_power': 0.5424120603264377, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:37:29,586 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0010003687239755732, 'batch_size': 256, 'epochs': 45, 'weight_decay': 2.6912951930279187e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.020943207242356435, 'amp': False, 'patch_size': 40, 'd_model': 96, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.25190247990879977, 'head_hidden': 50, 'use_focal_loss': False, 'focal_gamma': 1.8943990768285084, 'label_smoothing': 0.1704077924153783, 'class_weight_power': 0.5424120603264377, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:38:06,477 - INFO - _models.training_function_executor - Model parameter count: 115,488
2025-09-22 15:38:06,477 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5782072469711124, 1.3378363354360012, 1.2165808246065732, 1.1530584849701249, 1.1146097303769966, 1.0712357734381375, 1.0460739196312143, 1.0298577236605666, 1.0206088055174583, 1.0105310923226498, 0.9982216089428978, 0.9975548760281452, 0.9825930167188531, 0.9710547407703419, 0.9672253829167794, 0.9475555277645963, 0.9461196381204862, 0.9431224584736307, 0.9315977309149944, 0.9282366459136566, 0.9234863082571585, 0.9211854121217721, 0.9171572165751641, 0.9103912005850805, 0.898743484653417, 0.8955318175345641, 0.8992029638983188, 0.8891933925434211, 0.885193392525631, 0.8933107469668384, 0.8855687840235912, 0.8822665689799868, 0.8786373363977998, 0.8773870563774454, 0.8767635252819068, 0.8696130377881169, 0.8693789567609211, 0.8679858706437589, 0.8763245787943336, 0.875072394142032, 0.8692527842119112, 0.8693763864569521, 0.8626219463225905, 0.8596398791950777, 0.8663939418426915], 'val_losses': [1.9674481941694426, 1.8994783804599182, 1.8008711379716427, 1.8199967570867086, 1.6981351572444692, 1.7419753072853599, 1.7595028299509232, 1.6618167255444867, 1.64683903241991, 1.6885717913794078, 1.6438924661332124, 1.5959061254102953, 1.613128609547354, 1.5934557215998069, 1.5982208141720753, 1.5577411165871935, 1.5496865560828914, 1.635740834210932, 1.6003691807469544, 1.6185218942238564, 1.7147341773498328, 1.6952577193209648, 1.5408941648490686, 1.6530285642314653, 1.595070233247645, 1.5146249621124468, 1.5669055800586695, 1.5669196987343288, 1.5830804283137072, 1.5444885656627823, 1.5499495415292637, 1.5490096844373349, 1.5868822986986117, 1.583627867154976, 1.5790369224763428, 1.5595649595981163, 1.5746738183858355, 1.5475480807124784, 1.5619958302323762, 1.5535545107147595, 1.5345099040431616, 1.5521654482847347, 1.5569270945270473, 1.5410464997728313, 1.584922438231854], 'val_acc': [0.35308858539030197, 0.2379401077559203, 0.5683498308482646, 0.5119659190577622, 0.609071544919183, 0.6598170655306352, 0.6322515975441674, 0.6768575366495426, 0.6793634882846761, 0.6678361107630623, 0.7364991855657186, 0.7467735872697657, 0.7286054379150483, 0.7511590026312492, 0.7338679363488285, 0.7604310236812429, 0.7942613707555444, 0.7635634632251598, 0.7507831098859792, 0.7392557323643654, 0.6866307480265631, 0.7453953138704423, 0.8151860669089087, 0.7356221024934219, 0.7991479764440547, 0.8280917178298459, 0.7874953013406841, 0.8025310111514847, 0.7953890489913544, 0.8118030322014785, 0.8190702919433654, 0.8191955895251222, 0.7976444054629745, 0.8012780353339181, 0.8090464854028317, 0.816940233053502, 0.8115524370379652, 0.8217015411602556, 0.8124295201102619, 0.8209497556697156, 0.831098859792006, 0.8269640395940359, 0.8206991605062023, 0.829219396065656, 0.8066658313494549], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0010003687239755732, 'batch_size': 256, 'epochs': 45, 'weight_decay': 2.6912951930279187e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.020943207242356435, 'amp': False, 'patch_size': 40, 'd_model': 96, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.25190247990879977, 'head_hidden': 50, 'use_focal_loss': False, 'focal_gamma': 1.8943990768285084, 'label_smoothing': 0.1704077924153783, 'class_weight_power': 0.5424120603264377, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 2}, 'model_parameter_count': 115488, 'model_size_validation': 'PASS'}
2025-09-22 15:38:06,478 - INFO - _models.training_function_executor - BO Objective: base=0.8067, size_penalty=0.0000, final=0.8067
2025-09-22 15:38:06,478 - INFO - _models.training_function_executor - Model size: 115,488 parameters (PASS 256K limit)
2025-09-22 15:38:06,478 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 36.893s
2025-09-22 15:38:06,596 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8067
2025-09-22 15:38:06,596 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.118s
2025-09-22 15:38:06,596 - INFO - bo.run_bo - Recorded observation #31: hparams={'lr': 0.0010003687239755732, 'batch_size': np.int64(256), 'epochs': np.int64(45), 'weight_decay': 2.6912951930279187e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.020943207242356435, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(96), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.25190247990879977, 'head_hidden': np.int64(50), 'use_focal_loss': np.False_, 'focal_gamma': 1.8943990768285084, 'label_smoothing': 0.1704077924153783, 'class_weight_power': 0.5424120603264377, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)}, value=0.8067
2025-09-22 15:38:06,596 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 31: {'lr': 0.0010003687239755732, 'batch_size': np.int64(256), 'epochs': np.int64(45), 'weight_decay': 2.6912951930279187e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.020943207242356435, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(96), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.25190247990879977, 'head_hidden': np.int64(50), 'use_focal_loss': np.False_, 'focal_gamma': 1.8943990768285084, 'label_smoothing': 0.1704077924153783, 'class_weight_power': 0.5424120603264377, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)} -> 0.8067
2025-09-22 15:38:06,597 - INFO - bo.run_bo - üîçBO Trial 32: Using RF surrogate + Expected Improvement
2025-09-22 15:38:06,597 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:38:06,597 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:38:06,597 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:38:06,597 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00036893820629759925, 'batch_size': 32, 'epochs': 30, 'weight_decay': 0.00027571100934569825, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.3051526437442427, 'amp': False, 'patch_size': 20, 'd_model': 80, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.09864686045734686, 'head_hidden': 116, 'use_focal_loss': False, 'focal_gamma': 2.4966056162443238, 'label_smoothing': 0.037554953706023006, 'class_weight_power': 0.5420688691468655, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 3}
2025-09-22 15:38:06,598 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00036893820629759925, 'batch_size': 32, 'epochs': 30, 'weight_decay': 0.00027571100934569825, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.3051526437442427, 'amp': False, 'patch_size': 20, 'd_model': 80, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.09864686045734686, 'head_hidden': 116, 'use_focal_loss': False, 'focal_gamma': 2.4966056162443238, 'label_smoothing': 0.037554953706023006, 'class_weight_power': 0.5420688691468655, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 3}
2025-09-22 15:39:23,267 - INFO - _models.training_function_executor - Model parameter count: 173,893
2025-09-22 15:39:23,267 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9814425539007702, 0.7236418220959738, 0.6432063983981406, 0.5925894260977839, 0.5508240426583378, 0.5268371154047756, 0.5017812900160935, 0.4884607757433268, 0.4722462968104757, 0.45741841350399237, 0.43682487104947537, 0.43126514441715397, 0.4214731434758839, 0.4150794429244006, 0.39925038206417846, 0.4002390122781835, 0.3968588579271617, 0.3884851173627415, 0.3836842310250121, 0.38019864286870186, 0.3759930452932127, 0.3695140857223475, 0.3673177913302292, 0.36653817004605244, 0.35703169705033305, 0.35110652578566764, 0.35338563036359244, 0.3490085716542268, 0.34231647653873676, 0.3395729084163591], 'val_losses': [1.118705478113048, 0.9093193242445639, 0.8596484508599245, 0.8043656238651025, 0.7525275636923192, 0.7806237792180095, 0.8155308810858899, 0.7831037621228651, 0.7867081670620106, 0.7862063284999311, 0.7677527779460984, 0.7270578585566023, 0.7591162747629393, 0.7010564948215683, 0.7031891063526002, 0.6659235296176499, 0.6848614373731069, 0.6927800786448906, 0.6915017175668464, 0.6773936390622847, 0.6913052998215256, 0.6960426056509015, 0.6439681299018645, 0.6454786543872418, 0.6536480868733507, 0.6579559394372554, 0.6736953267775119, 0.6252271092755053, 0.6593039934385123, 0.6509700681673019], 'val_acc': [0.6263626112016039, 0.753915549429896, 0.785991730359604, 0.8105500563839118, 0.8423756421501065, 0.8477634381656434, 0.8034080942237815, 0.8239568976318757, 0.8210750532514722, 0.8183185064528254, 0.8490164139832101, 0.8690640270642777, 0.8550306979075304, 0.8735747400075179, 0.8782107505325147, 0.9110387169527628, 0.8931211627615587, 0.8862297957649418, 0.8970053877960156, 0.8901140207993986, 0.8909911038716952, 0.8828467610575116, 0.9156747274777597, 0.9141711564966796, 0.899887232176419, 0.9134193710061396, 0.9067785991730359, 0.9317128179426137, 0.9160506202230297, 0.9270768074176169], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00036893820629759925, 'batch_size': 32, 'epochs': 30, 'weight_decay': 0.00027571100934569825, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 3, 'grad_clip': 0.3051526437442427, 'amp': False, 'patch_size': 20, 'd_model': 80, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.09864686045734686, 'head_hidden': 116, 'use_focal_loss': False, 'focal_gamma': 2.4966056162443238, 'label_smoothing': 0.037554953706023006, 'class_weight_power': 0.5420688691468655, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 3}, 'model_parameter_count': 173893, 'model_size_validation': 'PASS'}
2025-09-22 15:39:23,267 - INFO - _models.training_function_executor - BO Objective: base=0.9271, size_penalty=0.0000, final=0.9271
2025-09-22 15:39:23,267 - INFO - _models.training_function_executor - Model size: 173,893 parameters (PASS 256K limit)
2025-09-22 15:39:23,267 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 76.671s
2025-09-22 15:39:23,387 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9271
2025-09-22 15:39:23,387 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.119s
2025-09-22 15:39:23,387 - INFO - bo.run_bo - Recorded observation #32: hparams={'lr': 0.00036893820629759925, 'batch_size': np.int64(32), 'epochs': np.int64(30), 'weight_decay': 0.00027571100934569825, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.3051526437442427, 'amp': np.False_, 'patch_size': np.int64(20), 'd_model': np.int64(80), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.09864686045734686, 'head_hidden': np.int64(116), 'use_focal_loss': np.False_, 'focal_gamma': 2.4966056162443238, 'label_smoothing': 0.037554953706023006, 'class_weight_power': 0.5420688691468655, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(3)}, value=0.9271
2025-09-22 15:39:23,387 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 32: {'lr': 0.00036893820629759925, 'batch_size': np.int64(32), 'epochs': np.int64(30), 'weight_decay': 0.00027571100934569825, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.3051526437442427, 'amp': np.False_, 'patch_size': np.int64(20), 'd_model': np.int64(80), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.09864686045734686, 'head_hidden': np.int64(116), 'use_focal_loss': np.False_, 'focal_gamma': 2.4966056162443238, 'label_smoothing': 0.037554953706023006, 'class_weight_power': 0.5420688691468655, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(3)} -> 0.9271
2025-09-22 15:39:23,387 - INFO - bo.run_bo - üîçBO Trial 33: Using RF surrogate + Expected Improvement
2025-09-22 15:39:23,387 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:39:23,387 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:39:23,387 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:39:23,387 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007252977917112599, 'batch_size': 32, 'epochs': 35, 'weight_decay': 2.373377497297425e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.8859015006057274, 'amp': False, 'patch_size': 50, 'd_model': 72, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.044651251527535414, 'head_hidden': 94, 'use_focal_loss': True, 'focal_gamma': 1.7930080243819966, 'label_smoothing': 0.10279913467766306, 'class_weight_power': 0.5438838997916643, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 7}
2025-09-22 15:39:23,389 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007252977917112599, 'batch_size': 32, 'epochs': 35, 'weight_decay': 2.373377497297425e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.8859015006057274, 'amp': False, 'patch_size': 50, 'd_model': 72, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.044651251527535414, 'head_hidden': 94, 'use_focal_loss': True, 'focal_gamma': 1.7930080243819966, 'label_smoothing': 0.10279913467766306, 'class_weight_power': 0.5438838997916643, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 7}
2025-09-22 15:40:32,471 - INFO - _models.training_function_executor - Model parameter count: 121,789
2025-09-22 15:40:32,471 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.32750357518263923, 0.20030548778891702, 0.14625080991436604, 0.12422960710354716, 0.0986946777115638, 0.08656377730514121, 0.07625391020266009, 0.06575762011521903, 0.05877429258478721, 0.05278866427610946, 0.04928060895431191, 0.04496140928946978, 0.042312685959572084, 0.039482081495499664, 0.03832140817413917, 0.03422194728421249, 0.03210628312171538, 0.030522517265252323, 0.03071987230845452, 0.027159436737737342, 0.026784668910370422, 0.025137059704282413, 0.024279662027855396, 0.022147040633399007, 0.02198016835086689, 0.02204399922236516, 0.02066077374010152, 0.019784494346242323, 0.019214583456323018, 0.02015844562056187, 0.017686710656022408, 0.017163951034125435, 0.01790957156577751, 0.0162118688146597, 0.0148465547238067], 'val_losses': [0.1845764010106542, 0.14899243610258645, 0.1049033158342427, 0.09418217637846306, 0.07905771344052477, 0.07355409150187227, 0.06952476387012872, 0.06569355535426544, 0.06531541504674411, 0.06623770801618634, 0.06332865076590236, 0.06835555172163717, 0.06237221821720641, 0.06046945173953831, 0.05737014712508856, 0.05438512161909944, 0.055745822244823585, 0.06202243468045625, 0.055376689611760135, 0.059627629444049784, 0.06828022633033085, 0.052986229382583246, 0.05370559328236969, 0.05611087108452642, 0.05089485768381403, 0.05496636841176835, 0.05540657341162852, 0.056987651558904485, 0.06286142128494625, 0.05578539554033671, 0.0577202812307467, 0.051660876661630564, 0.06274559341586707, 0.053888913510884685, 0.06382827166347828], 'val_acc': [0.590778097982709, 0.637388798396191, 0.7759679238190703, 0.7630622728981331, 0.823831600050119, 0.8487658188196968, 0.8418744518230799, 0.8673098609196842, 0.8785866432777847, 0.8585390301967172, 0.8812178924946749, 0.8837238441298083, 0.9030196717203358, 0.8720711690264378, 0.9095351459716827, 0.9085327653176294, 0.9184312742764065, 0.9283297832351836, 0.9188071670216765, 0.9027690765568225, 0.8904899135446686, 0.9280791880716702, 0.9032702668838491, 0.9272021049993735, 0.9249467485277534, 0.9205613331662699, 0.9367247212128806, 0.936975316376394, 0.9240696654554567, 0.9402330535020674, 0.9422378148101742, 0.9468738253351711, 0.9330910913419371, 0.9517604310236812, 0.9542663826588147], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007252977917112599, 'batch_size': 32, 'epochs': 35, 'weight_decay': 2.373377497297425e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.8859015006057274, 'amp': False, 'patch_size': 50, 'd_model': 72, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.044651251527535414, 'head_hidden': 94, 'use_focal_loss': True, 'focal_gamma': 1.7930080243819966, 'label_smoothing': 0.10279913467766306, 'class_weight_power': 0.5438838997916643, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 7}, 'model_parameter_count': 121789, 'model_size_validation': 'PASS'}
2025-09-22 15:40:32,471 - INFO - _models.training_function_executor - BO Objective: base=0.9543, size_penalty=0.0000, final=0.9543
2025-09-22 15:40:32,471 - INFO - _models.training_function_executor - Model size: 121,789 parameters (PASS 256K limit)
2025-09-22 15:40:32,471 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 69.084s
2025-09-22 15:40:32,591 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9543
2025-09-22 15:40:32,591 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.120s
2025-09-22 15:40:32,591 - INFO - bo.run_bo - Recorded observation #33: hparams={'lr': 0.0007252977917112599, 'batch_size': np.int64(32), 'epochs': np.int64(35), 'weight_decay': 2.373377497297425e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.8859015006057274, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(72), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.044651251527535414, 'head_hidden': np.int64(94), 'use_focal_loss': np.True_, 'focal_gamma': 1.7930080243819966, 'label_smoothing': 0.10279913467766306, 'class_weight_power': 0.5438838997916643, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(7)}, value=0.9543
2025-09-22 15:40:32,591 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 33: {'lr': 0.0007252977917112599, 'batch_size': np.int64(32), 'epochs': np.int64(35), 'weight_decay': 2.373377497297425e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.8859015006057274, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(72), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.044651251527535414, 'head_hidden': np.int64(94), 'use_focal_loss': np.True_, 'focal_gamma': 1.7930080243819966, 'label_smoothing': 0.10279913467766306, 'class_weight_power': 0.5438838997916643, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(7)} -> 0.9543
2025-09-22 15:40:32,591 - INFO - bo.run_bo - üîçBO Trial 34: Using RF surrogate + Expected Improvement
2025-09-22 15:40:32,591 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:40:32,592 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:40:32,592 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:40:32,592 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.0599232195332016e-05, 'batch_size': 32, 'epochs': 45, 'weight_decay': 3.3276882492955755e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.3123201995307487, 'amp': True, 'patch_size': 125, 'd_model': 90, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.02303622031326186, 'head_hidden': 117, 'use_focal_loss': True, 'focal_gamma': 1.475069913168662, 'label_smoothing': 0.02847485855103731, 'class_weight_power': 0.5441451514173318, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}
2025-09-22 15:40:32,593 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.0599232195332016e-05, 'batch_size': 32, 'epochs': 45, 'weight_decay': 3.3276882492955755e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.3123201995307487, 'amp': True, 'patch_size': 125, 'd_model': 90, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.02303622031326186, 'head_hidden': 117, 'use_focal_loss': True, 'focal_gamma': 1.475069913168662, 'label_smoothing': 0.02847485855103731, 'class_weight_power': 0.5441451514173318, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}
2025-09-22 15:42:28,126 - INFO - _models.training_function_executor - Model parameter count: 231,701
2025-09-22 15:42:28,127 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5961083684862174, 0.38987911813103704, 0.31349060581026417, 0.2587902102927137, 0.2243452697121305, 0.1984898677022894, 0.18465387609160655, 0.16487506655660805, 0.1541071581892515, 0.14064217533241616, 0.12995876982367935, 0.12141170303521627, 0.1152412085325815, 0.11220749905501841, 0.10436082617952849, 0.0981419997961538, 0.09391199320851608, 0.0905544276886953, 0.08514209305643204, 0.08422872860685864, 0.0781670039858028, 0.0761924558824008, 0.07193838762961316, 0.06619992511084895, 0.06767610372542585, 0.0624003865146153, 0.06080454207124864, 0.05984858634107951, 0.05552222279831942, 0.0552818693359017, 0.05327066535924435, 0.050587457436693045, 0.04931387207258302, 0.04744909561998992, 0.04332646934127944, 0.04272681268537593, 0.041228286141645615, 0.04276796877062104, 0.04232860510904288, 0.03979894504615307, 0.03871711145680235, 0.0350068938322007, 0.036462224833568084, 0.03594281656898419, 0.03468975147741617], 'val_losses': [0.3361868490448065, 0.25637452708585956, 0.21431882815385817, 0.18785952139529588, 0.17199387349250875, 0.1663954446241381, 0.1446102997732706, 0.1460273430571044, 0.13462848287337617, 0.12906880428051384, 0.13422048771038583, 0.12071572269092838, 0.12204007456183448, 0.11054882533309633, 0.1092027867629017, 0.10958358060483152, 0.1132668864195276, 0.11328507366484829, 0.1135232337094566, 0.11008818977653734, 0.10932995464296631, 0.10400569482997402, 0.1047595842558106, 0.11303424022091972, 0.10852717457073342, 0.1070092086851888, 0.1121286907178569, 0.10625231940037415, 0.09837776958725832, 0.10796484677523632, 0.11029240753944618, 0.10862137013555036, 0.10338777803512851, 0.10984278591918013, 0.11054790224620922, 0.10624890040819573, 0.10893457682116889, 0.11857691556686724, 0.1109445556002166, 0.10709723697952173, 0.11093690064770136, 0.1144119989079777, 0.11617174301688092, 0.12004871878998424, 0.11910805944178303], 'val_acc': [0.24094724971808043, 0.4629745645909034, 0.5281293071043729, 0.5836361358225787, 0.6145846385164766, 0.6342563588522742, 0.6882596165893998, 0.6848765818819696, 0.7109384788873575, 0.7268512717704548, 0.7314872822954517, 0.7591780478636763, 0.7609322140082696, 0.8092970805663451, 0.8136824959278286, 0.8214509459967423, 0.7975191078812179, 0.8148101741636387, 0.8173161257987721, 0.8356095727352462, 0.8411226663325397, 0.8622979576494174, 0.8516476632001002, 0.8526500438541537, 0.8483899260744268, 0.8714446811176544, 0.8654303971933341, 0.8721964666081945, 0.8723217641899511, 0.8864803909284551, 0.8686881343190076, 0.8921187821075053, 0.892244079689262, 0.9012655055757424, 0.8912416990352087, 0.9031449693020924, 0.8928705675980454, 0.9061521112642525, 0.9136699661696529, 0.9125422879338428, 0.9032702668838491, 0.9117905024433028, 0.9059015161007392, 0.8957524119784488, 0.9164265129682997], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.0599232195332016e-05, 'batch_size': 32, 'epochs': 45, 'weight_decay': 3.3276882492955755e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 4, 'grad_clip': 0.3123201995307487, 'amp': True, 'patch_size': 125, 'd_model': 90, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.02303622031326186, 'head_hidden': 117, 'use_focal_loss': True, 'focal_gamma': 1.475069913168662, 'label_smoothing': 0.02847485855103731, 'class_weight_power': 0.5441451514173318, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}, 'model_parameter_count': 231701, 'model_size_validation': 'PASS'}
2025-09-22 15:42:28,127 - INFO - _models.training_function_executor - BO Objective: base=0.9164, size_penalty=0.0000, final=0.9164
2025-09-22 15:42:28,127 - INFO - _models.training_function_executor - Model size: 231,701 parameters (PASS 256K limit)
2025-09-22 15:42:28,127 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 115.535s
2025-09-22 15:42:28,247 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9164
2025-09-22 15:42:28,247 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.120s
2025-09-22 15:42:28,247 - INFO - bo.run_bo - Recorded observation #34: hparams={'lr': 3.0599232195332016e-05, 'batch_size': np.int64(32), 'epochs': np.int64(45), 'weight_decay': 3.3276882492955755e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.3123201995307487, 'amp': np.True_, 'patch_size': np.int64(125), 'd_model': np.int64(90), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.02303622031326186, 'head_hidden': np.int64(117), 'use_focal_loss': np.True_, 'focal_gamma': 1.475069913168662, 'label_smoothing': 0.02847485855103731, 'class_weight_power': 0.5441451514173318, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(6)}, value=0.9164
2025-09-22 15:42:28,247 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 34: {'lr': 3.0599232195332016e-05, 'batch_size': np.int64(32), 'epochs': np.int64(45), 'weight_decay': 3.3276882492955755e-06, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.3123201995307487, 'amp': np.True_, 'patch_size': np.int64(125), 'd_model': np.int64(90), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.02303622031326186, 'head_hidden': np.int64(117), 'use_focal_loss': np.True_, 'focal_gamma': 1.475069913168662, 'label_smoothing': 0.02847485855103731, 'class_weight_power': 0.5441451514173318, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(6)} -> 0.9164
2025-09-22 15:42:28,248 - INFO - bo.run_bo - üîçBO Trial 35: Using RF surrogate + Expected Improvement
2025-09-22 15:42:28,248 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 15:42:28,248 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:42:28,248 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:42:28,248 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 6.0470280413446195e-05, 'batch_size': 32, 'epochs': 13, 'weight_decay': 2.253336955156096e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.3395764441121995, 'amp': False, 'patch_size': 100, 'd_model': 76, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.10791303213017958, 'head_hidden': 60, 'use_focal_loss': True, 'focal_gamma': 2.1466639233606415, 'label_smoothing': 0.031048079072454045, 'class_weight_power': 0.5443150270330179, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 10}
2025-09-22 15:42:28,249 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 6.0470280413446195e-05, 'batch_size': 32, 'epochs': 13, 'weight_decay': 2.253336955156096e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.3395764441121995, 'amp': False, 'patch_size': 100, 'd_model': 76, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.10791303213017958, 'head_hidden': 60, 'use_focal_loss': True, 'focal_gamma': 2.1466639233606415, 'label_smoothing': 0.031048079072454045, 'class_weight_power': 0.5443150270330179, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 10}
2025-09-22 15:42:53,598 - INFO - _models.training_function_executor - Model parameter count: 115,321
2025-09-22 15:42:53,599 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.48225785239061064, 0.30112052626360053, 0.24800032738954342, 0.2163399482015264, 0.19850296473770487, 0.1859340821625789, 0.17364777884559368, 0.16096939788020628, 0.1573319671601986, 0.14544199270052752, 0.1411961436099424, 0.1368248090063951, 0.13397965711018003], 'val_losses': [0.24230457655081936, 0.18708329137880064, 0.15689574610995047, 0.14671251846575645, 0.14593333894634974, 0.13858715048142206, 0.12397355711384683, 0.12145796646953913, 0.12205967922666858, 0.11355494719865462, 0.11287181518524754, 0.11361035283586313, 0.11026129878421667], 'val_acc': [0.24871569978699412, 0.40972309234431775, 0.4905400325773713, 0.5840120285678486, 0.569728104247588, 0.5688510211752913, 0.599548928705676, 0.6332539781982208, 0.6406465355218645, 0.6586893872948252, 0.6902643778975066, 0.6441548678110512, 0.685503069790753], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 6.0470280413446195e-05, 'batch_size': 32, 'epochs': 13, 'weight_decay': 2.253336955156096e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.3395764441121995, 'amp': False, 'patch_size': 100, 'd_model': 76, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.10791303213017958, 'head_hidden': 60, 'use_focal_loss': True, 'focal_gamma': 2.1466639233606415, 'label_smoothing': 0.031048079072454045, 'class_weight_power': 0.5443150270330179, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 10}, 'model_parameter_count': 115321, 'model_size_validation': 'PASS'}
2025-09-22 15:42:53,599 - INFO - _models.training_function_executor - BO Objective: base=0.6855, size_penalty=0.0000, final=0.6855
2025-09-22 15:42:53,599 - INFO - _models.training_function_executor - Model size: 115,321 parameters (PASS 256K limit)
2025-09-22 15:42:53,599 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 25.351s
2025-09-22 15:42:53,719 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6855
2025-09-22 15:42:53,719 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.121s
2025-09-22 15:42:53,719 - INFO - bo.run_bo - Recorded observation #35: hparams={'lr': 6.0470280413446195e-05, 'batch_size': np.int64(32), 'epochs': np.int64(13), 'weight_decay': 2.253336955156096e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.3395764441121995, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(76), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.10791303213017958, 'head_hidden': np.int64(60), 'use_focal_loss': np.True_, 'focal_gamma': 2.1466639233606415, 'label_smoothing': 0.031048079072454045, 'class_weight_power': 0.5443150270330179, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(10)}, value=0.6855
2025-09-22 15:42:53,719 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 35: {'lr': 6.0470280413446195e-05, 'batch_size': np.int64(32), 'epochs': np.int64(13), 'weight_decay': 2.253336955156096e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.3395764441121995, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(76), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.10791303213017958, 'head_hidden': np.int64(60), 'use_focal_loss': np.True_, 'focal_gamma': 2.1466639233606415, 'label_smoothing': 0.031048079072454045, 'class_weight_power': 0.5443150270330179, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(10)} -> 0.6855
2025-09-22 15:42:53,720 - INFO - bo.run_bo - üîçBO Trial 36: Using RF surrogate + Expected Improvement
2025-09-22 15:42:53,720 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:42:53,720 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:42:53,720 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:42:53,720 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0013523115496861967, 'batch_size': 256, 'epochs': 42, 'weight_decay': 0.00017990239965801636, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.3356725665059867, 'amp': True, 'patch_size': 200, 'd_model': 71, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.08632195891548668, 'head_hidden': 34, 'use_focal_loss': True, 'focal_gamma': 2.6661912848634923, 'label_smoothing': 0.10616793476656254, 'class_weight_power': 0.5445701827166868, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:42:53,722 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0013523115496861967, 'batch_size': 256, 'epochs': 42, 'weight_decay': 0.00017990239965801636, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.3356725665059867, 'amp': True, 'patch_size': 200, 'd_model': 71, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.08632195891548668, 'head_hidden': 34, 'use_focal_loss': True, 'focal_gamma': 2.6661912848634923, 'label_smoothing': 0.10616793476656254, 'class_weight_power': 0.5445701827166868, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:43:11,323 - INFO - _models.training_function_executor - Model parameter count: 113,877
2025-09-22 15:43:11,323 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4799151214311861, 0.31542066799979296, 0.23983811078510223, 0.2006706892281768, 0.169901380320616, 0.1514791091489481, 0.13328307766061348, 0.11892395488458278, 0.10812800091398438, 0.1020298752018493, 0.09499404339497129, 0.08820938461103696, 0.08186937304602766, 0.07596146241444243, 0.07121595786319909, 0.068903286191256, 0.06747857542658395, 0.06203396883009707, 0.05788880640409388, 0.055725731324360483, 0.05246358513362974, 0.0493980248028104, 0.04688118389787424, 0.04585813990487299, 0.044073657003706794, 0.04246823566790452, 0.043311500995889124, 0.03956152698179088, 0.037309605146880524, 0.03482245015236525, 0.03437541388935625, 0.03404400374764752, 0.03157470318731009, 0.031118561525887514, 0.029568069275730337, 0.028881299204960305, 0.029067399995924594, 0.02692367625761924, 0.02815151780715836, 0.027289970035245536, 0.026093962803180534, 0.026298095829684745], 'val_losses': [0.2700938269609856, 0.20067148242748375, 0.1693875905643981, 0.15426728038366777, 0.1254839164569793, 0.12164017788308737, 0.1089300554501967, 0.09584124282687667, 0.09277583083634747, 0.09926033506767147, 0.09485256916399573, 0.09719878206697716, 0.08292977015799768, 0.07954528938912163, 0.07888621602955655, 0.07814435085218482, 0.07106884305111344, 0.07206897547356751, 0.07168829839692022, 0.07643796530523152, 0.07368614884202394, 0.06612227571636435, 0.07479364797239568, 0.06854127845979546, 0.06758186159322412, 0.07092329384095088, 0.06773463880570665, 0.0673219723247254, 0.06595614443273715, 0.06313666838827237, 0.0644592872089483, 0.06560563687979615, 0.06500395951175761, 0.06576967839085807, 0.06537726911618258, 0.06647799585083197, 0.06413767373478961, 0.06514725472477949, 0.06585972158028479, 0.06624133617167574, 0.06563338026630297, 0.06479355900949471], 'val_acc': [0.29119158000250595, 0.2859290815687257, 0.44154867811051246, 0.4491918305976695, 0.5714822703921815, 0.5649667961408344, 0.5896504197468989, 0.6626989099110387, 0.6920185440421, 0.6895125924069665, 0.6891366996616966, 0.6569352211502318, 0.7417616839994988, 0.7220899636637013, 0.7253477007893747, 0.7556697155744894, 0.7867435158501441, 0.7808545295075805, 0.7869941110136575, 0.7537902518481393, 0.8156872572359354, 0.8357348703170029, 0.7694524495677233, 0.8126801152737753, 0.8275905275028193, 0.8035333918055382, 0.8361107630622729, 0.8461345696028066, 0.8436286179676732, 0.8274652299210625, 0.8396190953514597, 0.8565342688886104, 0.8488911164014534, 0.8505199849642902, 0.854404209998747, 0.855155995489287, 0.8549054003257737, 0.8684375391554943, 0.8641774213757675, 0.8680616464102243, 0.8653050996115775, 0.861671469740634], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0013523115496861967, 'batch_size': 256, 'epochs': 42, 'weight_decay': 0.00017990239965801636, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.3356725665059867, 'amp': True, 'patch_size': 200, 'd_model': 71, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.08632195891548668, 'head_hidden': 34, 'use_focal_loss': True, 'focal_gamma': 2.6661912848634923, 'label_smoothing': 0.10616793476656254, 'class_weight_power': 0.5445701827166868, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 6}, 'model_parameter_count': 113877, 'model_size_validation': 'PASS'}
2025-09-22 15:43:11,323 - INFO - _models.training_function_executor - BO Objective: base=0.8617, size_penalty=0.0000, final=0.8617
2025-09-22 15:43:11,323 - INFO - _models.training_function_executor - Model size: 113,877 parameters (PASS 256K limit)
2025-09-22 15:43:11,323 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 17.603s
2025-09-22 15:43:11,444 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8617
2025-09-22 15:43:11,444 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.121s
2025-09-22 15:43:11,444 - INFO - bo.run_bo - Recorded observation #36: hparams={'lr': 0.0013523115496861967, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'weight_decay': 0.00017990239965801636, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.3356725665059867, 'amp': np.True_, 'patch_size': np.int64(200), 'd_model': np.int64(71), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.08632195891548668, 'head_hidden': np.int64(34), 'use_focal_loss': np.True_, 'focal_gamma': 2.6661912848634923, 'label_smoothing': 0.10616793476656254, 'class_weight_power': 0.5445701827166868, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)}, value=0.8617
2025-09-22 15:43:11,444 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 36: {'lr': 0.0013523115496861967, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'weight_decay': 0.00017990239965801636, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.3356725665059867, 'amp': np.True_, 'patch_size': np.int64(200), 'd_model': np.int64(71), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.08632195891548668, 'head_hidden': np.int64(34), 'use_focal_loss': np.True_, 'focal_gamma': 2.6661912848634923, 'label_smoothing': 0.10616793476656254, 'class_weight_power': 0.5445701827166868, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)} -> 0.8617
2025-09-22 15:43:11,445 - INFO - bo.run_bo - üîçBO Trial 37: Using RF surrogate + Expected Improvement
2025-09-22 15:43:11,445 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:43:11,445 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:43:11,445 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:43:11,445 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015771132617666264, 'batch_size': 256, 'epochs': 9, 'weight_decay': 3.708492549784452e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 0, 'grad_clip': 0.9164621946209413, 'amp': True, 'patch_size': 25, 'd_model': 39, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.23551739690199974, 'head_hidden': 91, 'use_focal_loss': True, 'focal_gamma': 2.576560482635622, 'label_smoothing': 0.06802223454473837, 'class_weight_power': 0.5446490651079184, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 5}
2025-09-22 15:43:11,447 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015771132617666264, 'batch_size': 256, 'epochs': 9, 'weight_decay': 3.708492549784452e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 0, 'grad_clip': 0.9164621946209413, 'amp': True, 'patch_size': 25, 'd_model': 39, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.23551739690199974, 'head_hidden': 91, 'use_focal_loss': True, 'focal_gamma': 2.576560482635622, 'label_smoothing': 0.06802223454473837, 'class_weight_power': 0.5446490651079184, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 5}
2025-09-22 15:43:19,109 - INFO - _models.training_function_executor - Model parameter count: 64,186
2025-09-22 15:43:19,109 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3585588428218185, 0.2097509015892187, 0.1667631225010971, 0.14453035424266905, 0.1296760211297714, 0.10992009485845812, 0.09899275615277814, 0.09657398484925245, 0.09050685581047052], 'val_losses': [0.17545429468827056, 0.142097966441862, 0.09994412047440523, 0.10721815833554667, 0.09415501668720656, 0.08698252604707428, 0.07516395110558065, 0.07363078081012966, 0.07230383469392342], 'val_acc': [0.4569602806665831, 0.6440295702292945, 0.7244706177170781, 0.6991605062022302, 0.737125673474502, 0.6824959278285929, 0.7445182307981456, 0.7292319258238316, 0.7348703170028819], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0015771132617666264, 'batch_size': 256, 'epochs': 9, 'weight_decay': 3.708492549784452e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 0, 'grad_clip': 0.9164621946209413, 'amp': True, 'patch_size': 25, 'd_model': 39, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.23551739690199974, 'head_hidden': 91, 'use_focal_loss': True, 'focal_gamma': 2.576560482635622, 'label_smoothing': 0.06802223454473837, 'class_weight_power': 0.5446490651079184, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 5}, 'model_parameter_count': 64186, 'model_size_validation': 'PASS'}
2025-09-22 15:43:19,109 - INFO - _models.training_function_executor - BO Objective: base=0.7349, size_penalty=0.0000, final=0.7349
2025-09-22 15:43:19,109 - INFO - _models.training_function_executor - Model size: 64,186 parameters (PASS 256K limit)
2025-09-22 15:43:19,109 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 7.664s
2025-09-22 15:43:19,231 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7349
2025-09-22 15:43:19,231 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.122s
2025-09-22 15:43:19,231 - INFO - bo.run_bo - Recorded observation #37: hparams={'lr': 0.0015771132617666264, 'batch_size': np.int64(256), 'epochs': np.int64(9), 'weight_decay': 3.708492549784452e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(0), 'grad_clip': 0.9164621946209413, 'amp': np.True_, 'patch_size': np.int64(25), 'd_model': np.int64(39), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.23551739690199974, 'head_hidden': np.int64(91), 'use_focal_loss': np.True_, 'focal_gamma': 2.576560482635622, 'label_smoothing': 0.06802223454473837, 'class_weight_power': 0.5446490651079184, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(5)}, value=0.7349
2025-09-22 15:43:19,231 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 37: {'lr': 0.0015771132617666264, 'batch_size': np.int64(256), 'epochs': np.int64(9), 'weight_decay': 3.708492549784452e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(0), 'grad_clip': 0.9164621946209413, 'amp': np.True_, 'patch_size': np.int64(25), 'd_model': np.int64(39), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.23551739690199974, 'head_hidden': np.int64(91), 'use_focal_loss': np.True_, 'focal_gamma': 2.576560482635622, 'label_smoothing': 0.06802223454473837, 'class_weight_power': 0.5446490651079184, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(5)} -> 0.7349
2025-09-22 15:43:19,232 - INFO - bo.run_bo - üîçBO Trial 38: Using RF surrogate + Expected Improvement
2025-09-22 15:43:19,232 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:43:19,232 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:43:19,232 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:43:19,232 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007950345551020804, 'batch_size': 32, 'epochs': 15, 'weight_decay': 0.000441351495926963, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 0, 'grad_clip': 0.39560395244931534, 'amp': False, 'patch_size': 50, 'd_model': 49, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.3711315359841729, 'head_hidden': 50, 'use_focal_loss': True, 'focal_gamma': 1.5492542544561512, 'label_smoothing': 0.11615939128264123, 'class_weight_power': 0.5446889478470903, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 4}
2025-09-22 15:43:19,234 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007950345551020804, 'batch_size': 32, 'epochs': 15, 'weight_decay': 0.000441351495926963, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 0, 'grad_clip': 0.39560395244931534, 'amp': False, 'patch_size': 50, 'd_model': 49, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.3711315359841729, 'head_hidden': 50, 'use_focal_loss': True, 'focal_gamma': 1.5492542544561512, 'label_smoothing': 0.11615939128264123, 'class_weight_power': 0.5446889478470903, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 4}
2025-09-22 15:43:53,190 - INFO - _models.training_function_executor - Model parameter count: 67,780
2025-09-22 15:43:53,190 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.48517183610282627, 0.3856960900298964, 0.35385414987967745, 0.32506670005820476, 0.31497408114638586, 0.3128965435569397, 0.3033409537656771, 0.2898103308244311, 0.28973012919184804, 0.28282856540722057, 0.28614596737328096, 0.27860289194578813, 0.27580952722207797, 0.27254064556026736, 0.27072335968761296], 'val_losses': [0.30646540799986044, 0.28076805650456, 0.26038083099897696, 0.24952410776511183, 0.2568340699593684, 0.24007686086128774, 0.2312624591037812, 0.252850312294779, 0.23278593468854278, 0.22296776437132806, 0.22107738200442084, 0.23293400684584203, 0.23396997395172378, 0.22314137750353197, 0.22763654079753615], 'val_acc': [0.292695150983586, 0.2973311615085829, 0.44493171281794264, 0.48114271394562086, 0.4484400451071294, 0.5639644154867811, 0.5841373261496053, 0.5333918055381531, 0.5575742388171908, 0.5805036962786618, 0.5886480390928455, 0.5590778097982709, 0.5782483398070417, 0.5812554817692018, 0.5706051873198847], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007950345551020804, 'batch_size': 32, 'epochs': 15, 'weight_decay': 0.000441351495926963, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 0, 'grad_clip': 0.39560395244931534, 'amp': False, 'patch_size': 50, 'd_model': 49, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.3711315359841729, 'head_hidden': 50, 'use_focal_loss': True, 'focal_gamma': 1.5492542544561512, 'label_smoothing': 0.11615939128264123, 'class_weight_power': 0.5446889478470903, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 4}, 'model_parameter_count': 67780, 'model_size_validation': 'PASS'}
2025-09-22 15:43:53,190 - INFO - _models.training_function_executor - BO Objective: base=0.5706, size_penalty=0.0000, final=0.5706
2025-09-22 15:43:53,190 - INFO - _models.training_function_executor - Model size: 67,780 parameters (PASS 256K limit)
2025-09-22 15:43:53,190 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 33.958s
2025-09-22 15:43:53,429 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5706
2025-09-22 15:43:53,429 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.238s
2025-09-22 15:43:53,429 - INFO - bo.run_bo - Recorded observation #38: hparams={'lr': 0.0007950345551020804, 'batch_size': np.int64(32), 'epochs': np.int64(15), 'weight_decay': 0.000441351495926963, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(0), 'grad_clip': 0.39560395244931534, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(49), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.3711315359841729, 'head_hidden': np.int64(50), 'use_focal_loss': np.True_, 'focal_gamma': 1.5492542544561512, 'label_smoothing': 0.11615939128264123, 'class_weight_power': 0.5446889478470903, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(4)}, value=0.5706
2025-09-22 15:43:53,429 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 38: {'lr': 0.0007950345551020804, 'batch_size': np.int64(32), 'epochs': np.int64(15), 'weight_decay': 0.000441351495926963, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(0), 'grad_clip': 0.39560395244931534, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(49), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.3711315359841729, 'head_hidden': np.int64(50), 'use_focal_loss': np.True_, 'focal_gamma': 1.5492542544561512, 'label_smoothing': 0.11615939128264123, 'class_weight_power': 0.5446889478470903, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(4)} -> 0.5706
2025-09-22 15:43:53,430 - INFO - bo.run_bo - üîçBO Trial 39: Using RF surrogate + Expected Improvement
2025-09-22 15:43:53,430 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:43:53,430 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:43:53,430 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:43:53,430 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0018034669719837876, 'batch_size': 64, 'epochs': 22, 'weight_decay': 2.2880528506080553e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 5, 'grad_clip': 0.23839017635611987, 'amp': False, 'patch_size': 50, 'd_model': 70, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.20612579497965478, 'head_hidden': 114, 'use_focal_loss': True, 'focal_gamma': 1.3525421690977348, 'label_smoothing': 0.10959659616364945, 'class_weight_power': 0.5446259985566662, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 4}
2025-09-22 15:43:53,431 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0018034669719837876, 'batch_size': 64, 'epochs': 22, 'weight_decay': 2.2880528506080553e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 5, 'grad_clip': 0.23839017635611987, 'amp': False, 'patch_size': 50, 'd_model': 70, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.20612579497965478, 'head_hidden': 114, 'use_focal_loss': True, 'focal_gamma': 1.3525421690977348, 'label_smoothing': 0.10959659616364945, 'class_weight_power': 0.5446259985566662, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 4}
2025-09-22 15:44:27,072 - INFO - _models.training_function_executor - Model parameter count: 187,740
2025-09-22 15:44:27,072 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.45439430335162295, 0.31515869232092353, 0.2677007941920962, 0.23117931243120324, 0.20649623717417548, 0.17162851222015593, 0.1449752803403524, 0.12922275649389559, 0.11326722286078422, 0.1028945815656372, 0.09225136712480561, 0.08246450260371652, 0.07580713416640145, 0.06726186099208017, 0.0621761356565637, 0.057074793768165086, 0.05166253681706762, 0.048245451388909726, 0.04102783215781861, 0.041378958204483, 0.040378327977844776, 0.03838067373382628], 'val_losses': [0.25429818151798067, 0.21885727478879508, 0.1703480793231562, 0.16366212278914444, 0.18994286760562734, 0.12452632961301124, 0.10986523459040122, 0.13563402147389284, 0.10241312398020373, 0.09415208954957326, 0.08401757444453081, 0.08837797511693395, 0.07300256326367063, 0.07618950493236992, 0.06961701479046704, 0.07258064685093701, 0.060157464937359566, 0.058876519393777506, 0.0673600652715703, 0.06341195276970703, 0.061661220108629275, 0.06353467889293485], 'val_acc': [0.3845382784112267, 0.47174539531387044, 0.6346322515975442, 0.69239443678737, 0.6231048740759303, 0.7674476882596166, 0.7784738754542038, 0.7436411477258489, 0.8044104748778349, 0.816940233053502, 0.839493797769703, 0.8468863550933466, 0.8502693898007768, 0.860543791504824, 0.876205989224408, 0.8897381280541286, 0.8973812805412855, 0.8997619345946624, 0.9001378273399323, 0.9072797895000626, 0.9084074677358727, 0.9015161007392557], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0018034669719837876, 'batch_size': 64, 'epochs': 22, 'weight_decay': 2.2880528506080553e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 5, 'grad_clip': 0.23839017635611987, 'amp': False, 'patch_size': 50, 'd_model': 70, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.20612579497965478, 'head_hidden': 114, 'use_focal_loss': True, 'focal_gamma': 1.3525421690977348, 'label_smoothing': 0.10959659616364945, 'class_weight_power': 0.5446259985566662, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 4}, 'model_parameter_count': 187740, 'model_size_validation': 'PASS'}
2025-09-22 15:44:27,072 - INFO - _models.training_function_executor - BO Objective: base=0.9015, size_penalty=0.0000, final=0.9015
2025-09-22 15:44:27,072 - INFO - _models.training_function_executor - Model size: 187,740 parameters (PASS 256K limit)
2025-09-22 15:44:27,073 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 33.643s
2025-09-22 15:44:27,196 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9015
2025-09-22 15:44:27,196 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.123s
2025-09-22 15:44:27,196 - INFO - bo.run_bo - Recorded observation #39: hparams={'lr': 0.0018034669719837876, 'batch_size': np.int64(64), 'epochs': np.int64(22), 'weight_decay': 2.2880528506080553e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.23839017635611987, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(70), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.20612579497965478, 'head_hidden': np.int64(114), 'use_focal_loss': np.True_, 'focal_gamma': 1.3525421690977348, 'label_smoothing': 0.10959659616364945, 'class_weight_power': 0.5446259985566662, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(4)}, value=0.9015
2025-09-22 15:44:27,196 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 39: {'lr': 0.0018034669719837876, 'batch_size': np.int64(64), 'epochs': np.int64(22), 'weight_decay': 2.2880528506080553e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.23839017635611987, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(70), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.20612579497965478, 'head_hidden': np.int64(114), 'use_focal_loss': np.True_, 'focal_gamma': 1.3525421690977348, 'label_smoothing': 0.10959659616364945, 'class_weight_power': 0.5446259985566662, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calib_batches': np.int64(4)} -> 0.9015
2025-09-22 15:44:27,196 - INFO - bo.run_bo - üîçBO Trial 40: Using RF surrogate + Expected Improvement
2025-09-22 15:44:27,196 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:44:27,196 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:44:27,196 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:44:27,197 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007371039199010225, 'batch_size': 32, 'epochs': 20, 'weight_decay': 2.7548677798343797e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 3, 'grad_clip': 0.03601587146783037, 'amp': False, 'patch_size': 40, 'd_model': 43, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.1651489479250827, 'head_hidden': 104, 'use_focal_loss': False, 'focal_gamma': 2.804062287120069, 'label_smoothing': 0.15103453716195717, 'class_weight_power': 0.5443931331959364, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 8}
2025-09-22 15:44:27,198 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007371039199010225, 'batch_size': 32, 'epochs': 20, 'weight_decay': 2.7548677798343797e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 3, 'grad_clip': 0.03601587146783037, 'amp': False, 'patch_size': 40, 'd_model': 43, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.1651489479250827, 'head_hidden': 104, 'use_focal_loss': False, 'focal_gamma': 2.804062287120069, 'label_smoothing': 0.15103453716195717, 'class_weight_power': 0.5443931331959364, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 8}
2025-09-22 15:45:12,471 - INFO - _models.training_function_executor - Model parameter count: 16,641
2025-09-22 15:45:12,471 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.418041581603118, 1.22346264628604, 1.1323037870328416, 1.0869585268069444, 1.05341842548107, 1.0302983083226973, 1.0101816234603411, 0.9770546806742515, 0.9657681871729418, 0.9554618206798808, 0.9518294791412306, 0.9306113412403894, 0.9195892604353418, 0.9177679686014116, 0.9108895200485537, 0.90459528514714, 0.8981512870342104, 0.9011341903015728, 0.8960092915996619, 0.8941293095853758], 'val_losses': [1.8570295820826501, 1.785437073867104, 1.6751494721920623, 1.7148930446917274, 1.7086709582586066, 1.6413337164483204, 1.58240108217546, 1.609480082846841, 1.5540979865678435, 1.5534280699426526, 1.5537687568583296, 1.5153518477383598, 1.5432221892184443, 1.5573748539838705, 1.493098613836759, 1.4811272310472763, 1.5233464860748847, 1.52901751259308, 1.5265532595489038, 1.5231831292881433], 'val_acc': [0.3425635885227415, 0.49906026813682497, 0.6479137952637514, 0.553690013782734, 0.5900263124921689, 0.6913920561333167, 0.7063024683623607, 0.6804911665204861, 0.7289813306603182, 0.7060518731988472, 0.7245959152988347, 0.7480265630873324, 0.7167021676481644, 0.7268512717704548, 0.7540408470116526, 0.7710813181305601, 0.7431399573988222, 0.7422628743265255, 0.7423881719082822, 0.7481518606690891], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007371039199010225, 'batch_size': 32, 'epochs': 20, 'weight_decay': 2.7548677798343797e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 3, 'grad_clip': 0.03601587146783037, 'amp': False, 'patch_size': 40, 'd_model': 43, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.1651489479250827, 'head_hidden': 104, 'use_focal_loss': False, 'focal_gamma': 2.804062287120069, 'label_smoothing': 0.15103453716195717, 'class_weight_power': 0.5443931331959364, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 8}, 'model_parameter_count': 16641, 'model_size_validation': 'PASS'}
2025-09-22 15:45:12,472 - INFO - _models.training_function_executor - BO Objective: base=0.7482, size_penalty=0.0000, final=0.7482
2025-09-22 15:45:12,472 - INFO - _models.training_function_executor - Model size: 16,641 parameters (PASS 256K limit)
2025-09-22 15:45:12,472 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 45.275s
2025-09-22 15:45:12,595 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7482
2025-09-22 15:45:12,596 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.124s
2025-09-22 15:45:12,596 - INFO - bo.run_bo - Recorded observation #40: hparams={'lr': 0.0007371039199010225, 'batch_size': np.int64(32), 'epochs': np.int64(20), 'weight_decay': 2.7548677798343797e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.03601587146783037, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(43), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.1651489479250827, 'head_hidden': np.int64(104), 'use_focal_loss': np.False_, 'focal_gamma': 2.804062287120069, 'label_smoothing': 0.15103453716195717, 'class_weight_power': 0.5443931331959364, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(8)}, value=0.7482
2025-09-22 15:45:12,596 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 40: {'lr': 0.0007371039199010225, 'batch_size': np.int64(32), 'epochs': np.int64(20), 'weight_decay': 2.7548677798343797e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(3), 'grad_clip': 0.03601587146783037, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(43), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.1651489479250827, 'head_hidden': np.int64(104), 'use_focal_loss': np.False_, 'focal_gamma': 2.804062287120069, 'label_smoothing': 0.15103453716195717, 'class_weight_power': 0.5443931331959364, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(8)} -> 0.7482
2025-09-22 15:45:12,596 - INFO - bo.run_bo - üîçBO Trial 41: Using RF surrogate + Expected Improvement
2025-09-22 15:45:12,596 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:45:12,596 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:45:12,596 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:45:12,596 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 8.304159060700898e-05, 'batch_size': 64, 'epochs': 59, 'weight_decay': 6.5896525528908555e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.6493926846325083, 'amp': True, 'patch_size': 25, 'd_model': 79, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.3703267218081124, 'head_hidden': 95, 'use_focal_loss': False, 'focal_gamma': 1.7762428982279748, 'label_smoothing': 0.15289198071719073, 'class_weight_power': 0.5446613284593225, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:45:12,598 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 8.304159060700898e-05, 'batch_size': 64, 'epochs': 59, 'weight_decay': 6.5896525528908555e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.6493926846325083, 'amp': True, 'patch_size': 25, 'd_model': 79, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.3703267218081124, 'head_hidden': 95, 'use_focal_loss': False, 'focal_gamma': 1.7762428982279748, 'label_smoothing': 0.15289198071719073, 'class_weight_power': 0.5446613284593225, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:47:01,035 - INFO - _models.training_function_executor - Model parameter count: 243,374
2025-09-22 15:47:01,036 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5133103297265578, 1.311690595407435, 1.220194952987517, 1.1688013372014079, 1.1167087089491492, 1.1010252368579745, 1.074600896099152, 1.0628647427434472, 1.0522149502549543, 1.0363262865704164, 1.0408640204935904, 1.024969852025193, 1.0123254477191688, 1.005314619246235, 0.9966895066306013, 0.9988125428159229, 0.9958423272867988, 0.9865895110675126, 0.9865516440348492, 0.9720469608838048, 0.9688536560771064, 0.9668089872564479, 0.9704597132757814, 0.9688506963777836, 0.9661260229810132, 0.9671962130761694, 0.9539801105313092, 0.9563234308509604, 0.9494992407798558, 0.9578096102238732, 0.9596283905220013, 0.9477771549704633, 0.9515772479050172, 0.9418545726830135, 0.9404531052337906, 0.9437683015427889, 0.9359343843763605, 0.9281584931447595, 0.9334787802786896, 0.9363562352573745, 0.9375469231222393, 0.9275828837597799, 0.9222661465629541, 0.9275876567114514, 0.925216594211545, 0.9234149516183354, 0.9253810684984087, 0.92587508429853, 0.9152460661824849, 0.919963584774341, 0.9055466683919249, 0.9120870098801658, 0.9159084834991398, 0.9070606831078077, 0.9129212757954303, 0.9087227322092347, 0.9035291763989545, 0.8983239637215709, 0.9074356487246715], 'val_losses': [1.9269263367652296, 1.9275277715744403, 1.923674585257328, 2.0062486780659174, 1.959714913484731, 2.047332443490152, 2.0190463429360053, 2.092205627358, 2.1363839570508985, 2.0391959670252886, 2.150006599973548, 2.166530506844778, 2.240493440490635, 2.226435144657689, 2.066438234773977, 2.1539427204368677, 2.1771148672200673, 2.250336277232465, 2.160627266291523, 2.220541822000668, 2.162490169949108, 2.1328788983046656, 2.1262011056017447, 2.1950639729630663, 2.087855579739241, 2.1830076703211523, 2.122230318760367, 2.2246712169138174, 2.189753526912986, 2.3256695064114026, 2.1888507343954706, 2.2279524055134905, 2.198074584466714, 2.1714662486875644, 2.2858862314378388, 2.1701236684405107, 2.2034978051933036, 2.176034705140898, 2.1122066525644625, 2.1737362364900545, 2.104124738076414, 2.1417912769520764, 2.106255398434724, 2.1493506154953095, 2.0791601022522697, 2.145705678621012, 2.0572266955973735, 2.0590927370537675, 2.0020606657598656, 2.1230669336171992, 2.1245465402300834, 2.1221417077665206, 2.123900268060225, 2.090319679309485, 2.187149449228837, 2.149099804318171, 2.173813742513481, 2.082837419542151, 2.0650077916494123], 'val_acc': [0.27189575241197844, 0.3109885979200601, 0.3857912542287934, 0.4007016664578374, 0.4669840872071169, 0.4682370630246836, 0.5083322891868187, 0.4810174163638642, 0.5014409221902018, 0.5610825711063776, 0.5258739506327528, 0.5073299085327653, 0.5174790126550558, 0.5034456834983084, 0.5767447688259617, 0.5345194837739632, 0.5716075679739381, 0.5356471620097732, 0.5679739381029946, 0.5282546046861295, 0.5708557824833981, 0.5818819696779852, 0.5905275028191956, 0.5587019170530009, 0.5960405964164892, 0.5416614459340935, 0.5880215511840622, 0.5566971557448941, 0.5673474501942113, 0.5310111514847763, 0.5797519107881218, 0.5644656058138078, 0.5892745270016289, 0.5831349454955519, 0.569352211502318, 0.6114521989725599, 0.5929081568725724, 0.6124545796266132, 0.637764691141461, 0.6059391053752663, 0.6115774965543165, 0.6203483272772835, 0.637388798396191, 0.6129557699536399, 0.6202230296955269, 0.6222277910036337, 0.6366370129056509, 0.6425259992482145, 0.6579376018042852, 0.6378899887232177, 0.6391429645407843, 0.6314998120536274, 0.6395188572860544, 0.6470367121914548, 0.6185941611326902, 0.6435283799022679, 0.6124545796266132, 0.6343816564340308, 0.6603182558576619], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 8.304159060700898e-05, 'batch_size': 64, 'epochs': 59, 'weight_decay': 6.5896525528908555e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.6493926846325083, 'amp': True, 'patch_size': 25, 'd_model': 79, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.3703267218081124, 'head_hidden': 95, 'use_focal_loss': False, 'focal_gamma': 1.7762428982279748, 'label_smoothing': 0.15289198071719073, 'class_weight_power': 0.5446613284593225, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 2}, 'model_parameter_count': 243374, 'model_size_validation': 'PASS'}
2025-09-22 15:47:01,036 - INFO - _models.training_function_executor - BO Objective: base=0.6603, size_penalty=0.0000, final=0.6603
2025-09-22 15:47:01,036 - INFO - _models.training_function_executor - Model size: 243,374 parameters (PASS 256K limit)
2025-09-22 15:47:01,036 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 108.440s
2025-09-22 15:47:01,160 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6603
2025-09-22 15:47:01,160 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.124s
2025-09-22 15:47:01,160 - INFO - bo.run_bo - Recorded observation #41: hparams={'lr': 8.304159060700898e-05, 'batch_size': np.int64(64), 'epochs': np.int64(59), 'weight_decay': 6.5896525528908555e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.6493926846325083, 'amp': np.True_, 'patch_size': np.int64(25), 'd_model': np.int64(79), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.3703267218081124, 'head_hidden': np.int64(95), 'use_focal_loss': np.False_, 'focal_gamma': 1.7762428982279748, 'label_smoothing': 0.15289198071719073, 'class_weight_power': 0.5446613284593225, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)}, value=0.6603
2025-09-22 15:47:01,160 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 41: {'lr': 8.304159060700898e-05, 'batch_size': np.int64(64), 'epochs': np.int64(59), 'weight_decay': 6.5896525528908555e-06, 'optimizer': np.str_('adam'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.6493926846325083, 'amp': np.True_, 'patch_size': np.int64(25), 'd_model': np.int64(79), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.3703267218081124, 'head_hidden': np.int64(95), 'use_focal_loss': np.False_, 'focal_gamma': 1.7762428982279748, 'label_smoothing': 0.15289198071719073, 'class_weight_power': 0.5446613284593225, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)} -> 0.6603
2025-09-22 15:47:01,161 - INFO - bo.run_bo - üîçBO Trial 42: Using RF surrogate + Expected Improvement
2025-09-22 15:47:01,161 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:47:01,161 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:47:01,161 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:47:01,161 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0005977288268836814, 'batch_size': 32, 'epochs': 15, 'weight_decay': 0.0025738643047888672, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.5632228638730111, 'amp': False, 'patch_size': 100, 'd_model': 77, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.03982896207879639, 'head_hidden': 79, 'use_focal_loss': False, 'focal_gamma': 1.4199637902953608, 'label_smoothing': 0.15049612767169634, 'class_weight_power': 0.12995363067801072, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:47:01,162 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0005977288268836814, 'batch_size': 32, 'epochs': 15, 'weight_decay': 0.0025738643047888672, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.5632228638730111, 'amp': False, 'patch_size': 100, 'd_model': 77, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.03982896207879639, 'head_hidden': 79, 'use_focal_loss': False, 'focal_gamma': 1.4199637902953608, 'label_smoothing': 0.15049612767169634, 'class_weight_power': 0.12995363067801072, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:47:45,045 - INFO - _models.training_function_executor - Model parameter count: 239,568
2025-09-22 15:47:45,045 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.049275677014356, 0.8487665145647308, 0.8066937688732156, 0.7839283032031621, 0.7697681932712532, 0.7577053516691491, 0.750919955541469, 0.7407688629196666, 0.733668524900878, 0.7305467455540147, 0.7203718577396809, 0.7177985371665929, 0.7168221031854323, 0.7109812727843976, 0.7056782601213738], 'val_losses': [0.8495841669328319, 0.8110324229800959, 0.7822031660077566, 0.7762686591252336, 0.789211364396457, 0.7703762717480618, 0.7617318684737227, 0.757100283210849, 0.7496644903911174, 0.7561079585123535, 0.7625788299455751, 0.7537848242481405, 0.7538442867409539, 0.7518565767974338, 0.746327212812488], 'val_acc': [0.9194336549304598, 0.9274527001628868, 0.9437413857912542, 0.9471244204986844, 0.9348452574865305, 0.9501315624608445, 0.9505074552061146, 0.9546422754040848, 0.9599047738378649, 0.9561458463851648, 0.9511339431148979, 0.9562711439669215, 0.9550181681493547, 0.9610324520736749, 0.9626613206365117], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0005977288268836814, 'batch_size': 32, 'epochs': 15, 'weight_decay': 0.0025738643047888672, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.5632228638730111, 'amp': False, 'patch_size': 100, 'd_model': 77, 'n_heads': 8, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.03982896207879639, 'head_hidden': 79, 'use_focal_loss': False, 'focal_gamma': 1.4199637902953608, 'label_smoothing': 0.15049612767169634, 'class_weight_power': 0.12995363067801072, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 2}, 'model_parameter_count': 239568, 'model_size_validation': 'PASS'}
2025-09-22 15:47:45,045 - INFO - _models.training_function_executor - BO Objective: base=0.9627, size_penalty=0.0000, final=0.9627
2025-09-22 15:47:45,045 - INFO - _models.training_function_executor - Model size: 239,568 parameters (PASS 256K limit)
2025-09-22 15:47:45,045 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 43.885s
2025-09-22 15:47:45,171 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9627
2025-09-22 15:47:45,171 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.126s
2025-09-22 15:47:45,171 - INFO - bo.run_bo - Recorded observation #42: hparams={'lr': 0.0005977288268836814, 'batch_size': np.int64(32), 'epochs': np.int64(15), 'weight_decay': 0.0025738643047888672, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.5632228638730111, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(77), 'n_heads': np.int64(8), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.03982896207879639, 'head_hidden': np.int64(79), 'use_focal_loss': np.False_, 'focal_gamma': 1.4199637902953608, 'label_smoothing': 0.15049612767169634, 'class_weight_power': 0.12995363067801072, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)}, value=0.9627
2025-09-22 15:47:45,171 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 42: {'lr': 0.0005977288268836814, 'batch_size': np.int64(32), 'epochs': np.int64(15), 'weight_decay': 0.0025738643047888672, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.5632228638730111, 'amp': np.False_, 'patch_size': np.int64(100), 'd_model': np.int64(77), 'n_heads': np.int64(8), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.03982896207879639, 'head_hidden': np.int64(79), 'use_focal_loss': np.False_, 'focal_gamma': 1.4199637902953608, 'label_smoothing': 0.15049612767169634, 'class_weight_power': 0.12995363067801072, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)} -> 0.9627
2025-09-22 15:47:45,172 - INFO - bo.run_bo - üîçBO Trial 43: Using RF surrogate + Expected Improvement
2025-09-22 15:47:45,172 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:47:45,172 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:47:45,172 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:47:45,172 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 5.255235039286419e-05, 'batch_size': 128, 'epochs': 42, 'weight_decay': 0.00042668747435946835, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.557402463583715, 'amp': False, 'patch_size': 25, 'd_model': 90, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.06773464201603903, 'head_hidden': 51, 'use_focal_loss': False, 'focal_gamma': 1.5460908301897827, 'label_smoothing': 0.15763863494079433, 'class_weight_power': 0.1804227161695833, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 4}
2025-09-22 15:47:45,173 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 5.255235039286419e-05, 'batch_size': 128, 'epochs': 42, 'weight_decay': 0.00042668747435946835, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.557402463583715, 'amp': False, 'patch_size': 25, 'd_model': 90, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.06773464201603903, 'head_hidden': 51, 'use_focal_loss': False, 'focal_gamma': 1.5460908301897827, 'label_smoothing': 0.15763863494079433, 'class_weight_power': 0.1804227161695833, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 4}
2025-09-22 15:48:40,106 - INFO - _models.training_function_executor - Model parameter count: 144,953
2025-09-22 15:48:40,107 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5107894500417787, 1.261125645687402, 1.145128828542736, 1.0656387110205778, 1.0166362905750275, 0.9870741668714824, 0.9531709743174187, 0.9326006311247478, 0.9168984784005381, 0.8983210185243086, 0.8820176322115949, 0.8705776681376628, 0.8598847254320474, 0.8503998598109002, 0.8457275195232747, 0.8410681562570388, 0.8326984298534308, 0.8300077246382837, 0.820454768656086, 0.8212607304479225, 0.8153024000437102, 0.813925909156941, 0.8113948018375436, 0.809035022239415, 0.8065230547102816, 0.807224155566494, 0.8021757581350264, 0.7982203699523516, 0.7967192089614036, 0.7957839255542365, 0.7949358420339446, 0.7937696320227213, 0.7905836130011572, 0.7848174249162815, 0.7858018083474251, 0.7834982174558568, 0.7831326447229123, 0.7839020676557288, 0.7827017457828827, 0.7750826887411794, 0.7766586128110348, 0.7750371016495001], 'val_losses': [1.2873223967576681, 1.1350567012543578, 1.045921655454637, 0.9865552154105344, 0.9596453856860258, 0.9323326285404305, 0.9308228052404796, 0.9079129045103116, 0.8918813881442714, 0.8877583384483981, 0.8706785399070466, 0.8663928537959668, 0.8662883775707116, 0.8551479836795403, 0.8516840183814058, 0.8510860947196817, 0.8536794176601763, 0.8385191678761869, 0.8557429676321183, 0.8440442889827996, 0.8459987788925701, 0.8406401183906616, 0.8492673876410722, 0.8307230512669564, 0.8375148130469925, 0.840676012784224, 0.8306901123108175, 0.8439475683517107, 0.8276828075467965, 0.825090437100624, 0.8274076846873397, 0.8308525182836084, 0.8262655061597648, 0.8260478208931656, 0.8215681395873048, 0.8238119645952775, 0.8295964883273294, 0.8237693014860064, 0.8285147688574338, 0.8241890614873081, 0.8290123942874903, 0.8213314893266309], 'val_acc': [0.7867435158501441, 0.8539030196717203, 0.8810925949129181, 0.8968800902142589, 0.9107881217892495, 0.9218143089838366, 0.915925322641273, 0.9284550808169403, 0.9371006139581506, 0.9309610324520736, 0.938854780102744, 0.9329657937601804, 0.9309610324520736, 0.9399824583385541, 0.9413607317378775, 0.9432401954642275, 0.9339681744142339, 0.9482520987344945, 0.9318381155243703, 0.9426137075554442, 0.9411101365743642, 0.9495050745520611, 0.9354717453953139, 0.9510086455331412, 0.9485026938980078, 0.945746147099361, 0.9503821576243578, 0.9416113269013908, 0.9547675729858414, 0.9575241197844881, 0.9572735246209748, 0.9508833479513845, 0.9566470367121914, 0.955268763312868, 0.9587770956020549, 0.9557699536398947, 0.9535145971682747, 0.9568976318757048, 0.9521363237689513, 0.9565217391304348, 0.9575241197844881, 0.9594035835108382], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 5.255235039286419e-05, 'batch_size': 128, 'epochs': 42, 'weight_decay': 0.00042668747435946835, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.557402463583715, 'amp': False, 'patch_size': 25, 'd_model': 90, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.06773464201603903, 'head_hidden': 51, 'use_focal_loss': False, 'focal_gamma': 1.5460908301897827, 'label_smoothing': 0.15763863494079433, 'class_weight_power': 0.1804227161695833, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 4}, 'model_parameter_count': 144953, 'model_size_validation': 'PASS'}
2025-09-22 15:48:40,107 - INFO - _models.training_function_executor - BO Objective: base=0.9594, size_penalty=0.0000, final=0.9594
2025-09-22 15:48:40,107 - INFO - _models.training_function_executor - Model size: 144,953 parameters (PASS 256K limit)
2025-09-22 15:48:40,107 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 54.935s
2025-09-22 15:48:40,232 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9594
2025-09-22 15:48:40,232 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.125s
2025-09-22 15:48:40,232 - INFO - bo.run_bo - Recorded observation #43: hparams={'lr': 5.255235039286419e-05, 'batch_size': np.int64(128), 'epochs': np.int64(42), 'weight_decay': 0.00042668747435946835, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.557402463583715, 'amp': np.False_, 'patch_size': np.int64(25), 'd_model': np.int64(90), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.06773464201603903, 'head_hidden': np.int64(51), 'use_focal_loss': np.False_, 'focal_gamma': 1.5460908301897827, 'label_smoothing': 0.15763863494079433, 'class_weight_power': 0.1804227161695833, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(4)}, value=0.9594
2025-09-22 15:48:40,232 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 43: {'lr': 5.255235039286419e-05, 'batch_size': np.int64(128), 'epochs': np.int64(42), 'weight_decay': 0.00042668747435946835, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.557402463583715, 'amp': np.False_, 'patch_size': np.int64(25), 'd_model': np.int64(90), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.06773464201603903, 'head_hidden': np.int64(51), 'use_focal_loss': np.False_, 'focal_gamma': 1.5460908301897827, 'label_smoothing': 0.15763863494079433, 'class_weight_power': 0.1804227161695833, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(4)} -> 0.9594
2025-09-22 15:48:40,233 - INFO - bo.run_bo - üîçBO Trial 44: Using RF surrogate + Expected Improvement
2025-09-22 15:48:40,233 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:48:40,233 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:48:40,233 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:48:40,233 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 9.705864376120174e-05, 'batch_size': 128, 'epochs': 12, 'weight_decay': 0.00117576849455241, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 0, 'grad_clip': 0.39806383464377426, 'amp': False, 'patch_size': 125, 'd_model': 83, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.02390715489125967, 'head_hidden': 84, 'use_focal_loss': True, 'focal_gamma': 1.9491321807278232, 'label_smoothing': 0.1343229314922921, 'class_weight_power': 0.2839940188596662, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}
2025-09-22 15:48:40,234 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 9.705864376120174e-05, 'batch_size': 128, 'epochs': 12, 'weight_decay': 0.00117576849455241, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 0, 'grad_clip': 0.39806383464377426, 'amp': False, 'patch_size': 125, 'd_model': 83, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.02390715489125967, 'head_hidden': 84, 'use_focal_loss': True, 'focal_gamma': 1.9491321807278232, 'label_smoothing': 0.1343229314922921, 'class_weight_power': 0.2839940188596662, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}
2025-09-22 15:48:49,792 - INFO - _models.training_function_executor - Model parameter count: 238,970
2025-09-22 15:48:49,792 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5404428100660996, 0.3488638314853742, 0.2813968864261882, 0.24240206113245666, 0.21789091467974991, 0.20109428850090139, 0.18100812640824726, 0.1713696679136745, 0.16300582125817042, 0.15673232142315008, 0.14953272709219737, 0.15316611846977018], 'val_losses': [0.3065859144988805, 0.23933067400456073, 0.21638369889407807, 0.19416691373686878, 0.17462854781159481, 0.1739335744404252, 0.15762501447746857, 0.15446977796239103, 0.14797471762039566, 0.14520071240770027, 0.14379448824628763, 0.14186539796043676], 'val_acc': [0.7589274527001629, 0.6734745019421126, 0.6979075303846636, 0.7272271645157249, 0.7656935221150232, 0.752036085703546, 0.7901265505575742, 0.7958902393183812, 0.8206991605062023, 0.8279664202480892, 0.830347074301466, 0.8319759428643028], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 9.705864376120174e-05, 'batch_size': 128, 'epochs': 12, 'weight_decay': 0.00117576849455241, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 0, 'grad_clip': 0.39806383464377426, 'amp': False, 'patch_size': 125, 'd_model': 83, 'n_heads': 4, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.02390715489125967, 'head_hidden': 84, 'use_focal_loss': True, 'focal_gamma': 1.9491321807278232, 'label_smoothing': 0.1343229314922921, 'class_weight_power': 0.2839940188596662, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 6}, 'model_parameter_count': 238970, 'model_size_validation': 'PASS'}
2025-09-22 15:48:49,792 - INFO - _models.training_function_executor - BO Objective: base=0.8320, size_penalty=0.0000, final=0.8320
2025-09-22 15:48:49,792 - INFO - _models.training_function_executor - Model size: 238,970 parameters (PASS 256K limit)
2025-09-22 15:48:49,792 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 9.560s
2025-09-22 15:48:49,919 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8320
2025-09-22 15:48:49,919 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.126s
2025-09-22 15:48:49,919 - INFO - bo.run_bo - Recorded observation #44: hparams={'lr': 9.705864376120174e-05, 'batch_size': np.int64(128), 'epochs': np.int64(12), 'weight_decay': 0.00117576849455241, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(0), 'grad_clip': 0.39806383464377426, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(83), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.02390715489125967, 'head_hidden': np.int64(84), 'use_focal_loss': np.True_, 'focal_gamma': 1.9491321807278232, 'label_smoothing': 0.1343229314922921, 'class_weight_power': 0.2839940188596662, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(6)}, value=0.8320
2025-09-22 15:48:49,919 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 44: {'lr': 9.705864376120174e-05, 'batch_size': np.int64(128), 'epochs': np.int64(12), 'weight_decay': 0.00117576849455241, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(0), 'grad_clip': 0.39806383464377426, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(83), 'n_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.02390715489125967, 'head_hidden': np.int64(84), 'use_focal_loss': np.True_, 'focal_gamma': 1.9491321807278232, 'label_smoothing': 0.1343229314922921, 'class_weight_power': 0.2839940188596662, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(6)} -> 0.8320
2025-09-22 15:48:49,919 - INFO - bo.run_bo - üîçBO Trial 45: Using RF surrogate + Expected Improvement
2025-09-22 15:48:49,919 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:48:49,920 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:48:49,920 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:48:49,920 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0009897777645446562, 'batch_size': 256, 'epochs': 6, 'weight_decay': 0.005439819541589563, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.7947630018336019, 'amp': False, 'patch_size': 50, 'd_model': 70, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.018293272168189715, 'head_hidden': 39, 'use_focal_loss': True, 'focal_gamma': 1.3533613463348955, 'label_smoothing': 0.08164029751859529, 'class_weight_power': 0.16385685412373255, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 5}
2025-09-22 15:48:49,921 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0009897777645446562, 'batch_size': 256, 'epochs': 6, 'weight_decay': 0.005439819541589563, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.7947630018336019, 'amp': False, 'patch_size': 50, 'd_model': 70, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.018293272168189715, 'head_hidden': 39, 'use_focal_loss': True, 'focal_gamma': 1.3533613463348955, 'label_smoothing': 0.08164029751859529, 'class_weight_power': 0.16385685412373255, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 5}
2025-09-22 15:48:54,446 - INFO - _models.training_function_executor - Model parameter count: 62,020
2025-09-22 15:48:54,446 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5094614897187153, 0.2878212038856683, 0.2026816987660476, 0.1486977524561784, 0.12063164421083358, 0.11118944061348411], 'val_losses': [0.28972879716282335, 0.1814336607874732, 0.13290872828690214, 0.11519537852392536, 0.10248479820473304, 0.09561507595973032], 'val_acc': [0.8179426137075555, 0.8832226538027816, 0.9186818694399198, 0.9282044856534268, 0.9263250219270768, 0.9263250219270768], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0009897777645446562, 'batch_size': 256, 'epochs': 6, 'weight_decay': 0.005439819541589563, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.7947630018336019, 'amp': False, 'patch_size': 50, 'd_model': 70, 'n_heads': 2, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.018293272168189715, 'head_hidden': 39, 'use_focal_loss': True, 'focal_gamma': 1.3533613463348955, 'label_smoothing': 0.08164029751859529, 'class_weight_power': 0.16385685412373255, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 5}, 'model_parameter_count': 62020, 'model_size_validation': 'PASS'}
2025-09-22 15:48:54,446 - INFO - _models.training_function_executor - BO Objective: base=0.9263, size_penalty=0.0000, final=0.9263
2025-09-22 15:48:54,446 - INFO - _models.training_function_executor - Model size: 62,020 parameters (PASS 256K limit)
2025-09-22 15:48:54,446 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 4.527s
2025-09-22 15:48:54,575 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9263
2025-09-22 15:48:54,575 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.129s
2025-09-22 15:48:54,575 - INFO - bo.run_bo - Recorded observation #45: hparams={'lr': 0.0009897777645446562, 'batch_size': np.int64(256), 'epochs': np.int64(6), 'weight_decay': 0.005439819541589563, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.7947630018336019, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(70), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.018293272168189715, 'head_hidden': np.int64(39), 'use_focal_loss': np.True_, 'focal_gamma': 1.3533613463348955, 'label_smoothing': 0.08164029751859529, 'class_weight_power': 0.16385685412373255, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(5)}, value=0.9263
2025-09-22 15:48:54,575 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 45: {'lr': 0.0009897777645446562, 'batch_size': np.int64(256), 'epochs': np.int64(6), 'weight_decay': 0.005439819541589563, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.7947630018336019, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(70), 'n_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.018293272168189715, 'head_hidden': np.int64(39), 'use_focal_loss': np.True_, 'focal_gamma': 1.3533613463348955, 'label_smoothing': 0.08164029751859529, 'class_weight_power': 0.16385685412373255, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(5)} -> 0.9263
2025-09-22 15:48:54,576 - INFO - bo.run_bo - üîçBO Trial 46: Using RF surrogate + Expected Improvement
2025-09-22 15:48:54,576 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:48:54,576 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:48:54,576 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:48:54,576 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00022717768090838034, 'batch_size': 64, 'epochs': 49, 'weight_decay': 0.0004991019055876184, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 1, 'grad_clip': 0.5169683234106014, 'amp': True, 'patch_size': 50, 'd_model': 116, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.3675256762291221, 'head_hidden': 72, 'use_focal_loss': True, 'focal_gamma': 1.1841258110465938, 'label_smoothing': 0.1102572291628167, 'class_weight_power': 0.5446549035058632, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:48:54,578 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00022717768090838034, 'batch_size': 64, 'epochs': 49, 'weight_decay': 0.0004991019055876184, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 1, 'grad_clip': 0.5169683234106014, 'amp': True, 'patch_size': 50, 'd_model': 116, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.3675256762291221, 'head_hidden': 72, 'use_focal_loss': True, 'focal_gamma': 1.1841258110465938, 'label_smoothing': 0.1102572291628167, 'class_weight_power': 0.5446549035058632, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:50:01,818 - INFO - _models.training_function_executor - Model parameter count: 112,056
2025-09-22 15:50:01,819 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5029449709334163, 0.332793855534174, 0.27512233000067443, 0.24272502192781223, 0.21850702477109032, 0.19017634825371443, 0.17469792060302664, 0.16102860665766103, 0.14724053159560102, 0.14257505701881257, 0.13384449652153735, 0.1300302850438965, 0.118851763633229, 0.11272226077497251, 0.11110060424321173, 0.10692411452121978, 0.10419913091348257, 0.1029334127934726, 0.09938176983223894, 0.09624817169050008, 0.09085735620978228, 0.09215246359085269, 0.08961213291222576, 0.08617136773422533, 0.08158879782602504, 0.08372505726342033, 0.08230222001075707, 0.0801834079064668, 0.08057228872062448, 0.07904601529396048, 0.07459006065231619, 0.0763697347581887, 0.07391344619493558, 0.0734328005633378, 0.07019529990165473, 0.06831347297571212, 0.06906730747054297, 0.0682833393964454, 0.06703818327098543, 0.06704594990499889, 0.06538448049984279, 0.06378080392270907, 0.06324052821253283, 0.06491112933802141, 0.06138159572369577, 0.06085568729567393, 0.059450674180424656, 0.05843930407599816, 0.05902772147464133], 'val_losses': [0.2959394716683254, 0.29109114274810155, 0.22534897984336333, 0.21420098103880658, 0.23074789572301296, 0.19287702775422738, 0.18424428481682845, 0.15315489551616543, 0.16323902543838154, 0.16500396607192802, 0.16836289567351267, 0.14409071317577613, 0.14785165942308226, 0.14790066029624882, 0.141679432889585, 0.1670321161873791, 0.1553077384616674, 0.1417811363851452, 0.15313239423064506, 0.1412979578129063, 0.1689484198168695, 0.1458022294963023, 0.14229555300693286, 0.16310185441030117, 0.1559666557100509, 0.13136288165381463, 0.14389292215056743, 0.13614681213366, 0.12882114521148075, 0.1335965961520347, 0.15720734813132392, 0.14805921501622524, 0.12851713503486992, 0.12597093196900144, 0.13545913955290326, 0.13277247667223002, 0.14446236039332735, 0.13499602011691178, 0.12446895809329522, 0.1259191019840743, 0.12935029180287985, 0.13150796797390257, 0.12931587825165314, 0.12166503624253787, 0.13995881554156134, 0.12972578417349692, 0.15763098131428832, 0.1413714742459998, 0.14745213657471937], 'val_acc': [0.38228292193960656, 0.5025686004260118, 0.5009397318631751, 0.6219771958401202, 0.6272396942739005, 0.6764816439042727, 0.6834983084826463, 0.7512843002130059, 0.7510337050494925, 0.7326149605312617, 0.7329908532765318, 0.777220899636637, 0.7648164390427264, 0.7725848891116401, 0.7867435158501441, 0.7440170404711189, 0.7496554316501691, 0.7905024433028442, 0.7697030447312367, 0.7782232802906904, 0.7693271519859667, 0.7836110763062273, 0.7874953013406841, 0.7745896504197469, 0.7961408344818945, 0.7822328029069039, 0.7777220899636637, 0.8190702919433654, 0.8270893371757925, 0.8319759428643028, 0.7951384538278411, 0.8091717829845884, 0.8323518356095727, 0.8225786242325523, 0.8133066031825585, 0.8196967798521488, 0.8351083824082195, 0.8118030322014785, 0.8300964791379526, 0.84826462849267, 0.8323518356095727, 0.8388673098609197, 0.8466357599298333, 0.8461345696028066, 0.8447562962034832, 0.8412479639142965, 0.8084199974940484, 0.8258363613582258, 0.8194461846886355], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00022717768090838034, 'batch_size': 64, 'epochs': 49, 'weight_decay': 0.0004991019055876184, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 1, 'grad_clip': 0.5169683234106014, 'amp': True, 'patch_size': 50, 'd_model': 116, 'n_heads': 8, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.3675256762291221, 'head_hidden': 72, 'use_focal_loss': True, 'focal_gamma': 1.1841258110465938, 'label_smoothing': 0.1102572291628167, 'class_weight_power': 0.5446549035058632, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 2}, 'model_parameter_count': 112056, 'model_size_validation': 'PASS'}
2025-09-22 15:50:01,819 - INFO - _models.training_function_executor - BO Objective: base=0.8194, size_penalty=0.0000, final=0.8194
2025-09-22 15:50:01,819 - INFO - _models.training_function_executor - Model size: 112,056 parameters (PASS 256K limit)
2025-09-22 15:50:01,819 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 67.243s
2025-09-22 15:50:01,945 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8194
2025-09-22 15:50:01,945 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.126s
2025-09-22 15:50:01,945 - INFO - bo.run_bo - Recorded observation #46: hparams={'lr': 0.00022717768090838034, 'batch_size': np.int64(64), 'epochs': np.int64(49), 'weight_decay': 0.0004991019055876184, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(1), 'grad_clip': 0.5169683234106014, 'amp': np.True_, 'patch_size': np.int64(50), 'd_model': np.int64(116), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.3675256762291221, 'head_hidden': np.int64(72), 'use_focal_loss': np.True_, 'focal_gamma': 1.1841258110465938, 'label_smoothing': 0.1102572291628167, 'class_weight_power': 0.5446549035058632, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)}, value=0.8194
2025-09-22 15:50:01,945 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 46: {'lr': 0.00022717768090838034, 'batch_size': np.int64(64), 'epochs': np.int64(49), 'weight_decay': 0.0004991019055876184, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(1), 'grad_clip': 0.5169683234106014, 'amp': np.True_, 'patch_size': np.int64(50), 'd_model': np.int64(116), 'n_heads': np.int64(8), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.3675256762291221, 'head_hidden': np.int64(72), 'use_focal_loss': np.True_, 'focal_gamma': 1.1841258110465938, 'label_smoothing': 0.1102572291628167, 'class_weight_power': 0.5446549035058632, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)} -> 0.8194
2025-09-22 15:50:01,946 - INFO - bo.run_bo - üîçBO Trial 47: Using RF surrogate + Expected Improvement
2025-09-22 15:50:01,946 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:50:01,946 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:50:01,946 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:50:01,946 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003560827019631585, 'batch_size': 32, 'epochs': 23, 'weight_decay': 4.9375486713495435e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.9710134302337216, 'amp': False, 'patch_size': 40, 'd_model': 74, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.05852092527028009, 'head_hidden': 89, 'use_focal_loss': True, 'focal_gamma': 1.4360089753771539, 'label_smoothing': 0.08923519566463582, 'class_weight_power': 0.18851735299222022, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 4}
2025-09-22 15:50:01,948 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003560827019631585, 'batch_size': 32, 'epochs': 23, 'weight_decay': 4.9375486713495435e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.9710134302337216, 'amp': False, 'patch_size': 40, 'd_model': 74, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.05852092527028009, 'head_hidden': 89, 'use_focal_loss': True, 'focal_gamma': 1.4360089753771539, 'label_smoothing': 0.08923519566463582, 'class_weight_power': 0.18851735299222022, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 4}
2025-09-22 15:50:52,683 - INFO - _models.training_function_executor - Model parameter count: 104,539
2025-09-22 15:50:52,683 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4155878681973422, 0.28278025666510437, 0.22995959506261746, 0.18915050705420516, 0.15467994738652235, 0.13732611006068335, 0.11773625854230654, 0.10989759120037225, 0.1036788271908413, 0.1021498753467361, 0.08753853312815221, 0.08707074691520984, 0.08306154503390747, 0.07851898079422416, 0.0750968970506109, 0.0704974783878258, 0.07144821028955962, 0.07047480463878236, 0.06853476989922942, 0.06582266856630839, 0.061609151757079844, 0.061134363136863726, 0.059357416269713706], 'val_losses': [0.25474280816856565, 0.19137139633146688, 0.1901610035572027, 0.12985886268942237, 0.10588677643482405, 0.09854255808974072, 0.10166284495626472, 0.07663937916195254, 0.09690470780765124, 0.09117183089256287, 0.07575335048532116, 0.09031150951867653, 0.06596558887309381, 0.06216354230496985, 0.07302661542381025, 0.0612849992960262, 0.05981575669187485, 0.06050209941914499, 0.06325223256971899, 0.06377556624306188, 0.056027451752047985, 0.05349899380895043, 0.06046735387785126], 'val_acc': [0.808169402330535, 0.8520235559453703, 0.8508958777095602, 0.900263124921689, 0.9244455582007267, 0.9194336549304598, 0.9168024057135697, 0.9471244204986844, 0.9169277032953264, 0.915549429896003, 0.9414860293196341, 0.9198095476757299, 0.9453702543540909, 0.9506327527878712, 0.9416113269013908, 0.9525122165142212, 0.9508833479513845, 0.9452449567723343, 0.9551434657311114, 0.9535145971682747, 0.9542663826588147, 0.9571482270392181, 0.9477509084074678], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003560827019631585, 'batch_size': 32, 'epochs': 23, 'weight_decay': 4.9375486713495435e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 2, 'grad_clip': 0.9710134302337216, 'amp': False, 'patch_size': 40, 'd_model': 74, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.05852092527028009, 'head_hidden': 89, 'use_focal_loss': True, 'focal_gamma': 1.4360089753771539, 'label_smoothing': 0.08923519566463582, 'class_weight_power': 0.18851735299222022, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 4}, 'model_parameter_count': 104539, 'model_size_validation': 'PASS'}
2025-09-22 15:50:52,683 - INFO - _models.training_function_executor - BO Objective: base=0.9478, size_penalty=0.0000, final=0.9478
2025-09-22 15:50:52,683 - INFO - _models.training_function_executor - Model size: 104,539 parameters (PASS 256K limit)
2025-09-22 15:50:52,683 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 50.737s
2025-09-22 15:50:52,810 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9478
2025-09-22 15:50:52,811 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.127s
2025-09-22 15:50:52,811 - INFO - bo.run_bo - Recorded observation #47: hparams={'lr': 0.003560827019631585, 'batch_size': np.int64(32), 'epochs': np.int64(23), 'weight_decay': 4.9375486713495435e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.9710134302337216, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(74), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.05852092527028009, 'head_hidden': np.int64(89), 'use_focal_loss': np.True_, 'focal_gamma': 1.4360089753771539, 'label_smoothing': 0.08923519566463582, 'class_weight_power': 0.18851735299222022, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(4)}, value=0.9478
2025-09-22 15:50:52,811 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 47: {'lr': 0.003560827019631585, 'batch_size': np.int64(32), 'epochs': np.int64(23), 'weight_decay': 4.9375486713495435e-05, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.9710134302337216, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(74), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.05852092527028009, 'head_hidden': np.int64(89), 'use_focal_loss': np.True_, 'focal_gamma': 1.4360089753771539, 'label_smoothing': 0.08923519566463582, 'class_weight_power': 0.18851735299222022, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(4)} -> 0.9478
2025-09-22 15:50:52,811 - INFO - bo.run_bo - üîçBO Trial 48: Using RF surrogate + Expected Improvement
2025-09-22 15:50:52,811 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:50:52,811 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:50:52,811 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:50:52,812 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00010823736211546206, 'batch_size': 64, 'epochs': 30, 'weight_decay': 0.0005445558315334579, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 2, 'grad_clip': 0.44871480779872774, 'amp': False, 'patch_size': 40, 'd_model': 76, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.08078450168651026, 'head_hidden': 97, 'use_focal_loss': True, 'focal_gamma': 1.6107958337486692, 'label_smoothing': 0.06023645803122692, 'class_weight_power': 0.11888625515186638, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:50:52,813 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00010823736211546206, 'batch_size': 64, 'epochs': 30, 'weight_decay': 0.0005445558315334579, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 2, 'grad_clip': 0.44871480779872774, 'amp': False, 'patch_size': 40, 'd_model': 76, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.08078450168651026, 'head_hidden': 97, 'use_focal_loss': True, 'focal_gamma': 1.6107958337486692, 'label_smoothing': 0.06023645803122692, 'class_weight_power': 0.11888625515186638, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 2}
2025-09-22 15:51:24,686 - INFO - _models.training_function_executor - Model parameter count: 156,961
2025-09-22 15:51:24,686 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5635795280600133, 0.3997422846889143, 0.2915909069146563, 0.23254872921680264, 0.1994069145670142, 0.17483081723182448, 0.15947786934774846, 0.1517640685579948, 0.13405169193014907, 0.13035843723847693, 0.11954319680575996, 0.11410516881649235, 0.10727477441553958, 0.10168804951821772, 0.0948378497237132, 0.09395755089452305, 0.09011439006250835, 0.08661304971753021, 0.0855899408372258, 0.08005478242317977, 0.08122609274086863, 0.07787589109876014, 0.07204533132558727, 0.07235504689096332, 0.07296645981005975, 0.07229686065417418, 0.07203619709961401, 0.06854880914193356, 0.0709535009835534, 0.07111851833425821], 'val_losses': [0.3793269795868693, 0.2867653122422182, 0.21441722099545935, 0.18293036820427425, 0.15770599080881906, 0.14278506862330803, 0.13358914251922963, 0.1385708073357837, 0.11638857813871993, 0.11398066203378897, 0.10769988819532018, 0.10262515166088941, 0.09739356148040933, 0.10308158903326356, 0.09442722730972447, 0.09521929103059139, 0.08768839495696007, 0.0900599227231213, 0.08945814488660332, 0.07934983140759683, 0.08059227354745192, 0.07791947570809474, 0.08098049364473389, 0.07335310752059326, 0.07790839846753907, 0.07533874257403857, 0.07607508549608394, 0.07266379684740074, 0.07510239733533593, 0.07325589083636438], 'val_acc': [0.7794762561082571, 0.806289938604185, 0.8639268262122541, 0.8879839619095351, 0.9134193710061396, 0.9131687758426262, 0.9234431775466734, 0.9184312742764065, 0.9325899010149105, 0.9236937727101867, 0.9258238316000501, 0.9278285929081569, 0.9374765067034206, 0.9283297832351836, 0.9406089462473374, 0.9344693647412605, 0.9413607317378775, 0.9441172785365243, 0.9338428768324771, 0.9454955519358477, 0.9456208495176043, 0.9487532890615211, 0.9498809672973312, 0.9498809672973312, 0.9482520987344945, 0.9485026938980078, 0.9491291818067912, 0.9512592406966546, 0.9492544793885478, 0.9511339431148979], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00010823736211546206, 'batch_size': 64, 'epochs': 30, 'weight_decay': 0.0005445558315334579, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 2, 'grad_clip': 0.44871480779872774, 'amp': False, 'patch_size': 40, 'd_model': 76, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.08078450168651026, 'head_hidden': 97, 'use_focal_loss': True, 'focal_gamma': 1.6107958337486692, 'label_smoothing': 0.06023645803122692, 'class_weight_power': 0.11888625515186638, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 2}, 'model_parameter_count': 156961, 'model_size_validation': 'PASS'}
2025-09-22 15:51:24,686 - INFO - _models.training_function_executor - BO Objective: base=0.9511, size_penalty=0.0000, final=0.9511
2025-09-22 15:51:24,686 - INFO - _models.training_function_executor - Model size: 156,961 parameters (PASS 256K limit)
2025-09-22 15:51:24,686 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 31.874s
2025-09-22 15:51:24,931 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9511
2025-09-22 15:51:24,931 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.245s
2025-09-22 15:51:24,931 - INFO - bo.run_bo - Recorded observation #48: hparams={'lr': 0.00010823736211546206, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'weight_decay': 0.0005445558315334579, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.44871480779872774, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(76), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.08078450168651026, 'head_hidden': np.int64(97), 'use_focal_loss': np.True_, 'focal_gamma': 1.6107958337486692, 'label_smoothing': 0.06023645803122692, 'class_weight_power': 0.11888625515186638, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)}, value=0.9511
2025-09-22 15:51:24,931 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 48: {'lr': 0.00010823736211546206, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'weight_decay': 0.0005445558315334579, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(2), 'grad_clip': 0.44871480779872774, 'amp': np.False_, 'patch_size': np.int64(40), 'd_model': np.int64(76), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.08078450168651026, 'head_hidden': np.int64(97), 'use_focal_loss': np.True_, 'focal_gamma': 1.6107958337486692, 'label_smoothing': 0.06023645803122692, 'class_weight_power': 0.11888625515186638, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(2)} -> 0.9511
2025-09-22 15:51:24,932 - INFO - bo.run_bo - üîçBO Trial 49: Using RF surrogate + Expected Improvement
2025-09-22 15:51:24,932 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:51:24,932 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:51:24,932 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:51:24,932 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0024247718134598945, 'batch_size': 64, 'epochs': 30, 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.20267879736918043, 'amp': False, 'patch_size': 50, 'd_model': 76, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.017179376622242063, 'head_hidden': 40, 'use_focal_loss': False, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}
2025-09-22 15:51:24,934 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0024247718134598945, 'batch_size': 64, 'epochs': 30, 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.20267879736918043, 'amp': False, 'patch_size': 50, 'd_model': 76, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.017179376622242063, 'head_hidden': 40, 'use_focal_loss': False, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}
2025-09-22 15:52:04,824 - INFO - _models.training_function_executor - Model parameter count: 188,729
2025-09-22 15:52:04,824 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1800516461405923, 1.046938104169155, 0.9920670220835691, 0.9392952104560774, 0.9154449311898913, 0.9012282948869924, 0.8782689603346017, 0.8691786511856351, 0.8586295348799617, 0.8506456773423672, 0.8365807485726828, 0.8340922662650412, 0.8273029213497163, 0.8210322658183584, 0.8137853236939754, 0.808828839244346, 0.8065471449162105, 0.8003829628760342, 0.7990348417071163, 0.7971097538279505, 0.7919790005301509, 0.7908658909662488, 0.7896376063910858, 0.7872486419909708, 0.7847326056788381, 0.7814802933138079, 0.7770065083594242, 0.7808580004070664, 0.7759171175885028, 0.7734097297809344], 'val_losses': [1.0818096258604142, 1.0276126824604093, 0.9377382402431396, 0.9297549380588137, 0.898666396555992, 0.886083843355235, 0.8726314904620781, 0.8607975393813768, 0.8473119524737794, 0.8426553116542567, 0.8372869779630168, 0.8338634056071712, 0.8293229508199812, 0.8231177910529437, 0.8190977122756476, 0.8273831226296658, 0.8132603536824985, 0.8092560331678946, 0.8160231458022024, 0.8082544746113337, 0.802871145867208, 0.8024249663255579, 0.8047216433642905, 0.8069547990064188, 0.806403994074801, 0.8005194566539917, 0.8023043711402634, 0.7976693710487986, 0.7949160585993739, 0.7958649645143248], 'val_acc': [0.814308983836612, 0.8554065906528004, 0.901014910412229, 0.899887232176419, 0.915549429896003, 0.9218143089838366, 0.9320887106878837, 0.9367247212128806, 0.9447437664453076, 0.9517604310236812, 0.9511339431148979, 0.9530134068412479, 0.9558952512216514, 0.9568976318757048, 0.961408344818945, 0.9530134068412479, 0.962536023054755, 0.9651672722716451, 0.9644154867811051, 0.9629119158000251, 0.9680491166520486, 0.969051497306102, 0.9652925698534018, 0.9644154867811051, 0.9662949505074552, 0.9699285803783987, 0.9684250093973187, 0.9710562586142087, 0.9730610199223155, 0.970179175541912], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0024247718134598945, 'batch_size': 64, 'epochs': 30, 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.20267879736918043, 'amp': False, 'patch_size': 50, 'd_model': 76, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.017179376622242063, 'head_hidden': 40, 'use_focal_loss': False, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}, 'model_parameter_count': 188729, 'model_size_validation': 'PASS'}
2025-09-22 15:52:04,824 - INFO - _models.training_function_executor - BO Objective: base=0.9702, size_penalty=0.0000, final=0.9702
2025-09-22 15:52:04,824 - INFO - _models.training_function_executor - Model size: 188,729 parameters (PASS 256K limit)
2025-09-22 15:52:04,824 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 39.892s
2025-09-22 15:52:04,952 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9702
2025-09-22 15:52:04,952 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.128s
2025-09-22 15:52:04,952 - INFO - bo.run_bo - Recorded observation #49: hparams={'lr': 0.0024247718134598945, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.20267879736918043, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(76), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.017179376622242063, 'head_hidden': np.int64(40), 'use_focal_loss': np.False_, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)}, value=0.9702
2025-09-22 15:52:04,952 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 49: {'lr': 0.0024247718134598945, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.20267879736918043, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(76), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.017179376622242063, 'head_hidden': np.int64(40), 'use_focal_loss': np.False_, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)} -> 0.9702
2025-09-22 15:52:04,953 - INFO - bo.run_bo - üîçBO Trial 50: Using RF surrogate + Expected Improvement
2025-09-22 15:52:04,953 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 15:52:04,953 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:52:04,953 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:52:04,953 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0006235514793058508, 'batch_size': 64, 'epochs': 21, 'weight_decay': 0.0064091432091461205, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.5818834089966101, 'amp': False, 'patch_size': 125, 'd_model': 90, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.15478130035604007, 'head_hidden': 68, 'use_focal_loss': True, 'focal_gamma': 1.756806634228896, 'label_smoothing': 0.08322941688932985, 'class_weight_power': 0.09709413680529702, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:52:04,955 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0006235514793058508, 'batch_size': 64, 'epochs': 21, 'weight_decay': 0.0064091432091461205, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.5818834089966101, 'amp': False, 'patch_size': 125, 'd_model': 90, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.15478130035604007, 'head_hidden': 68, 'use_focal_loss': True, 'focal_gamma': 1.756806634228896, 'label_smoothing': 0.08322941688932985, 'class_weight_power': 0.09709413680529702, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 6}
2025-09-22 15:52:29,568 - INFO - _models.training_function_executor - Model parameter count: 161,739
2025-09-22 15:52:29,568 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4804337604976524, 0.31596473831508004, 0.25171264205529975, 0.2145550042237718, 0.17738222030689554, 0.15553755082286427, 0.12945961489696065, 0.11906900966734686, 0.10428812567446759, 0.09078480504728352, 0.0829672704526376, 0.07616732666689109, 0.07317573677944375, 0.062473085937903156, 0.06183946134433693, 0.058158419482265016, 0.05688284136149772, 0.05386773780624516, 0.05009455694085176, 0.05087160600934363, 0.04783880877827349], 'val_losses': [0.32592183423025867, 0.22542119823987136, 0.19404291571523444, 0.17812636328466463, 0.13015156171402825, 0.11579240421844954, 0.116192404124286, 0.09164336685484745, 0.10319994874611309, 0.08357933027066021, 0.07954255727400622, 0.08063341837010875, 0.07043343805296651, 0.060663240684585874, 0.060693276364833786, 0.056832574329389964, 0.0584475795476191, 0.05795644059106978, 0.055351706630823505, 0.05362903347427707, 0.05225620040189964], 'val_acc': [0.7787244706177171, 0.8151860669089087, 0.8636762310487408, 0.8644280165392808, 0.8973812805412855, 0.9174288936223531, 0.9256985340182935, 0.9393559704297707, 0.9154241323142464, 0.9416113269013908, 0.9480015035709811, 0.939230672848014, 0.9520110261871946, 0.9579000125297582, 0.9563964415486781, 0.9595288810925949, 0.9575241197844881, 0.9590276907655683, 0.9594035835108382, 0.9609071544919183, 0.9620348327277284], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0006235514793058508, 'batch_size': 64, 'epochs': 21, 'weight_decay': 0.0064091432091461205, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': 4, 'grad_clip': 0.5818834089966101, 'amp': False, 'patch_size': 125, 'd_model': 90, 'n_heads': 1, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.15478130035604007, 'head_hidden': 68, 'use_focal_loss': True, 'focal_gamma': 1.756806634228896, 'label_smoothing': 0.08322941688932985, 'class_weight_power': 0.09709413680529702, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 6}, 'model_parameter_count': 161739, 'model_size_validation': 'PASS'}
2025-09-22 15:52:29,568 - INFO - _models.training_function_executor - BO Objective: base=0.9620, size_penalty=0.0000, final=0.9620
2025-09-22 15:52:29,568 - INFO - _models.training_function_executor - Model size: 161,739 parameters (PASS 256K limit)
2025-09-22 15:52:29,568 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 24.615s
2025-09-22 15:52:29,696 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9620
2025-09-22 15:52:29,696 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.128s
2025-09-22 15:52:29,696 - INFO - bo.run_bo - Recorded observation #50: hparams={'lr': 0.0006235514793058508, 'batch_size': np.int64(64), 'epochs': np.int64(21), 'weight_decay': 0.0064091432091461205, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.5818834089966101, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(90), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.15478130035604007, 'head_hidden': np.int64(68), 'use_focal_loss': np.True_, 'focal_gamma': 1.756806634228896, 'label_smoothing': 0.08322941688932985, 'class_weight_power': 0.09709413680529702, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)}, value=0.9620
2025-09-22 15:52:29,696 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 50: {'lr': 0.0006235514793058508, 'batch_size': np.int64(64), 'epochs': np.int64(21), 'weight_decay': 0.0064091432091461205, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('cosine'), 'warmup_epochs': np.int64(4), 'grad_clip': 0.5818834089966101, 'amp': np.False_, 'patch_size': np.int64(125), 'd_model': np.int64(90), 'n_heads': np.int64(1), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.15478130035604007, 'head_hidden': np.int64(68), 'use_focal_loss': np.True_, 'focal_gamma': 1.756806634228896, 'label_smoothing': 0.08322941688932985, 'class_weight_power': 0.09709413680529702, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calib_batches': np.int64(6)} -> 0.9620
2025-09-22 15:52:29,697 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.9702
2025-09-22 15:52:29,697 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.0024247718134598945, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.20267879736918043, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(76), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.017179376622242063, 'head_hidden': np.int64(40), 'use_focal_loss': np.False_, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)}
2025-09-22 15:52:29,697 - INFO - visualization - Generating BO visualization charts with 50 trials...
2025-09-22 15:52:31,031 - INFO - visualization - BO summary saved to: charts/20250922_155229_BO_Tiny1DTransformerRR/bo_summary.txt
2025-09-22 15:52:31,031 - INFO - visualization - BO charts saved to: charts/20250922_155229_BO_Tiny1DTransformerRR
2025-09-22 15:52:31,031 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä BO charts saved to: charts/20250922_155229_BO_Tiny1DTransformerRR
2025-09-22 15:52:31,088 - INFO - evaluation.code_generation_pipeline_orchestrator - üöÄ STEP 4: Final Training Execution
2025-09-22 15:52:31,088 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-22 15:52:31,178 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-22 15:52:31,190 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-22 15:52:31,204 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-22 15:52:31,205 - INFO - _models.training_function_executor - Loaded training function: Tiny1DTransformerRR
2025-09-22 15:52:31,205 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-22 15:52:31,205 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.0024247718134598945, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.20267879736918043, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(76), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.017179376622242063, 'head_hidden': np.int64(40), 'use_focal_loss': np.False_, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)}
2025-09-22 15:52:31,205 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 15:52:31,226 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformerRR
2025-09-22 15:52:31,226 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0024247718134598945, 'batch_size': np.int64(64), 'epochs': np.int64(30), 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': np.int64(5), 'grad_clip': 0.20267879736918043, 'amp': np.False_, 'patch_size': np.int64(50), 'd_model': np.int64(76), 'n_heads': np.int64(1), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.017179376622242063, 'head_hidden': np.int64(40), 'use_focal_loss': np.False_, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(1)}
2025-09-22 15:52:31,228 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0024247718134598945, 'batch_size': 64, 'epochs': 30, 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.20267879736918043, 'amp': False, 'patch_size': 50, 'd_model': 76, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.017179376622242063, 'head_hidden': 40, 'use_focal_loss': False, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}
2025-09-22 15:53:20,270 - INFO - _models.training_function_executor - Model parameter count: 188,729
2025-09-22 15:53:20,270 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1906426470090412, 1.049375567325326, 0.9727730201355629, 0.9219765626820164, 0.8850626174028912, 0.862597566561022, 0.8466264589270689, 0.836967711125552, 0.8301246107437177, 0.8215328834490864, 0.8108192104383192, 0.8054914821503921, 0.8022898122990906, 0.7983378025657575, 0.7946015137524822, 0.7931708903817435, 0.7894054217728598, 0.7855632674149159, 0.7837865669056235, 0.7804390338295062, 0.7823333515581744, 0.7802166788694661, 0.7764564907445656, 0.7752733372552163, 0.7751826993350325, 0.7741859612602564, 0.7703774152704498, 0.7692372100584586, 0.7706412059742638, 0.7685562010373511], 'val_losses': [1.0675034650260824, 0.9732684856356295, 0.924990310092554, 0.888238686364388, 0.8535985300186534, 0.8382316760385013, 0.8357953681393306, 0.8220867089485853, 0.8204621206702051, 0.8097773659165952, 0.8157396695830605, 0.8159496020945756, 0.8054115694164453, 0.8004123965102589, 0.8049221404499457, 0.8017929609951949, 0.8019370490257016, 0.802163715721957, 0.7960803652019984, 0.791500407886897, 0.8028653915353561, 0.7906651405481868, 0.7922694681282689, 0.7913620651538953, 0.7932004381411417, 0.7917878834006661, 0.7911130188880015, 0.7913995318796564, 0.7893364077951646, 0.7901805202970765], 'val_acc': [0.834820086198256, 0.8878420366843741, 0.9067856068958605, 0.9196151147639571, 0.9444722862583943, 0.9523904981457352, 0.9520898065550767, 0.9608098626841736, 0.9584043299589055, 0.9656209281347098, 0.9665230029066854, 0.9637165480605392, 0.9691289966923925, 0.968828305101734, 0.9669239250275634, 0.9691289966923925, 0.9670241555577829, 0.9664227723764659, 0.970632454645685, 0.9739400621429287, 0.9658213891951488, 0.9737396010824897, 0.9725368347198556, 0.9747419063846847, 0.9718352210083191, 0.9716347599478802, 0.9719354515385387, 0.9740402926731483, 0.9743409842638068, 0.9740402926731483], 'model_name': 'Tiny1DTransformerRR', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0024247718134598945, 'batch_size': 64, 'epochs': 30, 'weight_decay': 0.0016428459661794995, 'optimizer': np.str_('adamw'), 'scheduler': np.str_('none'), 'warmup_epochs': 5, 'grad_clip': 0.20267879736918043, 'amp': False, 'patch_size': 50, 'd_model': 76, 'n_heads': 1, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.017179376622242063, 'head_hidden': 40, 'use_focal_loss': False, 'focal_gamma': 2.9257598296044955, 'label_smoothing': 0.1845690778045238, 'class_weight_power': 0.10809981416897932, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 1}, 'model_parameter_count': 188729, 'model_size_validation': 'PASS'}
2025-09-22 15:53:20,270 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final validation metrics from training (avoids preprocessing mismatch)
2025-09-22 15:53:20,270 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 0.9740402926731483, 'macro_f1': None}
2025-09-22 15:53:20,278 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä STEP 5: Performance Analysis
2025-09-22 15:53:20,278 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: Tiny1DTransformerRR
2025-09-22 15:53:20,278 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.9702
2025-09-22 15:53:20,278 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: 0.9740
2025-09-22 15:53:20,278 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-09-22 15:53:20,278 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: Tiny1DTransformerRR
2025-09-22 15:53:20,278 - INFO - evaluation.code_generation_pipeline_orchestrator - Score: 0.9740
2025-09-22 15:53:20,278 - INFO - __main__ - AI-enhanced training completed!
2025-09-22 15:53:20,278 - INFO - __main__ - Final model achieved: {'acc': 0.9740402926731483, 'macro_f1': None}
2025-09-22 15:53:20,278 - INFO - __main__ - Pipeline completed successfully in single attempt
2025-09-22 15:53:20,278 - INFO - __main__ - Pipeline completed: Tiny1DTransformerRR, metrics: {'acc': 0.9740402926731483, 'macro_f1': None}
2025-09-22 15:53:20,279 - INFO - __main__ - Pipeline summary saved to charts/pipeline_summary_20250922_155320.json
2025-09-22 15:53:20,292 - INFO - __main__ - Model saved: trained_models/best_model_Tiny1DTransformerRR_20250922_155320.pth, performance: {'acc': 0.9740402926731483, 'macro_f1': None}
2025-09-22 15:53:20,292 - INFO - __main__ - AI-enhanced processing completed successfully
