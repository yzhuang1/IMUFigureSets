2025-09-22 18:46:57,119 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 18:46:57,227 - INFO - __main__ - Logging system initialized successfully
2025-09-22 18:46:57,227 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-22 18:46:57,227 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-22 18:46:57,227 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-22 18:46:57,227 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-22 18:46:57,228 - INFO - __main__ - Attempting to load: X.npy
2025-09-22 18:46:57,269 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-22 18:46:57,308 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-22 18:46:57,308 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-22 18:46:57,308 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-22 18:46:57,308 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-22 18:46:57,310 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-22 18:46:57,310 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-22 18:46:57,310 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-22 18:46:57,310 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-22 18:46:57,310 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-22 18:46:57,310 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-22 18:46:57,310 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-22 18:46:57,310 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-22 18:46:57,310 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-22 18:46:57,409 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-22 18:46:57,409 - INFO - class_balancing - Class imbalance analysis:
2025-09-22 18:46:57,409 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-22 18:46:57,409 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-22 18:46:57,409 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-22 18:46:57,409 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-22 18:46:57,409 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-22 18:46:57,410 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-22 18:46:57,410 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-22 18:46:57,410 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-22 18:46:57,581 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-22 18:46:57,582 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-22 18:46:57,582 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-22 18:46:57,582 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-22 18:46:57,582 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-22 18:46:57,582 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-22 18:46:57,582 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-22 18:50:36,637 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:50:36,662 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 18:50:36,662 - INFO - _models.ai_code_generator - Prompt length: 3411 characters
2025-09-22 18:50:36,662 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 18:50:36,662 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 18:50:36,662 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 18:50:37,455 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 429 Too Many Requests"
2025-09-22 18:50:37,456 - INFO - openai._base_client - Retrying request to /responses in 0.378039 seconds
2025-09-22 18:52:31,016 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:52:31,017 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 18:52:31,017 - INFO - _models.ai_code_generator - AI generated training function: CAT-Net-Tiny-1D-DualLead
2025-09-22 18:52:31,017 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-09-22 18:52:31,017 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-09-22 18:52:31,017 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: CAT-Net-Tiny-1D-DualLead
2025-09-22 18:52:31,017 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'weight_decay', 'scheduler', 'step_size', 'gamma', 'base_channels', 'cnn_depth', 'kernel_size', 'pool_stride', 'dropout', 'd_model', 'nhead', 'num_transformer_layers', 'mlp_ratio', 'focal_gamma', 'focal_alpha', 'label_smoothing', 'grad_clip', 'augment_noise_std', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'seed']
2025-09-22 18:52:31,017 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-09-22 18:52:31,018 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-22 18:52:31,018 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_CAT-Net-Tiny-1D-DualLead_1758585151.json
2025-09-22 18:52:31,018 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_CAT-Net-Tiny-1D-DualLead_1758585151.json
2025-09-22 18:52:31,018 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-22 18:52:31,018 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: CAT-Net-Tiny-1D-DualLead
2025-09-22 18:52:31,018 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 18:52:31,019 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 18:52:31,021 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-22 18:52:31,022 - INFO - package_installer - Available packages: {'torch'}
2025-09-22 18:52:31,022 - INFO - package_installer - Missing packages: set()
2025-09-22 18:52:31,022 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-22 18:52:31,022 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 18:52:31,022 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:52:31,022 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 18:52:31,022 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'scheduler', 'step_size', 'gamma', 'base_channels', 'cnn_depth', 'kernel_size', 'pool_stride', 'dropout', 'd_model', 'nhead', 'num_transformer_layers', 'mlp_ratio', 'focal_gamma', 'focal_alpha', 'label_smoothing', 'grad_clip', 'augment_noise_std', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'seed']
2025-09-22 18:52:31,022 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 18:52:31,022 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:52:31,022 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 18:52:31,056 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 18:52:31,186 - INFO - bo.run_bo - Converted GPT search space: 25 parameters
2025-09-22 18:52:31,186 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 18:52:31,186 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 18:52:31,187 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 18:52:31,187 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 18:52:31,187 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 18:52:31,187 - INFO - _models.training_function_executor - Executing training function: CAT-Net-Tiny-1D-DualLead
2025-09-22 18:52:31,187 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 32, 'epochs': 76, 'weight_decay': 9.846738873614564e-05, 'scheduler': 'none', 'step_size': 20, 'gamma': 0.1889776750780226, 'base_channels': 18, 'cnn_depth': 3, 'kernel_size': 7, 'pool_stride': 5, 'dropout': 0.0714334089609704, 'd_model': 18, 'nhead': 2, 'num_transformer_layers': 0, 'mlp_ratio': 3, 'focal_gamma': 3.6099938613341243, 'focal_alpha': 0.9385527090157504, 'label_smoothing': 7.787658410143285e-05, 'grad_clip': 4.961057796456088, 'augment_noise_std': 0.006174815096277166, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 500186}
2025-09-22 18:52:31,189 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 32, 'epochs': 76, 'weight_decay': 9.846738873614564e-05, 'scheduler': 'none', 'step_size': 20, 'gamma': 0.1889776750780226, 'base_channels': 18, 'cnn_depth': 3, 'kernel_size': 7, 'pool_stride': 5, 'dropout': 0.0714334089609704, 'd_model': 18, 'nhead': 2, 'num_transformer_layers': 0, 'mlp_ratio': 3, 'focal_gamma': 3.6099938613341243, 'focal_alpha': 0.9385527090157504, 'label_smoothing': 7.787658410143285e-05, 'grad_clip': 4.961057796456088, 'augment_noise_std': 0.006174815096277166, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 500186}
2025-09-22 18:52:31,216 - ERROR - _models.training_function_executor - Training execution failed: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
2025-09-22 18:52:31,216 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 18:52:31,216 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 18:52:31,216 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 18:52:31,217 - INFO - _models.ai_code_generator - Prompt length: 18442 characters
2025-09-22 18:52:31,217 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 18:52:31,217 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 18:52:31,217 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 18:54:16,284 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:54:16,289 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 18:54:16,290 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250922_185416_attempt1.txt
2025-09-22 18:54:16,290 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Optimizer & schedule\n    lr=1e-3,\n    batch_size=64,\n    epochs=20,\n    weight_decay=1e-4,\n    scheduler='none',\n    step_size=10,\n    gamma=0.5,\n    # Model size & architecture\n    base_channels=16,\n    cnn_depth=3,\n    kernel_size=5,\n    pool_stride=4,\n    dropout=0.1,\n    d_model=32,\n    nhead=2,\n    num_transformer_layers=1,\n    mlp_ratio=2,\n    # Loss & regularization\n    focal_gamma=2.0,\n    focal_alpha=0.25,\n    label_smoothing=0.0,\n    grad_clip=1.0,\n    augment_noise_std=0.001,\n    # Quantization options\n    quantization_bits=8,\n    quantize_weights=True,\n    quantize_activations=True,\n    # Misc\n    seed=42,\n):\n    import math\n    import io\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n    from torch.optim import AdamW\n    from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n    from torch.ao.quantization import quantize_dynamic\n\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    num_classes = 5\n\n    # Ensure tensor dtypes and force CPU for datasets/samplers to avoid device mismatch\n    X_train = X_train.float().detach().cpu().contiguous()\n    X_val = X_val.float().detach().cpu().contiguous()\n    y_train = y_train.long().detach().cpu().contiguous()\n    y_val = y_val.long().detach().cpu().contiguous()\n\n    # Shape normalization helper: expected (B, C=2, T=1000)\n    def ensure_ch_first(x):\n        if x.dim() != 3:\n            raise ValueError(f'Expected 3D tensor (N, T, C) or (N, C, T), got {list(x.shape)}')\n        # If last dim is 2, assume (N, T, C)\n        if x.shape[-1] == 2:\n            return x.permute(0, 2, 1).contiguous()\n        elif x.shape[1] == 2:\n            return x\n        else:\n            raise ValueError('Input must have 2 channels (leads) either as last or second dimension')\n\n    # Dataset & Sampler with class balancing (all CPU)\n    with torch.no_grad():\n        # Compute class weights and sample weights on CPU to avoid device mismatches\n        classes, counts = torch.unique(y_train, return_counts=True)\n        freq = torch.zeros(num_classes, dtype=torch.float, device='cpu')\n        freq[classes.cpu()] = counts.float().cpu()\n        freq = torch.clamp(freq, min=1.0)\n        class_weights = (1.0 / freq)\n        class_weights = class_weights * (num_classes / class_weights.sum())\n        sample_weights = class_weights[y_train]\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_sampler = WeightedRandomSampler(weights=sample_weights.double().cpu(), num_samples=len(sample_weights), replacement=True)\n\n    # IMPORTANT: pin_memory=False per requirement\n    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=train_sampler, shuffle=False, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Model components\n    class SEChannelAttention(nn.Module):\n        def __init__(self, channels, reduction=8):\n            super().__init__()\n            hidden = max(1, channels // reduction)\n            self.avg = nn.AdaptiveAvgPool1d(1)\n            self.fc1 = nn.Linear(channels, hidden, bias=True)\n            self.fc2 = nn.Linear(hidden, channels, bias=True)\n            self.act = nn.SiLU()\n            self.gate = nn.Sigmoid()\n        def forward(self, x):\n            # x: (B, C, T)\n            b, c, _ = x.shape\n            s = self.avg(x).view(b, c)\n            s = self.fc2(self.act(self.fc1(s)))\n            s = self.gate(s).view(b, c, 1)\n            return x * s\n\n    class ResidualBlock1D(nn.Module):\n        def __init__(self, c_in, c_out, k=5, se_reduction=8):\n            super().__init__()\n            p = k // 2\n            self.conv1 = nn.Conv1d(c_in, c_out, k, padding=p, bias=False)\n            self.bn1 = nn.BatchNorm1d(c_out)\n            self.conv2 = nn.Conv1d(c_out, c_out, k, padding=p, bias=False)\n            self.bn2 = nn.BatchNorm1d(c_out)\n            self.act = nn.SiLU()\n            self.se = SEChannelAttention(c_out, reduction=se_reduction)\n            self.down = None\n            if c_in != c_out:\n                self.down = nn.Conv1d(c_in, c_out, kernel_size=1, bias=False)\n        def forward(self, x):\n            identity = x\n            out = self.act(self.bn1(self.conv1(x)))\n            out = self.bn2(self.conv2(out))\n            out = self.se(out)\n            if self.down is not None:\n                identity = self.down(identity)\n            out = self.act(out + identity)\n            return out\n\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model, max_len=2048):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            self.register_buffer('pe', pe.unsqueeze(0), persistent=False)  # (1, max_len, d_model)\n        def forward(self, x):\n            # x: (B, T, D)\n            t = x.size(1)\n            return x + self.pe[:, :t, :]\n\n    class CATNetTiny(nn.Module):\n        def __init__(self, base_channels=16, depth=3, k=5, pool_stride=4, d_model=32, nhead=2, num_layers=1, mlp_ratio=2, dropout=0.1, num_classes=5):\n            super().__init__()\n            c1 = base_channels\n            c2 = max(base_channels, min(48, (3 * base_channels) // 2))\n            c3 = min(64, base_channels * 2)\n\n            # Stem + pooling to shrink sequence length quickly\n            p = k // 2\n            self.stem = nn.Sequential(\n                nn.Conv1d(2, c1, kernel_size=k, padding=p, bias=False),\n                nn.BatchNorm1d(c1),\n                nn.SiLU(),\n                nn.MaxPool1d(kernel_size=pool_stride, stride=pool_stride)\n            )\n\n            blocks = []\n            in_c = c1\n            for i in range(depth):\n                out_c = [c1, c2, c3][min(i, 2)]\n                blocks.append(ResidualBlock1D(in_c, out_c, k=k))\n                blocks.append(nn.MaxPool1d(kernel_size=pool_stride, stride=pool_stride))\n                in_c = out_c\n            self.cnn = nn.Sequential(*blocks)\n\n            # Project to transformer dimension\n            self.proj = nn.Conv1d(in_c, d_model, kernel_size=1, bias=True)\n\n            # Lightweight Transformer encoder\n            nhead = max(1, min(nhead, d_model))\n            # ensure divisibility\n            if d_model % nhead != 0:\n                # choose the largest head count that divides d_model\n                for h in reversed(range(1, nhead + 1)):\n                    if d_model % h == 0:\n                        nhead = h\n                        break\n            enc_layer = nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=nhead,\n                dim_feedforward=max(4, int(d_model * mlp_ratio)),\n                dropout=dropout,\n                activation='gelu',\n                batch_first=True,\n                norm_first=True,\n            )\n            self.transformer = (\n                nn.Identity() if num_layers <= 0 else nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n            )\n            self.pos = PositionalEncoding(d_model)\n            self.drop = nn.Dropout(dropout)\n\n            # Classification head\n            self.head = nn.Linear(d_model, num_classes)\n\n        def forward(self, x):\n            # x: (B, 2, 1000) or (B, 1000, 2)\n            if x.shape[-1] == 2:\n                x = x.permute(0, 2, 1).contiguous()\n            # CNN + channel attention pyramid\n            x = self.stem(x)\n            x = self.cnn(x)\n            x = self.proj(x)  # (B, D, T')\n            x = x.transpose(1, 2)  # (B, T', D)\n            x = self.pos(x)\n            x = self.transformer(x)\n            x = x.mean(dim=1)\n            x = self.drop(x)\n            logits = self.head(x)\n            return logits\n\n    # Enforce model size budget: ensure FP32 params < 256KB (very conservative)\n    # If not, shrink d_model/base_channels automatically.\n    def estimate_bytes_fp32(m):\n        return sum(p.numel() for p in m.parameters()) * 4\n\n    def build_model_with_budget():\n        nonlocal base_channels, d_model\n        # Clamp hyperparams to safe bounds\n        base_channels = int(max(8, min(32, base_channels)))\n        d_model = int(max(16, min(32, d_model)))\n        model = CATNetTiny(\n            base_channels=base_channels,\n            depth=int(max(2, min(3, cnn_depth))),\n            k=int(max(3, min(9, kernel_size))),\n            pool_stride=int(max(2, min(5, pool_stride))),\n            d_model=d_model,\n            nhead=int(max(1, min(4, nhead))),\n            num_layers=int(max(0, min(2, num_transformer_layers))),\n            mlp_ratio=int(max(2, min(4, mlp_ratio))),\n            dropout=float(max(0.0, min(0.5, dropout))),\n            num_classes=num_classes,\n        )\n        # If over budget in FP32, iteratively shrink\n        budget = 256 * 1024\n        while estimate_bytes_fp32(model) > budget and (d_model > 16 or base_channels > 8):\n            if d_model > 16:\n                d_model = max(16, d_model // 2)\n            elif base_channels > 8:\n                base_channels = max(8, base_channels // 2)\n            model = CATNetTiny(\n                base_channels=base_channels,\n                depth=int(max(2, min(3, cnn_depth))),\n                k=int(max(3, min(9, kernel_size))),\n                pool_stride=int(max(2, min(5, pool_stride))),\n                d_model=d_model,\n                nhead=int(max(1, min(4, nhead))),\n                num_layers=int(max(0, min(2, num_transformer_layers))),\n                mlp_ratio=int(max(2, min(4, mlp_ratio))),\n                dropout=float(max(0.0, min(0.5, dropout))),\n                num_classes=num_classes,\n            )\n        return model\n\n    model = build_model_with_budget().to(device)\n\n    # Log parameter count\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f'Model params: {total_params} (~{total_params*4/1024:.1f} KB in FP32)')\n\n    # Focal loss with class weights (move weights to compute device)\n    class_weights = class_weights.to(device)\n    def focal_loss(logits, targets):\n        # Cross-entropy per-sample\n        ce = F.cross_entropy(logits, targets, weight=class_weights, reduction='none', label_smoothing=float(label_smoothing))\n        if focal_gamma <= 0:\n            return ce.mean()\n        with torch.no_grad():\n            pt = F.softmax(logits, dim=1).gather(1, targets.view(-1, 1)).squeeze(1)\n        alpha = float(max(0.0, min(1.0, focal_alpha))) if focal_alpha is not None else 1.0\n        mod = (1.0 - pt).clamp(min=1e-6) ** float(focal_gamma)\n        loss = alpha * mod * ce\n        return loss.mean()\n\n    # Optimizer & scheduler\n    opt = AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    if scheduler == 'step':\n        sched = StepLR(opt, step_size=int(max(1, step_size)), gamma=float(gamma))\n    elif scheduler == 'cosine':\n        sched = CosineAnnealingLR(opt, T_max=int(max(1, epochs)))\n    else:\n        sched = None\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    def run_eval():\n        model.eval()\n        total, correct = 0, 0\n        loss_sum = 0.0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = ensure_ch_first(xb).to(device)\n                yb = yb.to(device)\n                logits = model(xb)\n                loss = F.cross_entropy(logits, yb, weight=class_weights, reduction='mean', label_smoothing=float(label_smoothing))\n                loss_sum += float(loss.item()) * yb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += yb.size(0)\n        return loss_sum / max(1, total), (correct / max(1, total))\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            # Augment: small Gaussian noise on input (train only)\n            if augment_noise_std and augment_noise_std > 0:\n                noise = torch.randn_like(xb) * float(augment_noise_std)\n                xb = xb + noise\n            xb = ensure_ch_first(xb).to(device)\n            yb = yb.to(device)\n\n            opt.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = focal_loss(logits, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip))\n            opt.step()\n            running += float(loss.item()) * yb.size(0)\n            seen += yb.size(0)\n        if sched is not None:\n            sched.step()\n\n        train_loss = running / max(1, seen)\n        val_loss, val_acc = run_eval()\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        print(f'Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}')\n\n    # Post-training quantization (dynamic). Quantized model is returned on CPU.\n    model.eval()\n    model_cpu = copy.deepcopy(model).to('cpu')\n\n    # Decide quantization approach\n    def apply_quantization(m):\n        # If user explicitly disables both, return as-is\n        if (not quantize_weights) and (not quantize_activations):\n            return m\n        bits = int(quantization_bits)\n        if bits >= 32:\n            return m  # no quantization\n        # Dynamic quantization on Linear layers only. This reduces model size and keeps ops portable.\n        if bits == 8:\n            qdtype = torch.qint8\n        elif bits == 16:\n            qdtype = torch.float16\n        else:\n            qdtype = torch.qint8\n        # Note: dynamic quantization mainly affects weights (and activation path for int8). We map both toggles to dynamic quantization behavior.\n        qm = quantize_dynamic(m, {nn.Linear}, dtype=qdtype)\n        return qm\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Estimate and enforce final storage size <= 256KB via state_dict serialization\n    try:\n        buf = io.BytesIO()\n        torch.save(quantized_model.state_dict(), buf)\n        q_size = len(buf.getvalue())\n    except Exception:\n        # Fallback: conservative estimate using FP32 params if state_dict not available\n        q_size = sum(p.numel() for p in model_cpu.parameters()) * 4\n    print(f'Quantized model state_dict size: {q_size/1024:.1f} KB')\n    if q_size > 256 * 1024:\n        print('WARNING: Quantized model exceeds 256KB. Consider reducing base_channels/d_model/num_layers.')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'param_count': total_params,\n        'fp32_param_kb': total_params * 4 / 1024.0,\n        'quantized_state_dict_bytes': int(q_size),\n    }\n\n    return quantized_model, metrics\n"}
2025-09-22 18:54:16,290 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-22 18:54:16,290 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train,\n    y_train,\n    X_val,\n    y_val,\n    device,\n    # Optimizer & schedule\n    lr=1e-3,\n    batch_size=64,\n    epochs=20,\n    weight_decay=1e-4,\n    scheduler='none',\n    step_size=10,\n    gamma=0.5,\n    # Model size & architecture\n    base_channels=16,\n    cnn_depth=3,\n    kernel_size=5,\n    pool_stride=4,\n    dropout=0.1,\n    d_model=32,\n    nhead=2,\n    num_transformer_layers=1,\n    mlp_ratio=2,\n    # Loss & regularization\n    focal_gamma=2.0,\n    focal_alpha=0.25,\n    label_smoothing=0.0,\n    grad_clip=1.0,\n    augment_noise_std=0.001,\n    # Quantization options\n    quantization_bits=8,\n    quantize_weights=True,\n    quantize_activations=True,\n    # Misc\n    seed=42,\n):\n    import math\n    import io\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n    from torch.optim import AdamW\n    from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n    from torch.ao.quantization import quantize_dynamic\n\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    num_classes = 5\n\n    # Ensure tensor dtypes and force CPU for datasets/samplers to avoid device mismatch\n    X_train = X_train.float().detach().cpu().contiguous()\n    X_val = X_val.float().detach().cpu().contiguous()\n    y_train = y_train.long().detach().cpu().contiguous()\n    y_val = y_val.long().detach().cpu().contiguous()\n\n    # Shape normalization helper: expected (B, C=2, T=1000)\n    def ensure_ch_first(x):\n        if x.dim() != 3:\n            raise ValueError(f'Expected 3D tensor (N, T, C) or (N, C, T), got {list(x.shape)}')\n        # If last dim is 2, assume (N, T, C)\n        if x.shape[-1] == 2:\n            return x.permute(0, 2, 1).contiguous()\n        elif x.shape[1] == 2:\n            return x\n        else:\n            raise ValueError('Input must have 2 channels (leads) either as last or second dimension')\n\n    # Dataset & Sampler with class balancing (all CPU)\n    with torch.no_grad():\n        # Compute class weights and sample weights on CPU to avoid device mismatches\n        classes, counts = torch.unique(y_train, return_counts=True)\n        freq = torch.zeros(num_classes, dtype=torch.float, device='cpu')\n        freq[classes.cpu()] = counts.float().cpu()\n        freq = torch.clamp(freq, min=1.0)\n        class_weights = (1.0 / freq)\n        class_weights = class_weights * (num_classes / class_weights.sum())\n        sample_weights = class_weights[y_train]\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_sampler = WeightedRandomSampler(weights=sample_weights.double().cpu(), num_samples=len(sample_weights), replacement=True)\n\n    # IMPORTANT: pin_memory=False per requirement\n    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=train_sampler, shuffle=False, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    # Model components\n    class SEChannelAttention(nn.Module):\n        def __init__(self, channels, reduction=8):\n            super().__init__()\n            hidden = max(1, channels // reduction)\n            self.avg = nn.AdaptiveAvgPool1d(1)\n            self.fc1 = nn.Linear(channels, hidden, bias=True)\n            self.fc2 = nn.Linear(hidden, channels, bias=True)\n            self.act = nn.SiLU()\n            self.gate = nn.Sigmoid()\n        def forward(self, x):\n            # x: (B, C, T)\n            b, c, _ = x.shape\n            s = self.avg(x).view(b, c)\n            s = self.fc2(self.act(self.fc1(s)))\n            s = self.gate(s).view(b, c, 1)\n            return x * s\n\n    class ResidualBlock1D(nn.Module):\n        def __init__(self, c_in, c_out, k=5, se_reduction=8):\n            super().__init__()\n            p = k // 2\n            self.conv1 = nn.Conv1d(c_in, c_out, k, padding=p, bias=False)\n            self.bn1 = nn.BatchNorm1d(c_out)\n            self.conv2 = nn.Conv1d(c_out, c_out, k, padding=p, bias=False)\n            self.bn2 = nn.BatchNorm1d(c_out)\n            self.act = nn.SiLU()\n            self.se = SEChannelAttention(c_out, reduction=se_reduction)\n            self.down = None\n            if c_in != c_out:\n                self.down = nn.Conv1d(c_in, c_out, kernel_size=1, bias=False)\n        def forward(self, x):\n            identity = x\n            out = self.act(self.bn1(self.conv1(x)))\n            out = self.bn2(self.conv2(out))\n            out = self.se(out)\n            if self.down is not None:\n                identity = self.down(identity)\n            out = self.act(out + identity)\n            return out\n\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model, max_len=2048):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            self.register_buffer('pe', pe.unsqueeze(0), persistent=False)  # (1, max_len, d_model)\n        def forward(self, x):\n            # x: (B, T, D)\n            t = x.size(1)\n            return x + self.pe[:, :t, :]\n\n    class CATNetTiny(nn.Module):\n        def __init__(self, base_channels=16, depth=3, k=5, pool_stride=4, d_model=32, nhead=2, num_layers=1, mlp_ratio=2, dropout=0.1, num_classes=5):\n            super().__init__()\n            c1 = base_channels\n            c2 = max(base_channels, min(48, (3 * base_channels) // 2))\n            c3 = min(64, base_channels * 2)\n\n            # Stem + pooling to shrink sequence length quickly\n            p = k // 2\n            self.stem = nn.Sequential(\n                nn.Conv1d(2, c1, kernel_size=k, padding=p, bias=False),\n                nn.BatchNorm1d(c1),\n                nn.SiLU(),\n                nn.MaxPool1d(kernel_size=pool_stride, stride=pool_stride)\n            )\n\n            blocks = []\n            in_c = c1\n            for i in range(depth):\n                out_c = [c1, c2, c3][min(i, 2)]\n                blocks.append(ResidualBlock1D(in_c, out_c, k=k))\n                blocks.append(nn.MaxPool1d(kernel_size=pool_stride, stride=pool_stride))\n                in_c = out_c\n            self.cnn = nn.Sequential(*blocks)\n\n            # Project to transformer dimension\n            self.proj = nn.Conv1d(in_c, d_model, kernel_size=1, bias=True)\n\n            # Lightweight Transformer encoder\n            nhead = max(1, min(nhead, d_model))\n            # ensure divisibility\n            if d_model % nhead != 0:\n                # choose the largest head count that divides d_model\n                for h in reversed(range(1, nhead + 1)):\n                    if d_model % h == 0:\n                        nhead = h\n                        break\n            enc_layer = nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=nhead,\n                dim_feedforward=max(4, int(d_model * mlp_ratio)),\n                dropout=dropout,\n                activation='gelu',\n                batch_first=True,\n                norm_first=True,\n            )\n            self.transformer = (\n                nn.Identity() if num_layers <= 0 else nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n            )\n            self.pos = PositionalEncoding(d_model)\n            self.drop = nn.Dropout(dropout)\n\n            # Classification head\n            self.head = nn.Linear(d_model, num_classes)\n\n        def forward(self, x):\n            # x: (B, 2, 1000) or (B, 1000, 2)\n            if x.shape[-1] == 2:\n                x = x.permute(0, 2, 1).contiguous()\n            # CNN + channel attention pyramid\n            x = self.stem(x)\n            x = self.cnn(x)\n            x = self.proj(x)  # (B, D, T')\n            x = x.transpose(1, 2)  # (B, T', D)\n            x = self.pos(x)\n            x = self.transformer(x)\n            x = x.mean(dim=1)\n            x = self.drop(x)\n            logits = self.head(x)\n            return logits\n\n    # Enforce model size budget: ensure FP32 params < 256KB (very conservative)\n    # If not, shrink d_model/base_channels automatically.\n    def estimate_bytes_fp32(m):\n        return sum(p.numel() for p in m.parameters()) * 4\n\n    def build_model_with_budget():\n        nonlocal base_channels, d_model\n        # Clamp hyperparams to safe bounds\n        base_channels = int(max(8, min(32, base_channels)))\n        d_model = int(max(16, min(32, d_model)))\n        model = CATNetTiny(\n            base_channels=base_channels,\n            depth=int(max(2, min(3, cnn_depth))),\n            k=int(max(3, min(9, kernel_size))),\n            pool_stride=int(max(2, min(5, pool_stride))),\n            d_model=d_model,\n            nhead=int(max(1, min(4, nhead))),\n            num_layers=int(max(0, min(2, num_transformer_layers))),\n            mlp_ratio=int(max(2, min(4, mlp_ratio))),\n            dropout=float(max(0.0, min(0.5, dropout))),\n            num_classes=num_classes,\n        )\n        # If over budget in FP32, iteratively shrink\n        budget = 256 * 1024\n        while estimate_bytes_fp32(model) > budget and (d_model > 16 or base_channels > 8):\n            if d_model > 16:\n                d_model = max(16, d_model // 2)\n            elif base_channels > 8:\n                base_channels = max(8, base_channels // 2)\n            model = CATNetTiny(\n                base_channels=base_channels,\n                depth=int(max(2, min(3, cnn_depth))),\n                k=int(max(3, min(9, kernel_size))),\n                pool_stride=int(max(2, min(5, pool_stride))),\n                d_model=d_model,\n                nhead=int(max(1, min(4, nhead))),\n                num_layers=int(max(0, min(2, num_transformer_layers))),\n                mlp_ratio=int(max(2, min(4, mlp_ratio))),\n                dropout=float(max(0.0, min(0.5, dropout))),\n                num_classes=num_classes,\n            )\n        return model\n\n    model = build_model_with_budget().to(device)\n\n    # Log parameter count\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f'Model params: {total_params} (~{total_params*4/1024:.1f} KB in FP32)')\n\n    # Focal loss with class weights (move weights to compute device)\n    class_weights = class_weights.to(device)\n    def focal_loss(logits, targets):\n        # Cross-entropy per-sample\n        ce = F.cross_entropy(logits, targets, weight=class_weights, reduction='none', label_smoothing=float(label_smoothing))\n        if focal_gamma <= 0:\n            return ce.mean()\n        with torch.no_grad():\n            pt = F.softmax(logits, dim=1).gather(1, targets.view(-1, 1)).squeeze(1)\n        alpha = float(max(0.0, min(1.0, focal_alpha))) if focal_alpha is not None else 1.0\n        mod = (1.0 - pt).clamp(min=1e-6) ** float(focal_gamma)\n        loss = alpha * mod * ce\n        return loss.mean()\n\n    # Optimizer & scheduler\n    opt = AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    if scheduler == 'step':\n        sched = StepLR(opt, step_size=int(max(1, step_size)), gamma=float(gamma))\n    elif scheduler == 'cosine':\n        sched = CosineAnnealingLR(opt, T_max=int(max(1, epochs)))\n    else:\n        sched = None\n\n    # Training loop\n    train_losses, val_losses, val_accs = [], [], []\n\n    def run_eval():\n        model.eval()\n        total, correct = 0, 0\n        loss_sum = 0.0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = ensure_ch_first(xb).to(device)\n                yb = yb.to(device)\n                logits = model(xb)\n                loss = F.cross_entropy(logits, yb, weight=class_weights, reduction='mean', label_smoothing=float(label_smoothing))\n                loss_sum += float(loss.item()) * yb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += yb.size(0)\n        return loss_sum / max(1, total), (correct / max(1, total))\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            # Augment: small Gaussian noise on input (train only)\n            if augment_noise_std and augment_noise_std > 0:\n                noise = torch.randn_like(xb) * float(augment_noise_std)\n                xb = xb + noise\n            xb = ensure_ch_first(xb).to(device)\n            yb = yb.to(device)\n\n            opt.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = focal_loss(logits, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip))\n            opt.step()\n            running += float(loss.item()) * yb.size(0)\n            seen += yb.size(0)\n        if sched is not None:\n            sched.step()\n\n        train_loss = running / max(1, seen)\n        val_loss, val_acc = run_eval()\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        print(f'Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}')\n\n    # Post-training quantization (dynamic). Quantized model is returned on CPU.\n    model.eval()\n    model_cpu = copy.deepcopy(model).to('cpu')\n\n    # Decide quantization approach\n    def apply_quantization(m):\n        # If user explicitly disables both, return as-is\n        if (not quantize_weights) and (not quantize_activations):\n            return m\n        bits = int(quantization_bits)\n        if bits >= 32:\n            return m  # no quantization\n        # Dynamic quantization on Linear layers only. This reduces model size and keeps ops portable.\n        if bits == 8:\n            qdtype = torch.qint8\n        elif bits == 16:\n            qdtype = torch.float16\n        else:\n            qdtype = torch.qint8\n        # Note: dynamic quantization mainly affects weights (and activation path for int8). We map both toggles to dynamic quantization behavior.\n        qm = quantize_dynamic(m, {nn.Linear}, dtype=qdtype)\n        return qm\n\n    quantized_model = apply_quantization(model_cpu)\n\n    # Estimate and enforce final storage size <= 256KB via state_dict serialization\n    try:\n        buf = io.BytesIO()\n        torch.save(quantized_model.state_dict(), buf)\n        q_size = len(buf.getvalue())\n    except Exception:\n        # Fallback: conservative estimate using FP32 params if state_dict not available\n        q_size = sum(p.numel() for p in model_cpu.parameters()) * 4\n    print(f'Quantized model state_dict size: {q_size/1024:.1f} KB')\n    if q_size > 256 * 1024:\n        print('WARNING: Quantized model exceeds 256KB. Consider reducing base_channels/d_model/num_layers.')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'param_count': total_params,\n        'fp32_param_kb': total_params * 4 / 1024.0,\n        'quantized_state_dict_bytes': int(q_size),\n    }\n\n    return quantized_model, metrics\n"}
2025-09-22 18:54:16,290 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-22 18:54:16,290 - ERROR - _models.training_function_executor - BO training objective failed: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
2025-09-22 18:54:16,290 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 105.103s
2025-09-22 18:54:16,290 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
2025-09-22 18:54:16,290 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-22 18:54:19,293 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-22 18:54:19,293 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß GPT has fixed the training function, reloading from JSON file
2025-09-22 18:54:19,293 - INFO - evaluation.code_generation_pipeline_orchestrator - Reloading fixed training function from: generated_training_functions/training_function_torch_tensor_CAT-Net-Tiny-1D-DualLead_1758585151.json
2025-09-22 18:54:19,294 - INFO - _models.training_function_executor - Loaded training function: CAT-Net-Tiny-1D-DualLead
2025-09-22 18:54:19,294 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-22 18:54:19,294 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: CAT-Net-Tiny-1D-DualLead
2025-09-22 18:54:19,294 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-22 18:54:19,294 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 1/4
2025-09-22 18:54:19,294 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 1: üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 18:54:19,294 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 18:54:19,296 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-22 18:54:19,297 - INFO - package_installer - Available packages: {'torch'}
2025-09-22 18:54:19,297 - INFO - package_installer - Missing packages: set()
2025-09-22 18:54:19,297 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-22 18:54:19,297 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 18:54:19,297 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:54:19,297 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 18:54:19,297 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'scheduler', 'step_size', 'gamma', 'base_channels', 'cnn_depth', 'kernel_size', 'pool_stride', 'dropout', 'd_model', 'nhead', 'num_transformer_layers', 'mlp_ratio', 'focal_gamma', 'focal_alpha', 'label_smoothing', 'grad_clip', 'augment_noise_std', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'seed']
2025-09-22 18:54:19,297 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 18:54:19,297 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:54:19,297 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 18:54:19,331 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 18:54:19,364 - INFO - bo.run_bo - Converted GPT search space: 25 parameters
2025-09-22 18:54:19,364 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 18:54:19,365 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 18:54:19,366 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 18:54:19,366 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 18:54:19,366 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 18:54:19,366 - INFO - _models.training_function_executor - Executing training function: CAT-Net-Tiny-1D-DualLead
2025-09-22 18:54:19,366 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 32, 'epochs': 76, 'weight_decay': 9.846738873614564e-05, 'scheduler': 'none', 'step_size': 20, 'gamma': 0.1889776750780226, 'base_channels': 18, 'cnn_depth': 3, 'kernel_size': 7, 'pool_stride': 5, 'dropout': 0.0714334089609704, 'd_model': 18, 'nhead': 2, 'num_transformer_layers': 0, 'mlp_ratio': 3, 'focal_gamma': 3.6099938613341243, 'focal_alpha': 0.9385527090157504, 'label_smoothing': 7.787658410143285e-05, 'grad_clip': 4.961057796456088, 'augment_noise_std': 0.006174815096277166, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 500186}
2025-09-22 18:54:19,367 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 32, 'epochs': 76, 'weight_decay': 9.846738873614564e-05, 'scheduler': 'none', 'step_size': 20, 'gamma': 0.1889776750780226, 'base_channels': 18, 'cnn_depth': 3, 'kernel_size': 7, 'pool_stride': 5, 'dropout': 0.0714334089609704, 'd_model': 18, 'nhead': 2, 'num_transformer_layers': 0, 'mlp_ratio': 3, 'focal_gamma': 3.6099938613341243, 'focal_alpha': 0.9385527090157504, 'label_smoothing': 7.787658410143285e-05, 'grad_clip': 4.961057796456088, 'augment_noise_std': 0.006174815096277166, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'seed': 500186}
2025-09-22 18:54:19,368 - ERROR - _models.training_function_executor - Training execution failed: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
2025-09-22 18:54:19,368 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 18:54:19,368 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 18:54:19,368 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 18:54:19,368 - INFO - _models.ai_code_generator - Prompt length: 18442 characters
2025-09-22 18:54:19,368 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 18:54:19,368 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 18:54:19,368 - INFO - _models.ai_code_generator - Model parameter: gpt-5
