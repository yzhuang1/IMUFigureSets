2025-09-20 18:16:09,246 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 18:16:09,593 - INFO - __main__ - Logging system initialized successfully
2025-09-20 18:16:09,593 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-20 18:16:09,594 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-20 18:16:09,595 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-20 18:16:09,595 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-20 18:16:09,596 - INFO - __main__ - Attempting to load: X.npy
2025-09-20 18:16:09,725 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-20 18:16:09,812 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-20 18:16:09,813 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-20 18:16:09,813 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-20 18:16:09,813 - INFO - __main__ - Flow: Code Generation â†’ BO â†’ Evaluation
2025-09-20 18:16:09,815 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 18:16:09,816 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-20 18:16:09,816 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-20 18:16:09,816 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-20 18:16:09,816 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation â†’ JSON Storage â†’ BO â†’ Training Execution â†’ Evaluation
2025-09-20 18:16:09,816 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-20 18:16:09,816 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-20 18:16:09,816 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-20 18:16:09,816 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-20 18:16:10,024 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-20 18:16:10,025 - INFO - class_balancing - Class imbalance analysis:
2025-09-20 18:16:10,025 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-20 18:16:10,025 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-20 18:16:10,025 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-20 18:16:10,025 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-20 18:16:10,025 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-20 18:16:10,025 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-20 18:16:10,025 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-20 18:16:10,025 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-20 18:16:10,710 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-20 18:16:10,719 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-20 18:16:10,719 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-20 18:16:10,719 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-20 18:16:10,720 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-20 18:16:10,720 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ¤– STEP 1: AI Training Code Generation
2025-09-20 18:16:10,720 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-20 18:16:10,720 - INFO - _models.ai_code_generator - Prompt length: 2552 characters
2025-09-20 18:16:10,720 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-20 18:16:10,720 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-20 18:16:10,720 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-20 18:17:28,220 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-20 18:17:28,307 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-20 18:17:28,307 - INFO - _models.ai_code_generator - AI generated training function: TinyECG1DCNN-PTQ
2025-09-20 18:17:28,307 - INFO - _models.ai_code_generator - Confidence: 0.90
2025-09-20 18:17:28,307 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: TinyECG1DCNN-PTQ
2025-09-20 18:17:28,307 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'ch1', 'ch2', 'ch3', 'calib_steps', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-20 18:17:28,308 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.90
2025-09-20 18:17:28,310 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ’¾ STEP 2: Save Training Function to JSON
2025-09-20 18:17:28,313 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions\training_function_torch_tensor_TinyECG1DCNN-PTQ_1758410248.json
2025-09-20 18:17:28,313 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions\training_function_torch_tensor_TinyECG1DCNN-PTQ_1758410248.json
2025-09-20 18:17:28,313 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ” STEP 3: Bayesian Optimization
2025-09-20 18:17:28,313 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: TinyECG1DCNN-PTQ
2025-09-20 18:17:28,313 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“¦ Installing dependencies for GPT-generated training code...
2025-09-20 18:17:28,314 - INFO - package_installer - ðŸ” Analyzing GPT-generated code for package dependencies...
2025-09-20 18:17:28,317 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-20 18:17:28,317 - INFO - package_installer - Available packages: {'torch'}
2025-09-20 18:17:28,317 - INFO - package_installer - Missing packages: set()
2025-09-20 18:17:28,317 - INFO - package_installer - âœ… All required packages are already available
2025-09-20 18:17:28,317 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-09-20 18:17:28,397 - INFO - data_splitting - Created BO subset: 5000 samples
2025-09-20 18:17:28,397 - INFO - data_splitting - BO subset class distribution: [3600  766   96  100  438]
2025-09-20 18:17:28,397 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 5000 samples (using bo_sample_num=5000)
2025-09-20 18:17:28,397 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'ch1', 'ch2', 'ch3', 'calib_steps', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-20 18:17:28,398 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 18:17:28,476 - INFO - data_splitting - Created BO subset: 5000 samples
2025-09-20 18:17:28,476 - INFO - data_splitting - BO subset class distribution: [3600  766   96  100  438]
2025-09-20 18:17:28,476 - INFO - _models.training_function_executor - Using BO subset for optimization: 5000 samples (bo_sample_num=5000)
2025-09-20 18:17:28,486 - INFO - _models.training_function_executor - BO splits - Train: 4000, Val: 1000
2025-09-20 18:17:28,755 - INFO - bo.run_bo - Converted GPT search space: 12 parameters
2025-09-20 18:17:28,755 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-20 18:17:28,756 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-20 18:17:28,756 - INFO - bo.run_bo - ðŸ”BO Trial 1: Initial random exploration
2025-09-20 18:17:28,756 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-20 18:17:28,758 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 18:17:28,758 - INFO - _models.training_function_executor - Executing training function: TinyECG1DCNN-PTQ
2025-09-20 18:17:28,758 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': 12, 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': 34, 'ch2': 118, 'ch3': 138, 'calib_steps': 97, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 18:17:28,759 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': 12, 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': 34, 'ch2': 118, 'ch3': 138, 'calib_steps': 97, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 18:17:33,782 - INFO - _models.training_function_executor - Model parameter count: 102,651
2025-09-20 18:17:33,782 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.99504070520401, 0.7714318437576294, 0.5771616444587707, 0.4525042262077332, 0.38535184717178345, 0.3436557300686836, 0.3084873006343842, 0.2853315556049347, 0.27203808426856996, 0.256098799854517, 0.2479393961429596, 0.22845449624955655], 'val_losses': [0.865678256034851, 0.6276135969161988, 0.46751310348510744, 0.38406139850616455, 0.3409610514640808, 0.2897304029464722, 0.26570008516311644, 0.24500588607788085, 0.2302459490299225, 0.22794826436042787, 0.21352231574058533, 0.20821758127212525], 'val_acc': [0.716, 0.799, 0.842, 0.891, 0.909, 0.922, 0.935, 0.931, 0.931, 0.937, 0.94, 0.946], 'param_count': 102651, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'model_name': 'TinyECG1DCNN-PTQ', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': 12, 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': 34, 'ch2': 118, 'ch3': 138, 'calib_steps': 97, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 102651, 'model_size_validation': 'PASS'}
2025-09-20 18:17:33,782 - INFO - _models.training_function_executor - BO Objective: base=0.9460, size_penalty=0.0000, final=0.9460
2025-09-20 18:17:33,782 - INFO - _models.training_function_executor - Model size: 102,651 parameters (PASS 256K limit)
2025-09-20 18:17:33,782 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.025s
2025-09-20 18:17:33,782 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9460
2025-09-20 18:17:33,782 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 18:17:33,782 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': np.int64(34), 'ch2': np.int64(118), 'ch3': np.int64(138), 'calib_steps': np.int64(97), 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, value=0.9460
2025-09-20 18:17:33,782 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': np.int64(34), 'ch2': np.int64(118), 'ch3': np.int64(138), 'calib_steps': np.int64(97), 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False} -> 0.9460
2025-09-20 18:17:33,783 - INFO - bo.run_bo - ðŸ”BO Trial 2: Initial random exploration
2025-09-20 18:17:33,783 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 18:17:33,783 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 18:17:33,783 - INFO - _models.training_function_executor - Executing training function: TinyECG1DCNN-PTQ
2025-09-20 18:17:33,783 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.6813042706040666e-06, 'batch_size': 128, 'epochs': 34, 'weight_decay': 7.068974950624601e-07, 'dropout': 0.09091248360355032, 'ch1': 36, 'ch2': 64, 'ch3': 121, 'calib_steps': 31, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 18:17:33,784 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.6813042706040666e-06, 'batch_size': 128, 'epochs': 34, 'weight_decay': 7.068974950624601e-07, 'dropout': 0.09091248360355032, 'ch1': 36, 'ch2': 64, 'ch3': 121, 'calib_steps': 31, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 18:17:37,250 - INFO - _models.training_function_executor - Model parameter count: 50,744
2025-09-20 18:17:37,250 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5596209211349488, 1.5562909545898438, 1.5529438743591308, 1.5493905487060546, 1.545927646636963, 1.5430437326431274, 1.5395120248794556, 1.5358576412200928, 1.5324677667617799, 1.5287747449874878, 1.525182318687439, 1.5215123739242553, 1.5177580118179321, 1.5136658115386963, 1.509772225379944, 1.50608873462677, 1.5021022787094116, 1.4980136976242064, 1.4938958396911621, 1.489330488204956, 1.485243327140808, 1.480731348991394, 1.476299451828003, 1.4716922340393066, 1.4664910545349121, 1.4615957374572754, 1.4567687177658082, 1.4519376039505005, 1.4468758192062379, 1.4413527631759644, 1.435476448059082, 1.4300771188735961, 1.4238143424987793, 1.4185600862503052], 'val_losses': [1.5579495239257812, 1.5545992984771728, 1.5512197408676147, 1.5478108625411988, 1.544369013786316, 1.5408623027801513, 1.537348093032837, 1.5337872467041016, 1.5301748104095458, 1.5264996309280396, 1.5227953157424927, 1.5189971895217895, 1.515155047416687, 1.5112901611328124, 1.5072612400054932, 1.503230438232422, 1.4991394844055175, 1.4949414567947388, 1.4906517992019652, 1.4862350912094116, 1.4817646493911742, 1.4772051105499266, 1.4725156574249267, 1.4676591606140137, 1.4627896614074707, 1.4577730054855347, 1.4526592864990235, 1.4474323577880859, 1.4420848693847657, 1.4366357688903808, 1.4309261226654053, 1.4251192150115968, 1.4191926536560058, 1.4131137447357178], 'val_acc': [0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72], 'param_count': 51354, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'model_name': 'TinyECG1DCNN-PTQ', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.6813042706040666e-06, 'batch_size': 128, 'epochs': 34, 'weight_decay': 7.068974950624601e-07, 'dropout': 0.09091248360355032, 'ch1': 36, 'ch2': 64, 'ch3': 121, 'calib_steps': 31, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 50744, 'model_size_validation': 'PASS'}
2025-09-20 18:17:37,250 - INFO - _models.training_function_executor - BO Objective: base=0.7200, size_penalty=0.0000, final=0.7200
2025-09-20 18:17:37,252 - INFO - _models.training_function_executor - Model size: 50,744 parameters (PASS 256K limit)
2025-09-20 18:17:37,252 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 3.468s
2025-09-20 18:17:37,252 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7200
2025-09-20 18:17:37,252 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 18:17:37,252 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 1.6813042706040666e-06, 'batch_size': 128, 'epochs': np.int64(34), 'weight_decay': 7.068974950624601e-07, 'dropout': 0.09091248360355032, 'ch1': np.int64(36), 'ch2': np.int64(64), 'ch3': np.int64(121), 'calib_steps': np.int64(31), 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, value=0.7200
2025-09-20 18:17:37,252 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 1.6813042706040666e-06, 'batch_size': 128, 'epochs': np.int64(34), 'weight_decay': 7.068974950624601e-07, 'dropout': 0.09091248360355032, 'ch1': np.int64(36), 'ch2': np.int64(64), 'ch3': np.int64(121), 'calib_steps': np.int64(31), 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False} -> 0.7200
2025-09-20 18:17:37,253 - INFO - bo.run_bo - ðŸ”BO Trial 3: Initial random exploration
2025-09-20 18:17:37,253 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 18:17:37,253 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 18:17:37,253 - INFO - _models.training_function_executor - Executing training function: TinyECG1DCNN-PTQ
2025-09-20 18:17:37,253 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.975977214318103e-05, 'batch_size': 32, 'epochs': 20, 'weight_decay': 8.532678095658718e-07, 'dropout': 0.04530321726641041, 'ch1': 59, 'ch2': 86, 'ch3': 127, 'calib_steps': 140, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:17:37,255 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.975977214318103e-05, 'batch_size': 32, 'epochs': 20, 'weight_decay': 8.532678095658718e-07, 'dropout': 0.04530321726641041, 'ch1': 59, 'ch2': 86, 'ch3': 127, 'calib_steps': 140, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 18:17:42,866 - INFO - _models.training_function_executor - Model parameter count: 81,446
2025-09-20 18:17:42,866 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.429707869052887, 1.0558237290382386, 1.0205448603630065, 1.0112912344932556, 1.0053970112800599, 1.0004486932754517, 0.9965399703979492, 0.9929740624427795, 0.98721701669693, 0.9821400151252747, 0.978177107334137, 0.9715216264724732, 0.9676917352676392, 0.9578422021865844, 0.951861508846283, 0.9451114287376404, 0.9382069082260132, 0.9301651020050049, 0.9224630975723267, 0.9134187393188477], 'val_losses': [1.1041661396026612, 1.0252197427749634, 1.00897935962677, 1.0020555238723754, 0.9977588853836059, 0.9927375202178955, 0.9883332948684692, 0.9830004758834839, 0.9783710565567016, 0.9727219972610474, 0.9675210771560669, 0.9626901597976685, 0.9566527013778686, 0.9488450536727905, 0.9424257879257202, 0.935414098739624, 0.9292205276489258, 0.9195983362197876, 0.9108066186904907, 0.9049680852890014], 'val_acc': [0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.715, 0.719, 0.717], 'param_count': 81446, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'model_name': 'TinyECG1DCNN-PTQ', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.975977214318103e-05, 'batch_size': 32, 'epochs': 20, 'weight_decay': 8.532678095658718e-07, 'dropout': 0.04530321726641041, 'ch1': 59, 'ch2': 86, 'ch3': 127, 'calib_steps': 140, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 81446, 'model_size_validation': 'PASS'}
2025-09-20 18:17:42,866 - INFO - _models.training_function_executor - BO Objective: base=0.7170, size_penalty=0.0000, final=0.7170
2025-09-20 18:17:42,866 - INFO - _models.training_function_executor - Model size: 81,446 parameters (PASS 256K limit)
2025-09-20 18:17:42,866 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 5.614s
2025-09-20 18:17:42,989 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7170
2025-09-20 18:17:42,989 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.123s
2025-09-20 18:17:42,989 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 3.975977214318103e-05, 'batch_size': 32, 'epochs': np.int64(20), 'weight_decay': 8.532678095658718e-07, 'dropout': 0.04530321726641041, 'ch1': np.int64(59), 'ch2': np.int64(86), 'ch3': np.int64(127), 'calib_steps': np.int64(140), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, value=0.7170
2025-09-20 18:17:42,989 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 3.975977214318103e-05, 'batch_size': 32, 'epochs': np.int64(20), 'weight_decay': 8.532678095658718e-07, 'dropout': 0.04530321726641041, 'ch1': np.int64(59), 'ch2': np.int64(86), 'ch3': np.int64(127), 'calib_steps': np.int64(140), 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True} -> 0.7170
2025-09-20 18:17:42,989 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.9460
2025-09-20 18:17:42,989 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': np.int64(34), 'ch2': np.int64(118), 'ch3': np.int64(138), 'calib_steps': np.int64(97), 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 18:17:42,991 - INFO - visualization - Generating BO visualization charts with 3 trials...
2025-09-20 18:17:44,766 - INFO - visualization - BO summary saved to: charts\BO_TinyECG1DCNN-PTQ_20250920_181742\bo_summary.txt
2025-09-20 18:17:44,766 - INFO - visualization - BO charts saved to: charts\BO_TinyECG1DCNN-PTQ_20250920_181742
2025-09-20 18:17:44,766 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“Š BO charts saved to: charts\BO_TinyECG1DCNN-PTQ_20250920_181742
2025-09-20 18:17:44,787 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸš€ STEP 4: Final Training Execution
2025-09-20 18:17:44,787 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-20 18:17:44,975 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 18:17:44,986 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 18:17:44,999 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 18:17:45,009 - INFO - _models.training_function_executor - Loaded training function: TinyECG1DCNN-PTQ
2025-09-20 18:17:45,009 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-20 18:17:45,009 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': np.int64(34), 'ch2': np.int64(118), 'ch3': np.int64(138), 'calib_steps': np.int64(97), 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 18:17:45,009 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 18:17:45,065 - INFO - _models.training_function_executor - Executing training function: TinyECG1DCNN-PTQ
2025-09-20 18:17:45,065 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': np.int64(34), 'ch2': np.int64(118), 'ch3': np.int64(138), 'calib_steps': np.int64(97), 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 18:17:45,067 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': 12, 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': 34, 'ch2': 118, 'ch3': 138, 'calib_steps': 97, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 18:18:17,780 - INFO - _models.training_function_executor - Model parameter count: 102,651
2025-09-20 18:18:17,780 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4599761414762105, 0.22087661778508016, 0.16812431007480277, 0.14312053917299988, 0.12949955072358502, 0.11779654427575666, 0.11201723651742412, 0.1053093815068438, 0.09788996834418442, 0.09463161881658329, 0.09052679249635914, 0.08723630138335685], 'val_losses': [0.2800799842957111, 0.1790768950841548, 0.14845318155742376, 0.13852165197533334, 0.11323236699815449, 0.10747442783714475, 0.10973710299002702, 0.09978246947339163, 0.09465750769649582, 0.0963490673975523, 0.09585396008758504, 0.09492073687234232], 'val_acc': [0.9306404730881026, 0.9529918813270523, 0.9610103237446126, 0.9600080184424176, 0.9701313019945875, 0.9667234639671244, 0.9664227723764659, 0.970431993585246, 0.9717349904780996, 0.969028766162173, 0.96963014934349, 0.9680264608599779], 'param_count': 102651, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'model_name': 'TinyECG1DCNN-PTQ', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': 12, 'weight_decay': 2.4810409748678097e-05, 'dropout': 0.07800932022121827, 'ch1': 34, 'ch2': 118, 'ch3': 138, 'calib_steps': 97, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 102651, 'model_size_validation': 'PASS'}
2025-09-20 18:18:17,780 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final validation metrics from training (avoids preprocessing mismatch)
2025-09-20 18:18:17,780 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 0.9680264608599779, 'macro_f1': None}
2025-09-20 18:18:17,780 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“Š STEP 5: Performance Analysis
2025-09-20 18:18:17,780 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: TinyECG1DCNN-PTQ
2025-09-20 18:18:17,780 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.9460
2025-09-20 18:18:17,780 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: None
2025-09-20 18:18:17,780 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-09-20 18:18:17,780 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: TinyECG1DCNN-PTQ
2025-09-20 18:18:17,781 - ERROR - __main__ - Unhandled exception: TypeError: unsupported format string passed to NoneType.__format__
Traceback (most recent call last):
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 484, in <module>
    processed_real_data = process_real_data()
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 342, in process_real_data
    result = process_data_with_ai_enhanced_evaluation(
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 136, in process_data_with_ai_enhanced_evaluation
    result = train_with_iterative_selection(data, labels, device=device, **kwargs)
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 85, in train_with_iterative_selection
    best_model, pipeline_results = orchestrator.run_complete_pipeline(
  File "D:\_A\GPT_research\ml_pipeline\evaluation\code_generation_pipeline_orchestrator.py", line 130, in run_complete_pipeline
    logger.info(f"Score: {performance_score:.4f}")
TypeError: unsupported format string passed to NoneType.__format__

