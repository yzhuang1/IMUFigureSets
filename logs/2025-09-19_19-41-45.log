2025-09-19 19:41:47,470 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-19 19:41:47,841 - INFO - __main__ - Logging system initialized successfully
2025-09-19 19:41:47,841 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-19 19:41:47,842 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-19 19:41:47,843 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-19 19:41:47,843 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-19 19:41:47,844 - INFO - __main__ - Attempting to load: X.npy
2025-09-19 19:41:47,975 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-19 19:41:48,063 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-19 19:41:48,064 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-19 19:41:48,064 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-19 19:41:48,064 - INFO - __main__ - Flow: Code Generation â†’ BO â†’ Evaluation
2025-09-19 19:41:48,066 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-19 19:41:48,066 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-19 19:41:48,066 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-19 19:41:48,066 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-19 19:41:48,066 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation â†’ JSON Storage â†’ BO â†’ Training Execution â†’ Evaluation
2025-09-19 19:41:48,066 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-19 19:41:48,066 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-19 19:41:48,066 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-19 19:41:48,067 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-19 19:41:48,265 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-19 19:41:48,265 - INFO - class_balancing - Class imbalance analysis:
2025-09-19 19:41:48,265 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-19 19:41:48,265 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-19 19:41:48,265 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-19 19:41:48,265 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-19 19:41:48,265 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-19 19:41:48,265 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-19 19:41:48,265 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-19 19:41:48,265 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-19 19:41:48,969 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-19 19:41:48,977 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-19 19:41:48,977 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-19 19:41:48,977 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-19 19:41:48,977 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-19 19:41:48,977 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ¤– STEP 1: AI Training Code Generation
2025-09-19 19:41:48,977 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-19 19:44:01,893 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-19 19:44:02,011 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-19 19:44:02,012 - INFO - _models.ai_code_generator - Prompt length: 3040 characters
2025-09-19 19:44:02,012 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-19 19:44:02,012 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-19 19:44:02,012 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-19 19:46:34,763 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-19 19:46:34,774 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-19 19:46:34,774 - WARNING - _models.ai_code_generator - Initial JSON parse failed: Expecting ',' delimiter: line 37 column 16 (char 17546), attempting to fix common issues
2025-09-19 19:46:34,775 - INFO - _models.ai_code_generator - Successfully fixed JSON formatting issues
2025-09-19 19:46:34,775 - INFO - _models.ai_code_generator - AI generated training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:34,775 - INFO - _models.ai_code_generator - Confidence: 0.90
2025-09-19 19:46:34,775 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-09-19 19:46:34,775 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:34,775 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'd_model', 'num_heads', 'num_transformer_layers', 'ff_mult', 'dropout', 'stem_channels_per_branch', 'stride', 'weight_decay', 'loss_type', 'label_smoothing', 'class_weighting', 'grad_clip_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibration_batches']
2025-09-19 19:46:34,775 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.90
2025-09-19 19:46:34,777 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ’¾ STEP 2: Save Training Function to JSON
2025-09-19 19:46:34,781 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions\training_function_torch_tensor_TinyCATNet-1D-CNN+Transformer_1758329194.json
2025-09-19 19:46:34,781 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions\training_function_torch_tensor_TinyCATNet-1D-CNN+Transformer_1758329194.json
2025-09-19 19:46:34,783 - INFO - _models.training_function_executor - Training function validation passed
2025-09-19 19:46:34,783 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ” STEP 3: Bayesian Optimization
2025-09-19 19:46:34,783 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:34,784 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“¦ Installing dependencies for GPT-generated training code...
2025-09-19 19:46:34,785 - INFO - package_installer - ðŸ” Analyzing GPT-generated code for package dependencies...
2025-09-19 19:46:34,789 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-19 19:46:34,789 - INFO - package_installer - Available packages: {'torch'}
2025-09-19 19:46:34,790 - INFO - package_installer - Missing packages: set()
2025-09-19 19:46:34,790 - INFO - package_installer - âœ… All required packages are already available
2025-09-19 19:46:34,790 - INFO - evaluation.code_generation_pipeline_orchestrator - âœ… All dependencies installed successfully
2025-09-19 19:46:34,790 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 62352 samples (using full dataset)
2025-09-19 19:46:34,790 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'd_model', 'num_heads', 'num_transformer_layers', 'ff_mult', 'dropout', 'stem_channels_per_branch', 'stride', 'weight_decay', 'loss_type', 'label_smoothing', 'class_weighting', 'grad_clip_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibration_batches']
2025-09-19 19:46:34,790 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-19 19:46:34,790 - INFO - _models.training_function_executor - Using centralized data splits for BO objective
2025-09-19 19:46:35,133 - INFO - bo.run_bo - Converted GPT search space: 19 parameters
2025-09-19 19:46:35,133 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-19 19:46:35,133 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-19 19:46:35,135 - INFO - bo.run_bo - ðŸ”BO Trial 1: Initial random exploration
2025-09-19 19:46:35,135 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 19:46:35,135 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:35,135 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:35,135 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': 12, 'd_model': 84, 'num_heads': 3, 'num_transformer_layers': 2, 'ff_mult': 4, 'dropout': 0.02999247474540087, 'stem_channels_per_branch': 18, 'stride': 5, 'weight_decay': 2.1618942406574448e-05, 'loss_type': 'ce', 'label_smoothing': 0.0650888472948853, 'class_weighting': True, 'grad_clip_norm': 3.6099938613341243, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 36}
2025-09-19 19:46:35,138 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': 12, 'd_model': 84, 'num_heads': 3, 'num_transformer_layers': 2, 'ff_mult': 4, 'dropout': 0.02999247474540087, 'stem_channels_per_branch': 18, 'stride': 5, 'weight_decay': 2.1618942406574448e-05, 'loss_type': 'ce', 'label_smoothing': 0.0650888472948853, 'class_weighting': True, 'grad_clip_norm': 3.6099938613341243, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 36}
2025-09-19 19:46:36,701 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:36,701 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:36,701 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:36,702 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 1.567s
2025-09-19 19:46:36,702 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:36,702 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-19 19:46:36,702 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(12), 'd_model': np.int64(84), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(4), 'dropout': 0.02999247474540087, 'stem_channels_per_branch': np.int64(18), 'stride': np.int64(5), 'weight_decay': 2.1618942406574448e-05, 'loss_type': 'ce', 'label_smoothing': 0.0650888472948853, 'class_weighting': True, 'grad_clip_norm': 3.6099938613341243, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(36)}, value=0.0000
2025-09-19 19:46:36,702 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(12), 'd_model': np.int64(84), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(4), 'dropout': 0.02999247474540087, 'stem_channels_per_branch': np.int64(18), 'stride': np.int64(5), 'weight_decay': 2.1618942406574448e-05, 'loss_type': 'ce', 'label_smoothing': 0.0650888472948853, 'class_weighting': True, 'grad_clip_norm': 3.6099938613341243, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(36)} -> 0.0000
2025-09-19 19:46:36,703 - INFO - bo.run_bo - ðŸ”BO Trial 2: Initial random exploration
2025-09-19 19:46:36,703 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 19:46:36,703 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:36,703 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:36,703 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.3205591037519604e-05, 'batch_size': 128, 'epochs': 48, 'd_model': 152, 'num_heads': 1, 'num_transformer_layers': 1, 'ff_mult': 4, 'dropout': 0.11995829151457665, 'stem_channels_per_branch': 23, 'stride': 4, 'weight_decay': 6.672367170464218e-05, 'loss_type': 'focal', 'label_smoothing': 0.01996737821583598, 'class_weighting': False, 'grad_clip_norm': 2.962072844310213, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 42}
2025-09-19 19:46:36,705 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.3205591037519604e-05, 'batch_size': 128, 'epochs': 48, 'd_model': 152, 'num_heads': 1, 'num_transformer_layers': 1, 'ff_mult': 4, 'dropout': 0.11995829151457665, 'stem_channels_per_branch': 23, 'stride': 4, 'weight_decay': 6.672367170464218e-05, 'loss_type': 'focal', 'label_smoothing': 0.01996737821583598, 'class_weighting': False, 'grad_clip_norm': 2.962072844310213, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 42}
2025-09-19 19:46:36,797 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:36,797 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:36,797 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:36,797 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.095s
2025-09-19 19:46:36,798 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:36,798 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-19 19:46:36,798 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 3.3205591037519604e-05, 'batch_size': 128, 'epochs': np.int64(48), 'd_model': np.int64(152), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(4), 'dropout': 0.11995829151457665, 'stem_channels_per_branch': np.int64(23), 'stride': np.int64(4), 'weight_decay': 6.672367170464218e-05, 'loss_type': 'focal', 'label_smoothing': 0.01996737821583598, 'class_weighting': False, 'grad_clip_norm': 2.962072844310213, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': np.int64(42)}, value=0.0000
2025-09-19 19:46:36,798 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 3.3205591037519604e-05, 'batch_size': 128, 'epochs': np.int64(48), 'd_model': np.int64(152), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(4), 'dropout': 0.11995829151457665, 'stem_channels_per_branch': np.int64(23), 'stride': np.int64(4), 'weight_decay': 6.672367170464218e-05, 'loss_type': 'focal', 'label_smoothing': 0.01996737821583598, 'class_weighting': False, 'grad_clip_norm': 2.962072844310213, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': np.int64(42)} -> 0.0000
2025-09-19 19:46:36,799 - INFO - bo.run_bo - ðŸ”BO Trial 3: Initial random exploration
2025-09-19 19:46:36,799 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 19:46:36,800 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:36,800 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:36,800 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.164996996763891e-06, 'batch_size': 256, 'epochs': 18, 'd_model': 72, 'num_heads': 2, 'num_transformer_layers': 1, 'ff_mult': 3, 'dropout': 0.2052699079536471, 'stem_channels_per_branch': 22, 'stride': 5, 'weight_decay': 3.077180271250685e-06, 'loss_type': 'ce', 'label_smoothing': 0.0034388521115218404, 'class_weighting': False, 'grad_clip_norm': 1.2938999080000848, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 45}
2025-09-19 19:46:36,802 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.164996996763891e-06, 'batch_size': 256, 'epochs': 18, 'd_model': 72, 'num_heads': 2, 'num_transformer_layers': 1, 'ff_mult': 3, 'dropout': 0.2052699079536471, 'stem_channels_per_branch': 22, 'stride': 5, 'weight_decay': 3.077180271250685e-06, 'loss_type': 'ce', 'label_smoothing': 0.0034388521115218404, 'class_weighting': False, 'grad_clip_norm': 1.2938999080000848, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 45}
2025-09-19 19:46:36,804 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-19 19:46:36,805 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-19 19:46:36,805 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-19 19:46:36,805 - INFO - _models.ai_code_generator - Prompt length: 821 characters
2025-09-19 19:46:36,805 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-19 19:46:36,805 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-19 19:46:36,805 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-19 19:46:36,811 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:36,811 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:36,811 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:36,811 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.012s
2025-09-19 19:46:36,965 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:36,965 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.153s
2025-09-19 19:46:36,965 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 1.164996996763891e-06, 'batch_size': 256, 'epochs': np.int64(18), 'd_model': np.int64(72), 'num_heads': np.int64(2), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(3), 'dropout': 0.2052699079536471, 'stem_channels_per_branch': np.int64(22), 'stride': np.int64(5), 'weight_decay': 3.077180271250685e-06, 'loss_type': 'ce', 'label_smoothing': 0.0034388521115218404, 'class_weighting': False, 'grad_clip_norm': 1.2938999080000848, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(45)}, value=0.0000
2025-09-19 19:46:36,965 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 1.164996996763891e-06, 'batch_size': 256, 'epochs': np.int64(18), 'd_model': np.int64(72), 'num_heads': np.int64(2), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(3), 'dropout': 0.2052699079536471, 'stem_channels_per_branch': np.int64(22), 'stride': np.int64(5), 'weight_decay': 3.077180271250685e-06, 'loss_type': 'ce', 'label_smoothing': 0.0034388521115218404, 'class_weighting': False, 'grad_clip_norm': 1.2938999080000848, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(45)} -> 0.0000
2025-09-19 19:46:36,965 - INFO - bo.run_bo - ðŸ”BO Trial 4: Using RF surrogate + Expected Improvement
2025-09-19 19:46:36,965 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 19:46:36,965 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:36,965 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:36,965 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0006894481952587278, 'batch_size': 128, 'epochs': 15, 'd_model': 151, 'num_heads': 1, 'num_transformer_layers': 1, 'ff_mult': 3, 'dropout': 0.2529913316529566, 'stem_channels_per_branch': 14, 'stride': 4, 'weight_decay': 0.005323888621998898, 'loss_type': np.str_('ce'), 'label_smoothing': 0.052788399678970976, 'class_weighting': True, 'grad_clip_norm': 3.8210755086231547, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 43}
2025-09-19 19:46:36,967 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0006894481952587278, 'batch_size': 128, 'epochs': 15, 'd_model': 151, 'num_heads': 1, 'num_transformer_layers': 1, 'ff_mult': 3, 'dropout': 0.2529913316529566, 'stem_channels_per_branch': 14, 'stride': 4, 'weight_decay': 0.005323888621998898, 'loss_type': np.str_('ce'), 'label_smoothing': 0.052788399678970976, 'class_weighting': True, 'grad_clip_norm': 3.8210755086231547, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 43}
2025-09-19 19:46:36,974 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:36,975 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:36,975 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:36,975 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.011s
2025-09-19 19:46:37,123 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:37,123 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.148s
2025-09-19 19:46:37,123 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 0.0006894481952587278, 'batch_size': np.int64(128), 'epochs': np.int64(15), 'd_model': np.int64(151), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(3), 'dropout': 0.2529913316529566, 'stem_channels_per_branch': np.int64(14), 'stride': np.int64(4), 'weight_decay': 0.005323888621998898, 'loss_type': np.str_('ce'), 'label_smoothing': 0.052788399678970976, 'class_weighting': np.True_, 'grad_clip_norm': 3.8210755086231547, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(43)}, value=0.0000
2025-09-19 19:46:37,123 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 0.0006894481952587278, 'batch_size': np.int64(128), 'epochs': np.int64(15), 'd_model': np.int64(151), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(3), 'dropout': 0.2529913316529566, 'stem_channels_per_branch': np.int64(14), 'stride': np.int64(4), 'weight_decay': 0.005323888621998898, 'loss_type': np.str_('ce'), 'label_smoothing': 0.052788399678970976, 'class_weighting': np.True_, 'grad_clip_norm': 3.8210755086231547, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(43)} -> 0.0000
2025-09-19 19:46:37,123 - INFO - bo.run_bo - ðŸ”BO Trial 5: Using RF surrogate + Expected Improvement
2025-09-19 19:46:37,123 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 19:46:37,123 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:37,124 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:37,124 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.041612013212491326, 'batch_size': 256, 'epochs': 18, 'd_model': 151, 'num_heads': 3, 'num_transformer_layers': 2, 'ff_mult': 3, 'dropout': 0.2579470928493083, 'stem_channels_per_branch': 8, 'stride': 5, 'weight_decay': 3.0982767549645e-05, 'loss_type': np.str_('focal'), 'label_smoothing': 0.002781194852295388, 'class_weighting': False, 'grad_clip_norm': 3.27091931592027, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 45}
2025-09-19 19:46:37,126 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.041612013212491326, 'batch_size': 256, 'epochs': 18, 'd_model': 151, 'num_heads': 3, 'num_transformer_layers': 2, 'ff_mult': 3, 'dropout': 0.2579470928493083, 'stem_channels_per_branch': 8, 'stride': 5, 'weight_decay': 3.0982767549645e-05, 'loss_type': np.str_('focal'), 'label_smoothing': 0.002781194852295388, 'class_weighting': False, 'grad_clip_norm': 3.27091931592027, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 45}
2025-09-19 19:46:37,141 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,141 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:37,141 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,141 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.018s
2025-09-19 19:46:37,286 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:37,286 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.146s
2025-09-19 19:46:37,286 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 0.041612013212491326, 'batch_size': np.int64(256), 'epochs': np.int64(18), 'd_model': np.int64(151), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(3), 'dropout': 0.2579470928493083, 'stem_channels_per_branch': np.int64(8), 'stride': np.int64(5), 'weight_decay': 3.0982767549645e-05, 'loss_type': np.str_('focal'), 'label_smoothing': 0.002781194852295388, 'class_weighting': np.False_, 'grad_clip_norm': 3.27091931592027, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(45)}, value=0.0000
2025-09-19 19:46:37,286 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 0.041612013212491326, 'batch_size': np.int64(256), 'epochs': np.int64(18), 'd_model': np.int64(151), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(3), 'dropout': 0.2579470928493083, 'stem_channels_per_branch': np.int64(8), 'stride': np.int64(5), 'weight_decay': 3.0982767549645e-05, 'loss_type': np.str_('focal'), 'label_smoothing': 0.002781194852295388, 'class_weighting': np.False_, 'grad_clip_norm': 3.27091931592027, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(45)} -> 0.0000
2025-09-19 19:46:37,286 - INFO - bo.run_bo - ðŸ”BO Trial 6: Using RF surrogate + Expected Improvement
2025-09-19 19:46:37,286 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 19:46:37,287 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:37,287 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:37,287 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.856690216767765e-05, 'batch_size': 64, 'epochs': 24, 'd_model': 127, 'num_heads': 1, 'num_transformer_layers': 1, 'ff_mult': 4, 'dropout': 0.11452037861754022, 'stem_channels_per_branch': 32, 'stride': 2, 'weight_decay': 0.00022510881583334328, 'loss_type': np.str_('ce'), 'label_smoothing': 0.0672735604816764, 'class_weighting': True, 'grad_clip_norm': 2.2252486429449307, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 37}
2025-09-19 19:46:37,289 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.856690216767765e-05, 'batch_size': 64, 'epochs': 24, 'd_model': 127, 'num_heads': 1, 'num_transformer_layers': 1, 'ff_mult': 4, 'dropout': 0.11452037861754022, 'stem_channels_per_branch': 32, 'stride': 2, 'weight_decay': 0.00022510881583334328, 'loss_type': np.str_('ce'), 'label_smoothing': 0.0672735604816764, 'class_weighting': True, 'grad_clip_norm': 2.2252486429449307, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 37}
2025-09-19 19:46:37,297 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,297 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:37,297 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,297 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.010s
2025-09-19 19:46:37,445 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:37,445 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.147s
2025-09-19 19:46:37,445 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 3.856690216767765e-05, 'batch_size': np.int64(64), 'epochs': np.int64(24), 'd_model': np.int64(127), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(4), 'dropout': 0.11452037861754022, 'stem_channels_per_branch': np.int64(32), 'stride': np.int64(2), 'weight_decay': 0.00022510881583334328, 'loss_type': np.str_('ce'), 'label_smoothing': 0.0672735604816764, 'class_weighting': np.True_, 'grad_clip_norm': 2.2252486429449307, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(37)}, value=0.0000
2025-09-19 19:46:37,445 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 3.856690216767765e-05, 'batch_size': np.int64(64), 'epochs': np.int64(24), 'd_model': np.int64(127), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(4), 'dropout': 0.11452037861754022, 'stem_channels_per_branch': np.int64(32), 'stride': np.int64(2), 'weight_decay': 0.00022510881583334328, 'loss_type': np.str_('ce'), 'label_smoothing': 0.0672735604816764, 'class_weighting': np.True_, 'grad_clip_norm': 2.2252486429449307, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(37)} -> 0.0000
2025-09-19 19:46:37,445 - INFO - bo.run_bo - ðŸ”BO Trial 7: Using RF surrogate + Expected Improvement
2025-09-19 19:46:37,445 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 19:46:37,445 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:37,446 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:37,446 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005961302648744911, 'batch_size': 64, 'epochs': 42, 'd_model': 108, 'num_heads': 3, 'num_transformer_layers': 2, 'ff_mult': 4, 'dropout': 0.11297541813747194, 'stem_channels_per_branch': 16, 'stride': 3, 'weight_decay': 4.376751364194916e-05, 'loss_type': np.str_('focal'), 'label_smoothing': 0.09464241605242918, 'class_weighting': True, 'grad_clip_norm': 3.2357702757845614, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 63}
2025-09-19 19:46:37,448 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005961302648744911, 'batch_size': 64, 'epochs': 42, 'd_model': 108, 'num_heads': 3, 'num_transformer_layers': 2, 'ff_mult': 4, 'dropout': 0.11297541813747194, 'stem_channels_per_branch': 16, 'stride': 3, 'weight_decay': 4.376751364194916e-05, 'loss_type': np.str_('focal'), 'label_smoothing': 0.09464241605242918, 'class_weighting': True, 'grad_clip_norm': 3.2357702757845614, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 63}
2025-09-19 19:46:37,461 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,461 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:37,462 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,462 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.017s
2025-09-19 19:46:37,608 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:37,608 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.146s
2025-09-19 19:46:37,608 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.005961302648744911, 'batch_size': np.int64(64), 'epochs': np.int64(42), 'd_model': np.int64(108), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(4), 'dropout': 0.11297541813747194, 'stem_channels_per_branch': np.int64(16), 'stride': np.int64(3), 'weight_decay': 4.376751364194916e-05, 'loss_type': np.str_('focal'), 'label_smoothing': 0.09464241605242918, 'class_weighting': np.True_, 'grad_clip_norm': 3.2357702757845614, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(63)}, value=0.0000
2025-09-19 19:46:37,608 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.005961302648744911, 'batch_size': np.int64(64), 'epochs': np.int64(42), 'd_model': np.int64(108), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(4), 'dropout': 0.11297541813747194, 'stem_channels_per_branch': np.int64(16), 'stride': np.int64(3), 'weight_decay': 4.376751364194916e-05, 'loss_type': np.str_('focal'), 'label_smoothing': 0.09464241605242918, 'class_weighting': np.True_, 'grad_clip_norm': 3.2357702757845614, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(63)} -> 0.0000
2025-09-19 19:46:37,609 - INFO - bo.run_bo - ðŸ”BO Trial 8: Using RF surrogate + Expected Improvement
2025-09-19 19:46:37,609 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 19:46:37,609 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:37,609 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:37,609 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0028072716667799586, 'batch_size': 128, 'epochs': 22, 'd_model': 116, 'num_heads': 3, 'num_transformer_layers': 1, 'ff_mult': 4, 'dropout': 0.2212516073931659, 'stem_channels_per_branch': 28, 'stride': 4, 'weight_decay': 7.88423159256791e-05, 'loss_type': np.str_('ce'), 'label_smoothing': 0.012009442104906034, 'class_weighting': True, 'grad_clip_norm': 3.4197427043772466, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 54}
2025-09-19 19:46:37,611 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0028072716667799586, 'batch_size': 128, 'epochs': 22, 'd_model': 116, 'num_heads': 3, 'num_transformer_layers': 1, 'ff_mult': 4, 'dropout': 0.2212516073931659, 'stem_channels_per_branch': 28, 'stride': 4, 'weight_decay': 7.88423159256791e-05, 'loss_type': np.str_('ce'), 'label_smoothing': 0.012009442104906034, 'class_weighting': True, 'grad_clip_norm': 3.4197427043772466, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 54}
2025-09-19 19:46:37,618 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,619 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:37,619 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,619 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.011s
2025-09-19 19:46:37,765 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:37,765 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.145s
2025-09-19 19:46:37,765 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 0.0028072716667799586, 'batch_size': np.int64(128), 'epochs': np.int64(22), 'd_model': np.int64(116), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(4), 'dropout': 0.2212516073931659, 'stem_channels_per_branch': np.int64(28), 'stride': np.int64(4), 'weight_decay': 7.88423159256791e-05, 'loss_type': np.str_('ce'), 'label_smoothing': 0.012009442104906034, 'class_weighting': np.True_, 'grad_clip_norm': 3.4197427043772466, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(54)}, value=0.0000
2025-09-19 19:46:37,765 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 0.0028072716667799586, 'batch_size': np.int64(128), 'epochs': np.int64(22), 'd_model': np.int64(116), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(1), 'ff_mult': np.int64(4), 'dropout': 0.2212516073931659, 'stem_channels_per_branch': np.int64(28), 'stride': np.int64(4), 'weight_decay': 7.88423159256791e-05, 'loss_type': np.str_('ce'), 'label_smoothing': 0.012009442104906034, 'class_weighting': np.True_, 'grad_clip_norm': 3.4197427043772466, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(54)} -> 0.0000
2025-09-19 19:46:37,765 - INFO - bo.run_bo - ðŸ”BO Trial 9: Using RF surrogate + Expected Improvement
2025-09-19 19:46:37,765 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 19:46:37,766 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:37,766 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:37,766 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001420033779949016, 'batch_size': 256, 'epochs': 16, 'd_model': 107, 'num_heads': 1, 'num_transformer_layers': 2, 'ff_mult': 3, 'dropout': 0.09351383818904123, 'stem_channels_per_branch': 10, 'stride': 2, 'weight_decay': 5.4968500947831254e-06, 'loss_type': np.str_('focal'), 'label_smoothing': 0.024496725360672173, 'class_weighting': False, 'grad_clip_norm': 2.160549503704548, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6}
2025-09-19 19:46:37,768 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001420033779949016, 'batch_size': 256, 'epochs': 16, 'd_model': 107, 'num_heads': 1, 'num_transformer_layers': 2, 'ff_mult': 3, 'dropout': 0.09351383818904123, 'stem_channels_per_branch': 10, 'stride': 2, 'weight_decay': 5.4968500947831254e-06, 'loss_type': np.str_('focal'), 'label_smoothing': 0.024496725360672173, 'class_weighting': False, 'grad_clip_norm': 2.160549503704548, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6}
2025-09-19 19:46:37,778 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,778 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:37,778 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,778 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.013s
2025-09-19 19:46:37,929 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:37,929 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.151s
2025-09-19 19:46:37,929 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 0.001420033779949016, 'batch_size': np.int64(256), 'epochs': np.int64(16), 'd_model': np.int64(107), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(3), 'dropout': 0.09351383818904123, 'stem_channels_per_branch': np.int64(10), 'stride': np.int64(2), 'weight_decay': 5.4968500947831254e-06, 'loss_type': np.str_('focal'), 'label_smoothing': 0.024496725360672173, 'class_weighting': np.False_, 'grad_clip_norm': 2.160549503704548, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6)}, value=0.0000
2025-09-19 19:46:37,929 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 0.001420033779949016, 'batch_size': np.int64(256), 'epochs': np.int64(16), 'd_model': np.int64(107), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(3), 'dropout': 0.09351383818904123, 'stem_channels_per_branch': np.int64(10), 'stride': np.int64(2), 'weight_decay': 5.4968500947831254e-06, 'loss_type': np.str_('focal'), 'label_smoothing': 0.024496725360672173, 'class_weighting': np.False_, 'grad_clip_norm': 2.160549503704548, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6)} -> 0.0000
2025-09-19 19:46:37,929 - INFO - bo.run_bo - ðŸ”BO Trial 10: Using RF surrogate + Expected Improvement
2025-09-19 19:46:37,929 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 19:46:37,929 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:37,929 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:37,929 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.18460519214398e-05, 'batch_size': 128, 'epochs': 48, 'd_model': 159, 'num_heads': 1, 'num_transformer_layers': 2, 'ff_mult': 2, 'dropout': 0.1781102370966922, 'stem_channels_per_branch': 27, 'stride': 4, 'weight_decay': 0.00018765769501343863, 'loss_type': np.str_('ce'), 'label_smoothing': 0.05224794116037818, 'class_weighting': True, 'grad_clip_norm': 2.1387360580528525, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 36}
2025-09-19 19:46:37,931 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.18460519214398e-05, 'batch_size': 128, 'epochs': 48, 'd_model': 159, 'num_heads': 1, 'num_transformer_layers': 2, 'ff_mult': 2, 'dropout': 0.1781102370966922, 'stem_channels_per_branch': 27, 'stride': 4, 'weight_decay': 0.00018765769501343863, 'loss_type': np.str_('ce'), 'label_smoothing': 0.05224794116037818, 'class_weighting': True, 'grad_clip_norm': 2.1387360580528525, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 36}
2025-09-19 19:46:37,944 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,944 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:37,944 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:37,944 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.015s
2025-09-19 19:46:38,089 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 19:46:38,089 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.144s
2025-09-19 19:46:38,089 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 2.18460519214398e-05, 'batch_size': np.int64(128), 'epochs': np.int64(48), 'd_model': np.int64(159), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(2), 'dropout': 0.1781102370966922, 'stem_channels_per_branch': np.int64(27), 'stride': np.int64(4), 'weight_decay': 0.00018765769501343863, 'loss_type': np.str_('ce'), 'label_smoothing': 0.05224794116037818, 'class_weighting': np.True_, 'grad_clip_norm': 2.1387360580528525, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(36)}, value=0.0000
2025-09-19 19:46:38,089 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 2.18460519214398e-05, 'batch_size': np.int64(128), 'epochs': np.int64(48), 'd_model': np.int64(159), 'num_heads': np.int64(1), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(2), 'dropout': 0.1781102370966922, 'stem_channels_per_branch': np.int64(27), 'stride': np.int64(4), 'weight_decay': 0.00018765769501343863, 'loss_type': np.str_('ce'), 'label_smoothing': 0.05224794116037818, 'class_weighting': np.True_, 'grad_clip_norm': 2.1387360580528525, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(36)} -> 0.0000
2025-09-19 19:46:38,089 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.0000
2025-09-19 19:46:38,089 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(12), 'd_model': np.int64(84), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(4), 'dropout': 0.02999247474540087, 'stem_channels_per_branch': np.int64(18), 'stride': np.int64(5), 'weight_decay': 2.1618942406574448e-05, 'loss_type': 'ce', 'label_smoothing': 0.0650888472948853, 'class_weighting': True, 'grad_clip_norm': 3.6099938613341243, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(36)}
2025-09-19 19:46:38,091 - INFO - visualization - Generating BO visualization charts with 10 trials...
2025-09-19 19:46:40,360 - INFO - visualization - BO summary saved to: charts\BO_TinyCATNet-1D-CNN+Transformer_20250919_194638\bo_summary.txt
2025-09-19 19:46:40,361 - INFO - visualization - BO charts saved to: charts\BO_TinyCATNet-1D-CNN+Transformer_20250919_194638
2025-09-19 19:46:40,361 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸ“Š BO charts saved to: charts\BO_TinyCATNet-1D-CNN+Transformer_20250919_194638
2025-09-19 19:46:41,361 - INFO - evaluation.code_generation_pipeline_orchestrator - ðŸš€ STEP 4: Final Training Execution
2025-09-19 19:46:41,361 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-19 19:46:41,556 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-19 19:46:41,564 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-19 19:46:41,577 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-19 19:46:41,587 - INFO - _models.training_function_executor - Loaded training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:41,587 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-19 19:46:41,587 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(12), 'd_model': np.int64(84), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(4), 'dropout': 0.02999247474540087, 'stem_channels_per_branch': np.int64(18), 'stride': np.int64(5), 'weight_decay': 2.1618942406574448e-05, 'loss_type': 'ce', 'label_smoothing': 0.0650888472948853, 'class_weighting': True, 'grad_clip_norm': 3.6099938613341243, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(36)}
2025-09-19 19:46:41,587 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 19:46:41,682 - INFO - _models.training_function_executor - Executing training function: TinyCATNet-1D-CNN+Transformer
2025-09-19 19:46:41,682 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': np.int64(12), 'd_model': np.int64(84), 'num_heads': np.int64(3), 'num_transformer_layers': np.int64(2), 'ff_mult': np.int64(4), 'dropout': 0.02999247474540087, 'stem_channels_per_branch': np.int64(18), 'stride': np.int64(5), 'weight_decay': 2.1618942406574448e-05, 'loss_type': 'ce', 'label_smoothing': 0.0650888472948853, 'class_weighting': True, 'grad_clip_norm': 3.6099938613341243, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(36)}
2025-09-19 19:46:41,685 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.009609812947036413, 'batch_size': 64, 'epochs': 12, 'd_model': 84, 'num_heads': 3, 'num_transformer_layers': 2, 'ff_mult': 4, 'dropout': 0.02999247474540087, 'stem_channels_per_branch': 18, 'stride': 5, 'weight_decay': 2.1618942406574448e-05, 'loss_type': 'ce', 'label_smoothing': 0.0650888472948853, 'class_weighting': True, 'grad_clip_norm': 3.6099938613341243, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 36}
2025-09-19 19:46:41,706 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-19 19:46:41,706 - ERROR - _models.training_function_executor - Training code: import math
from typing import Dict, Any
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.ao.quantizatio...
2025-09-19 19:46:41,717 - ERROR - __main__ - Unhandled exception: RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
Traceback (most recent call last):
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 484, in <module>
    processed_real_data = process_real_data()
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 342, in process_real_data
    result = process_data_with_ai_enhanced_evaluation(
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 136, in process_data_with_ai_enhanced_evaluation
    result = train_with_iterative_selection(data, labels, device=device, **kwargs)
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 85, in train_with_iterative_selection
    best_model, pipeline_results = orchestrator.run_complete_pipeline(
  File "D:\_A\GPT_research\ml_pipeline\evaluation\code_generation_pipeline_orchestrator.py", line 99, in run_complete_pipeline
    final_model, training_results = self._execute_final_training(
  File "D:\_A\GPT_research\ml_pipeline\evaluation\code_generation_pipeline_orchestrator.py", line 339, in _execute_final_training
    trained_model, training_metrics = training_executor.execute_training_function(
  File "D:\_A\GPT_research\ml_pipeline\_models\training_function_executor.py", line 266, in execute_training_function
    model, metrics = train_model(
  File "<string>", line 328, in train_model
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\utils\data\dataloader.py", line 701, in __next__
    data = self._next_data()
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\utils\data\dataloader.py", line 759, in _next_data
    data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\utils\data\_utils\pin_memory.py", line 98, in pin_memory
    clone[i] = pin_memory(item, device)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\utils\data\_utils\pin_memory.py", line 64, in pin_memory
    return data.pin_memory(device)
RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned

