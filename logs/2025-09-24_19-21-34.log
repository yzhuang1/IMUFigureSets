2025-09-24 19:21:34,970 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-24 19:21:35,077 - INFO - __main__ - Logging system initialized successfully
2025-09-24 19:21:35,077 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-24 19:21:35,077 - INFO - __main__ - Starting real data processing from data/dataset1/ directory
2025-09-24 19:21:35,078 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-24 19:21:35,078 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-24 19:21:35,078 - INFO - __main__ - Attempting to load: X.npy
2025-09-24 19:21:35,119 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-24 19:21:35,157 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-24 19:21:35,157 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-24 19:21:35,157 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-24 19:21:35,157 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-24 19:21:35,159 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-24 19:21:35,159 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-24 19:21:35,159 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-24 19:21:35,159 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-24 19:21:35,159 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-24 19:21:35,159 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-24 19:21:35,159 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-24 19:21:35,159 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-24 19:21:35,159 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-24 19:21:35,257 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-24 19:21:35,257 - INFO - class_balancing - Class imbalance analysis:
2025-09-24 19:21:35,257 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-24 19:21:35,257 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-24 19:21:35,257 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-24 19:21:35,258 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-24 19:21:35,258 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-24 19:21:35,258 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-24 19:21:35,258 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-24 19:21:35,258 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-24 19:21:35,429 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-24 19:21:35,429 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-24 19:21:35,429 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-24 19:21:35,429 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-24 19:21:35,429 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-24 19:21:35,429 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-24 19:21:35,429 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-24 19:23:23,167 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-24 19:23:23,222 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-24 19:23:23,222 - INFO - _models.ai_code_generator - Prompt length: 3898 characters
2025-09-24 19:23:23,222 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-24 19:23:23,222 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-24 19:23:23,222 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-24 19:25:13,145 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-24 19:25:13,174 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-24 19:25:13,174 - INFO - _models.ai_code_generator - AI generated training function: RRFusionRes1DCNN-5C
2025-09-24 19:25:13,174 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-09-24 19:25:13,174 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-09-24 19:25:13,174 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: RRFusionRes1DCNN-5C
2025-09-24 19:25:13,174 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'base_channels', 'loss_type', 'gamma', 'label_smoothing', 'scheduler', 'patience', 'use_amp', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibration_batches', 'beta_cb']
2025-09-24 19:25:13,174 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-09-24 19:25:13,174 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-24 19:25:13,175 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:25:13,175 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:25:13,175 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-24 19:25:13,175 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: RRFusionRes1DCNN-5C
2025-09-24 19:25:13,175 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-24 19:25:13,175 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-24 19:25:13,175 - WARNING - package_installer - Could not parse code for imports due to syntax error: unexpected character after line continuation character (<unknown>, line 1)
2025-09-24 19:25:13,176 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-24 19:25:13,176 - INFO - package_installer - Available packages: {'torch'}
2025-09-24 19:25:13,176 - INFO - package_installer - Missing packages: set()
2025-09-24 19:25:13,176 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-24 19:25:13,176 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-24 19:25:13,176 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:25:13,176 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-24 19:25:13,176 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'base_channels', 'loss_type', 'gamma', 'label_smoothing', 'scheduler', 'patience', 'use_amp', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibration_batches', 'beta_cb']
2025-09-24 19:25:13,176 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-24 19:25:13,176 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:25:13,176 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-24 19:25:13,211 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-24 19:25:13,339 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-24 19:25:13,339 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-24 19:25:13,340 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-24 19:25:13,340 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-24 19:25:13,340 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 19:25:13,340 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:25:13,341 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:25:13,341 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:25:13,342 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:25:13,822 - ERROR - _models.training_function_executor - Training execution failed: Given groups=1, weight of size [18, 2, 7], expected input[32, 1000, 2] to have 2 channels, but got 1000 channels instead
2025-09-24 19:25:13,823 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-24 19:25:13,823 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-24 19:25:13,823 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-24 19:25:13,823 - INFO - _models.ai_code_generator - Prompt length: 19255 characters
2025-09-24 19:25:13,823 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-24 19:25:13,823 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-24 19:25:13,823 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-24 19:26:02,399 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-24 19:26:02,400 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-24 19:26:02,401 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250924_192602_attempt1.txt
2025-09-24 19:26:02,401 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code":"def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a 5-class inter-patient 1D CNN + RR feature-fusion classifier.\n\n    Inputs:\n      - X_train, X_val: torch.Tensor of shape (N, 2, 1000) or (N, 1000, 2), dtype float32.\n      - y_train, y_val: torch.Tensor of shape (N,), dtype long.\n      - device: str or torch.device; training will run on GPU.\n    Hyperparameters (kwargs):\n      - lr (float), batch_size (int), epochs (int), weight_decay (float)\n      - dropout (float), base_channels (int)\n      - loss_type in {\"focal\", \"ce\", \"class_balanced\"}\n      - gamma (float, focal loss focusing)\n      - label_smoothing (float)\n      - scheduler in {\"none\", \"onecycle\", \"plateau\"}\n      - patience (int) for ReduceLROnPlateau\n      - use_amp (bool), grad_clip (float)\n      - quantization_bits in {8,16,32}, quantize_weights (bool), quantize_activations (bool)\n      - calibration_batches (int) for PTQ calibration (best-effort; dynamic quant used by default)\n    Returns: (quantized_model, metrics_dict) where metrics_dict has lists: train_losses, val_losses, val_acc\n    \"\"\"\n    # Helper: ensure channels-first layout (B, 2, L)\n    def _to_channels_first(X):\n        if X.dim() == 3:\n            # If last dim is channels=2 and middle is length (e.g., (B,1000,2)), permute\n            if X.shape[-1] == 2 and X.shape[1] != 2:\n                return X.permute(0, 2, 1).contiguous()\n        return X\n\n    # Robust device handling\n    device = torch.device(device)\n    if device.type != 'cuda':\n        if torch.cuda.is_available():\n            print(\"[Info] Overriding device to CUDA for training per requirement.\")\n            device = torch.device('cuda')\n        else:\n            raise RuntimeError(\"CUDA device required for training per spec but not available.\")\n\n    torch.backends.cudnn.benchmark = True\n\n    # Defaults\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 128))\n    epochs = int(kwargs.get('epochs', 15))\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    dropout = float(kwargs.get('dropout', 0.1))\n    base_channels = int(kwargs.get('base_channels', 16))\n    loss_type = str(kwargs.get('loss_type', 'focal'))\n    gamma = float(kwargs.get('gamma', 2.0))\n    label_smoothing = float(kwargs.get('label_smoothing', 0.0))\n    scheduler_type = str(kwargs.get('scheduler', 'onecycle'))\n    patience = int(kwargs.get('patience', 5))\n    use_amp = bool(kwargs.get('use_amp', True))\n    grad_clip = float(kwargs.get('grad_clip', 1.0))\n\n    quantization_bits = int(kwargs.get('quantization_bits', 8))\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))\n    calibration_batches = int(kwargs.get('calibration_batches', 2))\n\n    num_classes = 5\n\n    # Datasets and loaders\n    # Ensure dtypes and channels-first memory layout\n    X_train = _to_channels_first(X_train.float())\n    X_val = _to_channels_first(X_val.float())\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    # Validate expected shape after fix\n    if X_train.dim() != 3 or X_train.shape[1] != 2:\n        raise ValueError(f\"X_train must be (N,2,L). Got {tuple(X_train.shape)}\")\n    if X_val.dim() != 3 or X_val.shape[1] != 2:\n        raise ValueError(f\"X_val must be (N,2,L). Got {tuple(X_val.shape)}\")\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=False, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=0)\n\n    # Model\n    model = RRFusionRes1DCNN(in_ch=2, base_ch=base_channels, num_classes=num_classes, dropout=dropout, rr_feat_dim=5, rr_embed=8)\n    model = model.to(device)\n\n    # Loss\n    class_weights = make_class_weights(y_train, num_classes, smoothing=0.0, device=device)\n    if loss_type == 'focal':\n        criterion = FocalLoss(gamma=gamma, weight=class_weights, label_smoothing=label_smoothing).to(device)\n    elif loss_type == 'class_balanced':\n        beta = kwargs.get('beta_cb', 0.999)\n        counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n        eff_num = (1.0 - torch.pow(torch.tensor(beta), counts)) / (1.0 - beta + 1e-8)\n        cb_w = (eff_num.sum() / (eff_num + 1e-8))\n        cb_w = (cb_w / cb_w.mean()).to(device)\n        criterion = nn.CrossEntropyLoss(weight=cb_w, label_smoothing=label_smoothing).to(device)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    if scheduler_type == 'onecycle':\n        steps_per_epoch = max(1, math.ceil(len(train_loader)))\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n    elif scheduler_type == 'plateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=0.5, min_lr=1e-6)\n    else:\n        scheduler = None\n\n    scaler = GradScaler(enabled=use_amp)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            # Ensure batch is channels-first in case upstream data varies\n            if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            rr_feats = compute_rr_entropy_features(xb).to(device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=use_amp):\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            if grad_clip and grad_clip > 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n\n        train_epoch_loss = running_loss / max(1, total)\n        train_losses.append(train_epoch_loss)\n\n        # Validation\n        model.eval()\n        v_loss = 0.0\n        v_total = 0\n        v_correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                rr_feats = compute_rr_entropy_features(xb).to(device)\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n                v_loss += loss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                v_correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n\n        val_epoch_loss = v_loss / max(1, v_total)\n        val_epoch_acc = (v_correct / max(1, v_total))\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n        if scheduler_type == 'onecycle':\n            scheduler.step()\n        elif scheduler_type == 'plateau':\n            scheduler.step(val_epoch_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_epoch_loss:.6f} val_loss={val_epoch_loss:.6f} val_acc={val_epoch_acc:.4f}\")\n\n    # -------- Post-Training Quantization (best-effort, keep model size <= 256KB) --------\n    def approximate_model_size_bytes(m):\n        nbytes = 0\n        for k, v in m.state_dict().items():\n            nbytes += v.numel() * v.element_size()\n        return int(nbytes)\n\n    q_model = None\n    if quantization_bits == 8 and quantize_weights:\n        try:\n            q_model = torch.ao.quantization.quantize_dynamic(\n                copy.deepcopy(model).to('cpu').eval(),\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n            if quantize_activations:\n                print(\"[Info] quantize_activations requested, but dynamic quantization only quantizes weights of Linear layers. Proceeding with weight-only int8.\")\n        except Exception as e:\n            print(f\"[Warn] Dynamic quantization failed: {e}. Falling back to FP32 CPU model.\")\n            q_model = copy.deepcopy(model).to('cpu').eval()\n    elif quantization_bits == 16:\n        q_model = copy.deepcopy(model).to('cpu').eval().half()\n    else:\n        q_model = copy.deepcopy(model).to('cpu').eval()\n\n    size_bytes = approximate_model_size_bytes(q_model)\n    if size_bytes > 256 * 1024:\n        print(f\"[Warn] Quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Consider reducing base_channels or enabling int8.\")\n    else:\n        print(f\"[Info] Quantized model size ‚âà {size_bytes/1024:.1f}KB (<= 256KB).\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'quantized_model_size_bytes': size_bytes\n    }\n\n    return q_model, metrics\n"}
2025-09-24 19:26:02,401 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-24 19:26:02,401 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code":"def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a 5-class inter-patient 1D CNN + RR feature-fusion classifier.\n\n    Inputs:\n      - X_train, X_val: torch.Tensor of shape (N, 2, 1000) or (N, 1000, 2), dtype float32.\n      - y_train, y_val: torch.Tensor of shape (N,), dtype long.\n      - device: str or torch.device; training will run on GPU.\n    Hyperparameters (kwargs):\n      - lr (float), batch_size (int), epochs (int), weight_decay (float)\n      - dropout (float), base_channels (int)\n      - loss_type in {\"focal\", \"ce\", \"class_balanced\"}\n      - gamma (float, focal loss focusing)\n      - label_smoothing (float)\n      - scheduler in {\"none\", \"onecycle\", \"plateau\"}\n      - patience (int) for ReduceLROnPlateau\n      - use_amp (bool), grad_clip (float)\n      - quantization_bits in {8,16,32}, quantize_weights (bool), quantize_activations (bool)\n      - calibration_batches (int) for PTQ calibration (best-effort; dynamic quant used by default)\n    Returns: (quantized_model, metrics_dict) where metrics_dict has lists: train_losses, val_losses, val_acc\n    \"\"\"\n    # Helper: ensure channels-first layout (B, 2, L)\n    def _to_channels_first(X):\n        if X.dim() == 3:\n            # If last dim is channels=2 and middle is length (e.g., (B,1000,2)), permute\n            if X.shape[-1] == 2 and X.shape[1] != 2:\n                return X.permute(0, 2, 1).contiguous()\n        return X\n\n    # Robust device handling\n    device = torch.device(device)\n    if device.type != 'cuda':\n        if torch.cuda.is_available():\n            print(\"[Info] Overriding device to CUDA for training per requirement.\")\n            device = torch.device('cuda')\n        else:\n            raise RuntimeError(\"CUDA device required for training per spec but not available.\")\n\n    torch.backends.cudnn.benchmark = True\n\n    # Defaults\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 128))\n    epochs = int(kwargs.get('epochs', 15))\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    dropout = float(kwargs.get('dropout', 0.1))\n    base_channels = int(kwargs.get('base_channels', 16))\n    loss_type = str(kwargs.get('loss_type', 'focal'))\n    gamma = float(kwargs.get('gamma', 2.0))\n    label_smoothing = float(kwargs.get('label_smoothing', 0.0))\n    scheduler_type = str(kwargs.get('scheduler', 'onecycle'))\n    patience = int(kwargs.get('patience', 5))\n    use_amp = bool(kwargs.get('use_amp', True))\n    grad_clip = float(kwargs.get('grad_clip', 1.0))\n\n    quantization_bits = int(kwargs.get('quantization_bits', 8))\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))\n    calibration_batches = int(kwargs.get('calibration_batches', 2))\n\n    num_classes = 5\n\n    # Datasets and loaders\n    # Ensure dtypes and channels-first memory layout\n    X_train = _to_channels_first(X_train.float())\n    X_val = _to_channels_first(X_val.float())\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    # Validate expected shape after fix\n    if X_train.dim() != 3 or X_train.shape[1] != 2:\n        raise ValueError(f\"X_train must be (N,2,L). Got {tuple(X_train.shape)}\")\n    if X_val.dim() != 3 or X_val.shape[1] != 2:\n        raise ValueError(f\"X_val must be (N,2,L). Got {tuple(X_val.shape)}\")\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=False, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=0)\n\n    # Model\n    model = RRFusionRes1DCNN(in_ch=2, base_ch=base_channels, num_classes=num_classes, dropout=dropout, rr_feat_dim=5, rr_embed=8)\n    model = model.to(device)\n\n    # Loss\n    class_weights = make_class_weights(y_train, num_classes, smoothing=0.0, device=device)\n    if loss_type == 'focal':\n        criterion = FocalLoss(gamma=gamma, weight=class_weights, label_smoothing=label_smoothing).to(device)\n    elif loss_type == 'class_balanced':\n        beta = kwargs.get('beta_cb', 0.999)\n        counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n        eff_num = (1.0 - torch.pow(torch.tensor(beta), counts)) / (1.0 - beta + 1e-8)\n        cb_w = (eff_num.sum() / (eff_num + 1e-8))\n        cb_w = (cb_w / cb_w.mean()).to(device)\n        criterion = nn.CrossEntropyLoss(weight=cb_w, label_smoothing=label_smoothing).to(device)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    if scheduler_type == 'onecycle':\n        steps_per_epoch = max(1, math.ceil(len(train_loader)))\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n    elif scheduler_type == 'plateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=0.5, min_lr=1e-6)\n    else:\n        scheduler = None\n\n    scaler = GradScaler(enabled=use_amp)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            # Ensure batch is channels-first in case upstream data varies\n            if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            rr_feats = compute_rr_entropy_features(xb).to(device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=use_amp):\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            if grad_clip and grad_clip > 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n\n        train_epoch_loss = running_loss / max(1, total)\n        train_losses.append(train_epoch_loss)\n\n        # Validation\n        model.eval()\n        v_loss = 0.0\n        v_total = 0\n        v_correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                rr_feats = compute_rr_entropy_features(xb).to(device)\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n                v_loss += loss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                v_correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n\n        val_epoch_loss = v_loss / max(1, v_total)\n        val_epoch_acc = (v_correct / max(1, v_total))\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n        if scheduler_type == 'onecycle':\n            scheduler.step()\n        elif scheduler_type == 'plateau':\n            scheduler.step(val_epoch_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_epoch_loss:.6f} val_loss={val_epoch_loss:.6f} val_acc={val_epoch_acc:.4f}\")\n\n    # -------- Post-Training Quantization (best-effort, keep model size <= 256KB) --------\n    def approximate_model_size_bytes(m):\n        nbytes = 0\n        for k, v in m.state_dict().items():\n            nbytes += v.numel() * v.element_size()\n        return int(nbytes)\n\n    q_model = None\n    if quantization_bits == 8 and quantize_weights:\n        try:\n            q_model = torch.ao.quantization.quantize_dynamic(\n                copy.deepcopy(model).to('cpu').eval(),\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n            if quantize_activations:\n                print(\"[Info] quantize_activations requested, but dynamic quantization only quantizes weights of Linear layers. Proceeding with weight-only int8.\")\n        except Exception as e:\n            print(f\"[Warn] Dynamic quantization failed: {e}. Falling back to FP32 CPU model.\")\n            q_model = copy.deepcopy(model).to('cpu').eval()\n    elif quantization_bits == 16:\n        q_model = copy.deepcopy(model).to('cpu').eval().half()\n    else:\n        q_model = copy.deepcopy(model).to('cpu').eval()\n\n    size_bytes = approximate_model_size_bytes(q_model)\n    if size_bytes > 256 * 1024:\n        print(f\"[Warn] Quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Consider reducing base_channels or enabling int8.\")\n    else:\n        print(f\"[Info] Quantized model size ‚âà {size_bytes/1024:.1f}KB (<= 256KB).\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'quantized_model_size_bytes': size_bytes\n    }\n\n    return q_model, metrics\n"}
2025-09-24 19:26:02,401 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-24 19:26:02,401 - ERROR - _models.training_function_executor - BO training objective failed: Given groups=1, weight of size [18, 2, 7], expected input[32, 1000, 2] to have 2 channels, but got 1000 channels instead
2025-09-24 19:26:02,401 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 49.061s
2025-09-24 19:26:02,401 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: Given groups=1, weight of size [18, 2, 7], expected input[32, 1000, 2] to have 2 channels, but got 1000 channels instead
2025-09-24 19:26:02,401 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-24 19:26:05,405 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-24 19:26:05,405 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-09-24 19:26:05,405 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:26:05,406 - INFO - _models.training_function_executor - Loaded training function: RRFusionRes1DCNN-5C
2025-09-24 19:26:05,406 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-24 19:26:05,406 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-09-24 19:26:05,406 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:26:05,406 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: RRFusionRes1DCNN-5C
2025-09-24 19:26:05,406 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-24 19:26:05,406 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 1/4
2025-09-24 19:26:05,406 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 1: üì¶ Installing dependencies for GPT-generated training code...
2025-09-24 19:26:05,406 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-24 19:26:05,408 - INFO - package_installer - Extracted imports from code: set()
2025-09-24 19:26:05,408 - INFO - package_installer - ‚úÖ No external packages required
2025-09-24 19:26:05,408 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-24 19:26:05,408 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:26:05,408 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-24 19:26:05,408 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'base_channels', 'loss_type', 'gamma', 'label_smoothing', 'scheduler', 'patience', 'use_amp', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibration_batches', 'beta_cb']
2025-09-24 19:26:05,408 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-24 19:26:05,408 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:26:05,408 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-24 19:26:05,440 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-24 19:26:05,471 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-24 19:26:05,471 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-24 19:26:05,471 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-24 19:26:05,472 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-24 19:26:05,472 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:26:05,472 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:26:05,472 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:26:05,472 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:26:05,472 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:26:05,473 - ERROR - _models.training_function_executor - Training execution failed: name 'torch' is not defined
2025-09-24 19:26:05,473 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-24 19:26:05,473 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-24 19:26:05,473 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-24 19:26:05,473 - INFO - _models.ai_code_generator - Prompt length: 12976 characters
2025-09-24 19:26:05,473 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-24 19:26:05,473 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-24 19:26:05,473 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-24 19:26:58,310 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-24 19:26:58,310 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-24 19:26:58,311 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250924_192658_attempt1.txt
2025-09-24 19:26:58,311 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a 5-class inter-patient 1D CNN + RR feature-fusion classifier.\n\n    Inputs:\n      - X_train, X_val: torch.Tensor of shape (N, 2, 1000) or (N, 1000, 2), dtype float32.\n      - y_train, y_val: torch.Tensor of shape (N,), dtype long.\n      - device: str or torch.device; training will run on GPU.\n    Hyperparameters (kwargs):\n      - lr (float), batch_size (int), epochs (int), weight_decay (float)\n      - dropout (float), base_channels (int)\n      - loss_type in {\"focal\", \"ce\", \"class_balanced\"}\n      - gamma (float, focal loss focusing)\n      - label_smoothing (float)\n      - scheduler in {\"none\", \"onecycle\", \"plateau\"}\n      - patience (int) for ReduceLROnPlateau\n      - use_amp (bool), grad_clip (float)\n      - quantization_bits in {8,16,32}, quantize_weights (bool), quantize_activations (bool)\n      - calibration_batches (int) for PTQ calibration (best-effort; dynamic quant used by default)\n    Returns: (quantized_model, metrics_dict) where metrics_dict has lists: train_losses, val_losses, val_acc\n    \"\"\"\n    # Local imports to ensure required symbols exist\n    import math\n    import copy\n    import torch\n    from torch import nn\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch.cuda.amp import autocast, GradScaler\n\n    # Helper: ensure channels-first layout (B, 2, L)\n    def _to_channels_first(X):\n        if X.dim() == 3:\n            # If last dim is channels=2 and middle is length (e.g., (B,1000,2)), permute\n            if X.shape[-1] == 2 and X.shape[1] != 2:\n                return X.permute(0, 2, 1).contiguous()\n        return X\n\n    # Robust device handling\n    device = torch.device(device)\n    if device.type != 'cuda':\n        if torch.cuda.is_available():\n            print(\"[Info] Overriding device to CUDA for training per requirement.\")\n            device = torch.device('cuda')\n        else:\n            raise RuntimeError(\"CUDA device required for training per spec but not available.\")\n\n    torch.backends.cudnn.benchmark = True\n\n    # Defaults\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 128))\n    epochs = int(kwargs.get('epochs', 15))\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    dropout = float(kwargs.get('dropout', 0.1))\n    base_channels = int(kwargs.get('base_channels', 16))\n    loss_type = str(kwargs.get('loss_type', 'focal'))\n    gamma = float(kwargs.get('gamma', 2.0))\n    label_smoothing = float(kwargs.get('label_smoothing', 0.0))\n    scheduler_type = str(kwargs.get('scheduler', 'onecycle'))\n    patience = int(kwargs.get('patience', 5))\n    use_amp = bool(kwargs.get('use_amp', True))\n    grad_clip = float(kwargs.get('grad_clip', 1.0))\n\n    quantization_bits = int(kwargs.get('quantization_bits', 8))\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))\n    calibration_batches = int(kwargs.get('calibration_batches', 2))\n\n    num_classes = 5\n\n    # Datasets and loaders\n    # Ensure dtypes and channels-first memory layout\n    X_train = _to_channels_first(X_train.float())\n    X_val = _to_channels_first(X_val.float())\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    # Validate expected shape after fix\n    if X_train.dim() != 3 or X_train.shape[1] != 2:\n        raise ValueError(f\"X_train must be (N,2,L). Got {tuple(X_train.shape)}\")\n    if X_val.dim() != 3 or X_val.shape[1] != 2:\n        raise ValueError(f\"X_val must be (N,2,L). Got {tuple(X_val.shape)}\")\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=False, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=0)\n\n    # Model (assumes RRFusionRes1DCNN is defined elsewhere)\n    model = RRFusionRes1DCNN(in_ch=2, base_ch=base_channels, num_classes=num_classes, dropout=dropout, rr_feat_dim=5, rr_embed=8)\n    model = model.to(device)\n\n    # Loss (assumes FocalLoss and make_class_weights are defined elsewhere)\n    class_weights = make_class_weights(y_train, num_classes, smoothing=0.0, device=device)\n    if loss_type == 'focal':\n        criterion = FocalLoss(gamma=gamma, weight=class_weights, label_smoothing=label_smoothing).to(device)\n    elif loss_type == 'class_balanced':\n        beta = kwargs.get('beta_cb', 0.999)\n        counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n        eff_num = (1.0 - torch.pow(torch.tensor(beta), counts)) / (1.0 - beta + 1e-8)\n        cb_w = (eff_num.sum() / (eff_num + 1e-8))\n        cb_w = (cb_w / cb_w.mean()).to(device)\n        criterion = nn.CrossEntropyLoss(weight=cb_w, label_smoothing=label_smoothing).to(device)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    if scheduler_type == 'onecycle':\n        steps_per_epoch = max(1, len(train_loader))\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n    elif scheduler_type == 'plateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=0.5, min_lr=1e-6)\n    else:\n        scheduler = None\n\n    scaler = GradScaler(enabled=use_amp)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            # Ensure batch is channels-first in case upstream data varies\n            if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Assumes compute_rr_entropy_features is defined elsewhere\n            rr_feats = compute_rr_entropy_features(xb).to(device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=use_amp):\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            if grad_clip and grad_clip > 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            # OneCycleLR should step per batch\n            if scheduler_type == 'onecycle':\n                scheduler.step()\n\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n\n        train_epoch_loss = running_loss / max(1, total)\n        train_losses.append(train_epoch_loss)\n\n        # Validation\n        model.eval()\n        v_loss = 0.0\n        v_total = 0\n        v_correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                rr_feats = compute_rr_entropy_features(xb).to(device)\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n                v_loss += loss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                v_correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n\n        val_epoch_loss = v_loss / max(1, v_total)\n        val_epoch_acc = (v_correct / max(1, v_total))\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n        if scheduler_type == 'plateau':\n            scheduler.step(val_epoch_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_epoch_loss:.6f} val_loss={val_epoch_loss:.6f} val_acc={val_epoch_acc:.4f}\")\n\n    # -------- Post-Training Quantization (best-effort, keep model size <= 256KB) --------\n    def approximate_model_size_bytes(m):\n        nbytes = 0\n        for k, v in m.state_dict().items():\n            nbytes += v.numel() * v.element_size()\n        return int(nbytes)\n\n    q_model = None\n    if quantization_bits == 8 and quantize_weights:\n        try:\n            q_model = torch.ao.quantization.quantize_dynamic(\n                copy.deepcopy(model).to('cpu').eval(),\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n            if quantize_activations:\n                print(\"[Info] quantize_activations requested, but dynamic quantization only quantizes weights of Linear layers. Proceeding with weight-only int8.\")\n        except Exception as e:\n            print(f\"[Warn] Dynamic quantization failed: {e}. Falling back to FP32 CPU model.\")\n            q_model = copy.deepcopy(model).to('cpu').eval()\n    elif quantization_bits == 16:\n        q_model = copy.deepcopy(model).to('cpu').eval().half()\n    else:\n        q_model = copy.deepcopy(model).to('cpu').eval()\n\n    size_bytes = approximate_model_size_bytes(q_model)\n    if size_bytes > 256 * 1024:\n        print(f\"[Warn] Quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Consider reducing base_channels or enabling int8.\")\n    else:\n        print(f\"[Info] Quantized model size ‚âà {size_bytes/1024:.1f}KB (<= 256KB).\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'quantized_model_size_bytes': size_bytes\n    }\n\n    return q_model, metrics\n"}
2025-09-24 19:26:58,311 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-24 19:26:58,311 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a 5-class inter-patient 1D CNN + RR feature-fusion classifier.\n\n    Inputs:\n      - X_train, X_val: torch.Tensor of shape (N, 2, 1000) or (N, 1000, 2), dtype float32.\n      - y_train, y_val: torch.Tensor of shape (N,), dtype long.\n      - device: str or torch.device; training will run on GPU.\n    Hyperparameters (kwargs):\n      - lr (float), batch_size (int), epochs (int), weight_decay (float)\n      - dropout (float), base_channels (int)\n      - loss_type in {\"focal\", \"ce\", \"class_balanced\"}\n      - gamma (float, focal loss focusing)\n      - label_smoothing (float)\n      - scheduler in {\"none\", \"onecycle\", \"plateau\"}\n      - patience (int) for ReduceLROnPlateau\n      - use_amp (bool), grad_clip (float)\n      - quantization_bits in {8,16,32}, quantize_weights (bool), quantize_activations (bool)\n      - calibration_batches (int) for PTQ calibration (best-effort; dynamic quant used by default)\n    Returns: (quantized_model, metrics_dict) where metrics_dict has lists: train_losses, val_losses, val_acc\n    \"\"\"\n    # Local imports to ensure required symbols exist\n    import math\n    import copy\n    import torch\n    from torch import nn\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch.cuda.amp import autocast, GradScaler\n\n    # Helper: ensure channels-first layout (B, 2, L)\n    def _to_channels_first(X):\n        if X.dim() == 3:\n            # If last dim is channels=2 and middle is length (e.g., (B,1000,2)), permute\n            if X.shape[-1] == 2 and X.shape[1] != 2:\n                return X.permute(0, 2, 1).contiguous()\n        return X\n\n    # Robust device handling\n    device = torch.device(device)\n    if device.type != 'cuda':\n        if torch.cuda.is_available():\n            print(\"[Info] Overriding device to CUDA for training per requirement.\")\n            device = torch.device('cuda')\n        else:\n            raise RuntimeError(\"CUDA device required for training per spec but not available.\")\n\n    torch.backends.cudnn.benchmark = True\n\n    # Defaults\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 128))\n    epochs = int(kwargs.get('epochs', 15))\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    dropout = float(kwargs.get('dropout', 0.1))\n    base_channels = int(kwargs.get('base_channels', 16))\n    loss_type = str(kwargs.get('loss_type', 'focal'))\n    gamma = float(kwargs.get('gamma', 2.0))\n    label_smoothing = float(kwargs.get('label_smoothing', 0.0))\n    scheduler_type = str(kwargs.get('scheduler', 'onecycle'))\n    patience = int(kwargs.get('patience', 5))\n    use_amp = bool(kwargs.get('use_amp', True))\n    grad_clip = float(kwargs.get('grad_clip', 1.0))\n\n    quantization_bits = int(kwargs.get('quantization_bits', 8))\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))\n    calibration_batches = int(kwargs.get('calibration_batches', 2))\n\n    num_classes = 5\n\n    # Datasets and loaders\n    # Ensure dtypes and channels-first memory layout\n    X_train = _to_channels_first(X_train.float())\n    X_val = _to_channels_first(X_val.float())\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    # Validate expected shape after fix\n    if X_train.dim() != 3 or X_train.shape[1] != 2:\n        raise ValueError(f\"X_train must be (N,2,L). Got {tuple(X_train.shape)}\")\n    if X_val.dim() != 3 or X_val.shape[1] != 2:\n        raise ValueError(f\"X_val must be (N,2,L). Got {tuple(X_val.shape)}\")\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=False, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=0)\n\n    # Model (assumes RRFusionRes1DCNN is defined elsewhere)\n    model = RRFusionRes1DCNN(in_ch=2, base_ch=base_channels, num_classes=num_classes, dropout=dropout, rr_feat_dim=5, rr_embed=8)\n    model = model.to(device)\n\n    # Loss (assumes FocalLoss and make_class_weights are defined elsewhere)\n    class_weights = make_class_weights(y_train, num_classes, smoothing=0.0, device=device)\n    if loss_type == 'focal':\n        criterion = FocalLoss(gamma=gamma, weight=class_weights, label_smoothing=label_smoothing).to(device)\n    elif loss_type == 'class_balanced':\n        beta = kwargs.get('beta_cb', 0.999)\n        counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n        eff_num = (1.0 - torch.pow(torch.tensor(beta), counts)) / (1.0 - beta + 1e-8)\n        cb_w = (eff_num.sum() / (eff_num + 1e-8))\n        cb_w = (cb_w / cb_w.mean()).to(device)\n        criterion = nn.CrossEntropyLoss(weight=cb_w, label_smoothing=label_smoothing).to(device)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    if scheduler_type == 'onecycle':\n        steps_per_epoch = max(1, len(train_loader))\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n    elif scheduler_type == 'plateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=0.5, min_lr=1e-6)\n    else:\n        scheduler = None\n\n    scaler = GradScaler(enabled=use_amp)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            # Ensure batch is channels-first in case upstream data varies\n            if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            # Assumes compute_rr_entropy_features is defined elsewhere\n            rr_feats = compute_rr_entropy_features(xb).to(device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=use_amp):\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            if grad_clip and grad_clip > 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            # OneCycleLR should step per batch\n            if scheduler_type == 'onecycle':\n                scheduler.step()\n\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n\n        train_epoch_loss = running_loss / max(1, total)\n        train_losses.append(train_epoch_loss)\n\n        # Validation\n        model.eval()\n        v_loss = 0.0\n        v_total = 0\n        v_correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                rr_feats = compute_rr_entropy_features(xb).to(device)\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n                v_loss += loss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                v_correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n\n        val_epoch_loss = v_loss / max(1, v_total)\n        val_epoch_acc = (v_correct / max(1, v_total))\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n        if scheduler_type == 'plateau':\n            scheduler.step(val_epoch_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_epoch_loss:.6f} val_loss={val_epoch_loss:.6f} val_acc={val_epoch_acc:.4f}\")\n\n    # -------- Post-Training Quantization (best-effort, keep model size <= 256KB) --------\n    def approximate_model_size_bytes(m):\n        nbytes = 0\n        for k, v in m.state_dict().items():\n            nbytes += v.numel() * v.element_size()\n        return int(nbytes)\n\n    q_model = None\n    if quantization_bits == 8 and quantize_weights:\n        try:\n            q_model = torch.ao.quantization.quantize_dynamic(\n                copy.deepcopy(model).to('cpu').eval(),\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n            if quantize_activations:\n                print(\"[Info] quantize_activations requested, but dynamic quantization only quantizes weights of Linear layers. Proceeding with weight-only int8.\")\n        except Exception as e:\n            print(f\"[Warn] Dynamic quantization failed: {e}. Falling back to FP32 CPU model.\")\n            q_model = copy.deepcopy(model).to('cpu').eval()\n    elif quantization_bits == 16:\n        q_model = copy.deepcopy(model).to('cpu').eval().half()\n    else:\n        q_model = copy.deepcopy(model).to('cpu').eval()\n\n    size_bytes = approximate_model_size_bytes(q_model)\n    if size_bytes > 256 * 1024:\n        print(f\"[Warn] Quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Consider reducing base_channels or enabling int8.\")\n    else:\n        print(f\"[Info] Quantized model size ‚âà {size_bytes/1024:.1f}KB (<= 256KB).\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'quantized_model_size_bytes': size_bytes\n    }\n\n    return q_model, metrics\n"}
2025-09-24 19:26:58,311 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-24 19:26:58,311 - ERROR - _models.training_function_executor - BO training objective failed: name 'torch' is not defined
2025-09-24 19:26:58,311 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 52.839s
2025-09-24 19:26:58,311 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name 'torch' is not defined
2025-09-24 19:26:58,311 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-24 19:27:01,314 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-24 19:27:01,314 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-09-24 19:27:01,314 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:27:01,314 - INFO - _models.training_function_executor - Loaded training function: RRFusionRes1DCNN-5C
2025-09-24 19:27:01,314 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-24 19:27:01,314 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-09-24 19:27:01,315 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:27:01,315 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: RRFusionRes1DCNN-5C
2025-09-24 19:27:01,315 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-24 19:27:01,315 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 2/4
2025-09-24 19:27:01,315 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 2: üì¶ Installing dependencies for GPT-generated training code...
2025-09-24 19:27:01,315 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-24 19:27:01,316 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-24 19:27:01,316 - INFO - package_installer - Available packages: {'torch'}
2025-09-24 19:27:01,316 - INFO - package_installer - Missing packages: set()
2025-09-24 19:27:01,317 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-24 19:27:01,317 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-24 19:27:01,317 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:27:01,317 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-24 19:27:01,317 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'base_channels', 'loss_type', 'gamma', 'label_smoothing', 'scheduler', 'patience', 'use_amp', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibration_batches', 'beta_cb']
2025-09-24 19:27:01,317 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-24 19:27:01,317 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:27:01,317 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-24 19:27:01,352 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-24 19:27:01,383 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-24 19:27:01,383 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-24 19:27:01,384 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-24 19:27:01,384 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-24 19:27:01,384 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:27:01,385 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:27:01,385 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:27:01,385 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:27:01,385 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:27:01,386 - ERROR - _models.training_function_executor - Training execution failed: name 'RRFusionRes1DCNN' is not defined
2025-09-24 19:27:01,386 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-24 19:27:01,386 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-24 19:27:01,386 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-24 19:27:01,386 - INFO - _models.ai_code_generator - Prompt length: 13453 characters
2025-09-24 19:27:01,386 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-24 19:27:01,386 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-24 19:27:01,386 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-24 19:28:26,681 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-24 19:28:26,682 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-24 19:28:26,683 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250924_192826_attempt1.txt
2025-09-24 19:28:26,683 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a 5-class inter-patient 1D CNN + RR feature-fusion classifier.\n\n    Inputs:\n      - X_train, X_val: torch.Tensor of shape (N, 2, 1000) or (N, 1000, 2), dtype float32.\n      - y_train, y_val: torch.Tensor of shape (N,), dtype long.\n      - device: str or torch.device; training will run on GPU.\n    Hyperparameters (kwargs):\n      - lr (float), batch_size (int), epochs (int), weight_decay (float)\n      - dropout (float), base_channels (int)\n      - loss_type in {\"focal\", \"ce\", \"class_balanced\"}\n      - gamma (float, focal loss focusing)\n      - label_smoothing (float)\n      - scheduler in {\"none\", \"onecycle\", \"plateau\"}\n      - patience (int) for ReduceLROnPlateau\n      - use_amp (bool), grad_clip (float)\n      - quantization_bits in {8,16,32}, quantize_weights (bool), quantize_activations (bool)\n      - calibration_batches (int) for PTQ calibration (best-effort; dynamic quant used by default)\n    Returns: (quantized_model, metrics_dict) where metrics_dict has lists: train_losses, val_losses, val_acc\n    \"\"\"\n    import math\n    import copy\n    import torch\n    from torch import nn\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch.cuda.amp import autocast, GradScaler\n    import torch.nn.functional as F\n\n    # -------------------- Helper: ensure channels-first layout (B, 2, L) --------------------\n    def _to_channels_first(X):\n        if X.dim() == 3:\n            if X.shape[-1] == 2 and X.shape[1] != 2:\n                return X.permute(0, 2, 1).contiguous()\n        return X\n\n    # -------------------- Robust device handling --------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        if torch.cuda.is_available():\n            print(\"[Info] Overriding device to CUDA for training per requirement.\")\n            device = torch.device('cuda')\n        else:\n            raise RuntimeError(\"CUDA device required for training per spec but not available.\")\n\n    torch.backends.cudnn.benchmark = True\n\n    # -------------------- Defaults --------------------\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 128))\n    epochs = int(kwargs.get('epochs', 15))\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    dropout = float(kwargs.get('dropout', 0.1))\n    base_channels = int(kwargs.get('base_channels', 16))\n    loss_type = str(kwargs.get('loss_type', 'focal'))\n    gamma = float(kwargs.get('gamma', 2.0))\n    label_smoothing = float(kwargs.get('label_smoothing', 0.0))\n    scheduler_type = str(kwargs.get('scheduler', 'onecycle'))\n    patience = int(kwargs.get('patience', 5))\n    use_amp = bool(kwargs.get('use_amp', True))\n    grad_clip = float(kwargs.get('grad_clip', 1.0))\n\n    quantization_bits = int(kwargs.get('quantization_bits', 8))\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))\n    calibration_batches = int(kwargs.get('calibration_batches', 2))\n\n    num_classes = 5\n\n    # -------------------- Datasets and loaders --------------------\n    X_train = _to_channels_first(X_train.float())\n    X_val = _to_channels_first(X_val.float())\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    if X_train.dim() != 3 or X_train.shape[1] != 2:\n        raise ValueError(f\"X_train must be (N,2,L). Got {tuple(X_train.shape)}\")\n    if X_val.dim() != 3 or X_val.shape[1] != 2:\n        raise ValueError(f\"X_val must be (N,2,L). Got {tuple(X_val.shape)}\")\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=False, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=0)\n\n    # -------------------- Define missing components --------------------\n    class ResBlock1D(nn.Module):\n        def __init__(self, ch, dropout=0.0, kernel_size=7):\n            super().__init__()\n            pad = kernel_size // 2\n            self.conv1 = nn.Conv1d(ch, ch, kernel_size=kernel_size, padding=pad, bias=False)\n            self.bn1 = nn.BatchNorm1d(ch)\n            self.conv2 = nn.Conv1d(ch, ch, kernel_size=kernel_size, padding=pad, bias=False)\n            self.bn2 = nn.BatchNorm1d(ch)\n            self.act = nn.ReLU(inplace=True)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):\n            out = self.conv1(x)\n            out = self.bn1(out)\n            out = self.act(out)\n            out = self.drop(out)\n            out = self.conv2(out)\n            out = self.bn2(out)\n            out = out + x\n            out = self.act(out)\n            return out\n\n    class RRFusionRes1DCNN(nn.Module):\n        def __init__(self, in_ch=2, base_ch=16, num_classes=5, dropout=0.1, rr_feat_dim=5, rr_embed=8):\n            super().__init__()\n            ch1 = base_ch\n            ch2 = base_ch * 2\n            ch3 = base_ch * 4\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, ch1, kernel_size=9, padding=4, bias=False),\n                nn.BatchNorm1d(ch1),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.layer1 = nn.Sequential(\n                ResBlock1D(ch1, dropout=dropout, kernel_size=7),\n                nn.Conv1d(ch1, ch2, kernel_size=5, stride=2, padding=2, bias=False),\n                nn.BatchNorm1d(ch2),\n                nn.ReLU(inplace=True),\n            )\n            self.layer2 = nn.Sequential(\n                ResBlock1D(ch2, dropout=dropout, kernel_size=5),\n                nn.Conv1d(ch2, ch3, kernel_size=3, stride=2, padding=1, bias=False),\n                nn.BatchNorm1d(ch3),\n                nn.ReLU(inplace=True),\n                ResBlock1D(ch3, dropout=dropout, kernel_size=3),\n            )\n            self.gap = nn.AdaptiveAvgPool1d(1)\n            self.ecg_proj = nn.Sequential(\n                nn.Linear(ch3, ch3),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.rr_embed = nn.Sequential(\n                nn.Linear(rr_feat_dim, rr_embed),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.classifier = nn.Sequential(\n                nn.Linear(ch3 + rr_embed, max(32, base_ch * 4)),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(max(32, base_ch * 4), num_classes),\n            )\n        def forward(self, x, rr_feats):\n            x = self.stem(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.gap(x).squeeze(-1)  # (B, ch3)\n            x = self.ecg_proj(x)\n            rr = self.rr_embed(rr_feats)\n            z = torch.cat([x, rr], dim=1)\n            logits = self.classifier(z)\n            return logits\n\n    def make_class_weights(y, num_classes, smoothing=0.0, device='cpu'):\n        counts = torch.bincount(y.cpu(), minlength=num_classes).float()\n        eps = 1e-6\n        inv = 1.0 / (counts + eps)\n        w = inv / inv.mean()\n        return w.to(device)\n\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0, reduction='mean'):\n            super().__init__()\n            self.gamma = gamma\n            self.register_buffer('weight', weight if weight is not None else None)\n            self.label_smoothing = float(label_smoothing)\n            self.reduction = reduction\n        def forward(self, logits, target):\n            num_classes = logits.size(1)\n            log_probs = F.log_softmax(logits, dim=1)\n            probs = log_probs.exp()\n            one_hot = F.one_hot(target, num_classes=num_classes).float()\n            if self.label_smoothing > 0.0:\n                one_hot = one_hot * (1.0 - self.label_smoothing) + self.label_smoothing / num_classes\n            ce = -(one_hot * log_probs).sum(dim=1)\n            pt = (one_hot * probs).sum(dim=1).clamp_min(1e-6)\n            loss = ((1.0 - pt) ** self.gamma) * ce\n            if self.weight is not None:\n                w_per = (one_hot * self.weight.unsqueeze(0)).sum(dim=1)\n                loss = loss * w_per\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            else:\n                return loss\n\n    def compute_rr_entropy_features(x):\n        # x: (B,2,L). Use lead-0 to compute simple surrogate RR-like features.\n        sig = x[:, 0, :]\n        B, L = sig.shape\n        eps = 1e-8\n        mean = sig.mean(dim=1)\n        std = sig.std(dim=1, unbiased=False)\n        abs_mean = sig.abs().mean(dim=1)\n        ptp = sig.max(dim=1).values - sig.min(dim=1).values\n        # Spectral entropy\n        spec = torch.fft.rfft(sig, dim=1)\n        psd = (spec.real.pow(2) + spec.imag.pow(2)) + eps\n        p = psd / psd.sum(dim=1, keepdim=True)\n        se = -(p * (p + eps).log()).sum(dim=1)\n        se = se / torch.log(torch.tensor(p.size(1), device=p.device, dtype=p.dtype))\n        feats = torch.stack([mean, std, abs_mean, ptp, se], dim=1)\n        return feats\n\n    # -------------------- Model --------------------\n    model = RRFusionRes1DCNN(in_ch=2, base_ch=base_channels, num_classes=num_classes, dropout=dropout, rr_feat_dim=5, rr_embed=8)\n    model = model.to(device)\n\n    # -------------------- Loss --------------------\n    class_weights = make_class_weights(y_train, num_classes, smoothing=0.0, device=device)\n    if loss_type == 'focal':\n        criterion = FocalLoss(gamma=gamma, weight=class_weights, label_smoothing=label_smoothing).to(device)\n    elif loss_type == 'class_balanced':\n        beta = float(kwargs.get('beta_cb', 0.999))\n        counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n        eff_num = (1.0 - torch.pow(torch.tensor(beta), counts)) / (1.0 - beta + 1e-8)\n        cb_w = (eff_num.sum() / (eff_num + 1e-8))\n        cb_w = (cb_w / cb_w.mean()).to(device)\n        criterion = nn.CrossEntropyLoss(weight=cb_w, label_smoothing=label_smoothing).to(device)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    if scheduler_type == 'onecycle':\n        steps_per_epoch = max(1, len(train_loader))\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n    elif scheduler_type == 'plateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=0.5, min_lr=1e-6)\n    else:\n        scheduler = None\n\n    scaler = GradScaler(enabled=use_amp)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            rr_feats = compute_rr_entropy_features(xb).to(device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=use_amp):\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            if grad_clip and grad_clip > 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            if scheduler_type == 'onecycle':\n                scheduler.step()\n\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n\n        train_epoch_loss = running_loss / max(1, total)\n        train_losses.append(train_epoch_loss)\n\n        # Validation\n        model.eval()\n        v_loss = 0.0\n        v_total = 0\n        v_correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                rr_feats = compute_rr_entropy_features(xb).to(device)\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n                v_loss += loss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                v_correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n\n        val_epoch_loss = v_loss / max(1, v_total)\n        val_epoch_acc = (v_correct / max(1, v_total))\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n        if scheduler_type == 'plateau':\n            scheduler.step(val_epoch_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_epoch_loss:.6f} val_loss={val_epoch_loss:.6f} val_acc={val_epoch_acc:.4f}\")\n\n    # -------- Post-Training Quantization (best-effort, keep model size <= 256KB) --------\n    def approximate_model_size_bytes(m):\n        nbytes = 0\n        for k, v in m.state_dict().items():\n            nbytes += v.numel() * v.element_size()\n        return int(nbytes)\n\n    q_model = None\n    if quantization_bits == 8 and quantize_weights:\n        try:\n            q_model = torch.ao.quantization.quantize_dynamic(\n                copy.deepcopy(model).to('cpu').eval(),\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n            if quantize_activations:\n                print(\"[Info] quantize_activations requested, but dynamic quantization only quantizes weights of Linear layers. Proceeding with weight-only int8.\")\n        except Exception as e:\n            print(f\"[Warn] Dynamic quantization failed: {e}. Falling back to FP32 CPU model.\")\n            q_model = copy.deepcopy(model).to('cpu').eval()\n    elif quantization_bits == 16:\n        q_model = copy.deepcopy(model).to('cpu').eval().half()\n    else:\n        q_model = copy.deepcopy(model).to('cpu').eval()\n\n    size_bytes = approximate_model_size_bytes(q_model)\n    if size_bytes > 256 * 1024:\n        print(f\"[Warn] Quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Consider reducing base_channels or enabling int8.\")\n    else:\n        print(f\"[Info] Quantized model size ‚âà {size_bytes/1024:.1f}KB (<= 256KB).\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'quantized_model_size_bytes': size_bytes\n    }\n\n    return q_model, metrics\n"}
2025-09-24 19:28:26,683 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-24 19:28:26,683 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a 5-class inter-patient 1D CNN + RR feature-fusion classifier.\n\n    Inputs:\n      - X_train, X_val: torch.Tensor of shape (N, 2, 1000) or (N, 1000, 2), dtype float32.\n      - y_train, y_val: torch.Tensor of shape (N,), dtype long.\n      - device: str or torch.device; training will run on GPU.\n    Hyperparameters (kwargs):\n      - lr (float), batch_size (int), epochs (int), weight_decay (float)\n      - dropout (float), base_channels (int)\n      - loss_type in {\"focal\", \"ce\", \"class_balanced\"}\n      - gamma (float, focal loss focusing)\n      - label_smoothing (float)\n      - scheduler in {\"none\", \"onecycle\", \"plateau\"}\n      - patience (int) for ReduceLROnPlateau\n      - use_amp (bool), grad_clip (float)\n      - quantization_bits in {8,16,32}, quantize_weights (bool), quantize_activations (bool)\n      - calibration_batches (int) for PTQ calibration (best-effort; dynamic quant used by default)\n    Returns: (quantized_model, metrics_dict) where metrics_dict has lists: train_losses, val_losses, val_acc\n    \"\"\"\n    import math\n    import copy\n    import torch\n    from torch import nn\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch.cuda.amp import autocast, GradScaler\n    import torch.nn.functional as F\n\n    # -------------------- Helper: ensure channels-first layout (B, 2, L) --------------------\n    def _to_channels_first(X):\n        if X.dim() == 3:\n            if X.shape[-1] == 2 and X.shape[1] != 2:\n                return X.permute(0, 2, 1).contiguous()\n        return X\n\n    # -------------------- Robust device handling --------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        if torch.cuda.is_available():\n            print(\"[Info] Overriding device to CUDA for training per requirement.\")\n            device = torch.device('cuda')\n        else:\n            raise RuntimeError(\"CUDA device required for training per spec but not available.\")\n\n    torch.backends.cudnn.benchmark = True\n\n    # -------------------- Defaults --------------------\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 128))\n    epochs = int(kwargs.get('epochs', 15))\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    dropout = float(kwargs.get('dropout', 0.1))\n    base_channels = int(kwargs.get('base_channels', 16))\n    loss_type = str(kwargs.get('loss_type', 'focal'))\n    gamma = float(kwargs.get('gamma', 2.0))\n    label_smoothing = float(kwargs.get('label_smoothing', 0.0))\n    scheduler_type = str(kwargs.get('scheduler', 'onecycle'))\n    patience = int(kwargs.get('patience', 5))\n    use_amp = bool(kwargs.get('use_amp', True))\n    grad_clip = float(kwargs.get('grad_clip', 1.0))\n\n    quantization_bits = int(kwargs.get('quantization_bits', 8))\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))\n    calibration_batches = int(kwargs.get('calibration_batches', 2))\n\n    num_classes = 5\n\n    # -------------------- Datasets and loaders --------------------\n    X_train = _to_channels_first(X_train.float())\n    X_val = _to_channels_first(X_val.float())\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    if X_train.dim() != 3 or X_train.shape[1] != 2:\n        raise ValueError(f\"X_train must be (N,2,L). Got {tuple(X_train.shape)}\")\n    if X_val.dim() != 3 or X_val.shape[1] != 2:\n        raise ValueError(f\"X_val must be (N,2,L). Got {tuple(X_val.shape)}\")\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=False, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=0)\n\n    # -------------------- Define missing components --------------------\n    class ResBlock1D(nn.Module):\n        def __init__(self, ch, dropout=0.0, kernel_size=7):\n            super().__init__()\n            pad = kernel_size // 2\n            self.conv1 = nn.Conv1d(ch, ch, kernel_size=kernel_size, padding=pad, bias=False)\n            self.bn1 = nn.BatchNorm1d(ch)\n            self.conv2 = nn.Conv1d(ch, ch, kernel_size=kernel_size, padding=pad, bias=False)\n            self.bn2 = nn.BatchNorm1d(ch)\n            self.act = nn.ReLU(inplace=True)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):\n            out = self.conv1(x)\n            out = self.bn1(out)\n            out = self.act(out)\n            out = self.drop(out)\n            out = self.conv2(out)\n            out = self.bn2(out)\n            out = out + x\n            out = self.act(out)\n            return out\n\n    class RRFusionRes1DCNN(nn.Module):\n        def __init__(self, in_ch=2, base_ch=16, num_classes=5, dropout=0.1, rr_feat_dim=5, rr_embed=8):\n            super().__init__()\n            ch1 = base_ch\n            ch2 = base_ch * 2\n            ch3 = base_ch * 4\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, ch1, kernel_size=9, padding=4, bias=False),\n                nn.BatchNorm1d(ch1),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.layer1 = nn.Sequential(\n                ResBlock1D(ch1, dropout=dropout, kernel_size=7),\n                nn.Conv1d(ch1, ch2, kernel_size=5, stride=2, padding=2, bias=False),\n                nn.BatchNorm1d(ch2),\n                nn.ReLU(inplace=True),\n            )\n            self.layer2 = nn.Sequential(\n                ResBlock1D(ch2, dropout=dropout, kernel_size=5),\n                nn.Conv1d(ch2, ch3, kernel_size=3, stride=2, padding=1, bias=False),\n                nn.BatchNorm1d(ch3),\n                nn.ReLU(inplace=True),\n                ResBlock1D(ch3, dropout=dropout, kernel_size=3),\n            )\n            self.gap = nn.AdaptiveAvgPool1d(1)\n            self.ecg_proj = nn.Sequential(\n                nn.Linear(ch3, ch3),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.rr_embed = nn.Sequential(\n                nn.Linear(rr_feat_dim, rr_embed),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.classifier = nn.Sequential(\n                nn.Linear(ch3 + rr_embed, max(32, base_ch * 4)),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(max(32, base_ch * 4), num_classes),\n            )\n        def forward(self, x, rr_feats):\n            x = self.stem(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.gap(x).squeeze(-1)  # (B, ch3)\n            x = self.ecg_proj(x)\n            rr = self.rr_embed(rr_feats)\n            z = torch.cat([x, rr], dim=1)\n            logits = self.classifier(z)\n            return logits\n\n    def make_class_weights(y, num_classes, smoothing=0.0, device='cpu'):\n        counts = torch.bincount(y.cpu(), minlength=num_classes).float()\n        eps = 1e-6\n        inv = 1.0 / (counts + eps)\n        w = inv / inv.mean()\n        return w.to(device)\n\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0, reduction='mean'):\n            super().__init__()\n            self.gamma = gamma\n            self.register_buffer('weight', weight if weight is not None else None)\n            self.label_smoothing = float(label_smoothing)\n            self.reduction = reduction\n        def forward(self, logits, target):\n            num_classes = logits.size(1)\n            log_probs = F.log_softmax(logits, dim=1)\n            probs = log_probs.exp()\n            one_hot = F.one_hot(target, num_classes=num_classes).float()\n            if self.label_smoothing > 0.0:\n                one_hot = one_hot * (1.0 - self.label_smoothing) + self.label_smoothing / num_classes\n            ce = -(one_hot * log_probs).sum(dim=1)\n            pt = (one_hot * probs).sum(dim=1).clamp_min(1e-6)\n            loss = ((1.0 - pt) ** self.gamma) * ce\n            if self.weight is not None:\n                w_per = (one_hot * self.weight.unsqueeze(0)).sum(dim=1)\n                loss = loss * w_per\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            else:\n                return loss\n\n    def compute_rr_entropy_features(x):\n        # x: (B,2,L). Use lead-0 to compute simple surrogate RR-like features.\n        sig = x[:, 0, :]\n        B, L = sig.shape\n        eps = 1e-8\n        mean = sig.mean(dim=1)\n        std = sig.std(dim=1, unbiased=False)\n        abs_mean = sig.abs().mean(dim=1)\n        ptp = sig.max(dim=1).values - sig.min(dim=1).values\n        # Spectral entropy\n        spec = torch.fft.rfft(sig, dim=1)\n        psd = (spec.real.pow(2) + spec.imag.pow(2)) + eps\n        p = psd / psd.sum(dim=1, keepdim=True)\n        se = -(p * (p + eps).log()).sum(dim=1)\n        se = se / torch.log(torch.tensor(p.size(1), device=p.device, dtype=p.dtype))\n        feats = torch.stack([mean, std, abs_mean, ptp, se], dim=1)\n        return feats\n\n    # -------------------- Model --------------------\n    model = RRFusionRes1DCNN(in_ch=2, base_ch=base_channels, num_classes=num_classes, dropout=dropout, rr_feat_dim=5, rr_embed=8)\n    model = model.to(device)\n\n    # -------------------- Loss --------------------\n    class_weights = make_class_weights(y_train, num_classes, smoothing=0.0, device=device)\n    if loss_type == 'focal':\n        criterion = FocalLoss(gamma=gamma, weight=class_weights, label_smoothing=label_smoothing).to(device)\n    elif loss_type == 'class_balanced':\n        beta = float(kwargs.get('beta_cb', 0.999))\n        counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n        eff_num = (1.0 - torch.pow(torch.tensor(beta), counts)) / (1.0 - beta + 1e-8)\n        cb_w = (eff_num.sum() / (eff_num + 1e-8))\n        cb_w = (cb_w / cb_w.mean()).to(device)\n        criterion = nn.CrossEntropyLoss(weight=cb_w, label_smoothing=label_smoothing).to(device)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    if scheduler_type == 'onecycle':\n        steps_per_epoch = max(1, len(train_loader))\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n    elif scheduler_type == 'plateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=0.5, min_lr=1e-6)\n    else:\n        scheduler = None\n\n    scaler = GradScaler(enabled=use_amp)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            rr_feats = compute_rr_entropy_features(xb).to(device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=use_amp):\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            if grad_clip and grad_clip > 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            if scheduler_type == 'onecycle':\n                scheduler.step()\n\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n\n        train_epoch_loss = running_loss / max(1, total)\n        train_losses.append(train_epoch_loss)\n\n        # Validation\n        model.eval()\n        v_loss = 0.0\n        v_total = 0\n        v_correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                rr_feats = compute_rr_entropy_features(xb).to(device)\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n                v_loss += loss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                v_correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n\n        val_epoch_loss = v_loss / max(1, v_total)\n        val_epoch_acc = (v_correct / max(1, v_total))\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n        if scheduler_type == 'plateau':\n            scheduler.step(val_epoch_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_epoch_loss:.6f} val_loss={val_epoch_loss:.6f} val_acc={val_epoch_acc:.4f}\")\n\n    # -------- Post-Training Quantization (best-effort, keep model size <= 256KB) --------\n    def approximate_model_size_bytes(m):\n        nbytes = 0\n        for k, v in m.state_dict().items():\n            nbytes += v.numel() * v.element_size()\n        return int(nbytes)\n\n    q_model = None\n    if quantization_bits == 8 and quantize_weights:\n        try:\n            q_model = torch.ao.quantization.quantize_dynamic(\n                copy.deepcopy(model).to('cpu').eval(),\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n            if quantize_activations:\n                print(\"[Info] quantize_activations requested, but dynamic quantization only quantizes weights of Linear layers. Proceeding with weight-only int8.\")\n        except Exception as e:\n            print(f\"[Warn] Dynamic quantization failed: {e}. Falling back to FP32 CPU model.\")\n            q_model = copy.deepcopy(model).to('cpu').eval()\n    elif quantization_bits == 16:\n        q_model = copy.deepcopy(model).to('cpu').eval().half()\n    else:\n        q_model = copy.deepcopy(model).to('cpu').eval()\n\n    size_bytes = approximate_model_size_bytes(q_model)\n    if size_bytes > 256 * 1024:\n        print(f\"[Warn] Quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Consider reducing base_channels or enabling int8.\")\n    else:\n        print(f\"[Info] Quantized model size ‚âà {size_bytes/1024:.1f}KB (<= 256KB).\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'quantized_model_size_bytes': size_bytes\n    }\n\n    return q_model, metrics\n"}
2025-09-24 19:28:26,683 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-24 19:28:26,683 - ERROR - _models.training_function_executor - BO training objective failed: name 'RRFusionRes1DCNN' is not defined
2025-09-24 19:28:26,683 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 85.298s
2025-09-24 19:28:26,683 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name 'RRFusionRes1DCNN' is not defined
2025-09-24 19:28:26,683 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-24 19:28:29,686 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-24 19:28:29,686 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-09-24 19:28:29,687 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:28:29,687 - INFO - _models.training_function_executor - Loaded training function: RRFusionRes1DCNN-5C
2025-09-24 19:28:29,687 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-24 19:28:29,687 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-09-24 19:28:29,687 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:28:29,687 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: RRFusionRes1DCNN-5C
2025-09-24 19:28:29,687 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-24 19:28:29,687 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 3/4
2025-09-24 19:28:29,687 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 3: üì¶ Installing dependencies for GPT-generated training code...
2025-09-24 19:28:29,687 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-24 19:28:29,690 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-24 19:28:29,690 - INFO - package_installer - Available packages: {'torch'}
2025-09-24 19:28:29,690 - INFO - package_installer - Missing packages: set()
2025-09-24 19:28:29,690 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-24 19:28:29,690 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-24 19:28:29,690 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:28:29,690 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-24 19:28:29,690 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'base_channels', 'loss_type', 'gamma', 'label_smoothing', 'scheduler', 'patience', 'use_amp', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibration_batches', 'beta_cb']
2025-09-24 19:28:29,690 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-24 19:28:29,690 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:28:29,690 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-24 19:28:29,725 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-24 19:28:29,756 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-24 19:28:29,756 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-24 19:28:29,757 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-24 19:28:29,757 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-24 19:28:29,757 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 19:28:29,757 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:28:29,757 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:28:29,757 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:28:29,759 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:29:04,771 - ERROR - _models.training_function_executor - Training execution failed: 'torch.dtype' object has no attribute 'numel'
2025-09-24 19:29:04,771 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-24 19:29:04,771 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-24 19:29:04,771 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-24 19:29:04,771 - INFO - _models.ai_code_generator - Prompt length: 18556 characters
2025-09-24 19:29:04,771 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-24 19:29:04,771 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-24 19:29:04,771 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-24 19:30:36,832 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-24 19:30:36,833 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-24 19:30:36,833 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250924_193036_attempt1.txt
2025-09-24 19:30:36,833 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a 5-class inter-patient 1D CNN + RR feature-fusion classifier.\n\n    Inputs:\n      - X_train, X_val: torch.Tensor of shape (N, 2, 1000) or (N, 1000, 2), dtype float32.\n      - y_train, y_val: torch.Tensor of shape (N,), dtype long.\n      - device: str or torch.device; training will run on GPU.\n    Hyperparameters (kwargs):\n      - lr (float), batch_size (int), epochs (int), weight_decay (float)\n      - dropout (float), base_channels (int)\n      - loss_type in {\"focal\", \"ce\", \"class_balanced\"}\n      - gamma (float, focal loss focusing)\n      - label_smoothing (float)\n      - scheduler in {\"none\", \"onecycle\", \"plateau\"}\n      - patience (int) for ReduceLROnPlateau\n      - use_amp (bool), grad_clip (float)\n      - quantization_bits in {8,16,32}, quantize_weights (bool), quantize_activations (bool)\n      - calibration_batches (int) for PTQ calibration (best-effort; dynamic quant used by default)\n    Returns: (quantized_model, metrics_dict) where metrics_dict has lists: train_losses, val_losses, val_acc\n    \"\"\"\n    import math\n    import copy\n    import torch\n    from torch import nn\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch.cuda.amp import autocast, GradScaler\n    import torch.nn.functional as F\n\n    # -------------------- Helper: ensure channels-first layout (B, 2, L) --------------------\n    def _to_channels_first(X):\n        if X.dim() == 3:\n            if X.shape[-1] == 2 and X.shape[1] != 2:\n                return X.permute(0, 2, 1).contiguous()\n        return X\n\n    # -------------------- Robust device handling --------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        if torch.cuda.is_available():\n            print(\"[Info] Overriding device to CUDA for training per requirement.\")\n            device = torch.device('cuda')\n        else:\n            raise RuntimeError(\"CUDA device required for training per spec but not available.\")\n\n    torch.backends.cudnn.benchmark = True\n\n    # -------------------- Defaults --------------------\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 128))\n    epochs = int(kwargs.get('epochs', 15))\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    dropout = float(kwargs.get('dropout', 0.1))\n    base_channels = int(kwargs.get('base_channels', 16))\n    loss_type = str(kwargs.get('loss_type', 'focal'))\n    gamma = float(kwargs.get('gamma', 2.0))\n    label_smoothing = float(kwargs.get('label_smoothing', 0.0))\n    scheduler_type = str(kwargs.get('scheduler', 'onecycle'))\n    patience = int(kwargs.get('patience', 5))\n    use_amp = bool(kwargs.get('use_amp', True))\n    grad_clip = float(kwargs.get('grad_clip', 1.0))\n\n    quantization_bits = int(kwargs.get('quantization_bits', 8))\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))\n    calibration_batches = int(kwargs.get('calibration_batches', 2))\n\n    num_classes = 5\n\n    # -------------------- Datasets and loaders --------------------\n    X_train = _to_channels_first(X_train.float())\n    X_val = _to_channels_first(X_val.float())\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    if X_train.dim() != 3 or X_train.shape[1] != 2:\n        raise ValueError(f\"X_train must be (N,2,L). Got {tuple(X_train.shape)}\")\n    if X_val.dim() != 3 or X_val.shape[1] != 2:\n        raise ValueError(f\"X_val must be (N,2,L). Got {tuple(X_val.shape)}\")\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=False, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=0)\n\n    # -------------------- Define missing components --------------------\n    class ResBlock1D(nn.Module):\n        def __init__(self, ch, dropout=0.0, kernel_size=7):\n            super().__init__()\n            pad = kernel_size // 2\n            self.conv1 = nn.Conv1d(ch, ch, kernel_size=kernel_size, padding=pad, bias=False)\n            self.bn1 = nn.BatchNorm1d(ch)\n            self.conv2 = nn.Conv1d(ch, ch, kernel_size=kernel_size, padding=pad, bias=False)\n            self.bn2 = nn.BatchNorm1d(ch)\n            self.act = nn.ReLU(inplace=True)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):\n            out = self.conv1(x)\n            out = self.bn1(out)\n            out = self.act(out)\n            out = self.drop(out)\n            out = self.conv2(out)\n            out = self.bn2(out)\n            out = out + x\n            out = self.act(out)\n            return out\n\n    class RRFusionRes1DCNN(nn.Module):\n        def __init__(self, in_ch=2, base_ch=16, num_classes=5, dropout=0.1, rr_feat_dim=5, rr_embed=8):\n            super().__init__()\n            ch1 = base_ch\n            ch2 = base_ch * 2\n            ch3 = base_ch * 4\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, ch1, kernel_size=9, padding=4, bias=False),\n                nn.BatchNorm1d(ch1),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.layer1 = nn.Sequential(\n                ResBlock1D(ch1, dropout=dropout, kernel_size=7),\n                nn.Conv1d(ch1, ch2, kernel_size=5, stride=2, padding=2, bias=False),\n                nn.BatchNorm1d(ch2),\n                nn.ReLU(inplace=True),\n            )\n            self.layer2 = nn.Sequential(\n                ResBlock1D(ch2, dropout=dropout, kernel_size=5),\n                nn.Conv1d(ch2, ch3, kernel_size=3, stride=2, padding=1, bias=False),\n                nn.BatchNorm1d(ch3),\n                nn.ReLU(inplace=True),\n                ResBlock1D(ch3, dropout=dropout, kernel_size=3),\n            )\n            self.gap = nn.AdaptiveAvgPool1d(1)\n            self.ecg_proj = nn.Sequential(\n                nn.Linear(ch3, ch3),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.rr_embed = nn.Sequential(\n                nn.Linear(rr_feat_dim, rr_embed),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.classifier = nn.Sequential(\n                nn.Linear(ch3 + rr_embed, max(32, base_ch * 4)),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(max(32, base_ch * 4), num_classes),\n            )\n        def forward(self, x, rr_feats):\n            x = self.stem(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.gap(x).squeeze(-1)  # (B, ch3)\n            x = self.ecg_proj(x)\n            rr = self.rr_embed(rr_feats)\n            z = torch.cat([x, rr], dim=1)\n            logits = self.classifier(z)\n            return logits\n\n    def make_class_weights(y, num_classes, smoothing=0.0, device='cpu'):\n        counts = torch.bincount(y.cpu(), minlength=num_classes).float()\n        eps = 1e-6\n        inv = 1.0 / (counts + eps)\n        w = inv / inv.mean()\n        return w.to(device)\n\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0, reduction='mean'):\n            super().__init__()\n            self.gamma = gamma\n            # Avoid registering None or non-tensor buffers to prevent state_dict dtype issues\n            self.weight = weight  # tensor or None\n            self.label_smoothing = float(label_smoothing)\n            self.reduction = reduction\n        def forward(self, logits, target):\n            num_classes = logits.size(1)\n            log_probs = F.log_softmax(logits, dim=1)\n            probs = log_probs.exp()\n            one_hot = F.one_hot(target, num_classes=num_classes).float()\n            if self.label_smoothing > 0.0:\n                one_hot = one_hot * (1.0 - self.label_smoothing) + self.label_smoothing / num_classes\n            ce = -(one_hot * log_probs).sum(dim=1)\n            pt = (one_hot * probs).sum(dim=1).clamp_min(1e-6)\n            loss = ((1.0 - pt) ** self.gamma) * ce\n            weight = self.weight\n            if weight is not None:\n                if weight.device != logits.device:\n                    weight = weight.to(logits.device)\n                w_per = (one_hot * weight.unsqueeze(0)).sum(dim=1)\n                loss = loss * w_per\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            else:\n                return loss\n\n    def compute_rr_entropy_features(x):\n        # x: (B,2,L). Use lead-0 to compute simple surrogate RR-like features.\n        sig = x[:, 0, :]\n        B, L = sig.shape\n        eps = 1e-8\n        mean = sig.mean(dim=1)\n        std = sig.std(dim=1, unbiased=False)\n        abs_mean = sig.abs().mean(dim=1)\n        ptp = sig.max(dim=1).values - sig.min(dim=1).values\n        # Spectral entropy\n        spec = torch.fft.rfft(sig, dim=1)\n        psd = (spec.real.pow(2) + spec.imag.pow(2)) + eps\n        p = psd / psd.sum(dim=1, keepdim=True)\n        se = -(p * (p + eps).log()).sum(dim=1)\n        # Normalize by log of number of freq bins\n        denom = torch.log(torch.tensor(p.size(1), device=p.device, dtype=sig.dtype))\n        se = se / denom\n        feats = torch.stack([mean, std, abs_mean, ptp, se], dim=1)\n        return feats\n\n    # -------------------- Model --------------------\n    model = RRFusionRes1DCNN(in_ch=2, base_ch=base_channels, num_classes=num_classes, dropout=dropout, rr_feat_dim=5, rr_embed=8)\n    model = model.to(device)\n\n    # -------------------- Loss --------------------\n    class_weights = make_class_weights(y_train, num_classes, smoothing=0.0, device=device)\n    if loss_type == 'focal':\n        criterion = FocalLoss(gamma=gamma, weight=class_weights, label_smoothing=label_smoothing)\n    elif loss_type == 'class_balanced':\n        beta = float(kwargs.get('beta_cb', 0.999))\n        counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n        eff_num = (1.0 - torch.pow(torch.tensor(beta), counts)) / (1.0 - beta + 1e-8)\n        cb_w = (eff_num.sum() / (eff_num + 1e-8))\n        cb_w = (cb_w / cb_w.mean()).to(device)\n        criterion = nn.CrossEntropyLoss(weight=cb_w, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    if scheduler_type == 'onecycle':\n        steps_per_epoch = max(1, len(train_loader))\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n    elif scheduler_type == 'plateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=0.5, min_lr=1e-6)\n    else:\n        scheduler = None\n\n    scaler = GradScaler(enabled=use_amp)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            rr_feats = compute_rr_entropy_features(xb).to(device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=use_amp):\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            if grad_clip and grad_clip > 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            if scheduler_type == 'onecycle' and scheduler is not None:\n                scheduler.step()\n\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n\n        train_epoch_loss = running_loss / max(1, total)\n        train_losses.append(train_epoch_loss)\n\n        # Validation\n        model.eval()\n        v_loss = 0.0\n        v_total = 0\n        v_correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                rr_feats = compute_rr_entropy_features(xb).to(device)\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n                v_loss += loss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                v_correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n\n        val_epoch_loss = v_loss / max(1, v_total)\n        val_epoch_acc = (v_correct / max(1, v_total))\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n        if scheduler_type == 'plateau' and scheduler is not None:\n            scheduler.step(val_epoch_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_epoch_loss:.6f} val_loss={val_epoch_loss:.6f} val_acc={val_epoch_acc:.4f}\")\n\n    # -------- Post-Training Quantization (best-effort, keep model size <= 256KB) --------\n    def approximate_model_size_bytes(m):\n        nbytes = 0\n        for k, v in m.state_dict().items():\n            # Some quantized models may store non-tensor entries (e.g., packed params metadata)\n            if isinstance(v, torch.Tensor):\n                nbytes += v.numel() * v.element_size()\n            else:\n                # skip non-tensors to avoid errors like \"torch.dtype has no attribute numel\"\n                continue\n        return int(nbytes)\n\n    q_model = None\n    if quantization_bits == 8 and quantize_weights:\n        try:\n            q_model = torch.ao.quantization.quantize_dynamic(\n                copy.deepcopy(model).to('cpu').eval(),\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n            if quantize_activations:\n                print(\"[Info] quantize_activations requested, but dynamic quantization only quantizes weights of Linear layers. Proceeding with weight-only int8.\")\n        except Exception as e:\n            print(f\"[Warn] Dynamic quantization failed: {e}. Falling back to FP32 CPU model.\")\n            q_model = copy.deepcopy(model).to('cpu').eval()\n    elif quantization_bits == 16:\n        q_model = copy.deepcopy(model).to('cpu').eval().half()\n    else:\n        q_model = copy.deepcopy(model).to('cpu').eval()\n\n    size_bytes = approximate_model_size_bytes(q_model)\n    if size_bytes > 256 * 1024:\n        print(f\"[Warn] Quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Consider reducing base_channels or enabling int8.\")\n    else:\n        print(f\"[Info] Quantized model size ‚âà {size_bytes/1024:.1f}KB (<= 256KB).\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'quantized_model_size_bytes': size_bytes\n    }\n\n    return q_model, metrics\n"}
2025-09-24 19:30:36,834 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-24 19:30:36,834 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    \"\"\"\n    Train a 5-class inter-patient 1D CNN + RR feature-fusion classifier.\n\n    Inputs:\n      - X_train, X_val: torch.Tensor of shape (N, 2, 1000) or (N, 1000, 2), dtype float32.\n      - y_train, y_val: torch.Tensor of shape (N,), dtype long.\n      - device: str or torch.device; training will run on GPU.\n    Hyperparameters (kwargs):\n      - lr (float), batch_size (int), epochs (int), weight_decay (float)\n      - dropout (float), base_channels (int)\n      - loss_type in {\"focal\", \"ce\", \"class_balanced\"}\n      - gamma (float, focal loss focusing)\n      - label_smoothing (float)\n      - scheduler in {\"none\", \"onecycle\", \"plateau\"}\n      - patience (int) for ReduceLROnPlateau\n      - use_amp (bool), grad_clip (float)\n      - quantization_bits in {8,16,32}, quantize_weights (bool), quantize_activations (bool)\n      - calibration_batches (int) for PTQ calibration (best-effort; dynamic quant used by default)\n    Returns: (quantized_model, metrics_dict) where metrics_dict has lists: train_losses, val_losses, val_acc\n    \"\"\"\n    import math\n    import copy\n    import torch\n    from torch import nn\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch.cuda.amp import autocast, GradScaler\n    import torch.nn.functional as F\n\n    # -------------------- Helper: ensure channels-first layout (B, 2, L) --------------------\n    def _to_channels_first(X):\n        if X.dim() == 3:\n            if X.shape[-1] == 2 and X.shape[1] != 2:\n                return X.permute(0, 2, 1).contiguous()\n        return X\n\n    # -------------------- Robust device handling --------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        if torch.cuda.is_available():\n            print(\"[Info] Overriding device to CUDA for training per requirement.\")\n            device = torch.device('cuda')\n        else:\n            raise RuntimeError(\"CUDA device required for training per spec but not available.\")\n\n    torch.backends.cudnn.benchmark = True\n\n    # -------------------- Defaults --------------------\n    lr = float(kwargs.get('lr', 1e-3))\n    batch_size = int(kwargs.get('batch_size', 128))\n    epochs = int(kwargs.get('epochs', 15))\n    weight_decay = float(kwargs.get('weight_decay', 1e-4))\n    dropout = float(kwargs.get('dropout', 0.1))\n    base_channels = int(kwargs.get('base_channels', 16))\n    loss_type = str(kwargs.get('loss_type', 'focal'))\n    gamma = float(kwargs.get('gamma', 2.0))\n    label_smoothing = float(kwargs.get('label_smoothing', 0.0))\n    scheduler_type = str(kwargs.get('scheduler', 'onecycle'))\n    patience = int(kwargs.get('patience', 5))\n    use_amp = bool(kwargs.get('use_amp', True))\n    grad_clip = float(kwargs.get('grad_clip', 1.0))\n\n    quantization_bits = int(kwargs.get('quantization_bits', 8))\n    quantize_weights = bool(kwargs.get('quantize_weights', True))\n    quantize_activations = bool(kwargs.get('quantize_activations', False))\n    calibration_batches = int(kwargs.get('calibration_batches', 2))\n\n    num_classes = 5\n\n    # -------------------- Datasets and loaders --------------------\n    X_train = _to_channels_first(X_train.float())\n    X_val = _to_channels_first(X_val.float())\n    y_train = y_train.long()\n    y_val = y_val.long()\n\n    if X_train.dim() != 3 or X_train.shape[1] != 2:\n        raise ValueError(f\"X_train must be (N,2,L). Got {tuple(X_train.shape)}\")\n    if X_val.dim() != 3 or X_val.shape[1] != 2:\n        raise ValueError(f\"X_val must be (N,2,L). Got {tuple(X_val.shape)}\")\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=False, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=False, num_workers=0)\n\n    # -------------------- Define missing components --------------------\n    class ResBlock1D(nn.Module):\n        def __init__(self, ch, dropout=0.0, kernel_size=7):\n            super().__init__()\n            pad = kernel_size // 2\n            self.conv1 = nn.Conv1d(ch, ch, kernel_size=kernel_size, padding=pad, bias=False)\n            self.bn1 = nn.BatchNorm1d(ch)\n            self.conv2 = nn.Conv1d(ch, ch, kernel_size=kernel_size, padding=pad, bias=False)\n            self.bn2 = nn.BatchNorm1d(ch)\n            self.act = nn.ReLU(inplace=True)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):\n            out = self.conv1(x)\n            out = self.bn1(out)\n            out = self.act(out)\n            out = self.drop(out)\n            out = self.conv2(out)\n            out = self.bn2(out)\n            out = out + x\n            out = self.act(out)\n            return out\n\n    class RRFusionRes1DCNN(nn.Module):\n        def __init__(self, in_ch=2, base_ch=16, num_classes=5, dropout=0.1, rr_feat_dim=5, rr_embed=8):\n            super().__init__()\n            ch1 = base_ch\n            ch2 = base_ch * 2\n            ch3 = base_ch * 4\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, ch1, kernel_size=9, padding=4, bias=False),\n                nn.BatchNorm1d(ch1),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.layer1 = nn.Sequential(\n                ResBlock1D(ch1, dropout=dropout, kernel_size=7),\n                nn.Conv1d(ch1, ch2, kernel_size=5, stride=2, padding=2, bias=False),\n                nn.BatchNorm1d(ch2),\n                nn.ReLU(inplace=True),\n            )\n            self.layer2 = nn.Sequential(\n                ResBlock1D(ch2, dropout=dropout, kernel_size=5),\n                nn.Conv1d(ch2, ch3, kernel_size=3, stride=2, padding=1, bias=False),\n                nn.BatchNorm1d(ch3),\n                nn.ReLU(inplace=True),\n                ResBlock1D(ch3, dropout=dropout, kernel_size=3),\n            )\n            self.gap = nn.AdaptiveAvgPool1d(1)\n            self.ecg_proj = nn.Sequential(\n                nn.Linear(ch3, ch3),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.rr_embed = nn.Sequential(\n                nn.Linear(rr_feat_dim, rr_embed),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n            )\n            self.classifier = nn.Sequential(\n                nn.Linear(ch3 + rr_embed, max(32, base_ch * 4)),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(max(32, base_ch * 4), num_classes),\n            )\n        def forward(self, x, rr_feats):\n            x = self.stem(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.gap(x).squeeze(-1)  # (B, ch3)\n            x = self.ecg_proj(x)\n            rr = self.rr_embed(rr_feats)\n            z = torch.cat([x, rr], dim=1)\n            logits = self.classifier(z)\n            return logits\n\n    def make_class_weights(y, num_classes, smoothing=0.0, device='cpu'):\n        counts = torch.bincount(y.cpu(), minlength=num_classes).float()\n        eps = 1e-6\n        inv = 1.0 / (counts + eps)\n        w = inv / inv.mean()\n        return w.to(device)\n\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0, reduction='mean'):\n            super().__init__()\n            self.gamma = gamma\n            # Avoid registering None or non-tensor buffers to prevent state_dict dtype issues\n            self.weight = weight  # tensor or None\n            self.label_smoothing = float(label_smoothing)\n            self.reduction = reduction\n        def forward(self, logits, target):\n            num_classes = logits.size(1)\n            log_probs = F.log_softmax(logits, dim=1)\n            probs = log_probs.exp()\n            one_hot = F.one_hot(target, num_classes=num_classes).float()\n            if self.label_smoothing > 0.0:\n                one_hot = one_hot * (1.0 - self.label_smoothing) + self.label_smoothing / num_classes\n            ce = -(one_hot * log_probs).sum(dim=1)\n            pt = (one_hot * probs).sum(dim=1).clamp_min(1e-6)\n            loss = ((1.0 - pt) ** self.gamma) * ce\n            weight = self.weight\n            if weight is not None:\n                if weight.device != logits.device:\n                    weight = weight.to(logits.device)\n                w_per = (one_hot * weight.unsqueeze(0)).sum(dim=1)\n                loss = loss * w_per\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            else:\n                return loss\n\n    def compute_rr_entropy_features(x):\n        # x: (B,2,L). Use lead-0 to compute simple surrogate RR-like features.\n        sig = x[:, 0, :]\n        B, L = sig.shape\n        eps = 1e-8\n        mean = sig.mean(dim=1)\n        std = sig.std(dim=1, unbiased=False)\n        abs_mean = sig.abs().mean(dim=1)\n        ptp = sig.max(dim=1).values - sig.min(dim=1).values\n        # Spectral entropy\n        spec = torch.fft.rfft(sig, dim=1)\n        psd = (spec.real.pow(2) + spec.imag.pow(2)) + eps\n        p = psd / psd.sum(dim=1, keepdim=True)\n        se = -(p * (p + eps).log()).sum(dim=1)\n        # Normalize by log of number of freq bins\n        denom = torch.log(torch.tensor(p.size(1), device=p.device, dtype=sig.dtype))\n        se = se / denom\n        feats = torch.stack([mean, std, abs_mean, ptp, se], dim=1)\n        return feats\n\n    # -------------------- Model --------------------\n    model = RRFusionRes1DCNN(in_ch=2, base_ch=base_channels, num_classes=num_classes, dropout=dropout, rr_feat_dim=5, rr_embed=8)\n    model = model.to(device)\n\n    # -------------------- Loss --------------------\n    class_weights = make_class_weights(y_train, num_classes, smoothing=0.0, device=device)\n    if loss_type == 'focal':\n        criterion = FocalLoss(gamma=gamma, weight=class_weights, label_smoothing=label_smoothing)\n    elif loss_type == 'class_balanced':\n        beta = float(kwargs.get('beta_cb', 0.999))\n        counts = torch.bincount(y_train.cpu(), minlength=num_classes).float()\n        eff_num = (1.0 - torch.pow(torch.tensor(beta), counts)) / (1.0 - beta + 1e-8)\n        cb_w = (eff_num.sum() / (eff_num + 1e-8))\n        cb_w = (cb_w / cb_w.mean()).to(device)\n        criterion = nn.CrossEntropyLoss(weight=cb_w, label_smoothing=label_smoothing)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    if scheduler_type == 'onecycle':\n        steps_per_epoch = max(1, len(train_loader))\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n    elif scheduler_type == 'plateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=patience, factor=0.5, min_lr=1e-6)\n    else:\n        scheduler = None\n\n    scaler = GradScaler(enabled=use_amp)\n\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                xb = xb.permute(0, 2, 1).contiguous()\n\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n\n            rr_feats = compute_rr_entropy_features(xb).to(device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=use_amp):\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n            scaler.scale(loss).backward()\n            if grad_clip and grad_clip > 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            if scheduler_type == 'onecycle' and scheduler is not None:\n                scheduler.step()\n\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n\n        train_epoch_loss = running_loss / max(1, total)\n        train_losses.append(train_epoch_loss)\n\n        # Validation\n        model.eval()\n        v_loss = 0.0\n        v_total = 0\n        v_correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if xb.dim() == 3 and xb.shape[1] != 2 and xb.shape[-1] == 2:\n                    xb = xb.permute(0, 2, 1).contiguous()\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                rr_feats = compute_rr_entropy_features(xb).to(device)\n                logits = model(xb, rr_feats)\n                loss = criterion(logits, yb)\n                v_loss += loss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                v_correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n\n        val_epoch_loss = v_loss / max(1, v_total)\n        val_epoch_acc = (v_correct / max(1, v_total))\n        val_losses.append(val_epoch_loss)\n        val_accs.append(val_epoch_acc)\n\n        if scheduler_type == 'plateau' and scheduler is not None:\n            scheduler.step(val_epoch_acc)\n\n        print(f\"Epoch {epoch:03d}: train_loss={train_epoch_loss:.6f} val_loss={val_epoch_loss:.6f} val_acc={val_epoch_acc:.4f}\")\n\n    # -------- Post-Training Quantization (best-effort, keep model size <= 256KB) --------\n    def approximate_model_size_bytes(m):\n        nbytes = 0\n        for k, v in m.state_dict().items():\n            # Some quantized models may store non-tensor entries (e.g., packed params metadata)\n            if isinstance(v, torch.Tensor):\n                nbytes += v.numel() * v.element_size()\n            else:\n                # skip non-tensors to avoid errors like \"torch.dtype has no attribute numel\"\n                continue\n        return int(nbytes)\n\n    q_model = None\n    if quantization_bits == 8 and quantize_weights:\n        try:\n            q_model = torch.ao.quantization.quantize_dynamic(\n                copy.deepcopy(model).to('cpu').eval(),\n                {nn.Linear},\n                dtype=torch.qint8\n            )\n            if quantize_activations:\n                print(\"[Info] quantize_activations requested, but dynamic quantization only quantizes weights of Linear layers. Proceeding with weight-only int8.\")\n        except Exception as e:\n            print(f\"[Warn] Dynamic quantization failed: {e}. Falling back to FP32 CPU model.\")\n            q_model = copy.deepcopy(model).to('cpu').eval()\n    elif quantization_bits == 16:\n        q_model = copy.deepcopy(model).to('cpu').eval().half()\n    else:\n        q_model = copy.deepcopy(model).to('cpu').eval()\n\n    size_bytes = approximate_model_size_bytes(q_model)\n    if size_bytes > 256 * 1024:\n        print(f\"[Warn] Quantized model size {size_bytes/1024:.1f}KB exceeds 256KB. Consider reducing base_channels or enabling int8.\")\n    else:\n        print(f\"[Info] Quantized model size ‚âà {size_bytes/1024:.1f}KB (<= 256KB).\")\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'quantized_model_size_bytes': size_bytes\n    }\n\n    return q_model, metrics\n"}
2025-09-24 19:30:36,834 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-24 19:30:36,834 - ERROR - _models.training_function_executor - BO training objective failed: 'torch.dtype' object has no attribute 'numel'
2025-09-24 19:30:36,834 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 127.076s
2025-09-24 19:30:36,834 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: 'torch.dtype' object has no attribute 'numel'
2025-09-24 19:30:36,834 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-24 19:30:39,837 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-24 19:30:39,837 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-09-24 19:30:39,838 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:30:39,838 - INFO - _models.training_function_executor - Loaded training function: RRFusionRes1DCNN-5C
2025-09-24 19:30:39,838 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-24 19:30:39,838 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-09-24 19:30:39,838 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_RRFusionRes1DCNN-5C_1758759913.json
2025-09-24 19:30:39,838 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: RRFusionRes1DCNN-5C
2025-09-24 19:30:39,838 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-24 19:30:39,838 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 4/4
2025-09-24 19:30:39,838 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 4: üì¶ Installing dependencies for GPT-generated training code...
2025-09-24 19:30:39,838 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-24 19:30:39,840 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-24 19:30:39,841 - INFO - package_installer - Available packages: {'torch'}
2025-09-24 19:30:39,841 - INFO - package_installer - Missing packages: set()
2025-09-24 19:30:39,841 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-24 19:30:39,841 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-24 19:30:39,841 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:30:39,841 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-24 19:30:39,841 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'base_channels', 'loss_type', 'gamma', 'label_smoothing', 'scheduler', 'patience', 'use_amp', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibration_batches', 'beta_cb']
2025-09-24 19:30:39,841 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-24 19:30:39,841 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-24 19:30:39,841 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-24 19:30:39,875 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-24 19:30:39,907 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-24 19:30:39,907 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-24 19:30:39,908 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-24 19:30:39,908 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-24 19:30:39,908 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:30:39,909 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:30:39,909 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:30:39,909 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:30:39,910 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}
2025-09-24 19:31:14,535 - INFO - _models.training_function_executor - Model: 60,696 parameters, 65.2KB storage
2025-09-24 19:31:14,535 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.3461894571895054, 0.7983227966143419, 0.6683662945183918, 0.6051937582289459, 0.5549417874433528, 0.5235273586595746, 0.49876829136270473, 0.47816135811035715, 0.4559465754701691, 0.4365841529126104, 0.4219324232310993, 0.41309193108190334], 'val_losses': [0.9709250412852435, 0.718940134739132, 0.6015999170640793, 0.5649552994720799, 0.528133167866223, 0.4924184405449682, 0.4902966470142427, 0.45591779895450757, 0.43263934260845005, 0.4215942156402976, 0.4122002676695066, 0.41132997870519933], 'val_acc': [0.6772334293948127, 0.9072797895000626, 0.8119283297832351, 0.9360982333040972, 0.9161759178047864, 0.9645407843628618, 0.962536023054755, 0.9577747149480015, 0.9536398947500313, 0.9651672722716451, 0.9666708432527252, 0.9656684625986719], 'quantized_model_size_bytes': 245928, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': 18, 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': 6, 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 10, 'beta_cb': 0.9524231675200606}, 'model_parameter_count': 60696, 'model_storage_size_kb': 65.20078125, 'model_size_validation': 'PASS'}
2025-09-24 19:31:14,535 - INFO - _models.training_function_executor - BO Objective: base=0.9657, size_penalty=0.0000, final=0.9657
2025-09-24 19:31:14,535 - INFO - _models.training_function_executor - Model: 60,696 parameters, 65.2KB (PASS 256KB limit)
2025-09-24 19:31:14,535 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 34.627s
2025-09-24 19:31:14,535 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9657
2025-09-24 19:31:14,535 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-24 19:31:14,535 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': np.int64(18), 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': np.int64(6), 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': np.int64(10), 'beta_cb': 0.9524231675200606}, value=0.9657
2025-09-24 19:31:14,536 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.0015352246941973508, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'base_channels': np.int64(18), 'loss_type': 'ce', 'gamma': 1.6685430556951095, 'label_smoothing': 0.01428668179219408, 'scheduler': 'onecycle', 'patience': np.int64(6), 'use_amp': False, 'grad_clip': 1.664885281600844, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': np.int64(10), 'beta_cb': 0.9524231675200606} -> 0.9657
2025-09-24 19:31:14,536 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-09-24 19:31:14,536 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 19:31:14,536 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:31:14,536 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:31:14,536 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 5.342937261279778e-05, 'batch_size': 64, 'epochs': 46, 'weight_decay': 1.536960311060885e-06, 'dropout': 0.4868777594207297, 'base_channels': 22, 'loss_type': 'ce', 'gamma': 3.9258798069650687, 'label_smoothing': 0.01996737821583598, 'scheduler': 'onecycle', 'patience': 10, 'use_amp': True, 'grad_clip': 1.215089703802877, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9385031086037376}
2025-09-24 19:31:14,538 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 5.342937261279778e-05, 'batch_size': 64, 'epochs': 46, 'weight_decay': 1.536960311060885e-06, 'dropout': 0.4868777594207297, 'base_channels': 22, 'loss_type': 'ce', 'gamma': 3.9258798069650687, 'label_smoothing': 0.01996737821583598, 'scheduler': 'onecycle', 'patience': 10, 'use_amp': True, 'grad_clip': 1.215089703802877, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9385031086037376}
2025-09-24 19:33:03,042 - INFO - _models.training_function_executor - Model: 90,376 parameters, 97.1KB storage
2025-09-24 19:33:03,042 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.7623716941719436, 1.750933734743203, 1.73758374875423, 1.703643875406921, 1.650836878285478, 1.58434006983567, 1.5345831716381413, 1.490125659514918, 1.4540625627008823, 1.4105291706470942, 1.3557577926674997, 1.3100566389424548, 1.2614898810622515, 1.2201300045715233, 1.1779300316669243, 1.1284773938993358, 1.0883292682606358, 1.0414296635723135, 1.009489240123713, 0.9672493036388623, 0.9362889993490627, 0.912344564550731, 0.8782868010729726, 0.8595847267090025, 0.8301979634795643, 0.8220509776039029, 0.8121260054759826, 0.7898134002801906, 0.7743992580938948, 0.7748640658610992, 0.7627476267870202, 0.7505930774556856, 0.7570120009328736, 0.7456010198867983, 0.7462059028281666, 0.741388463463897, 0.7408412566976067, 0.73218177405492, 0.7364587954507228, 0.7304090124448768, 0.7300157304269137, 0.7207206145241211, 0.7318724017747007, 0.7306265402224094, 0.7287593311233277, 0.7284759783085338], 'val_losses': [1.7503260511784995, 1.7454365336376687, 1.7372667070857686, 1.7053567448022027, 1.6262484585308308, 1.547048372936165, 1.5011505301980623, 1.4759481102284178, 1.3958786732570045, 1.3486153389514053, 1.2766364118148026, 1.2777035455329722, 1.379977594131725, 1.2931005753695315, 1.2351931823314155, 1.136201285879768, 0.990704784341331, 0.9615084384411084, 0.9192013443151134, 0.9112943042834709, 0.8362299687670312, 0.8285397187448537, 0.8010368330140204, 0.7685160253403733, 0.7441034217986707, 0.7372043031287543, 0.7070958694409732, 0.7228455710360288, 0.7019078019134025, 0.6862491527277043, 0.679326954379056, 0.6742532809804427, 0.6739471940196049, 0.6790089912394237, 0.6821183903954289, 0.6822932195938286, 0.6661574115222625, 0.6647843894855732, 0.6610609495375422, 0.6587570677752363, 0.6619275651200943, 0.6627133370879718, 0.6562163834657874, 0.6610967015252199, 0.662419862383336, 0.6571219938468431], 'val_acc': [0.6020548803408095, 0.6001754166144594, 0.2401954642275404, 0.33040972309234434, 0.38566595664703673, 0.3889236937727102, 0.3824082195213632, 0.39005137200852025, 0.43340433529632877, 0.45608319759428645, 0.445558200726726, 0.445934093471996, 0.42500939731863174, 0.46936474126049366, 0.48189449943616086, 0.5341435910286931, 0.6078185691016164, 0.6237313619847137, 0.6365117153238943, 0.6734745019421126, 0.7115649667961408, 0.6801152737752162, 0.6846259867184563, 0.7435158501440923, 0.760055130935973, 0.7812304222528506, 0.7868688134319007, 0.7339932339305851, 0.7673223906778599, 0.7813557198346072, 0.8034080942237815, 0.8101741636386418, 0.8010274401704047, 0.7838616714697406, 0.7846134569602806, 0.7785991730359604, 0.7937601804285177, 0.7970179175541912, 0.8109259491291818, 0.8124295201102619, 0.799398571607568, 0.7948878586643278, 0.8036586893872948, 0.7883723844129809, 0.7938854780102744, 0.8100488660568851], 'quantized_model_size_bytes': 365320, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 5.342937261279778e-05, 'batch_size': 64, 'epochs': 46, 'weight_decay': 1.536960311060885e-06, 'dropout': 0.4868777594207297, 'base_channels': 22, 'loss_type': 'ce', 'gamma': 3.9258798069650687, 'label_smoothing': 0.01996737821583598, 'scheduler': 'onecycle', 'patience': 10, 'use_amp': True, 'grad_clip': 1.215089703802877, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9385031086037376}, 'model_parameter_count': 90376, 'model_storage_size_kb': 97.08359375, 'model_size_validation': 'PASS'}
2025-09-24 19:33:03,042 - INFO - _models.training_function_executor - BO Objective: base=0.8100, size_penalty=0.0000, final=0.8100
2025-09-24 19:33:03,042 - INFO - _models.training_function_executor - Model: 90,376 parameters, 97.1KB (PASS 256KB limit)
2025-09-24 19:33:03,042 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 108.506s
2025-09-24 19:33:03,042 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8100
2025-09-24 19:33:03,042 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-24 19:33:03,042 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 5.342937261279778e-05, 'batch_size': 64, 'epochs': np.int64(46), 'weight_decay': 1.536960311060885e-06, 'dropout': 0.4868777594207297, 'base_channels': np.int64(22), 'loss_type': 'ce', 'gamma': 3.9258798069650687, 'label_smoothing': 0.01996737821583598, 'scheduler': 'onecycle', 'patience': np.int64(10), 'use_amp': True, 'grad_clip': 1.215089703802877, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(2), 'beta_cb': 0.9385031086037376}, value=0.8100
2025-09-24 19:33:03,042 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 5.342937261279778e-05, 'batch_size': 64, 'epochs': np.int64(46), 'weight_decay': 1.536960311060885e-06, 'dropout': 0.4868777594207297, 'base_channels': np.int64(22), 'loss_type': 'ce', 'gamma': 3.9258798069650687, 'label_smoothing': 0.01996737821583598, 'scheduler': 'onecycle', 'patience': np.int64(10), 'use_amp': True, 'grad_clip': 1.215089703802877, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(2), 'beta_cb': 0.9385031086037376} -> 0.8100
2025-09-24 19:33:03,043 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-09-24 19:33:03,044 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 19:33:03,044 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:33:03,044 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:33:03,044 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.1584172310543985e-06, 'batch_size': 32, 'epochs': 32, 'weight_decay': 5.7624872164786094e-05, 'dropout': 0.06101911742238943, 'base_channels': 15, 'loss_type': 'focal', 'gamma': 4.5466020103939115, 'label_smoothing': 0.025877998160001695, 'scheduler': 'onecycle', 'patience': 3, 'use_amp': True, 'grad_clip': 0.41588332573637776, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9394755085765812}
2025-09-24 19:33:03,045 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.1584172310543985e-06, 'batch_size': 32, 'epochs': 32, 'weight_decay': 5.7624872164786094e-05, 'dropout': 0.06101911742238943, 'base_channels': 15, 'loss_type': 'focal', 'gamma': 4.5466020103939115, 'label_smoothing': 0.025877998160001695, 'scheduler': 'onecycle', 'patience': 3, 'use_amp': True, 'grad_clip': 0.41588332573637776, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9394755085765812}
2025-09-24 19:34:34,740 - INFO - _models.training_function_executor - Model: 50,453 parameters, 108.4KB storage
2025-09-24 19:34:34,740 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.13007277983620016, 0.1300648461727519, 0.1296352501271852, 0.12933392538844138, 0.12865964643286348, 0.12771460877986598, 0.12667329386026027, 0.12533564412528117, 0.12396519088277026, 0.12251243886260375, 0.12091439688186208, 0.11913307335776083, 0.11771904778748277, 0.11607003723057432, 0.1143789860877582, 0.1130215899092069, 0.11180929244787581, 0.11066437239450563, 0.10934361167228958, 0.10853277322303882, 0.10766037148169824, 0.10703769730585423, 0.10631034817719666, 0.10567256162577963, 0.10511880780235029, 0.10517703244336636, 0.1048175659745673, 0.10458374463825325, 0.10447110477602131, 0.10455071870522911, 0.10424310849513961, 0.10448347762191117], 'val_losses': [0.1303873020049818, 0.13034522792100608, 0.12971606413354075, 0.12922526849602983, 0.12841575956556706, 0.12744527217245777, 0.12644571274922256, 0.12463437011816675, 0.12256409026483533, 0.12086152304941783, 0.11941487618820393, 0.11749300648489926, 0.11581753512575817, 0.11467662114216381, 0.11227332316399696, 0.11146684079081504, 0.10897005442658278, 0.10785617995547436, 0.10696080060508403, 0.10640167680196394, 0.1049100658250115, 0.1039431602007359, 0.10377910300340588, 0.10305244846218377, 0.10306019475175958, 0.1022581161478157, 0.10171494113817921, 0.10192728607394076, 0.10238853497637228, 0.10261626396729166, 0.10215679748720609, 0.10222950530685498], 'val_acc': [0.069164265129683, 0.07480265630873324, 0.08683122415737377, 0.11552437037965167, 0.16739756922691393, 0.200225535647162, 0.2528505199849643, 0.3034707430146598, 0.33993233930585137, 0.3841623856659567, 0.42500939731863174, 0.45984212504698657, 0.46021801779225663, 0.4641022428267134, 0.48928705675980455, 0.4823956897631876, 0.4786367623104874, 0.5114647287307355, 0.508206991605062, 0.5157248465104624, 0.5079563964415487, 0.5127177045483022, 0.4871569978699411, 0.513594787620599, 0.4953013406841248, 0.492544793885478, 0.4610951008645533, 0.5105876456584388, 0.5026938980077684, 0.4840245583260243, 0.5105876456584388, 0.5064528254604687], 'quantized_model_size_bytes': 102238, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.1584172310543985e-06, 'batch_size': 32, 'epochs': 32, 'weight_decay': 5.7624872164786094e-05, 'dropout': 0.06101911742238943, 'base_channels': 15, 'loss_type': 'focal', 'gamma': 4.5466020103939115, 'label_smoothing': 0.025877998160001695, 'scheduler': 'onecycle', 'patience': 3, 'use_amp': True, 'grad_clip': 0.41588332573637776, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9394755085765812}, 'model_parameter_count': 50453, 'model_storage_size_kb': 108.3951171875, 'model_size_validation': 'PASS'}
2025-09-24 19:34:34,740 - INFO - _models.training_function_executor - BO Objective: base=0.5065, size_penalty=0.0000, final=0.5065
2025-09-24 19:34:34,740 - INFO - _models.training_function_executor - Model: 50,453 parameters, 108.4KB (PASS 256KB limit)
2025-09-24 19:34:34,740 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 91.696s
2025-09-24 19:34:34,816 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5065
2025-09-24 19:34:34,816 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.076s
2025-09-24 19:34:34,817 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 1.1584172310543985e-06, 'batch_size': 32, 'epochs': np.int64(32), 'weight_decay': 5.7624872164786094e-05, 'dropout': 0.06101911742238943, 'base_channels': np.int64(15), 'loss_type': 'focal', 'gamma': 4.5466020103939115, 'label_smoothing': 0.025877998160001695, 'scheduler': 'onecycle', 'patience': np.int64(3), 'use_amp': True, 'grad_clip': 0.41588332573637776, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(2), 'beta_cb': 0.9394755085765812}, value=0.5065
2025-09-24 19:34:34,817 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 1.1584172310543985e-06, 'batch_size': 32, 'epochs': np.int64(32), 'weight_decay': 5.7624872164786094e-05, 'dropout': 0.06101911742238943, 'base_channels': np.int64(15), 'loss_type': 'focal', 'gamma': 4.5466020103939115, 'label_smoothing': 0.025877998160001695, 'scheduler': 'onecycle', 'patience': np.int64(3), 'use_amp': True, 'grad_clip': 0.41588332573637776, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': np.int64(2), 'beta_cb': 0.9394755085765812} -> 0.5065
2025-09-24 19:34:34,817 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-09-24 19:34:34,817 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:34:34,817 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:34:34,817 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:34:34,817 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005089035297840717, 'batch_size': 128, 'epochs': 21, 'weight_decay': 3.434044061139326e-05, 'dropout': 0.1070286940013698, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 4.403623042697627, 'label_smoothing': 0.06593468847836195, 'scheduler': np.str_('plateau'), 'patience': 10, 'use_amp': False, 'grad_clip': 1.6992356271577254, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9209366217457654}
2025-09-24 19:34:34,818 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005089035297840717, 'batch_size': 128, 'epochs': 21, 'weight_decay': 3.434044061139326e-05, 'dropout': 0.1070286940013698, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 4.403623042697627, 'label_smoothing': 0.06593468847836195, 'scheduler': np.str_('plateau'), 'patience': 10, 'use_amp': False, 'grad_clip': 1.6992356271577254, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9209366217457654}
2025-09-24 19:35:04,265 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB storage
2025-09-24 19:35:04,266 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7982628751760048, 0.5266326669192466, 0.44833850347132764, 0.41887374476536104, 0.409599197635121, 0.3943564347488459, 0.38957360952871317, 0.38298706181891873, 0.3779184187389916, 0.3739512438204333, 0.3717411278733573, 0.3711677073640707, 0.364918068688499, 0.36581625234840603, 0.35900983573615697, 0.358046031920036, 0.35910026672859374, 0.3551889874002104, 0.35630579240634375, 0.3542522153426273, 0.3508738573099465], 'val_losses': [0.5942557705866187, 0.4672514764516668, 0.4173960355854381, 0.41876664421280857, 0.3818288366681964, 0.38075316637892603, 0.37173090014985266, 0.3780605587060403, 0.35640443630526797, 0.3583488758943266, 0.3705042987565262, 0.3544516923649839, 0.3538329739899821, 0.34829815960426314, 0.35131150105441367, 0.3579359888627167, 0.35368623610367944, 0.35160788150975436, 0.3472003478212862, 0.3439411870848638, 0.34220585986167146], 'val_acc': [0.8818443804034583, 0.9302092469615336, 0.9498809672973312, 0.9481268011527377, 0.9624107254729983, 0.9607818569101616, 0.9660443553439418, 0.9639142964540784, 0.9693020924696153, 0.9689261997243453, 0.9671720335797519, 0.9698032827966421, 0.9718080441047487, 0.9720586392682621, 0.9728104247588022, 0.9710562586142087, 0.970930961032452, 0.9718080441047487, 0.9731863175040721, 0.9745645909033955, 0.9776970304473124], 'quantized_model_size_bytes': 93164, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005089035297840717, 'batch_size': 128, 'epochs': 21, 'weight_decay': 3.434044061139326e-05, 'dropout': 0.1070286940013698, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 4.403623042697627, 'label_smoothing': 0.06593468847836195, 'scheduler': np.str_('plateau'), 'patience': 10, 'use_amp': False, 'grad_clip': 1.6992356271577254, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9209366217457654}, 'model_parameter_count': 22853, 'model_storage_size_kb': 98.19648437500001, 'model_size_validation': 'PASS'}
2025-09-24 19:35:04,266 - INFO - _models.training_function_executor - BO Objective: base=0.9777, size_penalty=0.0000, final=0.9777
2025-09-24 19:35:04,266 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB (PASS 256KB limit)
2025-09-24 19:35:04,266 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 29.449s
2025-09-24 19:35:04,341 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9777
2025-09-24 19:35:04,341 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.075s
2025-09-24 19:35:04,341 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 0.005089035297840717, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'weight_decay': 3.434044061139326e-05, 'dropout': 0.1070286940013698, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 4.403623042697627, 'label_smoothing': 0.06593468847836195, 'scheduler': np.str_('plateau'), 'patience': np.int64(10), 'use_amp': np.False_, 'grad_clip': 1.6992356271577254, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9209366217457654}, value=0.9777
2025-09-24 19:35:04,341 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 0.005089035297840717, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'weight_decay': 3.434044061139326e-05, 'dropout': 0.1070286940013698, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 4.403623042697627, 'label_smoothing': 0.06593468847836195, 'scheduler': np.str_('plateau'), 'patience': np.int64(10), 'use_amp': np.False_, 'grad_clip': 1.6992356271577254, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9209366217457654} -> 0.9777
2025-09-24 19:35:04,341 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-09-24 19:35:04,341 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:35:04,341 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:35:04,341 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:35:04,341 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 4.560850789860337e-06, 'batch_size': 256, 'epochs': 11, 'weight_decay': 0.00021183411411206888, 'dropout': 0.0910081487285211, 'base_channels': 24, 'loss_type': np.str_('class_balanced'), 'gamma': 4.2640048293735715, 'label_smoothing': 0.046617230534171106, 'scheduler': np.str_('none'), 'patience': 9, 'use_amp': True, 'grad_clip': 0.5769225407980371, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.9821402441818721}
2025-09-24 19:35:04,343 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 4.560850789860337e-06, 'batch_size': 256, 'epochs': 11, 'weight_decay': 0.00021183411411206888, 'dropout': 0.0910081487285211, 'base_channels': 24, 'loss_type': np.str_('class_balanced'), 'gamma': 4.2640048293735715, 'label_smoothing': 0.046617230534171106, 'scheduler': np.str_('none'), 'patience': 9, 'use_amp': True, 'grad_clip': 0.5769225407980371, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.9821402441818721}
2025-09-24 19:35:23,645 - INFO - _models.training_function_executor - Model: 127,349 parameters, 273.6KB storage
2025-09-24 19:35:23,645 - WARNING - _models.training_function_executor - Model storage 273.6KB exceeds 256KB limit!
2025-09-24 19:35:23,645 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5689357456936743, 1.5102898018015762, 1.4348835581597637, 1.3559219530231241, 1.2812906338809087, 1.2071099271452548, 1.136438664725164, 1.0729374270299785, 1.0176765987797123, 0.971627172656224, 0.935402378880081], 'val_losses': [1.5452032917917948, 1.4690264326903832, 1.378126574823274, 1.3019250786524041, 1.2283680676189137, 1.1559422273352666, 1.0906996759252878, 1.0327991874814557, 0.9830528312855056, 0.9439294988567437, 0.914395407811246], 'val_acc': [0.1553690013782734, 0.6867560456083197, 0.7198346071920811, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946], 'quantized_model_size_bytes': 256786, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 4.560850789860337e-06, 'batch_size': 256, 'epochs': 11, 'weight_decay': 0.00021183411411206888, 'dropout': 0.0910081487285211, 'base_channels': 24, 'loss_type': np.str_('class_balanced'), 'gamma': 4.2640048293735715, 'label_smoothing': 0.046617230534171106, 'scheduler': np.str_('none'), 'patience': 9, 'use_amp': True, 'grad_clip': 0.5769225407980371, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.9821402441818721}, 'model_parameter_count': 127349, 'model_storage_size_kb': 273.60136718750005, 'model_size_validation': 'FAIL'}
2025-09-24 19:35:23,645 - INFO - _models.training_function_executor - BO Objective: base=0.7201, size_penalty=0.0344, final=0.6857
2025-09-24 19:35:23,645 - INFO - _models.training_function_executor - Model: 127,349 parameters, 273.6KB (FAIL 256KB limit)
2025-09-24 19:35:23,645 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 19.304s
2025-09-24 19:35:23,721 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6857
2025-09-24 19:35:23,721 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.076s
2025-09-24 19:35:23,721 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 4.560850789860337e-06, 'batch_size': np.int64(256), 'epochs': np.int64(11), 'weight_decay': 0.00021183411411206888, 'dropout': 0.0910081487285211, 'base_channels': np.int64(24), 'loss_type': np.str_('class_balanced'), 'gamma': 4.2640048293735715, 'label_smoothing': 0.046617230534171106, 'scheduler': np.str_('none'), 'patience': np.int64(9), 'use_amp': np.True_, 'grad_clip': 0.5769225407980371, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9821402441818721}, value=0.6857
2025-09-24 19:35:23,721 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 4.560850789860337e-06, 'batch_size': np.int64(256), 'epochs': np.int64(11), 'weight_decay': 0.00021183411411206888, 'dropout': 0.0910081487285211, 'base_channels': np.int64(24), 'loss_type': np.str_('class_balanced'), 'gamma': 4.2640048293735715, 'label_smoothing': 0.046617230534171106, 'scheduler': np.str_('none'), 'patience': np.int64(9), 'use_amp': np.True_, 'grad_clip': 0.5769225407980371, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9821402441818721} -> 0.6857
2025-09-24 19:35:23,721 - INFO - bo.run_bo - üîçBO Trial 6: Using RF surrogate + Expected Improvement
2025-09-24 19:35:23,721 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:35:23,721 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:35:23,721 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:35:23,721 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.3778249745932526e-05, 'batch_size': 32, 'epochs': 26, 'weight_decay': 0.004789954156458351, 'dropout': 0.4797124215013895, 'base_channels': 16, 'loss_type': np.str_('focal'), 'gamma': 0.41879377012338187, 'label_smoothing': 0.027514645362060837, 'scheduler': np.str_('onecycle'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.398971072717083, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 9, 'beta_cb': 0.9224385017917074}
2025-09-24 19:35:23,723 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.3778249745932526e-05, 'batch_size': 32, 'epochs': 26, 'weight_decay': 0.004789954156458351, 'dropout': 0.4797124215013895, 'base_channels': 16, 'loss_type': np.str_('focal'), 'gamma': 0.41879377012338187, 'label_smoothing': 0.027514645362060837, 'scheduler': np.str_('onecycle'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.398971072717083, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 9, 'beta_cb': 0.9224385017917074}
2025-09-24 19:36:29,135 - INFO - _models.training_function_executor - Model: 57,269 parameters, 246.1KB storage
2025-09-24 19:36:29,135 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.33781456668562965, 0.3360734700305201, 0.3314693432883713, 0.32276574893093785, 0.30413605386992093, 0.28650940174361583, 0.27145085610964875, 0.25751109816946416, 0.24571394460426366, 0.23735270155266713, 0.2275223651581665, 0.21998244625667737, 0.21403733539500264, 0.21015401387703478, 0.20807200443851834, 0.20423277738574333, 0.20267287822806052, 0.20030709286215267, 0.20059697024114143, 0.19653882855444488, 0.19820894382438223, 0.19463526594432692, 0.1955151564457346, 0.19690887451674136, 0.19454828131325103, 0.19580616949177646], 'val_losses': [0.3364552985291002, 0.33461477288684816, 0.32976041395343675, 0.31422586678196474, 0.2954262467893558, 0.27146400787405855, 0.2600308971034003, 0.2502961178599496, 0.2286897745347982, 0.21448524667272412, 0.20794125087815946, 0.19896106417839904, 0.18799002691888672, 0.1857366829572831, 0.18778732283557362, 0.1812961933315883, 0.17618343540263226, 0.17446377287157763, 0.1737189498081077, 0.17309761549981073, 0.17168516453876934, 0.17066245937766386, 0.17202177175325703, 0.17114694821772578, 0.16996824047743506, 0.17034516425572485], 'val_acc': [0.0875830096479138, 0.08996366370129057, 0.07593033454454329, 0.43804034582132567, 0.5747400075178549, 0.4464352837990227, 0.3811552437037965, 0.36749780729231923, 0.43941861922064906, 0.4682370630246836, 0.4683623606064403, 0.4652299210625235, 0.49605312617466485, 0.5553188823455707, 0.5016915173537151, 0.5243703796516728, 0.5312617466482897, 0.5643403082320512, 0.5396566846259867, 0.5709810800651547, 0.5535647162009774, 0.5412855531888234, 0.5605813807793509, 0.5627114396692143, 0.5495551935847638, 0.5583260243077308], 'quantized_model_size_bytes': 231836, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.3778249745932526e-05, 'batch_size': 32, 'epochs': 26, 'weight_decay': 0.004789954156458351, 'dropout': 0.4797124215013895, 'base_channels': 16, 'loss_type': np.str_('focal'), 'gamma': 0.41879377012338187, 'label_smoothing': 0.027514645362060837, 'scheduler': np.str_('onecycle'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.398971072717083, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 9, 'beta_cb': 0.9224385017917074}, 'model_parameter_count': 57269, 'model_storage_size_kb': 246.077734375, 'model_size_validation': 'PASS'}
2025-09-24 19:36:29,135 - INFO - _models.training_function_executor - BO Objective: base=0.5583, size_penalty=0.0000, final=0.5583
2025-09-24 19:36:29,135 - INFO - _models.training_function_executor - Model: 57,269 parameters, 246.1KB (PASS 256KB limit)
2025-09-24 19:36:29,135 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 65.414s
2025-09-24 19:36:29,211 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5583
2025-09-24 19:36:29,211 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.076s
2025-09-24 19:36:29,211 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 3.3778249745932526e-05, 'batch_size': np.int64(32), 'epochs': np.int64(26), 'weight_decay': 0.004789954156458351, 'dropout': 0.4797124215013895, 'base_channels': np.int64(16), 'loss_type': np.str_('focal'), 'gamma': 0.41879377012338187, 'label_smoothing': 0.027514645362060837, 'scheduler': np.str_('onecycle'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 1.398971072717083, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(9), 'beta_cb': 0.9224385017917074}, value=0.5583
2025-09-24 19:36:29,211 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 3.3778249745932526e-05, 'batch_size': np.int64(32), 'epochs': np.int64(26), 'weight_decay': 0.004789954156458351, 'dropout': 0.4797124215013895, 'base_channels': np.int64(16), 'loss_type': np.str_('focal'), 'gamma': 0.41879377012338187, 'label_smoothing': 0.027514645362060837, 'scheduler': np.str_('onecycle'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 1.398971072717083, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(9), 'beta_cb': 0.9224385017917074} -> 0.5583
2025-09-24 19:36:29,212 - INFO - bo.run_bo - üîçBO Trial 7: Using RF surrogate + Expected Improvement
2025-09-24 19:36:29,212 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:36:29,212 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:36:29,212 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:36:29,212 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.007919994415635049, 'batch_size': 32, 'epochs': 26, 'weight_decay': 0.0006708385199181597, 'dropout': 0.08778953009986402, 'base_channels': 20, 'loss_type': np.str_('focal'), 'gamma': 0.6542839051021816, 'label_smoothing': 0.08385921519794631, 'scheduler': np.str_('none'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.132159456256945, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9090283086038695}
2025-09-24 19:36:29,213 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.007919994415635049, 'batch_size': 32, 'epochs': 26, 'weight_decay': 0.0006708385199181597, 'dropout': 0.08778953009986402, 'base_channels': 20, 'loss_type': np.str_('focal'), 'gamma': 0.6542839051021816, 'label_smoothing': 0.08385921519794631, 'scheduler': np.str_('none'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.132159456256945, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9090283086038695}
2025-09-24 19:37:45,384 - INFO - _models.training_function_executor - Model: 88,853 parameters, 190.9KB storage
2025-09-24 19:37:45,384 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.26759901607930575, 0.22511552661191483, 0.18625782187444034, 0.15564904923971795, 0.13189837881651148, 0.119382465572445, 0.10975532815912009, 0.10478752683630504, 0.0982018096962209, 0.094052141061258, 0.09002113571604706, 0.08529916762511197, 0.08692993590092968, 0.08392888863138308, 0.08142914112499886, 0.08324678576097518, 0.07792440423483396, 0.07508463286024673, 0.07640288862587141, 0.07245526201621595, 0.07448297507737217, 0.0731728579979121, 0.07246193597251056, 0.07275837940513327, 0.0700736330657627, 0.0691233881599054], 'val_losses': [0.2255328280549945, 0.19633341002488194, 0.15854438807959545, 0.1233069910159641, 0.10583649725076653, 0.12248938967300049, 0.0926333177494474, 0.0994411988984133, 0.08567325738927892, 0.07429619522385095, 0.0772926140968221, 0.07690301729822918, 0.08103082790483494, 0.0711963184176942, 0.0693138940541043, 0.06954833021245793, 0.07347040318439008, 0.06893440646577667, 0.06605446602098476, 0.06579643037404023, 0.06512518706006254, 0.06428214629638047, 0.0667790107120026, 0.061083550245790846, 0.06625266387720721, 0.06518684842542902], 'val_acc': [0.7208369878461346, 0.6647036712191454, 0.8716952762811677, 0.8867309860919684, 0.9183059766946498, 0.9087833604811427, 0.8862297957649418, 0.9275779977446436, 0.9303345445432903, 0.9339681744142339, 0.9318381155243703, 0.936975316376394, 0.9303345445432903, 0.961784237564215, 0.9282044856534268, 0.930459842125047, 0.9353464478135572, 0.9406089462473374, 0.9426137075554442, 0.9273274025811302, 0.9557699536398947, 0.945746147099361, 0.9476256108257111, 0.9436160882094976, 0.9436160882094976, 0.9515098358601679], 'quantized_model_size_bytes': 179458, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.007919994415635049, 'batch_size': 32, 'epochs': 26, 'weight_decay': 0.0006708385199181597, 'dropout': 0.08778953009986402, 'base_channels': 20, 'loss_type': np.str_('focal'), 'gamma': 0.6542839051021816, 'label_smoothing': 0.08385921519794631, 'scheduler': np.str_('none'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.132159456256945, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9090283086038695}, 'model_parameter_count': 88853, 'model_storage_size_kb': 190.8951171875, 'model_size_validation': 'PASS'}
2025-09-24 19:37:45,384 - INFO - _models.training_function_executor - BO Objective: base=0.9515, size_penalty=0.0000, final=0.9515
2025-09-24 19:37:45,384 - INFO - _models.training_function_executor - Model: 88,853 parameters, 190.9KB (PASS 256KB limit)
2025-09-24 19:37:45,384 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 76.173s
2025-09-24 19:37:45,460 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9515
2025-09-24 19:37:45,460 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.076s
2025-09-24 19:37:45,460 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.007919994415635049, 'batch_size': np.int64(32), 'epochs': np.int64(26), 'weight_decay': 0.0006708385199181597, 'dropout': 0.08778953009986402, 'base_channels': np.int64(20), 'loss_type': np.str_('focal'), 'gamma': 0.6542839051021816, 'label_smoothing': 0.08385921519794631, 'scheduler': np.str_('none'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 1.132159456256945, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9090283086038695}, value=0.9515
2025-09-24 19:37:45,460 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.007919994415635049, 'batch_size': np.int64(32), 'epochs': np.int64(26), 'weight_decay': 0.0006708385199181597, 'dropout': 0.08778953009986402, 'base_channels': np.int64(20), 'loss_type': np.str_('focal'), 'gamma': 0.6542839051021816, 'label_smoothing': 0.08385921519794631, 'scheduler': np.str_('none'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 1.132159456256945, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9090283086038695} -> 0.9515
2025-09-24 19:37:45,461 - INFO - bo.run_bo - üîçBO Trial 8: Using RF surrogate + Expected Improvement
2025-09-24 19:37:45,461 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:37:45,461 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:37:45,461 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:37:45,461 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004039338758100801, 'batch_size': 256, 'epochs': 7, 'weight_decay': 0.0006162875823482692, 'dropout': 0.4778436746347924, 'base_channels': 22, 'loss_type': np.str_('ce'), 'gamma': 1.4648131025906939, 'label_smoothing': 0.010258329212993612, 'scheduler': np.str_('none'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.4681627823406314, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9968535843382488}
2025-09-24 19:37:45,462 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004039338758100801, 'batch_size': 256, 'epochs': 7, 'weight_decay': 0.0006162875823482692, 'dropout': 0.4778436746347924, 'base_channels': 22, 'loss_type': np.str_('ce'), 'gamma': 1.4648131025906939, 'label_smoothing': 0.010258329212993612, 'scheduler': np.str_('none'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.4681627823406314, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9968535843382488}
2025-09-24 19:38:02,379 - INFO - _models.training_function_executor - Model: 107,237 parameters, 460.8KB storage
2025-09-24 19:38:02,379 - WARNING - _models.training_function_executor - Model storage 460.8KB exceeds 256KB limit!
2025-09-24 19:38:02,379 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.3894719131510536, 1.2398841410648462, 1.1221191421063872, 1.0036881219749594, 0.9388035414803167, 0.8892993164033373, 0.8386226309326789], 'val_losses': [1.265953879878569, 1.1083283536551576, 0.9528196053970369, 0.8375394431783836, 0.8330761042547591, 0.902053725030873, 0.7220606816142084], 'val_acc': [0.35446685878962536, 0.4476882596165894, 0.5125924069665455, 0.5417867435158501, 0.5875203608570354, 0.6816188447562962, 0.69164265129683], 'quantized_model_size_bytes': 216394, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004039338758100801, 'batch_size': 256, 'epochs': 7, 'weight_decay': 0.0006162875823482692, 'dropout': 0.4778436746347924, 'base_channels': 22, 'loss_type': np.str_('ce'), 'gamma': 1.4648131025906939, 'label_smoothing': 0.010258329212993612, 'scheduler': np.str_('none'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.4681627823406314, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9968535843382488}, 'model_parameter_count': 107237, 'model_storage_size_kb': 460.78398437500005, 'model_size_validation': 'FAIL'}
2025-09-24 19:38:02,380 - INFO - _models.training_function_executor - BO Objective: base=0.6916, size_penalty=0.4000, final=0.2917
2025-09-24 19:38:02,380 - INFO - _models.training_function_executor - Model: 107,237 parameters, 460.8KB (FAIL 256KB limit)
2025-09-24 19:38:02,380 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 16.919s
2025-09-24 19:38:02,458 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2917
2025-09-24 19:38:02,458 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.078s
2025-09-24 19:38:02,458 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 0.004039338758100801, 'batch_size': np.int64(256), 'epochs': np.int64(7), 'weight_decay': 0.0006162875823482692, 'dropout': 0.4778436746347924, 'base_channels': np.int64(22), 'loss_type': np.str_('ce'), 'gamma': 1.4648131025906939, 'label_smoothing': 0.010258329212993612, 'scheduler': np.str_('none'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.4681627823406314, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9968535843382488}, value=0.2917
2025-09-24 19:38:02,458 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 0.004039338758100801, 'batch_size': np.int64(256), 'epochs': np.int64(7), 'weight_decay': 0.0006162875823482692, 'dropout': 0.4778436746347924, 'base_channels': np.int64(22), 'loss_type': np.str_('ce'), 'gamma': 1.4648131025906939, 'label_smoothing': 0.010258329212993612, 'scheduler': np.str_('none'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.4681627823406314, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9968535843382488} -> 0.2917
2025-09-24 19:38:02,458 - INFO - bo.run_bo - üîçBO Trial 9: Using RF surrogate + Expected Improvement
2025-09-24 19:38:02,458 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:38:02,458 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:38:02,458 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:38:02,458 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0018450029894281756, 'batch_size': 64, 'epochs': 6, 'weight_decay': 4.6665490852114286e-05, 'dropout': 0.0352080018550553, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 0.7715678930518933, 'label_smoothing': 0.07133225288872355, 'scheduler': np.str_('plateau'), 'patience': 9, 'use_amp': False, 'grad_clip': 0.30632123653236826, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9043287189781258}
2025-09-24 19:38:02,459 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0018450029894281756, 'batch_size': 64, 'epochs': 6, 'weight_decay': 4.6665490852114286e-05, 'dropout': 0.0352080018550553, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 0.7715678930518933, 'label_smoothing': 0.07133225288872355, 'scheduler': np.str_('plateau'), 'patience': 9, 'use_amp': False, 'grad_clip': 0.30632123653236826, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9043287189781258}
2025-09-24 19:38:10,698 - INFO - _models.training_function_executor - Model: 14,837 parameters, 63.8KB storage
2025-09-24 19:38:10,699 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.19531295497577353, 0.10997975851132771, 0.09299810816453138, 0.08400383568125978, 0.07893006254319085, 0.07306352407893146], 'val_losses': [0.11736204880227304, 0.09719754839608909, 0.1484417890227352, 0.09191665467859256, 0.06531510195152126, 0.06193753010761975], 'val_acc': [0.9022678862297958, 0.916677108131813, 0.9280791880716702, 0.8213256484149856, 0.9249467485277534, 0.9471244204986844], 'quantized_model_size_bytes': 60764, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0018450029894281756, 'batch_size': 64, 'epochs': 6, 'weight_decay': 4.6665490852114286e-05, 'dropout': 0.0352080018550553, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 0.7715678930518933, 'label_smoothing': 0.07133225288872355, 'scheduler': np.str_('plateau'), 'patience': 9, 'use_amp': False, 'grad_clip': 0.30632123653236826, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9043287189781258}, 'model_parameter_count': 14837, 'model_storage_size_kb': 63.752734375, 'model_size_validation': 'PASS'}
2025-09-24 19:38:10,699 - INFO - _models.training_function_executor - BO Objective: base=0.9471, size_penalty=0.0000, final=0.9471
2025-09-24 19:38:10,699 - INFO - _models.training_function_executor - Model: 14,837 parameters, 63.8KB (PASS 256KB limit)
2025-09-24 19:38:10,699 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 8.241s
2025-09-24 19:38:10,780 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9471
2025-09-24 19:38:10,780 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.081s
2025-09-24 19:38:10,780 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 0.0018450029894281756, 'batch_size': np.int64(64), 'epochs': np.int64(6), 'weight_decay': 4.6665490852114286e-05, 'dropout': 0.0352080018550553, 'base_channels': np.int64(8), 'loss_type': np.str_('focal'), 'gamma': 0.7715678930518933, 'label_smoothing': 0.07133225288872355, 'scheduler': np.str_('plateau'), 'patience': np.int64(9), 'use_amp': np.False_, 'grad_clip': 0.30632123653236826, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9043287189781258}, value=0.9471
2025-09-24 19:38:10,780 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 0.0018450029894281756, 'batch_size': np.int64(64), 'epochs': np.int64(6), 'weight_decay': 4.6665490852114286e-05, 'dropout': 0.0352080018550553, 'base_channels': np.int64(8), 'loss_type': np.str_('focal'), 'gamma': 0.7715678930518933, 'label_smoothing': 0.07133225288872355, 'scheduler': np.str_('plateau'), 'patience': np.int64(9), 'use_amp': np.False_, 'grad_clip': 0.30632123653236826, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9043287189781258} -> 0.9471
2025-09-24 19:38:10,780 - INFO - bo.run_bo - üîçBO Trial 10: Using RF surrogate + Expected Improvement
2025-09-24 19:38:10,780 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:38:10,780 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:38:10,780 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:38:10,780 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00546390276296302, 'batch_size': 256, 'epochs': 17, 'weight_decay': 1.091734478951149e-05, 'dropout': 0.046214648949905186, 'base_channels': 23, 'loss_type': np.str_('class_balanced'), 'gamma': 1.2068162404111296, 'label_smoothing': 0.059914742904353946, 'scheduler': np.str_('plateau'), 'patience': 4, 'use_amp': False, 'grad_clip': 1.6015590279694611, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9206057915864577}
2025-09-24 19:38:10,782 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00546390276296302, 'batch_size': 256, 'epochs': 17, 'weight_decay': 1.091734478951149e-05, 'dropout': 0.046214648949905186, 'base_channels': 23, 'loss_type': np.str_('class_balanced'), 'gamma': 1.2068162404111296, 'label_smoothing': 0.059914742904353946, 'scheduler': np.str_('plateau'), 'patience': 4, 'use_amp': False, 'grad_clip': 1.6015590279694611, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9206057915864577}
2025-09-24 19:38:52,458 - INFO - _models.training_function_executor - Model: 117,077 parameters, 503.1KB storage
2025-09-24 19:38:52,458 - WARNING - _models.training_function_executor - Model storage 503.1KB exceeds 256KB limit!
2025-09-24 19:38:52,458 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7616342884688451, 0.45486754845849175, 0.39024182920807254, 0.36865003225002224, 0.3582029983593147, 0.35060569747227033, 0.3442311065894479, 0.3376050320827165, 0.336482647184299, 0.3326820431356684, 0.3289984930627004, 0.3227877929618595, 0.3196894887380723, 0.31937003079667015, 0.31971596850528233, 0.31495919402793654, 0.3144620510704912], 'val_losses': [0.7455058461247702, 0.3934425955098937, 0.3618652400232888, 0.3613587460068589, 0.35825349313396365, 0.3662052462609127, 0.34183646190541384, 0.33730401179946135, 0.3342116381863337, 0.3341874493839836, 0.3273624432906966, 0.3202381691617814, 0.31886847010084207, 0.3151770935980483, 0.31168430206799863, 0.3207930416529705, 0.3196501258418548], 'val_acc': [0.8214509459967423, 0.9525122165142212, 0.9619095351459717, 0.9639142964540784, 0.9609071544919183, 0.9557699536398947, 0.9691767948878587, 0.9723092344317754, 0.9706803658689387, 0.9715574489412354, 0.9753163763939356, 0.977822328029069, 0.9758175667209623, 0.9771958401202857, 0.9800776845006891, 0.9759428643027189, 0.977070542538529], 'quantized_model_size_bytes': 472244, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00546390276296302, 'batch_size': 256, 'epochs': 17, 'weight_decay': 1.091734478951149e-05, 'dropout': 0.046214648949905186, 'base_channels': 23, 'loss_type': np.str_('class_balanced'), 'gamma': 1.2068162404111296, 'label_smoothing': 0.059914742904353946, 'scheduler': np.str_('plateau'), 'patience': 4, 'use_amp': False, 'grad_clip': 1.6015590279694611, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9206057915864577}, 'model_parameter_count': 117077, 'model_storage_size_kb': 503.06523437500005, 'model_size_validation': 'FAIL'}
2025-09-24 19:38:52,458 - INFO - _models.training_function_executor - BO Objective: base=0.9771, size_penalty=0.4825, final=0.4945
2025-09-24 19:38:52,458 - INFO - _models.training_function_executor - Model: 117,077 parameters, 503.1KB (FAIL 256KB limit)
2025-09-24 19:38:52,458 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 41.677s
2025-09-24 19:38:52,541 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.4945
2025-09-24 19:38:52,541 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.083s
2025-09-24 19:38:52,541 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 0.00546390276296302, 'batch_size': np.int64(256), 'epochs': np.int64(17), 'weight_decay': 1.091734478951149e-05, 'dropout': 0.046214648949905186, 'base_channels': np.int64(23), 'loss_type': np.str_('class_balanced'), 'gamma': 1.2068162404111296, 'label_smoothing': 0.059914742904353946, 'scheduler': np.str_('plateau'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 1.6015590279694611, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9206057915864577}, value=0.4945
2025-09-24 19:38:52,541 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 0.00546390276296302, 'batch_size': np.int64(256), 'epochs': np.int64(17), 'weight_decay': 1.091734478951149e-05, 'dropout': 0.046214648949905186, 'base_channels': np.int64(23), 'loss_type': np.str_('class_balanced'), 'gamma': 1.2068162404111296, 'label_smoothing': 0.059914742904353946, 'scheduler': np.str_('plateau'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 1.6015590279694611, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9206057915864577} -> 0.4945
2025-09-24 19:38:52,542 - INFO - bo.run_bo - üîçBO Trial 11: Using RF surrogate + Expected Improvement
2025-09-24 19:38:52,542 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:38:52,542 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:38:52,542 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:38:52,542 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0021132274308314604, 'batch_size': 64, 'epochs': 20, 'weight_decay': 0.007122310714599921, 'dropout': 0.07872770483929926, 'base_channels': 11, 'loss_type': np.str_('ce'), 'gamma': 3.065523219026358, 'label_smoothing': 0.08256773031152653, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': False, 'grad_clip': 0.5085622269021716, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 5, 'beta_cb': 0.9810704359097564}
2025-09-24 19:38:52,543 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0021132274308314604, 'batch_size': 64, 'epochs': 20, 'weight_decay': 0.007122310714599921, 'dropout': 0.07872770483929926, 'base_channels': 11, 'loss_type': np.str_('ce'), 'gamma': 3.065523219026358, 'label_smoothing': 0.08256773031152653, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': False, 'grad_clip': 0.5085622269021716, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 5, 'beta_cb': 0.9810704359097564}
2025-09-24 19:39:26,458 - INFO - _models.training_function_executor - Model: 27,509 parameters, 59.1KB storage
2025-09-24 19:39:26,458 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.7405541985830268, 1.413261988607896, 1.34520848960322, 1.3002279818933702, 1.2691522341408168, 1.2620769942463652, 1.2413628880542886, 1.2311264903937553, 1.2302756641971861, 1.2242961681953772, 1.2070006340685164, 1.1998071354351085, 1.2001468196151377, 1.1903726416632252, 1.197499233906428, 1.1864596688052458, 1.1844641910707487, 1.1807369938755015, 1.1811184032038855, 1.1788421866327876], 'val_losses': [1.479267171862848, 1.3443606893993263, 1.2680643296842213, 1.2259450902946012, 1.2329141510233457, 1.2165464908314025, 1.2230319522732915, 1.1981760365161482, 1.180570966512424, 1.172139821532316, 1.1861941066694623, 1.1811120497255811, 1.1988124353158833, 1.1680069486534157, 1.1574733370961499, 1.1860716056441352, 1.1669484832176005, 1.173004205745797, 1.1635317734616872, 1.1691400277377102], 'val_acc': [0.3084826462849267, 0.661070041348202, 0.7610575115900263, 0.8919934845257487, 0.8457586768575367, 0.8715699786994111, 0.8228292193960657, 0.8555318882345571, 0.8418744518230799, 0.8812178924946749, 0.9362235308858539, 0.9432401954642275, 0.9373512091216639, 0.9269515098358602, 0.9120410976068162, 0.8483899260744268, 0.9314622227791004, 0.9101616338804661, 0.9379776970304473, 0.938854780102744], 'quantized_model_size_bytes': 56014, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0021132274308314604, 'batch_size': 64, 'epochs': 20, 'weight_decay': 0.007122310714599921, 'dropout': 0.07872770483929926, 'base_channels': 11, 'loss_type': np.str_('ce'), 'gamma': 3.065523219026358, 'label_smoothing': 0.08256773031152653, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': False, 'grad_clip': 0.5085622269021716, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 5, 'beta_cb': 0.9810704359097564}, 'model_parameter_count': 27509, 'model_storage_size_kb': 59.1013671875, 'model_size_validation': 'PASS'}
2025-09-24 19:39:26,458 - INFO - _models.training_function_executor - BO Objective: base=0.9389, size_penalty=0.0000, final=0.9389
2025-09-24 19:39:26,458 - INFO - _models.training_function_executor - Model: 27,509 parameters, 59.1KB (PASS 256KB limit)
2025-09-24 19:39:26,458 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 33.917s
2025-09-24 19:39:26,542 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9389
2025-09-24 19:39:26,543 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-09-24 19:39:26,543 - INFO - bo.run_bo - Recorded observation #11: hparams={'lr': 0.0021132274308314604, 'batch_size': np.int64(64), 'epochs': np.int64(20), 'weight_decay': 0.007122310714599921, 'dropout': 0.07872770483929926, 'base_channels': np.int64(11), 'loss_type': np.str_('ce'), 'gamma': 3.065523219026358, 'label_smoothing': 0.08256773031152653, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 0.5085622269021716, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9810704359097564}, value=0.9389
2025-09-24 19:39:26,543 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 11: {'lr': 0.0021132274308314604, 'batch_size': np.int64(64), 'epochs': np.int64(20), 'weight_decay': 0.007122310714599921, 'dropout': 0.07872770483929926, 'base_channels': np.int64(11), 'loss_type': np.str_('ce'), 'gamma': 3.065523219026358, 'label_smoothing': 0.08256773031152653, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 0.5085622269021716, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9810704359097564} -> 0.9389
2025-09-24 19:39:26,543 - INFO - bo.run_bo - üîçBO Trial 12: Using RF surrogate + Expected Improvement
2025-09-24 19:39:26,543 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:39:26,543 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:39:26,543 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:39:26,543 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0052184773690551285, 'batch_size': 64, 'epochs': 49, 'weight_decay': 0.005457532262376124, 'dropout': 0.2681965753161043, 'base_channels': 12, 'loss_type': np.str_('focal'), 'gamma': 1.874786972651631, 'label_smoothing': 0.06571514274763961, 'scheduler': np.str_('onecycle'), 'patience': 6, 'use_amp': False, 'grad_clip': 0.46869668740734094, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 9, 'beta_cb': 0.9642500059568132}
2025-09-24 19:39:26,544 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0052184773690551285, 'batch_size': 64, 'epochs': 49, 'weight_decay': 0.005457532262376124, 'dropout': 0.2681965753161043, 'base_channels': 12, 'loss_type': np.str_('focal'), 'gamma': 1.874786972651631, 'label_smoothing': 0.06571514274763961, 'scheduler': np.str_('onecycle'), 'patience': 6, 'use_amp': False, 'grad_clip': 0.46869668740734094, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 9, 'beta_cb': 0.9642500059568132}
2025-09-24 19:40:46,655 - INFO - _models.training_function_executor - Model: 32,597 parameters, 70.0KB storage
2025-09-24 19:40:46,655 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.21608996304754544, 0.15971378422439186, 0.11952189313424535, 0.09271972647196249, 0.07842264551762367, 0.07193924212389559, 0.06790672595122085, 0.06058788195315901, 0.05951139839694053, 0.056188092612683975, 0.053315793574173764, 0.051308111275137784, 0.04841781441086468, 0.04707977415348072, 0.04417998448009125, 0.04243454261734361, 0.04193223174993982, 0.04117555779859499, 0.03809046640343801, 0.03896574746427935, 0.036867953549612024, 0.033551519801997436, 0.03594280629558913, 0.03267877501087619, 0.03070014789226722, 0.0309158985437989, 0.029702518606644857, 0.029778549816473772, 0.02955781606505644, 0.027402668964969998, 0.027068810132171835, 0.02446633306993979, 0.024089013849866833, 0.024276200891992426, 0.021829855026743127, 0.022098632538188494, 0.02138369612943535, 0.020618731558680697, 0.019794260035309076, 0.01877961645409695, 0.019561275406687947, 0.01838902411566979, 0.018145557933659736, 0.017274029907342676, 0.01720849965037846, 0.017114213469864168, 0.01635380910159978, 0.017252671124800865, 0.015937191301185587], 'val_losses': [0.16776173862597948, 0.11527072720144674, 0.08681293238360935, 0.06703765469637898, 0.0770309170916473, 0.05531712076029836, 0.04792509580635738, 0.04161973399771901, 0.04277585753362335, 0.04935434218246362, 0.0403435136200999, 0.04209500057661299, 0.04457850177193432, 0.03413212810319896, 0.04111781119908206, 0.035265721883421745, 0.0406874716249217, 0.03567705578684747, 0.036907327671690165, 0.03540422657243053, 0.033075841677478644, 0.03209316832208421, 0.031230817179365603, 0.029554767579180064, 0.03412801888861792, 0.030749237110009545, 0.02264889505970838, 0.027156822495895315, 0.021342751334677244, 0.021059852296969838, 0.025101424383797484, 0.023813547761876278, 0.023046355713669957, 0.01924484963751111, 0.01996991183301169, 0.01843267291282042, 0.0177403337065102, 0.01937178595817981, 0.018232296232744173, 0.01912569878241305, 0.01725511375920633, 0.01770906754621302, 0.017206249069415484, 0.017181712478705476, 0.01787190740026488, 0.0173973516936724, 0.017252902355749437, 0.01713111944045732, 0.017249987768624273], 'val_acc': [0.6735997995238692, 0.7395063275278787, 0.838742012279163, 0.8758300964791379, 0.7955143465731112, 0.9022678862297958, 0.8446309986217266, 0.8737000375892745, 0.9013908031574991, 0.907154491918306, 0.9013908031574991, 0.8896128304723718, 0.9270768074176169, 0.9402330535020674, 0.9201854404209999, 0.9423631123919308, 0.8958777095602055, 0.9020172910662824, 0.8686881343190076, 0.8872321764189951, 0.8695652173913043, 0.8782107505325147, 0.8856033078561584, 0.9191830597669465, 0.8681869439919809, 0.8901140207993986, 0.9330910913419371, 0.9463726350081444, 0.9414860293196341, 0.9557699536398947, 0.9137952637514096, 0.9294574614709936, 0.9413607317378775, 0.9364741260493672, 0.9408595414108508, 0.9374765067034206, 0.9467485277534143, 0.9418619220649042, 0.9448690640270643, 0.9456208495176043, 0.9348452574865305, 0.9368500187946373, 0.9433654930459842, 0.9476256108257111, 0.9353464478135572, 0.9391053752662574, 0.940358351083824, 0.9402330535020674, 0.9411101365743642], 'quantized_model_size_bytes': 66274, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0052184773690551285, 'batch_size': 64, 'epochs': 49, 'weight_decay': 0.005457532262376124, 'dropout': 0.2681965753161043, 'base_channels': 12, 'loss_type': np.str_('focal'), 'gamma': 1.874786972651631, 'label_smoothing': 0.06571514274763961, 'scheduler': np.str_('onecycle'), 'patience': 6, 'use_amp': False, 'grad_clip': 0.46869668740734094, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 9, 'beta_cb': 0.9642500059568132}, 'model_parameter_count': 32597, 'model_storage_size_kb': 70.03261718750001, 'model_size_validation': 'PASS'}
2025-09-24 19:40:46,656 - INFO - _models.training_function_executor - BO Objective: base=0.9411, size_penalty=0.0000, final=0.9411
2025-09-24 19:40:46,656 - INFO - _models.training_function_executor - Model: 32,597 parameters, 70.0KB (PASS 256KB limit)
2025-09-24 19:40:46,656 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 80.113s
2025-09-24 19:40:46,743 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9411
2025-09-24 19:40:46,743 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.087s
2025-09-24 19:40:46,743 - INFO - bo.run_bo - Recorded observation #12: hparams={'lr': 0.0052184773690551285, 'batch_size': np.int64(64), 'epochs': np.int64(49), 'weight_decay': 0.005457532262376124, 'dropout': 0.2681965753161043, 'base_channels': np.int64(12), 'loss_type': np.str_('focal'), 'gamma': 1.874786972651631, 'label_smoothing': 0.06571514274763961, 'scheduler': np.str_('onecycle'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 0.46869668740734094, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(9), 'beta_cb': 0.9642500059568132}, value=0.9411
2025-09-24 19:40:46,743 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 12: {'lr': 0.0052184773690551285, 'batch_size': np.int64(64), 'epochs': np.int64(49), 'weight_decay': 0.005457532262376124, 'dropout': 0.2681965753161043, 'base_channels': np.int64(12), 'loss_type': np.str_('focal'), 'gamma': 1.874786972651631, 'label_smoothing': 0.06571514274763961, 'scheduler': np.str_('onecycle'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 0.46869668740734094, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(9), 'beta_cb': 0.9642500059568132} -> 0.9411
2025-09-24 19:40:46,743 - INFO - bo.run_bo - üîçBO Trial 13: Using RF surrogate + Expected Improvement
2025-09-24 19:40:46,743 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:40:46,743 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:40:46,743 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:40:46,743 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.009934674800342987, 'batch_size': 64, 'epochs': 18, 'weight_decay': 0.0026838277006931833, 'dropout': 0.4279458887965337, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 3.0800853797819716, 'label_smoothing': 0.06818546352150284, 'scheduler': np.str_('none'), 'patience': 4, 'use_amp': False, 'grad_clip': 0.2885710160607467, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.9785071213295737}
2025-09-24 19:40:46,745 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.009934674800342987, 'batch_size': 64, 'epochs': 18, 'weight_decay': 0.0026838277006931833, 'dropout': 0.4279458887965337, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 3.0800853797819716, 'label_smoothing': 0.06818546352150284, 'scheduler': np.str_('none'), 'patience': 4, 'use_amp': False, 'grad_clip': 0.2885710160607467, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.9785071213295737}
2025-09-24 19:41:11,352 - INFO - _models.training_function_executor - Model: 14,837 parameters, 63.8KB storage
2025-09-24 19:41:11,352 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.16091284297748126, 0.14165697398611587, 0.13599998963554433, 0.13319736888480882, 0.1313281233815695, 0.12547710252961872, 0.12122780151430185, 0.1173261170789935, 0.11053627505406018, 0.10155627104948738, 0.09349468649222552, 0.08694725007209003, 0.08312050995631923, 0.07955089093017365, 0.07909177750637518, 0.07386116903805402, 0.07264230191181079, 0.07094620657469236], 'val_losses': [0.1347555631575407, 0.14347625284726093, 0.12142395998317786, 0.11472814254459919, 0.14194075559807248, 0.1217886196353011, 0.12229107752724289, 0.0979164689433872, 0.08596167699307251, 0.07539782542471614, 0.06202535739999609, 0.05645853643794658, 0.051029066501640165, 0.056658740716106146, 0.05879787173107698, 0.04571858234586848, 0.04886637674942155, 0.05393500733304227], 'val_acc': [0.38190702919433656, 0.3609823330409723, 0.6669590276907655, 0.6157123167522867, 0.5445432903144969, 0.6650795639644155, 0.47174539531387044, 0.5776218518982583, 0.7189575241197845, 0.7606816188447563, 0.7985214885352713, 0.8347324896629496, 0.8871068788372385, 0.8451321889487533, 0.8788372384412981, 0.8788372384412981, 0.8898634256358853, 0.909033955644656], 'quantized_model_size_bytes': 60764, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.009934674800342987, 'batch_size': 64, 'epochs': 18, 'weight_decay': 0.0026838277006931833, 'dropout': 0.4279458887965337, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 3.0800853797819716, 'label_smoothing': 0.06818546352150284, 'scheduler': np.str_('none'), 'patience': 4, 'use_amp': False, 'grad_clip': 0.2885710160607467, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.9785071213295737}, 'model_parameter_count': 14837, 'model_storage_size_kb': 63.752734375, 'model_size_validation': 'PASS'}
2025-09-24 19:41:11,352 - INFO - _models.training_function_executor - BO Objective: base=0.9090, size_penalty=0.0000, final=0.9090
2025-09-24 19:41:11,352 - INFO - _models.training_function_executor - Model: 14,837 parameters, 63.8KB (PASS 256KB limit)
2025-09-24 19:41:11,352 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 24.609s
2025-09-24 19:41:11,440 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9090
2025-09-24 19:41:11,441 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.088s
2025-09-24 19:41:11,441 - INFO - bo.run_bo - Recorded observation #13: hparams={'lr': 0.009934674800342987, 'batch_size': np.int64(64), 'epochs': np.int64(18), 'weight_decay': 0.0026838277006931833, 'dropout': 0.4279458887965337, 'base_channels': np.int64(8), 'loss_type': np.str_('focal'), 'gamma': 3.0800853797819716, 'label_smoothing': 0.06818546352150284, 'scheduler': np.str_('none'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 0.2885710160607467, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9785071213295737}, value=0.9090
2025-09-24 19:41:11,441 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 13: {'lr': 0.009934674800342987, 'batch_size': np.int64(64), 'epochs': np.int64(18), 'weight_decay': 0.0026838277006931833, 'dropout': 0.4279458887965337, 'base_channels': np.int64(8), 'loss_type': np.str_('focal'), 'gamma': 3.0800853797819716, 'label_smoothing': 0.06818546352150284, 'scheduler': np.str_('none'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 0.2885710160607467, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9785071213295737} -> 0.9090
2025-09-24 19:41:11,441 - INFO - bo.run_bo - üîçBO Trial 14: Using RF surrogate + Expected Improvement
2025-09-24 19:41:11,441 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:41:11,441 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:41:11,441 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:41:11,441 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006848017599196966, 'batch_size': 64, 'epochs': 9, 'weight_decay': 0.0003991164317024957, 'dropout': 0.010377630492929892, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 0.6139179043894156, 'label_smoothing': 0.06810291161118615, 'scheduler': np.str_('plateau'), 'patience': 5, 'use_amp': True, 'grad_clip': 1.4515062708983435, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 10, 'beta_cb': 0.9289781643808026}
2025-09-24 19:41:11,442 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006848017599196966, 'batch_size': 64, 'epochs': 9, 'weight_decay': 0.0003991164317024957, 'dropout': 0.010377630492929892, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 0.6139179043894156, 'label_smoothing': 0.06810291161118615, 'scheduler': np.str_('plateau'), 'patience': 5, 'use_amp': True, 'grad_clip': 1.4515062708983435, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 10, 'beta_cb': 0.9289781643808026}
2025-09-24 19:41:25,735 - INFO - _models.training_function_executor - Model: 14,837 parameters, 31.9KB storage
2025-09-24 19:41:25,735 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.24461703904980656, 0.19788894217007516, 0.1804824173599841, 0.1563908880125986, 0.1284793477779798, 0.11581307736632725, 0.10306173792498133, 0.09310594615032435, 0.08758651859085295], 'val_losses': [0.20624516893338327, 0.19428934373289194, 1.0058255289195699, 0.1298629113485484, 0.11559189656414927, 0.11743008074872387, 0.09815747556068323, 0.08430532427184222, 0.08231193504461129], 'val_acc': [0.6569352211502318, 0.6392682621225411, 0.5016915173537151, 0.8525247462723969, 0.8411226663325397, 0.8859792006014284, 0.9196842500939731, 0.8944994361608821, 0.9139205613331662], 'quantized_model_size_bytes': 30418, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006848017599196966, 'batch_size': 64, 'epochs': 9, 'weight_decay': 0.0003991164317024957, 'dropout': 0.010377630492929892, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 0.6139179043894156, 'label_smoothing': 0.06810291161118615, 'scheduler': np.str_('plateau'), 'patience': 5, 'use_amp': True, 'grad_clip': 1.4515062708983435, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 10, 'beta_cb': 0.9289781643808026}, 'model_parameter_count': 14837, 'model_storage_size_kb': 31.8763671875, 'model_size_validation': 'PASS'}
2025-09-24 19:41:25,735 - INFO - _models.training_function_executor - BO Objective: base=0.9139, size_penalty=0.0000, final=0.9139
2025-09-24 19:41:25,735 - INFO - _models.training_function_executor - Model: 14,837 parameters, 31.9KB (PASS 256KB limit)
2025-09-24 19:41:25,735 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 14.294s
2025-09-24 19:41:25,824 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9139
2025-09-24 19:41:25,824 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.089s
2025-09-24 19:41:25,824 - INFO - bo.run_bo - Recorded observation #14: hparams={'lr': 0.006848017599196966, 'batch_size': np.int64(64), 'epochs': np.int64(9), 'weight_decay': 0.0003991164317024957, 'dropout': 0.010377630492929892, 'base_channels': np.int64(8), 'loss_type': np.str_('focal'), 'gamma': 0.6139179043894156, 'label_smoothing': 0.06810291161118615, 'scheduler': np.str_('plateau'), 'patience': np.int64(5), 'use_amp': np.True_, 'grad_clip': 1.4515062708983435, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(10), 'beta_cb': 0.9289781643808026}, value=0.9139
2025-09-24 19:41:25,824 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 14: {'lr': 0.006848017599196966, 'batch_size': np.int64(64), 'epochs': np.int64(9), 'weight_decay': 0.0003991164317024957, 'dropout': 0.010377630492929892, 'base_channels': np.int64(8), 'loss_type': np.str_('focal'), 'gamma': 0.6139179043894156, 'label_smoothing': 0.06810291161118615, 'scheduler': np.str_('plateau'), 'patience': np.int64(5), 'use_amp': np.True_, 'grad_clip': 1.4515062708983435, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(10), 'beta_cb': 0.9289781643808026} -> 0.9139
2025-09-24 19:41:25,824 - INFO - bo.run_bo - üîçBO Trial 15: Using RF surrogate + Expected Improvement
2025-09-24 19:41:25,825 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:41:25,825 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:41:25,825 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:41:25,825 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0019829807816324927, 'batch_size': 32, 'epochs': 32, 'weight_decay': 0.007291163639345443, 'dropout': 0.1005473609478226, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 1.5397754871082816, 'label_smoothing': 0.07457849404199256, 'scheduler': np.str_('plateau'), 'patience': 4, 'use_amp': False, 'grad_clip': 1.9398951881047302, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 3, 'beta_cb': 0.9278767442508984}
2025-09-24 19:41:25,826 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0019829807816324927, 'batch_size': 32, 'epochs': 32, 'weight_decay': 0.007291163639345443, 'dropout': 0.1005473609478226, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 1.5397754871082816, 'label_smoothing': 0.07457849404199256, 'scheduler': np.str_('plateau'), 'patience': 4, 'use_amp': False, 'grad_clip': 1.9398951881047302, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 3, 'beta_cb': 0.9278767442508984}
2025-09-24 19:42:32,752 - INFO - _models.training_function_executor - Model: 14,837 parameters, 63.8KB storage
2025-09-24 19:42:32,752 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2032046878774397, 0.1478629834793364, 0.11478975047356943, 0.09352779070640856, 0.08040533432795866, 0.07314531393056263, 0.07038940599476623, 0.062584082398525, 0.06032222030253218, 0.05584808643748018, 0.05481789210689952, 0.05192032679637691, 0.049481731355451676, 0.041722556397762754, 0.040915251258577826, 0.039501160293835255, 0.04046386758523306, 0.03819321190216413, 0.0367255212322038, 0.037383636559250005, 0.03308445877650756, 0.031156901637145854, 0.03144004142014965, 0.03061784366443597, 0.030045309021725423, 0.028654851198452676, 0.028172764695857104, 0.028280908928916437, 0.028700600168599905, 0.028200373345991202, 0.02728606808183897, 0.026422919083111375], 'val_losses': [0.15877907630301197, 0.11600133397927664, 0.08770524459294606, 0.10241854224247138, 0.06734978674146977, 0.05848030445943454, 0.053476088159295075, 0.05034952379866749, 0.05040775800524999, 0.04885191891644402, 0.04766571714721696, 0.0423053642133919, 0.05156145789183825, 0.0354625921831321, 0.034918552100501585, 0.03543780950294702, 0.03403780374677389, 0.03474168045813391, 0.03381836606833051, 0.03251855534940954, 0.030056123655252537, 0.03060342626786892, 0.030291195580302407, 0.031481018993112474, 0.02922973277525949, 0.02621084065376674, 0.027283387670106508, 0.028458797710235517, 0.027873938655124365, 0.02588262190710197, 0.025652716653028266, 0.025972071607046267], 'val_acc': [0.747400075178549, 0.724846510462348, 0.8476381405838868, 0.7836110763062273, 0.8575366495426638, 0.8961283047237188, 0.9030196717203358, 0.9223154993108633, 0.9008896128304724, 0.9216890114020799, 0.915925322641273, 0.9075303846635759, 0.837990226788623, 0.9307104372885603, 0.9558952512216514, 0.9283297832351836, 0.9340934719959905, 0.9121663951885729, 0.9193083573487032, 0.9160506202230297, 0.9287056759804536, 0.9448690640270643, 0.9372259115399073, 0.9322140082696404, 0.9274527001628868, 0.9412354341561208, 0.9379776970304473, 0.9374765067034206, 0.946122039844631, 0.9487532890615211, 0.9423631123919308, 0.9467485277534143], 'quantized_model_size_bytes': 30418, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0019829807816324927, 'batch_size': 32, 'epochs': 32, 'weight_decay': 0.007291163639345443, 'dropout': 0.1005473609478226, 'base_channels': 8, 'loss_type': np.str_('focal'), 'gamma': 1.5397754871082816, 'label_smoothing': 0.07457849404199256, 'scheduler': np.str_('plateau'), 'patience': 4, 'use_amp': False, 'grad_clip': 1.9398951881047302, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 3, 'beta_cb': 0.9278767442508984}, 'model_parameter_count': 14837, 'model_storage_size_kb': 63.752734375, 'model_size_validation': 'PASS'}
2025-09-24 19:42:32,752 - INFO - _models.training_function_executor - BO Objective: base=0.9467, size_penalty=0.0000, final=0.9467
2025-09-24 19:42:32,752 - INFO - _models.training_function_executor - Model: 14,837 parameters, 63.8KB (PASS 256KB limit)
2025-09-24 19:42:32,752 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 66.928s
2025-09-24 19:42:32,842 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9467
2025-09-24 19:42:32,842 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.089s
2025-09-24 19:42:32,842 - INFO - bo.run_bo - Recorded observation #15: hparams={'lr': 0.0019829807816324927, 'batch_size': np.int64(32), 'epochs': np.int64(32), 'weight_decay': 0.007291163639345443, 'dropout': 0.1005473609478226, 'base_channels': np.int64(8), 'loss_type': np.str_('focal'), 'gamma': 1.5397754871082816, 'label_smoothing': 0.07457849404199256, 'scheduler': np.str_('plateau'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 1.9398951881047302, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9278767442508984}, value=0.9467
2025-09-24 19:42:32,842 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 15: {'lr': 0.0019829807816324927, 'batch_size': np.int64(32), 'epochs': np.int64(32), 'weight_decay': 0.007291163639345443, 'dropout': 0.1005473609478226, 'base_channels': np.int64(8), 'loss_type': np.str_('focal'), 'gamma': 1.5397754871082816, 'label_smoothing': 0.07457849404199256, 'scheduler': np.str_('plateau'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 1.9398951881047302, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9278767442508984} -> 0.9467
2025-09-24 19:42:32,842 - INFO - bo.run_bo - üîçBO Trial 16: Using RF surrogate + Expected Improvement
2025-09-24 19:42:32,842 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:42:32,842 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:42:32,842 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:42:32,842 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006344561991395911, 'batch_size': 256, 'epochs': 36, 'weight_decay': 0.008354953360649687, 'dropout': 0.09919123600177168, 'base_channels': 8, 'loss_type': np.str_('class_balanced'), 'gamma': 2.858269594068937, 'label_smoothing': 0.09228011642239412, 'scheduler': np.str_('plateau'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.3812888883553367, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9652601773143933}
2025-09-24 19:42:32,844 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006344561991395911, 'batch_size': 256, 'epochs': 36, 'weight_decay': 0.008354953360649687, 'dropout': 0.09919123600177168, 'base_channels': 8, 'loss_type': np.str_('class_balanced'), 'gamma': 2.858269594068937, 'label_smoothing': 0.09228011642239412, 'scheduler': np.str_('plateau'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.3812888883553367, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9652601773143933}
2025-09-24 19:43:12,155 - INFO - _models.training_function_executor - Model: 14,837 parameters, 63.8KB storage
2025-09-24 19:43:12,155 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9141440696537512, 0.6452251683667508, 0.5574099470868287, 0.5299037331473957, 0.5133900032261016, 0.5035440710283396, 0.49838776363524506, 0.4882369319459712, 0.47726021558013665, 0.47212106751435173, 0.46531018798588714, 0.46175380424610285, 0.45671555399521285, 0.4539476423952449, 0.4511449464727187, 0.4492858107679713, 0.444471701953964, 0.4469215076332593, 0.4424850177922479, 0.43823766195938013, 0.4387117324849924, 0.43542448320928917, 0.4358837666324971, 0.43629770652283567, 0.4388287320205137, 0.4306484299173885, 0.4262544511021616, 0.4254200208031384, 0.4239993121463195, 0.42529224201323146, 0.42265170002362296, 0.42298481721528225, 0.42153795602398325, 0.4214280493851761, 0.4242020045876731, 0.4218336075681592], 'val_losses': [0.720170854894817, 0.5712594621956102, 0.5118232745975498, 0.5157265644550861, 0.4889839221034309, 0.48570272628562877, 0.4747038333991942, 0.4932564959805676, 0.4701853916618463, 0.4627785817929459, 0.4541613588788996, 0.44700886032632414, 0.4577600849016823, 0.4560554717704626, 0.4501507900969873, 0.43697403712076077, 0.43647252093235545, 0.42773454003115374, 0.42489885051602067, 0.4285573800686173, 0.4236624463431116, 0.42420428549566747, 0.42415761980492434, 0.4315937508797081, 0.4234370661101504, 0.4200184945660775, 0.4144086223760802, 0.41529594939643166, 0.4182840117669138, 0.4128692454898018, 0.4117416490544772, 0.41294110174571014, 0.41564898823395274, 0.4103985949239849, 0.4143578986478709, 0.41379021457107634], 'val_acc': [0.8495176043102368, 0.9050244330284426, 0.9426137075554442, 0.9340934719959905, 0.9490038842250345, 0.9516351334419245, 0.9516351334419245, 0.9452449567723343, 0.9548928705675981, 0.9592782859290816, 0.9634131061270518, 0.9661696529256986, 0.961408344818945, 0.9551434657311114, 0.962536023054755, 0.9684250093973187, 0.9696779852148854, 0.9721839368500188, 0.9759428643027189, 0.9751910788121789, 0.977446435283799, 0.975566971557449, 0.9749404836486656, 0.9736875078310988, 0.9768199473750157, 0.975566971557449, 0.9786994111013657, 0.9788247086831224, 0.977070542538529, 0.9798270893371758, 0.9808294699912292, 0.9793258990101491, 0.9794511965919058, 0.9814559579000125, 0.9808294699912292, 0.9804535772459592], 'quantized_model_size_bytes': 30418, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006344561991395911, 'batch_size': 256, 'epochs': 36, 'weight_decay': 0.008354953360649687, 'dropout': 0.09919123600177168, 'base_channels': 8, 'loss_type': np.str_('class_balanced'), 'gamma': 2.858269594068937, 'label_smoothing': 0.09228011642239412, 'scheduler': np.str_('plateau'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.3812888883553367, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9652601773143933}, 'model_parameter_count': 14837, 'model_storage_size_kb': 63.752734375, 'model_size_validation': 'PASS'}
2025-09-24 19:43:12,155 - INFO - _models.training_function_executor - BO Objective: base=0.9805, size_penalty=0.0000, final=0.9805
2025-09-24 19:43:12,155 - INFO - _models.training_function_executor - Model: 14,837 parameters, 63.8KB (PASS 256KB limit)
2025-09-24 19:43:12,155 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 39.313s
2025-09-24 19:43:12,377 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9805
2025-09-24 19:43:12,377 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.222s
2025-09-24 19:43:12,377 - INFO - bo.run_bo - Recorded observation #16: hparams={'lr': 0.006344561991395911, 'batch_size': np.int64(256), 'epochs': np.int64(36), 'weight_decay': 0.008354953360649687, 'dropout': 0.09919123600177168, 'base_channels': np.int64(8), 'loss_type': np.str_('class_balanced'), 'gamma': 2.858269594068937, 'label_smoothing': 0.09228011642239412, 'scheduler': np.str_('plateau'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.3812888883553367, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9652601773143933}, value=0.9805
2025-09-24 19:43:12,377 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 16: {'lr': 0.006344561991395911, 'batch_size': np.int64(256), 'epochs': np.int64(36), 'weight_decay': 0.008354953360649687, 'dropout': 0.09919123600177168, 'base_channels': np.int64(8), 'loss_type': np.str_('class_balanced'), 'gamma': 2.858269594068937, 'label_smoothing': 0.09228011642239412, 'scheduler': np.str_('plateau'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.3812888883553367, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9652601773143933} -> 0.9805
2025-09-24 19:43:12,377 - INFO - bo.run_bo - üîçBO Trial 17: Using RF surrogate + Expected Improvement
2025-09-24 19:43:12,377 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:43:12,377 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:43:12,377 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:43:12,377 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004451073809947629, 'batch_size': 256, 'epochs': 27, 'weight_decay': 0.0014385512174681057, 'dropout': 0.17381539725156256, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 4.496480955203446, 'label_smoothing': 0.07176676888572879, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.479767217333263, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9083036598180358}
2025-09-24 19:43:12,379 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004451073809947629, 'batch_size': 256, 'epochs': 27, 'weight_decay': 0.0014385512174681057, 'dropout': 0.17381539725156256, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 4.496480955203446, 'label_smoothing': 0.07176676888572879, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.479767217333263, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9083036598180358}
2025-09-24 19:44:14,034 - INFO - _models.training_function_executor - Model: 97,829 parameters, 210.2KB storage
2025-09-24 19:44:14,034 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1381964054269882, 0.7001477318156525, 0.4816588610293994, 0.44824936237527296, 0.43157205727899284, 0.41135784314919493, 0.405254811316442, 0.3907626375676889, 0.38689241948941966, 0.38447524478489065, 0.3787073580049816, 0.3714385328221746, 0.3690946313128922, 0.3659348296687085, 0.3624662159762466, 0.35862142917293804, 0.3569184224792946, 0.35457650826482423, 0.352531217647085, 0.350502397668424, 0.34733707067012204, 0.3443773153696585, 0.34366406737419347, 0.34165742291964424, 0.33947341262508157, 0.3388931236182546, 0.3390989252102538], 'val_losses': [0.8907142095197758, 0.7145447051774558, 0.4879093726331646, 0.43047672628519934, 0.41084043215785226, 0.40888796889935797, 0.506713761854345, 0.38629092469339066, 0.37971705851765775, 0.3786665946301206, 0.3718829591118024, 0.35758352943596544, 0.3657851155024396, 0.36077661359912094, 0.352905984033007, 0.35073671039521076, 0.34861316649242535, 0.35582988348958366, 0.34650106781662116, 0.345245036351742, 0.3414709198715364, 0.3405532154608544, 0.33704307228756225, 0.3372884745504336, 0.33607968327664833, 0.33604341879059474, 0.33610451803994856], 'val_acc': [0.7328655556947751, 0.8275905275028193, 0.9345946623230171, 0.9503821576243578, 0.9567723342939481, 0.9602806665831349, 0.915173537150733, 0.967547926325022, 0.970179175541912, 0.970930961032452, 0.9734369126675855, 0.9780729231925824, 0.9741886981581256, 0.9751910788121789, 0.9779476256108257, 0.9799523869189325, 0.9809547675729858, 0.9768199473750157, 0.9810800651547426, 0.9812053627364992, 0.982458338554066, 0.983210124044606, 0.9849642901891994, 0.9844630998621726, 0.985089587770956, 0.985841373261496, 0.9859666708432527], 'quantized_model_size_bytes': 197494, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004451073809947629, 'batch_size': 256, 'epochs': 27, 'weight_decay': 0.0014385512174681057, 'dropout': 0.17381539725156256, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 4.496480955203446, 'label_smoothing': 0.07176676888572879, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.479767217333263, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9083036598180358}, 'model_parameter_count': 97829, 'model_storage_size_kb': 210.17949218750002, 'model_size_validation': 'PASS'}
2025-09-24 19:44:14,034 - INFO - _models.training_function_executor - BO Objective: base=0.9860, size_penalty=0.0000, final=0.9860
2025-09-24 19:44:14,034 - INFO - _models.training_function_executor - Model: 97,829 parameters, 210.2KB (PASS 256KB limit)
2025-09-24 19:44:14,035 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 61.657s
2025-09-24 19:44:14,127 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9860
2025-09-24 19:44:14,127 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.093s
2025-09-24 19:44:14,127 - INFO - bo.run_bo - Recorded observation #17: hparams={'lr': 0.004451073809947629, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'weight_decay': 0.0014385512174681057, 'dropout': 0.17381539725156256, 'base_channels': np.int64(21), 'loss_type': np.str_('class_balanced'), 'gamma': 4.496480955203446, 'label_smoothing': 0.07176676888572879, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.479767217333263, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9083036598180358}, value=0.9860
2025-09-24 19:44:14,127 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 17: {'lr': 0.004451073809947629, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'weight_decay': 0.0014385512174681057, 'dropout': 0.17381539725156256, 'base_channels': np.int64(21), 'loss_type': np.str_('class_balanced'), 'gamma': 4.496480955203446, 'label_smoothing': 0.07176676888572879, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.479767217333263, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9083036598180358} -> 0.9860
2025-09-24 19:44:14,128 - INFO - bo.run_bo - üîçBO Trial 18: Using RF surrogate + Expected Improvement
2025-09-24 19:44:14,128 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:44:14,128 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:44:14,128 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:44:14,128 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0013557693387366895, 'batch_size': 128, 'epochs': 19, 'weight_decay': 0.005903629508625535, 'dropout': 0.4754992212793031, 'base_channels': 22, 'loss_type': np.str_('ce'), 'gamma': 3.3113789575911046, 'label_smoothing': 0.08230916853946703, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.7794076504355494, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9065755532752962}
2025-09-24 19:44:14,129 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0013557693387366895, 'batch_size': 128, 'epochs': 19, 'weight_decay': 0.005903629508625535, 'dropout': 0.4754992212793031, 'base_channels': 22, 'loss_type': np.str_('ce'), 'gamma': 3.3113789575911046, 'label_smoothing': 0.08230916853946703, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.7794076504355494, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9065755532752962}
2025-09-24 19:45:03,075 - INFO - _models.training_function_executor - Model: 107,237 parameters, 230.4KB storage
2025-09-24 19:45:03,075 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.0677022322210883, 1.9022621121565038, 1.753385588268594, 1.5104789641687446, 1.377252963186524, 1.3291607942922639, 1.2948829155684047, 1.2678607999092841, 1.2538580342394865, 1.232699971418163, 1.223944279789286, 1.2136741317357014, 1.2028996606382165, 1.1960749002909021, 1.1891769364644713, 1.18458931514338, 1.176340998628775, 1.1764963516909552, 1.16846201948081], 'val_losses': [1.9263111168453082, 1.7720688071081834, 1.536667295482699, 1.3192106534807087, 1.244796871646068, 1.2412322484346099, 1.206941966200335, 1.1851501627055614, 1.1815369913354283, 1.1842198294366069, 1.1632287116326152, 1.1780098333288265, 1.1437837088889249, 1.1437410772362084, 1.1482268280887975, 1.139522912044929, 1.1343225182902616, 1.1304602691993098, 1.1317485508920555], 'val_acc': [0.20774339055256233, 0.3180052624984338, 0.38366119533892995, 0.7685753664954267, 0.7262247838616714, 0.8457586768575367, 0.7541661445934094, 0.8250845758676858, 0.9170530008770831, 0.8349830848264629, 0.8854780102744017, 0.7599298333542163, 0.8798396190953515, 0.8774589650419747, 0.8896128304723718, 0.907906277408846, 0.907906277408846, 0.9069038967547927, 0.9077809798270894], 'quantized_model_size_bytes': 216394, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0013557693387366895, 'batch_size': 128, 'epochs': 19, 'weight_decay': 0.005903629508625535, 'dropout': 0.4754992212793031, 'base_channels': 22, 'loss_type': np.str_('ce'), 'gamma': 3.3113789575911046, 'label_smoothing': 0.08230916853946703, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.7794076504355494, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9065755532752962}, 'model_parameter_count': 107237, 'model_storage_size_kb': 230.39199218750002, 'model_size_validation': 'PASS'}
2025-09-24 19:45:03,075 - INFO - _models.training_function_executor - BO Objective: base=0.9078, size_penalty=0.0000, final=0.9078
2025-09-24 19:45:03,075 - INFO - _models.training_function_executor - Model: 107,237 parameters, 230.4KB (PASS 256KB limit)
2025-09-24 19:45:03,075 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 48.948s
2025-09-24 19:45:03,167 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9078
2025-09-24 19:45:03,167 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-09-24 19:45:03,167 - INFO - bo.run_bo - Recorded observation #18: hparams={'lr': 0.0013557693387366895, 'batch_size': np.int64(128), 'epochs': np.int64(19), 'weight_decay': 0.005903629508625535, 'dropout': 0.4754992212793031, 'base_channels': np.int64(22), 'loss_type': np.str_('ce'), 'gamma': 3.3113789575911046, 'label_smoothing': 0.08230916853946703, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.7794076504355494, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9065755532752962}, value=0.9078
2025-09-24 19:45:03,167 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 18: {'lr': 0.0013557693387366895, 'batch_size': np.int64(128), 'epochs': np.int64(19), 'weight_decay': 0.005903629508625535, 'dropout': 0.4754992212793031, 'base_channels': np.int64(22), 'loss_type': np.str_('ce'), 'gamma': 3.3113789575911046, 'label_smoothing': 0.08230916853946703, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.7794076504355494, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9065755532752962} -> 0.9078
2025-09-24 19:45:03,167 - INFO - bo.run_bo - üîçBO Trial 19: Using RF surrogate + Expected Improvement
2025-09-24 19:45:03,167 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:45:03,167 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:45:03,167 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:45:03,167 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.007204395298747223, 'batch_size': 256, 'epochs': 40, 'weight_decay': 1.4004600536073278e-05, 'dropout': 0.4907337975295342, 'base_channels': 22, 'loss_type': np.str_('class_balanced'), 'gamma': 2.669335234131771, 'label_smoothing': 0.06454014121603564, 'scheduler': np.str_('onecycle'), 'patience': 7, 'use_amp': True, 'grad_clip': 0.3664770531450721, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 1, 'beta_cb': 0.9471677066419162}
2025-09-24 19:45:03,169 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.007204395298747223, 'batch_size': 256, 'epochs': 40, 'weight_decay': 1.4004600536073278e-05, 'dropout': 0.4907337975295342, 'base_channels': 22, 'loss_type': np.str_('class_balanced'), 'gamma': 2.669335234131771, 'label_smoothing': 0.06454014121603564, 'scheduler': np.str_('onecycle'), 'patience': 7, 'use_amp': True, 'grad_clip': 0.3664770531450721, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 1, 'beta_cb': 0.9471677066419162}
2025-09-24 19:46:13,918 - INFO - _models.training_function_executor - Model: 107,237 parameters, 460.8KB storage
2025-09-24 19:46:13,919 - WARNING - _models.training_function_executor - Model storage 460.8KB exceeds 256KB limit!
2025-09-24 19:46:13,919 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0930552015316366, 0.8220856158714622, 0.6159308950507738, 0.5084885744645147, 0.4835010262267637, 0.46480018633318565, 0.45221304218896713, 0.43714263388635044, 0.4254665936036198, 0.4155387504318209, 0.40806710473319974, 0.40115237154802447, 0.39329056293673187, 0.3865852725743702, 0.38481816612841047, 0.3796481832250879, 0.3730565592806706, 0.37092609425757356, 0.3727721423914277, 0.3658887916996235, 0.3634610571620929, 0.36049252497359324, 0.35806889740390013, 0.35443516539828285, 0.3533010703073021, 0.3479268758218472, 0.34873757456973603, 0.346973095812363, 0.3456127041190488, 0.34297463555975755, 0.3424063459763651, 0.33907489454828776, 0.340706637707942, 0.3358154654898702, 0.33550554305932107, 0.3338180157701589, 0.33398470405972447, 0.3348853266881835, 0.3339348900039195, 0.3308257922337392], 'val_losses': [0.8989179118386696, 0.677459276009287, 0.5130974616850248, 0.4633367283722763, 0.44410585381007556, 0.4256648202204432, 0.4103892847135721, 0.3943033403672755, 0.38365065989496117, 0.41541957856718276, 0.4084315085348338, 0.37939416819266425, 0.3701928234043583, 0.3530286746079754, 0.3591887146858117, 0.35343975915360343, 0.3504936874562555, 0.3443146563460543, 0.34575778295963633, 0.3457677466515953, 0.3376034946313793, 0.3388624826315247, 0.34673149712535917, 0.3342595892044524, 0.3333999322531979, 0.328546262216096, 0.33917789499706796, 0.3259392408907316, 0.32996786460737854, 0.32403909469245296, 0.32464055839658545, 0.3233707772208222, 0.33138406888358113, 0.3240137962661072, 0.3228789170707561, 0.32195457418436035, 0.321123785332411, 0.32134787094367506, 0.3212625993035996, 0.3210820974211483], 'val_acc': [0.7227164515724847, 0.836862548552813, 0.9229419872196467, 0.9411101365743642, 0.9404836486655808, 0.9431148978824708, 0.9527628116777346, 0.9538904899135446, 0.9558952512216514, 0.9487532890615211, 0.9553940608946248, 0.9584012028567849, 0.9646660819446184, 0.9708056634506954, 0.9688009021425886, 0.9716827465229921, 0.969427390051372, 0.9758175667209623, 0.9716827465229921, 0.9728104247588022, 0.9773211377020423, 0.9758175667209623, 0.9731863175040721, 0.9773211377020423, 0.977822328029069, 0.978950006264879, 0.9748151860669089, 0.9799523869189325, 0.977446435283799, 0.9808294699912292, 0.9798270893371758, 0.9809547675729858, 0.9771958401202857, 0.9800776845006891, 0.9808294699912292, 0.9818318506452826, 0.9809547675729858, 0.9813306603182559, 0.9812053627364992, 0.9813306603182559], 'quantized_model_size_bytes': 432716, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.007204395298747223, 'batch_size': 256, 'epochs': 40, 'weight_decay': 1.4004600536073278e-05, 'dropout': 0.4907337975295342, 'base_channels': 22, 'loss_type': np.str_('class_balanced'), 'gamma': 2.669335234131771, 'label_smoothing': 0.06454014121603564, 'scheduler': np.str_('onecycle'), 'patience': 7, 'use_amp': True, 'grad_clip': 0.3664770531450721, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 1, 'beta_cb': 0.9471677066419162}, 'model_parameter_count': 107237, 'model_storage_size_kb': 460.78398437500005, 'model_size_validation': 'FAIL'}
2025-09-24 19:46:13,919 - INFO - _models.training_function_executor - BO Objective: base=0.9813, size_penalty=0.4000, final=0.5814
2025-09-24 19:46:13,919 - INFO - _models.training_function_executor - Model: 107,237 parameters, 460.8KB (FAIL 256KB limit)
2025-09-24 19:46:13,919 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 70.752s
2025-09-24 19:46:14,011 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5814
2025-09-24 19:46:14,011 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-09-24 19:46:14,011 - INFO - bo.run_bo - Recorded observation #19: hparams={'lr': 0.007204395298747223, 'batch_size': np.int64(256), 'epochs': np.int64(40), 'weight_decay': 1.4004600536073278e-05, 'dropout': 0.4907337975295342, 'base_channels': np.int64(22), 'loss_type': np.str_('class_balanced'), 'gamma': 2.669335234131771, 'label_smoothing': 0.06454014121603564, 'scheduler': np.str_('onecycle'), 'patience': np.int64(7), 'use_amp': np.True_, 'grad_clip': 0.3664770531450721, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9471677066419162}, value=0.5814
2025-09-24 19:46:14,011 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 19: {'lr': 0.007204395298747223, 'batch_size': np.int64(256), 'epochs': np.int64(40), 'weight_decay': 1.4004600536073278e-05, 'dropout': 0.4907337975295342, 'base_channels': np.int64(22), 'loss_type': np.str_('class_balanced'), 'gamma': 2.669335234131771, 'label_smoothing': 0.06454014121603564, 'scheduler': np.str_('onecycle'), 'patience': np.int64(7), 'use_amp': np.True_, 'grad_clip': 0.3664770531450721, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9471677066419162} -> 0.5814
2025-09-24 19:46:14,012 - INFO - bo.run_bo - üîçBO Trial 20: Using RF surrogate + Expected Improvement
2025-09-24 19:46:14,012 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:46:14,012 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:46:14,012 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:46:14,012 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.009825468492135508, 'batch_size': 256, 'epochs': 46, 'weight_decay': 6.906541330452645e-06, 'dropout': 0.4919221605503273, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 4.390797557117163, 'label_smoothing': 0.07370144735670744, 'scheduler': np.str_('plateau'), 'patience': 2, 'use_amp': False, 'grad_clip': 1.9501375396086362, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9656053909560631}
2025-09-24 19:46:14,013 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.009825468492135508, 'batch_size': 256, 'epochs': 46, 'weight_decay': 6.906541330452645e-06, 'dropout': 0.4919221605503273, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 4.390797557117163, 'label_smoothing': 0.07370144735670744, 'scheduler': np.str_('plateau'), 'patience': 2, 'use_amp': False, 'grad_clip': 1.9501375396086362, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9656053909560631}
2025-09-24 19:47:58,937 - INFO - _models.training_function_executor - Model: 82,404 parameters, 88.5KB storage
2025-09-24 19:47:58,937 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9287809450270945, 0.687714843947842, 0.5790440544233515, 0.5327980992370047, 0.4973234526785723, 0.48519507598612566, 0.4731614547960629, 0.47167064703650163, 0.4603023970892259, 0.45507320280799224, 0.4550854166031362, 0.4496316806163883, 0.44729903636561097, 0.4385775009858273, 0.43756956939663955, 0.4334625109600266, 0.43131162411466867, 0.42898368664512737, 0.4291779959171565, 0.42710860836152253, 0.42552287874972244, 0.4256166575849196, 0.4100756755531818, 0.41018677055621006, 0.40821908827048026, 0.4086204858194134, 0.40771614315478205, 0.4059345837214184, 0.402260303138824, 0.4028314226162418, 0.40449091772034895, 0.4021045118780381, 0.40242785543807374, 0.40002994485932536, 0.40192473842338655, 0.3985642952124427, 0.3949437833781216, 0.3931208177231698, 0.3899824548033893, 0.39010632849948035, 0.3883797918962655, 0.38813539895931287, 0.38613953264446793, 0.38467396583222685, 0.3855045303845657, 0.38616125945238616], 'val_losses': [0.837443401357641, 0.561917785967282, 0.5000342262775553, 0.45861335126562924, 0.433291215217587, 0.4468772429494448, 0.42367783537963744, 0.42797299788987303, 0.44264586441066683, 0.4161122458170207, 0.4132238364640879, 0.426529062913035, 0.40960561289268693, 0.40766955495941176, 0.40138020753382503, 0.3990395228160143, 0.408458115500087, 0.39926228668438496, 0.3889267752155925, 0.3907254039008969, 0.39666612301982895, 0.4042539502382129, 0.37548804600919017, 0.3842288173166616, 0.37940544972158646, 0.37768346370349415, 0.3767916281716804, 0.3768196078548305, 0.37698524509242803, 0.3745298337300101, 0.3736947980751267, 0.3808503821296152, 0.37178861833861443, 0.3748772413251753, 0.37428375370982536, 0.37347536880320853, 0.3688270225439465, 0.3733350335767493, 0.3670472242698949, 0.3674591204797754, 0.3668856322317251, 0.36582322455471195, 0.3651750927064939, 0.3649589708202122, 0.3640372415052561, 0.36407122629475375], 'val_acc': [0.7515348953765192, 0.9062774088460093, 0.9274527001628868, 0.9454955519358477, 0.9522616213507079, 0.9516351334419245, 0.9535145971682747, 0.9548928705675981, 0.9443678737000376, 0.9567723342939481, 0.9573988222027315, 0.952637514095978, 0.9604059641648917, 0.9607818569101616, 0.962536023054755, 0.961408344818945, 0.962536023054755, 0.9640395940358351, 0.969051497306102, 0.9657937601804285, 0.9654178674351584, 0.960656559328405, 0.9731863175040721, 0.9711815561959655, 0.9735622102493422, 0.9740634005763689, 0.9744392933216389, 0.9753163763939356, 0.9723092344317754, 0.9741886981581256, 0.9764440546297456, 0.9706803658689387, 0.9765693522115023, 0.9740634005763689, 0.9745645909033955, 0.9741886981581256, 0.9783235183560958, 0.9750657812304222, 0.975566971557449, 0.975566971557449, 0.976318757047989, 0.9780729231925824, 0.9776970304473124, 0.9788247086831224, 0.9771958401202857, 0.977446435283799], 'quantized_model_size_bytes': 333264, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.009825468492135508, 'batch_size': 256, 'epochs': 46, 'weight_decay': 6.906541330452645e-06, 'dropout': 0.4919221605503273, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 4.390797557117163, 'label_smoothing': 0.07370144735670744, 'scheduler': np.str_('plateau'), 'patience': 2, 'use_amp': False, 'grad_clip': 1.9501375396086362, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9656053909560631}, 'model_parameter_count': 82404, 'model_storage_size_kb': 88.51992187500001, 'model_size_validation': 'PASS'}
2025-09-24 19:47:58,937 - INFO - _models.training_function_executor - BO Objective: base=0.9774, size_penalty=0.0000, final=0.9774
2025-09-24 19:47:58,937 - INFO - _models.training_function_executor - Model: 82,404 parameters, 88.5KB (PASS 256KB limit)
2025-09-24 19:47:58,937 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 104.926s
2025-09-24 19:47:59,032 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9774
2025-09-24 19:47:59,033 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-09-24 19:47:59,033 - INFO - bo.run_bo - Recorded observation #20: hparams={'lr': 0.009825468492135508, 'batch_size': np.int64(256), 'epochs': np.int64(46), 'weight_decay': 6.906541330452645e-06, 'dropout': 0.4919221605503273, 'base_channels': np.int64(21), 'loss_type': np.str_('class_balanced'), 'gamma': 4.390797557117163, 'label_smoothing': 0.07370144735670744, 'scheduler': np.str_('plateau'), 'patience': np.int64(2), 'use_amp': np.False_, 'grad_clip': 1.9501375396086362, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9656053909560631}, value=0.9774
2025-09-24 19:47:59,033 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 20: {'lr': 0.009825468492135508, 'batch_size': np.int64(256), 'epochs': np.int64(46), 'weight_decay': 6.906541330452645e-06, 'dropout': 0.4919221605503273, 'base_channels': np.int64(21), 'loss_type': np.str_('class_balanced'), 'gamma': 4.390797557117163, 'label_smoothing': 0.07370144735670744, 'scheduler': np.str_('plateau'), 'patience': np.int64(2), 'use_amp': np.False_, 'grad_clip': 1.9501375396086362, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9656053909560631} -> 0.9774
2025-09-24 19:47:59,033 - INFO - bo.run_bo - üîçBO Trial 21: Using RF surrogate + Expected Improvement
2025-09-24 19:47:59,033 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:47:59,033 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:47:59,033 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:47:59,033 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002109518701872943, 'batch_size': 256, 'epochs': 27, 'weight_decay': 0.0004172071787774661, 'dropout': 0.08406797513942195, 'base_channels': 24, 'loss_type': np.str_('class_balanced'), 'gamma': 3.363078445748781, 'label_smoothing': 0.08514382815558319, 'scheduler': np.str_('none'), 'patience': 4, 'use_amp': False, 'grad_clip': 1.4276177271139756, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9782149617543052}
2025-09-24 19:47:59,035 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002109518701872943, 'batch_size': 256, 'epochs': 27, 'weight_decay': 0.0004172071787774661, 'dropout': 0.08406797513942195, 'base_channels': 24, 'loss_type': np.str_('class_balanced'), 'gamma': 3.363078445748781, 'label_smoothing': 0.08514382815558319, 'scheduler': np.str_('none'), 'patience': 4, 'use_amp': False, 'grad_clip': 1.4276177271139756, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9782149617543052}
2025-09-24 19:49:06,797 - INFO - _models.training_function_executor - Model: 127,349 parameters, 547.2KB storage
2025-09-24 19:49:06,797 - WARNING - _models.training_function_executor - Model storage 547.2KB exceeds 256KB limit!
2025-09-24 19:49:06,798 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7095269467008893, 0.48356352811416387, 0.457390820717179, 0.44083093514856353, 0.4320234472299471, 0.42354314965726486, 0.416719754109372, 0.4098636233873454, 0.40869810790476097, 0.4056969857111695, 0.4026852203075313, 0.40036653419285273, 0.39746603829588906, 0.39630779809298733, 0.39693190308698334, 0.39446707424209915, 0.39483808590027974, 0.39382498616660344, 0.39236849980355354, 0.3899705185891094, 0.39097431344361294, 0.3895857926075707, 0.3870854269256531, 0.38694341916942065, 0.38665129150177113, 0.3855556420975116, 0.38506485584389166], 'val_losses': [0.49550159482964296, 0.45942155649945515, 0.45045532736320004, 0.42757430576677685, 0.43731356798564414, 0.41951887116308517, 0.4112921910482516, 0.411247826215543, 0.4032333774601602, 0.40804172452418314, 0.40111095546301917, 0.39590693859957826, 0.4092008598562381, 0.417605286970307, 0.3898539241730547, 0.39596879368004023, 0.40987665797217926, 0.38767931585980453, 0.3943982436578983, 0.4300472296400038, 0.3869355432805882, 0.390371037035893, 0.3947191315775093, 0.3894821121166589, 0.38355498197547416, 0.38848219307244775, 0.39119993746392695], 'val_acc': [0.9401077559203107, 0.9577747149480015, 0.9575241197844881, 0.9637889988723217, 0.9600300714196216, 0.9704297707054254, 0.9725598295952889, 0.9704297707054254, 0.9760681618844756, 0.9739381029946123, 0.9765693522115023, 0.9780729231925824, 0.9720586392682621, 0.970930961032452, 0.9805788748277158, 0.9776970304473124, 0.9715574489412354, 0.9818318506452826, 0.9779476256108257, 0.9629119158000251, 0.9820824458087959, 0.9800776845006891, 0.9771958401202857, 0.9798270893371758, 0.9827089337175793, 0.9802029820824458, 0.9773211377020423], 'quantized_model_size_bytes': 256786, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002109518701872943, 'batch_size': 256, 'epochs': 27, 'weight_decay': 0.0004172071787774661, 'dropout': 0.08406797513942195, 'base_channels': 24, 'loss_type': np.str_('class_balanced'), 'gamma': 3.363078445748781, 'label_smoothing': 0.08514382815558319, 'scheduler': np.str_('none'), 'patience': 4, 'use_amp': False, 'grad_clip': 1.4276177271139756, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9782149617543052}, 'model_parameter_count': 127349, 'model_storage_size_kb': 547.2027343750001, 'model_size_validation': 'FAIL'}
2025-09-24 19:49:06,798 - INFO - _models.training_function_executor - BO Objective: base=0.9773, size_penalty=0.5688, final=0.4086
2025-09-24 19:49:06,798 - INFO - _models.training_function_executor - Model: 127,349 parameters, 547.2KB (FAIL 256KB limit)
2025-09-24 19:49:06,798 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 67.764s
2025-09-24 19:49:07,018 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.4086
2025-09-24 19:49:07,019 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.221s
2025-09-24 19:49:07,019 - INFO - bo.run_bo - Recorded observation #21: hparams={'lr': 0.002109518701872943, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'weight_decay': 0.0004172071787774661, 'dropout': 0.08406797513942195, 'base_channels': np.int64(24), 'loss_type': np.str_('class_balanced'), 'gamma': 3.363078445748781, 'label_smoothing': 0.08514382815558319, 'scheduler': np.str_('none'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 1.4276177271139756, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9782149617543052}, value=0.4086
2025-09-24 19:49:07,019 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 21: {'lr': 0.002109518701872943, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'weight_decay': 0.0004172071787774661, 'dropout': 0.08406797513942195, 'base_channels': np.int64(24), 'loss_type': np.str_('class_balanced'), 'gamma': 3.363078445748781, 'label_smoothing': 0.08514382815558319, 'scheduler': np.str_('none'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 1.4276177271139756, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9782149617543052} -> 0.4086
2025-09-24 19:49:07,019 - INFO - bo.run_bo - üîçBO Trial 22: Using RF surrogate + Expected Improvement
2025-09-24 19:49:07,019 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:49:07,019 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:49:07,019 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:49:07,019 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00998860881291476, 'batch_size': 256, 'epochs': 42, 'weight_decay': 1.0149194198613957e-05, 'dropout': 0.2650958932754248, 'base_channels': 20, 'loss_type': np.str_('class_balanced'), 'gamma': 2.644613344863241, 'label_smoothing': 0.07936110387000894, 'scheduler': np.str_('plateau'), 'patience': 2, 'use_amp': True, 'grad_clip': 1.4815271658243316, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 1, 'beta_cb': 0.9298012544212022}
2025-09-24 19:49:07,020 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00998860881291476, 'batch_size': 256, 'epochs': 42, 'weight_decay': 1.0149194198613957e-05, 'dropout': 0.2650958932754248, 'base_channels': 20, 'loss_type': np.str_('class_balanced'), 'gamma': 2.644613344863241, 'label_smoothing': 0.07936110387000894, 'scheduler': np.str_('plateau'), 'patience': 2, 'use_amp': True, 'grad_clip': 1.4815271658243316, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 1, 'beta_cb': 0.9298012544212022}
2025-09-24 19:50:15,513 - INFO - _models.training_function_executor - Model: 88,853 parameters, 381.8KB storage
2025-09-24 19:50:15,513 - WARNING - _models.training_function_executor - Model storage 381.8KB exceeds 256KB limit!
2025-09-24 19:50:15,513 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9054026001322685, 0.6360287157488367, 0.511978032465877, 0.48264983802855516, 0.46437751934671223, 0.4567785119357251, 0.44583193702894625, 0.43889777778391664, 0.43739224381305386, 0.4302020460592193, 0.4223184676118144, 0.4217132068392112, 0.41574908598650184, 0.41485461938188134, 0.41278859454262873, 0.41222112855752024, 0.4101387831335979, 0.406501513448637, 0.40209304297197784, 0.40250484018988564, 0.4003418311651752, 0.4005224869349313, 0.3983980303318829, 0.39904557230986176, 0.3972358545279983, 0.39723413301000426, 0.3945463821448296, 0.38742606056680196, 0.3857262327698087, 0.38467765535189646, 0.3830424321794541, 0.3823566573862182, 0.3799617997927388, 0.3793422644909763, 0.3770781328544967, 0.3767579987178832, 0.37620156255240533, 0.3748747578682362, 0.3736322029304322, 0.3734216401319928, 0.3733917530620807, 0.3720988168795747], 'val_losses': [0.8058297048608636, 0.5911374836245241, 0.4689513648598882, 0.4555142602067949, 0.4366250914778122, 0.4459280723722458, 0.4184079690544755, 0.40958052215652646, 0.4098488861211125, 0.4126405592117673, 0.4098690125469038, 0.40653312515083256, 0.3969259216602905, 0.3896299361540342, 0.3937438833184775, 0.39273076608745305, 0.3936703625724304, 0.38905236278839717, 0.3839629446335446, 0.3851842642271075, 0.39209079438471106, 0.3842320752956413, 0.4021927767074163, 0.3782662561686322, 0.3930913445689956, 0.38429415526209043, 0.3823857923010181, 0.37499834694012174, 0.3704769007395778, 0.3723015999109908, 0.368487940383068, 0.37460454826426076, 0.36626500744163026, 0.3674041751803439, 0.36669667859585897, 0.367268626167278, 0.3655358489788112, 0.36465510480640795, 0.363126754544161, 0.3622748121598687, 0.3624570847468752, 0.36510461995273347], 'val_acc': [0.807417616839995, 0.8924946748527753, 0.9424884099736875, 0.9506327527878712, 0.9550181681493547, 0.9502568600426011, 0.961784237564215, 0.9646660819446184, 0.9650419746898885, 0.9644154867811051, 0.9667961408344818, 0.9670467359979953, 0.9721839368500188, 0.9736875078310988, 0.9736875078310988, 0.9739381029946123, 0.9719333416865055, 0.9734369126675855, 0.9764440546297456, 0.976318757047989, 0.9731863175040721, 0.977070542538529, 0.9674226287432652, 0.9794511965919058, 0.9706803658689387, 0.9750657812304222, 0.976318757047989, 0.9794511965919058, 0.9815812554817692, 0.9812053627364992, 0.9815812554817692, 0.9798270893371758, 0.9830848264628492, 0.9838366119533893, 0.9837113143716326, 0.9823330409723092, 0.9837113143716326, 0.9838366119533893, 0.9837113143716326, 0.9845883974439293, 0.9842125046986593, 0.9840872071169027], 'quantized_model_size_bytes': 179458, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00998860881291476, 'batch_size': 256, 'epochs': 42, 'weight_decay': 1.0149194198613957e-05, 'dropout': 0.2650958932754248, 'base_channels': 20, 'loss_type': np.str_('class_balanced'), 'gamma': 2.644613344863241, 'label_smoothing': 0.07936110387000894, 'scheduler': np.str_('plateau'), 'patience': 2, 'use_amp': True, 'grad_clip': 1.4815271658243316, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 1, 'beta_cb': 0.9298012544212022}, 'model_parameter_count': 88853, 'model_storage_size_kb': 381.790234375, 'model_size_validation': 'FAIL'}
2025-09-24 19:50:15,514 - INFO - _models.training_function_executor - BO Objective: base=0.9841, size_penalty=0.2457, final=0.7384
2025-09-24 19:50:15,514 - INFO - _models.training_function_executor - Model: 88,853 parameters, 381.8KB (FAIL 256KB limit)
2025-09-24 19:50:15,514 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 68.495s
2025-09-24 19:50:15,609 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7384
2025-09-24 19:50:15,609 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-09-24 19:50:15,609 - INFO - bo.run_bo - Recorded observation #22: hparams={'lr': 0.00998860881291476, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'weight_decay': 1.0149194198613957e-05, 'dropout': 0.2650958932754248, 'base_channels': np.int64(20), 'loss_type': np.str_('class_balanced'), 'gamma': 2.644613344863241, 'label_smoothing': 0.07936110387000894, 'scheduler': np.str_('plateau'), 'patience': np.int64(2), 'use_amp': np.True_, 'grad_clip': 1.4815271658243316, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9298012544212022}, value=0.7384
2025-09-24 19:50:15,609 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 22: {'lr': 0.00998860881291476, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'weight_decay': 1.0149194198613957e-05, 'dropout': 0.2650958932754248, 'base_channels': np.int64(20), 'loss_type': np.str_('class_balanced'), 'gamma': 2.644613344863241, 'label_smoothing': 0.07936110387000894, 'scheduler': np.str_('plateau'), 'patience': np.int64(2), 'use_amp': np.True_, 'grad_clip': 1.4815271658243316, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9298012544212022} -> 0.7384
2025-09-24 19:50:15,609 - INFO - bo.run_bo - üîçBO Trial 23: Using RF surrogate + Expected Improvement
2025-09-24 19:50:15,610 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:50:15,610 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:50:15,610 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:50:15,610 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.007735545730660902, 'batch_size': 256, 'epochs': 23, 'weight_decay': 0.0004765459858622387, 'dropout': 0.07132631814660441, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 3.3977123923619263, 'label_smoothing': 0.07571702224229081, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': False, 'grad_clip': 1.4949306824461661, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9880040389879325}
2025-09-24 19:50:15,611 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.007735545730660902, 'batch_size': 256, 'epochs': 23, 'weight_decay': 0.0004765459858622387, 'dropout': 0.07132631814660441, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 3.3977123923619263, 'label_smoothing': 0.07571702224229081, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': False, 'grad_clip': 1.4949306824461661, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9880040389879325}
2025-09-24 19:50:52,402 - INFO - _models.training_function_executor - Model: 50,453 parameters, 216.8KB storage
2025-09-24 19:50:52,402 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8372734984957073, 0.6324247086428649, 0.4941512090171059, 0.45209072869668626, 0.4358838595002045, 0.4210224467512398, 0.4127341508482593, 0.4027617469533129, 0.39867206603308253, 0.39400436781449255, 0.3919053674789799, 0.3891130780190098, 0.38609560622822553, 0.38508428168342373, 0.3813875464601325, 0.37852282660484643, 0.3765205552693138, 0.3744412229943387, 0.3757089395046279, 0.3724513861151136, 0.3720536535759302, 0.36840125541039787, 0.3682171784072234], 'val_losses': [0.7463438381244778, 0.5894039580209888, 0.463388880091403, 0.46884111513575877, 0.42441184511983504, 0.4043887613740067, 0.4017671721573985, 0.4098020632969618, 0.3883495269308949, 0.3819548931878815, 0.38559847258338503, 0.38126336595809274, 0.3906725285092387, 0.37883054400174454, 0.37522089157975597, 0.36891390204937474, 0.37885662590691116, 0.36365942167258924, 0.37497010457756197, 0.3684057417616123, 0.3681485232270581, 0.36950184926698715, 0.36174467963662127], 'val_acc': [0.8208244580879589, 0.8913669966169653, 0.9428643027189575, 0.9426137075554442, 0.9542663826588147, 0.9646660819446184, 0.9657937601804285, 0.9640395940358351, 0.9699285803783987, 0.9718080441047487, 0.9708056634506954, 0.9749404836486656, 0.969427390051372, 0.9726851271770455, 0.977070542538529, 0.9786994111013657, 0.976318757047989, 0.9800776845006891, 0.9753163763939356, 0.9768199473750157, 0.9784488159378524, 0.9773211377020423, 0.9794511965919058], 'quantized_model_size_bytes': 204404, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.007735545730660902, 'batch_size': 256, 'epochs': 23, 'weight_decay': 0.0004765459858622387, 'dropout': 0.07132631814660441, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 3.3977123923619263, 'label_smoothing': 0.07571702224229081, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': False, 'grad_clip': 1.4949306824461661, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9880040389879325}, 'model_parameter_count': 50453, 'model_storage_size_kb': 216.790234375, 'model_size_validation': 'PASS'}
2025-09-24 19:50:52,402 - INFO - _models.training_function_executor - BO Objective: base=0.9795, size_penalty=0.0000, final=0.9795
2025-09-24 19:50:52,402 - INFO - _models.training_function_executor - Model: 50,453 parameters, 216.8KB (PASS 256KB limit)
2025-09-24 19:50:52,402 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 36.793s
2025-09-24 19:50:52,498 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9795
2025-09-24 19:50:52,499 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.096s
2025-09-24 19:50:52,499 - INFO - bo.run_bo - Recorded observation #23: hparams={'lr': 0.007735545730660902, 'batch_size': np.int64(256), 'epochs': np.int64(23), 'weight_decay': 0.0004765459858622387, 'dropout': 0.07132631814660441, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 3.3977123923619263, 'label_smoothing': 0.07571702224229081, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 1.4949306824461661, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9880040389879325}, value=0.9795
2025-09-24 19:50:52,499 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 23: {'lr': 0.007735545730660902, 'batch_size': np.int64(256), 'epochs': np.int64(23), 'weight_decay': 0.0004765459858622387, 'dropout': 0.07132631814660441, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 3.3977123923619263, 'label_smoothing': 0.07571702224229081, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 1.4949306824461661, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9880040389879325} -> 0.9795
2025-09-24 19:50:52,499 - INFO - bo.run_bo - üîçBO Trial 24: Using RF surrogate + Expected Improvement
2025-09-24 19:50:52,499 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:50:52,499 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:50:52,499 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:50:52,499 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0009861987631370364, 'batch_size': 128, 'epochs': 21, 'weight_decay': 0.000546155801026514, 'dropout': 0.17201402539546504, 'base_channels': 18, 'loss_type': np.str_('class_balanced'), 'gamma': 4.419976470337066, 'label_smoothing': 0.08230263753831589, 'scheduler': np.str_('onecycle'), 'patience': 4, 'use_amp': False, 'grad_clip': 0.30287565362971264, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 9, 'beta_cb': 0.9829569055509204}
2025-09-24 19:50:52,501 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0009861987631370364, 'batch_size': 128, 'epochs': 21, 'weight_decay': 0.000546155801026514, 'dropout': 0.17201402539546504, 'base_channels': 18, 'loss_type': np.str_('class_balanced'), 'gamma': 4.419976470337066, 'label_smoothing': 0.08230263753831589, 'scheduler': np.str_('onecycle'), 'patience': 4, 'use_amp': False, 'grad_clip': 0.30287565362971264, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 9, 'beta_cb': 0.9829569055509204}
2025-09-24 19:51:38,795 - INFO - _models.training_function_executor - Model: 72,197 parameters, 310.2KB storage
2025-09-24 19:51:38,795 - WARNING - _models.training_function_executor - Model storage 310.2KB exceeds 256KB limit!
2025-09-24 19:51:38,795 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.27594353270046, 0.8566410011622002, 0.6089771322146792, 0.5121116676880028, 0.48242228410672905, 0.45983690439794006, 0.44640978777860074, 0.43387979888730793, 0.42237117230431, 0.4172565838812109, 0.41142489778081737, 0.4060319716315802, 0.40121640162342037, 0.39761395155552637, 0.39460890629691625, 0.39200331279983747, 0.38865393515954816, 0.3864981982917321, 0.384529209809506, 0.3838699922276944, 0.3821556843800663], 'val_losses': [0.9288847396105188, 0.6868876958756709, 0.5536220120302254, 0.4805876415537019, 0.452940930035341, 0.4365232638765466, 0.4685711904570447, 0.41211409930850673, 0.40160825671206857, 0.41616394236457926, 0.39125661186938326, 0.40235458829919435, 0.38922653342679453, 0.3900749500481023, 0.3796323395106984, 0.37926408068576023, 0.37959119663727253, 0.3766558246271396, 0.3749886610583843, 0.3748873793799512, 0.3746472047379315], 'val_acc': [0.7200852023555946, 0.8649292068663075, 0.9293321638892369, 0.945746147099361, 0.9560205488034081, 0.9570229294574615, 0.9456208495176043, 0.9685503069790753, 0.9714321513594788, 0.962160130309485, 0.977446435283799, 0.9691767948878587, 0.9764440546297456, 0.9743139957398822, 0.9812053627364992, 0.9800776845006891, 0.978950006264879, 0.9810800651547426, 0.9823330409723092, 0.9823330409723092, 0.982458338554066], 'quantized_model_size_bytes': 145978, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0009861987631370364, 'batch_size': 128, 'epochs': 21, 'weight_decay': 0.000546155801026514, 'dropout': 0.17201402539546504, 'base_channels': 18, 'loss_type': np.str_('class_balanced'), 'gamma': 4.419976470337066, 'label_smoothing': 0.08230263753831589, 'scheduler': np.str_('onecycle'), 'patience': 4, 'use_amp': False, 'grad_clip': 0.30287565362971264, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 9, 'beta_cb': 0.9829569055509204}, 'model_parameter_count': 72197, 'model_storage_size_kb': 310.22148437500005, 'model_size_validation': 'FAIL'}
2025-09-24 19:51:38,796 - INFO - _models.training_function_executor - BO Objective: base=0.9825, size_penalty=0.1059, final=0.8766
2025-09-24 19:51:38,796 - INFO - _models.training_function_executor - Model: 72,197 parameters, 310.2KB (FAIL 256KB limit)
2025-09-24 19:51:38,796 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 46.296s
2025-09-24 19:51:38,892 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8766
2025-09-24 19:51:38,892 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.097s
2025-09-24 19:51:38,892 - INFO - bo.run_bo - Recorded observation #24: hparams={'lr': 0.0009861987631370364, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'weight_decay': 0.000546155801026514, 'dropout': 0.17201402539546504, 'base_channels': np.int64(18), 'loss_type': np.str_('class_balanced'), 'gamma': 4.419976470337066, 'label_smoothing': 0.08230263753831589, 'scheduler': np.str_('onecycle'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 0.30287565362971264, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(9), 'beta_cb': 0.9829569055509204}, value=0.8766
2025-09-24 19:51:38,892 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 24: {'lr': 0.0009861987631370364, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'weight_decay': 0.000546155801026514, 'dropout': 0.17201402539546504, 'base_channels': np.int64(18), 'loss_type': np.str_('class_balanced'), 'gamma': 4.419976470337066, 'label_smoothing': 0.08230263753831589, 'scheduler': np.str_('onecycle'), 'patience': np.int64(4), 'use_amp': np.False_, 'grad_clip': 0.30287565362971264, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(9), 'beta_cb': 0.9829569055509204} -> 0.8766
2025-09-24 19:51:38,893 - INFO - bo.run_bo - üîçBO Trial 25: Using RF surrogate + Expected Improvement
2025-09-24 19:51:38,893 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:51:38,893 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:51:38,893 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:51:38,893 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001940366526878127, 'batch_size': 256, 'epochs': 14, 'weight_decay': 0.000310957113680119, 'dropout': 0.0741804597058466, 'base_channels': 13, 'loss_type': np.str_('class_balanced'), 'gamma': 3.608276788742626, 'label_smoothing': 0.06746712918839036, 'scheduler': np.str_('none'), 'patience': 10, 'use_amp': False, 'grad_clip': 0.5471450192900692, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.903182944961301}
2025-09-24 19:51:38,894 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001940366526878127, 'batch_size': 256, 'epochs': 14, 'weight_decay': 0.000310957113680119, 'dropout': 0.0741804597058466, 'base_channels': 13, 'loss_type': np.str_('class_balanced'), 'gamma': 3.608276788742626, 'label_smoothing': 0.06746712918839036, 'scheduler': np.str_('none'), 'patience': 10, 'use_amp': False, 'grad_clip': 0.5471450192900692, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.903182944961301}
2025-09-24 19:51:59,690 - INFO - _models.training_function_executor - Model: 38,117 parameters, 163.8KB storage
2025-09-24 19:51:59,691 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8042501800453791, 0.47116047969771274, 0.42851432428896824, 0.4015728715397946, 0.3894875974888618, 0.3821148522511179, 0.37536308614957403, 0.36844936076611073, 0.3667795335404785, 0.35889332351243775, 0.3575204162282035, 0.3538501438026391, 0.3516050365243539, 0.34839926997592746], 'val_losses': [0.6126737959469161, 0.43972554109087864, 0.4088762732634134, 0.4063116661058812, 0.41307858500828976, 0.3759284508196935, 0.3672250500374787, 0.3560913266200288, 0.36366839306782606, 0.3572278428743872, 0.3454244639278728, 0.35057740279597877, 0.37037112814877327, 0.3440630723175108], 'val_acc': [0.8819696779852149, 0.9442425761182809, 0.9557699536398947, 0.9550181681493547, 0.951885728605438, 0.9640395940358351, 0.968675604560832, 0.9711815561959655, 0.9710562586142087, 0.9739381029946123, 0.977070542538529, 0.9734369126675855, 0.9605312617466483, 0.977070542538529], 'quantized_model_size_bytes': 77398, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001940366526878127, 'batch_size': 256, 'epochs': 14, 'weight_decay': 0.000310957113680119, 'dropout': 0.0741804597058466, 'base_channels': 13, 'loss_type': np.str_('class_balanced'), 'gamma': 3.608276788742626, 'label_smoothing': 0.06746712918839036, 'scheduler': np.str_('none'), 'patience': 10, 'use_amp': False, 'grad_clip': 0.5471450192900692, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.903182944961301}, 'model_parameter_count': 38117, 'model_storage_size_kb': 163.78398437500002, 'model_size_validation': 'PASS'}
2025-09-24 19:51:59,691 - INFO - _models.training_function_executor - BO Objective: base=0.9771, size_penalty=0.0000, final=0.9771
2025-09-24 19:51:59,691 - INFO - _models.training_function_executor - Model: 38,117 parameters, 163.8KB (PASS 256KB limit)
2025-09-24 19:51:59,691 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 20.798s
2025-09-24 19:51:59,788 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9771
2025-09-24 19:51:59,788 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-09-24 19:51:59,789 - INFO - bo.run_bo - Recorded observation #25: hparams={'lr': 0.001940366526878127, 'batch_size': np.int64(256), 'epochs': np.int64(14), 'weight_decay': 0.000310957113680119, 'dropout': 0.0741804597058466, 'base_channels': np.int64(13), 'loss_type': np.str_('class_balanced'), 'gamma': 3.608276788742626, 'label_smoothing': 0.06746712918839036, 'scheduler': np.str_('none'), 'patience': np.int64(10), 'use_amp': np.False_, 'grad_clip': 0.5471450192900692, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.903182944961301}, value=0.9771
2025-09-24 19:51:59,789 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 25: {'lr': 0.001940366526878127, 'batch_size': np.int64(256), 'epochs': np.int64(14), 'weight_decay': 0.000310957113680119, 'dropout': 0.0741804597058466, 'base_channels': np.int64(13), 'loss_type': np.str_('class_balanced'), 'gamma': 3.608276788742626, 'label_smoothing': 0.06746712918839036, 'scheduler': np.str_('none'), 'patience': np.int64(10), 'use_amp': np.False_, 'grad_clip': 0.5471450192900692, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.903182944961301} -> 0.9771
2025-09-24 19:51:59,789 - INFO - bo.run_bo - üîçBO Trial 26: Using RF surrogate + Expected Improvement
2025-09-24 19:51:59,789 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:51:59,789 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:51:59,789 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:51:59,789 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001878448800574828, 'batch_size': 256, 'epochs': 40, 'weight_decay': 0.0013473423339255771, 'dropout': 0.04592714673539123, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 3.14845579817969, 'label_smoothing': 0.07551748995466288, 'scheduler': np.str_('none'), 'patience': 8, 'use_amp': False, 'grad_clip': 1.803502227318958, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9507104295984646}
2025-09-24 19:51:59,790 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001878448800574828, 'batch_size': 256, 'epochs': 40, 'weight_decay': 0.0013473423339255771, 'dropout': 0.04592714673539123, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 3.14845579817969, 'label_smoothing': 0.07551748995466288, 'scheduler': np.str_('none'), 'patience': 8, 'use_amp': False, 'grad_clip': 1.803502227318958, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9507104295984646}
2025-09-24 19:52:50,458 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB storage
2025-09-24 19:52:50,458 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9365576461484527, 0.5925351139619544, 0.47246896108917635, 0.44308470650147636, 0.4271903846420949, 0.41601765337651475, 0.40761687468754987, 0.4006857110926506, 0.39339458318229115, 0.38979676513823036, 0.3868181976060485, 0.38263564060248406, 0.3808141036469495, 0.37746622361093635, 0.3757525584798026, 0.37355859295297855, 0.3718229401383458, 0.37234010591160566, 0.37045291767746824, 0.36942011691893384, 0.3670954068412168, 0.3664841219543369, 0.36458727893190634, 0.36405100449170513, 0.36432114484834965, 0.3620897359340609, 0.36170576172574354, 0.36261479345900677, 0.3597088241302306, 0.3579597035085961, 0.35860058183180554, 0.3581901929064308, 0.35948663051126223, 0.35687400017361837, 0.3575538870854974, 0.3557988513773726, 0.3547282336301397, 0.3558341383082606, 0.35449777415972566, 0.35479431699620556], 'val_losses': [0.8382366955944984, 0.48192378380456646, 0.4579372734089063, 0.42617975903728406, 0.4141601578228277, 0.40323245359190035, 0.411843190663979, 0.4060686181928711, 0.3848182868575142, 0.39121801947878204, 0.38307515250502816, 0.3708951013859853, 0.37275259969169044, 0.36587844310869116, 0.36607244474116224, 0.3733825182768355, 0.3681040569223149, 0.3745802483632143, 0.36511010966673185, 0.36756525241989585, 0.36373903281317366, 0.3638610558288167, 0.35661543089261166, 0.36462655092537394, 0.3677120405903826, 0.3658790265038085, 0.3613144091766082, 0.36267203020879224, 0.3558298902745504, 0.35754962329532314, 0.362617125354036, 0.35444257190959105, 0.35836159604905077, 0.3573280768225089, 0.35497903131539477, 0.3573391174795096, 0.3547475855984704, 0.36539849796893825, 0.3569560652313309, 0.3570034580449684], 'val_acc': [0.7659441172785365, 0.9358476381405839, 0.9402330535020674, 0.9561458463851648, 0.9584012028567849, 0.9636637012905651, 0.9573988222027315, 0.9615336424007017, 0.9711815561959655, 0.9685503069790753, 0.9698032827966421, 0.9768199473750157, 0.9761934594662323, 0.9779476256108257, 0.9771958401202857, 0.9769452449567724, 0.976694649793259, 0.9738128054128555, 0.9773211377020423, 0.978198220774339, 0.9792006014283924, 0.9785741135196091, 0.9813306603182559, 0.977822328029069, 0.977446435283799, 0.978198220774339, 0.9800776845006891, 0.977822328029069, 0.9822077433905526, 0.983586016789876, 0.9784488159378524, 0.9825836361358226, 0.9793258990101491, 0.9798270893371758, 0.9822077433905526, 0.9819571482270392, 0.9827089337175793, 0.977822328029069, 0.9813306603182559, 0.9808294699912292], 'quantized_model_size_bytes': 93164, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001878448800574828, 'batch_size': 256, 'epochs': 40, 'weight_decay': 0.0013473423339255771, 'dropout': 0.04592714673539123, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 3.14845579817969, 'label_smoothing': 0.07551748995466288, 'scheduler': np.str_('none'), 'patience': 8, 'use_amp': False, 'grad_clip': 1.803502227318958, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9507104295984646}, 'model_parameter_count': 22853, 'model_storage_size_kb': 98.19648437500001, 'model_size_validation': 'PASS'}
2025-09-24 19:52:50,458 - INFO - _models.training_function_executor - BO Objective: base=0.9808, size_penalty=0.0000, final=0.9808
2025-09-24 19:52:50,458 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB (PASS 256KB limit)
2025-09-24 19:52:50,458 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 50.669s
2025-09-24 19:52:50,557 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9808
2025-09-24 19:52:50,557 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.099s
2025-09-24 19:52:50,557 - INFO - bo.run_bo - Recorded observation #26: hparams={'lr': 0.001878448800574828, 'batch_size': np.int64(256), 'epochs': np.int64(40), 'weight_decay': 0.0013473423339255771, 'dropout': 0.04592714673539123, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 3.14845579817969, 'label_smoothing': 0.07551748995466288, 'scheduler': np.str_('none'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 1.803502227318958, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9507104295984646}, value=0.9808
2025-09-24 19:52:50,557 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 26: {'lr': 0.001878448800574828, 'batch_size': np.int64(256), 'epochs': np.int64(40), 'weight_decay': 0.0013473423339255771, 'dropout': 0.04592714673539123, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 3.14845579817969, 'label_smoothing': 0.07551748995466288, 'scheduler': np.str_('none'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 1.803502227318958, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9507104295984646} -> 0.9808
2025-09-24 19:52:50,557 - INFO - bo.run_bo - üîçBO Trial 27: Using RF surrogate + Expected Improvement
2025-09-24 19:52:50,557 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:52:50,557 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:52:50,557 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:52:50,557 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004012399309464637, 'batch_size': 256, 'epochs': 19, 'weight_decay': 2.6892574211006748e-06, 'dropout': 0.13368197941823826, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 3.365302541874839, 'label_smoothing': 0.07107916338107824, 'scheduler': np.str_('onecycle'), 'patience': 8, 'use_amp': False, 'grad_clip': 0.9397654537930751, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9787627613899523}
2025-09-24 19:52:50,559 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004012399309464637, 'batch_size': 256, 'epochs': 19, 'weight_decay': 2.6892574211006748e-06, 'dropout': 0.13368197941823826, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 3.365302541874839, 'label_smoothing': 0.07107916338107824, 'scheduler': np.str_('onecycle'), 'patience': 8, 'use_amp': False, 'grad_clip': 0.9397654537930751, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9787627613899523}
2025-09-24 19:53:14,339 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB storage
2025-09-24 19:53:14,339 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.3036770031957277, 0.8595023670523805, 0.5605082011004757, 0.48613205220777506, 0.45278375096978785, 0.43689513494207893, 0.4238752743278691, 0.4124148998566933, 0.4035348161563492, 0.3964752933847812, 0.3892270308223071, 0.3849616604046546, 0.37823994466824007, 0.37444999042812177, 0.37122635690710265, 0.3689407008651265, 0.36553543574924374, 0.3623191332587793, 0.3635228950140175], 'val_losses': [0.9532495428335546, 0.6803822081618673, 0.47959858558482954, 0.5047906893691149, 0.45118796915028747, 0.4155017660683131, 0.3899265144451148, 0.3920813531986739, 0.3983450375647641, 0.37497764371000364, 0.366223707673783, 0.36811626479524434, 0.36146938745546214, 0.3645064736231125, 0.3518578034454891, 0.3508407048990278, 0.3495619537021982, 0.3484575356991723, 0.3487100835925519], 'val_acc': [0.7200852023555946, 0.870442300463601, 0.9349705550682872, 0.9225660944743767, 0.9408595414108508, 0.9520110261871946, 0.9645407843628618, 0.9615336424007017, 0.9602806665831349, 0.9693020924696153, 0.9706803658689387, 0.9714321513594788, 0.9748151860669089, 0.9726851271770455, 0.9768199473750157, 0.977822328029069, 0.9784488159378524, 0.978950006264879, 0.9788247086831224], 'quantized_model_size_bytes': 46618, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004012399309464637, 'batch_size': 256, 'epochs': 19, 'weight_decay': 2.6892574211006748e-06, 'dropout': 0.13368197941823826, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 3.365302541874839, 'label_smoothing': 0.07107916338107824, 'scheduler': np.str_('onecycle'), 'patience': 8, 'use_amp': False, 'grad_clip': 0.9397654537930751, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9787627613899523}, 'model_parameter_count': 22853, 'model_storage_size_kb': 98.19648437500001, 'model_size_validation': 'PASS'}
2025-09-24 19:53:14,339 - INFO - _models.training_function_executor - BO Objective: base=0.9788, size_penalty=0.0000, final=0.9788
2025-09-24 19:53:14,339 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB (PASS 256KB limit)
2025-09-24 19:53:14,339 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 23.782s
2025-09-24 19:53:14,438 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9788
2025-09-24 19:53:14,438 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.099s
2025-09-24 19:53:14,438 - INFO - bo.run_bo - Recorded observation #27: hparams={'lr': 0.004012399309464637, 'batch_size': np.int64(256), 'epochs': np.int64(19), 'weight_decay': 2.6892574211006748e-06, 'dropout': 0.13368197941823826, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 3.365302541874839, 'label_smoothing': 0.07107916338107824, 'scheduler': np.str_('onecycle'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 0.9397654537930751, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9787627613899523}, value=0.9788
2025-09-24 19:53:14,438 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 27: {'lr': 0.004012399309464637, 'batch_size': np.int64(256), 'epochs': np.int64(19), 'weight_decay': 2.6892574211006748e-06, 'dropout': 0.13368197941823826, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 3.365302541874839, 'label_smoothing': 0.07107916338107824, 'scheduler': np.str_('onecycle'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 0.9397654537930751, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9787627613899523} -> 0.9788
2025-09-24 19:53:14,438 - INFO - bo.run_bo - üîçBO Trial 28: Using RF surrogate + Expected Improvement
2025-09-24 19:53:14,438 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:53:14,438 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:53:14,438 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:53:14,438 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 7.227572608165168e-05, 'batch_size': 256, 'epochs': 12, 'weight_decay': 2.2450264963201607e-05, 'dropout': 0.08386874835522558, 'base_channels': 11, 'loss_type': np.str_('class_balanced'), 'gamma': 4.088160220392197, 'label_smoothing': 0.06757043170804512, 'scheduler': np.str_('onecycle'), 'patience': 9, 'use_amp': False, 'grad_clip': 1.6951185849415715, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9933654062600548}
2025-09-24 19:53:14,440 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 7.227572608165168e-05, 'batch_size': 256, 'epochs': 12, 'weight_decay': 2.2450264963201607e-05, 'dropout': 0.08386874835522558, 'base_channels': 11, 'loss_type': np.str_('class_balanced'), 'gamma': 4.088160220392197, 'label_smoothing': 0.06757043170804512, 'scheduler': np.str_('onecycle'), 'patience': 9, 'use_amp': False, 'grad_clip': 1.6951185849415715, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9933654062600548}
2025-09-24 19:53:30,703 - INFO - _models.training_function_executor - Model: 27,509 parameters, 118.2KB storage
2025-09-24 19:53:30,703 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.668005730493159, 1.5969074267595404, 1.3164468475389357, 0.9883314577838522, 0.9003307724217106, 0.8344137143103643, 0.7702990737067568, 0.7250572910539765, 0.6979160626699455, 0.6836712229701095, 0.6751421450620098, 0.673911699399768], 'val_losses': [1.6452460936504623, 1.4965451392684122, 1.078165696731054, 0.9194222806524982, 0.8508959356786434, 0.7764298096215153, 0.7196990572753489, 0.6826697442932186, 0.6645041332703726, 0.6531354555912207, 0.6483150367494246, 0.6475267221329879], 'val_acc': [0.07405087081819321, 0.7140709184312742, 0.7200852023555946, 0.7200852023555946, 0.7599298333542163, 0.7754667334920436, 0.7902518481393309, 0.7936348828467611, 0.7976444054629745, 0.8120536273649919, 0.8200726725974189, 0.8213256484149856], 'quantized_model_size_bytes': 56014, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 7.227572608165168e-05, 'batch_size': 256, 'epochs': 12, 'weight_decay': 2.2450264963201607e-05, 'dropout': 0.08386874835522558, 'base_channels': 11, 'loss_type': np.str_('class_balanced'), 'gamma': 4.088160220392197, 'label_smoothing': 0.06757043170804512, 'scheduler': np.str_('onecycle'), 'patience': 9, 'use_amp': False, 'grad_clip': 1.6951185849415715, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9933654062600548}, 'model_parameter_count': 27509, 'model_storage_size_kb': 118.202734375, 'model_size_validation': 'PASS'}
2025-09-24 19:53:30,703 - INFO - _models.training_function_executor - BO Objective: base=0.8213, size_penalty=0.0000, final=0.8213
2025-09-24 19:53:30,703 - INFO - _models.training_function_executor - Model: 27,509 parameters, 118.2KB (PASS 256KB limit)
2025-09-24 19:53:30,703 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 16.265s
2025-09-24 19:53:30,803 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8213
2025-09-24 19:53:30,803 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-24 19:53:30,803 - INFO - bo.run_bo - Recorded observation #28: hparams={'lr': 7.227572608165168e-05, 'batch_size': np.int64(256), 'epochs': np.int64(12), 'weight_decay': 2.2450264963201607e-05, 'dropout': 0.08386874835522558, 'base_channels': np.int64(11), 'loss_type': np.str_('class_balanced'), 'gamma': 4.088160220392197, 'label_smoothing': 0.06757043170804512, 'scheduler': np.str_('onecycle'), 'patience': np.int64(9), 'use_amp': np.False_, 'grad_clip': 1.6951185849415715, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9933654062600548}, value=0.8213
2025-09-24 19:53:30,803 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 28: {'lr': 7.227572608165168e-05, 'batch_size': np.int64(256), 'epochs': np.int64(12), 'weight_decay': 2.2450264963201607e-05, 'dropout': 0.08386874835522558, 'base_channels': np.int64(11), 'loss_type': np.str_('class_balanced'), 'gamma': 4.088160220392197, 'label_smoothing': 0.06757043170804512, 'scheduler': np.str_('onecycle'), 'patience': np.int64(9), 'use_amp': np.False_, 'grad_clip': 1.6951185849415715, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9933654062600548} -> 0.8213
2025-09-24 19:53:30,804 - INFO - bo.run_bo - üîçBO Trial 29: Using RF surrogate + Expected Improvement
2025-09-24 19:53:30,804 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:53:30,804 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:53:30,804 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:53:30,804 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0026502600835266346, 'batch_size': 128, 'epochs': 29, 'weight_decay': 0.0050183904647305155, 'dropout': 0.07910178704032957, 'base_channels': 22, 'loss_type': np.str_('class_balanced'), 'gamma': 2.5754378173844, 'label_smoothing': 0.06851826679157282, 'scheduler': np.str_('none'), 'patience': 9, 'use_amp': False, 'grad_clip': 1.478248877957214, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 4, 'beta_cb': 0.9639498246464645}
2025-09-24 19:53:30,805 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0026502600835266346, 'batch_size': 128, 'epochs': 29, 'weight_decay': 0.0050183904647305155, 'dropout': 0.07910178704032957, 'base_channels': 22, 'loss_type': np.str_('class_balanced'), 'gamma': 2.5754378173844, 'label_smoothing': 0.06851826679157282, 'scheduler': np.str_('none'), 'patience': 9, 'use_amp': False, 'grad_clip': 1.478248877957214, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 4, 'beta_cb': 0.9639498246464645}
2025-09-24 19:54:45,380 - INFO - _models.training_function_executor - Model: 107,237 parameters, 230.4KB storage
2025-09-24 19:54:45,381 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.647704662806303, 0.4363234470378515, 0.41514207571217454, 0.3962327576535382, 0.3822842197790819, 0.37339538369035224, 0.36949607942858886, 0.3645741968093217, 0.35891034040326175, 0.3566776604432157, 0.35478376353387864, 0.3531781131714929, 0.3490222136502621, 0.3485716861807396, 0.3475648891252522, 0.34574377360982644, 0.34605358673812, 0.34422198574005936, 0.34262565263690453, 0.3406904645160756, 0.34134755581892756, 0.3415755929737929, 0.34020116736565276, 0.3370431465388308, 0.3378981214580435, 0.33542196373497674, 0.33601717207118337, 0.334315159969251, 0.3314269751555877], 'val_losses': [0.4493894238636885, 0.4062749226728697, 0.4139589773370365, 0.38400052704991533, 0.3785265824532183, 0.36189286027123735, 0.3741631088602171, 0.3555111412961088, 0.3611946521195504, 0.354211947416366, 0.34509810590980017, 0.3505457630769574, 0.3480631749801148, 0.36139920294889755, 0.3386377458240197, 0.34691859887978915, 0.33742841887393404, 0.3406398228319826, 0.342800684873509, 0.34739709149146647, 0.33656008050355407, 0.3365094589568696, 0.34658053206286177, 0.3410587846045356, 0.33421962267323974, 0.33335455752841037, 0.3397719672995542, 0.33477512107294555, 0.3366854381119155], 'val_acc': [0.9367247212128806, 0.9505074552061146, 0.9555193584763814, 0.9626613206365117, 0.9647913795263752, 0.9711815561959655, 0.9651672722716451, 0.9748151860669089, 0.9718080441047487, 0.9736875078310988, 0.9783235183560958, 0.9764440546297456, 0.975566971557449, 0.9711815561959655, 0.9819571482270392, 0.9764440546297456, 0.9814559579000125, 0.9812053627364992, 0.9783235183560958, 0.9764440546297456, 0.9807041724094725, 0.9815812554817692, 0.9759428643027189, 0.9794511965919058, 0.983210124044606, 0.9828342312993359, 0.9784488159378524, 0.9814559579000125, 0.9808294699912292], 'quantized_model_size_bytes': 216394, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0026502600835266346, 'batch_size': 128, 'epochs': 29, 'weight_decay': 0.0050183904647305155, 'dropout': 0.07910178704032957, 'base_channels': 22, 'loss_type': np.str_('class_balanced'), 'gamma': 2.5754378173844, 'label_smoothing': 0.06851826679157282, 'scheduler': np.str_('none'), 'patience': 9, 'use_amp': False, 'grad_clip': 1.478248877957214, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 4, 'beta_cb': 0.9639498246464645}, 'model_parameter_count': 107237, 'model_storage_size_kb': 230.39199218750002, 'model_size_validation': 'PASS'}
2025-09-24 19:54:45,381 - INFO - _models.training_function_executor - BO Objective: base=0.9808, size_penalty=0.0000, final=0.9808
2025-09-24 19:54:45,381 - INFO - _models.training_function_executor - Model: 107,237 parameters, 230.4KB (PASS 256KB limit)
2025-09-24 19:54:45,381 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 74.577s
2025-09-24 19:54:45,481 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9808
2025-09-24 19:54:45,481 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-24 19:54:45,481 - INFO - bo.run_bo - Recorded observation #29: hparams={'lr': 0.0026502600835266346, 'batch_size': np.int64(128), 'epochs': np.int64(29), 'weight_decay': 0.0050183904647305155, 'dropout': 0.07910178704032957, 'base_channels': np.int64(22), 'loss_type': np.str_('class_balanced'), 'gamma': 2.5754378173844, 'label_smoothing': 0.06851826679157282, 'scheduler': np.str_('none'), 'patience': np.int64(9), 'use_amp': np.False_, 'grad_clip': 1.478248877957214, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(4), 'beta_cb': 0.9639498246464645}, value=0.9808
2025-09-24 19:54:45,481 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 29: {'lr': 0.0026502600835266346, 'batch_size': np.int64(128), 'epochs': np.int64(29), 'weight_decay': 0.0050183904647305155, 'dropout': 0.07910178704032957, 'base_channels': np.int64(22), 'loss_type': np.str_('class_balanced'), 'gamma': 2.5754378173844, 'label_smoothing': 0.06851826679157282, 'scheduler': np.str_('none'), 'patience': np.int64(9), 'use_amp': np.False_, 'grad_clip': 1.478248877957214, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(4), 'beta_cb': 0.9639498246464645} -> 0.9808
2025-09-24 19:54:45,482 - INFO - bo.run_bo - üîçBO Trial 30: Using RF surrogate + Expected Improvement
2025-09-24 19:54:45,482 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:54:45,482 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:54:45,482 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:54:45,482 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0076463146522013055, 'batch_size': 64, 'epochs': 29, 'weight_decay': 0.00040123949395187675, 'dropout': 0.08138886530760227, 'base_channels': 22, 'loss_type': np.str_('class_balanced'), 'gamma': 3.394290378320906, 'label_smoothing': 0.07498390875641422, 'scheduler': np.str_('plateau'), 'patience': 7, 'use_amp': False, 'grad_clip': 1.7746355939293488, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.956881867927302}
2025-09-24 19:54:45,483 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0076463146522013055, 'batch_size': 64, 'epochs': 29, 'weight_decay': 0.00040123949395187675, 'dropout': 0.08138886530760227, 'base_channels': 22, 'loss_type': np.str_('class_balanced'), 'gamma': 3.394290378320906, 'label_smoothing': 0.07498390875641422, 'scheduler': np.str_('plateau'), 'patience': 7, 'use_amp': False, 'grad_clip': 1.7746355939293488, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.956881867927302}
2025-09-24 19:56:11,470 - INFO - _models.training_function_executor - Model: 107,237 parameters, 460.8KB storage
2025-09-24 19:56:11,470 - WARNING - _models.training_function_executor - Model storage 460.8KB exceeds 256KB limit!
2025-09-24 19:56:11,470 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7658734812184734, 0.4957457546157444, 0.4485469729187188, 0.4251711374358254, 0.41261929587191165, 0.40895248170417753, 0.39633869823460827, 0.39193921490812406, 0.39042055309033535, 0.38563953428695935, 0.38366939120210064, 0.3819873653919637, 0.37922042015818236, 0.3773202989633768, 0.37617720649123876, 0.3748224505260356, 0.37310263736614796, 0.370713466877881, 0.37181126648287155, 0.3620614802563276, 0.35921015189158023, 0.35895529433725937, 0.3576832070997573, 0.3573125668311856, 0.35793825635126436, 0.3572967234739059, 0.3545221716377463, 0.3544408213316272, 0.3536234913844944], 'val_losses': [0.5267990397833415, 0.45041706058378883, 0.4619153193394771, 0.4123221879733742, 0.4095386316148638, 0.387088545827157, 0.42037907580716344, 0.3882595005974616, 0.37412410179126715, 0.3844697426665234, 0.3736151169900114, 0.3756447080501771, 0.37063709691553365, 0.37824887050720196, 0.3667438667887898, 0.37426607936430273, 0.3727934693404419, 0.3671956423186013, 0.37395233727684446, 0.3573950974396963, 0.35857317197025607, 0.3566135100935384, 0.35956447784083034, 0.3550597348916413, 0.35746684709656973, 0.3576572307846089, 0.3535144361483544, 0.36001295471920314, 0.3609084435840729], 'val_acc': [0.9194336549304598, 0.9481268011527377, 0.9391053752662574, 0.9591529883473249, 0.9620348327277284, 0.9730610199223155, 0.9580253101115148, 0.9713068537777221, 0.9769452449567724, 0.9728104247588022, 0.9794511965919058, 0.9756922691392056, 0.9780729231925824, 0.9721839368500188, 0.9783235183560958, 0.9750657812304222, 0.9748151860669089, 0.977822328029069, 0.9741886981581256, 0.9822077433905526, 0.9805788748277158, 0.9810800651547426, 0.9799523869189325, 0.9814559579000125, 0.9810800651547426, 0.9812053627364992, 0.9825836361358226, 0.9788247086831224, 0.978198220774339], 'quantized_model_size_bytes': 216394, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0076463146522013055, 'batch_size': 64, 'epochs': 29, 'weight_decay': 0.00040123949395187675, 'dropout': 0.08138886530760227, 'base_channels': 22, 'loss_type': np.str_('class_balanced'), 'gamma': 3.394290378320906, 'label_smoothing': 0.07498390875641422, 'scheduler': np.str_('plateau'), 'patience': 7, 'use_amp': False, 'grad_clip': 1.7746355939293488, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.956881867927302}, 'model_parameter_count': 107237, 'model_storage_size_kb': 460.78398437500005, 'model_size_validation': 'FAIL'}
2025-09-24 19:56:11,470 - INFO - _models.training_function_executor - BO Objective: base=0.9782, size_penalty=0.4000, final=0.5782
2025-09-24 19:56:11,470 - INFO - _models.training_function_executor - Model: 107,237 parameters, 460.8KB (FAIL 256KB limit)
2025-09-24 19:56:11,470 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 85.988s
2025-09-24 19:56:11,571 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5782
2025-09-24 19:56:11,571 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.101s
2025-09-24 19:56:11,571 - INFO - bo.run_bo - Recorded observation #30: hparams={'lr': 0.0076463146522013055, 'batch_size': np.int64(64), 'epochs': np.int64(29), 'weight_decay': 0.00040123949395187675, 'dropout': 0.08138886530760227, 'base_channels': np.int64(22), 'loss_type': np.str_('class_balanced'), 'gamma': 3.394290378320906, 'label_smoothing': 0.07498390875641422, 'scheduler': np.str_('plateau'), 'patience': np.int64(7), 'use_amp': np.False_, 'grad_clip': 1.7746355939293488, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.956881867927302}, value=0.5782
2025-09-24 19:56:11,571 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 30: {'lr': 0.0076463146522013055, 'batch_size': np.int64(64), 'epochs': np.int64(29), 'weight_decay': 0.00040123949395187675, 'dropout': 0.08138886530760227, 'base_channels': np.int64(22), 'loss_type': np.str_('class_balanced'), 'gamma': 3.394290378320906, 'label_smoothing': 0.07498390875641422, 'scheduler': np.str_('plateau'), 'patience': np.int64(7), 'use_amp': np.False_, 'grad_clip': 1.7746355939293488, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.956881867927302} -> 0.5782
2025-09-24 19:56:11,572 - INFO - bo.run_bo - üîçBO Trial 31: Using RF surrogate + Expected Improvement
2025-09-24 19:56:11,572 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:56:11,572 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:56:11,572 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:56:11,572 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006113449048138171, 'batch_size': 256, 'epochs': 48, 'weight_decay': 0.00042238222617430423, 'dropout': 0.2628046092149793, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 3.8059940084021284, 'label_smoothing': 0.08615173099610943, 'scheduler': np.str_('plateau'), 'patience': 7, 'use_amp': False, 'grad_clip': 0.6863043992281294, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9837099159087801}
2025-09-24 19:56:11,573 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006113449048138171, 'batch_size': 256, 'epochs': 48, 'weight_decay': 0.00042238222617430423, 'dropout': 0.2628046092149793, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 3.8059940084021284, 'label_smoothing': 0.08615173099610943, 'scheduler': np.str_('plateau'), 'patience': 7, 'use_amp': False, 'grad_clip': 0.6863043992281294, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9837099159087801}
2025-09-24 19:58:01,174 - INFO - _models.training_function_executor - Model: 97,829 parameters, 420.4KB storage
2025-09-24 19:58:01,174 - WARNING - _models.training_function_executor - Model storage 420.4KB exceeds 256KB limit!
2025-09-24 19:58:01,174 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8527319092624779, 0.5563324029794504, 0.5091388005550556, 0.4941013538870668, 0.47694792223821375, 0.46693598001862074, 0.4654328501082989, 0.4562602663041655, 0.4520730771012568, 0.4423584922393365, 0.4411653241664437, 0.4402051975758225, 0.43770516064835235, 0.43136583402469536, 0.43095606821833504, 0.4284298894737566, 0.4280435278317618, 0.4247155562586994, 0.42459652940533055, 0.42290661009026514, 0.4168529250181674, 0.4191687551334314, 0.4172400932404174, 0.41766621643927143, 0.4139953415078955, 0.41431359637109605, 0.41297252030856285, 0.4108364396234863, 0.4140295744254073, 0.40841260438044114, 0.40842362633475315, 0.409428228035579, 0.41002409238136867, 0.40969031154073143, 0.40618423034018325, 0.4063057452071502, 0.4072791806095746, 0.40710964133572697, 0.4070615805728698, 0.40460037954154665, 0.40376149661610516, 0.40463068863598173, 0.40305746891006167, 0.40488511885003786, 0.40257720715852546, 0.40378186983769876, 0.3958187411078511, 0.3942316373469361], 'val_losses': [0.6184557925533074, 0.49556840759607623, 0.4899402274895455, 0.46721107256829236, 0.45416210130437723, 0.442780048838834, 0.44702778173529584, 0.4433593428264867, 0.43005500440534206, 0.42522484706377506, 0.42079902287862486, 0.4207259283196043, 0.41683782263890107, 0.41888942415366837, 0.4155470527715065, 0.42952356676741094, 0.41180099233800105, 0.4128195776883828, 0.40635238789687583, 0.4033512292597142, 0.4047901477525145, 0.4046390814980444, 0.41522709565448845, 0.4038762949773982, 0.4010343335353257, 0.4021628990894481, 0.39928641513152435, 0.40134825000427166, 0.4018577249885696, 0.39615726402238743, 0.39933905577528167, 0.3958013344716195, 0.39473342950773366, 0.39465851849324274, 0.3998551679096669, 0.3978002863358382, 0.3947390013134819, 0.39153949948289823, 0.3954124420959689, 0.3939938911398675, 0.39728549259853996, 0.3931536130843314, 0.4077087991399852, 0.39301167218073646, 0.3966642369185604, 0.39344307800430267, 0.3875057748080704, 0.3882893267811248], 'val_acc': [0.8966294950507455, 0.9393559704297707, 0.9418619220649042, 0.9523869189324646, 0.9542663826588147, 0.9600300714196216, 0.9577747149480015, 0.9610324520736749, 0.9642901891993485, 0.9667961408344818, 0.9708056634506954, 0.967547926325022, 0.9711815561959655, 0.9680491166520486, 0.9713068537777221, 0.9672973311615086, 0.9735622102493422, 0.9723092344317754, 0.9749404836486656, 0.9754416739756923, 0.9751910788121789, 0.976694649793259, 0.9677985214885353, 0.977070542538529, 0.9760681618844756, 0.9771958401202857, 0.9790753038466358, 0.9771958401202857, 0.9792006014283924, 0.9805788748277158, 0.9790753038466358, 0.9802029820824458, 0.9797017917554192, 0.9795764941736624, 0.9769452449567724, 0.9779476256108257, 0.9812053627364992, 0.9815812554817692, 0.9804535772459592, 0.9812053627364992, 0.9804535772459592, 0.9800776845006891, 0.976694649793259, 0.9809547675729858, 0.9799523869189325, 0.9808294699912292, 0.9833354216263626, 0.9825836361358226], 'quantized_model_size_bytes': 394916, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006113449048138171, 'batch_size': 256, 'epochs': 48, 'weight_decay': 0.00042238222617430423, 'dropout': 0.2628046092149793, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 3.8059940084021284, 'label_smoothing': 0.08615173099610943, 'scheduler': np.str_('plateau'), 'patience': 7, 'use_amp': False, 'grad_clip': 0.6863043992281294, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9837099159087801}, 'model_parameter_count': 97829, 'model_storage_size_kb': 420.35898437500003, 'model_size_validation': 'FAIL'}
2025-09-24 19:58:01,174 - INFO - _models.training_function_executor - BO Objective: base=0.9826, size_penalty=0.3210, final=0.6616
2025-09-24 19:58:01,174 - INFO - _models.training_function_executor - Model: 97,829 parameters, 420.4KB (FAIL 256KB limit)
2025-09-24 19:58:01,174 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 109.602s
2025-09-24 19:58:01,276 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6616
2025-09-24 19:58:01,276 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.102s
2025-09-24 19:58:01,276 - INFO - bo.run_bo - Recorded observation #31: hparams={'lr': 0.006113449048138171, 'batch_size': np.int64(256), 'epochs': np.int64(48), 'weight_decay': 0.00042238222617430423, 'dropout': 0.2628046092149793, 'base_channels': np.int64(21), 'loss_type': np.str_('class_balanced'), 'gamma': 3.8059940084021284, 'label_smoothing': 0.08615173099610943, 'scheduler': np.str_('plateau'), 'patience': np.int64(7), 'use_amp': np.False_, 'grad_clip': 0.6863043992281294, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9837099159087801}, value=0.6616
2025-09-24 19:58:01,276 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 31: {'lr': 0.006113449048138171, 'batch_size': np.int64(256), 'epochs': np.int64(48), 'weight_decay': 0.00042238222617430423, 'dropout': 0.2628046092149793, 'base_channels': np.int64(21), 'loss_type': np.str_('class_balanced'), 'gamma': 3.8059940084021284, 'label_smoothing': 0.08615173099610943, 'scheduler': np.str_('plateau'), 'patience': np.int64(7), 'use_amp': np.False_, 'grad_clip': 0.6863043992281294, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9837099159087801} -> 0.6616
2025-09-24 19:58:01,277 - INFO - bo.run_bo - üîçBO Trial 32: Using RF surrogate + Expected Improvement
2025-09-24 19:58:01,277 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:58:01,277 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:58:01,277 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:58:01,277 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005281673242604177, 'batch_size': 128, 'epochs': 21, 'weight_decay': 0.002784682041112693, 'dropout': 0.08628188344209388, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 3.614871960796752, 'label_smoothing': 0.06345215620921811, 'scheduler': np.str_('onecycle'), 'patience': 6, 'use_amp': True, 'grad_clip': 0.9573279367714644, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 9, 'beta_cb': 0.9567212751724004}
2025-09-24 19:58:01,278 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005281673242604177, 'batch_size': 128, 'epochs': 21, 'weight_decay': 0.002784682041112693, 'dropout': 0.08628188344209388, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 3.614871960796752, 'label_smoothing': 0.06345215620921811, 'scheduler': np.str_('onecycle'), 'patience': 6, 'use_amp': True, 'grad_clip': 0.9573279367714644, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 9, 'beta_cb': 0.9567212751724004}
2025-09-24 19:58:28,600 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB storage
2025-09-24 19:58:28,601 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0752839799124925, 0.6006058898033498, 0.450563273946545, 0.4203054936249648, 0.4066376546247559, 0.39129422596215463, 0.37941888080232367, 0.3663891707086789, 0.3582066042872183, 0.35344120039212595, 0.3453343524836666, 0.34443298765947034, 0.33874590615410927, 0.33585193243607186, 0.3310403684918804, 0.32794065658798144, 0.3243222297744726, 0.3212937622776016, 0.3203081805173218, 0.31831948785477443, 0.31573560213947816], 'val_losses': [0.8208022812317672, 0.4823370642111484, 0.422683522551449, 0.5225206464921124, 0.38896007488401535, 0.3745460219736342, 0.3643409083016817, 0.35319118245609005, 0.3552079087823663, 0.33846368342395294, 0.3345308517277532, 0.33533648673999045, 0.3471496403889464, 0.32149785098359185, 0.3214315178026578, 0.31820257416779407, 0.3180621523573918, 0.31710484320321874, 0.31437753729616513, 0.3134031934317544, 0.3131876975324067], 'val_acc': [0.7273524620974815, 0.9193083573487032, 0.9442425761182809, 0.9052750281919559, 0.9525122165142212, 0.9582759052750282, 0.9636637012905651, 0.968299711815562, 0.9679238190702919, 0.9760681618844756, 0.9769452449567724, 0.9725598295952889, 0.9679238190702919, 0.9805788748277158, 0.9803282796642024, 0.9822077433905526, 0.9814559579000125, 0.9817065530635258, 0.9834607192081193, 0.9829595288810926, 0.9830848264628492], 'quantized_model_size_bytes': 46618, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005281673242604177, 'batch_size': 128, 'epochs': 21, 'weight_decay': 0.002784682041112693, 'dropout': 0.08628188344209388, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 3.614871960796752, 'label_smoothing': 0.06345215620921811, 'scheduler': np.str_('onecycle'), 'patience': 6, 'use_amp': True, 'grad_clip': 0.9573279367714644, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 9, 'beta_cb': 0.9567212751724004}, 'model_parameter_count': 22853, 'model_storage_size_kb': 98.19648437500001, 'model_size_validation': 'PASS'}
2025-09-24 19:58:28,601 - INFO - _models.training_function_executor - BO Objective: base=0.9831, size_penalty=0.0000, final=0.9831
2025-09-24 19:58:28,601 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB (PASS 256KB limit)
2025-09-24 19:58:28,601 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 27.324s
2025-09-24 19:58:28,702 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9831
2025-09-24 19:58:28,702 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.101s
2025-09-24 19:58:28,702 - INFO - bo.run_bo - Recorded observation #32: hparams={'lr': 0.005281673242604177, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'weight_decay': 0.002784682041112693, 'dropout': 0.08628188344209388, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 3.614871960796752, 'label_smoothing': 0.06345215620921811, 'scheduler': np.str_('onecycle'), 'patience': np.int64(6), 'use_amp': np.True_, 'grad_clip': 0.9573279367714644, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(9), 'beta_cb': 0.9567212751724004}, value=0.9831
2025-09-24 19:58:28,702 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 32: {'lr': 0.005281673242604177, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'weight_decay': 0.002784682041112693, 'dropout': 0.08628188344209388, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 3.614871960796752, 'label_smoothing': 0.06345215620921811, 'scheduler': np.str_('onecycle'), 'patience': np.int64(6), 'use_amp': np.True_, 'grad_clip': 0.9573279367714644, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(9), 'beta_cb': 0.9567212751724004} -> 0.9831
2025-09-24 19:58:28,702 - INFO - bo.run_bo - üîçBO Trial 33: Using RF surrogate + Expected Improvement
2025-09-24 19:58:28,702 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:58:28,703 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:58:28,703 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:58:28,703 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004816419256431413, 'batch_size': 64, 'epochs': 20, 'weight_decay': 0.006445242030827269, 'dropout': 0.1584149713912227, 'base_channels': 12, 'loss_type': np.str_('class_balanced'), 'gamma': 4.758928901208344, 'label_smoothing': 0.00749717029224999, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': False, 'grad_clip': 1.8088707065278464, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 4, 'beta_cb': 0.9463454213127485}
2025-09-24 19:58:28,704 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004816419256431413, 'batch_size': 64, 'epochs': 20, 'weight_decay': 0.006445242030827269, 'dropout': 0.1584149713912227, 'base_channels': 12, 'loss_type': np.str_('class_balanced'), 'gamma': 4.758928901208344, 'label_smoothing': 0.00749717029224999, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': False, 'grad_clip': 1.8088707065278464, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 4, 'beta_cb': 0.9463454213127485}
2025-09-24 19:59:04,055 - INFO - _models.training_function_executor - Model: 27,216 parameters, 29.2KB storage
2025-09-24 19:59:04,055 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4965783698746343, 0.26319363037827315, 0.22517122763091152, 0.20139080139455673, 0.18811009812979787, 0.1790162478598856, 0.17086153560354608, 0.16393963403536987, 0.15629504995810165, 0.15311656228135345, 0.15082129653196297, 0.14768446045630565, 0.14506025375554507, 0.14375472377424525, 0.14140287179412225, 0.13606028034253828, 0.13568127463755145, 0.13485098157127157, 0.13150068198167356, 0.13201810404669054], 'val_losses': [0.2577543763095851, 0.24244849959078207, 0.18626139956284787, 0.18784358821153013, 0.1672209434966488, 0.15021738966199005, 0.14277654555399247, 0.1653489308232667, 0.16879158934664418, 0.13909528701619955, 0.143700102228163, 0.14563846109975714, 0.13394471733936944, 0.1373973393997383, 0.13107176003502718, 0.12906838084742953, 0.11418430119017255, 0.12411418776922306, 0.11788301219723306, 0.1149497468670035], 'val_acc': [0.9328404961784238, 0.9362235308858539, 0.953765192331788, 0.9601553690013783, 0.9619095351459717, 0.9681744142338052, 0.9714321513594788, 0.9619095351459717, 0.9639142964540784, 0.9731863175040721, 0.9714321513594788, 0.970179175541912, 0.9730610199223155, 0.9713068537777221, 0.9749404836486656, 0.9746898884851523, 0.9768199473750157, 0.975566971557449, 0.9761934594662323, 0.976318757047989], 'quantized_model_size_bytes': 111000, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004816419256431413, 'batch_size': 64, 'epochs': 20, 'weight_decay': 0.006445242030827269, 'dropout': 0.1584149713912227, 'base_channels': 12, 'loss_type': np.str_('class_balanced'), 'gamma': 4.758928901208344, 'label_smoothing': 0.00749717029224999, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': False, 'grad_clip': 1.8088707065278464, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 4, 'beta_cb': 0.9463454213127485}, 'model_parameter_count': 27216, 'model_storage_size_kb': 29.235937500000002, 'model_size_validation': 'PASS'}
2025-09-24 19:59:04,055 - INFO - _models.training_function_executor - BO Objective: base=0.9763, size_penalty=0.0000, final=0.9763
2025-09-24 19:59:04,055 - INFO - _models.training_function_executor - Model: 27,216 parameters, 29.2KB (PASS 256KB limit)
2025-09-24 19:59:04,055 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 35.353s
2025-09-24 19:59:04,158 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9763
2025-09-24 19:59:04,158 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.102s
2025-09-24 19:59:04,158 - INFO - bo.run_bo - Recorded observation #33: hparams={'lr': 0.004816419256431413, 'batch_size': np.int64(64), 'epochs': np.int64(20), 'weight_decay': 0.006445242030827269, 'dropout': 0.1584149713912227, 'base_channels': np.int64(12), 'loss_type': np.str_('class_balanced'), 'gamma': 4.758928901208344, 'label_smoothing': 0.00749717029224999, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 1.8088707065278464, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(4), 'beta_cb': 0.9463454213127485}, value=0.9763
2025-09-24 19:59:04,158 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 33: {'lr': 0.004816419256431413, 'batch_size': np.int64(64), 'epochs': np.int64(20), 'weight_decay': 0.006445242030827269, 'dropout': 0.1584149713912227, 'base_channels': np.int64(12), 'loss_type': np.str_('class_balanced'), 'gamma': 4.758928901208344, 'label_smoothing': 0.00749717029224999, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.False_, 'grad_clip': 1.8088707065278464, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(4), 'beta_cb': 0.9463454213127485} -> 0.9763
2025-09-24 19:59:04,158 - INFO - bo.run_bo - üîçBO Trial 34: Using RF surrogate + Expected Improvement
2025-09-24 19:59:04,158 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:59:04,158 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:59:04,159 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:59:04,159 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003993160243921063, 'batch_size': 256, 'epochs': 18, 'weight_decay': 0.0004727607289436717, 'dropout': 0.13835655976153258, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 4.720683456067167, 'label_smoothing': 0.053321875874818, 'scheduler': np.str_('plateau'), 'patience': 2, 'use_amp': True, 'grad_clip': 1.9314724496427562, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 5, 'beta_cb': 0.9362224978759259}
2025-09-24 19:59:04,160 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003993160243921063, 'batch_size': 256, 'epochs': 18, 'weight_decay': 0.0004727607289436717, 'dropout': 0.13835655976153258, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 4.720683456067167, 'label_smoothing': 0.053321875874818, 'scheduler': np.str_('plateau'), 'patience': 2, 'use_amp': True, 'grad_clip': 1.9314724496427562, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 5, 'beta_cb': 0.9362224978759259}
2025-09-24 19:59:24,705 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB storage
2025-09-24 19:59:24,705 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9008850845721336, 0.5620719113341102, 0.45114517805850646, 0.41361828714154497, 0.3963458807632797, 0.3826788135744167, 0.3744269476772471, 0.3651660957518061, 0.3577069758321432, 0.35338097561762394, 0.34788643345887943, 0.3386872421286218, 0.3416974416047384, 0.3330014808279595, 0.33060965512482454, 0.3277694822194205, 0.32476340675717724, 0.3231788088260907], 'val_losses': [0.7011531392683468, 0.5209004650990059, 0.4137491485690461, 0.3993761144497239, 0.36849307873885057, 0.3662375931549514, 0.3605084255358253, 0.34734381618258975, 0.3630901360224996, 0.34753402731887917, 0.33240896287141863, 0.32191543930635286, 0.3254247594411444, 0.31661829669408936, 0.3141092710395816, 0.31280176817452215, 0.31754811269988126, 0.3149509850211228], 'val_acc': [0.8126801152737753, 0.8947500313243955, 0.9371006139581506, 0.9476256108257111, 0.9507580503696279, 0.9550181681493547, 0.9522616213507079, 0.9576494173662449, 0.9565217391304348, 0.9592782859290816, 0.962160130309485, 0.9656684625986719, 0.9680491166520486, 0.9679238190702919, 0.9700538779601554, 0.9703044731236687, 0.9661696529256986, 0.968299711815562], 'quantized_model_size_bytes': 93164, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003993160243921063, 'batch_size': 256, 'epochs': 18, 'weight_decay': 0.0004727607289436717, 'dropout': 0.13835655976153258, 'base_channels': 10, 'loss_type': np.str_('class_balanced'), 'gamma': 4.720683456067167, 'label_smoothing': 0.053321875874818, 'scheduler': np.str_('plateau'), 'patience': 2, 'use_amp': True, 'grad_clip': 1.9314724496427562, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 5, 'beta_cb': 0.9362224978759259}, 'model_parameter_count': 22853, 'model_storage_size_kb': 98.19648437500001, 'model_size_validation': 'PASS'}
2025-09-24 19:59:24,705 - INFO - _models.training_function_executor - BO Objective: base=0.9683, size_penalty=0.0000, final=0.9683
2025-09-24 19:59:24,705 - INFO - _models.training_function_executor - Model: 22,853 parameters, 98.2KB (PASS 256KB limit)
2025-09-24 19:59:24,705 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 20.547s
2025-09-24 19:59:24,807 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9683
2025-09-24 19:59:24,808 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.102s
2025-09-24 19:59:24,808 - INFO - bo.run_bo - Recorded observation #34: hparams={'lr': 0.003993160243921063, 'batch_size': np.int64(256), 'epochs': np.int64(18), 'weight_decay': 0.0004727607289436717, 'dropout': 0.13835655976153258, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 4.720683456067167, 'label_smoothing': 0.053321875874818, 'scheduler': np.str_('plateau'), 'patience': np.int64(2), 'use_amp': np.True_, 'grad_clip': 1.9314724496427562, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9362224978759259}, value=0.9683
2025-09-24 19:59:24,808 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 34: {'lr': 0.003993160243921063, 'batch_size': np.int64(256), 'epochs': np.int64(18), 'weight_decay': 0.0004727607289436717, 'dropout': 0.13835655976153258, 'base_channels': np.int64(10), 'loss_type': np.str_('class_balanced'), 'gamma': 4.720683456067167, 'label_smoothing': 0.053321875874818, 'scheduler': np.str_('plateau'), 'patience': np.int64(2), 'use_amp': np.True_, 'grad_clip': 1.9314724496427562, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9362224978759259} -> 0.9683
2025-09-24 19:59:24,808 - INFO - bo.run_bo - üîçBO Trial 35: Using RF surrogate + Expected Improvement
2025-09-24 19:59:24,808 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 19:59:24,808 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 19:59:24,808 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 19:59:24,808 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003099952701594268, 'batch_size': 32, 'epochs': 39, 'weight_decay': 0.0027467175960918778, 'dropout': 0.19069666869789403, 'base_channels': 17, 'loss_type': np.str_('class_balanced'), 'gamma': 4.683475179675703, 'label_smoothing': 0.015672035363564367, 'scheduler': np.str_('none'), 'patience': 5, 'use_amp': False, 'grad_clip': 1.1916759921728293, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9306025418555105}
2025-09-24 19:59:24,810 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003099952701594268, 'batch_size': 32, 'epochs': 39, 'weight_decay': 0.0027467175960918778, 'dropout': 0.19069666869789403, 'base_channels': 17, 'loss_type': np.str_('class_balanced'), 'gamma': 4.683475179675703, 'label_smoothing': 0.015672035363564367, 'scheduler': np.str_('none'), 'patience': 5, 'use_amp': False, 'grad_clip': 1.1916759921728293, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9306025418555105}
2025-09-24 20:01:17,295 - INFO - _models.training_function_executor - Model: 64,517 parameters, 277.2KB storage
2025-09-24 20:01:17,295 - WARNING - _models.training_function_executor - Model storage 277.2KB exceeds 256KB limit!
2025-09-24 20:01:17,295 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5747613719605087, 0.2948505312089532, 0.258155331649947, 0.22960715087677563, 0.22147130161450068, 0.21049348272517882, 0.20299758450752609, 0.19499588677583138, 0.1910050850255042, 0.18696523116942887, 0.18195705494917497, 0.17726935192191562, 0.17982438873894416, 0.1756171278420503, 0.1710673531266665, 0.1705903666714127, 0.16665890150646687, 0.16520483077811265, 0.1669971105723633, 0.16229662678651977, 0.1614090960196369, 0.159894053081621, 0.15947236947049415, 0.1603262749393008, 0.15717524734930455, 0.1574749846489971, 0.15683120048615962, 0.15419787071504992, 0.15466042076448516, 0.1529499294210454, 0.1537217722946445, 0.15298826753984876, 0.15315029224021368, 0.15118374556038824, 0.1504657780904166, 0.1506241540629415, 0.1479649576175722, 0.14969307129325504, 0.14983669242938652], 'val_losses': [0.28448352019017, 0.24500127525246543, 0.2210733141087751, 0.2004165113898331, 0.1998634952872099, 0.1899081690167144, 0.17497090458899808, 0.17536771006937268, 0.16825119315898293, 0.16966493148251652, 0.16025915996216694, 0.1700319185910884, 0.16124764113569004, 0.15382260142778517, 0.15601560229974915, 0.15362102439298678, 0.1476782121771545, 0.15415315298848306, 0.15273512364210542, 0.14802765070294216, 0.15037532101641202, 0.15248711830451514, 0.1478739295100198, 0.15301611258143874, 0.15193690344469094, 0.1521002906386886, 0.1426488845558367, 0.14565407600248687, 0.1458009531505125, 0.14439646768444478, 0.15091016969649718, 0.1482681281329725, 0.14411921297125224, 0.1534763951040781, 0.1451105774569192, 0.1420930735435481, 0.14476008099536372, 0.13849491576639994, 0.13671105469382955], 'val_acc': [0.9407342438290941, 0.9510086455331412, 0.9546422754040848, 0.9609071544919183, 0.9619095351459717, 0.9665455456709685, 0.9685503069790753, 0.9676732239067786, 0.9714321513594788, 0.9716827465229921, 0.9761934594662323, 0.9703044731236687, 0.975566971557449, 0.9764440546297456, 0.9765693522115023, 0.9773211377020423, 0.9784488159378524, 0.9792006014283924, 0.9776970304473124, 0.9793258990101491, 0.9768199473750157, 0.9764440546297456, 0.9780729231925824, 0.976318757047989, 0.9785741135196091, 0.9797017917554192, 0.9793258990101491, 0.9810800651547426, 0.9795764941736624, 0.9809547675729858, 0.9784488159378524, 0.9788247086831224, 0.9803282796642024, 0.9784488159378524, 0.9786994111013657, 0.9813306603182559, 0.9800776845006891, 0.9825836361358226, 0.9812053627364992], 'quantized_model_size_bytes': 130534, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003099952701594268, 'batch_size': 32, 'epochs': 39, 'weight_decay': 0.0027467175960918778, 'dropout': 0.19069666869789403, 'base_channels': 17, 'loss_type': np.str_('class_balanced'), 'gamma': 4.683475179675703, 'label_smoothing': 0.015672035363564367, 'scheduler': np.str_('none'), 'patience': 5, 'use_amp': False, 'grad_clip': 1.1916759921728293, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9306025418555105}, 'model_parameter_count': 64517, 'model_storage_size_kb': 277.22148437500005, 'model_size_validation': 'FAIL'}
2025-09-24 20:01:17,295 - INFO - _models.training_function_executor - BO Objective: base=0.9812, size_penalty=0.0414, final=0.9398
2025-09-24 20:01:17,295 - INFO - _models.training_function_executor - Model: 64,517 parameters, 277.2KB (FAIL 256KB limit)
2025-09-24 20:01:17,295 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 112.487s
2025-09-24 20:01:17,535 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9398
2025-09-24 20:01:17,535 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.240s
2025-09-24 20:01:17,535 - INFO - bo.run_bo - Recorded observation #35: hparams={'lr': 0.003099952701594268, 'batch_size': np.int64(32), 'epochs': np.int64(39), 'weight_decay': 0.0027467175960918778, 'dropout': 0.19069666869789403, 'base_channels': np.int64(17), 'loss_type': np.str_('class_balanced'), 'gamma': 4.683475179675703, 'label_smoothing': 0.015672035363564367, 'scheduler': np.str_('none'), 'patience': np.int64(5), 'use_amp': np.False_, 'grad_clip': 1.1916759921728293, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9306025418555105}, value=0.9398
2025-09-24 20:01:17,535 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 35: {'lr': 0.003099952701594268, 'batch_size': np.int64(32), 'epochs': np.int64(39), 'weight_decay': 0.0027467175960918778, 'dropout': 0.19069666869789403, 'base_channels': np.int64(17), 'loss_type': np.str_('class_balanced'), 'gamma': 4.683475179675703, 'label_smoothing': 0.015672035363564367, 'scheduler': np.str_('none'), 'patience': np.int64(5), 'use_amp': np.False_, 'grad_clip': 1.1916759921728293, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9306025418555105} -> 0.9398
2025-09-24 20:01:17,536 - INFO - bo.run_bo - üîçBO Trial 36: Using RF surrogate + Expected Improvement
2025-09-24 20:01:17,536 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 20:01:17,536 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:01:17,536 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:01:17,536 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.008096524583383545, 'batch_size': 256, 'epochs': 15, 'weight_decay': 0.003654256954238407, 'dropout': 0.10012467908731795, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 2.7077985078435445, 'label_smoothing': 0.04076386863932811, 'scheduler': np.str_('onecycle'), 'patience': 2, 'use_amp': False, 'grad_clip': 1.5088032710209553, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9629601970513929}
2025-09-24 20:01:17,537 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.008096524583383545, 'batch_size': 256, 'epochs': 15, 'weight_decay': 0.003654256954238407, 'dropout': 0.10012467908731795, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 2.7077985078435445, 'label_smoothing': 0.04076386863932811, 'scheduler': np.str_('onecycle'), 'patience': 2, 'use_amp': False, 'grad_clip': 1.5088032710209553, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9629601970513929}
2025-09-24 20:01:51,536 - INFO - _models.training_function_executor - Model: 97,829 parameters, 420.4KB storage
2025-09-24 20:01:51,536 - WARNING - _models.training_function_executor - Model storage 420.4KB exceeds 256KB limit!
2025-09-24 20:01:51,537 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9555685781832831, 0.4717887933102705, 0.35167913518035465, 0.3240618523330134, 0.3092641010864133, 0.2963724898387551, 0.2832622505838559, 0.27566906749311554, 0.26515390662678245, 0.2573088931958187, 0.25325054926213975, 0.24648431068213889, 0.2426832788904086, 0.23786258836238117, 0.2363524887701124], 'val_losses': [0.6954232475211217, 0.35878116022732426, 0.42676890783628985, 0.3277565638678607, 0.33285722735083173, 0.28710248046554043, 0.2673199337805263, 0.2574565368305993, 0.2582506671045645, 0.24956461395510546, 0.24511588386777738, 0.24072427663598767, 0.2383683403621803, 0.2352311447096594, 0.2353629945528685], 'val_acc': [0.8015286304974314, 0.9432401954642275, 0.9141711564966796, 0.9562711439669215, 0.9501315624608445, 0.9665455456709685, 0.9714321513594788, 0.9738128054128555, 0.9736875078310988, 0.978950006264879, 0.9783235183560958, 0.9795764941736624, 0.9797017917554192, 0.9827089337175793, 0.982458338554066], 'quantized_model_size_bytes': 394916, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.008096524583383545, 'batch_size': 256, 'epochs': 15, 'weight_decay': 0.003654256954238407, 'dropout': 0.10012467908731795, 'base_channels': 21, 'loss_type': np.str_('class_balanced'), 'gamma': 2.7077985078435445, 'label_smoothing': 0.04076386863932811, 'scheduler': np.str_('onecycle'), 'patience': 2, 'use_amp': False, 'grad_clip': 1.5088032710209553, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9629601970513929}, 'model_parameter_count': 97829, 'model_storage_size_kb': 420.35898437500003, 'model_size_validation': 'FAIL'}
2025-09-24 20:01:51,537 - INFO - _models.training_function_executor - BO Objective: base=0.9825, size_penalty=0.3210, final=0.6614
2025-09-24 20:01:51,537 - INFO - _models.training_function_executor - Model: 97,829 parameters, 420.4KB (FAIL 256KB limit)
2025-09-24 20:01:51,537 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 34.001s
2025-09-24 20:01:51,641 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6614
2025-09-24 20:01:51,641 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.104s
2025-09-24 20:01:51,641 - INFO - bo.run_bo - Recorded observation #36: hparams={'lr': 0.008096524583383545, 'batch_size': np.int64(256), 'epochs': np.int64(15), 'weight_decay': 0.003654256954238407, 'dropout': 0.10012467908731795, 'base_channels': np.int64(21), 'loss_type': np.str_('class_balanced'), 'gamma': 2.7077985078435445, 'label_smoothing': 0.04076386863932811, 'scheduler': np.str_('onecycle'), 'patience': np.int64(2), 'use_amp': np.False_, 'grad_clip': 1.5088032710209553, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9629601970513929}, value=0.6614
2025-09-24 20:01:51,641 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 36: {'lr': 0.008096524583383545, 'batch_size': np.int64(256), 'epochs': np.int64(15), 'weight_decay': 0.003654256954238407, 'dropout': 0.10012467908731795, 'base_channels': np.int64(21), 'loss_type': np.str_('class_balanced'), 'gamma': 2.7077985078435445, 'label_smoothing': 0.04076386863932811, 'scheduler': np.str_('onecycle'), 'patience': np.int64(2), 'use_amp': np.False_, 'grad_clip': 1.5088032710209553, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9629601970513929} -> 0.6614
2025-09-24 20:01:51,642 - INFO - bo.run_bo - üîçBO Trial 37: Using RF surrogate + Expected Improvement
2025-09-24 20:01:51,642 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 20:01:51,642 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:01:51,642 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:01:51,642 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002288334708030584, 'batch_size': 128, 'epochs': 28, 'weight_decay': 0.004529091518111991, 'dropout': 0.051063770620297336, 'base_channels': 17, 'loss_type': np.str_('class_balanced'), 'gamma': 4.580499368941425, 'label_smoothing': 0.08190923981068463, 'scheduler': np.str_('none'), 'patience': 10, 'use_amp': True, 'grad_clip': 0.8573624631486989, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9550954680015822}
2025-09-24 20:01:51,643 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002288334708030584, 'batch_size': 128, 'epochs': 28, 'weight_decay': 0.004529091518111991, 'dropout': 0.051063770620297336, 'base_channels': 17, 'loss_type': np.str_('class_balanced'), 'gamma': 4.580499368941425, 'label_smoothing': 0.08190923981068463, 'scheduler': np.str_('none'), 'patience': 10, 'use_amp': True, 'grad_clip': 0.8573624631486989, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9550954680015822}
2025-09-24 20:02:42,968 - INFO - _models.training_function_executor - Model: 64,517 parameters, 277.2KB storage
2025-09-24 20:02:42,968 - WARNING - _models.training_function_executor - Model storage 277.2KB exceeds 256KB limit!
2025-09-24 20:02:42,968 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6896970530382744, 0.47102632750896634, 0.4399128720802393, 0.4265508110151365, 0.4173124567016386, 0.4114847090850889, 0.4052109819064182, 0.3998472952467044, 0.39679489541508606, 0.3935562996429201, 0.39252510419638165, 0.3924593157568914, 0.3885653538351851, 0.38844941837598, 0.38566915130337404, 0.38515302222391884, 0.3830226489081029, 0.38409297897605615, 0.3851589911035871, 0.37988793947783095, 0.37941464501410405, 0.37981257661446566, 0.37817918645149223, 0.37844081329017776, 0.37816371063658366, 0.3776394293658384, 0.3758689763738455, 0.3742094458267921], 'val_losses': [0.4830051123901741, 0.44184629755231647, 0.43596658338402283, 0.42597153998305515, 0.42246089096939243, 0.4068225458023498, 0.39607853313882074, 0.39367777556498357, 0.3908158322949559, 0.39181124994501165, 0.38997879877169317, 0.3851644070416431, 0.39268775851322346, 0.4158500807653646, 0.3903035857058784, 0.3896012079400566, 0.38423252810722097, 0.3864927337464198, 0.39364084718281456, 0.3826582389149118, 0.38771743304507506, 0.388213639547018, 0.37645057538684606, 0.3866478287513476, 0.379278154895055, 0.3757297387921919, 0.37967721590077486, 0.3813474572193292], 'val_acc': [0.9463726350081444, 0.954141085077058, 0.9565217391304348, 0.9612830472371883, 0.9654178674351584, 0.9716827465229921, 0.9746898884851523, 0.9758175667209623, 0.9783235183560958, 0.9753163763939356, 0.9758175667209623, 0.9780729231925824, 0.9751910788121789, 0.9631625109635384, 0.9753163763939356, 0.976318757047989, 0.9800776845006891, 0.9779476256108257, 0.9764440546297456, 0.9792006014283924, 0.977822328029069, 0.9776970304473124, 0.9823330409723092, 0.9776970304473124, 0.9807041724094725, 0.9829595288810926, 0.977822328029069, 0.9807041724094725], 'quantized_model_size_bytes': 130534, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002288334708030584, 'batch_size': 128, 'epochs': 28, 'weight_decay': 0.004529091518111991, 'dropout': 0.051063770620297336, 'base_channels': 17, 'loss_type': np.str_('class_balanced'), 'gamma': 4.580499368941425, 'label_smoothing': 0.08190923981068463, 'scheduler': np.str_('none'), 'patience': 10, 'use_amp': True, 'grad_clip': 0.8573624631486989, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 2, 'beta_cb': 0.9550954680015822}, 'model_parameter_count': 64517, 'model_storage_size_kb': 277.22148437500005, 'model_size_validation': 'FAIL'}
2025-09-24 20:02:42,968 - INFO - _models.training_function_executor - BO Objective: base=0.9807, size_penalty=0.0414, final=0.9393
2025-09-24 20:02:42,968 - INFO - _models.training_function_executor - Model: 64,517 parameters, 277.2KB (FAIL 256KB limit)
2025-09-24 20:02:42,968 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 51.327s
2025-09-24 20:02:43,074 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9393
2025-09-24 20:02:43,074 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.106s
2025-09-24 20:02:43,074 - INFO - bo.run_bo - Recorded observation #37: hparams={'lr': 0.002288334708030584, 'batch_size': np.int64(128), 'epochs': np.int64(28), 'weight_decay': 0.004529091518111991, 'dropout': 0.051063770620297336, 'base_channels': np.int64(17), 'loss_type': np.str_('class_balanced'), 'gamma': 4.580499368941425, 'label_smoothing': 0.08190923981068463, 'scheduler': np.str_('none'), 'patience': np.int64(10), 'use_amp': np.True_, 'grad_clip': 0.8573624631486989, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9550954680015822}, value=0.9393
2025-09-24 20:02:43,074 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 37: {'lr': 0.002288334708030584, 'batch_size': np.int64(128), 'epochs': np.int64(28), 'weight_decay': 0.004529091518111991, 'dropout': 0.051063770620297336, 'base_channels': np.int64(17), 'loss_type': np.str_('class_balanced'), 'gamma': 4.580499368941425, 'label_smoothing': 0.08190923981068463, 'scheduler': np.str_('none'), 'patience': np.int64(10), 'use_amp': np.True_, 'grad_clip': 0.8573624631486989, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9550954680015822} -> 0.9393
2025-09-24 20:02:43,075 - INFO - bo.run_bo - üîçBO Trial 38: Using RF surrogate + Expected Improvement
2025-09-24 20:02:43,075 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:02:43,075 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:02:43,075 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:02:43,075 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0036865101702348822, 'batch_size': 32, 'epochs': 20, 'weight_decay': 3.674476197098944e-05, 'dropout': 0.05148138023781569, 'base_channels': 20, 'loss_type': np.str_('class_balanced'), 'gamma': 0.070263042455192, 'label_smoothing': 0.07198169417720948, 'scheduler': np.str_('none'), 'patience': 2, 'use_amp': True, 'grad_clip': 1.6846413223888697, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 7, 'beta_cb': 0.9798414534291829}
2025-09-24 20:02:43,077 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0036865101702348822, 'batch_size': 32, 'epochs': 20, 'weight_decay': 3.674476197098944e-05, 'dropout': 0.05148138023781569, 'base_channels': 20, 'loss_type': np.str_('class_balanced'), 'gamma': 0.070263042455192, 'label_smoothing': 0.07198169417720948, 'scheduler': np.str_('none'), 'patience': 2, 'use_amp': True, 'grad_clip': 1.6846413223888697, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 7, 'beta_cb': 0.9798414534291829}
2025-09-24 20:03:42,995 - INFO - _models.training_function_executor - Model: 88,853 parameters, 381.8KB storage
2025-09-24 20:03:42,995 - WARNING - _models.training_function_executor - Model storage 381.8KB exceeds 256KB limit!
2025-09-24 20:03:42,995 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6111195800455501, 0.44466811809138584, 0.4216948925563799, 0.40894205392357386, 0.39901886303965634, 0.39149606403945864, 0.3832500746555766, 0.37763746034843976, 0.372622336406582, 0.3706078271515495, 0.36860664640569196, 0.36663249870202963, 0.36598823512141593, 0.36163650399550484, 0.3596794494431154, 0.3589037461425414, 0.3570788943393747, 0.35659544411743843, 0.3549384005551902, 0.35452290109680196], 'val_losses': [0.5171601130555796, 0.43487190153198185, 0.4003529342146031, 0.3947472073592394, 0.39462780225786287, 0.3714350202659843, 0.37906840077290155, 0.3706110753506411, 0.3770029786878504, 0.3671650170174596, 0.378006359088273, 0.3599360696789376, 0.38479523497586143, 0.35379065469219156, 0.358454395073904, 0.3543058507291017, 0.3552966038395985, 0.351379268944017, 0.3582466948138429, 0.3494390112096662], 'val_acc': [0.92356847512843, 0.9505074552061146, 0.9616589399824583, 0.962160130309485, 0.9645407843628618, 0.9731863175040721, 0.9671720335797519, 0.9724345320135321, 0.969427390051372, 0.9734369126675855, 0.969427390051372, 0.9756922691392056, 0.9652925698534018, 0.9794511965919058, 0.9790753038466358, 0.9804535772459592, 0.977822328029069, 0.9804535772459592, 0.9793258990101491, 0.9820824458087959], 'quantized_model_size_bytes': 358844, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0036865101702348822, 'batch_size': 32, 'epochs': 20, 'weight_decay': 3.674476197098944e-05, 'dropout': 0.05148138023781569, 'base_channels': 20, 'loss_type': np.str_('class_balanced'), 'gamma': 0.070263042455192, 'label_smoothing': 0.07198169417720948, 'scheduler': np.str_('none'), 'patience': 2, 'use_amp': True, 'grad_clip': 1.6846413223888697, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 7, 'beta_cb': 0.9798414534291829}, 'model_parameter_count': 88853, 'model_storage_size_kb': 381.790234375, 'model_size_validation': 'FAIL'}
2025-09-24 20:03:42,995 - INFO - _models.training_function_executor - BO Objective: base=0.9821, size_penalty=0.2457, final=0.7364
2025-09-24 20:03:42,995 - INFO - _models.training_function_executor - Model: 88,853 parameters, 381.8KB (FAIL 256KB limit)
2025-09-24 20:03:42,995 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 59.920s
2025-09-24 20:03:43,102 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7364
2025-09-24 20:03:43,102 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.107s
2025-09-24 20:03:43,102 - INFO - bo.run_bo - Recorded observation #38: hparams={'lr': 0.0036865101702348822, 'batch_size': np.int64(32), 'epochs': np.int64(20), 'weight_decay': 3.674476197098944e-05, 'dropout': 0.05148138023781569, 'base_channels': np.int64(20), 'loss_type': np.str_('class_balanced'), 'gamma': 0.070263042455192, 'label_smoothing': 0.07198169417720948, 'scheduler': np.str_('none'), 'patience': np.int64(2), 'use_amp': np.True_, 'grad_clip': 1.6846413223888697, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(7), 'beta_cb': 0.9798414534291829}, value=0.7364
2025-09-24 20:03:43,102 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 38: {'lr': 0.0036865101702348822, 'batch_size': np.int64(32), 'epochs': np.int64(20), 'weight_decay': 3.674476197098944e-05, 'dropout': 0.05148138023781569, 'base_channels': np.int64(20), 'loss_type': np.str_('class_balanced'), 'gamma': 0.070263042455192, 'label_smoothing': 0.07198169417720948, 'scheduler': np.str_('none'), 'patience': np.int64(2), 'use_amp': np.True_, 'grad_clip': 1.6846413223888697, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(7), 'beta_cb': 0.9798414534291829} -> 0.7364
2025-09-24 20:03:43,103 - INFO - bo.run_bo - üîçBO Trial 39: Using RF surrogate + Expected Improvement
2025-09-24 20:03:43,103 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 20:03:43,103 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:03:43,103 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:03:43,103 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0063307045731232844, 'batch_size': 256, 'epochs': 33, 'weight_decay': 0.0008208267011067495, 'dropout': 0.12438211606852104, 'base_channels': 9, 'loss_type': np.str_('class_balanced'), 'gamma': 3.5677568237651247, 'label_smoothing': 0.035656656335074165, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': True, 'grad_clip': 1.0963074725678161, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.940179219278624}
2025-09-24 20:03:43,104 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0063307045731232844, 'batch_size': 256, 'epochs': 33, 'weight_decay': 0.0008208267011067495, 'dropout': 0.12438211606852104, 'base_channels': 9, 'loss_type': np.str_('class_balanced'), 'gamma': 3.5677568237651247, 'label_smoothing': 0.035656656335074165, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': True, 'grad_clip': 1.0963074725678161, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.940179219278624}
2025-09-24 20:04:20,746 - INFO - _models.training_function_executor - Model: 18,629 parameters, 40.0KB storage
2025-09-24 20:04:20,746 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7242582323866874, 0.40857293427548785, 0.3445867808529919, 0.32255470181651924, 0.3092082323167414, 0.30601557361725984, 0.2989718510674171, 0.29199732053669364, 0.28778263660812314, 0.28191342252397866, 0.27828939873947484, 0.27212327585981233, 0.27024209424300183, 0.27285276616205567, 0.2658283747449068, 0.26302695215491506, 0.2615647383415359, 0.2574369506219894, 0.2570305565386171, 0.2549118509426439, 0.2533639560722899, 0.2521914117022146, 0.25178404924781633, 0.2477571781102351, 0.2431281440246375, 0.24596563559484724, 0.24744893053743588, 0.24280250176012047, 0.24295970895376787, 0.24190003684158332, 0.24029952578387329, 0.24059363103721748, 0.2408263551303103], 'val_losses': [0.49624372679866685, 0.3327377158259258, 0.3798124961664228, 0.3063340603524828, 0.291837746954968, 0.28427658309883946, 0.28251019005577693, 0.2706250638990769, 0.26803836942675, 0.27003175053078493, 0.2647602229866075, 0.26839294897077554, 0.24534105251955965, 0.25601977795874054, 0.2646039081477175, 0.24820206066283826, 0.25560180375359737, 0.26434877048442723, 0.2415763786425045, 0.24427284359887136, 0.24279434198336022, 0.23520856016546454, 0.24183027890074776, 0.2316749290653136, 0.22924044834329227, 0.23624183771163954, 0.2323009618510729, 0.2365864550443001, 0.23436243090694103, 0.2282657786529773, 0.23009326420336973, 0.24196895294179893, 0.2320989125273219], 'val_acc': [0.893747650670342, 0.9439919809547676, 0.9223154993108633, 0.9515098358601679, 0.9560205488034081, 0.9595288810925949, 0.9601553690013783, 0.9646660819446184, 0.9619095351459717, 0.9642901891993485, 0.9642901891993485, 0.9636637012905651, 0.9751910788121789, 0.9662949505074552, 0.967547926325022, 0.9719333416865055, 0.9711815561959655, 0.9681744142338052, 0.9746898884851523, 0.9731863175040721, 0.9704297707054254, 0.9786994111013657, 0.9731863175040721, 0.9800776845006891, 0.9790753038466358, 0.976694649793259, 0.9764440546297456, 0.9749404836486656, 0.9769452449567724, 0.9794511965919058, 0.9784488159378524, 0.9746898884851523, 0.9785741135196091], 'quantized_model_size_bytes': 38086, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0063307045731232844, 'batch_size': 256, 'epochs': 33, 'weight_decay': 0.0008208267011067495, 'dropout': 0.12438211606852104, 'base_channels': 9, 'loss_type': np.str_('class_balanced'), 'gamma': 3.5677568237651247, 'label_smoothing': 0.035656656335074165, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': True, 'grad_clip': 1.0963074725678161, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.940179219278624}, 'model_parameter_count': 18629, 'model_storage_size_kb': 40.0232421875, 'model_size_validation': 'PASS'}
2025-09-24 20:04:20,746 - INFO - _models.training_function_executor - BO Objective: base=0.9786, size_penalty=0.0000, final=0.9786
2025-09-24 20:04:20,746 - INFO - _models.training_function_executor - Model: 18,629 parameters, 40.0KB (PASS 256KB limit)
2025-09-24 20:04:20,746 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 37.643s
2025-09-24 20:04:20,853 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9786
2025-09-24 20:04:20,853 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.107s
2025-09-24 20:04:20,853 - INFO - bo.run_bo - Recorded observation #39: hparams={'lr': 0.0063307045731232844, 'batch_size': np.int64(256), 'epochs': np.int64(33), 'weight_decay': 0.0008208267011067495, 'dropout': 0.12438211606852104, 'base_channels': np.int64(9), 'loss_type': np.str_('class_balanced'), 'gamma': 3.5677568237651247, 'label_smoothing': 0.035656656335074165, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.True_, 'grad_clip': 1.0963074725678161, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6), 'beta_cb': 0.940179219278624}, value=0.9786
2025-09-24 20:04:20,853 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 39: {'lr': 0.0063307045731232844, 'batch_size': np.int64(256), 'epochs': np.int64(33), 'weight_decay': 0.0008208267011067495, 'dropout': 0.12438211606852104, 'base_channels': np.int64(9), 'loss_type': np.str_('class_balanced'), 'gamma': 3.5677568237651247, 'label_smoothing': 0.035656656335074165, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.True_, 'grad_clip': 1.0963074725678161, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6), 'beta_cb': 0.940179219278624} -> 0.9786
2025-09-24 20:04:20,853 - INFO - bo.run_bo - üîçBO Trial 40: Using RF surrogate + Expected Improvement
2025-09-24 20:04:20,853 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-24 20:04:20,853 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:04:20,853 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:04:20,853 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005719977928443852, 'batch_size': 64, 'epochs': 23, 'weight_decay': 0.0007997358374677225, 'dropout': 0.13795708533139303, 'base_channels': 13, 'loss_type': np.str_('class_balanced'), 'gamma': 1.197963660217767, 'label_smoothing': 0.023747258256101782, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': True, 'grad_clip': 1.6838408230755348, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9405031292440644}
2025-09-24 20:04:20,855 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005719977928443852, 'batch_size': 64, 'epochs': 23, 'weight_decay': 0.0007997358374677225, 'dropout': 0.13795708533139303, 'base_channels': 13, 'loss_type': np.str_('class_balanced'), 'gamma': 1.197963660217767, 'label_smoothing': 0.023747258256101782, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': True, 'grad_clip': 1.6838408230755348, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9405031292440644}
2025-09-24 20:05:03,871 - INFO - _models.training_function_executor - Model: 38,117 parameters, 163.8KB storage
2025-09-24 20:05:03,871 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6712133884653988, 0.35930930172460135, 0.3035860393398579, 0.28106119990240525, 0.2652316425739365, 0.2500790492229039, 0.24280712245602776, 0.238936961643813, 0.23409924244565503, 0.2258456454894165, 0.2243594205088586, 0.2213998679291637, 0.2164060008286668, 0.2175593939251131, 0.2089040723347154, 0.2083176263154003, 0.20923954763011562, 0.2063358748492765, 0.2047176438061414, 0.19916099426337316, 0.20160644253841847, 0.20123657946881915, 0.19940000461130944], 'val_losses': [0.4216011575836389, 0.3065037784936688, 0.2653451268049488, 0.2533733490740143, 0.26047216752555014, 0.2266468691536096, 0.24902514029140685, 0.21795015379108743, 0.21260170869145562, 0.2232114774285963, 0.20701510386275193, 0.20754880642864043, 0.21218580967045236, 0.2110358737003025, 0.1987716875687501, 0.19746459452389864, 0.18816127435312044, 0.1901976427268958, 0.18936451540833024, 0.19158043942121677, 0.19650479263566875, 0.1904085034334395, 0.18548794496650908], 'val_acc': [0.8854780102744017, 0.9362235308858539, 0.9486279914797644, 0.9557699536398947, 0.9507580503696279, 0.962160130309485, 0.9604059641648917, 0.9676732239067786, 0.9688009021425886, 0.9656684625986719, 0.9724345320135321, 0.969051497306102, 0.9680491166520486, 0.970179175541912, 0.9735622102493422, 0.9743139957398822, 0.9758175667209623, 0.978198220774339, 0.977446435283799, 0.9769452449567724, 0.9753163763939356, 0.9761934594662323, 0.9779476256108257], 'quantized_model_size_bytes': 154724, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005719977928443852, 'batch_size': 64, 'epochs': 23, 'weight_decay': 0.0007997358374677225, 'dropout': 0.13795708533139303, 'base_channels': 13, 'loss_type': np.str_('class_balanced'), 'gamma': 1.197963660217767, 'label_smoothing': 0.023747258256101782, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': True, 'grad_clip': 1.6838408230755348, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibration_batches': 2, 'beta_cb': 0.9405031292440644}, 'model_parameter_count': 38117, 'model_storage_size_kb': 163.78398437500002, 'model_size_validation': 'PASS'}
2025-09-24 20:05:03,871 - INFO - _models.training_function_executor - BO Objective: base=0.9779, size_penalty=0.0000, final=0.9779
2025-09-24 20:05:03,871 - INFO - _models.training_function_executor - Model: 38,117 parameters, 163.8KB (PASS 256KB limit)
2025-09-24 20:05:03,871 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 43.018s
2025-09-24 20:05:03,978 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9779
2025-09-24 20:05:03,978 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.107s
2025-09-24 20:05:03,978 - INFO - bo.run_bo - Recorded observation #40: hparams={'lr': 0.005719977928443852, 'batch_size': np.int64(64), 'epochs': np.int64(23), 'weight_decay': 0.0007997358374677225, 'dropout': 0.13795708533139303, 'base_channels': np.int64(13), 'loss_type': np.str_('class_balanced'), 'gamma': 1.197963660217767, 'label_smoothing': 0.023747258256101782, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.True_, 'grad_clip': 1.6838408230755348, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9405031292440644}, value=0.9779
2025-09-24 20:05:03,978 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 40: {'lr': 0.005719977928443852, 'batch_size': np.int64(64), 'epochs': np.int64(23), 'weight_decay': 0.0007997358374677225, 'dropout': 0.13795708533139303, 'base_channels': np.int64(13), 'loss_type': np.str_('class_balanced'), 'gamma': 1.197963660217767, 'label_smoothing': 0.023747258256101782, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.True_, 'grad_clip': 1.6838408230755348, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(2), 'beta_cb': 0.9405031292440644} -> 0.9779
2025-09-24 20:05:03,979 - INFO - bo.run_bo - üîçBO Trial 41: Using RF surrogate + Expected Improvement
2025-09-24 20:05:03,979 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:05:03,979 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:05:03,979 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:05:03,979 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0056277963285182335, 'batch_size': 256, 'epochs': 33, 'weight_decay': 0.00014635921555395472, 'dropout': 0.20248883601834938, 'base_channels': 11, 'loss_type': np.str_('class_balanced'), 'gamma': 1.3789649326384488, 'label_smoothing': 0.0005051883238813671, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.334215604393964, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9101713245076372}
2025-09-24 20:05:03,981 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0056277963285182335, 'batch_size': 256, 'epochs': 33, 'weight_decay': 0.00014635921555395472, 'dropout': 0.20248883601834938, 'base_channels': 11, 'loss_type': np.str_('class_balanced'), 'gamma': 1.3789649326384488, 'label_smoothing': 0.0005051883238813671, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.334215604393964, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9101713245076372}
2025-09-24 20:05:47,671 - INFO - _models.training_function_executor - Model: 22,924 parameters, 24.6KB storage
2025-09-24 20:05:47,671 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.2701619222204261, 0.7497667887528782, 0.46446954973605187, 0.2691745228461633, 0.22209252906371055, 0.20216639283956397, 0.19300107759341661, 0.1758334940168167, 0.172671796438748, 0.15687840603728664, 0.1534925347110463, 0.1409720312271266, 0.13050636277477967, 0.12388103666571759, 0.11942039808096848, 0.11256759870437155, 0.10769813382347003, 0.10437795655383011, 0.10025923878059043, 0.0931707822878446, 0.09262942074511063, 0.0888521781729374, 0.08446406403065934, 0.08561652217749885, 0.08014710745832411, 0.07640623333335718, 0.07578686552460479, 0.07254126842413743, 0.07065927305177735, 0.07067067367158483, 0.06606711366800286, 0.06798809083443598, 0.0665669921247974], 'val_losses': [0.853727449317457, 0.6231893036183581, 0.34129799432028446, 0.22813679502039771, 0.18211954936446767, 0.21773646817464545, 0.1725167518724566, 0.15127015434408111, 0.1504311835373005, 0.1561158146298913, 0.1280783974255047, 0.13466268795584546, 0.10384392167109323, 0.10329916597704678, 0.10262193311882892, 0.10063790434185178, 0.09317579847998435, 0.09429984894772397, 0.08537563635298907, 0.0768085883867545, 0.07471067796705988, 0.07165106738170815, 0.07525786173820137, 0.08334848396559434, 0.06632102621037057, 0.0667843844327558, 0.06672692951340078, 0.061865741909558486, 0.0616538669681089, 0.06155021335005895, 0.06220593302321754, 0.06087214994581402, 0.061089723239923656], 'val_acc': [0.7200852023555946, 0.7598045357724595, 0.908658062899386, 0.9359729357223405, 0.9437413857912542, 0.9423631123919308, 0.9521363237689513, 0.9535145971682747, 0.9561458463851648, 0.9575241197844881, 0.9659190577621852, 0.9607818569101616, 0.969051497306102, 0.9684250093973187, 0.9715574489412354, 0.9711815561959655, 0.9729357223405588, 0.9703044731236687, 0.9730610199223155, 0.9756922691392056, 0.9761934594662323, 0.9780729231925824, 0.9771958401202857, 0.9745645909033955, 0.9792006014283924, 0.9793258990101491, 0.9785741135196091, 0.9822077433905526, 0.9804535772459592, 0.9810800651547426, 0.9800776845006891, 0.9812053627364992, 0.9809547675729858], 'quantized_model_size_bytes': 93664, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0056277963285182335, 'batch_size': 256, 'epochs': 33, 'weight_decay': 0.00014635921555395472, 'dropout': 0.20248883601834938, 'base_channels': 11, 'loss_type': np.str_('class_balanced'), 'gamma': 1.3789649326384488, 'label_smoothing': 0.0005051883238813671, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': False, 'grad_clip': 1.334215604393964, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9101713245076372}, 'model_parameter_count': 22924, 'model_storage_size_kb': 24.625390625, 'model_size_validation': 'PASS'}
2025-09-24 20:05:47,671 - INFO - _models.training_function_executor - BO Objective: base=0.9810, size_penalty=0.0000, final=0.9810
2025-09-24 20:05:47,671 - INFO - _models.training_function_executor - Model: 22,924 parameters, 24.6KB (PASS 256KB limit)
2025-09-24 20:05:47,671 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 43.692s
2025-09-24 20:05:47,781 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9810
2025-09-24 20:05:47,782 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.110s
2025-09-24 20:05:47,782 - INFO - bo.run_bo - Recorded observation #41: hparams={'lr': 0.0056277963285182335, 'batch_size': np.int64(256), 'epochs': np.int64(33), 'weight_decay': 0.00014635921555395472, 'dropout': 0.20248883601834938, 'base_channels': np.int64(11), 'loss_type': np.str_('class_balanced'), 'gamma': 1.3789649326384488, 'label_smoothing': 0.0005051883238813671, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.334215604393964, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9101713245076372}, value=0.9810
2025-09-24 20:05:47,782 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 41: {'lr': 0.0056277963285182335, 'batch_size': np.int64(256), 'epochs': np.int64(33), 'weight_decay': 0.00014635921555395472, 'dropout': 0.20248883601834938, 'base_channels': np.int64(11), 'loss_type': np.str_('class_balanced'), 'gamma': 1.3789649326384488, 'label_smoothing': 0.0005051883238813671, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.False_, 'grad_clip': 1.334215604393964, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9101713245076372} -> 0.9810
2025-09-24 20:05:47,782 - INFO - bo.run_bo - üîçBO Trial 42: Using RF surrogate + Expected Improvement
2025-09-24 20:05:47,782 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:05:47,782 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:05:47,782 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:05:47,782 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.006015930863223933, 'batch_size': 128, 'epochs': 34, 'weight_decay': 0.0018868133898932318, 'dropout': 0.15901875379845312, 'base_channels': 19, 'loss_type': np.str_('class_balanced'), 'gamma': 1.6399613865909526, 'label_smoothing': 0.035511785738708286, 'scheduler': np.str_('none'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.163172781801481, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9271142653403739}
2025-09-24 20:05:47,784 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.006015930863223933, 'batch_size': 128, 'epochs': 34, 'weight_decay': 0.0018868133898932318, 'dropout': 0.15901875379845312, 'base_channels': 19, 'loss_type': np.str_('class_balanced'), 'gamma': 1.6399613865909526, 'label_smoothing': 0.035511785738708286, 'scheduler': np.str_('none'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.163172781801481, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9271142653403739}
2025-09-24 20:07:05,693 - INFO - _models.training_function_executor - Model: 80,309 parameters, 345.1KB storage
2025-09-24 20:07:05,693 - WARNING - _models.training_function_executor - Model storage 345.1KB exceeds 256KB limit!
2025-09-24 20:07:05,693 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6359256669732856, 0.3630010396698657, 0.3252783144362061, 0.3025454119516749, 0.29419462544627606, 0.2871089502737284, 0.2783402408330328, 0.27567110513623594, 0.2647045786365532, 0.2602387263596523, 0.2605694625135151, 0.25828232194189626, 0.252403281302103, 0.25253125054194797, 0.24858079915323328, 0.2477613619383395, 0.24991082017576993, 0.24524573968085148, 0.24163426959368609, 0.2403525920924442, 0.23935090426017297, 0.23878453125094193, 0.24197224218236008, 0.23851845851687042, 0.2356219441286772, 0.23599895974397994, 0.23488183024327447, 0.2335052554146507, 0.23212350281270805, 0.23250280303980173, 0.22985274351614443, 0.22988972755406498, 0.23219376219383955, 0.2288481264123641], 'val_losses': [0.3700042014812978, 0.3387678280414876, 0.2917187097205239, 0.2769049624761324, 0.287425251713807, 0.2681767741602594, 0.2563969958356532, 0.2596031918761396, 0.24855356500749526, 0.24421047213650812, 0.26009978040121146, 0.24571385346899174, 0.2581873627708843, 0.23888478707347116, 0.25028783272283894, 0.2555576848395261, 0.23595901546084244, 0.2399334601629439, 0.23196594107159993, 0.25622716278692875, 0.2273816198144188, 0.23153362622678975, 0.23304018583263075, 0.231982222241636, 0.2286831242908826, 0.22636567430155063, 0.2280727234449531, 0.22120493182919704, 0.2264956369157752, 0.22592756410183396, 0.22328826808426078, 0.2243988154944494, 0.23222795822629055, 0.2278112422298211], 'val_acc': [0.9330910913419371, 0.9437413857912542, 0.9595288810925949, 0.9615336424007017, 0.9611577496554317, 0.968299711815562, 0.9669214384162386, 0.9681744142338052, 0.9700538779601554, 0.9731863175040721, 0.9665455456709685, 0.9713068537777221, 0.9667961408344818, 0.9756922691392056, 0.970930961032452, 0.9689261997243453, 0.9756922691392056, 0.9771958401202857, 0.9771958401202857, 0.970179175541912, 0.9788247086831224, 0.9768199473750157, 0.9764440546297456, 0.9776970304473124, 0.977070542538529, 0.9819571482270392, 0.9790753038466358, 0.982458338554066, 0.9790753038466358, 0.9804535772459592, 0.9814559579000125, 0.9788247086831224, 0.9771958401202857, 0.9790753038466358], 'quantized_model_size_bytes': 324500, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.006015930863223933, 'batch_size': 128, 'epochs': 34, 'weight_decay': 0.0018868133898932318, 'dropout': 0.15901875379845312, 'base_channels': 19, 'loss_type': np.str_('class_balanced'), 'gamma': 1.6399613865909526, 'label_smoothing': 0.035511785738708286, 'scheduler': np.str_('none'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.163172781801481, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 1, 'beta_cb': 0.9271142653403739}, 'model_parameter_count': 80309, 'model_storage_size_kb': 345.07773437500003, 'model_size_validation': 'FAIL'}
2025-09-24 20:07:05,693 - INFO - _models.training_function_executor - BO Objective: base=0.9791, size_penalty=0.1740, final=0.8051
2025-09-24 20:07:05,693 - INFO - _models.training_function_executor - Model: 80,309 parameters, 345.1KB (FAIL 256KB limit)
2025-09-24 20:07:05,693 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 77.911s
2025-09-24 20:07:05,801 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8051
2025-09-24 20:07:05,801 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.108s
2025-09-24 20:07:05,801 - INFO - bo.run_bo - Recorded observation #42: hparams={'lr': 0.006015930863223933, 'batch_size': np.int64(128), 'epochs': np.int64(34), 'weight_decay': 0.0018868133898932318, 'dropout': 0.15901875379845312, 'base_channels': np.int64(19), 'loss_type': np.str_('class_balanced'), 'gamma': 1.6399613865909526, 'label_smoothing': 0.035511785738708286, 'scheduler': np.str_('none'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 1.163172781801481, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9271142653403739}, value=0.8051
2025-09-24 20:07:05,801 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 42: {'lr': 0.006015930863223933, 'batch_size': np.int64(128), 'epochs': np.int64(34), 'weight_decay': 0.0018868133898932318, 'dropout': 0.15901875379845312, 'base_channels': np.int64(19), 'loss_type': np.str_('class_balanced'), 'gamma': 1.6399613865909526, 'label_smoothing': 0.035511785738708286, 'scheduler': np.str_('none'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 1.163172781801481, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(1), 'beta_cb': 0.9271142653403739} -> 0.8051
2025-09-24 20:07:05,802 - INFO - bo.run_bo - üîçBO Trial 43: Using RF surrogate + Expected Improvement
2025-09-24 20:07:05,802 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:07:05,802 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:07:05,802 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:07:05,802 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0020798909576863647, 'batch_size': 256, 'epochs': 27, 'weight_decay': 0.004673274950756892, 'dropout': 0.4794929324088386, 'base_channels': 11, 'loss_type': np.str_('class_balanced'), 'gamma': 4.298534189666461, 'label_smoothing': 0.005786040287896167, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': True, 'grad_clip': 1.1938332588279754, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9417315677720657}
2025-09-24 20:07:05,803 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0020798909576863647, 'batch_size': 256, 'epochs': 27, 'weight_decay': 0.004673274950756892, 'dropout': 0.4794929324088386, 'base_channels': 11, 'loss_type': np.str_('class_balanced'), 'gamma': 4.298534189666461, 'label_smoothing': 0.005786040287896167, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': True, 'grad_clip': 1.1938332588279754, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9417315677720657}
2025-09-24 20:07:38,243 - INFO - _models.training_function_executor - Model: 27,509 parameters, 118.2KB storage
2025-09-24 20:07:38,243 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8928710692968411, 0.6177980707495582, 0.40665030802539376, 0.317858431495394, 0.28141952438342244, 0.26580378738082383, 0.2519999519193568, 0.24507505738809512, 0.23063978574990562, 0.23368107402419527, 0.22389478338966057, 0.2178926642323535, 0.20826971583567597, 0.20381059256804054, 0.2053845780193703, 0.20232263284548405, 0.1998024420403181, 0.19835891311471446, 0.19142419119366008, 0.18955041848479465, 0.18846051288932666, 0.18600652416705846, 0.182197986711361, 0.17883355957977126, 0.18348153064850894, 0.177220571537019, 0.17441734695538758], 'val_losses': [0.8502499823611477, 0.47829048793920226, 0.2828671199186391, 0.24894046316864107, 0.22801075860262365, 0.21833347363656302, 0.30259375148713624, 0.20089432732897017, 0.19275382733960422, 0.2788170453073715, 0.18562222996864564, 0.16670722245344796, 0.1760941987952677, 0.16428946299729769, 0.2133679000760899, 0.15910577620760627, 0.16618243672580083, 0.16365530526051023, 0.15253506348147575, 0.15476893666931418, 0.15129623528396766, 0.14465435341237676, 0.19435480277709474, 0.14491887116712998, 0.14664210220421933, 0.13957264035465933, 0.14269240679798859], 'val_acc': [0.699661696529257, 0.8661821826838741, 0.9314622227791004, 0.9429896003007142, 0.9443678737000376, 0.9448690640270643, 0.9180553815311364, 0.945746147099361, 0.9553940608946248, 0.930459842125047, 0.9561458463851648, 0.9594035835108382, 0.9611577496554317, 0.9609071544919183, 0.952637514095978, 0.9629119158000251, 0.9607818569101616, 0.9640395940358351, 0.9635384037088084, 0.9650419746898885, 0.9657937601804285, 0.9670467359979953, 0.9547675729858414, 0.970930961032452, 0.9676732239067786, 0.9689261997243453, 0.9689261997243453], 'quantized_model_size_bytes': 111956, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0020798909576863647, 'batch_size': 256, 'epochs': 27, 'weight_decay': 0.004673274950756892, 'dropout': 0.4794929324088386, 'base_channels': 11, 'loss_type': np.str_('class_balanced'), 'gamma': 4.298534189666461, 'label_smoothing': 0.005786040287896167, 'scheduler': np.str_('plateau'), 'patience': 8, 'use_amp': True, 'grad_clip': 1.1938332588279754, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9417315677720657}, 'model_parameter_count': 27509, 'model_storage_size_kb': 118.202734375, 'model_size_validation': 'PASS'}
2025-09-24 20:07:38,243 - INFO - _models.training_function_executor - BO Objective: base=0.9689, size_penalty=0.0000, final=0.9689
2025-09-24 20:07:38,244 - INFO - _models.training_function_executor - Model: 27,509 parameters, 118.2KB (PASS 256KB limit)
2025-09-24 20:07:38,244 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 32.442s
2025-09-24 20:07:38,354 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9689
2025-09-24 20:07:38,354 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.110s
2025-09-24 20:07:38,354 - INFO - bo.run_bo - Recorded observation #43: hparams={'lr': 0.0020798909576863647, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'weight_decay': 0.004673274950756892, 'dropout': 0.4794929324088386, 'base_channels': np.int64(11), 'loss_type': np.str_('class_balanced'), 'gamma': 4.298534189666461, 'label_smoothing': 0.005786040287896167, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.True_, 'grad_clip': 1.1938332588279754, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9417315677720657}, value=0.9689
2025-09-24 20:07:38,354 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 43: {'lr': 0.0020798909576863647, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'weight_decay': 0.004673274950756892, 'dropout': 0.4794929324088386, 'base_channels': np.int64(11), 'loss_type': np.str_('class_balanced'), 'gamma': 4.298534189666461, 'label_smoothing': 0.005786040287896167, 'scheduler': np.str_('plateau'), 'patience': np.int64(8), 'use_amp': np.True_, 'grad_clip': 1.1938332588279754, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9417315677720657} -> 0.9689
2025-09-24 20:07:38,354 - INFO - bo.run_bo - üîçBO Trial 44: Using RF surrogate + Expected Improvement
2025-09-24 20:07:38,354 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:07:38,354 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:07:38,355 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:07:38,355 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003355284403900242, 'batch_size': 64, 'epochs': 24, 'weight_decay': 0.0006410605192010394, 'dropout': 0.35674891891500343, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 3.19670226853381, 'label_smoothing': 0.05516462616421797, 'scheduler': np.str_('plateau'), 'patience': 5, 'use_amp': True, 'grad_clip': 1.4022521601278757, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9623373425352183}
2025-09-24 20:07:38,356 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003355284403900242, 'batch_size': 64, 'epochs': 24, 'weight_decay': 0.0006410605192010394, 'dropout': 0.35674891891500343, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 3.19670226853381, 'label_smoothing': 0.05516462616421797, 'scheduler': np.str_('plateau'), 'patience': 5, 'use_amp': True, 'grad_clip': 1.4022521601278757, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9623373425352183}
2025-09-24 20:08:25,140 - INFO - _models.training_function_executor - Model: 50,453 parameters, 216.8KB storage
2025-09-24 20:08:25,141 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8603552558519703, 0.5226293076206089, 0.46476445611013983, 0.4395171102668858, 0.4221345807112903, 0.4088865619414067, 0.4019405649680226, 0.3939342534581667, 0.38671757189756917, 0.38152216691774105, 0.3770556090242025, 0.37133919483206085, 0.36692578094803896, 0.36249686869314857, 0.3594590439569553, 0.3548849183836516, 0.3518854804586521, 0.34897322347934684, 0.3460442002967602, 0.34254752184795534, 0.34089688814134855, 0.33876718138257794, 0.33755509015036766, 0.333489932455231], 'val_losses': [0.5918301071921711, 0.4321377313446243, 0.4175789947759805, 0.39359433664637483, 0.3884701955633614, 0.37473013916793524, 0.3660300210586931, 0.36191298437438174, 0.3569489563636173, 0.3504695307153939, 0.344578685979021, 0.3407981750578558, 0.33889845391515355, 0.33442024364759415, 0.3369672094957077, 0.32551598822229594, 0.32425495734595366, 0.3225176985058296, 0.3167051433463874, 0.31713791174960904, 0.31503008531756366, 0.3128997605705213, 0.31087110968942167, 0.31520473693056733], 'val_acc': [0.8956271143966922, 0.9344693647412605, 0.9437413857912542, 0.9458714446811176, 0.9488785866432777, 0.9548928705675981, 0.9567723342939481, 0.9566470367121914, 0.9586517980202982, 0.9591529883473249, 0.9619095351459717, 0.9631625109635384, 0.9656684625986719, 0.9671720335797519, 0.9629119158000251, 0.9676732239067786, 0.968299711815562, 0.9706803658689387, 0.9708056634506954, 0.9736875078310988, 0.9715574489412354, 0.9756922691392056, 0.9739381029946123, 0.9738128054128555], 'quantized_model_size_bytes': 102238, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003355284403900242, 'batch_size': 64, 'epochs': 24, 'weight_decay': 0.0006410605192010394, 'dropout': 0.35674891891500343, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 3.19670226853381, 'label_smoothing': 0.05516462616421797, 'scheduler': np.str_('plateau'), 'patience': 5, 'use_amp': True, 'grad_clip': 1.4022521601278757, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 5, 'beta_cb': 0.9623373425352183}, 'model_parameter_count': 50453, 'model_storage_size_kb': 216.790234375, 'model_size_validation': 'PASS'}
2025-09-24 20:08:25,141 - INFO - _models.training_function_executor - BO Objective: base=0.9738, size_penalty=0.0000, final=0.9738
2025-09-24 20:08:25,141 - INFO - _models.training_function_executor - Model: 50,453 parameters, 216.8KB (PASS 256KB limit)
2025-09-24 20:08:25,141 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 46.786s
2025-09-24 20:08:25,253 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9738
2025-09-24 20:08:25,253 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.112s
2025-09-24 20:08:25,253 - INFO - bo.run_bo - Recorded observation #44: hparams={'lr': 0.0003355284403900242, 'batch_size': np.int64(64), 'epochs': np.int64(24), 'weight_decay': 0.0006410605192010394, 'dropout': 0.35674891891500343, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 3.19670226853381, 'label_smoothing': 0.05516462616421797, 'scheduler': np.str_('plateau'), 'patience': np.int64(5), 'use_amp': np.True_, 'grad_clip': 1.4022521601278757, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9623373425352183}, value=0.9738
2025-09-24 20:08:25,253 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 44: {'lr': 0.0003355284403900242, 'batch_size': np.int64(64), 'epochs': np.int64(24), 'weight_decay': 0.0006410605192010394, 'dropout': 0.35674891891500343, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 3.19670226853381, 'label_smoothing': 0.05516462616421797, 'scheduler': np.str_('plateau'), 'patience': np.int64(5), 'use_amp': np.True_, 'grad_clip': 1.4022521601278757, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(5), 'beta_cb': 0.9623373425352183} -> 0.9738
2025-09-24 20:08:25,254 - INFO - bo.run_bo - üîçBO Trial 45: Using RF surrogate + Expected Improvement
2025-09-24 20:08:25,254 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:08:25,254 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:08:25,254 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:08:25,254 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004755117209830757, 'batch_size': 64, 'epochs': 26, 'weight_decay': 6.376317577341345e-05, 'dropout': 0.04032817198921474, 'base_channels': 18, 'loss_type': np.str_('class_balanced'), 'gamma': 3.8826977537993166, 'label_smoothing': 0.06396717138699758, 'scheduler': np.str_('none'), 'patience': 4, 'use_amp': True, 'grad_clip': 1.3719285299043829, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9522306992356935}
2025-09-24 20:08:25,256 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004755117209830757, 'batch_size': 64, 'epochs': 26, 'weight_decay': 6.376317577341345e-05, 'dropout': 0.04032817198921474, 'base_channels': 18, 'loss_type': np.str_('class_balanced'), 'gamma': 3.8826977537993166, 'label_smoothing': 0.06396717138699758, 'scheduler': np.str_('none'), 'patience': 4, 'use_amp': True, 'grad_clip': 1.3719285299043829, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9522306992356935}
2025-09-24 20:09:21,284 - INFO - _models.training_function_executor - Model: 72,197 parameters, 155.1KB storage
2025-09-24 20:09:21,284 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7497394034704409, 0.5084540328950963, 0.42027231241584745, 0.3974239792537895, 0.3785593639645545, 0.3670632302736805, 0.35984639590557954, 0.3564761195433221, 0.3486682768435353, 0.3475190118756677, 0.34490336107082936, 0.34163476110505486, 0.3385871073243179, 0.33862178608263077, 0.33735733191045314, 0.3334826299022643, 0.3326839491885075, 0.3304807460097462, 0.3318315094355947, 0.328114385257272, 0.3294552513666777, 0.32747567446183373, 0.3255521551344518, 0.3248508650848853, 0.32542050790544763, 0.32361641851664374], 'val_losses': [0.6020583233137564, 0.43831846562711657, 0.416192758503063, 0.3720694228441042, 0.36752997162706225, 0.3768865297035143, 0.3569715440445056, 0.3513050267397947, 0.37408500715255616, 0.3367229573351147, 0.3407883246117346, 0.3437297136252453, 0.33561840823323724, 0.3350376315156315, 0.3417662287688975, 0.32703653735604266, 0.3312479401610128, 0.3263415355884615, 0.33051958043478796, 0.3234036315385149, 0.34741091591540235, 0.32666343131649095, 0.3286748439234131, 0.32764851251590704, 0.327983014992254, 0.32211659003089743], 'val_acc': [0.8645533141210374, 0.9391053752662574, 0.9490038842250345, 0.9592782859290816, 0.9635384037088084, 0.9616589399824583, 0.9669214384162386, 0.970930961032452, 0.962160130309485, 0.9754416739756923, 0.9739381029946123, 0.9724345320135321, 0.976694649793259, 0.977446435283799, 0.9741886981581256, 0.9779476256108257, 0.9758175667209623, 0.978950006264879, 0.9758175667209623, 0.9799523869189325, 0.9710562586142087, 0.9805788748277158, 0.9786994111013657, 0.977822328029069, 0.9804535772459592, 0.9800776845006891], 'quantized_model_size_bytes': 145978, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004755117209830757, 'batch_size': 64, 'epochs': 26, 'weight_decay': 6.376317577341345e-05, 'dropout': 0.04032817198921474, 'base_channels': 18, 'loss_type': np.str_('class_balanced'), 'gamma': 3.8826977537993166, 'label_smoothing': 0.06396717138699758, 'scheduler': np.str_('none'), 'patience': 4, 'use_amp': True, 'grad_clip': 1.3719285299043829, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibration_batches': 6, 'beta_cb': 0.9522306992356935}, 'model_parameter_count': 72197, 'model_storage_size_kb': 155.11074218750002, 'model_size_validation': 'PASS'}
2025-09-24 20:09:21,284 - INFO - _models.training_function_executor - BO Objective: base=0.9801, size_penalty=0.0000, final=0.9801
2025-09-24 20:09:21,284 - INFO - _models.training_function_executor - Model: 72,197 parameters, 155.1KB (PASS 256KB limit)
2025-09-24 20:09:21,284 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 56.030s
2025-09-24 20:09:21,395 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9801
2025-09-24 20:09:21,395 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-09-24 20:09:21,395 - INFO - bo.run_bo - Recorded observation #45: hparams={'lr': 0.004755117209830757, 'batch_size': np.int64(64), 'epochs': np.int64(26), 'weight_decay': 6.376317577341345e-05, 'dropout': 0.04032817198921474, 'base_channels': np.int64(18), 'loss_type': np.str_('class_balanced'), 'gamma': 3.8826977537993166, 'label_smoothing': 0.06396717138699758, 'scheduler': np.str_('none'), 'patience': np.int64(4), 'use_amp': np.True_, 'grad_clip': 1.3719285299043829, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9522306992356935}, value=0.9801
2025-09-24 20:09:21,395 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 45: {'lr': 0.004755117209830757, 'batch_size': np.int64(64), 'epochs': np.int64(26), 'weight_decay': 6.376317577341345e-05, 'dropout': 0.04032817198921474, 'base_channels': np.int64(18), 'loss_type': np.str_('class_balanced'), 'gamma': 3.8826977537993166, 'label_smoothing': 0.06396717138699758, 'scheduler': np.str_('none'), 'patience': np.int64(4), 'use_amp': np.True_, 'grad_clip': 1.3719285299043829, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9522306992356935} -> 0.9801
2025-09-24 20:09:21,395 - INFO - bo.run_bo - üîçBO Trial 46: Using RF surrogate + Expected Improvement
2025-09-24 20:09:21,395 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:09:21,395 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:09:21,396 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:09:21,396 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.005853258601820083, 'batch_size': 256, 'epochs': 25, 'weight_decay': 0.001713746867901851, 'dropout': 0.09726271998249628, 'base_channels': 16, 'loss_type': np.str_('class_balanced'), 'gamma': 2.46180210372806, 'label_smoothing': 0.00264473827801266, 'scheduler': np.str_('none'), 'patience': 7, 'use_amp': True, 'grad_clip': 1.4636026796724173, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 4, 'beta_cb': 0.9503952001332416}
2025-09-24 20:09:21,397 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.005853258601820083, 'batch_size': 256, 'epochs': 25, 'weight_decay': 0.001713746867901851, 'dropout': 0.09726271998249628, 'base_channels': 16, 'loss_type': np.str_('class_balanced'), 'gamma': 2.46180210372806, 'label_smoothing': 0.00264473827801266, 'scheduler': np.str_('none'), 'patience': 7, 'use_amp': True, 'grad_clip': 1.4636026796724173, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 4, 'beta_cb': 0.9503952001332416}
2025-09-24 20:09:53,815 - INFO - _models.training_function_executor - Model: 57,269 parameters, 246.1KB storage
2025-09-24 20:09:53,815 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5590635215375738, 0.23704708089359242, 0.2024437428198523, 0.16907479836045436, 0.1586098297335833, 0.14445581057814397, 0.13502143631592597, 0.12743335826181176, 0.12069887096223401, 0.11700406483669865, 0.11942041743291977, 0.10937120468549344, 0.10881662338900207, 0.10293806588609046, 0.10304226329270018, 0.09780890726180161, 0.10310521410704705, 0.0956824170572863, 0.09547032802831192, 0.09362065102433, 0.09141577244004716, 0.08819277240952704, 0.09134325546181016, 0.09170811005475597, 0.08687860511015319], 'val_losses': [0.287753959974166, 0.2501650873901641, 0.17794042115368472, 0.14964775339304812, 0.15534686303246276, 0.12843801400264454, 0.12292692674093565, 0.11339968563810265, 0.11725160213673477, 0.11146133256565731, 0.11722724687958283, 0.12105399519746546, 0.10625145651466371, 0.11347919000227967, 0.10281077383933039, 0.09275152429036516, 0.09809928714422456, 0.09083738441459667, 0.09204842337946174, 0.11635685105391604, 0.09807502227985773, 0.08320154611321413, 0.0824831292208701, 0.08718547414320332, 0.08323388338245824], 'val_acc': [0.9251973436912667, 0.930459842125047, 0.9495050745520611, 0.9597794762561083, 0.9591529883473249, 0.9639142964540784, 0.9665455456709685, 0.9684250093973187, 0.9680491166520486, 0.9714321513594788, 0.9714321513594788, 0.9698032827966421, 0.9733116150858289, 0.9720586392682621, 0.9750657812304222, 0.978198220774339, 0.9745645909033955, 0.976318757047989, 0.9771958401202857, 0.9705550682871821, 0.9746898884851523, 0.9808294699912292, 0.9798270893371758, 0.9776970304473124, 0.9800776845006891], 'quantized_model_size_bytes': 115954, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.005853258601820083, 'batch_size': 256, 'epochs': 25, 'weight_decay': 0.001713746867901851, 'dropout': 0.09726271998249628, 'base_channels': 16, 'loss_type': np.str_('class_balanced'), 'gamma': 2.46180210372806, 'label_smoothing': 0.00264473827801266, 'scheduler': np.str_('none'), 'patience': 7, 'use_amp': True, 'grad_clip': 1.4636026796724173, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 4, 'beta_cb': 0.9503952001332416}, 'model_parameter_count': 57269, 'model_storage_size_kb': 246.077734375, 'model_size_validation': 'PASS'}
2025-09-24 20:09:53,815 - INFO - _models.training_function_executor - BO Objective: base=0.9801, size_penalty=0.0000, final=0.9801
2025-09-24 20:09:53,815 - INFO - _models.training_function_executor - Model: 57,269 parameters, 246.1KB (PASS 256KB limit)
2025-09-24 20:09:53,815 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 32.419s
2025-09-24 20:09:53,925 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9801
2025-09-24 20:09:53,925 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.110s
2025-09-24 20:09:53,926 - INFO - bo.run_bo - Recorded observation #46: hparams={'lr': 0.005853258601820083, 'batch_size': np.int64(256), 'epochs': np.int64(25), 'weight_decay': 0.001713746867901851, 'dropout': 0.09726271998249628, 'base_channels': np.int64(16), 'loss_type': np.str_('class_balanced'), 'gamma': 2.46180210372806, 'label_smoothing': 0.00264473827801266, 'scheduler': np.str_('none'), 'patience': np.int64(7), 'use_amp': np.True_, 'grad_clip': 1.4636026796724173, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(4), 'beta_cb': 0.9503952001332416}, value=0.9801
2025-09-24 20:09:53,926 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 46: {'lr': 0.005853258601820083, 'batch_size': np.int64(256), 'epochs': np.int64(25), 'weight_decay': 0.001713746867901851, 'dropout': 0.09726271998249628, 'base_channels': np.int64(16), 'loss_type': np.str_('class_balanced'), 'gamma': 2.46180210372806, 'label_smoothing': 0.00264473827801266, 'scheduler': np.str_('none'), 'patience': np.int64(7), 'use_amp': np.True_, 'grad_clip': 1.4636026796724173, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(4), 'beta_cb': 0.9503952001332416} -> 0.9801
2025-09-24 20:09:53,926 - INFO - bo.run_bo - üîçBO Trial 47: Using RF surrogate + Expected Improvement
2025-09-24 20:09:53,926 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:09:53,926 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:09:53,926 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:09:53,926 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0003073800804956575, 'batch_size': 256, 'epochs': 31, 'weight_decay': 0.0003749181630423452, 'dropout': 0.1934277757583996, 'base_channels': 9, 'loss_type': np.str_('class_balanced'), 'gamma': 0.9404509508593214, 'label_smoothing': 0.03043583403314572, 'scheduler': np.str_('plateau'), 'patience': 7, 'use_amp': False, 'grad_clip': 1.350234517636252, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.9607446473476068}
2025-09-24 20:09:53,928 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0003073800804956575, 'batch_size': 256, 'epochs': 31, 'weight_decay': 0.0003749181630423452, 'dropout': 0.1934277757583996, 'base_channels': 9, 'loss_type': np.str_('class_balanced'), 'gamma': 0.9404509508593214, 'label_smoothing': 0.03043583403314572, 'scheduler': np.str_('plateau'), 'patience': 7, 'use_amp': False, 'grad_clip': 1.350234517636252, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.9607446473476068}
2025-09-24 20:10:30,365 - INFO - _models.training_function_executor - Model: 15,444 parameters, 16.6KB storage
2025-09-24 20:10:30,365 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.2539860869628037, 0.831198715014972, 0.6653352922239807, 0.553217489356494, 0.49702917481859393, 0.44827554768175837, 0.39963657331583186, 0.37479708813726237, 0.3583705604601051, 0.3479747085623494, 0.34057064607370224, 0.3358336990404573, 0.33049739752687973, 0.3243920516576421, 0.32100989565165033, 0.3169233460543288, 0.3126775983808423, 0.3102844534327175, 0.3074940730316785, 0.3059228230588094, 0.3026736027918539, 0.2990138374862525, 0.29600924390698385, 0.2917989724081523, 0.2906592972288379, 0.28572404246818256, 0.2852758854133751, 0.2805526450622262, 0.27761889490343944, 0.27959464775572307, 0.2760397167239494], 'val_losses': [0.8870088564484747, 0.725084069206547, 0.558752114955621, 0.49162500561247074, 0.42888692268197837, 0.38611211709966636, 0.3530106839493184, 0.335897454866609, 0.3207508941769017, 0.31779519308070436, 0.3131809305885564, 0.3107756297550409, 0.30259274742781345, 0.2988271485323775, 0.2962619667251282, 0.29133895270142723, 0.29008721749281585, 0.28798117896112757, 0.28505024462359513, 0.2761336095073799, 0.27424377723902843, 0.2747533914551761, 0.27705392387670286, 0.26697391930110115, 0.26728312578950814, 0.2685729761557417, 0.26047960698022055, 0.2610998787205256, 0.2572075954103153, 0.2621094681758538, 0.2655282828827547], 'val_acc': [0.7200852023555946, 0.7199599047738379, 0.8438792131311865, 0.8938729482520987, 0.9152988347324896, 0.9317128179426137, 0.9409848389926074, 0.9441172785365243, 0.9439919809547676, 0.9467485277534143, 0.9454955519358477, 0.9473750156621977, 0.9471244204986844, 0.9488785866432777, 0.9491291818067912, 0.9491291818067912, 0.9496303721338178, 0.9481268011527377, 0.9517604310236812, 0.9530134068412479, 0.9575241197844881, 0.9568976318757048, 0.9582759052750282, 0.961408344818945, 0.9594035835108382, 0.9616589399824583, 0.962536023054755, 0.9622854278912417, 0.9627866182182684, 0.9605312617466483, 0.9611577496554317], 'quantized_model_size_bytes': 63408, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0003073800804956575, 'batch_size': 256, 'epochs': 31, 'weight_decay': 0.0003749181630423452, 'dropout': 0.1934277757583996, 'base_channels': 9, 'loss_type': np.str_('class_balanced'), 'gamma': 0.9404509508593214, 'label_smoothing': 0.03043583403314572, 'scheduler': np.str_('plateau'), 'patience': 7, 'use_amp': False, 'grad_clip': 1.350234517636252, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 6, 'beta_cb': 0.9607446473476068}, 'model_parameter_count': 15444, 'model_storage_size_kb': 16.590234375, 'model_size_validation': 'PASS'}
2025-09-24 20:10:30,366 - INFO - _models.training_function_executor - BO Objective: base=0.9612, size_penalty=0.0000, final=0.9612
2025-09-24 20:10:30,366 - INFO - _models.training_function_executor - Model: 15,444 parameters, 16.6KB (PASS 256KB limit)
2025-09-24 20:10:30,366 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 36.439s
2025-09-24 20:10:30,476 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9612
2025-09-24 20:10:30,476 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-09-24 20:10:30,477 - INFO - bo.run_bo - Recorded observation #47: hparams={'lr': 0.0003073800804956575, 'batch_size': np.int64(256), 'epochs': np.int64(31), 'weight_decay': 0.0003749181630423452, 'dropout': 0.1934277757583996, 'base_channels': np.int64(9), 'loss_type': np.str_('class_balanced'), 'gamma': 0.9404509508593214, 'label_smoothing': 0.03043583403314572, 'scheduler': np.str_('plateau'), 'patience': np.int64(7), 'use_amp': np.False_, 'grad_clip': 1.350234517636252, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9607446473476068}, value=0.9612
2025-09-24 20:10:30,477 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 47: {'lr': 0.0003073800804956575, 'batch_size': np.int64(256), 'epochs': np.int64(31), 'weight_decay': 0.0003749181630423452, 'dropout': 0.1934277757583996, 'base_channels': np.int64(9), 'loss_type': np.str_('class_balanced'), 'gamma': 0.9404509508593214, 'label_smoothing': 0.03043583403314572, 'scheduler': np.str_('plateau'), 'patience': np.int64(7), 'use_amp': np.False_, 'grad_clip': 1.350234517636252, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(6), 'beta_cb': 0.9607446473476068} -> 0.9612
2025-09-24 20:10:30,477 - INFO - bo.run_bo - üîçBO Trial 48: Using RF surrogate + Expected Improvement
2025-09-24 20:10:30,477 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:10:30,477 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:10:30,477 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:10:30,477 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001839138966230835, 'batch_size': 256, 'epochs': 20, 'weight_decay': 0.0006162912331866664, 'dropout': 0.4802599462424312, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 2.9707824648679364, 'label_smoothing': 0.06983887141261087, 'scheduler': np.str_('plateau'), 'patience': 4, 'use_amp': True, 'grad_clip': 1.2790578309181688, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9755761025917479}
2025-09-24 20:10:30,479 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001839138966230835, 'batch_size': 256, 'epochs': 20, 'weight_decay': 0.0006162912331866664, 'dropout': 0.4802599462424312, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 2.9707824648679364, 'label_smoothing': 0.06983887141261087, 'scheduler': np.str_('plateau'), 'patience': 4, 'use_amp': True, 'grad_clip': 1.2790578309181688, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9755761025917479}
2025-09-24 20:10:57,655 - INFO - _models.training_function_executor - Model: 50,453 parameters, 216.8KB storage
2025-09-24 20:10:57,656 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9529488755266525, 0.6945247759803496, 0.5408228410144342, 0.5028488844940247, 0.48241848603819404, 0.46612959092853024, 0.4591328862992548, 0.4488087427316257, 0.4413605399918288, 0.43450635980907004, 0.43381427167348685, 0.42700733949030284, 0.420989690565269, 0.41660276460785006, 0.4120814653580094, 0.4110878350960784, 0.4120059920132909, 0.40510520173303655, 0.4012067312622632, 0.4005210061059561], 'val_losses': [0.8636179777512707, 0.5259263070920792, 0.47573897228356277, 0.47319860835001293, 0.4370553255857882, 0.43761016656441015, 0.41681710572682473, 0.41160321980248027, 0.4065729381633035, 0.4154905869561678, 0.39913637970282817, 0.3934936444273331, 0.39148039564484716, 0.3820624240223529, 0.3754989021165884, 0.37725703826585133, 0.3750161642255139, 0.36910219632612856, 0.37809541932010904, 0.3741353412362182], 'val_acc': [0.7200852023555946, 0.9195589525122165, 0.939230672848014, 0.937727101866934, 0.9473750156621977, 0.9458714446811176, 0.9490038842250345, 0.954516977822328, 0.9558952512216514, 0.9540157874953014, 0.9579000125297582, 0.9597794762561083, 0.9619095351459717, 0.9665455456709685, 0.9684250093973187, 0.967547926325022, 0.968299711815562, 0.9699285803783987, 0.9667961408344818, 0.9654178674351584], 'quantized_model_size_bytes': 204404, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001839138966230835, 'batch_size': 256, 'epochs': 20, 'weight_decay': 0.0006162912331866664, 'dropout': 0.4802599462424312, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 2.9707824648679364, 'label_smoothing': 0.06983887141261087, 'scheduler': np.str_('plateau'), 'patience': 4, 'use_amp': True, 'grad_clip': 1.2790578309181688, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9755761025917479}, 'model_parameter_count': 50453, 'model_storage_size_kb': 216.790234375, 'model_size_validation': 'PASS'}
2025-09-24 20:10:57,656 - INFO - _models.training_function_executor - BO Objective: base=0.9654, size_penalty=0.0000, final=0.9654
2025-09-24 20:10:57,656 - INFO - _models.training_function_executor - Model: 50,453 parameters, 216.8KB (PASS 256KB limit)
2025-09-24 20:10:57,656 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 27.179s
2025-09-24 20:10:57,767 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9654
2025-09-24 20:10:57,767 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-09-24 20:10:57,767 - INFO - bo.run_bo - Recorded observation #48: hparams={'lr': 0.001839138966230835, 'batch_size': np.int64(256), 'epochs': np.int64(20), 'weight_decay': 0.0006162912331866664, 'dropout': 0.4802599462424312, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 2.9707824648679364, 'label_smoothing': 0.06983887141261087, 'scheduler': np.str_('plateau'), 'patience': np.int64(4), 'use_amp': np.True_, 'grad_clip': 1.2790578309181688, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9755761025917479}, value=0.9654
2025-09-24 20:10:57,767 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 48: {'lr': 0.001839138966230835, 'batch_size': np.int64(256), 'epochs': np.int64(20), 'weight_decay': 0.0006162912331866664, 'dropout': 0.4802599462424312, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 2.9707824648679364, 'label_smoothing': 0.06983887141261087, 'scheduler': np.str_('plateau'), 'patience': np.int64(4), 'use_amp': np.True_, 'grad_clip': 1.2790578309181688, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9755761025917479} -> 0.9654
2025-09-24 20:10:57,768 - INFO - bo.run_bo - üîçBO Trial 49: Using RF surrogate + Expected Improvement
2025-09-24 20:10:57,768 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:10:57,768 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:10:57,768 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:10:57,768 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004959854185547425, 'batch_size': 256, 'epochs': 28, 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': True, 'grad_clip': 1.2587146054481926, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9702543209337828}
2025-09-24 20:10:57,769 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004959854185547425, 'batch_size': 256, 'epochs': 28, 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': True, 'grad_clip': 1.2587146054481926, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9702543209337828}
2025-09-24 20:11:35,409 - INFO - _models.training_function_executor - Model: 50,453 parameters, 216.8KB storage
2025-09-24 20:11:35,409 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1877046840660586, 0.658103232372824, 0.33261013220053576, 0.2700071884346901, 0.24707174000164822, 0.23030330989262074, 0.21436385976037028, 0.20163439278879236, 0.19413797943898206, 0.19090266204924414, 0.18329166453837586, 0.17827831569203562, 0.17275782708883008, 0.1726072302655693, 0.1673721038210016, 0.1626838624857349, 0.15888083346788587, 0.15696148150885378, 0.15361582846615476, 0.15361097545114125, 0.14811675487516726, 0.14517599654011726, 0.14227180913684923, 0.1399116158850438, 0.13672289662538076, 0.13698626956059753, 0.13514151907268146, 0.13457995060476202], 'val_losses': [0.8266588471584371, 0.47621549772164706, 0.3950286232623043, 0.24745555438242256, 0.32185103585846725, 0.21481609162255466, 0.2149242037236728, 0.2752326616699751, 0.1887725616379678, 0.21864207325506263, 0.17915577016039877, 0.18937406936596696, 0.17017802053145517, 0.19741265575452013, 0.17521675217832752, 0.17432156614814542, 0.15700188368024898, 0.16168978634991601, 0.15750094263524642, 0.17018110045080062, 0.15225803843119318, 0.15015952053479034, 0.14914852618991903, 0.14727269103541707, 0.14813509856440224, 0.1476846523186181, 0.14628827323734037, 0.14615302634947194], 'val_acc': [0.7200852023555946, 0.8904899135446686, 0.9111640145345195, 0.9522616213507079, 0.9335922816689638, 0.9629119158000251, 0.9652925698534018, 0.9433654930459842, 0.9730610199223155, 0.9627866182182684, 0.9748151860669089, 0.9684250093973187, 0.977446435283799, 0.9680491166520486, 0.9745645909033955, 0.9775717328655557, 0.982458338554066, 0.9814559579000125, 0.9820824458087959, 0.976318757047989, 0.9837113143716326, 0.9842125046986593, 0.9845883974439293, 0.9857160756797394, 0.9859666708432527, 0.9854654805162261, 0.9867184563337927, 0.9869690514973061], 'quantized_model_size_bytes': 204404, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004959854185547425, 'batch_size': 256, 'epochs': 28, 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': True, 'grad_clip': 1.2587146054481926, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9702543209337828}, 'model_parameter_count': 50453, 'model_storage_size_kb': 216.790234375, 'model_size_validation': 'PASS'}
2025-09-24 20:11:35,409 - INFO - _models.training_function_executor - BO Objective: base=0.9870, size_penalty=0.0000, final=0.9870
2025-09-24 20:11:35,409 - INFO - _models.training_function_executor - Model: 50,453 parameters, 216.8KB (PASS 256KB limit)
2025-09-24 20:11:35,409 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 37.641s
2025-09-24 20:11:35,521 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9870
2025-09-24 20:11:35,521 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.112s
2025-09-24 20:11:35,521 - INFO - bo.run_bo - Recorded observation #49: hparams={'lr': 0.004959854185547425, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.True_, 'grad_clip': 1.2587146054481926, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9702543209337828}, value=0.9870
2025-09-24 20:11:35,521 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 49: {'lr': 0.004959854185547425, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.True_, 'grad_clip': 1.2587146054481926, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9702543209337828} -> 0.9870
2025-09-24 20:11:35,522 - INFO - bo.run_bo - üîçBO Trial 50: Using RF surrogate + Expected Improvement
2025-09-24 20:11:35,522 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-24 20:11:35,522 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:11:35,522 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:11:35,522 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0018247134370373871, 'batch_size': 128, 'epochs': 21, 'weight_decay': 0.00011546868217018823, 'dropout': 0.11244036393904108, 'base_channels': 16, 'loss_type': np.str_('class_balanced'), 'gamma': 0.028214905568166308, 'label_smoothing': 0.016806457095973407, 'scheduler': np.str_('plateau'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.7091436891064762, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 4, 'beta_cb': 0.9610291711690754}
2025-09-24 20:11:35,524 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0018247134370373871, 'batch_size': 128, 'epochs': 21, 'weight_decay': 0.00011546868217018823, 'dropout': 0.11244036393904108, 'base_channels': 16, 'loss_type': np.str_('class_balanced'), 'gamma': 0.028214905568166308, 'label_smoothing': 0.016806457095973407, 'scheduler': np.str_('plateau'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.7091436891064762, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 4, 'beta_cb': 0.9610291711690754}
2025-09-24 20:12:13,786 - INFO - _models.training_function_executor - Model: 57,269 parameters, 123.0KB storage
2025-09-24 20:12:13,786 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5803207035475169, 0.28085434013331206, 0.2512999110985074, 0.22393145102985265, 0.2122033091406511, 0.2042875694787185, 0.19344910480651745, 0.18889042211087526, 0.18070142000332007, 0.17577055751618276, 0.17396951016281734, 0.17203828415166234, 0.1676000389395787, 0.1665758827132726, 0.1627742532362142, 0.16219649086613422, 0.16134310502648685, 0.1597867967914535, 0.1596479778781151, 0.15915560588150535, 0.15529056260733887], 'val_losses': [0.3035724597058041, 0.303588200834308, 0.24069731849549245, 0.21112600233475376, 0.21380650159590017, 0.19002036307258663, 0.1761417398064705, 0.16912732239266026, 0.17372076669113928, 0.157136069491922, 0.16508127050118374, 0.16828767533903358, 0.16195985180455452, 0.16528372906074684, 0.16799637856018632, 0.16533296079642193, 0.15049593135865763, 0.1473313717577873, 0.16533776351666482, 0.15721700271707537, 0.14548614262123033], 'val_acc': [0.9325899010149105, 0.9401077559203107, 0.9510086455331412, 0.961784237564215, 0.9571482270392181, 0.968299711815562, 0.9730610199223155, 0.9739381029946123, 0.9744392933216389, 0.976694649793259, 0.9746898884851523, 0.9721839368500188, 0.9756922691392056, 0.9741886981581256, 0.9741886981581256, 0.9741886981581256, 0.9788247086831224, 0.9807041724094725, 0.9735622102493422, 0.977446435283799, 0.9815812554817692], 'quantized_model_size_bytes': 115954, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0018247134370373871, 'batch_size': 128, 'epochs': 21, 'weight_decay': 0.00011546868217018823, 'dropout': 0.11244036393904108, 'base_channels': 16, 'loss_type': np.str_('class_balanced'), 'gamma': 0.028214905568166308, 'label_smoothing': 0.016806457095973407, 'scheduler': np.str_('plateau'), 'patience': 6, 'use_amp': False, 'grad_clip': 1.7091436891064762, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibration_batches': 4, 'beta_cb': 0.9610291711690754}, 'model_parameter_count': 57269, 'model_storage_size_kb': 123.0388671875, 'model_size_validation': 'PASS'}
2025-09-24 20:12:13,786 - INFO - _models.training_function_executor - BO Objective: base=0.9816, size_penalty=0.0000, final=0.9816
2025-09-24 20:12:13,786 - INFO - _models.training_function_executor - Model: 57,269 parameters, 123.0KB (PASS 256KB limit)
2025-09-24 20:12:13,786 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 38.264s
2025-09-24 20:12:13,902 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9816
2025-09-24 20:12:13,903 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.116s
2025-09-24 20:12:13,903 - INFO - bo.run_bo - Recorded observation #50: hparams={'lr': 0.0018247134370373871, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'weight_decay': 0.00011546868217018823, 'dropout': 0.11244036393904108, 'base_channels': np.int64(16), 'loss_type': np.str_('class_balanced'), 'gamma': 0.028214905568166308, 'label_smoothing': 0.016806457095973407, 'scheduler': np.str_('plateau'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 1.7091436891064762, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(4), 'beta_cb': 0.9610291711690754}, value=0.9816
2025-09-24 20:12:13,903 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 50: {'lr': 0.0018247134370373871, 'batch_size': np.int64(128), 'epochs': np.int64(21), 'weight_decay': 0.00011546868217018823, 'dropout': 0.11244036393904108, 'base_channels': np.int64(16), 'loss_type': np.str_('class_balanced'), 'gamma': 0.028214905568166308, 'label_smoothing': 0.016806457095973407, 'scheduler': np.str_('plateau'), 'patience': np.int64(6), 'use_amp': np.False_, 'grad_clip': 1.7091436891064762, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibration_batches': np.int64(4), 'beta_cb': 0.9610291711690754} -> 0.9816
2025-09-24 20:12:13,903 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.9870
2025-09-24 20:12:13,903 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.004959854185547425, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.True_, 'grad_clip': 1.2587146054481926, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9702543209337828}
2025-09-24 20:12:13,903 - INFO - visualization - Generating BO visualization charts with 50 trials...
2025-09-24 20:12:15,387 - INFO - visualization - BO summary saved to: charts/20250924_201213_BO_RRFusionRes1DCNN-5C/bo_summary.txt
2025-09-24 20:12:15,387 - INFO - visualization - BO charts saved to: charts/20250924_201213_BO_RRFusionRes1DCNN-5C
2025-09-24 20:12:15,387 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä BO charts saved to: charts/20250924_201213_BO_RRFusionRes1DCNN-5C
2025-09-24 20:12:15,440 - INFO - evaluation.code_generation_pipeline_orchestrator - üöÄ STEP 4: Final Training Execution
2025-09-24 20:12:15,440 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-24 20:12:15,533 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-24 20:12:15,547 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-24 20:12:15,564 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-24 20:12:15,565 - INFO - _models.training_function_executor - Loaded training function: RRFusionRes1DCNN-5C
2025-09-24 20:12:15,565 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-24 20:12:15,565 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.004959854185547425, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.True_, 'grad_clip': 1.2587146054481926, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9702543209337828}
2025-09-24 20:12:15,565 - INFO - evaluation.code_generation_pipeline_orchestrator - Using test set for final training evaluation
2025-09-24 20:12:15,565 - INFO - _models.training_function_executor - Using device: cuda
2025-09-24 20:12:15,588 - INFO - _models.training_function_executor - Executing training function: RRFusionRes1DCNN-5C
2025-09-24 20:12:15,588 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004959854185547425, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': np.int64(15), 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': np.int64(3), 'use_amp': np.True_, 'grad_clip': 1.2587146054481926, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibration_batches': np.int64(3), 'beta_cb': 0.9702543209337828}
2025-09-24 20:12:15,590 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004959854185547425, 'batch_size': 256, 'epochs': 28, 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': True, 'grad_clip': 1.2587146054481926, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9702543209337828}
2025-09-24 20:13:03,869 - INFO - _models.training_function_executor - Model: 50,453 parameters, 216.8KB storage
2025-09-24 20:13:03,869 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9840489096324159, 0.5041973173331527, 0.2963574090980011, 0.25394962239284563, 0.22731062695802262, 0.21024193590224602, 0.2018195273117151, 0.18946116094409512, 0.18348271615426828, 0.17926563365562687, 0.1768748109447736, 0.17463799075499092, 0.16962281474134114, 0.16555152733085055, 0.1641737561084408, 0.15773316009721472, 0.15557606232672955, 0.153337226811656, 0.1511315005893409, 0.14833001325201203, 0.1449632250316256, 0.14268141839154358, 0.13958770392795705, 0.1375099433692054, 0.13543951234152152, 0.13410595300776917, 0.13296807915235195, 0.13264500748876965], 'val_losses': [0.7350707663328147, 0.5019711654825358, 0.36593963801454554, 0.40354114740778685, 0.21955458295320454, 0.2201606490284924, 0.1856778681746616, 0.2089297517234384, 0.20454358008461138, 0.18981357616612948, 0.1750978673637635, 0.1723413035344049, 0.1679893046122834, 0.16961253708581403, 0.1594399511324223, 0.1670386412719919, 0.15853695134818913, 0.15723257799517917, 0.15436161450534477, 0.1517867122161227, 0.1483138006253113, 0.1470509033048844, 0.1453746343486321, 0.14360537476345947, 0.1424721451456053, 0.14264930562218442, 0.14202377745940972, 0.14234954572060413], 'val_acc': [0.7583994868093978, 0.8693769545345201, 0.9101916446155079, 0.9005693208243124, 0.9622323791195574, 0.9635955416566434, 0.9726565632266859, 0.9638360997514233, 0.9672840991099351, 0.9725763771950926, 0.9761045625851976, 0.9792318178173363, 0.9789912597225563, 0.9758640044904178, 0.9823590730494748, 0.97834977146981, 0.9809959105123888, 0.9809157244807954, 0.9830005613022211, 0.9838826076497474, 0.9850052120920536, 0.9851655841552401, 0.9872504209766658, 0.9877315371662256, 0.9871702349450726, 0.9868494908186994, 0.9870900489134793, 0.9871702349450726], 'quantized_model_size_bytes': 204404, 'model_name': 'RRFusionRes1DCNN-5C', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004959854185547425, 'batch_size': 256, 'epochs': 28, 'weight_decay': 0.00021566950120975755, 'dropout': 0.032968548155650375, 'base_channels': 15, 'loss_type': np.str_('class_balanced'), 'gamma': 2.046391358400678, 'label_smoothing': 0.02033230778878499, 'scheduler': np.str_('onecycle'), 'patience': 3, 'use_amp': True, 'grad_clip': 1.2587146054481926, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibration_batches': 3, 'beta_cb': 0.9702543209337828}, 'model_parameter_count': 50453, 'model_storage_size_kb': 216.790234375, 'model_size_validation': 'PASS'}
2025-09-24 20:13:03,869 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final test metrics from training (avoids preprocessing mismatch)
2025-09-24 20:13:03,869 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 0.9871702349450726, 'macro_f1': None}
2025-09-24 20:13:03,877 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä STEP 5: Performance Analysis
2025-09-24 20:13:03,877 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: RRFusionRes1DCNN-5C
2025-09-24 20:13:03,877 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.9870
2025-09-24 20:13:03,877 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: 0.9872
2025-09-24 20:13:03,877 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-09-24 20:13:03,877 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: RRFusionRes1DCNN-5C
2025-09-24 20:13:03,877 - INFO - evaluation.code_generation_pipeline_orchestrator - Score: 0.9872
2025-09-24 20:13:03,877 - INFO - __main__ - AI-enhanced training completed!
2025-09-24 20:13:03,877 - INFO - __main__ - Final model achieved: {'acc': 0.9871702349450726, 'macro_f1': None}
2025-09-24 20:13:03,877 - INFO - __main__ - Pipeline completed successfully in single attempt
2025-09-24 20:13:03,877 - INFO - __main__ - Pipeline completed: RRFusionRes1DCNN-5C, metrics: {'acc': 0.9871702349450726, 'macro_f1': None}
2025-09-24 20:13:03,877 - INFO - __main__ - Pipeline summary saved to charts/pipeline_summary_20250924_201303.json
2025-09-24 20:13:03,880 - INFO - __main__ - Model saved: trained_models/best_model_RRFusionRes1DCNN-5C_20250924_201303.pth, performance: {'acc': 0.9871702349450726, 'macro_f1': None}
2025-09-24 20:13:03,880 - INFO - __main__ - AI-enhanced processing completed successfully
