2025-09-25 16:50:07,534 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-25 16:50:07,642 - INFO - __main__ - Logging system initialized successfully
2025-09-25 16:50:07,642 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-25 16:50:07,642 - INFO - __main__ - Starting real data processing from data/dataset1/ directory
2025-09-25 16:50:07,643 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-25 16:50:07,643 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-25 16:50:07,643 - INFO - __main__ - Attempting to load: X.npy
2025-09-25 16:50:07,683 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-25 16:50:07,723 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-25 16:50:07,723 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-25 16:50:07,724 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-25 16:50:07,724 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-25 16:50:07,726 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-25 16:50:07,727 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-25 16:50:07,727 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-25 16:50:07,727 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-25 16:50:07,727 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-25 16:50:07,727 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-25 16:50:07,727 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-25 16:50:07,727 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-25 16:50:07,727 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-25 16:50:07,839 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-25 16:50:07,839 - INFO - class_balancing - Class imbalance analysis:
2025-09-25 16:50:07,839 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-25 16:50:07,839 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-25 16:50:07,839 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-25 16:50:07,839 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-25 16:50:07,840 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-25 16:50:07,840 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-25 16:50:07,840 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-25 16:50:07,840 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-25 16:50:08,011 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-25 16:50:08,012 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-25 16:50:08,012 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-25 16:50:08,015 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-25 16:50:08,015 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-25 16:50:08,015 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-25 16:50:08,015 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-25 16:52:19,118 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-25 16:52:19,142 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-25 16:52:19,142 - INFO - _models.ai_code_generator - Prompt length: 3679 characters
2025-09-25 16:52:19,142 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-25 16:52:19,142 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-25 16:52:19,142 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-25 16:53:54,551 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-25 16:53:54,551 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-25 16:53:54,552 - INFO - _models.ai_code_generator - AI generated training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:53:54,552 - INFO - _models.ai_code_generator - Confidence: 0.78
2025-09-25 16:53:54,552 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-09-25 16:53:54,552 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:53:54,552 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'label_smoothing', 'grad_clip_norm', 'use_scheduler', 'channels_base', 'se_reduction', 'dilation_b1', 'dilation_b2', 'dilation_b3', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_steps', 'seed']
2025-09-25 16:53:54,552 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.78
2025-09-25 16:53:54,552 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-25 16:53:54,552 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:53:54,552 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:53:54,553 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-25 16:53:54,553 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:53:54,553 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-25 16:53:54,553 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-25 16:53:54,553 - WARNING - package_installer - Could not parse code for imports due to syntax error: unexpected character after line continuation character (<unknown>, line 1)
2025-09-25 16:53:54,553 - INFO - package_installer - Extracted imports from code: set()
2025-09-25 16:53:54,553 - INFO - package_installer - ‚úÖ No external packages required
2025-09-25 16:53:54,553 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-25 16:53:54,553 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:53:54,553 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-25 16:53:54,553 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'label_smoothing', 'grad_clip_norm', 'use_scheduler', 'channels_base', 'se_reduction', 'dilation_b1', 'dilation_b2', 'dilation_b3', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_steps', 'seed']
2025-09-25 16:53:54,553 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-25 16:53:54,553 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:53:54,553 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-25 16:53:54,588 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-25 16:53:54,720 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-25 16:53:54,720 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-25 16:53:54,720 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-25 16:53:54,721 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-25 16:53:54,721 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-25 16:53:54,721 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 16:53:54,721 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:53:54,721 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}
2025-09-25 16:53:54,743 - ERROR - _models.training_function_executor - Training execution failed: cannot import name 'prepare_fx' from 'torch.ao.quantization.fx' (/home/shiyuanduan/anaconda3/envs/GPT/lib/python3.10/site-packages/torch/ao/quantization/fx/__init__.py)
2025-09-25 16:53:54,743 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-25 16:53:54,744 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-25 16:53:54,744 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-25 16:53:54,744 - INFO - _models.ai_code_generator - Prompt length: 14768 characters
2025-09-25 16:53:54,744 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-25 16:53:54,744 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-25 16:53:54,744 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-25 16:55:10,835 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-25 16:55:10,836 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-25 16:55:10,836 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250925_165510_attempt1.txt
2025-09-25 16:55:10,836 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code":"def train_model(\n    X_train, y_train, X_val, y_val, device,\n    lr=1e-3, batch_size=256, epochs=20, weight_decay=1e-4,\n    dropout=0.1, label_smoothing=0.1, grad_clip_norm=1.0, use_scheduler=True,\n    channels_base=16, se_reduction=8, dilation_b1=1, dilation_b2=2, dilation_b3=1,\n    quantization_bits=8, quantize_weights=True, quantize_activations=True, calibrate_steps=64, seed=42\n):\n    import copy\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader\n    from torch.ao.quantization import get_default_qconfig, QConfigMapping\n\n    device = torch.device(device)\n    if device.type != 'cuda' or not torch.cuda.is_available():\n        raise RuntimeError('CUDA device is required. Please pass device=\"cuda\" and ensure a GPU is available.')\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n    num_classes = 5\n\n    train_ds = TensorDatasetECG(X_train, y_train)\n    val_ds = TensorDatasetECG(X_val, y_val)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False, drop_last=False)\n\n    model = ECGNet1D(\n        in_ch=2, base_ch=int(channels_base), k_init=7, k_mid=5, k_small=3,\n        d_b1=int(dilation_b1), d_b2=int(dilation_b2), d_b3=int(dilation_b3),\n        dropout=float(dropout), se_reduction=int(se_reduction), n_classes=num_classes\n    ).to(device)\n\n    class_weights = _make_class_weights(y_train, num_classes, device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(label_smoothing)).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=float(lr) * 0.1) if use_scheduler else None\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running_loss = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            seen += xb.size(0)\n        train_loss = running_loss / max(1, seen)\n\n        val_loss, val_acc = _evaluate(model, val_loader, device, criterion)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}/{int(epochs):03d} - train_loss: {train_loss:.6f} - val_loss: {val_loss:.6f} - val_acc: {val_acc:.4f}')\n\n    # Local helper using the correct FX import path for PyTorch >= 2.0\n    def _quantize_int8_fx_local(model_fp32, calib_loader, example_input, quantize_activations=True):\n        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n        model_cpu = copy.deepcopy(model_fp32).to('cpu').eval()\n        torch.backends.quantized.engine = 'fbgemm'\n        qconfig = get_default_qconfig('fbgemm')\n        if not quantize_activations:\n            return model_cpu\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        prepared = prepare_fx(model_cpu, qconfig_mapping, example_inputs=example_input)\n        with torch.inference_mode():\n            for xb, _ in calib_loader:\n                xb = xb.to('cpu')\n                prepared(xb)\n        quantized_model = convert_fx(prepared)\n        return quantized_model\n\n    calib_bs = min(batch_size, 256)\n    calib_count = int(calibrate_steps)\n    if calib_count <= 0:\n        calib_count = 1\n    calib_indices = torch.arange(min(len(train_ds), calib_bs * calib_count))\n    X_calib = train_ds.X[calib_indices]\n    y_calib = train_ds.y[calib_indices]\n    calib_loader = DataLoader(TensorDatasetECG(X_calib, y_calib), batch_size=calib_bs, shuffle=False, num_workers=0, pin_memory=False)\n\n    try:\n        ex_xb, _ = next(iter(calib_loader))\n    except StopIteration:\n        ex_xb = torch.zeros(1, 2, 1000)\n    if ex_xb.ndim == 3 and ex_xb.shape[1] != 2 and ex_xb.shape[-1] == 2:\n        ex_xb = ex_xb.permute(0, 2, 1)\n\n    quantized_model = None\n    if quantize_weights and quantization_bits == 8:\n        try:\n            quantized_model = _quantize_int8_fx_local(model.to('cpu'), calib_loader, example_input=(ex_xb,), quantize_activations=bool(quantize_activations))\n        except Exception:\n            # Fallback to fp32 if quantization APIs are unavailable\n            quantized_model = copy.deepcopy(model).to('cpu')\n    elif quantize_weights and quantization_bits == 16:\n        quantized_model = copy.deepcopy(model).to('cpu').half()\n    else:\n        quantized_model = copy.deepcopy(model).to('cpu')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return quantized_model, metrics\n"}
2025-09-25 16:55:10,837 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-25 16:55:10,837 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code":"def train_model(\n    X_train, y_train, X_val, y_val, device,\n    lr=1e-3, batch_size=256, epochs=20, weight_decay=1e-4,\n    dropout=0.1, label_smoothing=0.1, grad_clip_norm=1.0, use_scheduler=True,\n    channels_base=16, se_reduction=8, dilation_b1=1, dilation_b2=2, dilation_b3=1,\n    quantization_bits=8, quantize_weights=True, quantize_activations=True, calibrate_steps=64, seed=42\n):\n    import copy\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader\n    from torch.ao.quantization import get_default_qconfig, QConfigMapping\n\n    device = torch.device(device)\n    if device.type != 'cuda' or not torch.cuda.is_available():\n        raise RuntimeError('CUDA device is required. Please pass device=\"cuda\" and ensure a GPU is available.')\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n    num_classes = 5\n\n    train_ds = TensorDatasetECG(X_train, y_train)\n    val_ds = TensorDatasetECG(X_val, y_val)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False, drop_last=False)\n\n    model = ECGNet1D(\n        in_ch=2, base_ch=int(channels_base), k_init=7, k_mid=5, k_small=3,\n        d_b1=int(dilation_b1), d_b2=int(dilation_b2), d_b3=int(dilation_b3),\n        dropout=float(dropout), se_reduction=int(se_reduction), n_classes=num_classes\n    ).to(device)\n\n    class_weights = _make_class_weights(y_train, num_classes, device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(label_smoothing)).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=float(lr) * 0.1) if use_scheduler else None\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running_loss = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            seen += xb.size(0)\n        train_loss = running_loss / max(1, seen)\n\n        val_loss, val_acc = _evaluate(model, val_loader, device, criterion)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}/{int(epochs):03d} - train_loss: {train_loss:.6f} - val_loss: {val_loss:.6f} - val_acc: {val_acc:.4f}')\n\n    # Local helper using the correct FX import path for PyTorch >= 2.0\n    def _quantize_int8_fx_local(model_fp32, calib_loader, example_input, quantize_activations=True):\n        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n        model_cpu = copy.deepcopy(model_fp32).to('cpu').eval()\n        torch.backends.quantized.engine = 'fbgemm'\n        qconfig = get_default_qconfig('fbgemm')\n        if not quantize_activations:\n            return model_cpu\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        prepared = prepare_fx(model_cpu, qconfig_mapping, example_inputs=example_input)\n        with torch.inference_mode():\n            for xb, _ in calib_loader:\n                xb = xb.to('cpu')\n                prepared(xb)\n        quantized_model = convert_fx(prepared)\n        return quantized_model\n\n    calib_bs = min(batch_size, 256)\n    calib_count = int(calibrate_steps)\n    if calib_count <= 0:\n        calib_count = 1\n    calib_indices = torch.arange(min(len(train_ds), calib_bs * calib_count))\n    X_calib = train_ds.X[calib_indices]\n    y_calib = train_ds.y[calib_indices]\n    calib_loader = DataLoader(TensorDatasetECG(X_calib, y_calib), batch_size=calib_bs, shuffle=False, num_workers=0, pin_memory=False)\n\n    try:\n        ex_xb, _ = next(iter(calib_loader))\n    except StopIteration:\n        ex_xb = torch.zeros(1, 2, 1000)\n    if ex_xb.ndim == 3 and ex_xb.shape[1] != 2 and ex_xb.shape[-1] == 2:\n        ex_xb = ex_xb.permute(0, 2, 1)\n\n    quantized_model = None\n    if quantize_weights and quantization_bits == 8:\n        try:\n            quantized_model = _quantize_int8_fx_local(model.to('cpu'), calib_loader, example_input=(ex_xb,), quantize_activations=bool(quantize_activations))\n        except Exception:\n            # Fallback to fp32 if quantization APIs are unavailable\n            quantized_model = copy.deepcopy(model).to('cpu')\n    elif quantize_weights and quantization_bits == 16:\n        quantized_model = copy.deepcopy(model).to('cpu').half()\n    else:\n        quantized_model = copy.deepcopy(model).to('cpu')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return quantized_model, metrics\n"}
2025-09-25 16:55:10,837 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-25 16:55:10,837 - ERROR - _models.training_function_executor - BO training objective failed: cannot import name 'prepare_fx' from 'torch.ao.quantization.fx' (/home/shiyuanduan/anaconda3/envs/GPT/lib/python3.10/site-packages/torch/ao/quantization/fx/__init__.py)
2025-09-25 16:55:10,837 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 76.116s
2025-09-25 16:55:10,837 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: cannot import name 'prepare_fx' from 'torch.ao.quantization.fx' (/home/shiyuanduan/anaconda3/envs/GPT/lib/python3.10/site-packages/torch/ao/quantization/fx/__init__.py)
2025-09-25 16:55:10,837 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-25 16:55:13,840 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-25 16:55:13,840 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-09-25 16:55:13,840 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:55:13,841 - INFO - _models.training_function_executor - Loaded training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:55:13,841 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-25 16:55:13,841 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-09-25 16:55:13,841 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:55:13,841 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:55:13,841 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-25 16:55:13,841 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 1/4
2025-09-25 16:55:13,841 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 1: üì¶ Installing dependencies for GPT-generated training code...
2025-09-25 16:55:13,841 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-25 16:55:13,842 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-25 16:55:13,842 - INFO - package_installer - Available packages: {'torch'}
2025-09-25 16:55:13,842 - INFO - package_installer - Missing packages: set()
2025-09-25 16:55:13,842 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-25 16:55:13,842 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-25 16:55:13,842 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:55:13,842 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-25 16:55:13,842 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'label_smoothing', 'grad_clip_norm', 'use_scheduler', 'channels_base', 'se_reduction', 'dilation_b1', 'dilation_b2', 'dilation_b3', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_steps', 'seed']
2025-09-25 16:55:13,842 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-25 16:55:13,842 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:55:13,842 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-25 16:55:13,877 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-25 16:55:13,909 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-25 16:55:13,909 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-25 16:55:13,909 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-25 16:55:13,910 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-25 16:55:13,910 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-25 16:55:13,910 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 16:55:13,910 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:55:13,910 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}
2025-09-25 16:55:13,910 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}
2025-09-25 16:55:13,921 - ERROR - _models.training_function_executor - Training execution failed: name 'TensorDatasetECG' is not defined
2025-09-25 16:55:13,921 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-25 16:55:13,921 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-25 16:55:13,921 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-25 16:55:13,921 - INFO - _models.ai_code_generator - Prompt length: 9003 characters
2025-09-25 16:55:13,921 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-25 16:55:13,921 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-25 16:55:13,921 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-25 16:56:41,765 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-25 16:56:41,766 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-25 16:56:41,766 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250925_165641_attempt1.txt
2025-09-25 16:56:41,766 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train, y_train, X_val, y_val, device,\n    lr=1e-3, batch_size=256, epochs=20, weight_decay=1e-4,\n    dropout=0.1, label_smoothing=0.1, grad_clip_norm=1.0, use_scheduler=True,\n    channels_base=16, se_reduction=8, dilation_b1=1, dilation_b2=2, dilation_b3=1,\n    quantization_bits=8, quantize_weights=True, quantize_activations=True, calibrate_steps=64, seed=42\n):\n    import copy\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, Dataset\n    from torch.ao.quantization import get_default_qconfig, QConfigMapping\n\n    class TensorDatasetECG(Dataset):\n        def __init__(self, X, y):\n            X_t = torch.as_tensor(X)\n            y_t = torch.as_tensor(y)\n            if X_t.dtype != torch.float32:\n                X_t = X_t.float()\n            # Ensure [N, C, L] when possible; expect C=2 for ECGNet1D(in_ch=2)\n            if X_t.ndim == 3 and X_t.shape[1] != 2 and X_t.shape[-1] == 2:\n                # Convert [N, L, 2] -> [N, 2, L]\n                X_t = X_t.permute(0, 2, 1).contiguous()\n            elif X_t.ndim == 2:\n                # If missing channel dim, add one channel\n                X_t = X_t.unsqueeze(1)\n            if y_t.dtype != torch.long:\n                y_t = y_t.long()\n            self.X = X_t\n            self.y = y_t\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            return self.X[idx], self.y[idx]\n\n    device = torch.device(device)\n    if device.type != 'cuda' or not torch.cuda.is_available():\n        raise RuntimeError('CUDA device is required. Please pass device=\"cuda\" and ensure a GPU is available.')\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n    num_classes = 5\n\n    train_ds = TensorDatasetECG(X_train, y_train)\n    val_ds = TensorDatasetECG(X_val, y_val)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False, drop_last=False)\n\n    model = ECGNet1D(\n        in_ch=2, base_ch=int(channels_base), k_init=7, k_mid=5, k_small=3,\n        d_b1=int(dilation_b1), d_b2=int(dilation_b2), d_b3=int(dilation_b3),\n        dropout=float(dropout), se_reduction=int(se_reduction), n_classes=num_classes\n    ).to(device)\n\n    class_weights = _make_class_weights(y_train, num_classes, device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(label_smoothing)).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=float(lr) * 0.1) if use_scheduler else None\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running_loss = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            seen += xb.size(0)\n        train_loss = running_loss / max(1, seen)\n\n        val_loss, val_acc = _evaluate(model, val_loader, device, criterion)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}/{int(epochs):03d} - train_loss: {train_loss:.6f} - val_loss: {val_loss:.6f} - val_acc: {val_acc:.4f}')\n\n    # Local helper using the correct FX import path for PyTorch >= 2.0\n    def _quantize_int8_fx_local(model_fp32, calib_loader, example_input, quantize_activations=True):\n        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n        model_cpu = copy.deepcopy(model_fp32).to('cpu').eval()\n        torch.backends.quantized.engine = 'fbgemm'\n        qconfig = get_default_qconfig('fbgemm')\n        if not quantize_activations:\n            return model_cpu\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        prepared = prepare_fx(model_cpu, qconfig_mapping, example_inputs=example_input)\n        with torch.inference_mode():\n            for xb, _ in calib_loader:\n                xb = xb.to('cpu')\n                prepared(xb)\n        quantized_model = convert_fx(prepared)\n        return quantized_model\n\n    calib_bs = min(batch_size, 256)\n    calib_count = int(calibrate_steps)\n    if calib_count <= 0:\n        calib_count = 1\n    calib_indices = torch.arange(min(len(train_ds), calib_bs * calib_count))\n    X_calib = train_ds.X[calib_indices]\n    y_calib = train_ds.y[calib_indices]\n    calib_loader = DataLoader(TensorDatasetECG(X_calib, y_calib), batch_size=calib_bs, shuffle=False, num_workers=0, pin_memory=False)\n\n    try:\n        ex_xb, _ = next(iter(calib_loader))\n    except StopIteration:\n        ex_xb = torch.zeros(1, 2, 1000)\n    if ex_xb.ndim == 3 and ex_xb.shape[1] != 2 and ex_xb.shape[-1] == 2:\n        ex_xb = ex_xb.permute(0, 2, 1)\n\n    quantized_model = None\n    if quantize_weights and quantization_bits == 8:\n        try:\n            quantized_model = _quantize_int8_fx_local(model.to('cpu'), calib_loader, example_input=(ex_xb,), quantize_activations=bool(quantize_activations))\n        except Exception:\n            # Fallback to fp32 if quantization APIs are unavailable\n            quantized_model = copy.deepcopy(model).to('cpu')\n    elif quantize_weights and quantization_bits == 16:\n        quantized_model = copy.deepcopy(model).to('cpu').half()\n    else:\n        quantized_model = copy.deepcopy(model).to('cpu')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return quantized_model, metrics\n"}
2025-09-25 16:56:41,766 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-25 16:56:41,766 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train, y_train, X_val, y_val, device,\n    lr=1e-3, batch_size=256, epochs=20, weight_decay=1e-4,\n    dropout=0.1, label_smoothing=0.1, grad_clip_norm=1.0, use_scheduler=True,\n    channels_base=16, se_reduction=8, dilation_b1=1, dilation_b2=2, dilation_b3=1,\n    quantization_bits=8, quantize_weights=True, quantize_activations=True, calibrate_steps=64, seed=42\n):\n    import copy\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, Dataset\n    from torch.ao.quantization import get_default_qconfig, QConfigMapping\n\n    class TensorDatasetECG(Dataset):\n        def __init__(self, X, y):\n            X_t = torch.as_tensor(X)\n            y_t = torch.as_tensor(y)\n            if X_t.dtype != torch.float32:\n                X_t = X_t.float()\n            # Ensure [N, C, L] when possible; expect C=2 for ECGNet1D(in_ch=2)\n            if X_t.ndim == 3 and X_t.shape[1] != 2 and X_t.shape[-1] == 2:\n                # Convert [N, L, 2] -> [N, 2, L]\n                X_t = X_t.permute(0, 2, 1).contiguous()\n            elif X_t.ndim == 2:\n                # If missing channel dim, add one channel\n                X_t = X_t.unsqueeze(1)\n            if y_t.dtype != torch.long:\n                y_t = y_t.long()\n            self.X = X_t\n            self.y = y_t\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            return self.X[idx], self.y[idx]\n\n    device = torch.device(device)\n    if device.type != 'cuda' or not torch.cuda.is_available():\n        raise RuntimeError('CUDA device is required. Please pass device=\"cuda\" and ensure a GPU is available.')\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n    num_classes = 5\n\n    train_ds = TensorDatasetECG(X_train, y_train)\n    val_ds = TensorDatasetECG(X_val, y_val)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False, drop_last=False)\n\n    model = ECGNet1D(\n        in_ch=2, base_ch=int(channels_base), k_init=7, k_mid=5, k_small=3,\n        d_b1=int(dilation_b1), d_b2=int(dilation_b2), d_b3=int(dilation_b3),\n        dropout=float(dropout), se_reduction=int(se_reduction), n_classes=num_classes\n    ).to(device)\n\n    class_weights = _make_class_weights(y_train, num_classes, device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(label_smoothing)).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=float(lr) * 0.1) if use_scheduler else None\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running_loss = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            seen += xb.size(0)\n        train_loss = running_loss / max(1, seen)\n\n        val_loss, val_acc = _evaluate(model, val_loader, device, criterion)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}/{int(epochs):03d} - train_loss: {train_loss:.6f} - val_loss: {val_loss:.6f} - val_acc: {val_acc:.4f}')\n\n    # Local helper using the correct FX import path for PyTorch >= 2.0\n    def _quantize_int8_fx_local(model_fp32, calib_loader, example_input, quantize_activations=True):\n        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n        model_cpu = copy.deepcopy(model_fp32).to('cpu').eval()\n        torch.backends.quantized.engine = 'fbgemm'\n        qconfig = get_default_qconfig('fbgemm')\n        if not quantize_activations:\n            return model_cpu\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        prepared = prepare_fx(model_cpu, qconfig_mapping, example_inputs=example_input)\n        with torch.inference_mode():\n            for xb, _ in calib_loader:\n                xb = xb.to('cpu')\n                prepared(xb)\n        quantized_model = convert_fx(prepared)\n        return quantized_model\n\n    calib_bs = min(batch_size, 256)\n    calib_count = int(calibrate_steps)\n    if calib_count <= 0:\n        calib_count = 1\n    calib_indices = torch.arange(min(len(train_ds), calib_bs * calib_count))\n    X_calib = train_ds.X[calib_indices]\n    y_calib = train_ds.y[calib_indices]\n    calib_loader = DataLoader(TensorDatasetECG(X_calib, y_calib), batch_size=calib_bs, shuffle=False, num_workers=0, pin_memory=False)\n\n    try:\n        ex_xb, _ = next(iter(calib_loader))\n    except StopIteration:\n        ex_xb = torch.zeros(1, 2, 1000)\n    if ex_xb.ndim == 3 and ex_xb.shape[1] != 2 and ex_xb.shape[-1] == 2:\n        ex_xb = ex_xb.permute(0, 2, 1)\n\n    quantized_model = None\n    if quantize_weights and quantization_bits == 8:\n        try:\n            quantized_model = _quantize_int8_fx_local(model.to('cpu'), calib_loader, example_input=(ex_xb,), quantize_activations=bool(quantize_activations))\n        except Exception:\n            # Fallback to fp32 if quantization APIs are unavailable\n            quantized_model = copy.deepcopy(model).to('cpu')\n    elif quantize_weights and quantization_bits == 16:\n        quantized_model = copy.deepcopy(model).to('cpu').half()\n    else:\n        quantized_model = copy.deepcopy(model).to('cpu')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return quantized_model, metrics\n"}
2025-09-25 16:56:41,766 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-25 16:56:41,766 - ERROR - _models.training_function_executor - BO training objective failed: name 'TensorDatasetECG' is not defined
2025-09-25 16:56:41,766 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 87.857s
2025-09-25 16:56:41,766 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name 'TensorDatasetECG' is not defined
2025-09-25 16:56:41,766 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-25 16:56:44,769 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-25 16:56:44,770 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-09-25 16:56:44,770 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:56:44,770 - INFO - _models.training_function_executor - Loaded training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:56:44,770 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-25 16:56:44,770 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-09-25 16:56:44,770 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:56:44,770 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:56:44,771 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-25 16:56:44,771 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 2/4
2025-09-25 16:56:44,771 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 2: üì¶ Installing dependencies for GPT-generated training code...
2025-09-25 16:56:44,771 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-25 16:56:44,772 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-25 16:56:44,772 - INFO - package_installer - Available packages: {'torch'}
2025-09-25 16:56:44,772 - INFO - package_installer - Missing packages: set()
2025-09-25 16:56:44,772 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-25 16:56:44,772 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-25 16:56:44,772 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:56:44,772 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-25 16:56:44,772 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'label_smoothing', 'grad_clip_norm', 'use_scheduler', 'channels_base', 'se_reduction', 'dilation_b1', 'dilation_b2', 'dilation_b3', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_steps', 'seed']
2025-09-25 16:56:44,772 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-25 16:56:44,772 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:56:44,772 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-25 16:56:44,806 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-25 16:56:44,842 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-25 16:56:44,842 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-25 16:56:44,843 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-25 16:56:44,843 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-25 16:56:44,844 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-25 16:56:44,844 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 16:56:44,844 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:56:44,844 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}
2025-09-25 16:56:44,845 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}
2025-09-25 16:56:44,852 - ERROR - _models.training_function_executor - Training execution failed: name 'ECGNet1D' is not defined
2025-09-25 16:56:44,852 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-25 16:56:44,852 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-25 16:56:44,852 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-25 16:56:44,852 - INFO - _models.ai_code_generator - Prompt length: 9880 characters
2025-09-25 16:56:44,852 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-25 16:56:44,852 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-25 16:56:44,852 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-25 16:58:43,327 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-25 16:58:43,376 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-25 16:58:43,377 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250925_165843_attempt1.txt
2025-09-25 16:58:43,377 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train, y_train, X_val, y_val, device,\n    lr=1e-3, batch_size=256, epochs=20, weight_decay=1e-4,\n    dropout=0.1, label_smoothing=0.1, grad_clip_norm=1.0, use_scheduler=True,\n    channels_base=16, se_reduction=8, dilation_b1=1, dilation_b2=2, dilation_b3=1,\n    quantization_bits=8, quantize_weights=True, quantize_activations=True, calibrate_steps=64, seed=42\n):\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n    from torch.ao.quantization import get_default_qconfig, QConfigMapping\n\n    class TensorDatasetECG(Dataset):\n        def __init__(self, X, y):\n            X_t = torch.as_tensor(X)\n            y_t = torch.as_tensor(y)\n            if X_t.dtype != torch.float32:\n                X_t = X_t.float()\n            # Ensure [N, C, L] when possible; expect C=2 for ECGNet1D(in_ch=2)\n            if X_t.ndim == 3 and X_t.shape[1] != 2 and X_t.shape[-1] == 2:\n                # Convert [N, L, 2] -> [N, 2, L]\n                X_t = X_t.permute(0, 2, 1).contiguous()\n            elif X_t.ndim == 2:\n                # If missing channel dim, add one channel\n                X_t = X_t.unsqueeze(1)\n            if y_t.dtype != torch.long:\n                y_t = y_t.long()\n            self.X = X_t\n            self.y = y_t\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            return self.X[idx], self.y[idx]\n\n    def _make_class_weights(y, num_classes, device):\n        y_t = torch.as_tensor(y).long().view(-1)\n        counts = torch.bincount(y_t, minlength=num_classes).float()\n        # Inverse frequency weighting with stability\n        counts = torch.clamp(counts, min=1.0)\n        weights = counts.sum() / (counts * num_classes)\n        return weights.to(device)\n\n    def _evaluate(model, loader, device, criterion):\n        model.eval()\n        total_loss = 0.0\n        total = 0\n        correct = 0\n        with torch.inference_mode():\n            for xb, yb in loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                total_loss += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        val_loss = total_loss / max(1, total)\n        val_acc = (correct / max(1, total)) if total > 0 else 0.0\n        return val_loss, val_acc\n\n    class SqueezeExcite1D(nn.Module):\n        def __init__(self, channels, reduction=8):\n            super().__init__()\n            hidden = max(1, channels // int(reduction))\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Sequential(\n                nn.Conv1d(channels, hidden, kernel_size=1, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv1d(hidden, channels, kernel_size=1, bias=True),\n                nn.Sigmoid(),\n            )\n        def forward(self, x):\n            w = self.pool(x)\n            w = self.fc(w)\n            return x * w\n\n    class ConvBlock1D(nn.Module):\n        def __init__(self, in_ch, out_ch, k, dilation=1, dropout=0.0, se_reduction=8):\n            super().__init__()\n            pad = (k - 1) // 2 * dilation\n            self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=pad, dilation=dilation, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.relu = nn.ReLU(inplace=True)\n            self.do = nn.Dropout(p=dropout) if dropout and dropout > 0 else nn.Identity()\n            self.se = SqueezeExcite1D(out_ch, reduction=se_reduction)\n            self.residual = (in_ch == out_ch)\n        def forward(self, x):\n            out = self.conv(x)\n            out = self.bn(out)\n            out = self.relu(out)\n            out = self.do(out)\n            out = self.se(out)\n            if self.residual:\n                out = out + x\n            return out\n\n    class ECGNet1D(nn.Module):\n        def __init__(self, in_ch=2, base_ch=16, k_init=7, k_mid=5, k_small=3,\n                     d_b1=1, d_b2=2, d_b3=1, dropout=0.1, se_reduction=8, n_classes=5):\n            super().__init__()\n            pad_init = (k_init - 1) // 2\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=k_init, padding=pad_init, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.ReLU(inplace=True),\n            )\n            self.block1 = ConvBlock1D(base_ch, base_ch, k=k_mid, dilation=d_b1, dropout=dropout, se_reduction=se_reduction)\n            self.block2 = ConvBlock1D(base_ch, base_ch * 2, k=k_mid, dilation=d_b2, dropout=dropout, se_reduction=se_reduction)\n            self.block3 = ConvBlock1D(base_ch * 2, base_ch * 4, k=k_small, dilation=d_b3, dropout=dropout, se_reduction=se_reduction)\n            self.head = nn.Sequential(\n                nn.Dropout(p=dropout) if dropout and dropout > 0 else nn.Identity(),\n                nn.Linear(base_ch * 4, n_classes)\n            )\n        def forward(self, x):\n            x = self.stem(x)\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.block3(x)\n            x = F.adaptive_avg_pool1d(x, 1).squeeze(-1)\n            x = self.head(x)\n            return x\n\n    def _resolve_device(dev):\n        if isinstance(dev, torch.device):\n            if dev.type == 'cuda' and not torch.cuda.is_available():\n                return torch.device('cpu')\n            return dev\n        if isinstance(dev, str):\n            if dev.startswith('cuda') and torch.cuda.is_available():\n                return torch.device(dev)\n            return torch.device('cpu')\n        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    device = _resolve_device(device)\n    torch.manual_seed(seed)\n    if device.type == 'cuda':\n        torch.cuda.manual_seed_all(seed)\n\n    num_classes = 5\n\n    train_ds = TensorDatasetECG(X_train, y_train)\n    val_ds = TensorDatasetECG(X_val, y_val)\n    pin_mem = device.type == 'cuda'\n    train_loader = DataLoader(train_ds, batch_size=int(batch_size), shuffle=True, num_workers=0, pin_memory=pin_mem, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=int(batch_size), shuffle=False, num_workers=0, pin_memory=pin_mem, drop_last=False)\n\n    model = ECGNet1D(\n        in_ch=2, base_ch=int(channels_base), k_init=7, k_mid=5, k_small=3,\n        d_b1=int(dilation_b1), d_b2=int(dilation_b2), d_b3=int(dilation_b3),\n        dropout=float(dropout), se_reduction=int(se_reduction), n_classes=num_classes\n    ).to(device)\n\n    class_weights = _make_class_weights(y_train, num_classes, device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(label_smoothing)).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=float(lr) * 0.1) if use_scheduler else None\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running_loss = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            seen += xb.size(0)\n        train_loss = running_loss / max(1, seen)\n\n        val_loss, val_acc = _evaluate(model, val_loader, device, criterion)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}/{int(epochs):03d} - train_loss: {train_loss:.6f} - val_loss: {val_loss:.6f} - val_acc: {val_acc:.4f}')\n\n    def _quantize_int8_fx_local(model_fp32, calib_loader, example_input, quantize_activations=True):\n        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n        model_cpu = copy.deepcopy(model_fp32).to('cpu').eval()\n        torch.backends.quantized.engine = 'fbgemm'\n        qconfig = get_default_qconfig('fbgemm')\n        if not quantize_activations:\n            return model_cpu\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        prepared = prepare_fx(model_cpu, qconfig_mapping, example_inputs=example_input)\n        with torch.inference_mode():\n            for xb, _ in calib_loader:\n                xb = xb.to('cpu')\n                prepared(xb)\n        quantized_model = convert_fx(prepared)\n        return quantized_model\n\n    calib_bs = int(min(int(batch_size), 256))\n    calib_count = int(calibrate_steps)\n    if calib_count <= 0:\n        calib_count = 1\n    calib_indices = torch.arange(min(len(train_ds), calib_bs * calib_count))\n    X_calib = train_ds.X[calib_indices]\n    y_calib = train_ds.y[calib_indices]\n    calib_loader = DataLoader(TensorDatasetECG(X_calib, y_calib), batch_size=calib_bs, shuffle=False, num_workers=0, pin_memory=False)\n\n    try:\n        ex_xb, _ = next(iter(calib_loader))\n    except StopIteration:\n        ex_xb = torch.zeros(1, 2, 1000)\n    if ex_xb.ndim == 3 and ex_xb.shape[1] != 2 and ex_xb.shape[-1] == 2:\n        ex_xb = ex_xb.permute(0, 2, 1)\n\n    quantized_model = None\n    if quantize_weights and int(quantization_bits) == 8:\n        try:\n            quantized_model = _quantize_int8_fx_local(model.to('cpu'), calib_loader, example_input=(ex_xb,), quantize_activations=bool(quantize_activations))\n        except Exception:\n            quantized_model = copy.deepcopy(model).to('cpu')\n    elif quantize_weights and int(quantization_bits) == 16:\n        quantized_model = copy.deepcopy(model).to('cpu').half()\n    else:\n        quantized_model = copy.deepcopy(model).to('cpu')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return quantized_model, metrics\n"}
2025-09-25 16:58:43,377 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-25 16:58:43,377 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train, y_train, X_val, y_val, device,\n    lr=1e-3, batch_size=256, epochs=20, weight_decay=1e-4,\n    dropout=0.1, label_smoothing=0.1, grad_clip_norm=1.0, use_scheduler=True,\n    channels_base=16, se_reduction=8, dilation_b1=1, dilation_b2=2, dilation_b3=1,\n    quantization_bits=8, quantize_weights=True, quantize_activations=True, calibrate_steps=64, seed=42\n):\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n    from torch.ao.quantization import get_default_qconfig, QConfigMapping\n\n    class TensorDatasetECG(Dataset):\n        def __init__(self, X, y):\n            X_t = torch.as_tensor(X)\n            y_t = torch.as_tensor(y)\n            if X_t.dtype != torch.float32:\n                X_t = X_t.float()\n            # Ensure [N, C, L] when possible; expect C=2 for ECGNet1D(in_ch=2)\n            if X_t.ndim == 3 and X_t.shape[1] != 2 and X_t.shape[-1] == 2:\n                # Convert [N, L, 2] -> [N, 2, L]\n                X_t = X_t.permute(0, 2, 1).contiguous()\n            elif X_t.ndim == 2:\n                # If missing channel dim, add one channel\n                X_t = X_t.unsqueeze(1)\n            if y_t.dtype != torch.long:\n                y_t = y_t.long()\n            self.X = X_t\n            self.y = y_t\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            return self.X[idx], self.y[idx]\n\n    def _make_class_weights(y, num_classes, device):\n        y_t = torch.as_tensor(y).long().view(-1)\n        counts = torch.bincount(y_t, minlength=num_classes).float()\n        # Inverse frequency weighting with stability\n        counts = torch.clamp(counts, min=1.0)\n        weights = counts.sum() / (counts * num_classes)\n        return weights.to(device)\n\n    def _evaluate(model, loader, device, criterion):\n        model.eval()\n        total_loss = 0.0\n        total = 0\n        correct = 0\n        with torch.inference_mode():\n            for xb, yb in loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                total_loss += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        val_loss = total_loss / max(1, total)\n        val_acc = (correct / max(1, total)) if total > 0 else 0.0\n        return val_loss, val_acc\n\n    class SqueezeExcite1D(nn.Module):\n        def __init__(self, channels, reduction=8):\n            super().__init__()\n            hidden = max(1, channels // int(reduction))\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Sequential(\n                nn.Conv1d(channels, hidden, kernel_size=1, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv1d(hidden, channels, kernel_size=1, bias=True),\n                nn.Sigmoid(),\n            )\n        def forward(self, x):\n            w = self.pool(x)\n            w = self.fc(w)\n            return x * w\n\n    class ConvBlock1D(nn.Module):\n        def __init__(self, in_ch, out_ch, k, dilation=1, dropout=0.0, se_reduction=8):\n            super().__init__()\n            pad = (k - 1) // 2 * dilation\n            self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=pad, dilation=dilation, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.relu = nn.ReLU(inplace=True)\n            self.do = nn.Dropout(p=dropout) if dropout and dropout > 0 else nn.Identity()\n            self.se = SqueezeExcite1D(out_ch, reduction=se_reduction)\n            self.residual = (in_ch == out_ch)\n        def forward(self, x):\n            out = self.conv(x)\n            out = self.bn(out)\n            out = self.relu(out)\n            out = self.do(out)\n            out = self.se(out)\n            if self.residual:\n                out = out + x\n            return out\n\n    class ECGNet1D(nn.Module):\n        def __init__(self, in_ch=2, base_ch=16, k_init=7, k_mid=5, k_small=3,\n                     d_b1=1, d_b2=2, d_b3=1, dropout=0.1, se_reduction=8, n_classes=5):\n            super().__init__()\n            pad_init = (k_init - 1) // 2\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=k_init, padding=pad_init, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.ReLU(inplace=True),\n            )\n            self.block1 = ConvBlock1D(base_ch, base_ch, k=k_mid, dilation=d_b1, dropout=dropout, se_reduction=se_reduction)\n            self.block2 = ConvBlock1D(base_ch, base_ch * 2, k=k_mid, dilation=d_b2, dropout=dropout, se_reduction=se_reduction)\n            self.block3 = ConvBlock1D(base_ch * 2, base_ch * 4, k=k_small, dilation=d_b3, dropout=dropout, se_reduction=se_reduction)\n            self.head = nn.Sequential(\n                nn.Dropout(p=dropout) if dropout and dropout > 0 else nn.Identity(),\n                nn.Linear(base_ch * 4, n_classes)\n            )\n        def forward(self, x):\n            x = self.stem(x)\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.block3(x)\n            x = F.adaptive_avg_pool1d(x, 1).squeeze(-1)\n            x = self.head(x)\n            return x\n\n    def _resolve_device(dev):\n        if isinstance(dev, torch.device):\n            if dev.type == 'cuda' and not torch.cuda.is_available():\n                return torch.device('cpu')\n            return dev\n        if isinstance(dev, str):\n            if dev.startswith('cuda') and torch.cuda.is_available():\n                return torch.device(dev)\n            return torch.device('cpu')\n        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    device = _resolve_device(device)\n    torch.manual_seed(seed)\n    if device.type == 'cuda':\n        torch.cuda.manual_seed_all(seed)\n\n    num_classes = 5\n\n    train_ds = TensorDatasetECG(X_train, y_train)\n    val_ds = TensorDatasetECG(X_val, y_val)\n    pin_mem = device.type == 'cuda'\n    train_loader = DataLoader(train_ds, batch_size=int(batch_size), shuffle=True, num_workers=0, pin_memory=pin_mem, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=int(batch_size), shuffle=False, num_workers=0, pin_memory=pin_mem, drop_last=False)\n\n    model = ECGNet1D(\n        in_ch=2, base_ch=int(channels_base), k_init=7, k_mid=5, k_small=3,\n        d_b1=int(dilation_b1), d_b2=int(dilation_b2), d_b3=int(dilation_b3),\n        dropout=float(dropout), se_reduction=int(se_reduction), n_classes=num_classes\n    ).to(device)\n\n    class_weights = _make_class_weights(y_train, num_classes, device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(label_smoothing)).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=float(lr) * 0.1) if use_scheduler else None\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running_loss = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            seen += xb.size(0)\n        train_loss = running_loss / max(1, seen)\n\n        val_loss, val_acc = _evaluate(model, val_loader, device, criterion)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}/{int(epochs):03d} - train_loss: {train_loss:.6f} - val_loss: {val_loss:.6f} - val_acc: {val_acc:.4f}')\n\n    def _quantize_int8_fx_local(model_fp32, calib_loader, example_input, quantize_activations=True):\n        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n        model_cpu = copy.deepcopy(model_fp32).to('cpu').eval()\n        torch.backends.quantized.engine = 'fbgemm'\n        qconfig = get_default_qconfig('fbgemm')\n        if not quantize_activations:\n            return model_cpu\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        prepared = prepare_fx(model_cpu, qconfig_mapping, example_inputs=example_input)\n        with torch.inference_mode():\n            for xb, _ in calib_loader:\n                xb = xb.to('cpu')\n                prepared(xb)\n        quantized_model = convert_fx(prepared)\n        return quantized_model\n\n    calib_bs = int(min(int(batch_size), 256))\n    calib_count = int(calibrate_steps)\n    if calib_count <= 0:\n        calib_count = 1\n    calib_indices = torch.arange(min(len(train_ds), calib_bs * calib_count))\n    X_calib = train_ds.X[calib_indices]\n    y_calib = train_ds.y[calib_indices]\n    calib_loader = DataLoader(TensorDatasetECG(X_calib, y_calib), batch_size=calib_bs, shuffle=False, num_workers=0, pin_memory=False)\n\n    try:\n        ex_xb, _ = next(iter(calib_loader))\n    except StopIteration:\n        ex_xb = torch.zeros(1, 2, 1000)\n    if ex_xb.ndim == 3 and ex_xb.shape[1] != 2 and ex_xb.shape[-1] == 2:\n        ex_xb = ex_xb.permute(0, 2, 1)\n\n    quantized_model = None\n    if quantize_weights and int(quantization_bits) == 8:\n        try:\n            quantized_model = _quantize_int8_fx_local(model.to('cpu'), calib_loader, example_input=(ex_xb,), quantize_activations=bool(quantize_activations))\n        except Exception:\n            quantized_model = copy.deepcopy(model).to('cpu')\n    elif quantize_weights and int(quantization_bits) == 16:\n        quantized_model = copy.deepcopy(model).to('cpu').half()\n    else:\n        quantized_model = copy.deepcopy(model).to('cpu')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return quantized_model, metrics\n"}
2025-09-25 16:58:43,377 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-25 16:58:43,377 - ERROR - _models.training_function_executor - BO training objective failed: name 'ECGNet1D' is not defined
2025-09-25 16:58:43,377 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 118.533s
2025-09-25 16:58:43,377 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name 'ECGNet1D' is not defined
2025-09-25 16:58:43,377 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-25 16:58:46,380 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-25 16:58:46,380 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-09-25 16:58:46,380 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:58:46,381 - INFO - _models.training_function_executor - Loaded training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:58:46,381 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-25 16:58:46,381 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-09-25 16:58:46,381 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:58:46,381 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:58:46,381 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-25 16:58:46,381 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 3/4
2025-09-25 16:58:46,381 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 3: üì¶ Installing dependencies for GPT-generated training code...
2025-09-25 16:58:46,381 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-25 16:58:46,383 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-25 16:58:46,383 - INFO - package_installer - Available packages: {'torch'}
2025-09-25 16:58:46,383 - INFO - package_installer - Missing packages: set()
2025-09-25 16:58:46,383 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-25 16:58:46,383 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-25 16:58:46,383 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:58:46,383 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-25 16:58:46,383 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'label_smoothing', 'grad_clip_norm', 'use_scheduler', 'channels_base', 'se_reduction', 'dilation_b1', 'dilation_b2', 'dilation_b3', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_steps', 'seed']
2025-09-25 16:58:46,383 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-25 16:58:46,383 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:58:46,383 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-25 16:58:46,418 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-25 16:58:46,454 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-25 16:58:46,454 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-25 16:58:46,454 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-25 16:58:46,455 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-25 16:58:46,455 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 16:58:46,455 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 16:58:46,455 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:58:46,455 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}
2025-09-25 16:58:46,456 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}
2025-09-25 16:58:46,905 - ERROR - _models.training_function_executor - Training execution failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-25 16:58:46,905 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-25 16:58:46,905 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-25 16:58:46,905 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-25 16:58:46,905 - INFO - _models.ai_code_generator - Prompt length: 14069 characters
2025-09-25 16:58:46,905 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-25 16:58:46,906 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-25 16:58:46,906 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-25 16:59:49,574 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-25 16:59:49,575 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-25 16:59:49,575 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250925_165949_attempt1.txt
2025-09-25 16:59:49,575 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train, y_train, X_val, y_val, device,\n    lr=1e-3, batch_size=256, epochs=20, weight_decay=1e-4,\n    dropout=0.1, label_smoothing=0.1, grad_clip_norm=1.0, use_scheduler=True,\n    channels_base=16, se_reduction=8, dilation_b1=1, dilation_b2=2, dilation_b3=1,\n    quantization_bits=8, quantize_weights=True, quantize_activations=True, calibrate_steps=64, seed=42\n):\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n    try:\n        from torch.ao.quantization import get_default_qconfig\n        try:\n            from torch.ao.quantization import QConfigMapping\n        except Exception:\n            from torch.ao.quantization.qconfig_mapping import QConfigMapping\n    except Exception:\n        from torch.ao.quantization import get_default_qconfig\n        from torch.ao.quantization.qconfig_mapping import QConfigMapping\n\n    class TensorDatasetECG(Dataset):\n        def __init__(self, X, y):\n            X_t = torch.as_tensor(X)\n            y_t = torch.as_tensor(y)\n            # Ensure dataset tensors are on CPU so DataLoader pin_memory works correctly\n            if X_t.is_cuda:\n                X_t = X_t.detach().cpu()\n            if y_t.is_cuda:\n                y_t = y_t.detach().cpu()\n            if X_t.dtype != torch.float32:\n                X_t = X_t.float()\n            # Ensure [N, C, L] when possible; expect C=2 for ECGNet1D(in_ch=2)\n            if X_t.ndim == 3 and X_t.shape[1] != 2 and X_t.shape[-1] == 2:\n                # Convert [N, L, 2] -> [N, 2, L]\n                X_t = X_t.permute(0, 2, 1).contiguous()\n            elif X_t.ndim == 2:\n                # If missing channel dim, add one channel\n                X_t = X_t.unsqueeze(1)\n            if y_t.dtype != torch.long:\n                y_t = y_t.long()\n            self.X = X_t.contiguous()\n            self.y = y_t.contiguous()\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            return self.X[idx], self.y[idx]\n\n    def _make_class_weights(y, num_classes, device):\n        y_t = torch.as_tensor(y).long().view(-1)\n        counts = torch.bincount(y_t, minlength=num_classes).float()\n        # Inverse frequency weighting with stability\n        counts = torch.clamp(counts, min=1.0)\n        weights = counts.sum() / (counts * num_classes)\n        return weights.to(device)\n\n    def _evaluate(model, loader, device, criterion):\n        model.eval()\n        total_loss = 0.0\n        total = 0\n        correct = 0\n        with torch.inference_mode():\n            for xb, yb in loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                total_loss += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        val_loss = total_loss / max(1, total)\n        val_acc = (correct / max(1, total)) if total > 0 else 0.0\n        return val_loss, val_acc\n\n    class SqueezeExcite1D(nn.Module):\n        def __init__(self, channels, reduction=8):\n            super().__init__()\n            hidden = max(1, channels // int(reduction))\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Sequential(\n                nn.Conv1d(channels, hidden, kernel_size=1, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv1d(hidden, channels, kernel_size=1, bias=True),\n                nn.Sigmoid(),\n            )\n        def forward(self, x):\n            w = self.pool(x)\n            w = self.fc(w)\n            return x * w\n\n    class ConvBlock1D(nn.Module):\n        def __init__(self, in_ch, out_ch, k, dilation=1, dropout=0.0, se_reduction=8):\n            super().__init__()\n            pad = (k - 1) // 2 * dilation\n            self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=pad, dilation=dilation, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.relu = nn.ReLU(inplace=True)\n            self.do = nn.Dropout(p=dropout) if dropout and dropout > 0 else nn.Identity()\n            self.se = SqueezeExcite1D(out_ch, reduction=se_reduction)\n            self.residual = (in_ch == out_ch)\n        def forward(self, x):\n            out = self.conv(x)\n            out = self.bn(out)\n            out = self.relu(out)\n            out = self.do(out)\n            out = self.se(out)\n            if self.residual:\n                out = out + x\n            return out\n\n    class ECGNet1D(nn.Module):\n        def __init__(self, in_ch=2, base_ch=16, k_init=7, k_mid=5, k_small=3,\n                     d_b1=1, d_b2=2, d_b3=1, dropout=0.1, se_reduction=8, n_classes=5):\n            super().__init__()\n            pad_init = (k_init - 1) // 2\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=k_init, padding=pad_init, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.ReLU(inplace=True),\n            )\n            self.block1 = ConvBlock1D(base_ch, base_ch, k=k_mid, dilation=d_b1, dropout=dropout, se_reduction=se_reduction)\n            self.block2 = ConvBlock1D(base_ch, base_ch * 2, k=k_mid, dilation=d_b2, dropout=dropout, se_reduction=se_reduction)\n            self.block3 = ConvBlock1D(base_ch * 2, base_ch * 4, k=k_small, dilation=d_b3, dropout=dropout, se_reduction=se_reduction)\n            self.head = nn.Sequential(\n                nn.Dropout(p=dropout) if dropout and dropout > 0 else nn.Identity(),\n                nn.Linear(base_ch * 4, n_classes)\n            )\n        def forward(self, x):\n            x = self.stem(x)\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.block3(x)\n            x = F.adaptive_avg_pool1d(x, 1).squeeze(-1)\n            x = self.head(x)\n            return x\n\n    def _resolve_device(dev):\n        if isinstance(dev, torch.device):\n            if dev.type == 'cuda' and not torch.cuda.is_available():\n                return torch.device('cpu')\n            return dev\n        if isinstance(dev, str):\n            if dev.startswith('cuda') and torch.cuda.is_available():\n                return torch.device(dev)\n            return torch.device('cpu')\n        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    device = _resolve_device(device)\n    torch.manual_seed(seed)\n    if device.type == 'cuda':\n        torch.cuda.manual_seed_all(seed)\n\n    num_classes = 5\n\n    train_ds = TensorDatasetECG(X_train, y_train)\n    val_ds = TensorDatasetECG(X_val, y_val)\n    pin_mem = device.type == 'cuda'\n    train_loader = DataLoader(train_ds, batch_size=int(batch_size), shuffle=True, num_workers=0, pin_memory=pin_mem, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=int(batch_size), shuffle=False, num_workers=0, pin_memory=pin_mem, drop_last=False)\n\n    model = ECGNet1D(\n        in_ch=2, base_ch=int(channels_base), k_init=7, k_mid=5, k_small=3,\n        d_b1=int(dilation_b1), d_b2=int(dilation_b2), d_b3=int(dilation_b3),\n        dropout=float(dropout), se_reduction=int(se_reduction), n_classes=num_classes\n    ).to(device)\n\n    class_weights = _make_class_weights(y_train, num_classes, device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(label_smoothing)).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=float(lr) * 0.1) if use_scheduler else None\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running_loss = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            seen += xb.size(0)\n        train_loss = running_loss / max(1, seen)\n\n        val_loss, val_acc = _evaluate(model, val_loader, device, criterion)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}/{int(epochs):03d} - train_loss: {train_loss:.6f} - val_loss: {val_loss:.6f} - val_acc: {val_acc:.4f}')\n\n    def _quantize_int8_fx_local(model_fp32, calib_loader, example_input, quantize_activations=True):\n        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n        model_cpu = copy.deepcopy(model_fp32).to('cpu').eval()\n        torch.backends.quantized.engine = 'fbgemm'\n        qconfig = get_default_qconfig('fbgemm')\n        if not quantize_activations:\n            return model_cpu\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        prepared = prepare_fx(model_cpu, qconfig_mapping, example_inputs=example_input)\n        with torch.inference_mode():\n            for xb, _ in calib_loader:\n                xb = xb.to('cpu')\n                prepared(xb)\n        quantized_model = convert_fx(prepared)\n        return quantized_model\n\n    calib_bs = int(min(int(batch_size), 256))\n    calib_count = int(calibrate_steps)\n    if calib_count <= 0:\n        calib_count = 1\n    calib_indices = torch.arange(min(len(train_ds), calib_bs * calib_count))\n    X_calib = train_ds.X[calib_indices]\n    y_calib = train_ds.y[calib_indices]\n    calib_loader = DataLoader(TensorDatasetECG(X_calib, y_calib), batch_size=calib_bs, shuffle=False, num_workers=0, pin_memory=False)\n\n    try:\n        ex_xb, _ = next(iter(calib_loader))\n    except StopIteration:\n        ex_xb = torch.zeros(1, 2, 1000)\n    if ex_xb.ndim == 3 and ex_xb.shape[1] != 2 and ex_xb.shape[-1] == 2:\n        ex_xb = ex_xb.permute(0, 2, 1)\n\n    quantized_model = None\n    if quantize_weights and int(quantization_bits) == 8:\n        try:\n            quantized_model = _quantize_int8_fx_local(model.to('cpu'), calib_loader, example_input=(ex_xb,), quantize_activations=bool(quantize_activations))\n        except Exception:\n            quantized_model = copy.deepcopy(model).to('cpu')\n    elif quantize_weights and int(quantization_bits) == 16:\n        quantized_model = copy.deepcopy(model).to('cpu').half()\n    else:\n        quantized_model = copy.deepcopy(model).to('cpu')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return quantized_model, metrics\n"}
2025-09-25 16:59:49,575 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-25 16:59:49,575 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train, y_train, X_val, y_val, device,\n    lr=1e-3, batch_size=256, epochs=20, weight_decay=1e-4,\n    dropout=0.1, label_smoothing=0.1, grad_clip_norm=1.0, use_scheduler=True,\n    channels_base=16, se_reduction=8, dilation_b1=1, dilation_b2=2, dilation_b3=1,\n    quantization_bits=8, quantize_weights=True, quantize_activations=True, calibrate_steps=64, seed=42\n):\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n    try:\n        from torch.ao.quantization import get_default_qconfig\n        try:\n            from torch.ao.quantization import QConfigMapping\n        except Exception:\n            from torch.ao.quantization.qconfig_mapping import QConfigMapping\n    except Exception:\n        from torch.ao.quantization import get_default_qconfig\n        from torch.ao.quantization.qconfig_mapping import QConfigMapping\n\n    class TensorDatasetECG(Dataset):\n        def __init__(self, X, y):\n            X_t = torch.as_tensor(X)\n            y_t = torch.as_tensor(y)\n            # Ensure dataset tensors are on CPU so DataLoader pin_memory works correctly\n            if X_t.is_cuda:\n                X_t = X_t.detach().cpu()\n            if y_t.is_cuda:\n                y_t = y_t.detach().cpu()\n            if X_t.dtype != torch.float32:\n                X_t = X_t.float()\n            # Ensure [N, C, L] when possible; expect C=2 for ECGNet1D(in_ch=2)\n            if X_t.ndim == 3 and X_t.shape[1] != 2 and X_t.shape[-1] == 2:\n                # Convert [N, L, 2] -> [N, 2, L]\n                X_t = X_t.permute(0, 2, 1).contiguous()\n            elif X_t.ndim == 2:\n                # If missing channel dim, add one channel\n                X_t = X_t.unsqueeze(1)\n            if y_t.dtype != torch.long:\n                y_t = y_t.long()\n            self.X = X_t.contiguous()\n            self.y = y_t.contiguous()\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            return self.X[idx], self.y[idx]\n\n    def _make_class_weights(y, num_classes, device):\n        y_t = torch.as_tensor(y).long().view(-1)\n        counts = torch.bincount(y_t, minlength=num_classes).float()\n        # Inverse frequency weighting with stability\n        counts = torch.clamp(counts, min=1.0)\n        weights = counts.sum() / (counts * num_classes)\n        return weights.to(device)\n\n    def _evaluate(model, loader, device, criterion):\n        model.eval()\n        total_loss = 0.0\n        total = 0\n        correct = 0\n        with torch.inference_mode():\n            for xb, yb in loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                total_loss += loss.item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        val_loss = total_loss / max(1, total)\n        val_acc = (correct / max(1, total)) if total > 0 else 0.0\n        return val_loss, val_acc\n\n    class SqueezeExcite1D(nn.Module):\n        def __init__(self, channels, reduction=8):\n            super().__init__()\n            hidden = max(1, channels // int(reduction))\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Sequential(\n                nn.Conv1d(channels, hidden, kernel_size=1, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv1d(hidden, channels, kernel_size=1, bias=True),\n                nn.Sigmoid(),\n            )\n        def forward(self, x):\n            w = self.pool(x)\n            w = self.fc(w)\n            return x * w\n\n    class ConvBlock1D(nn.Module):\n        def __init__(self, in_ch, out_ch, k, dilation=1, dropout=0.0, se_reduction=8):\n            super().__init__()\n            pad = (k - 1) // 2 * dilation\n            self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=pad, dilation=dilation, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.relu = nn.ReLU(inplace=True)\n            self.do = nn.Dropout(p=dropout) if dropout and dropout > 0 else nn.Identity()\n            self.se = SqueezeExcite1D(out_ch, reduction=se_reduction)\n            self.residual = (in_ch == out_ch)\n        def forward(self, x):\n            out = self.conv(x)\n            out = self.bn(out)\n            out = self.relu(out)\n            out = self.do(out)\n            out = self.se(out)\n            if self.residual:\n                out = out + x\n            return out\n\n    class ECGNet1D(nn.Module):\n        def __init__(self, in_ch=2, base_ch=16, k_init=7, k_mid=5, k_small=3,\n                     d_b1=1, d_b2=2, d_b3=1, dropout=0.1, se_reduction=8, n_classes=5):\n            super().__init__()\n            pad_init = (k_init - 1) // 2\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=k_init, padding=pad_init, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.ReLU(inplace=True),\n            )\n            self.block1 = ConvBlock1D(base_ch, base_ch, k=k_mid, dilation=d_b1, dropout=dropout, se_reduction=se_reduction)\n            self.block2 = ConvBlock1D(base_ch, base_ch * 2, k=k_mid, dilation=d_b2, dropout=dropout, se_reduction=se_reduction)\n            self.block3 = ConvBlock1D(base_ch * 2, base_ch * 4, k=k_small, dilation=d_b3, dropout=dropout, se_reduction=se_reduction)\n            self.head = nn.Sequential(\n                nn.Dropout(p=dropout) if dropout and dropout > 0 else nn.Identity(),\n                nn.Linear(base_ch * 4, n_classes)\n            )\n        def forward(self, x):\n            x = self.stem(x)\n            x = self.block1(x)\n            x = self.block2(x)\n            x = self.block3(x)\n            x = F.adaptive_avg_pool1d(x, 1).squeeze(-1)\n            x = self.head(x)\n            return x\n\n    def _resolve_device(dev):\n        if isinstance(dev, torch.device):\n            if dev.type == 'cuda' and not torch.cuda.is_available():\n                return torch.device('cpu')\n            return dev\n        if isinstance(dev, str):\n            if dev.startswith('cuda') and torch.cuda.is_available():\n                return torch.device(dev)\n            return torch.device('cpu')\n        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    device = _resolve_device(device)\n    torch.manual_seed(seed)\n    if device.type == 'cuda':\n        torch.cuda.manual_seed_all(seed)\n\n    num_classes = 5\n\n    train_ds = TensorDatasetECG(X_train, y_train)\n    val_ds = TensorDatasetECG(X_val, y_val)\n    pin_mem = device.type == 'cuda'\n    train_loader = DataLoader(train_ds, batch_size=int(batch_size), shuffle=True, num_workers=0, pin_memory=pin_mem, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=int(batch_size), shuffle=False, num_workers=0, pin_memory=pin_mem, drop_last=False)\n\n    model = ECGNet1D(\n        in_ch=2, base_ch=int(channels_base), k_init=7, k_mid=5, k_small=3,\n        d_b1=int(dilation_b1), d_b2=int(dilation_b2), d_b3=int(dilation_b3),\n        dropout=float(dropout), se_reduction=int(se_reduction), n_classes=num_classes\n    ).to(device)\n\n    class_weights = _make_class_weights(y_train, num_classes, device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(label_smoothing)).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=float(lr) * 0.1) if use_scheduler else None\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(1, int(epochs) + 1):\n        model.train()\n        running_loss = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip_norm and grad_clip_norm > 0.0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip_norm))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            seen += xb.size(0)\n        train_loss = running_loss / max(1, seen)\n\n        val_loss, val_acc = _evaluate(model, val_loader, device, criterion)\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}/{int(epochs):03d} - train_loss: {train_loss:.6f} - val_loss: {val_loss:.6f} - val_acc: {val_acc:.4f}')\n\n    def _quantize_int8_fx_local(model_fp32, calib_loader, example_input, quantize_activations=True):\n        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n        model_cpu = copy.deepcopy(model_fp32).to('cpu').eval()\n        torch.backends.quantized.engine = 'fbgemm'\n        qconfig = get_default_qconfig('fbgemm')\n        if not quantize_activations:\n            return model_cpu\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        prepared = prepare_fx(model_cpu, qconfig_mapping, example_inputs=example_input)\n        with torch.inference_mode():\n            for xb, _ in calib_loader:\n                xb = xb.to('cpu')\n                prepared(xb)\n        quantized_model = convert_fx(prepared)\n        return quantized_model\n\n    calib_bs = int(min(int(batch_size), 256))\n    calib_count = int(calibrate_steps)\n    if calib_count <= 0:\n        calib_count = 1\n    calib_indices = torch.arange(min(len(train_ds), calib_bs * calib_count))\n    X_calib = train_ds.X[calib_indices]\n    y_calib = train_ds.y[calib_indices]\n    calib_loader = DataLoader(TensorDatasetECG(X_calib, y_calib), batch_size=calib_bs, shuffle=False, num_workers=0, pin_memory=False)\n\n    try:\n        ex_xb, _ = next(iter(calib_loader))\n    except StopIteration:\n        ex_xb = torch.zeros(1, 2, 1000)\n    if ex_xb.ndim == 3 and ex_xb.shape[1] != 2 and ex_xb.shape[-1] == 2:\n        ex_xb = ex_xb.permute(0, 2, 1)\n\n    quantized_model = None\n    if quantize_weights and int(quantization_bits) == 8:\n        try:\n            quantized_model = _quantize_int8_fx_local(model.to('cpu'), calib_loader, example_input=(ex_xb,), quantize_activations=bool(quantize_activations))\n        except Exception:\n            quantized_model = copy.deepcopy(model).to('cpu')\n    elif quantize_weights and int(quantization_bits) == 16:\n        quantized_model = copy.deepcopy(model).to('cpu').half()\n    else:\n        quantized_model = copy.deepcopy(model).to('cpu')\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return quantized_model, metrics\n"}
2025-09-25 16:59:49,575 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-25 16:59:49,575 - ERROR - _models.training_function_executor - BO training objective failed: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-25 16:59:49,575 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 63.120s
2025-09-25 16:59:49,576 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
2025-09-25 16:59:49,576 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-25 16:59:52,578 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-25 16:59:52,578 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-09-25 16:59:52,578 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:59:52,578 - INFO - _models.training_function_executor - Loaded training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:59:52,578 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-25 16:59:52,578 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-09-25 16:59:52,579 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_TwoLead1D-CNN-SE-Residual-Dilated__MIT-BIH_5-class_1758837234.json
2025-09-25 16:59:52,579 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:59:52,579 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-25 16:59:52,579 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 4/4
2025-09-25 16:59:52,579 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 4: üì¶ Installing dependencies for GPT-generated training code...
2025-09-25 16:59:52,579 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-25 16:59:52,581 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-25 16:59:52,581 - INFO - package_installer - Available packages: {'torch'}
2025-09-25 16:59:52,581 - INFO - package_installer - Missing packages: set()
2025-09-25 16:59:52,581 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-25 16:59:52,581 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-25 16:59:52,581 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:59:52,581 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-25 16:59:52,581 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'label_smoothing', 'grad_clip_norm', 'use_scheduler', 'channels_base', 'se_reduction', 'dilation_b1', 'dilation_b2', 'dilation_b3', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_steps', 'seed']
2025-09-25 16:59:52,581 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-25 16:59:52,582 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-25 16:59:52,582 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-25 16:59:52,616 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-25 16:59:52,649 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-25 16:59:52,649 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-25 16:59:52,649 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-25 16:59:52,650 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-25 16:59:52,650 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 16:59:52,650 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 16:59:52,650 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 16:59:52,650 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}
2025-09-25 16:59:52,651 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}
2025-09-25 17:00:14,759 - INFO - _models.training_function_executor - Model: 5,820 parameters, 25.0KB storage
2025-09-25 17:00:14,759 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5162058584194855, 1.2875462616759241, 1.176425572949999, 1.0776071269399476, 1.0000908677188038, 0.9528898286986753, 0.9166015823118273, 0.886434558478042, 0.8605691104612938, 0.8375065967903189, 0.8259963016553626, 0.8094984390000497], 'val_losses': [1.3324087364992363, 1.227679261840416, 1.07119286505445, 1.0829238373300303, 0.9714830698726201, 0.9543314681825684, 0.8654353000245109, 0.8527025636493256, 0.8479999802701381, 0.8033302412073708, 0.7818978635190259, 0.7535375756079833], 'val_acc': [0.3592281668963789, 0.4657311113895502, 0.4939230672848014, 0.6395188572860544, 0.6081944618468863, 0.5942864302718958, 0.7849893497055507, 0.7869941110136575, 0.7614334043352964, 0.7851146472873074, 0.8398696905149731, 0.8562836737250971], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 36, 'seed': 1184}, 'model_parameter_count': 5820, 'model_storage_size_kb': 25.007812500000004, 'model_size_validation': 'PASS'}
2025-09-25 17:00:14,759 - INFO - _models.training_function_executor - BO Objective: base=0.8563, size_penalty=0.0000, final=0.8563
2025-09-25 17:00:14,759 - INFO - _models.training_function_executor - Model: 5,820 parameters, 25.0KB (PASS 256KB limit)
2025-09-25 17:00:14,759 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 22.109s
2025-09-25 17:00:14,759 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8563
2025-09-25 17:00:14,759 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-25 17:00:14,759 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': np.int64(11), 'se_reduction': np.int64(11), 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': np.int64(36), 'seed': np.int64(1184)}, value=0.8563
2025-09-25 17:00:14,759 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.0008838704519727185, 'batch_size': 64, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'label_smoothing': 0.031198904067240536, 'grad_clip_norm': 0.29041806084099736, 'use_scheduler': False, 'channels_base': np.int64(11), 'se_reduction': np.int64(11), 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': np.int64(36), 'seed': np.int64(1184)} -> 0.8563
2025-09-25 17:00:14,760 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-09-25 17:00:14,760 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-25 17:00:14,760 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:00:14,760 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:00:14,760 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.334697757417809e-05, 'batch_size': 256, 'epochs': 48, 'weight_decay': 1.2366582530130827e-06, 'dropout': 0.26238733012919463, 'label_smoothing': 0.07997219434305111, 'grad_clip_norm': 0.23332831606807716, 'use_scheduler': False, 'channels_base': 22, 'se_reduction': 6, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 33, 'seed': 8792}
2025-09-25 17:00:14,761 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.334697757417809e-05, 'batch_size': 256, 'epochs': 48, 'weight_decay': 1.2366582530130827e-06, 'dropout': 0.26238733012919463, 'label_smoothing': 0.07997219434305111, 'grad_clip_norm': 0.23332831606807716, 'use_scheduler': False, 'channels_base': 22, 'se_reduction': 6, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 33, 'seed': 8792}
2025-09-25 17:02:09,032 - INFO - _models.training_function_executor - Model: 23,371 parameters, 100.4KB storage
2025-09-25 17:02:09,032 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.078829028898211, 2.046407143648371, 2.0201071239789656, 1.9978920505320417, 1.9759197000034754, 1.9594149174137723, 1.9435642647929192, 1.9323359254614711, 1.9203574864841868, 1.9078572519373052, 1.8999393547708647, 1.8907120076164776, 1.8825378102795864, 1.878570541699787, 1.8700307591780083, 1.8677043997845217, 1.8573992658639071, 1.8514278101987551, 1.8443153530300453, 1.840090865961603, 1.8373871911226671, 1.827542123506867, 1.8237598779054154, 1.8143433484540294, 1.8071462629739343, 1.8051868979951715, 1.8002724146158091, 1.7914017262966813, 1.7917339277944107, 1.7827997881603717, 1.7791796476176347, 1.7710377397541115, 1.7689911630918957, 1.7648233904432422, 1.7610208548516053, 1.7549055659546535, 1.7442858160219854, 1.7420270709328547, 1.7388577880900602, 1.7286404332163727, 1.7248179700310897, 1.7262616761961636, 1.7173008025336698, 1.7132303783612244, 1.7058855569239784, 1.7046515731156084, 1.6986997736294616, 1.693156968377323], 'val_losses': [2.0609017263273146, 2.027036246583062, 2.0037021379455293, 1.981629937024601, 1.9621506543883016, 1.9446942312139988, 1.9317427115278456, 1.9164182842860709, 1.905346494497116, 1.8938335901273997, 1.8836559323854665, 1.8747859661956432, 1.8695101647758794, 1.8609282547721677, 1.8491978954811799, 1.8440309697084147, 1.8382841730279114, 1.830568701264076, 1.8242340121946155, 1.8173307691984435, 1.81209379628015, 1.80841990180996, 1.8015269591177694, 1.794600793822728, 1.7883401265973033, 1.7847012420776658, 1.7790201190658834, 1.7733321739548564, 1.7685806517343505, 1.7625489797438016, 1.758459909176441, 1.7500269479014314, 1.7450977697540921, 1.7395097066728473, 1.7357588032405935, 1.7281866188620494, 1.7226549510205393, 1.7207238082822411, 1.7159573673499944, 1.7058563777907811, 1.702319464428415, 1.6969971434691544, 1.6922578831511232, 1.6887901890115053, 1.6835684389596655, 1.677291542945119, 1.6729654626520096, 1.6726950348924685], 'val_acc': [0.05976694649793259, 0.12366871319383536, 0.14709936098233303, 0.1756672096228543, 0.176920185440421, 0.17666959027690765, 0.18481393309109134, 0.1844380403458213, 0.18168149354717453, 0.17792256609447438, 0.18656809923568476, 0.1881969677985215, 0.19095351459716828, 0.18995113394311489, 0.19095351459716828, 0.20010023806540533, 0.2061145219897256, 0.2092469615336424, 0.21626362611201605, 0.21313118656809923, 0.22077433905525623, 0.22290439794511965, 0.22616213507079314, 0.2325523117403834, 0.2334293948126801, 0.2348076682120035, 0.24445558200726725, 0.24182433279037716, 0.2348076682120035, 0.23505826337551686, 0.246084450570104, 0.2442049868437539, 0.246084450570104, 0.25247462723969427, 0.24959278285929082, 0.2549805788748277, 0.2636261120160381, 0.2646284926700915, 0.26788622979576493, 0.26951509835860166, 0.26650795639644154, 0.26512968299711814, 0.27151985966670844, 0.27277283548427517, 0.28091717829845886, 0.2779100363362987, 0.2821701541160256, 0.28041598797143213], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.334697757417809e-05, 'batch_size': 256, 'epochs': 48, 'weight_decay': 1.2366582530130827e-06, 'dropout': 0.26238733012919463, 'label_smoothing': 0.07997219434305111, 'grad_clip_norm': 0.23332831606807716, 'use_scheduler': False, 'channels_base': 22, 'se_reduction': 6, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 33, 'seed': 8792}, 'model_parameter_count': 23371, 'model_storage_size_kb': 100.42226562500001, 'model_size_validation': 'PASS'}
2025-09-25 17:02:09,032 - INFO - _models.training_function_executor - BO Objective: base=0.2804, size_penalty=0.0000, final=0.2804
2025-09-25 17:02:09,032 - INFO - _models.training_function_executor - Model: 23,371 parameters, 100.4KB (PASS 256KB limit)
2025-09-25 17:02:09,032 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 114.272s
2025-09-25 17:02:09,032 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2804
2025-09-25 17:02:09,032 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-25 17:02:09,032 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 1.334697757417809e-05, 'batch_size': 256, 'epochs': np.int64(48), 'weight_decay': 1.2366582530130827e-06, 'dropout': 0.26238733012919463, 'label_smoothing': 0.07997219434305111, 'grad_clip_norm': 0.23332831606807716, 'use_scheduler': False, 'channels_base': np.int64(22), 'se_reduction': np.int64(6), 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': np.int64(33), 'seed': np.int64(8792)}, value=0.2804
2025-09-25 17:02:09,032 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 1.334697757417809e-05, 'batch_size': 256, 'epochs': np.int64(48), 'weight_decay': 1.2366582530130827e-06, 'dropout': 0.26238733012919463, 'label_smoothing': 0.07997219434305111, 'grad_clip_norm': 0.23332831606807716, 'use_scheduler': False, 'channels_base': np.int64(22), 'se_reduction': np.int64(6), 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': np.int64(33), 'seed': np.int64(8792)} -> 0.2804
2025-09-25 17:02:09,033 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-09-25 17:02:09,033 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-25 17:02:09,033 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:02:09,033 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:02:09,033 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0037311604524743166, 'batch_size': 512, 'epochs': 13, 'weight_decay': 1.1584172310543985e-06, 'dropout': 0.11544691281107453, 'label_smoothing': 0.04820509320520235, 'grad_clip_norm': 3.4163175941272916, 'use_scheduler': False, 'channels_base': 15, 'se_reduction': 6, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 69, 'seed': 9692}
2025-09-25 17:02:09,034 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0037311604524743166, 'batch_size': 512, 'epochs': 13, 'weight_decay': 1.1584172310543985e-06, 'dropout': 0.11544691281107453, 'label_smoothing': 0.04820509320520235, 'grad_clip_norm': 3.4163175941272916, 'use_scheduler': False, 'channels_base': 15, 'se_reduction': 6, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 69, 'seed': 9692}
2025-09-25 17:02:29,349 - INFO - _models.training_function_executor - Model: 11,212 parameters, 48.2KB storage
2025-09-25 17:02:29,349 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6016726706763862, 1.330658050735534, 1.1852087797319606, 1.0845283308681328, 1.0273953611732811, 0.988137271616896, 0.9666585014886853, 0.952860190221377, 0.9381221670467065, 0.9251616257151106, 0.9091851759520038, 0.8998089666052599, 0.8875243975749251], 'val_losses': [1.4645212018509255, 1.2674467704929724, 1.1511517174485004, 1.0837002806370157, 1.007061365970826, 0.9920049994351228, 0.9650642030625367, 0.9651506933528459, 0.9278932804751255, 0.9168920495683067, 0.9206307188578229, 0.8961445925050833, 0.8793584908956334], 'val_acc': [0.32715198596667083, 0.5214885352712693, 0.4559579000125298, 0.605688510211753, 0.6829971181556196, 0.7561709059015161, 0.7985214885352713, 0.7813557198346072, 0.7007893747650671, 0.7561709059015161, 0.8713193835358978, 0.8485152236561835, 0.7968926199724345], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0037311604524743166, 'batch_size': 512, 'epochs': 13, 'weight_decay': 1.1584172310543985e-06, 'dropout': 0.11544691281107453, 'label_smoothing': 0.04820509320520235, 'grad_clip_norm': 3.4163175941272916, 'use_scheduler': False, 'channels_base': 15, 'se_reduction': 6, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 69, 'seed': 9692}, 'model_parameter_count': 11212, 'model_storage_size_kb': 48.1765625, 'model_size_validation': 'PASS'}
2025-09-25 17:02:29,349 - INFO - _models.training_function_executor - BO Objective: base=0.7969, size_penalty=0.0000, final=0.7969
2025-09-25 17:02:29,349 - INFO - _models.training_function_executor - Model: 11,212 parameters, 48.2KB (PASS 256KB limit)
2025-09-25 17:02:29,349 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 20.316s
2025-09-25 17:02:29,429 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7969
2025-09-25 17:02:29,429 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.080s
2025-09-25 17:02:29,429 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.0037311604524743166, 'batch_size': 512, 'epochs': np.int64(13), 'weight_decay': 1.1584172310543985e-06, 'dropout': 0.11544691281107453, 'label_smoothing': 0.04820509320520235, 'grad_clip_norm': 3.4163175941272916, 'use_scheduler': False, 'channels_base': np.int64(15), 'se_reduction': np.int64(6), 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': np.int64(69), 'seed': np.int64(9692)}, value=0.7969
2025-09-25 17:02:29,429 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.0037311604524743166, 'batch_size': 512, 'epochs': np.int64(13), 'weight_decay': 1.1584172310543985e-06, 'dropout': 0.11544691281107453, 'label_smoothing': 0.04820509320520235, 'grad_clip_norm': 3.4163175941272916, 'use_scheduler': False, 'channels_base': np.int64(15), 'se_reduction': np.int64(6), 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': np.int64(69), 'seed': np.int64(9692)} -> 0.7969
2025-09-25 17:02:29,429 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-09-25 17:02:29,429 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:02:29,429 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:02:29,429 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:02:29,429 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0038589082671053893, 'batch_size': 512, 'epochs': 25, 'weight_decay': 1.7102831300855793e-05, 'dropout': 0.17194039771667616, 'label_smoothing': 0.000983284341439661, 'grad_clip_norm': 3.1407458048205568, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 7, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 62, 'seed': 716}
2025-09-25 17:02:29,430 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0038589082671053893, 'batch_size': 512, 'epochs': 25, 'weight_decay': 1.7102831300855793e-05, 'dropout': 0.17194039771667616, 'label_smoothing': 0.000983284341439661, 'grad_clip_norm': 3.1407458048205568, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 7, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 62, 'seed': 716}
2025-09-25 17:03:02,568 - INFO - _models.training_function_executor - Model: 6,043 parameters, 26.0KB storage
2025-09-25 17:03:02,569 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.286139305367661, 0.9605838480802481, 0.7934975308799559, 0.6860994929916013, 0.5834597109743771, 0.5256591883289878, 0.4844921894719964, 0.4490178424360997, 0.4120059859329562, 0.3970048650462044, 0.37282002315658513, 0.3664397416601981, 0.34596989505034736, 0.33375145914997256, 0.3255709195202934, 0.3149238502156244, 0.30795490845542606, 0.2973520493397597, 0.2987469389348036, 0.2941003136057433, 0.2831775649723409, 0.27638131691239715, 0.26994272152276594, 0.27181717355663276, 0.2627089516113072], 'val_losses': [1.1963582399001682, 0.8547505720816913, 0.7488422508661556, 0.657198807654293, 0.5551850683198299, 0.5488997221911646, 0.45557556381546427, 0.3995400191608071, 0.38832770257557175, 0.3907614250654207, 0.33462938564027733, 0.37931906294216267, 0.33768285869387005, 0.3127858663408877, 0.30031476636127696, 0.34190464042927177, 0.2912734632947782, 0.313114805224769, 0.3536733385764063, 0.3252171973711027, 0.2717281557454335, 0.25733250376201633, 0.2860246117667527, 0.3029802172252102, 0.25328111445421325], 'val_acc': [0.3592281668963789, 0.49993735120912164, 0.5328906152111265, 0.6544292695150984, 0.7529131687758426, 0.6286179676732239, 0.7747149480015035, 0.868562836737251, 0.8139330910913419, 0.8457586768575367, 0.8264628492670092, 0.822328029069039, 0.8565342688886104, 0.8731988472622478, 0.8654303971933341, 0.9287056759804536, 0.9119158000250596, 0.862047362485904, 0.9292068663074803, 0.9114146096980328, 0.862047362485904, 0.8948753289061521, 0.8921187821075053, 0.8966294950507455, 0.9028943741385791], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0038589082671053893, 'batch_size': 512, 'epochs': 25, 'weight_decay': 1.7102831300855793e-05, 'dropout': 0.17194039771667616, 'label_smoothing': 0.000983284341439661, 'grad_clip_norm': 3.1407458048205568, 'use_scheduler': False, 'channels_base': 11, 'se_reduction': 7, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 62, 'seed': 716}, 'model_parameter_count': 6043, 'model_storage_size_kb': 25.966015625, 'model_size_validation': 'PASS'}
2025-09-25 17:03:02,569 - INFO - _models.training_function_executor - BO Objective: base=0.9029, size_penalty=0.0000, final=0.9029
2025-09-25 17:03:02,569 - INFO - _models.training_function_executor - Model: 6,043 parameters, 26.0KB (PASS 256KB limit)
2025-09-25 17:03:02,569 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 33.140s
2025-09-25 17:03:02,647 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9029
2025-09-25 17:03:02,647 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.079s
2025-09-25 17:03:02,648 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 0.0038589082671053893, 'batch_size': np.int64(512), 'epochs': np.int64(25), 'weight_decay': 1.7102831300855793e-05, 'dropout': 0.17194039771667616, 'label_smoothing': 0.000983284341439661, 'grad_clip_norm': 3.1407458048205568, 'use_scheduler': np.False_, 'channels_base': np.int64(11), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(62), 'seed': np.int64(716)}, value=0.9029
2025-09-25 17:03:02,648 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 0.0038589082671053893, 'batch_size': np.int64(512), 'epochs': np.int64(25), 'weight_decay': 1.7102831300855793e-05, 'dropout': 0.17194039771667616, 'label_smoothing': 0.000983284341439661, 'grad_clip_norm': 3.1407458048205568, 'use_scheduler': np.False_, 'channels_base': np.int64(11), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(62), 'seed': np.int64(716)} -> 0.9029
2025-09-25 17:03:02,648 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-09-25 17:03:02,648 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:03:02,648 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:03:02,648 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:03:02,648 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 9.445376892045972e-05, 'batch_size': 128, 'epochs': 8, 'weight_decay': 0.00016331665992737898, 'dropout': 0.31107498260479505, 'label_smoothing': 0.17324353275716853, 'grad_clip_norm': 2.958962387849979, 'use_scheduler': False, 'channels_base': 23, 'se_reduction': 11, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 33, 'seed': 7453}
2025-09-25 17:03:02,649 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 9.445376892045972e-05, 'batch_size': 128, 'epochs': 8, 'weight_decay': 0.00016331665992737898, 'dropout': 0.31107498260479505, 'label_smoothing': 0.17324353275716853, 'grad_clip_norm': 2.958962387849979, 'use_scheduler': False, 'channels_base': 23, 'se_reduction': 11, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 33, 'seed': 7453}
2025-09-25 17:03:23,529 - INFO - _models.training_function_executor - Model: 23,893 parameters, 102.7KB storage
2025-09-25 17:03:23,530 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.5561844147943678, 2.4148361467573496, 2.380464367302192, 2.3565965710780272, 2.3216650571955464, 2.3007297070594084, 2.2684226701557084, 2.2503630289759426], 'val_losses': [2.445922062489909, 2.375768987237175, 2.3350351184490794, 2.3073299034483346, 2.283002409745601, 2.261141348416876, 2.2312240950103215, 2.210652207569466], 'val_acc': [0.0745520611452199, 0.08194461846886356, 0.08670592657561708, 0.0875830096479138, 0.08958777095602055, 0.09823330409723093, 0.10675353965668463, 0.14334043352963288], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 9.445376892045972e-05, 'batch_size': 128, 'epochs': 8, 'weight_decay': 0.00016331665992737898, 'dropout': 0.31107498260479505, 'label_smoothing': 0.17324353275716853, 'grad_clip_norm': 2.958962387849979, 'use_scheduler': False, 'channels_base': 23, 'se_reduction': 11, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 33, 'seed': 7453}, 'model_parameter_count': 23893, 'model_storage_size_kb': 102.66523437500001, 'model_size_validation': 'PASS'}
2025-09-25 17:03:23,530 - INFO - _models.training_function_executor - BO Objective: base=0.1433, size_penalty=0.0000, final=0.1433
2025-09-25 17:03:23,530 - INFO - _models.training_function_executor - Model: 23,893 parameters, 102.7KB (PASS 256KB limit)
2025-09-25 17:03:23,530 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 20.882s
2025-09-25 17:03:23,608 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1433
2025-09-25 17:03:23,609 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.079s
2025-09-25 17:03:23,609 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 9.445376892045972e-05, 'batch_size': np.int64(128), 'epochs': np.int64(8), 'weight_decay': 0.00016331665992737898, 'dropout': 0.31107498260479505, 'label_smoothing': 0.17324353275716853, 'grad_clip_norm': 2.958962387849979, 'use_scheduler': np.False_, 'channels_base': np.int64(23), 'se_reduction': np.int64(11), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(33), 'seed': np.int64(7453)}, value=0.1433
2025-09-25 17:03:23,609 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 9.445376892045972e-05, 'batch_size': np.int64(128), 'epochs': np.int64(8), 'weight_decay': 0.00016331665992737898, 'dropout': 0.31107498260479505, 'label_smoothing': 0.17324353275716853, 'grad_clip_norm': 2.958962387849979, 'use_scheduler': np.False_, 'channels_base': np.int64(23), 'se_reduction': np.int64(11), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(33), 'seed': np.int64(7453)} -> 0.1433
2025-09-25 17:03:23,609 - INFO - bo.run_bo - üîçBO Trial 6: Using RF surrogate + Expected Improvement
2025-09-25 17:03:23,609 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:03:23,609 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:03:23,609 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:03:23,609 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008334357299161823, 'batch_size': 512, 'epochs': 13, 'weight_decay': 1.567626935817878e-06, 'dropout': 0.25604230359854036, 'label_smoothing': 0.13455226114747806, 'grad_clip_norm': 4.806447285975828, 'use_scheduler': False, 'channels_base': 17, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 108, 'seed': 3361}
2025-09-25 17:03:23,610 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008334357299161823, 'batch_size': 512, 'epochs': 13, 'weight_decay': 1.567626935817878e-06, 'dropout': 0.25604230359854036, 'label_smoothing': 0.13455226114747806, 'grad_clip_norm': 4.806447285975828, 'use_scheduler': False, 'channels_base': 17, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 108, 'seed': 3361}
2025-09-25 17:03:48,077 - INFO - _models.training_function_executor - Model: 13,824 parameters, 29.7KB storage
2025-09-25 17:03:48,077 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.268255378796456, 2.1157009088839596, 2.051009306538558, 1.980651216175653, 1.9096625990703755, 1.8582180878148358, 1.8191435399092972, 1.7834493412998533, 1.7588871856965551, 1.7385740382619899, 1.719671461220183, 1.7085390162939222, 1.6931999067583094], 'val_losses': [2.146606304011926, 2.0625123222763566, 2.0002274895622505, 1.9520675842362587, 1.8691228790818533, 1.8144002545075584, 1.8654294087465297, 1.7602667286945397, 1.7043487462255384, 1.7087831141102987, 1.6900794960146723, 1.7426337406100851, 1.6737430652211653], 'val_acc': [0.07893747650670342, 0.08896128304723719, 0.11239193083573487, 0.17403834106001753, 0.20849517604310236, 0.20185440420999876, 0.24445558200726725, 0.24846510462348076, 0.27151985966670844, 0.2554817692018544, 0.23230171657687007, 0.2713945620849518, 0.2658814684876582], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008334357299161823, 'batch_size': 512, 'epochs': 13, 'weight_decay': 1.567626935817878e-06, 'dropout': 0.25604230359854036, 'label_smoothing': 0.13455226114747806, 'grad_clip_norm': 4.806447285975828, 'use_scheduler': False, 'channels_base': 17, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 1, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 108, 'seed': 3361}, 'model_parameter_count': 13824, 'model_storage_size_kb': 29.700000000000003, 'model_size_validation': 'PASS'}
2025-09-25 17:03:48,077 - INFO - _models.training_function_executor - BO Objective: base=0.2659, size_penalty=0.0000, final=0.2659
2025-09-25 17:03:48,077 - INFO - _models.training_function_executor - Model: 13,824 parameters, 29.7KB (PASS 256KB limit)
2025-09-25 17:03:48,077 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 24.469s
2025-09-25 17:03:48,155 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2659
2025-09-25 17:03:48,156 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.078s
2025-09-25 17:03:48,156 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 0.0008334357299161823, 'batch_size': np.int64(512), 'epochs': np.int64(13), 'weight_decay': 1.567626935817878e-06, 'dropout': 0.25604230359854036, 'label_smoothing': 0.13455226114747806, 'grad_clip_norm': 4.806447285975828, 'use_scheduler': np.False_, 'channels_base': np.int64(17), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(108), 'seed': np.int64(3361)}, value=0.2659
2025-09-25 17:03:48,156 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 0.0008334357299161823, 'batch_size': np.int64(512), 'epochs': np.int64(13), 'weight_decay': 1.567626935817878e-06, 'dropout': 0.25604230359854036, 'label_smoothing': 0.13455226114747806, 'grad_clip_norm': 4.806447285975828, 'use_scheduler': np.False_, 'channels_base': np.int64(17), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(108), 'seed': np.int64(3361)} -> 0.2659
2025-09-25 17:03:48,156 - INFO - bo.run_bo - üîçBO Trial 7: Using RF surrogate + Expected Improvement
2025-09-25 17:03:48,156 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:03:48,156 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:03:48,156 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:03:48,156 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001646771580976391, 'batch_size': 512, 'epochs': 34, 'weight_decay': 1.464007808775521e-05, 'dropout': 0.48313492274363157, 'label_smoothing': 0.06650220840354046, 'grad_clip_norm': 2.5086467002639967, 'use_scheduler': False, 'channels_base': 19, 'se_reduction': 4, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 162, 'seed': 3490}
2025-09-25 17:03:48,157 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001646771580976391, 'batch_size': 512, 'epochs': 34, 'weight_decay': 1.464007808775521e-05, 'dropout': 0.48313492274363157, 'label_smoothing': 0.06650220840354046, 'grad_clip_norm': 2.5086467002639967, 'use_scheduler': False, 'channels_base': 19, 'se_reduction': 4, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 162, 'seed': 3490}
2025-09-25 17:05:12,048 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-09-25 17:05:12,049 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.8325872486987904, 1.6145663569864763, 1.4763297683496663, 1.3945075711507013, 1.3159899677552291, 1.249078948078861, 1.2209112184804936, 1.1897603981821205, 1.1693883254721258, 1.150127894667849, 1.1381655743153245, 1.1241356048993112, 1.120116128907468, 1.1158480596352658, 1.1000909634118645, 1.095093316647678, 1.0924457632292521, 1.0922950190029186, 1.0878904045880993, 1.08276715711204, 1.0741528168853256, 1.0786642231999628, 1.0655129382463415, 1.0673588909489078, 1.0637501285917323, 1.0600619728290448, 1.059729729436474, 1.0545228095321701, 1.0573254622500579, 1.0511359520560162, 1.053593543886343, 1.048234887474578, 1.0480288019105015, 1.0436211397178607], 'val_losses': [1.707295724867938, 1.51827496978039, 1.3932636561302325, 1.283774067399317, 1.216757660625483, 1.2324438266864084, 1.231581518067825, 1.1474117657367844, 1.1598314326324195, 1.1136133297689725, 1.1151516058886803, 1.0785828950694594, 1.141529333739334, 1.0763037818040693, 1.1007488916846628, 1.0668034559801218, 1.0959833849642961, 1.0601905421974096, 1.0494722905814415, 1.0442265140250009, 1.0325961769889311, 1.042766407324458, 1.049203200382691, 1.0312396746174075, 1.0342992940891238, 1.0239265071406218, 1.0364820987803942, 1.0209259557449164, 1.038781889308561, 1.0278213569027874, 1.0177993751411643, 1.0282477971650412, 1.0262950525712793, 1.0333324879456427], 'val_acc': [0.28630497431399576, 0.3444430522490916, 0.4148602931963413, 0.45107129432401954, 0.5341435910286931, 0.4752537276030573, 0.44117278536524246, 0.5188572860543792, 0.5618343565969177, 0.7085578248339807, 0.7104372885603308, 0.7274777596792382, 0.6484149855907781, 0.7321137702042351, 0.7331161508582884, 0.7481518606690891, 0.7819822077433906, 0.7871194085954141, 0.7690765568224533, 0.7962661320636512, 0.84638516476632, 0.8958777095602055, 0.7565467986467861, 0.8254604686129557, 0.8445057010399699, 0.782608695652174, 0.8833479513845383, 0.8602931963413106, 0.867811051246711, 0.854404209998747, 0.8322265380278161, 0.8373637388798396, 0.8898634256358853, 0.8731988472622478], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001646771580976391, 'batch_size': 512, 'epochs': 34, 'weight_decay': 1.464007808775521e-05, 'dropout': 0.48313492274363157, 'label_smoothing': 0.06650220840354046, 'grad_clip_norm': 2.5086467002639967, 'use_scheduler': False, 'channels_base': 19, 'se_reduction': 4, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 162, 'seed': 3490}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-09-25 17:05:12,049 - INFO - _models.training_function_executor - BO Objective: base=0.8732, size_penalty=0.0000, final=0.8732
2025-09-25 17:05:12,049 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-09-25 17:05:12,049 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 83.893s
2025-09-25 17:05:12,129 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8732
2025-09-25 17:05:12,129 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.080s
2025-09-25 17:05:12,129 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.001646771580976391, 'batch_size': np.int64(512), 'epochs': np.int64(34), 'weight_decay': 1.464007808775521e-05, 'dropout': 0.48313492274363157, 'label_smoothing': 0.06650220840354046, 'grad_clip_norm': 2.5086467002639967, 'use_scheduler': np.False_, 'channels_base': np.int64(19), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(162), 'seed': np.int64(3490)}, value=0.8732
2025-09-25 17:05:12,129 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.001646771580976391, 'batch_size': np.int64(512), 'epochs': np.int64(34), 'weight_decay': 1.464007808775521e-05, 'dropout': 0.48313492274363157, 'label_smoothing': 0.06650220840354046, 'grad_clip_norm': 2.5086467002639967, 'use_scheduler': np.False_, 'channels_base': np.int64(19), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(162), 'seed': np.int64(3490)} -> 0.8732
2025-09-25 17:05:12,129 - INFO - bo.run_bo - üîçBO Trial 8: Using RF surrogate + Expected Improvement
2025-09-25 17:05:12,129 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:05:12,129 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:05:12,129 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:05:12,129 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0018405412795251673, 'batch_size': 64, 'epochs': 39, 'weight_decay': 8.449815485544768e-05, 'dropout': 0.38439003944235917, 'label_smoothing': 0.05731316215639258, 'grad_clip_norm': 0.490627050264026, 'use_scheduler': False, 'channels_base': 9, 'se_reduction': 13, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 184, 'seed': 4520}
2025-09-25 17:05:12,130 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0018405412795251673, 'batch_size': 64, 'epochs': 39, 'weight_decay': 8.449815485544768e-05, 'dropout': 0.38439003944235917, 'label_smoothing': 0.05731316215639258, 'grad_clip_norm': 0.490627050264026, 'use_scheduler': False, 'channels_base': 9, 'se_reduction': 13, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 184, 'seed': 4520}
2025-09-25 17:06:16,870 - INFO - _models.training_function_executor - Model: 3,879 parameters, 4.2KB storage
2025-09-25 17:06:16,870 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.7022971927613666, 1.5131558111182013, 1.3961109551773092, 1.330657162616431, 1.2704495957362056, 1.2287382606171995, 1.1966234111802319, 1.178643510570406, 1.1507097808571531, 1.1416071381249404, 1.136817856885114, 1.1260395888617825, 1.1228968441772031, 1.1133600904526084, 1.1065183545220965, 1.1113216258419512, 1.0979845783217033, 1.0975557961634361, 1.094503422090853, 1.0805511529988687, 1.0829892642190386, 1.0916074231181554, 1.0824537871027562, 1.0858506005468238, 1.0761767783357816, 1.0693996437240527, 1.073341963684985, 1.0727659555204163, 1.065364101628173, 1.0696295880492392, 1.0589860334949932, 1.0750052857935977, 1.0733924608042682, 1.056938544519301, 1.0654976235682387, 1.0615143191376393, 1.05539033245115, 1.058799597612282, 1.0561502531119031], 'val_losses': [1.5552747345496474, 1.3929492690751104, 1.2762229420938314, 1.2485926262254838, 1.2337168168788595, 1.1272461469901442, 1.135570700821938, 1.1242380950881372, 1.1598786360457822, 1.0949876007617318, 1.1165796460521025, 1.0732721604286406, 1.0546159265692887, 1.1000515550707324, 1.0508863826754578, 1.0736029518530372, 1.0328496835267928, 1.0517480602062461, 1.0659501108963707, 1.0355471189182484, 1.0424472355421377, 1.0336048800818918, 1.0260441835893603, 1.0547922049648943, 1.027990990422331, 1.036398255082692, 1.0233681496186537, 1.0316233525612557, 1.0311755598405448, 1.0233823977949446, 1.0515751207461856, 1.029121775239381, 1.047690051403459, 1.0224803852984188, 1.022703050118224, 1.0091259795581917, 1.007168070831748, 1.0169252176778296, 1.0433133162288286], 'val_acc': [0.30434782608695654, 0.4637263500814434, 0.46284926700914675, 0.5201102618719459, 0.5394060894624734, 0.5942864302718958, 0.5504322766570605, 0.6630748026563087, 0.6608194461846887, 0.7242200225535648, 0.6664578373637389, 0.7064277659441173, 0.7718331036211001, 0.7668212003508332, 0.779100363362987, 0.7465229921062524, 0.8267134444305225, 0.8516476632001002, 0.7836110763062273, 0.8342312993359228, 0.8586643277784739, 0.8491417115649668, 0.8498934970555069, 0.8526500438541537, 0.8388673098609197, 0.8198220774339056, 0.8262122541034959, 0.8272146347575492, 0.8109259491291818, 0.8863550933466984, 0.7664453076055632, 0.8546548051622603, 0.7861170279413607, 0.8298458839744393, 0.8526500438541537, 0.84488159378524, 0.8468863550933466, 0.8585390301967172, 0.814308983836612], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0018405412795251673, 'batch_size': 64, 'epochs': 39, 'weight_decay': 8.449815485544768e-05, 'dropout': 0.38439003944235917, 'label_smoothing': 0.05731316215639258, 'grad_clip_norm': 0.490627050264026, 'use_scheduler': False, 'channels_base': 9, 'se_reduction': 13, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 184, 'seed': 4520}, 'model_parameter_count': 3879, 'model_storage_size_kb': 4.1668945312500005, 'model_size_validation': 'PASS'}
2025-09-25 17:06:16,870 - INFO - _models.training_function_executor - BO Objective: base=0.8143, size_penalty=0.0000, final=0.8143
2025-09-25 17:06:16,870 - INFO - _models.training_function_executor - Model: 3,879 parameters, 4.2KB (PASS 256KB limit)
2025-09-25 17:06:16,870 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 64.741s
2025-09-25 17:06:16,952 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8143
2025-09-25 17:06:16,952 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.082s
2025-09-25 17:06:16,952 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 0.0018405412795251673, 'batch_size': np.int64(64), 'epochs': np.int64(39), 'weight_decay': 8.449815485544768e-05, 'dropout': 0.38439003944235917, 'label_smoothing': 0.05731316215639258, 'grad_clip_norm': 0.490627050264026, 'use_scheduler': np.False_, 'channels_base': np.int64(9), 'se_reduction': np.int64(13), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(184), 'seed': np.int64(4520)}, value=0.8143
2025-09-25 17:06:16,952 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 0.0018405412795251673, 'batch_size': np.int64(64), 'epochs': np.int64(39), 'weight_decay': 8.449815485544768e-05, 'dropout': 0.38439003944235917, 'label_smoothing': 0.05731316215639258, 'grad_clip_norm': 0.490627050264026, 'use_scheduler': np.False_, 'channels_base': np.int64(9), 'se_reduction': np.int64(13), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(184), 'seed': np.int64(4520)} -> 0.8143
2025-09-25 17:06:16,952 - INFO - bo.run_bo - üîçBO Trial 9: Using RF surrogate + Expected Improvement
2025-09-25 17:06:16,952 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:06:16,952 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:06:16,952 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:06:16,952 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004968915344513474, 'batch_size': 256, 'epochs': 39, 'weight_decay': 0.0006841400028776404, 'dropout': 0.39650161807947865, 'label_smoothing': 0.03770716016008769, 'grad_clip_norm': 0.48356232914713426, 'use_scheduler': False, 'channels_base': 9, 'se_reduction': 10, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 132, 'seed': 996}
2025-09-25 17:06:16,953 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004968915344513474, 'batch_size': 256, 'epochs': 39, 'weight_decay': 0.0006841400028776404, 'dropout': 0.39650161807947865, 'label_smoothing': 0.03770716016008769, 'grad_clip_norm': 0.48356232914713426, 'use_scheduler': False, 'channels_base': 9, 'se_reduction': 10, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 132, 'seed': 996}
2025-09-25 17:07:06,316 - INFO - _models.training_function_executor - Model: 3,952 parameters, 17.0KB storage
2025-09-25 17:07:06,316 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5532212480172154, 1.3221041437184657, 1.2284678469862447, 1.139747206595586, 1.0615987629645922, 1.0074270825500615, 0.9855139785268061, 0.9703234013050858, 0.9489746859762015, 0.926873704172953, 0.9240016461322396, 0.9092918677396189, 0.9030491928817805, 0.8892365714995308, 0.8805665808863998, 0.8750946409768099, 0.8726613723898996, 0.8690069403874076, 0.8618051526286796, 0.8618701134536723, 0.8517913867577132, 0.8512394627831549, 0.841750403527228, 0.8460761186036423, 0.8330263580306672, 0.8379046528540962, 0.8375379723089962, 0.8358039003552543, 0.8348631013310934, 0.8311328007434539, 0.8259203964911155, 0.8196590365919296, 0.8221209602196429, 0.8135160285794779, 0.8113461631404104, 0.8113368274954721, 0.8091848288058742, 0.8103668737358473, 0.8107989847527498], 'val_losses': [1.3461898588145411, 1.2266425090749644, 1.1081049799575586, 1.0550422561958819, 0.9809698343964063, 0.934601758390034, 0.9125068956352896, 0.8819281941083958, 0.8876729456049923, 0.8676338874820241, 0.9061406560147889, 0.8353457901504998, 0.8397552443542452, 0.8845386114982029, 0.8459481427938683, 0.8044491642594801, 0.806544932114004, 0.797575565481407, 0.8189135290539484, 0.7983258281968737, 0.8120423309367756, 0.8095519634405035, 0.7828266692929303, 0.8033229578797523, 0.7859136212844475, 0.7806293626519287, 0.7760320122626443, 0.8038409528778903, 0.7917736428316606, 0.7718214153899927, 0.7675580541198228, 0.7691169709971698, 0.7547928444978632, 0.7772371834672613, 0.7632828545352298, 0.7618934617516332, 0.8042487050937002, 0.7552908187730467, 0.7689726280697067], 'val_acc': [0.361107630622729, 0.47888735747400074, 0.5113394311489788, 0.653050996115775, 0.6278661821826839, 0.8057887482771583, 0.7047988973812805, 0.8327277283548428, 0.8738253351710312, 0.8283423129933593, 0.8225786242325523, 0.8715699786994111, 0.84488159378524, 0.7678235810048866, 0.8485152236561835, 0.8581631374514472, 0.8571607567973938, 0.821952136323769, 0.807417616839995, 0.8619220649041474, 0.877333667460218, 0.8364866558075429, 0.913294073424383, 0.8554065906528004, 0.8813431900764315, 0.8151860669089087, 0.8976318757047989, 0.8336048114271395, 0.9219396065655933, 0.8820949755669716, 0.8681869439919809, 0.8919934845257487, 0.8557824833980704, 0.8729482520987345, 0.8924946748527753, 0.9094098483899261, 0.8576619471244205, 0.9130434782608695, 0.8518982583636135], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004968915344513474, 'batch_size': 256, 'epochs': 39, 'weight_decay': 0.0006841400028776404, 'dropout': 0.39650161807947865, 'label_smoothing': 0.03770716016008769, 'grad_clip_norm': 0.48356232914713426, 'use_scheduler': False, 'channels_base': 9, 'se_reduction': 10, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 132, 'seed': 996}, 'model_parameter_count': 3952, 'model_storage_size_kb': 16.981250000000003, 'model_size_validation': 'PASS'}
2025-09-25 17:07:06,316 - INFO - _models.training_function_executor - BO Objective: base=0.8519, size_penalty=0.0000, final=0.8519
2025-09-25 17:07:06,316 - INFO - _models.training_function_executor - Model: 3,952 parameters, 17.0KB (PASS 256KB limit)
2025-09-25 17:07:06,317 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 49.364s
2025-09-25 17:07:06,401 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8519
2025-09-25 17:07:06,401 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-09-25 17:07:06,401 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 0.004968915344513474, 'batch_size': np.int64(256), 'epochs': np.int64(39), 'weight_decay': 0.0006841400028776404, 'dropout': 0.39650161807947865, 'label_smoothing': 0.03770716016008769, 'grad_clip_norm': 0.48356232914713426, 'use_scheduler': np.False_, 'channels_base': np.int64(9), 'se_reduction': np.int64(10), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(132), 'seed': np.int64(996)}, value=0.8519
2025-09-25 17:07:06,401 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 0.004968915344513474, 'batch_size': np.int64(256), 'epochs': np.int64(39), 'weight_decay': 0.0006841400028776404, 'dropout': 0.39650161807947865, 'label_smoothing': 0.03770716016008769, 'grad_clip_norm': 0.48356232914713426, 'use_scheduler': np.False_, 'channels_base': np.int64(9), 'se_reduction': np.int64(10), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(132), 'seed': np.int64(996)} -> 0.8519
2025-09-25 17:07:06,401 - INFO - bo.run_bo - üîçBO Trial 10: Using RF surrogate + Expected Improvement
2025-09-25 17:07:06,401 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:07:06,401 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:07:06,401 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:07:06,401 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0018259788292038777, 'batch_size': 256, 'epochs': 35, 'weight_decay': 0.00012249914218517177, 'dropout': 0.07601199715807112, 'label_smoothing': 0.05771020576152858, 'grad_clip_norm': 2.1828777721421986, 'use_scheduler': True, 'channels_base': 14, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 116, 'seed': 1790}
2025-09-25 17:07:06,402 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0018259788292038777, 'batch_size': 256, 'epochs': 35, 'weight_decay': 0.00012249914218517177, 'dropout': 0.07601199715807112, 'label_smoothing': 0.05771020576152858, 'grad_clip_norm': 2.1828777721421986, 'use_scheduler': True, 'channels_base': 14, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 116, 'seed': 1790}
2025-09-25 17:08:01,200 - INFO - _models.training_function_executor - Model: 9,155 parameters, 39.3KB storage
2025-09-25 17:08:01,200 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.657655284547628, 1.3561906363549352, 1.1955298549066984, 1.118869936933375, 1.0825107026301062, 1.05511998759062, 1.0293808588799247, 1.0188650460491628, 1.0020333257372904, 0.9939528869099327, 0.9837949950380687, 0.9697936967333146, 0.9650829154417501, 0.9609245696551921, 0.9564777871415463, 0.9501459339569226, 0.9403282654067723, 0.9370192479987202, 0.9341691723743096, 0.932715448440923, 0.924590762854332, 0.9224406659324915, 0.9216564333618492, 0.9172825663829064, 0.9170087910708982, 0.9107438660323708, 0.9114778757774975, 0.9081613103727931, 0.9095942528460762, 0.9049657597688342, 0.9010832839840707, 0.9034236828881957, 0.9010040348112308, 0.9031542094396184, 0.901394108585803], 'val_losses': [1.4819774248892466, 1.2581286446042355, 1.1849682155729102, 1.0996155261903788, 1.1442047211030808, 1.0190697832672746, 1.0455434409437765, 1.0115645048597703, 1.0096177083629516, 0.9827430176714013, 1.0119467289760558, 0.9934285968904313, 1.0521478089842935, 0.9566241295291971, 0.9557985094701597, 0.9333830658706294, 0.9640502985251784, 0.9628868477936899, 0.9287451065777089, 0.9303055540121165, 0.9385638008917084, 0.9151517327992992, 0.916431255126086, 0.9127346899049082, 0.902401599888813, 0.9146617296539352, 0.9046339813352393, 0.9097677990094027, 0.8984838188956695, 0.9010943487882166, 0.9042270980283381, 0.9020535813553742, 0.8972165626088057, 0.8980988202395557, 0.8962217278833632], 'val_acc': [0.3678737000375893, 0.5464227540408471, 0.6113269013908031, 0.6199724345320136, 0.4638516476632001, 0.7262247838616714, 0.6957774714948002, 0.7031700288184438, 0.8864803909284551, 0.8518982583636135, 0.776845006891367, 0.7509084074677359, 0.6173411853151234, 0.884225034456835, 0.8445057010399699, 0.8357348703170029, 0.901766695902769, 0.7517854905400326, 0.8946247337426387, 0.8496429018919934, 0.8332289186818694, 0.8779601553690014, 0.8966294950507455, 0.8866056885102117, 0.8805914045858915, 0.9199348452574865, 0.8807167021676482, 0.9112893121162762, 0.9013908031574991, 0.899887232176419, 0.9081568725723593, 0.8972559829595289, 0.9077809798270894, 0.8859792006014284, 0.9072797895000626], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0018259788292038777, 'batch_size': 256, 'epochs': 35, 'weight_decay': 0.00012249914218517177, 'dropout': 0.07601199715807112, 'label_smoothing': 0.05771020576152858, 'grad_clip_norm': 2.1828777721421986, 'use_scheduler': True, 'channels_base': 14, 'se_reduction': 11, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 116, 'seed': 1790}, 'model_parameter_count': 9155, 'model_storage_size_kb': 39.337890625, 'model_size_validation': 'PASS'}
2025-09-25 17:08:01,200 - INFO - _models.training_function_executor - BO Objective: base=0.9073, size_penalty=0.0000, final=0.9073
2025-09-25 17:08:01,200 - INFO - _models.training_function_executor - Model: 9,155 parameters, 39.3KB (PASS 256KB limit)
2025-09-25 17:08:01,201 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 54.799s
2025-09-25 17:08:01,287 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9073
2025-09-25 17:08:01,287 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.087s
2025-09-25 17:08:01,288 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 0.0018259788292038777, 'batch_size': np.int64(256), 'epochs': np.int64(35), 'weight_decay': 0.00012249914218517177, 'dropout': 0.07601199715807112, 'label_smoothing': 0.05771020576152858, 'grad_clip_norm': 2.1828777721421986, 'use_scheduler': np.True_, 'channels_base': np.int64(14), 'se_reduction': np.int64(11), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(116), 'seed': np.int64(1790)}, value=0.9073
2025-09-25 17:08:01,288 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 0.0018259788292038777, 'batch_size': np.int64(256), 'epochs': np.int64(35), 'weight_decay': 0.00012249914218517177, 'dropout': 0.07601199715807112, 'label_smoothing': 0.05771020576152858, 'grad_clip_norm': 2.1828777721421986, 'use_scheduler': np.True_, 'channels_base': np.int64(14), 'se_reduction': np.int64(11), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(116), 'seed': np.int64(1790)} -> 0.9073
2025-09-25 17:08:01,288 - INFO - bo.run_bo - üîçBO Trial 11: Using RF surrogate + Expected Improvement
2025-09-25 17:08:01,288 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:08:01,288 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:08:01,288 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:08:01,288 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004797209528299423, 'batch_size': 256, 'epochs': 28, 'weight_decay': 0.007855712601838184, 'dropout': 0.1218187157744107, 'label_smoothing': 0.06723609057446699, 'grad_clip_norm': 2.2420980531441645, 'use_scheduler': False, 'channels_base': 16, 'se_reduction': 14, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 99, 'seed': 2525}
2025-09-25 17:08:01,289 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004797209528299423, 'batch_size': 256, 'epochs': 28, 'weight_decay': 0.007855712601838184, 'dropout': 0.1218187157744107, 'label_smoothing': 0.06723609057446699, 'grad_clip_norm': 2.2420980531441645, 'use_scheduler': False, 'channels_base': 16, 'se_reduction': 14, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 99, 'seed': 2525}
2025-09-25 17:08:48,654 - INFO - _models.training_function_executor - Model: 11,580 parameters, 49.8KB storage
2025-09-25 17:08:48,654 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6564551304445954, 1.4570161403653854, 1.3392123111682073, 1.2556452557350748, 1.2027315947285266, 1.171026185472795, 1.152249953384048, 1.1317117107850703, 1.1138270798248706, 1.108111559450337, 1.093216345456819, 1.084844906135254, 1.0852431686951531, 1.0807982167366232, 1.0662697958728893, 1.0579453423197764, 1.0673307733152182, 1.051036773026559, 1.0489369362879304, 1.0520991260043526, 1.043491977567314, 1.0427977819025704, 1.0416830103674377, 1.0422966919928771, 1.0352807979037342, 1.0346865664522358, 1.0316994195406708, 1.029046089337463], 'val_losses': [1.5039462384507256, 1.4085283101075514, 1.2695347094443221, 1.2321966719678166, 1.1798494464397251, 1.1846875186373365, 1.1402396430438688, 1.107926768477497, 1.1043693282941487, 1.0891059171644133, 1.0671623289667265, 1.0894864818055354, 1.0740604788091095, 1.056838700920997, 1.0565957531954706, 1.041565418661634, 1.0584277328192, 1.0420897412070085, 1.0290847011204576, 1.0385876254559818, 1.0428510716409256, 1.0500933265584467, 1.0441297227884352, 1.0628685481923816, 1.0411639146645884, 1.0425834910132625, 1.0257287687144623, 1.027608463002721], 'val_acc': [0.43540909660443555, 0.3827841122666333, 0.6055632126299962, 0.6431524871569979, 0.678235810048866, 0.6500438541536149, 0.7734619721839369, 0.7961408344818945, 0.7586768575366496, 0.8436286179676732, 0.7955143465731112, 0.7763438165643403, 0.8196967798521488, 0.7823581004886606, 0.6873825335171031, 0.7900012529758176, 0.6356346322515976, 0.8515223656183436, 0.7877458965041975, 0.7777220899636637, 0.7577997744643529, 0.7824833980704172, 0.8384914171156497, 0.7572985841373262, 0.8289688009021426, 0.7767197093096103, 0.8163137451447187, 0.8576619471244205], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004797209528299423, 'batch_size': 256, 'epochs': 28, 'weight_decay': 0.007855712601838184, 'dropout': 0.1218187157744107, 'label_smoothing': 0.06723609057446699, 'grad_clip_norm': 2.2420980531441645, 'use_scheduler': False, 'channels_base': 16, 'se_reduction': 14, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 99, 'seed': 2525}, 'model_parameter_count': 11580, 'model_storage_size_kb': 49.75781250000001, 'model_size_validation': 'PASS'}
2025-09-25 17:08:48,654 - INFO - _models.training_function_executor - BO Objective: base=0.8577, size_penalty=0.0000, final=0.8577
2025-09-25 17:08:48,654 - INFO - _models.training_function_executor - Model: 11,580 parameters, 49.8KB (PASS 256KB limit)
2025-09-25 17:08:48,654 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 47.366s
2025-09-25 17:08:48,741 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8577
2025-09-25 17:08:48,741 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.087s
2025-09-25 17:08:48,741 - INFO - bo.run_bo - Recorded observation #11: hparams={'lr': 0.004797209528299423, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'weight_decay': 0.007855712601838184, 'dropout': 0.1218187157744107, 'label_smoothing': 0.06723609057446699, 'grad_clip_norm': 2.2420980531441645, 'use_scheduler': np.False_, 'channels_base': np.int64(16), 'se_reduction': np.int64(14), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(99), 'seed': np.int64(2525)}, value=0.8577
2025-09-25 17:08:48,741 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 11: {'lr': 0.004797209528299423, 'batch_size': np.int64(256), 'epochs': np.int64(28), 'weight_decay': 0.007855712601838184, 'dropout': 0.1218187157744107, 'label_smoothing': 0.06723609057446699, 'grad_clip_norm': 2.2420980531441645, 'use_scheduler': np.False_, 'channels_base': np.int64(16), 'se_reduction': np.int64(14), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(99), 'seed': np.int64(2525)} -> 0.8577
2025-09-25 17:08:48,741 - INFO - bo.run_bo - üîçBO Trial 12: Using RF surrogate + Expected Improvement
2025-09-25 17:08:48,741 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:08:48,742 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:08:48,742 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:08:48,742 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0012542318030538677, 'batch_size': 512, 'epochs': 46, 'weight_decay': 0.00019158047676758752, 'dropout': 0.10380989186138814, 'label_smoothing': 0.0762052114400768, 'grad_clip_norm': 1.5777879156824994, 'use_scheduler': True, 'channels_base': 17, 'se_reduction': 16, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 223, 'seed': 2541}
2025-09-25 17:08:48,743 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0012542318030538677, 'batch_size': 512, 'epochs': 46, 'weight_decay': 0.00019158047676758752, 'dropout': 0.10380989186138814, 'label_smoothing': 0.0762052114400768, 'grad_clip_norm': 1.5777879156824994, 'use_scheduler': True, 'channels_base': 17, 'se_reduction': 16, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 223, 'seed': 2541}
2025-09-25 17:10:15,388 - INFO - _models.training_function_executor - Model: 12,966 parameters, 13.9KB storage
2025-09-25 17:10:15,389 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.835800223111083, 1.6357923031555315, 1.4925649987797247, 1.3856699706147093, 1.3327225857508742, 1.2916936662133407, 1.2622322531170376, 1.2492454011491885, 1.2326700728261426, 1.2205183345886212, 1.2080344476709688, 1.1996867069828963, 1.1908682878089945, 1.1839389788926338, 1.180167155229511, 1.1731067296762057, 1.168321998660899, 1.160749833367916, 1.1571396345804386, 1.1513183815894605, 1.1484171442006705, 1.1466558726102836, 1.1412832518084413, 1.135029177238808, 1.1361575389727687, 1.1327819348994799, 1.127958309582084, 1.1241329803363134, 1.1239438833525817, 1.1215814229737986, 1.1169077552127065, 1.113082939534432, 1.1157276024403318, 1.112611820303221, 1.1141563973114483, 1.113883854336627, 1.1128102173367729, 1.1090721301378523, 1.1063081093368923, 1.1049145617267113, 1.1045000582743911, 1.104717285323217, 1.1020854046420712, 1.1017069790374738, 1.1001024172874643, 1.1048074029604265], 'val_losses': [1.7305534732564323, 1.5876581524565858, 1.45463439358041, 1.360686491641346, 1.326260075505224, 1.2552276639289508, 1.3333924455520818, 1.3232284127254412, 1.2292026056112346, 1.2509715805644006, 1.2301905409851523, 1.2217634825938897, 1.2871067337696451, 1.1852244371878535, 1.2199458225435698, 1.1588543998375078, 1.1692257693889012, 1.2035380369468882, 1.2423142611629427, 1.1393530771197897, 1.1668551116939057, 1.1653799568258003, 1.1373808334202653, 1.150682101944547, 1.1323364026384266, 1.1369989420772868, 1.1339045216036148, 1.1441230658913686, 1.1256691473735154, 1.1149114775278264, 1.1223571987143734, 1.1157173828110183, 1.131833634091535, 1.1289135245150035, 1.1440062140360943, 1.113260622804826, 1.1081410488259626, 1.105804749799393, 1.1086649103670128, 1.1045866896822076, 1.1024776957355007, 1.1054906739192025, 1.1083085009245448, 1.1054859114475917, 1.0993667513247853, 1.1019172418119016], 'val_acc': [0.2551058764565844, 0.361107630622729, 0.4181180303220148, 0.4014534519483774, 0.5218644280165393, 0.5350206741009899, 0.4102242826713444, 0.4162385665956647, 0.5254980578874827, 0.46360105249968675, 0.6202230296955269, 0.6115774965543165, 0.4109760681618845, 0.5355218644280165, 0.4854028317253477, 0.6023054755043228, 0.7208369878461346, 0.483773963162511, 0.3800275654679865, 0.7269765693522116, 0.8159378523994487, 0.5420373386793634, 0.7630622728981331, 0.6887608069164265, 0.7128179426137076, 0.6211001127678236, 0.7905024433028442, 0.653050996115775, 0.7583009647913795, 0.7017917554191204, 0.8428768324771332, 0.7567973938102994, 0.6842500939731864, 0.6950256860042601, 0.5404084701165267, 0.7567973938102994, 0.7269765693522116, 0.8184438040345822, 0.7689512592406966, 0.8244580879589024, 0.791379526375141, 0.8021551184062148, 0.7580503696278662, 0.7510337050494925, 0.7889988723217642, 0.7430146598170655], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0012542318030538677, 'batch_size': 512, 'epochs': 46, 'weight_decay': 0.00019158047676758752, 'dropout': 0.10380989186138814, 'label_smoothing': 0.0762052114400768, 'grad_clip_norm': 1.5777879156824994, 'use_scheduler': True, 'channels_base': 17, 'se_reduction': 16, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 223, 'seed': 2541}, 'model_parameter_count': 12966, 'model_storage_size_kb': 13.9283203125, 'model_size_validation': 'PASS'}
2025-09-25 17:10:15,389 - INFO - _models.training_function_executor - BO Objective: base=0.7430, size_penalty=0.0000, final=0.7430
2025-09-25 17:10:15,389 - INFO - _models.training_function_executor - Model: 12,966 parameters, 13.9KB (PASS 256KB limit)
2025-09-25 17:10:15,389 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 86.647s
2025-09-25 17:10:15,475 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7430
2025-09-25 17:10:15,475 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.086s
2025-09-25 17:10:15,475 - INFO - bo.run_bo - Recorded observation #12: hparams={'lr': 0.0012542318030538677, 'batch_size': np.int64(512), 'epochs': np.int64(46), 'weight_decay': 0.00019158047676758752, 'dropout': 0.10380989186138814, 'label_smoothing': 0.0762052114400768, 'grad_clip_norm': 1.5777879156824994, 'use_scheduler': np.True_, 'channels_base': np.int64(17), 'se_reduction': np.int64(16), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(223), 'seed': np.int64(2541)}, value=0.7430
2025-09-25 17:10:15,475 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 12: {'lr': 0.0012542318030538677, 'batch_size': np.int64(512), 'epochs': np.int64(46), 'weight_decay': 0.00019158047676758752, 'dropout': 0.10380989186138814, 'label_smoothing': 0.0762052114400768, 'grad_clip_norm': 1.5777879156824994, 'use_scheduler': np.True_, 'channels_base': np.int64(17), 'se_reduction': np.int64(16), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(223), 'seed': np.int64(2541)} -> 0.7430
2025-09-25 17:10:15,475 - INFO - bo.run_bo - üîçBO Trial 13: Using RF surrogate + Expected Improvement
2025-09-25 17:10:15,475 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:10:15,476 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:10:15,476 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:10:15,476 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003422688837675557, 'batch_size': 128, 'epochs': 7, 'weight_decay': 4.634269418831586e-05, 'dropout': 0.48886763831789043, 'label_smoothing': 0.0004507036854339842, 'grad_clip_norm': 4.64162265697403, 'use_scheduler': False, 'channels_base': 17, 'se_reduction': 15, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 46, 'seed': 1612}
2025-09-25 17:10:15,477 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003422688837675557, 'batch_size': 128, 'epochs': 7, 'weight_decay': 4.634269418831586e-05, 'dropout': 0.48886763831789043, 'label_smoothing': 0.0004507036854339842, 'grad_clip_norm': 4.64162265697403, 'use_scheduler': False, 'channels_base': 17, 'se_reduction': 15, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 46, 'seed': 1612}
2025-09-25 17:10:29,942 - INFO - _models.training_function_executor - Model: 12,966 parameters, 55.7KB storage
2025-09-25 17:10:29,942 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1653886797792372, 0.8808567986035599, 0.6995374770875944, 0.5855467218233634, 0.5097547761387116, 0.4687122960805079, 0.4436683088202145], 'val_losses': [1.0055715149543203, 0.7065904313438653, 0.617621087278135, 0.46443153202884735, 0.4248703586220248, 0.41056261421026646, 0.5216682731448044], 'val_acc': [0.4238817190828217, 0.567848640521238, 0.6318757047988974, 0.8065405337676983, 0.7884976819947375, 0.8057887482771583, 0.738253351710312], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003422688837675557, 'batch_size': 128, 'epochs': 7, 'weight_decay': 4.634269418831586e-05, 'dropout': 0.48886763831789043, 'label_smoothing': 0.0004507036854339842, 'grad_clip_norm': 4.64162265697403, 'use_scheduler': False, 'channels_base': 17, 'se_reduction': 15, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 46, 'seed': 1612}, 'model_parameter_count': 12966, 'model_storage_size_kb': 55.71328125, 'model_size_validation': 'PASS'}
2025-09-25 17:10:29,942 - INFO - _models.training_function_executor - BO Objective: base=0.7383, size_penalty=0.0000, final=0.7383
2025-09-25 17:10:29,942 - INFO - _models.training_function_executor - Model: 12,966 parameters, 55.7KB (PASS 256KB limit)
2025-09-25 17:10:29,942 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 14.467s
2025-09-25 17:10:30,030 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7383
2025-09-25 17:10:30,030 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.088s
2025-09-25 17:10:30,030 - INFO - bo.run_bo - Recorded observation #13: hparams={'lr': 0.003422688837675557, 'batch_size': np.int64(128), 'epochs': np.int64(7), 'weight_decay': 4.634269418831586e-05, 'dropout': 0.48886763831789043, 'label_smoothing': 0.0004507036854339842, 'grad_clip_norm': 4.64162265697403, 'use_scheduler': np.False_, 'channels_base': np.int64(17), 'se_reduction': np.int64(15), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(46), 'seed': np.int64(1612)}, value=0.7383
2025-09-25 17:10:30,030 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 13: {'lr': 0.003422688837675557, 'batch_size': np.int64(128), 'epochs': np.int64(7), 'weight_decay': 4.634269418831586e-05, 'dropout': 0.48886763831789043, 'label_smoothing': 0.0004507036854339842, 'grad_clip_norm': 4.64162265697403, 'use_scheduler': np.False_, 'channels_base': np.int64(17), 'se_reduction': np.int64(15), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(46), 'seed': np.int64(1612)} -> 0.7383
2025-09-25 17:10:30,031 - INFO - bo.run_bo - üîçBO Trial 14: Using RF surrogate + Expected Improvement
2025-09-25 17:10:30,031 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:10:30,031 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:10:30,031 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:10:30,031 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0036637719553075866, 'batch_size': 256, 'epochs': 19, 'weight_decay': 3.6521474840665654e-06, 'dropout': 0.15813769909214695, 'label_smoothing': 0.06131591453275481, 'grad_clip_norm': 2.092072143532225, 'use_scheduler': False, 'channels_base': 12, 'se_reduction': 4, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 251, 'seed': 631}
2025-09-25 17:10:30,032 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0036637719553075866, 'batch_size': 256, 'epochs': 19, 'weight_decay': 3.6521474840665654e-06, 'dropout': 0.15813769909214695, 'label_smoothing': 0.06131591453275481, 'grad_clip_norm': 2.092072143532225, 'use_scheduler': False, 'channels_base': 12, 'se_reduction': 4, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 251, 'seed': 631}
2025-09-25 17:10:58,112 - INFO - _models.training_function_executor - Model: 7,838 parameters, 33.7KB storage
2025-09-25 17:10:58,112 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6329158864978621, 1.3564656311470185, 1.233710838642066, 1.1708421928894623, 1.1195265499417675, 1.0896944397729549, 1.0791559615268282, 1.0519204521632541, 1.0403261288393224, 1.0309681461011468, 1.0207065942050468, 1.0068547518798605, 1.0044939960281163, 1.0002856552050055, 0.9916370709177, 0.986427195651435, 0.9870259093561631, 0.9800560986962462, 0.9742697120343923], 'val_losses': [1.4155667750044674, 1.2617082041078544, 1.190192863765239, 1.1589190339761306, 1.1026414831006397, 1.1023877994590887, 1.0885302043920053, 1.0559423960565162, 1.0506767233337262, 1.005165161771025, 1.0323482220848914, 1.0086626633562967, 0.9797682098063173, 0.9842610839554099, 0.9714966190442332, 0.9742794747550361, 1.0168070266784812, 0.9862995729935138, 0.9576279526141603], 'val_acc': [0.4404209998747024, 0.5896504197468989, 0.6459090339556447, 0.61709059015161, 0.6571858163137452, 0.7014158626738504, 0.8373637388798396, 0.745520611452199, 0.8259616589399824, 0.7598045357724595, 0.8205738629244456, 0.8229545169778223, 0.8343565969176795, 0.798646786117028, 0.8085452950758051, 0.8633003383034707, 0.8522741511088836, 0.8060393434406716, 0.8787119408595414], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0036637719553075866, 'batch_size': 256, 'epochs': 19, 'weight_decay': 3.6521474840665654e-06, 'dropout': 0.15813769909214695, 'label_smoothing': 0.06131591453275481, 'grad_clip_norm': 2.092072143532225, 'use_scheduler': False, 'channels_base': 12, 'se_reduction': 4, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 251, 'seed': 631}, 'model_parameter_count': 7838, 'model_storage_size_kb': 33.678906250000004, 'model_size_validation': 'PASS'}
2025-09-25 17:10:58,112 - INFO - _models.training_function_executor - BO Objective: base=0.8787, size_penalty=0.0000, final=0.8787
2025-09-25 17:10:58,112 - INFO - _models.training_function_executor - Model: 7,838 parameters, 33.7KB (PASS 256KB limit)
2025-09-25 17:10:58,113 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 28.082s
2025-09-25 17:10:58,203 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8787
2025-09-25 17:10:58,203 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.090s
2025-09-25 17:10:58,203 - INFO - bo.run_bo - Recorded observation #14: hparams={'lr': 0.0036637719553075866, 'batch_size': np.int64(256), 'epochs': np.int64(19), 'weight_decay': 3.6521474840665654e-06, 'dropout': 0.15813769909214695, 'label_smoothing': 0.06131591453275481, 'grad_clip_norm': 2.092072143532225, 'use_scheduler': np.False_, 'channels_base': np.int64(12), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(251), 'seed': np.int64(631)}, value=0.8787
2025-09-25 17:10:58,203 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 14: {'lr': 0.0036637719553075866, 'batch_size': np.int64(256), 'epochs': np.int64(19), 'weight_decay': 3.6521474840665654e-06, 'dropout': 0.15813769909214695, 'label_smoothing': 0.06131591453275481, 'grad_clip_norm': 2.092072143532225, 'use_scheduler': np.False_, 'channels_base': np.int64(12), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(251), 'seed': np.int64(631)} -> 0.8787
2025-09-25 17:10:58,203 - INFO - bo.run_bo - üîçBO Trial 15: Using RF surrogate + Expected Improvement
2025-09-25 17:10:58,203 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:10:58,203 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:10:58,203 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:10:58,203 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0002533106501814656, 'batch_size': 256, 'epochs': 26, 'weight_decay': 4.310854698087383e-06, 'dropout': 0.10767625127199645, 'label_smoothing': 0.0021026834283404353, 'grad_clip_norm': 2.456664326653717, 'use_scheduler': False, 'channels_base': 12, 'se_reduction': 4, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 113, 'seed': 1467}
2025-09-25 17:10:58,204 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0002533106501814656, 'batch_size': 256, 'epochs': 26, 'weight_decay': 4.310854698087383e-06, 'dropout': 0.10767625127199645, 'label_smoothing': 0.0021026834283404353, 'grad_clip_norm': 2.456664326653717, 'use_scheduler': False, 'channels_base': 12, 'se_reduction': 4, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 113, 'seed': 1467}
2025-09-25 17:11:43,261 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-09-25 17:11:43,261 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5054037781230445, 1.3640111595064544, 1.2625919865496875, 1.1760077150088408, 1.098466065512285, 1.0254312994799564, 0.9585198803742353, 0.8936706879356058, 0.852866812985452, 0.8177532911304394, 0.7856496546447932, 0.7602608848255917, 0.7403672129140777, 0.7183065097276659, 0.6977878520465064, 0.6773298039336411, 0.6634122014449018, 0.6477018720624262, 0.634288093354235, 0.6199607690321011, 0.6098822220332326, 0.5997045871343087, 0.5814002026454199, 0.576539016231515, 0.5631961939597987, 0.5500611099903921], 'val_losses': [1.4184156602344005, 1.302543874612026, 1.218203636342579, 1.1328212512703024, 1.0625103865107741, 0.9794147228137128, 0.9307277799175431, 0.8559511354984967, 0.8249945679548584, 0.8099719869987422, 0.7715184290581457, 0.7547374458379307, 0.7240803088031755, 0.6926359407526257, 0.673631993410746, 0.6594709362799684, 0.6765099819583891, 0.6503293958056497, 0.6403643645830672, 0.6186510894758901, 0.6124239541798393, 0.6211190368653897, 0.5621802975750256, 0.5483098575128169, 0.5596285734375591, 0.5413625643092309], 'val_acc': [0.27816063149981207, 0.3550933466984087, 0.3939355970429771, 0.4014534519483774, 0.46259867184563336, 0.5016915173537151, 0.5105876456584388, 0.5338929958651798, 0.5212379401077559, 0.5229921062523494, 0.6357599298333542, 0.6474126049367247, 0.6421501065029445, 0.6659566470367122, 0.6360105249968676, 0.6426512968299711, 0.6619471244204986, 0.7204610951008645, 0.7025435409096604, 0.7172033579751911, 0.7015411602556071, 0.7054253852900639, 0.7512843002130059, 0.7376268638015286, 0.7564215010650295, 0.7534143591028694], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0002533106501814656, 'batch_size': 256, 'epochs': 26, 'weight_decay': 4.310854698087383e-06, 'dropout': 0.10767625127199645, 'label_smoothing': 0.0021026834283404353, 'grad_clip_norm': 2.456664326653717, 'use_scheduler': False, 'channels_base': 12, 'se_reduction': 4, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 113, 'seed': 1467}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-09-25 17:11:43,261 - INFO - _models.training_function_executor - BO Objective: base=0.7534, size_penalty=0.0000, final=0.7534
2025-09-25 17:11:43,261 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-09-25 17:11:43,261 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 45.058s
2025-09-25 17:11:43,352 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7534
2025-09-25 17:11:43,352 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.091s
2025-09-25 17:11:43,352 - INFO - bo.run_bo - Recorded observation #15: hparams={'lr': 0.0002533106501814656, 'batch_size': np.int64(256), 'epochs': np.int64(26), 'weight_decay': 4.310854698087383e-06, 'dropout': 0.10767625127199645, 'label_smoothing': 0.0021026834283404353, 'grad_clip_norm': 2.456664326653717, 'use_scheduler': np.False_, 'channels_base': np.int64(12), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(113), 'seed': np.int64(1467)}, value=0.7534
2025-09-25 17:11:43,352 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 15: {'lr': 0.0002533106501814656, 'batch_size': np.int64(256), 'epochs': np.int64(26), 'weight_decay': 4.310854698087383e-06, 'dropout': 0.10767625127199645, 'label_smoothing': 0.0021026834283404353, 'grad_clip_norm': 2.456664326653717, 'use_scheduler': np.False_, 'channels_base': np.int64(12), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(113), 'seed': np.int64(1467)} -> 0.7534
2025-09-25 17:11:43,352 - INFO - bo.run_bo - üîçBO Trial 16: Using RF surrogate + Expected Improvement
2025-09-25 17:11:43,352 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:11:43,352 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:11:43,353 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:11:43,353 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004051095035443367, 'batch_size': 256, 'epochs': 32, 'weight_decay': 0.0003277068047970935, 'dropout': 0.38304591012403266, 'label_smoothing': 0.06567391812025981, 'grad_clip_norm': 0.9167171648724705, 'use_scheduler': True, 'channels_base': 29, 'se_reduction': 9, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 178, 'seed': 3239}
2025-09-25 17:11:43,354 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004051095035443367, 'batch_size': 256, 'epochs': 32, 'weight_decay': 0.0003277068047970935, 'dropout': 0.38304591012403266, 'label_smoothing': 0.06567391812025981, 'grad_clip_norm': 0.9167171648724705, 'use_scheduler': True, 'channels_base': 29, 'se_reduction': 9, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 178, 'seed': 3239}
2025-09-25 17:13:19,020 - INFO - _models.training_function_executor - Model: 38,132 parameters, 163.8KB storage
2025-09-25 17:13:19,020 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6153551547990288, 1.308580195071408, 1.1890015585046176, 1.1368475335285793, 1.1075708030201725, 1.078944639466077, 1.0696771050240392, 1.0510006794874087, 1.0497133766570808, 1.0427739859389111, 1.0321367035584876, 1.0244531557456378, 1.0203967178293223, 1.0110412116181122, 1.0052416818958922, 1.002891232759527, 0.9974322776255503, 0.9960337608032267, 0.9945908251947819, 0.984987001750223, 0.985240773442462, 0.9819998718669561, 0.9736775337550005, 0.9741053893647315, 0.9697759082636035, 0.9699347506555904, 0.9628463362787084, 0.9646592914835946, 0.9633087091150467, 0.9608832219637109, 0.9597359973509815, 0.9603416431675734], 'val_losses': [1.4341637605474253, 1.2345850276194215, 1.136141800130613, 1.0867884350426438, 1.0827582203913206, 1.054187217700931, 1.0343102240469833, 1.0308086051482064, 1.0326759158900292, 1.0299190979765078, 1.0097038146921036, 1.0194949531089152, 1.0138683705054463, 0.9989324823804195, 1.0069266736783744, 0.9821859760670386, 1.0033474960597202, 0.9752688548302325, 0.9782414194366834, 0.9809617410550334, 0.9745903564207448, 0.9629872113507338, 0.9680607784303742, 0.9721373444944228, 0.9683334239218382, 0.9645078676324368, 0.9578128602925645, 0.9616196585723203, 0.9572466564961146, 0.9564804463977429, 0.9568079217993933, 0.9561162601809591], 'val_acc': [0.3367998997619346, 0.5782483398070417, 0.7618092970805663, 0.6997869941110136, 0.8388673098609197, 0.7744643528379902, 0.8097982708933718, 0.8532765317629369, 0.8778348577872447, 0.7927577997744644, 0.8554065906528004, 0.800902142588648, 0.907906277408846, 0.9084074677358727, 0.8847262247838616, 0.8742012279163012, 0.9437413857912542, 0.907154491918306, 0.9008896128304724, 0.9051497306101992, 0.9131687758426262, 0.8951259240696654, 0.8951259240696654, 0.9195589525122165, 0.9005137200852024, 0.9191830597669465, 0.9150482395689763, 0.9011402079939858, 0.9214384162385666, 0.9245708557824834, 0.9204360355845133, 0.9310863300338303], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004051095035443367, 'batch_size': 256, 'epochs': 32, 'weight_decay': 0.0003277068047970935, 'dropout': 0.38304591012403266, 'label_smoothing': 0.06567391812025981, 'grad_clip_norm': 0.9167171648724705, 'use_scheduler': True, 'channels_base': 29, 'se_reduction': 9, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 178, 'seed': 3239}, 'model_parameter_count': 38132, 'model_storage_size_kb': 163.84843750000002, 'model_size_validation': 'PASS'}
2025-09-25 17:13:19,020 - INFO - _models.training_function_executor - BO Objective: base=0.9311, size_penalty=0.0000, final=0.9311
2025-09-25 17:13:19,020 - INFO - _models.training_function_executor - Model: 38,132 parameters, 163.8KB (PASS 256KB limit)
2025-09-25 17:13:19,020 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 95.668s
2025-09-25 17:13:19,111 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9311
2025-09-25 17:13:19,111 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.091s
2025-09-25 17:13:19,111 - INFO - bo.run_bo - Recorded observation #16: hparams={'lr': 0.004051095035443367, 'batch_size': np.int64(256), 'epochs': np.int64(32), 'weight_decay': 0.0003277068047970935, 'dropout': 0.38304591012403266, 'label_smoothing': 0.06567391812025981, 'grad_clip_norm': 0.9167171648724705, 'use_scheduler': np.True_, 'channels_base': np.int64(29), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(178), 'seed': np.int64(3239)}, value=0.9311
2025-09-25 17:13:19,111 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 16: {'lr': 0.004051095035443367, 'batch_size': np.int64(256), 'epochs': np.int64(32), 'weight_decay': 0.0003277068047970935, 'dropout': 0.38304591012403266, 'label_smoothing': 0.06567391812025981, 'grad_clip_norm': 0.9167171648724705, 'use_scheduler': np.True_, 'channels_base': np.int64(29), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(178), 'seed': np.int64(3239)} -> 0.9311
2025-09-25 17:13:19,111 - INFO - bo.run_bo - üîçBO Trial 17: Using RF surrogate + Expected Improvement
2025-09-25 17:13:19,111 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:13:19,111 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:13:19,111 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:13:19,111 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0024876776459602378, 'batch_size': 256, 'epochs': 14, 'weight_decay': 0.00044097078880716694, 'dropout': 0.26002566371859653, 'label_smoothing': 0.07469168841118305, 'grad_clip_norm': 1.6251436280539469, 'use_scheduler': False, 'channels_base': 26, 'se_reduction': 8, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 44, 'seed': 892}
2025-09-25 17:13:19,112 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0024876776459602378, 'batch_size': 256, 'epochs': 14, 'weight_decay': 0.00044097078880716694, 'dropout': 0.26002566371859653, 'label_smoothing': 0.07469168841118305, 'grad_clip_norm': 1.6251436280539469, 'use_scheduler': False, 'channels_base': 26, 'se_reduction': 8, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 44, 'seed': 892}
2025-09-25 17:13:58,106 - INFO - _models.training_function_executor - Model: 31,357 parameters, 134.7KB storage
2025-09-25 17:13:58,106 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.672645172162996, 1.3959504949551, 1.2698894896998842, 1.2181453441044734, 1.1910226391890075, 1.1689839466462901, 1.1471449607439486, 1.133072169252839, 1.1247819012162068, 1.1133679438215396, 1.1065276911304778, 1.1034832846172486, 1.0900551947093087, 1.090146396313455], 'val_losses': [1.5100326636667734, 1.3722349084001662, 1.2276820516305496, 1.1926896912369598, 1.1625882660651055, 1.1515966405128868, 1.2359355402058518, 1.1469020834693244, 1.1085476268130934, 1.0975409027269767, 1.1334259046257744, 1.1047448202297363, 1.126958165666451, 1.0928553193900956], 'val_acc': [0.3017165768700664, 0.5648414985590778, 0.531136449066533, 0.6119533892995865, 0.6779852148853527, 0.6149605312617467, 0.5265004385415362, 0.7131938353589776, 0.7797268512717704, 0.8402455832602431, 0.8502693898007768, 0.8496429018919934, 0.792131311865681, 0.8750783109885979], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0024876776459602378, 'batch_size': 256, 'epochs': 14, 'weight_decay': 0.00044097078880716694, 'dropout': 0.26002566371859653, 'label_smoothing': 0.07469168841118305, 'grad_clip_norm': 1.6251436280539469, 'use_scheduler': False, 'channels_base': 26, 'se_reduction': 8, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 44, 'seed': 892}, 'model_parameter_count': 31357, 'model_storage_size_kb': 134.73710937500002, 'model_size_validation': 'PASS'}
2025-09-25 17:13:58,106 - INFO - _models.training_function_executor - BO Objective: base=0.8751, size_penalty=0.0000, final=0.8751
2025-09-25 17:13:58,106 - INFO - _models.training_function_executor - Model: 31,357 parameters, 134.7KB (PASS 256KB limit)
2025-09-25 17:13:58,106 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 38.995s
2025-09-25 17:13:58,198 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8751
2025-09-25 17:13:58,198 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-09-25 17:13:58,198 - INFO - bo.run_bo - Recorded observation #17: hparams={'lr': 0.0024876776459602378, 'batch_size': np.int64(256), 'epochs': np.int64(14), 'weight_decay': 0.00044097078880716694, 'dropout': 0.26002566371859653, 'label_smoothing': 0.07469168841118305, 'grad_clip_norm': 1.6251436280539469, 'use_scheduler': np.False_, 'channels_base': np.int64(26), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(44), 'seed': np.int64(892)}, value=0.8751
2025-09-25 17:13:58,198 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 17: {'lr': 0.0024876776459602378, 'batch_size': np.int64(256), 'epochs': np.int64(14), 'weight_decay': 0.00044097078880716694, 'dropout': 0.26002566371859653, 'label_smoothing': 0.07469168841118305, 'grad_clip_norm': 1.6251436280539469, 'use_scheduler': np.False_, 'channels_base': np.int64(26), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(44), 'seed': np.int64(892)} -> 0.8751
2025-09-25 17:13:58,198 - INFO - bo.run_bo - üîçBO Trial 18: Using RF surrogate + Expected Improvement
2025-09-25 17:13:58,198 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:13:58,199 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:13:58,199 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:13:58,199 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0287507073579447e-05, 'batch_size': 256, 'epochs': 37, 'weight_decay': 0.0029869743778628507, 'dropout': 0.2516237401208431, 'label_smoothing': 0.059518597192608026, 'grad_clip_norm': 0.7229872539652134, 'use_scheduler': True, 'channels_base': 26, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 156, 'seed': 4439}
2025-09-25 17:13:58,200 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0287507073579447e-05, 'batch_size': 256, 'epochs': 37, 'weight_decay': 0.0029869743778628507, 'dropout': 0.2516237401208431, 'label_smoothing': 0.059518597192608026, 'grad_clip_norm': 0.7229872539652134, 'use_scheduler': True, 'channels_base': 26, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 156, 'seed': 4439}
2025-09-25 17:15:41,316 - INFO - _models.training_function_executor - Model: 31,671 parameters, 136.1KB storage
2025-09-25 17:15:41,316 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.0136143741641126, 1.9811421261766413, 1.9545830545376721, 1.931650054128801, 1.9114259468472117, 1.8906721011613115, 1.873189537006038, 1.856695366832609, 1.8442932013726243, 1.8320112030252875, 1.819040252754377, 1.8108990648592473, 1.8011545399968547, 1.792443689362535, 1.7848414025487291, 1.7818867877563118, 1.775548780849097, 1.7684611551998306, 1.7633165339981745, 1.7583387070639003, 1.7544276527872555, 1.749362799927349, 1.7477169478406016, 1.7440837559363778, 1.7416806500406823, 1.738840664239305, 1.7344406195549955, 1.7331102932357991, 1.7330421135359322, 1.7331370011803366, 1.7323998614361018, 1.7284835193015518, 1.728363185479625, 1.7273147111398641, 1.7270135199953074, 1.7271459088290997, 1.7240313135494292], 'val_losses': [1.982424165294099, 1.9548634534610452, 1.9307758109400168, 1.9086490776466853, 1.8890721208842203, 1.8717691210663239, 1.856324433354324, 1.8438392959879597, 1.8301665999808416, 1.817602989981608, 1.807385726704542, 1.797823041890322, 1.788563599937199, 1.781942016542772, 1.7735716150362917, 1.7687192564096654, 1.761321758704143, 1.7562943190325024, 1.7509103313661611, 1.7454917240584946, 1.7422020694931981, 1.7385701562300495, 1.7342828225259002, 1.732001355829252, 1.7304696166534768, 1.72728132136617, 1.724964152181855, 1.7233234090235072, 1.7222768105057722, 1.7221704729367222, 1.7204435032690517, 1.7194606829161574, 1.7190966990907872, 1.718287684567634, 1.7153949650318876, 1.7167037681684265, 1.7131608163723446], 'val_acc': [0.6148352336799899, 0.3746397694524496, 0.34268888610449816, 0.31863175040721714, 0.3142463350457336, 0.3180052624984338, 0.3192582383160005, 0.3133692519734369, 0.3088585390301967, 0.31148978824708684, 0.3106127051747901, 0.30748026563087333, 0.3035960405964165, 0.31048740759303345, 0.3045984212504699, 0.3071043728856033, 0.30547550432276654, 0.30271895752411976, 0.30409723092344315, 0.30560080190452327, 0.3018418744518231, 0.30196717203357976, 0.2998371131437163, 0.3022177671970931, 0.3008394937977697, 0.29758175667209624, 0.29795764941736624, 0.29520110261871946, 0.29419872196466607, 0.2968299711815562, 0.29557699536398946, 0.29394812680115273, 0.2936975316376394, 0.29256985340182934, 0.2905650920937226, 0.2891868186943992, 0.2891868186943992], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.0287507073579447e-05, 'batch_size': 256, 'epochs': 37, 'weight_decay': 0.0029869743778628507, 'dropout': 0.2516237401208431, 'label_smoothing': 0.059518597192608026, 'grad_clip_norm': 0.7229872539652134, 'use_scheduler': True, 'channels_base': 26, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 156, 'seed': 4439}, 'model_parameter_count': 31671, 'model_storage_size_kb': 136.08632812500002, 'model_size_validation': 'PASS'}
2025-09-25 17:15:41,316 - INFO - _models.training_function_executor - BO Objective: base=0.2892, size_penalty=0.0000, final=0.2892
2025-09-25 17:15:41,316 - INFO - _models.training_function_executor - Model: 31,671 parameters, 136.1KB (PASS 256KB limit)
2025-09-25 17:15:41,316 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 103.118s
2025-09-25 17:15:41,408 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2892
2025-09-25 17:15:41,408 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-09-25 17:15:41,408 - INFO - bo.run_bo - Recorded observation #18: hparams={'lr': 1.0287507073579447e-05, 'batch_size': np.int64(256), 'epochs': np.int64(37), 'weight_decay': 0.0029869743778628507, 'dropout': 0.2516237401208431, 'label_smoothing': 0.059518597192608026, 'grad_clip_norm': 0.7229872539652134, 'use_scheduler': np.True_, 'channels_base': np.int64(26), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(156), 'seed': np.int64(4439)}, value=0.2892
2025-09-25 17:15:41,408 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 18: {'lr': 1.0287507073579447e-05, 'batch_size': np.int64(256), 'epochs': np.int64(37), 'weight_decay': 0.0029869743778628507, 'dropout': 0.2516237401208431, 'label_smoothing': 0.059518597192608026, 'grad_clip_norm': 0.7229872539652134, 'use_scheduler': np.True_, 'channels_base': np.int64(26), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(156), 'seed': np.int64(4439)} -> 0.2892
2025-09-25 17:15:41,408 - INFO - bo.run_bo - üîçBO Trial 19: Using RF surrogate + Expected Improvement
2025-09-25 17:15:41,408 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:15:41,408 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:15:41,408 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:15:41,409 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004493611861854842, 'batch_size': 256, 'epochs': 25, 'weight_decay': 0.0006941953058962637, 'dropout': 0.32126702169642707, 'label_smoothing': 0.14283467521449303, 'grad_clip_norm': 1.170002654652908, 'use_scheduler': False, 'channels_base': 29, 'se_reduction': 5, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 84, 'seed': 1100}
2025-09-25 17:15:41,410 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004493611861854842, 'batch_size': 256, 'epochs': 25, 'weight_decay': 0.0006941953058962637, 'dropout': 0.32126702169642707, 'label_smoothing': 0.14283467521449303, 'grad_clip_norm': 1.170002654652908, 'use_scheduler': False, 'channels_base': 29, 'se_reduction': 5, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 84, 'seed': 1100}
2025-09-25 17:16:55,945 - INFO - _models.training_function_executor - Model: 41,398 parameters, 44.5KB storage
2025-09-25 17:16:55,945 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.094435893962382, 1.8553518176403678, 1.7520078386395728, 1.7185680359350843, 1.6907659242786726, 1.6745525342802787, 1.6633137652021308, 1.6564005050235564, 1.6493413211020507, 1.6542093566839502, 1.6373276161272954, 1.6329470743449026, 1.6367712940215406, 1.6262434187738444, 1.6268232758397787, 1.6238659681788559, 1.621285201911471, 1.6185874715132347, 1.6167030724293718, 1.6103308588005907, 1.6032439052164265, 1.6080534299005562, 1.5988774610014596, 1.6055727399717845, 1.5994479886437591], 'val_losses': [1.942170831885587, 1.7756535244860336, 1.7302820471919071, 1.684822722882977, 1.6699258713267557, 1.6611858464589828, 1.6361837482679429, 1.6216101022968166, 1.6190324577976327, 1.6273004330872982, 1.6119139567306953, 1.6485708285328022, 1.602189855667571, 1.6195591471724275, 1.6079575294032908, 1.6060965942150518, 1.6020860214338553, 1.6022493053596878, 1.5884417178502075, 1.5963770936357244, 1.5855891854523148, 1.6026111790267854, 1.5842398298875593, 1.589030026134849, 1.588491100697959], 'val_acc': [0.16877584262623732, 0.3373010900889613, 0.2514722465856409, 0.2571106377646911, 0.25159754416739755, 0.2594912918180679, 0.25924069665455457, 0.26375140959779475, 0.2626237313619847, 0.26274902894374136, 0.25974188698158124, 0.26700914672346826, 0.26437789750657814, 0.26600676606941487, 0.26237313619847136, 0.26061897005387796, 0.2652549805788748, 0.2671344443052249, 0.26788622979576493, 0.26650795639644154, 0.26738503946873826, 0.2645031950883348, 0.2648790878336048, 0.26700914672346826, 0.2648790878336048], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004493611861854842, 'batch_size': 256, 'epochs': 25, 'weight_decay': 0.0006941953058962637, 'dropout': 0.32126702169642707, 'label_smoothing': 0.14283467521449303, 'grad_clip_norm': 1.170002654652908, 'use_scheduler': False, 'channels_base': 29, 'se_reduction': 5, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 84, 'seed': 1100}, 'model_parameter_count': 41398, 'model_storage_size_kb': 44.4705078125, 'model_size_validation': 'PASS'}
2025-09-25 17:16:55,945 - INFO - _models.training_function_executor - BO Objective: base=0.2649, size_penalty=0.0000, final=0.2649
2025-09-25 17:16:55,945 - INFO - _models.training_function_executor - Model: 41,398 parameters, 44.5KB (PASS 256KB limit)
2025-09-25 17:16:55,945 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 74.537s
2025-09-25 17:16:56,039 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2649
2025-09-25 17:16:56,039 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.094s
2025-09-25 17:16:56,039 - INFO - bo.run_bo - Recorded observation #19: hparams={'lr': 0.004493611861854842, 'batch_size': np.int64(256), 'epochs': np.int64(25), 'weight_decay': 0.0006941953058962637, 'dropout': 0.32126702169642707, 'label_smoothing': 0.14283467521449303, 'grad_clip_norm': 1.170002654652908, 'use_scheduler': np.False_, 'channels_base': np.int64(29), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(84), 'seed': np.int64(1100)}, value=0.2649
2025-09-25 17:16:56,039 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 19: {'lr': 0.004493611861854842, 'batch_size': np.int64(256), 'epochs': np.int64(25), 'weight_decay': 0.0006941953058962637, 'dropout': 0.32126702169642707, 'label_smoothing': 0.14283467521449303, 'grad_clip_norm': 1.170002654652908, 'use_scheduler': np.False_, 'channels_base': np.int64(29), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(84), 'seed': np.int64(1100)} -> 0.2649
2025-09-25 17:16:56,040 - INFO - bo.run_bo - üîçBO Trial 20: Using RF surrogate + Expected Improvement
2025-09-25 17:16:56,040 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:16:56,040 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:16:56,040 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:16:56,040 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003394799250463432, 'batch_size': 64, 'epochs': 20, 'weight_decay': 5.895241172569044e-06, 'dropout': 0.4406068560471966, 'label_smoothing': 0.07731944541376244, 'grad_clip_norm': 0.6807064969590944, 'use_scheduler': False, 'channels_base': 23, 'se_reduction': 9, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 125, 'seed': 235}
2025-09-25 17:16:56,041 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003394799250463432, 'batch_size': 64, 'epochs': 20, 'weight_decay': 5.895241172569044e-06, 'dropout': 0.4406068560471966, 'label_smoothing': 0.07731944541376244, 'grad_clip_norm': 0.6807064969590944, 'use_scheduler': False, 'channels_base': 23, 'se_reduction': 9, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 125, 'seed': 235}
2025-09-25 17:17:54,572 - INFO - _models.training_function_executor - Model: 24,356 parameters, 104.7KB storage
2025-09-25 17:17:54,573 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.712844261985537, 1.3897625759042782, 1.3041079602690973, 1.2704827673202268, 1.2505122538449094, 1.2289595144402878, 1.219350660115754, 1.200755494924669, 1.2025376626827795, 1.196150689515218, 1.1880636019533903, 1.1869135125218202, 1.1898393909521086, 1.1686358945253137, 1.1739973711910707, 1.1681554959881848, 1.1674381342032938, 1.1707609230513876, 1.1634560685683113, 1.1651543491567746], 'val_losses': [1.4898238143083251, 1.3037123356736584, 1.2265852472713725, 1.1945959423855275, 1.1812245256992497, 1.1932523265217736, 1.1678272644957568, 1.1396914559817077, 1.1548614015915, 1.1570900745401405, 1.173308551453391, 1.1402193270983096, 1.143596889470517, 1.1316110928126437, 1.1279678216570228, 1.1396373137691658, 1.1265120447436276, 1.1229573579050218, 1.1333560155546616, 1.1190851266006825], 'val_acc': [0.39180553815311364, 0.6812429520110261, 0.7811051246710938, 0.645783736373888, 0.76920185440421, 0.7681994737501566, 0.8576619471244205, 0.8030322014785115, 0.9013908031574991, 0.8847262247838616, 0.8770830722967047, 0.8590402205237438, 0.8422503445683498, 0.8435033203859166, 0.862047362485904, 0.8643027189575241, 0.883473248966295, 0.8392432026061897, 0.84563337927578, 0.8789625360230547], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003394799250463432, 'batch_size': 64, 'epochs': 20, 'weight_decay': 5.895241172569044e-06, 'dropout': 0.4406068560471966, 'label_smoothing': 0.07731944541376244, 'grad_clip_norm': 0.6807064969590944, 'use_scheduler': False, 'channels_base': 23, 'se_reduction': 9, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 125, 'seed': 235}, 'model_parameter_count': 24356, 'model_storage_size_kb': 104.65468750000001, 'model_size_validation': 'PASS'}
2025-09-25 17:17:54,573 - INFO - _models.training_function_executor - BO Objective: base=0.8790, size_penalty=0.0000, final=0.8790
2025-09-25 17:17:54,573 - INFO - _models.training_function_executor - Model: 24,356 parameters, 104.7KB (PASS 256KB limit)
2025-09-25 17:17:54,573 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 58.533s
2025-09-25 17:17:54,666 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8790
2025-09-25 17:17:54,667 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.094s
2025-09-25 17:17:54,667 - INFO - bo.run_bo - Recorded observation #20: hparams={'lr': 0.003394799250463432, 'batch_size': np.int64(64), 'epochs': np.int64(20), 'weight_decay': 5.895241172569044e-06, 'dropout': 0.4406068560471966, 'label_smoothing': 0.07731944541376244, 'grad_clip_norm': 0.6807064969590944, 'use_scheduler': np.False_, 'channels_base': np.int64(23), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(125), 'seed': np.int64(235)}, value=0.8790
2025-09-25 17:17:54,667 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 20: {'lr': 0.003394799250463432, 'batch_size': np.int64(64), 'epochs': np.int64(20), 'weight_decay': 5.895241172569044e-06, 'dropout': 0.4406068560471966, 'label_smoothing': 0.07731944541376244, 'grad_clip_norm': 0.6807064969590944, 'use_scheduler': np.False_, 'channels_base': np.int64(23), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(125), 'seed': np.int64(235)} -> 0.8790
2025-09-25 17:17:54,667 - INFO - bo.run_bo - üîçBO Trial 21: Using RF surrogate + Expected Improvement
2025-09-25 17:17:54,667 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:17:54,667 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:17:54,667 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:17:54,667 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003928795660564294, 'batch_size': 256, 'epochs': 30, 'weight_decay': 0.001442603372462028, 'dropout': 0.02340077995131507, 'label_smoothing': 0.06273717593266458, 'grad_clip_norm': 1.9904999552289664, 'use_scheduler': False, 'channels_base': 29, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 234, 'seed': 6608}
2025-09-25 17:17:54,668 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003928795660564294, 'batch_size': 256, 'epochs': 30, 'weight_decay': 0.001442603372462028, 'dropout': 0.02340077995131507, 'label_smoothing': 0.06273717593266458, 'grad_clip_norm': 1.9904999552289664, 'use_scheduler': False, 'channels_base': 29, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 234, 'seed': 6608}
2025-09-25 17:19:24,118 - INFO - _models.training_function_executor - Model: 39,357 parameters, 169.1KB storage
2025-09-25 17:19:24,118 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5498109717486037, 1.2627598510194653, 1.1343276172085626, 1.0790778702819936, 1.047736399802514, 1.037979762084322, 1.0174164761764433, 1.0052462090220482, 0.9968591986288646, 0.9857385536260825, 0.9846848767640891, 0.9778041999165219, 0.97098452191684, 0.9649026721124978, 0.9610082131147915, 0.9617779340801675, 0.95902333699891, 0.9534983205469659, 0.9446606367515583, 0.950958945576202, 0.9479346668959613, 0.944241896808041, 0.9414618329043167, 0.9382944061982954, 0.93572175866869, 0.937260218044236, 0.9313326466932328, 0.9312630407483014, 0.9357859092118264, 0.9262509085467872], 'val_losses': [1.494899378909068, 1.2117515764892468, 1.128128948478737, 1.1008420587407513, 1.0813656680163637, 1.0373341088575194, 1.0288986386160641, 0.9954235821107235, 0.997038109442029, 1.0181442295873155, 0.9966298336523276, 1.0210483812804867, 1.0010665105019816, 0.9664913184241594, 0.9858636840246268, 0.9806070932393802, 1.0028383967542152, 0.9622088997992279, 0.9868850908905563, 0.9650662126673775, 0.9910973872685549, 0.9629675573669958, 1.0042781205482612, 0.9511031594555445, 0.9691942475550367, 0.9527561275631306, 0.9404782498783523, 0.9634686861401467, 0.9545646718145537, 0.9442438520206393], 'val_acc': [0.3899260744267636, 0.4953013406841248, 0.7526625736123292, 0.7603057260994863, 0.7742137576744769, 0.7480265630873324, 0.8170655306352588, 0.8372384412980829, 0.8262122541034959, 0.9000125297581757, 0.8663074802656309, 0.8838491417115649, 0.8943741385791254, 0.8829720586392683, 0.7480265630873324, 0.8371131437163263, 0.7888735747400075, 0.9209372259115399, 0.7976444054629745, 0.8676857536649543, 0.8785866432777847, 0.816188447562962, 0.8237063024683624, 0.9069038967547927, 0.9183059766946498, 0.9259491291818068, 0.8863550933466984, 0.9218143089838366, 0.9126675855155996, 0.92206490414735], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003928795660564294, 'batch_size': 256, 'epochs': 30, 'weight_decay': 0.001442603372462028, 'dropout': 0.02340077995131507, 'label_smoothing': 0.06273717593266458, 'grad_clip_norm': 1.9904999552289664, 'use_scheduler': False, 'channels_base': 29, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 234, 'seed': 6608}, 'model_parameter_count': 39357, 'model_storage_size_kb': 169.11210937500002, 'model_size_validation': 'PASS'}
2025-09-25 17:19:24,118 - INFO - _models.training_function_executor - BO Objective: base=0.9221, size_penalty=0.0000, final=0.9221
2025-09-25 17:19:24,118 - INFO - _models.training_function_executor - Model: 39,357 parameters, 169.1KB (PASS 256KB limit)
2025-09-25 17:19:24,118 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 89.451s
2025-09-25 17:19:24,213 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9221
2025-09-25 17:19:24,214 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-09-25 17:19:24,214 - INFO - bo.run_bo - Recorded observation #21: hparams={'lr': 0.003928795660564294, 'batch_size': np.int64(256), 'epochs': np.int64(30), 'weight_decay': 0.001442603372462028, 'dropout': 0.02340077995131507, 'label_smoothing': 0.06273717593266458, 'grad_clip_norm': 1.9904999552289664, 'use_scheduler': np.False_, 'channels_base': np.int64(29), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(234), 'seed': np.int64(6608)}, value=0.9221
2025-09-25 17:19:24,214 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 21: {'lr': 0.003928795660564294, 'batch_size': np.int64(256), 'epochs': np.int64(30), 'weight_decay': 0.001442603372462028, 'dropout': 0.02340077995131507, 'label_smoothing': 0.06273717593266458, 'grad_clip_norm': 1.9904999552289664, 'use_scheduler': np.False_, 'channels_base': np.int64(29), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(234), 'seed': np.int64(6608)} -> 0.9221
2025-09-25 17:19:24,214 - INFO - bo.run_bo - üîçBO Trial 22: Using RF surrogate + Expected Improvement
2025-09-25 17:19:24,214 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:19:24,214 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:19:24,214 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:19:24,214 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0023080534965567224, 'batch_size': 256, 'epochs': 27, 'weight_decay': 6.982326585979751e-05, 'dropout': 0.1403977977177299, 'label_smoothing': 0.07290221345582885, 'grad_clip_norm': 1.0629282752092368, 'use_scheduler': True, 'channels_base': 27, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 230, 'seed': 8025}
2025-09-25 17:19:24,215 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0023080534965567224, 'batch_size': 256, 'epochs': 27, 'weight_decay': 6.982326585979751e-05, 'dropout': 0.1403977977177299, 'label_smoothing': 0.07290221345582885, 'grad_clip_norm': 1.0629282752092368, 'use_scheduler': True, 'channels_base': 27, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 230, 'seed': 8025}
2025-09-25 17:20:42,019 - INFO - _models.training_function_executor - Model: 34,158 parameters, 36.7KB storage
2025-09-25 17:20:42,019 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6532475619232005, 1.383037706725905, 1.2704572143088053, 1.2176595123452814, 1.1764335075767738, 1.1549594135611247, 1.1375087860797695, 1.1255160957799835, 1.1093543009441629, 1.1002717724577755, 1.091285023919152, 1.0867441903864072, 1.0739740159649511, 1.0635247180042375, 1.060845042288507, 1.0539134041268892, 1.0469600168739954, 1.0466254770249774, 1.0410672524211155, 1.0354382260264794, 1.0296593490233343, 1.0312759403379863, 1.0272533715833672, 1.021435682301503, 1.0239895347089085, 1.0218789491222358, 1.0197253516490898], 'val_losses': [1.4694678022245793, 1.2982922955542755, 1.2957454871420364, 1.1980558869355067, 1.2188165113899347, 1.1465381354620068, 1.1432469244992698, 1.1174160915573592, 1.1113881786848545, 1.1120119385216831, 1.1065479727141496, 1.0789432134145545, 1.0694906694998703, 1.0587040814611341, 1.0728075639031434, 1.0613716641339932, 1.0468162262205223, 1.0424749266158786, 1.0370908749012433, 1.0385258930575771, 1.0366935966278046, 1.0288691182525485, 1.030564241086073, 1.0326754088808667, 1.031415374158633, 1.0247378674325391, 1.0311975457019755], 'val_acc': [0.4706177170780604, 0.5377772208996366, 0.41824332790377144, 0.7035459215637139, 0.6580628993860419, 0.5496804911665205, 0.563588522741511, 0.7951384538278411, 0.661821826838742, 0.6536774840245583, 0.7547926325021928, 0.8134319007643153, 0.8036586893872948, 0.8165643403082321, 0.8646786117027941, 0.7960155369001378, 0.8673098609196842, 0.8833479513845383, 0.8768324771331913, 0.8555318882345571, 0.876205989224408, 0.8586643277784739, 0.8401202856784864, 0.8837238441298083, 0.8894875328906152, 0.8634256358852274, 0.8591655181055006], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0023080534965567224, 'batch_size': 256, 'epochs': 27, 'weight_decay': 6.982326585979751e-05, 'dropout': 0.1403977977177299, 'label_smoothing': 0.07290221345582885, 'grad_clip_norm': 1.0629282752092368, 'use_scheduler': True, 'channels_base': 27, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 230, 'seed': 8025}, 'model_parameter_count': 34158, 'model_storage_size_kb': 36.6931640625, 'model_size_validation': 'PASS'}
2025-09-25 17:20:42,020 - INFO - _models.training_function_executor - BO Objective: base=0.8592, size_penalty=0.0000, final=0.8592
2025-09-25 17:20:42,020 - INFO - _models.training_function_executor - Model: 34,158 parameters, 36.7KB (PASS 256KB limit)
2025-09-25 17:20:42,020 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 77.806s
2025-09-25 17:20:42,115 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8592
2025-09-25 17:20:42,115 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-09-25 17:20:42,115 - INFO - bo.run_bo - Recorded observation #22: hparams={'lr': 0.0023080534965567224, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'weight_decay': 6.982326585979751e-05, 'dropout': 0.1403977977177299, 'label_smoothing': 0.07290221345582885, 'grad_clip_norm': 1.0629282752092368, 'use_scheduler': np.True_, 'channels_base': np.int64(27), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(230), 'seed': np.int64(8025)}, value=0.8592
2025-09-25 17:20:42,115 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 22: {'lr': 0.0023080534965567224, 'batch_size': np.int64(256), 'epochs': np.int64(27), 'weight_decay': 6.982326585979751e-05, 'dropout': 0.1403977977177299, 'label_smoothing': 0.07290221345582885, 'grad_clip_norm': 1.0629282752092368, 'use_scheduler': np.True_, 'channels_base': np.int64(27), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(230), 'seed': np.int64(8025)} -> 0.8592
2025-09-25 17:20:42,115 - INFO - bo.run_bo - üîçBO Trial 23: Using RF surrogate + Expected Improvement
2025-09-25 17:20:42,115 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:20:42,115 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:20:42,115 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:20:42,115 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003496847572888418, 'batch_size': 128, 'epochs': 40, 'weight_decay': 5.89111509381707e-06, 'dropout': 0.013358322059075004, 'label_smoothing': 0.07805148074337888, 'grad_clip_norm': 1.2660238021094263, 'use_scheduler': True, 'channels_base': 29, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 152, 'seed': 49}
2025-09-25 17:20:42,116 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003496847572888418, 'batch_size': 128, 'epochs': 40, 'weight_decay': 5.89111509381707e-06, 'dropout': 0.013358322059075004, 'label_smoothing': 0.07805148074337888, 'grad_clip_norm': 1.2660238021094263, 'use_scheduler': True, 'channels_base': 29, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 152, 'seed': 49}
2025-09-25 17:22:49,459 - INFO - _models.training_function_executor - Model: 38,715 parameters, 166.4KB storage
2025-09-25 17:22:49,460 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6411093683860896, 1.4014711553947454, 1.3004830163571446, 1.2449343536288704, 1.2195525959470654, 1.1912715754868808, 1.1789330872356252, 1.1670065279335275, 1.1540301363782521, 1.1509509707320227, 1.1372328448205224, 1.1313432774100727, 1.1281231651739927, 1.1234843856174128, 1.1141364350599876, 1.1086762120235805, 1.1104419796149338, 1.1082820777505, 1.0968852980977906, 1.0986135984928698, 1.096112818876738, 1.0946684503033992, 1.0928070724367092, 1.0813405258040214, 1.080613126477827, 1.078774206068411, 1.0756676553638007, 1.0711475872804515, 1.0727100492782193, 1.0657419055915958, 1.0672422236519135, 1.064482420146105, 1.064770534799365, 1.0555518898446956, 1.0567143260919185, 1.057718287978716, 1.0569686399786125, 1.050199366454563, 1.05351788552039, 1.04869978858561], 'val_losses': [1.4948698357671832, 1.3641849223600415, 1.2702514913473164, 1.2365703490455264, 1.2158477330264583, 1.1759105686734546, 1.1962027167515146, 1.1611765927650652, 1.1468262452258307, 1.1397704453474895, 1.1311400128462428, 1.12042459203641, 1.1286408844306017, 1.1441257076445057, 1.1150423941673782, 1.1466292274758536, 1.1230035678976804, 1.10849132733213, 1.103649628011524, 1.0948562220872278, 1.087984734408435, 1.099918955595119, 1.1016332057914164, 1.087403181366657, 1.0889874127645747, 1.0866007314650221, 1.0931863044381007, 1.0832458524055133, 1.0899161634969765, 1.07887414042147, 1.081882530623339, 1.0828220728509872, 1.0752056499065843, 1.0757775374663352, 1.0772505065221085, 1.074736251297518, 1.0758132175191752, 1.0729014114872912, 1.07665337368803, 1.0719206487157114], 'val_acc': [0.4687382533517103, 0.4966796140834482, 0.6858789625360231, 0.5036962786618219, 0.7079313369251974, 0.7816063149981205, 0.6559328404961784, 0.8566595664703671, 0.8381155243703796, 0.800902142588648, 0.8051622603683749, 0.8633003383034707, 0.805538153113645, 0.8799649166771081, 0.8505199849642902, 0.7575491793008395, 0.7823581004886606, 0.8327277283548428, 0.909033955644656, 0.8800902142588648, 0.8917428893622353, 0.8690640270642777, 0.885352712692645, 0.8725723593534644, 0.9105375266257362, 0.8590402205237438, 0.9016413983210124, 0.9055256233554693, 0.8932464603433153, 0.9089086580628993, 0.9302092469615336, 0.8950006264879088, 0.9055256233554693, 0.9233178799649167, 0.8938729482520987, 0.9105375266257362, 0.9248214509459968, 0.9102869314622227, 0.9272021049993735, 0.9194336549304598], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003496847572888418, 'batch_size': 128, 'epochs': 40, 'weight_decay': 5.89111509381707e-06, 'dropout': 0.013358322059075004, 'label_smoothing': 0.07805148074337888, 'grad_clip_norm': 1.2660238021094263, 'use_scheduler': True, 'channels_base': 29, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 152, 'seed': 49}, 'model_parameter_count': 38715, 'model_storage_size_kb': 166.353515625, 'model_size_validation': 'PASS'}
2025-09-25 17:22:49,460 - INFO - _models.training_function_executor - BO Objective: base=0.9194, size_penalty=0.0000, final=0.9194
2025-09-25 17:22:49,460 - INFO - _models.training_function_executor - Model: 38,715 parameters, 166.4KB (PASS 256KB limit)
2025-09-25 17:22:49,460 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 127.345s
2025-09-25 17:22:49,556 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9194
2025-09-25 17:22:49,556 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.096s
2025-09-25 17:22:49,556 - INFO - bo.run_bo - Recorded observation #23: hparams={'lr': 0.003496847572888418, 'batch_size': np.int64(128), 'epochs': np.int64(40), 'weight_decay': 5.89111509381707e-06, 'dropout': 0.013358322059075004, 'label_smoothing': 0.07805148074337888, 'grad_clip_norm': 1.2660238021094263, 'use_scheduler': np.True_, 'channels_base': np.int64(29), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(152), 'seed': np.int64(49)}, value=0.9194
2025-09-25 17:22:49,556 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 23: {'lr': 0.003496847572888418, 'batch_size': np.int64(128), 'epochs': np.int64(40), 'weight_decay': 5.89111509381707e-06, 'dropout': 0.013358322059075004, 'label_smoothing': 0.07805148074337888, 'grad_clip_norm': 1.2660238021094263, 'use_scheduler': np.True_, 'channels_base': np.int64(29), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(152), 'seed': np.int64(49)} -> 0.9194
2025-09-25 17:22:49,556 - INFO - bo.run_bo - üîçBO Trial 24: Using RF surrogate + Expected Improvement
2025-09-25 17:22:49,556 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:22:49,556 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:22:49,556 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:22:49,556 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0032462809272171705, 'batch_size': 128, 'epochs': 34, 'weight_decay': 1.958555712435425e-05, 'dropout': 0.362325006877321, 'label_smoothing': 0.07887511924176305, 'grad_clip_norm': 3.6937800988710063, 'use_scheduler': False, 'channels_base': 19, 'se_reduction': 10, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 189, 'seed': 9543}
2025-09-25 17:22:49,558 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0032462809272171705, 'batch_size': 128, 'epochs': 34, 'weight_decay': 1.958555712435425e-05, 'dropout': 0.362325006877321, 'label_smoothing': 0.07887511924176305, 'grad_clip_norm': 3.6937800988710063, 'use_scheduler': False, 'channels_base': 19, 'se_reduction': 10, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 189, 'seed': 9543}
2025-09-25 17:24:04,633 - INFO - _models.training_function_executor - Model: 16,508 parameters, 17.7KB storage
2025-09-25 17:24:04,634 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.7273710661654715, 1.4823729341700722, 1.3525911378437754, 1.298498860988253, 1.2678190877151323, 1.2476859562932423, 1.2282447174867845, 1.222310772561431, 1.207599217725219, 1.1923180031148324, 1.196066935924354, 1.1914762736629008, 1.188740309059323, 1.1784960958356678, 1.1740669422354595, 1.166166112789873, 1.170493214768888, 1.1690001888867851, 1.159353061905071, 1.1628251107299261, 1.1558724671786162, 1.1522288942487902, 1.1510034775340354, 1.1548883117324849, 1.1425156854684724, 1.1471532486855034, 1.1463245495748196, 1.142005906709098, 1.1369233715337415, 1.1376540414395946, 1.1368850178651797, 1.1352275770170528, 1.1322226458648443, 1.1345826846161189], 'val_losses': [1.5334377395167205, 1.3861526204087564, 1.2815010901934938, 1.2236558765475425, 1.2052218892523228, 1.190954293960946, 1.1915728644968617, 1.1680351303060197, 1.1584785757111422, 1.1711776118248032, 1.1908328918836542, 1.1730610913552821, 1.1518417731993569, 1.1394879904893866, 1.1376083478818872, 1.1281737366679796, 1.1350100935439003, 1.1342805127066309, 1.1409336023948469, 1.1231503410456458, 1.129343025293913, 1.121834123632003, 1.1184325671527577, 1.1247158445461758, 1.1269376959034172, 1.1226889932682873, 1.1086440932973258, 1.116487474552657, 1.1281660733224754, 1.1070029520477631, 1.1211945745912415, 1.1201648702598757, 1.1082292302421664, 1.1020769845302683], 'val_acc': [0.4197468988848515, 0.6040596416489161, 0.5361483523367999, 0.6122039844630999, 0.6256108257110637, 0.5416614459340935, 0.7111890740508708, 0.7282295451697782, 0.7308607943866683, 0.6946497932589901, 0.7499060268136825, 0.7551685252474627, 0.6601929582759053, 0.8002756546798647, 0.7678235810048866, 0.808921187821075, 0.8005262498433781, 0.7970179175541912, 0.777596792381907, 0.7888735747400075, 0.8160631499812053, 0.7861170279413607, 0.8258363613582258, 0.7877458965041975, 0.8090464854028317, 0.7920060142839244, 0.8619220649041474, 0.7660694148602932, 0.7734619721839369, 0.885352712692645, 0.8436286179676732, 0.8798396190953515, 0.8446309986217266, 0.8996366370129056], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0032462809272171705, 'batch_size': 128, 'epochs': 34, 'weight_decay': 1.958555712435425e-05, 'dropout': 0.362325006877321, 'label_smoothing': 0.07887511924176305, 'grad_clip_norm': 3.6937800988710063, 'use_scheduler': False, 'channels_base': 19, 'se_reduction': 10, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 189, 'seed': 9543}, 'model_parameter_count': 16508, 'model_storage_size_kb': 17.733203125000003, 'model_size_validation': 'PASS'}
2025-09-25 17:24:04,634 - INFO - _models.training_function_executor - BO Objective: base=0.8996, size_penalty=0.0000, final=0.8996
2025-09-25 17:24:04,634 - INFO - _models.training_function_executor - Model: 16,508 parameters, 17.7KB (PASS 256KB limit)
2025-09-25 17:24:04,634 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 75.077s
2025-09-25 17:24:04,730 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8996
2025-09-25 17:24:04,731 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.097s
2025-09-25 17:24:04,731 - INFO - bo.run_bo - Recorded observation #24: hparams={'lr': 0.0032462809272171705, 'batch_size': np.int64(128), 'epochs': np.int64(34), 'weight_decay': 1.958555712435425e-05, 'dropout': 0.362325006877321, 'label_smoothing': 0.07887511924176305, 'grad_clip_norm': 3.6937800988710063, 'use_scheduler': np.False_, 'channels_base': np.int64(19), 'se_reduction': np.int64(10), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(189), 'seed': np.int64(9543)}, value=0.8996
2025-09-25 17:24:04,731 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 24: {'lr': 0.0032462809272171705, 'batch_size': np.int64(128), 'epochs': np.int64(34), 'weight_decay': 1.958555712435425e-05, 'dropout': 0.362325006877321, 'label_smoothing': 0.07887511924176305, 'grad_clip_norm': 3.6937800988710063, 'use_scheduler': np.False_, 'channels_base': np.int64(19), 'se_reduction': np.int64(10), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(189), 'seed': np.int64(9543)} -> 0.8996
2025-09-25 17:24:04,731 - INFO - bo.run_bo - üîçBO Trial 25: Using RF surrogate + Expected Improvement
2025-09-25 17:24:04,731 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:24:04,731 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:24:04,731 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:24:04,731 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0038127012643755826, 'batch_size': 128, 'epochs': 23, 'weight_decay': 1.1643770327703791e-05, 'dropout': 0.4172077889604705, 'label_smoothing': 0.07838980598513842, 'grad_clip_norm': 3.0296793555787604, 'use_scheduler': True, 'channels_base': 31, 'se_reduction': 5, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 164, 'seed': 457}
2025-09-25 17:24:04,732 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0038127012643755826, 'batch_size': 128, 'epochs': 23, 'weight_decay': 1.1643770327703791e-05, 'dropout': 0.4172077889604705, 'label_smoothing': 0.07838980598513842, 'grad_clip_norm': 3.0296793555787604, 'use_scheduler': True, 'channels_base': 31, 'se_reduction': 5, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 164, 'seed': 457}
2025-09-25 17:25:21,604 - INFO - _models.training_function_executor - Model: 47,105 parameters, 202.4KB storage
2025-09-25 17:25:21,604 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.698791367635054, 1.4416585635881998, 1.3252188268693823, 1.279398678306783, 1.2645894938407303, 1.2427792330342076, 1.2285147142698565, 1.208791612327814, 1.199169367360422, 1.1872679692956585, 1.178533914139765, 1.1658911298877244, 1.1646734602980173, 1.1553680142808729, 1.1445950265236637, 1.1454683099926606, 1.1426391389978428, 1.134498124862305, 1.1334712479051274, 1.1246809860527995, 1.1218767283431301, 1.1198380716917333, 1.1225037511546776], 'val_losses': [1.5026038120659562, 1.3876084383530898, 1.2516080120454947, 1.275243102003049, 1.211888074785258, 1.2025630987313378, 1.1937214534633098, 1.1931708634549432, 1.1647996538609375, 1.1570566486795508, 1.1380992938575702, 1.1353206312188886, 1.1465743929950565, 1.1251742076252416, 1.1300226390966719, 1.1275305013999561, 1.1194789354073167, 1.1211384143645327, 1.1135301609383448, 1.1123287806817426, 1.107691525307653, 1.1083521777832812, 1.1070206644480993], 'val_acc': [0.41749154241323144, 0.44630998621726603, 0.698534018293447, 0.754291442175166, 0.6589399824583385, 0.8149354717453953, 0.8253351710311991, 0.8000250595163514, 0.7879964916677108, 0.7824833980704172, 0.7871194085954141, 0.8034080942237815, 0.7883723844129809, 0.8486405212379401, 0.8205738629244456, 0.8369878461345696, 0.8342312993359228, 0.8289688009021426, 0.8248339807041725, 0.8369878461345696, 0.8602931963413106, 0.8465104623480767, 0.8378649292068663], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0038127012643755826, 'batch_size': 128, 'epochs': 23, 'weight_decay': 1.1643770327703791e-05, 'dropout': 0.4172077889604705, 'label_smoothing': 0.07838980598513842, 'grad_clip_norm': 3.0296793555787604, 'use_scheduler': True, 'channels_base': 31, 'se_reduction': 5, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 164, 'seed': 457}, 'model_parameter_count': 47105, 'model_storage_size_kb': 202.40429687500003, 'model_size_validation': 'PASS'}
2025-09-25 17:25:21,604 - INFO - _models.training_function_executor - BO Objective: base=0.8379, size_penalty=0.0000, final=0.8379
2025-09-25 17:25:21,604 - INFO - _models.training_function_executor - Model: 47,105 parameters, 202.4KB (PASS 256KB limit)
2025-09-25 17:25:21,604 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 76.873s
2025-09-25 17:25:21,701 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8379
2025-09-25 17:25:21,701 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.097s
2025-09-25 17:25:21,701 - INFO - bo.run_bo - Recorded observation #25: hparams={'lr': 0.0038127012643755826, 'batch_size': np.int64(128), 'epochs': np.int64(23), 'weight_decay': 1.1643770327703791e-05, 'dropout': 0.4172077889604705, 'label_smoothing': 0.07838980598513842, 'grad_clip_norm': 3.0296793555787604, 'use_scheduler': np.True_, 'channels_base': np.int64(31), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(164), 'seed': np.int64(457)}, value=0.8379
2025-09-25 17:25:21,701 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 25: {'lr': 0.0038127012643755826, 'batch_size': np.int64(128), 'epochs': np.int64(23), 'weight_decay': 1.1643770327703791e-05, 'dropout': 0.4172077889604705, 'label_smoothing': 0.07838980598513842, 'grad_clip_norm': 3.0296793555787604, 'use_scheduler': np.True_, 'channels_base': np.int64(31), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(164), 'seed': np.int64(457)} -> 0.8379
2025-09-25 17:25:21,702 - INFO - bo.run_bo - üîçBO Trial 26: Using RF surrogate + Expected Improvement
2025-09-25 17:25:21,702 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:25:21,702 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:25:21,702 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:25:21,702 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002231697041721208, 'batch_size': 512, 'epochs': 42, 'weight_decay': 0.00809374655906349, 'dropout': 0.3786630147427274, 'label_smoothing': 0.06176652651276674, 'grad_clip_norm': 0.7265392978644037, 'use_scheduler': True, 'channels_base': 30, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 162, 'seed': 80}
2025-09-25 17:25:21,703 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002231697041721208, 'batch_size': 512, 'epochs': 42, 'weight_decay': 0.00809374655906349, 'dropout': 0.3786630147427274, 'label_smoothing': 0.06176652651276674, 'grad_clip_norm': 0.7265392978644037, 'use_scheduler': True, 'channels_base': 30, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 162, 'seed': 80}
2025-09-25 17:27:29,904 - INFO - _models.training_function_executor - Model: 42,124 parameters, 181.0KB storage
2025-09-25 17:27:29,904 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6646990271972109, 1.3909887179013007, 1.2726330528005945, 1.2022099657300398, 1.1540040333453334, 1.1166653367013175, 1.0970124079425827, 1.0653583348979292, 1.0522426729721635, 1.0411079851669787, 1.0307451521652375, 1.0270208772649383, 1.0133675854333724, 1.0062259765539618, 0.9955494003786134, 0.9895043412616549, 0.9820668737164558, 0.9814646772337083, 0.9779225412875351, 0.975085981738443, 0.9688167308530977, 0.966369798186578, 0.9606634295009504, 0.9546728799155125, 0.9534230430043364, 0.9510856193957373, 0.9450694479985788, 0.9451568957330798, 0.9406417825913379, 0.9392599555344688, 0.9353437073543004, 0.9340497936233185, 0.9318298988259683, 0.9304590661420494, 0.9284887102034645, 0.9299072992292684, 0.9249078777840691, 0.9230871838133119, 0.9250112476830747, 0.9230809115834752, 0.9206179548011173, 0.9212185213993999], 'val_losses': [1.5201914092977906, 1.2898212441853183, 1.288369198398712, 1.3065212129605561, 1.147303285821012, 1.1031886206699664, 1.0959012922435876, 1.0377922358331846, 1.0241122929139657, 1.0243889846653587, 1.013657041695942, 1.026559423719219, 0.9915567455457843, 1.0449891208541855, 0.9838268768815763, 0.9707187197438727, 0.9852400705742964, 0.967172732830884, 0.9905711558286058, 0.9575030550056346, 0.9687213202554052, 0.9489471529483376, 0.9472402578725984, 0.9528149355549723, 0.9660097758000155, 0.9397949997366946, 0.938373008365545, 0.9363058101800791, 0.9342885047193608, 0.9539615007409356, 0.9272399294751282, 0.9300882616313145, 0.9298556700279772, 0.9323302596572228, 0.9281574058257881, 0.9240020032500194, 0.9243787344558663, 0.9221375617238905, 0.919126123492393, 0.9214576929527601, 0.9303841691180856, 0.9260036345188996], 'val_acc': [0.2760305726099486, 0.4428016539280792, 0.5298834732489663, 0.49880967297331164, 0.5125924069665455, 0.6025560706678361, 0.5585766194712442, 0.6990352086204736, 0.7987720836987846, 0.7797268512717704, 0.7871194085954141, 0.7685753664954267, 0.8759553940608946, 0.661821826838742, 0.8095476757298584, 0.8526500438541537, 0.7442676356346323, 0.8695652173913043, 0.7479012655055758, 0.8942488409973688, 0.8589149229419872, 0.8856033078561584, 0.8843503320385917, 0.9280791880716702, 0.8490164139832101, 0.8495176043102368, 0.868562836737251, 0.8417491542413231, 0.8782107505325147, 0.8323518356095727, 0.8815937852399449, 0.8993860418493923, 0.8745771206615712, 0.860919684250094, 0.8725723593534644, 0.8962536023054755, 0.9130434782608695, 0.871194085954141, 0.8866056885102117, 0.8859792006014284, 0.8595414108507706, 0.9048991354466859], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002231697041721208, 'batch_size': 512, 'epochs': 42, 'weight_decay': 0.00809374655906349, 'dropout': 0.3786630147427274, 'label_smoothing': 0.06176652651276674, 'grad_clip_norm': 0.7265392978644037, 'use_scheduler': True, 'channels_base': 30, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 162, 'seed': 80}, 'model_parameter_count': 42124, 'model_storage_size_kb': 181.0015625, 'model_size_validation': 'PASS'}
2025-09-25 17:27:29,904 - INFO - _models.training_function_executor - BO Objective: base=0.9049, size_penalty=0.0000, final=0.9049
2025-09-25 17:27:29,904 - INFO - _models.training_function_executor - Model: 42,124 parameters, 181.0KB (PASS 256KB limit)
2025-09-25 17:27:29,904 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 128.203s
2025-09-25 17:27:30,000 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9049
2025-09-25 17:27:30,000 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.096s
2025-09-25 17:27:30,000 - INFO - bo.run_bo - Recorded observation #26: hparams={'lr': 0.002231697041721208, 'batch_size': np.int64(512), 'epochs': np.int64(42), 'weight_decay': 0.00809374655906349, 'dropout': 0.3786630147427274, 'label_smoothing': 0.06176652651276674, 'grad_clip_norm': 0.7265392978644037, 'use_scheduler': np.True_, 'channels_base': np.int64(30), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(162), 'seed': np.int64(80)}, value=0.9049
2025-09-25 17:27:30,000 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 26: {'lr': 0.002231697041721208, 'batch_size': np.int64(512), 'epochs': np.int64(42), 'weight_decay': 0.00809374655906349, 'dropout': 0.3786630147427274, 'label_smoothing': 0.06176652651276674, 'grad_clip_norm': 0.7265392978644037, 'use_scheduler': np.True_, 'channels_base': np.int64(30), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(162), 'seed': np.int64(80)} -> 0.9049
2025-09-25 17:27:30,001 - INFO - bo.run_bo - üîçBO Trial 27: Using RF surrogate + Expected Improvement
2025-09-25 17:27:30,001 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:27:30,001 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:27:30,001 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:27:30,001 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0016906017999628474, 'batch_size': 512, 'epochs': 49, 'weight_decay': 0.0012768855334309665, 'dropout': 0.10220382882281873, 'label_smoothing': 0.07938490805436439, 'grad_clip_norm': 3.8102916962242204, 'use_scheduler': False, 'channels_base': 30, 'se_reduction': 9, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 212, 'seed': 3006}
2025-09-25 17:27:30,002 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0016906017999628474, 'batch_size': 512, 'epochs': 49, 'weight_decay': 0.0012768855334309665, 'dropout': 0.10220382882281873, 'label_smoothing': 0.07938490805436439, 'grad_clip_norm': 3.8102916962242204, 'use_scheduler': False, 'channels_base': 30, 'se_reduction': 9, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 212, 'seed': 3006}
2025-09-25 17:29:59,549 - INFO - _models.training_function_executor - Model: 40,857 parameters, 175.6KB storage
2025-09-25 17:29:59,550 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.7983142582242382, 1.55684885299658, 1.406879277895952, 1.3082806768824544, 1.2495764050160914, 1.2257240741200544, 1.2125832427396805, 1.19673566452577, 1.181212356805062, 1.163757164193825, 1.1645173713108825, 1.1512875537900993, 1.1441329625138095, 1.140431037486296, 1.13190697456696, 1.1263797189367597, 1.1228500136313977, 1.1264853529608518, 1.1195322146441253, 1.1143760406421666, 1.1131866838879534, 1.111023058972853, 1.1066463176190575, 1.1035233433263119, 1.1027614765140648, 1.0972687489585433, 1.098461467853126, 1.0941963389086902, 1.1006189830260695, 1.0931781682869406, 1.0896351055569717, 1.0884223973747706, 1.083660147917755, 1.0852362418336812, 1.0805059758149207, 1.0860745656483977, 1.0828499367477353, 1.0840027636238623, 1.0806539104691582, 1.0735897968777572, 1.0736816178675759, 1.0763361262142914, 1.0723639597358163, 1.0774139747883866, 1.0691355537401794, 1.0675897060493529, 1.0765811762082913, 1.0717208357759203, 1.0682347122859535], 'val_losses': [1.660002968003914, 1.5528524795162275, 1.3628518930608924, 1.328634698623076, 1.2796815994493556, 1.2671934968023248, 1.2042439640830473, 1.2148608946737498, 1.2431904079621037, 1.1832802155579172, 1.2928059434639811, 1.1491138376309569, 1.1421732284150137, 1.144001339569589, 1.1320783731379076, 1.1596944228462551, 1.2093887167039066, 1.1181687804305633, 1.1215750268341798, 1.1224108718866812, 1.1051753497694299, 1.1239289377674007, 1.1032558748139847, 1.1269761408022092, 1.1803053687530238, 1.1472842790118256, 1.1445311662652202, 1.1110942100406127, 1.2029095494258735, 1.0951868374385625, 1.1068579974949413, 1.0913560747756739, 1.0870509605153194, 1.0943859035606418, 1.120126688574478, 1.1080716766520649, 1.1000106640588578, 1.1789308140681451, 1.0891154173426931, 1.0861556699977695, 1.0835560133188025, 1.089786560803394, 1.0863353920197998, 1.0990122419568085, 1.0982866917856204, 1.0931618009619122, 1.0831928617478372, 1.0954275758265553, 1.0851946335480545], 'val_acc': [0.21889487532890614, 0.3038466357599298, 0.5045733617341185, 0.5140959779476256, 0.5233679989976193, 0.39180553815311364, 0.5561959654178674, 0.48014033329156747, 0.5625861420874577, 0.7266006766069415, 0.399323393058514, 0.8029069038967548, 0.7485277534143591, 0.8084199974940484, 0.7931336925197344, 0.6819947375015663, 0.8237063024683624, 0.7910036336298709, 0.8438792131311865, 0.7705801278035334, 0.7633128680616464, 0.8413732614960531, 0.8046610700413482, 0.7818569101616338, 0.8779601553690014, 0.8203232677609322, 0.7441423380528756, 0.8203232677609322, 0.8739506327527878, 0.8179426137075555, 0.8483899260744268, 0.8795890239318381, 0.8386167146974063, 0.8505199849642902, 0.714572108758301, 0.9091592532264128, 0.8260869565217391, 0.7098108006515474, 0.8441298082946999, 0.9013908031574991, 0.877333667460218, 0.8728229545169778, 0.8483899260744268, 0.8480140333291567, 0.8401202856784864, 0.84638516476632, 0.8877333667460218, 0.9109134193710061, 0.8630497431399574], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0016906017999628474, 'batch_size': 512, 'epochs': 49, 'weight_decay': 0.0012768855334309665, 'dropout': 0.10220382882281873, 'label_smoothing': 0.07938490805436439, 'grad_clip_norm': 3.8102916962242204, 'use_scheduler': False, 'channels_base': 30, 'se_reduction': 9, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 212, 'seed': 3006}, 'model_parameter_count': 40857, 'model_storage_size_kb': 175.55742187500002, 'model_size_validation': 'PASS'}
2025-09-25 17:29:59,550 - INFO - _models.training_function_executor - BO Objective: base=0.8630, size_penalty=0.0000, final=0.8630
2025-09-25 17:29:59,550 - INFO - _models.training_function_executor - Model: 40,857 parameters, 175.6KB (PASS 256KB limit)
2025-09-25 17:29:59,550 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 149.549s
2025-09-25 17:29:59,646 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8630
2025-09-25 17:29:59,646 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.096s
2025-09-25 17:29:59,646 - INFO - bo.run_bo - Recorded observation #27: hparams={'lr': 0.0016906017999628474, 'batch_size': np.int64(512), 'epochs': np.int64(49), 'weight_decay': 0.0012768855334309665, 'dropout': 0.10220382882281873, 'label_smoothing': 0.07938490805436439, 'grad_clip_norm': 3.8102916962242204, 'use_scheduler': np.False_, 'channels_base': np.int64(30), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(212), 'seed': np.int64(3006)}, value=0.8630
2025-09-25 17:29:59,646 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 27: {'lr': 0.0016906017999628474, 'batch_size': np.int64(512), 'epochs': np.int64(49), 'weight_decay': 0.0012768855334309665, 'dropout': 0.10220382882281873, 'label_smoothing': 0.07938490805436439, 'grad_clip_norm': 3.8102916962242204, 'use_scheduler': np.False_, 'channels_base': np.int64(30), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(212), 'seed': np.int64(3006)} -> 0.8630
2025-09-25 17:29:59,646 - INFO - bo.run_bo - üîçBO Trial 28: Using RF surrogate + Expected Improvement
2025-09-25 17:29:59,646 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:29:59,646 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:29:59,646 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:29:59,646 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004837527203455189, 'batch_size': 128, 'epochs': 10, 'weight_decay': 0.00013196755228563398, 'dropout': 0.22557977962317471, 'label_smoothing': 0.07920834949158367, 'grad_clip_norm': 0.2851184621086495, 'use_scheduler': True, 'channels_base': 23, 'se_reduction': 8, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 27, 'seed': 8408}
2025-09-25 17:29:59,647 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004837527203455189, 'batch_size': 128, 'epochs': 10, 'weight_decay': 0.00013196755228563398, 'dropout': 0.22557977962317471, 'label_smoothing': 0.07920834949158367, 'grad_clip_norm': 0.2851184621086495, 'use_scheduler': True, 'channels_base': 23, 'se_reduction': 8, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 27, 'seed': 8408}
2025-09-25 17:30:25,881 - INFO - _models.training_function_executor - Model: 24,541 parameters, 105.4KB storage
2025-09-25 17:30:25,881 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6627198637747336, 1.3702372065598112, 1.2743713236525778, 1.231911733971052, 1.2085657573241442, 1.1854447246837618, 1.1669619287408033, 1.1570813025470157, 1.140384875058463, 1.1382268283486878], 'val_losses': [1.442304335834239, 1.3263746370365261, 1.2124024436910295, 1.1943555949789109, 1.1762278716585985, 1.1573290019208007, 1.1372880560460656, 1.128449332428791, 1.1283030756701953, 1.1220647759372437], 'val_acc': [0.4283924320260619, 0.6723468237063025, 0.682871820573863, 0.7316125798772084, 0.7164515724846511, 0.8138077935095853, 0.8130560080190452, 0.8105500563839118, 0.8168149354717454, 0.8440045107129432], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004837527203455189, 'batch_size': 128, 'epochs': 10, 'weight_decay': 0.00013196755228563398, 'dropout': 0.22557977962317471, 'label_smoothing': 0.07920834949158367, 'grad_clip_norm': 0.2851184621086495, 'use_scheduler': True, 'channels_base': 23, 'se_reduction': 8, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 27, 'seed': 8408}, 'model_parameter_count': 24541, 'model_storage_size_kb': 105.44960937500001, 'model_size_validation': 'PASS'}
2025-09-25 17:30:25,881 - INFO - _models.training_function_executor - BO Objective: base=0.8440, size_penalty=0.0000, final=0.8440
2025-09-25 17:30:25,881 - INFO - _models.training_function_executor - Model: 24,541 parameters, 105.4KB (PASS 256KB limit)
2025-09-25 17:30:25,881 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 26.235s
2025-09-25 17:30:25,977 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8440
2025-09-25 17:30:25,977 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-09-25 17:30:25,977 - INFO - bo.run_bo - Recorded observation #28: hparams={'lr': 0.004837527203455189, 'batch_size': np.int64(128), 'epochs': np.int64(10), 'weight_decay': 0.00013196755228563398, 'dropout': 0.22557977962317471, 'label_smoothing': 0.07920834949158367, 'grad_clip_norm': 0.2851184621086495, 'use_scheduler': np.True_, 'channels_base': np.int64(23), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(27), 'seed': np.int64(8408)}, value=0.8440
2025-09-25 17:30:25,977 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 28: {'lr': 0.004837527203455189, 'batch_size': np.int64(128), 'epochs': np.int64(10), 'weight_decay': 0.00013196755228563398, 'dropout': 0.22557977962317471, 'label_smoothing': 0.07920834949158367, 'grad_clip_norm': 0.2851184621086495, 'use_scheduler': np.True_, 'channels_base': np.int64(23), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(27), 'seed': np.int64(8408)} -> 0.8440
2025-09-25 17:30:25,977 - INFO - bo.run_bo - üîçBO Trial 29: Using RF surrogate + Expected Improvement
2025-09-25 17:30:25,977 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:30:25,977 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:30:25,977 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:30:25,977 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0011712027479829532, 'batch_size': 256, 'epochs': 42, 'weight_decay': 0.004770459853501036, 'dropout': 0.18664466895322926, 'label_smoothing': 0.07938500642267332, 'grad_clip_norm': 4.709325723278958, 'use_scheduler': True, 'channels_base': 30, 'se_reduction': 9, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 158, 'seed': 3561}
2025-09-25 17:30:25,978 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0011712027479829532, 'batch_size': 256, 'epochs': 42, 'weight_decay': 0.004770459853501036, 'dropout': 0.18664466895322926, 'label_smoothing': 0.07938500642267332, 'grad_clip_norm': 4.709325723278958, 'use_scheduler': True, 'channels_base': 30, 'se_reduction': 9, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 158, 'seed': 3561}
2025-09-25 17:32:53,903 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-09-25 17:32:53,903 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.7544477487594021, 1.470827108523946, 1.3313314353217645, 1.2743183378998066, 1.242764425875941, 1.2246262128500325, 1.2031379666830466, 1.190401346493129, 1.179885365044873, 1.1685379652945174, 1.1658998243052108, 1.1546092362637337, 1.14982419972491, 1.1437044278334747, 1.1386844206359703, 1.1314610426906087, 1.130697810806338, 1.1231886779470701, 1.118812047841088, 1.115688184723461, 1.1099219358949866, 1.1094877619539156, 1.1034745110144417, 1.105601170541529, 1.096762396722189, 1.0959431509794912, 1.0938747950194418, 1.0925916421870123, 1.0877354220221023, 1.0888246830306705, 1.0819160815666662, 1.085814903031367, 1.0793807129063329, 1.0784639100011524, 1.0798630629200328, 1.0764720562432482, 1.077759481695994, 1.072868642067471, 1.073569482726807, 1.0725659084690509, 1.071963249765101, 1.0701627633466542], 'val_losses': [1.5607759666657959, 1.3822319368688165, 1.2660568947930069, 1.2372685118943971, 1.2312836845392214, 1.2423099925263814, 1.1941142192446728, 1.1773661326651077, 1.1916829028265201, 1.2074078158946313, 1.1648206611038703, 1.1684173631453003, 1.1371672747353536, 1.1349728148826874, 1.1345431696665107, 1.128923223977877, 1.1318112805498795, 1.1901328762048708, 1.1275631255216878, 1.1091655990172562, 1.1137887131763033, 1.0972291051168934, 1.096545746072225, 1.1020369304479656, 1.0963024352669581, 1.0921605866798676, 1.0971095170342613, 1.091745234270649, 1.0942336537308928, 1.0879521129454248, 1.0885408778579562, 1.089538688902239, 1.0826742475813396, 1.0842199201139349, 1.0799525999631547, 1.0786488884269945, 1.0800441040787807, 1.084805159741454, 1.0824984454702367, 1.0778889859802605, 1.0782040243175093, 1.0788290956805724], 'val_acc': [0.2680115273775216, 0.521613832853026, 0.5090840746773587, 0.6462849267009146, 0.6123292820448565, 0.7025435409096604, 0.7559203107380028, 0.7412604936724722, 0.8121789249467485, 0.8119283297832351, 0.823079814559579, 0.5977947625610825, 0.7430146598170655, 0.7799774464352838, 0.8179426137075555, 0.8594161132690139, 0.7576744768825961, 0.575867685753665, 0.8362360606440296, 0.8427515348953765, 0.7598045357724595, 0.8455080816940233, 0.8734494424257612, 0.7534143591028694, 0.84751284300213, 0.8958777095602055, 0.8239568976318757, 0.8661821826838741, 0.84563337927578, 0.8673098609196842, 0.8323518356095727, 0.8458839744392933, 0.8858539030196717, 0.9008896128304724, 0.8784613456960281, 0.8818443804034583, 0.8545295075805037, 0.8555318882345571, 0.8798396190953515, 0.8851021175291317, 0.8716952762811677, 0.8703170028818443], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0011712027479829532, 'batch_size': 256, 'epochs': 42, 'weight_decay': 0.004770459853501036, 'dropout': 0.18664466895322926, 'label_smoothing': 0.07938500642267332, 'grad_clip_norm': 4.709325723278958, 'use_scheduler': True, 'channels_base': 30, 'se_reduction': 9, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 158, 'seed': 3561}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-09-25 17:32:53,903 - INFO - _models.training_function_executor - BO Objective: base=0.8703, size_penalty=0.0000, final=0.8703
2025-09-25 17:32:53,903 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-09-25 17:32:53,903 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 147.926s
2025-09-25 17:32:54,117 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8703
2025-09-25 17:32:54,117 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.214s
2025-09-25 17:32:54,118 - INFO - bo.run_bo - Recorded observation #29: hparams={'lr': 0.0011712027479829532, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'weight_decay': 0.004770459853501036, 'dropout': 0.18664466895322926, 'label_smoothing': 0.07938500642267332, 'grad_clip_norm': 4.709325723278958, 'use_scheduler': np.True_, 'channels_base': np.int64(30), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(158), 'seed': np.int64(3561)}, value=0.8703
2025-09-25 17:32:54,118 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 29: {'lr': 0.0011712027479829532, 'batch_size': np.int64(256), 'epochs': np.int64(42), 'weight_decay': 0.004770459853501036, 'dropout': 0.18664466895322926, 'label_smoothing': 0.07938500642267332, 'grad_clip_norm': 4.709325723278958, 'use_scheduler': np.True_, 'channels_base': np.int64(30), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(158), 'seed': np.int64(3561)} -> 0.8703
2025-09-25 17:32:54,118 - INFO - bo.run_bo - üîçBO Trial 30: Using RF surrogate + Expected Improvement
2025-09-25 17:32:54,118 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:32:54,118 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:32:54,118 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:32:54,118 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002718777751375157, 'batch_size': 256, 'epochs': 29, 'weight_decay': 0.00030154583329969104, 'dropout': 0.05409300218853842, 'label_smoothing': 0.07961445225160235, 'grad_clip_norm': 1.2019187318421256, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 12, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 227, 'seed': 1013}
2025-09-25 17:32:54,119 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002718777751375157, 'batch_size': 256, 'epochs': 29, 'weight_decay': 0.00030154583329969104, 'dropout': 0.05409300218853842, 'label_smoothing': 0.07961445225160235, 'grad_clip_norm': 1.2019187318421256, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 12, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 227, 'seed': 1013}
2025-09-25 17:34:23,074 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-09-25 17:34:23,074 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6877481758163204, 1.3771962563846583, 1.270348303987878, 1.2230218404091395, 1.1935837814724468, 1.171382050942468, 1.157706417315355, 1.14998580057049, 1.1371333395460512, 1.1342519564470717, 1.1247772628702444, 1.1207688682133015, 1.1184578788156136, 1.1099983571193588, 1.1030682257996822, 1.1001454973655496, 1.0982556173897244, 1.0958484715751153, 1.0868177901114064, 1.091488033265162, 1.086349344625474, 1.0827647698865486, 1.0794433948745203, 1.078123912661012, 1.0751222544653285, 1.0763375726935298, 1.0722552650431825, 1.069301736114714, 1.0678427120656615], 'val_losses': [1.4662503581823165, 1.316922324983238, 1.335146303449324, 1.225264270896228, 1.1792391138467524, 1.209163235476883, 1.1725012470256593, 1.1339527071157116, 1.160256573078761, 1.1434765315536808, 1.12537190496227, 1.1088129330093648, 1.1184780285361715, 1.1068024292071053, 1.1294743670002438, 1.1308497652465606, 1.094795544902389, 1.099536057567704, 1.1135882541300166, 1.097406464312522, 1.091269245304241, 1.085943758121754, 1.0833990210325344, 1.090341536461926, 1.082304545235117, 1.0787799398846203, 1.079834820393437, 1.0798361028380419, 1.0796495868424518], 'val_acc': [0.2642525999248215, 0.530384663575993, 0.3730109008896128, 0.6688384914171156, 0.6259867184563338, 0.7213381781731613, 0.7943866683373011, 0.7364991855657186, 0.8110512467109385, 0.877333667460218, 0.8319759428643028, 0.8119283297832351, 0.84751284300213, 0.8397443929332163, 0.831850645282546, 0.8535271269264503, 0.8440045107129432, 0.8316000501190327, 0.8342312993359228, 0.8800902142588648, 0.8661821826838741, 0.8775842626237313, 0.8882345570730484, 0.913294073424383, 0.9154241323142464, 0.9026437789750658, 0.8893622353088585, 0.9067785991730359, 0.8726976569352212], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002718777751375157, 'batch_size': 256, 'epochs': 29, 'weight_decay': 0.00030154583329969104, 'dropout': 0.05409300218853842, 'label_smoothing': 0.07961445225160235, 'grad_clip_norm': 1.2019187318421256, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 12, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 4, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 227, 'seed': 1013}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-09-25 17:34:23,074 - INFO - _models.training_function_executor - BO Objective: base=0.8727, size_penalty=0.0000, final=0.8727
2025-09-25 17:34:23,074 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-09-25 17:34:23,074 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 88.956s
2025-09-25 17:34:23,174 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8727
2025-09-25 17:34:23,174 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-25 17:34:23,174 - INFO - bo.run_bo - Recorded observation #30: hparams={'lr': 0.002718777751375157, 'batch_size': np.int64(256), 'epochs': np.int64(29), 'weight_decay': 0.00030154583329969104, 'dropout': 0.05409300218853842, 'label_smoothing': 0.07961445225160235, 'grad_clip_norm': 1.2019187318421256, 'use_scheduler': np.True_, 'channels_base': np.int64(24), 'se_reduction': np.int64(12), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(227), 'seed': np.int64(1013)}, value=0.8727
2025-09-25 17:34:23,174 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 30: {'lr': 0.002718777751375157, 'batch_size': np.int64(256), 'epochs': np.int64(29), 'weight_decay': 0.00030154583329969104, 'dropout': 0.05409300218853842, 'label_smoothing': 0.07961445225160235, 'grad_clip_norm': 1.2019187318421256, 'use_scheduler': np.True_, 'channels_base': np.int64(24), 'se_reduction': np.int64(12), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(227), 'seed': np.int64(1013)} -> 0.8727
2025-09-25 17:34:23,175 - INFO - bo.run_bo - üîçBO Trial 31: Using RF surrogate + Expected Improvement
2025-09-25 17:34:23,175 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:34:23,175 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:34:23,175 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:34:23,175 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0016808583998706263, 'batch_size': 512, 'epochs': 33, 'weight_decay': 0.0009018684977568005, 'dropout': 0.34447342909843304, 'label_smoothing': 0.07970964351725457, 'grad_clip_norm': 0.7986916124346961, 'use_scheduler': True, 'channels_base': 18, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 233, 'seed': 7259}
2025-09-25 17:34:23,176 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0016808583998706263, 'batch_size': 512, 'epochs': 33, 'weight_decay': 0.0009018684977568005, 'dropout': 0.34447342909843304, 'label_smoothing': 0.07970964351725457, 'grad_clip_norm': 0.7986916124346961, 'use_scheduler': True, 'channels_base': 18, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 233, 'seed': 7259}
2025-09-25 17:35:29,435 - INFO - _models.training_function_executor - Model: 15,338 parameters, 65.9KB storage
2025-09-25 17:35:29,435 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.8844705401508632, 1.6481852075478018, 1.5231434536007111, 1.4554138030301724, 1.4091754736086406, 1.36821888481447, 1.333479280404286, 1.3131514409877258, 1.284173583097858, 1.2682547606393604, 1.2631382977227215, 1.248486316753383, 1.235909336846981, 1.2288021857669111, 1.2224827561503802, 1.2139836520818272, 1.2107230963886364, 1.2018118714590187, 1.1990183271292407, 1.19485424627163, 1.1916700558745614, 1.186085298911815, 1.1842709346301976, 1.1789166580871528, 1.1794774904102623, 1.174905562910173, 1.1726460990625542, 1.1735383066113676, 1.1674602781176937, 1.171783831235076, 1.1688646551055784, 1.1666047960109953, 1.1640207497535768], 'val_losses': [1.7319033115135072, 1.6480092455339737, 1.4712818514029644, 1.3945607958155428, 1.5388091522236393, 1.3028944760631938, 1.3360611018255053, 1.2813558680956532, 1.2393013804855988, 1.30680994508918, 1.2235771648986284, 1.2058493476092644, 1.1964595053462124, 1.2014472209903893, 1.1941556507536468, 1.2090770599612346, 1.1812281178406434, 1.1817574637110349, 1.1733969587563362, 1.1690193725220768, 1.164161534450389, 1.1598452055249622, 1.1662917860975361, 1.15609573732416, 1.1568196881102704, 1.152070800652556, 1.1486004462383725, 1.1489482608269874, 1.146463497616178, 1.1472486119986687, 1.144056043339887, 1.1460199003574498, 1.1404886494123792], 'val_acc': [0.1939606565593284, 0.26951509835860166, 0.3692519734369127, 0.4034582132564842, 0.22891868186943992, 0.4181180303220148, 0.3959403583510838, 0.593409347199599, 0.5791254228793384, 0.4895376519233179, 0.5856408971306853, 0.6127051747901265, 0.5461721588773336, 0.6616965292569853, 0.6540533767698283, 0.6578123042225285, 0.6913920561333167, 0.6556822453326651, 0.6796140834481894, 0.7417616839994988, 0.659942363112392, 0.7560456083197594, 0.7377521613832853, 0.7289813306603182, 0.7344944242576118, 0.7387545420373387, 0.7380027565467987, 0.7243453201353214, 0.7536649542663827, 0.7594286430271896, 0.7292319258238316, 0.7140709184312742, 0.7504072171407092], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0016808583998706263, 'batch_size': 512, 'epochs': 33, 'weight_decay': 0.0009018684977568005, 'dropout': 0.34447342909843304, 'label_smoothing': 0.07970964351725457, 'grad_clip_norm': 0.7986916124346961, 'use_scheduler': True, 'channels_base': 18, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 233, 'seed': 7259}, 'model_parameter_count': 15338, 'model_storage_size_kb': 65.90546875000001, 'model_size_validation': 'PASS'}
2025-09-25 17:35:29,435 - INFO - _models.training_function_executor - BO Objective: base=0.7504, size_penalty=0.0000, final=0.7504
2025-09-25 17:35:29,435 - INFO - _models.training_function_executor - Model: 15,338 parameters, 65.9KB (PASS 256KB limit)
2025-09-25 17:35:29,435 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 66.261s
2025-09-25 17:35:29,534 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7504
2025-09-25 17:35:29,534 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-09-25 17:35:29,534 - INFO - bo.run_bo - Recorded observation #31: hparams={'lr': 0.0016808583998706263, 'batch_size': np.int64(512), 'epochs': np.int64(33), 'weight_decay': 0.0009018684977568005, 'dropout': 0.34447342909843304, 'label_smoothing': 0.07970964351725457, 'grad_clip_norm': 0.7986916124346961, 'use_scheduler': np.True_, 'channels_base': np.int64(18), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(233), 'seed': np.int64(7259)}, value=0.7504
2025-09-25 17:35:29,534 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 31: {'lr': 0.0016808583998706263, 'batch_size': np.int64(512), 'epochs': np.int64(33), 'weight_decay': 0.0009018684977568005, 'dropout': 0.34447342909843304, 'label_smoothing': 0.07970964351725457, 'grad_clip_norm': 0.7986916124346961, 'use_scheduler': np.True_, 'channels_base': np.int64(18), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(233), 'seed': np.int64(7259)} -> 0.7504
2025-09-25 17:35:29,534 - INFO - bo.run_bo - üîçBO Trial 32: Using RF surrogate + Expected Improvement
2025-09-25 17:35:29,534 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:35:29,534 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:35:29,534 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:35:29,534 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.8972111403838395e-06, 'batch_size': 128, 'epochs': 39, 'weight_decay': 2.7090982735436847e-05, 'dropout': 0.03821467978475957, 'label_smoothing': 0.06306820244932744, 'grad_clip_norm': 1.6058202395089414, 'use_scheduler': False, 'channels_base': 32, 'se_reduction': 11, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 238, 'seed': 1947}
2025-09-25 17:35:29,536 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.8972111403838395e-06, 'batch_size': 128, 'epochs': 39, 'weight_decay': 2.7090982735436847e-05, 'dropout': 0.03821467978475957, 'label_smoothing': 0.06306820244932744, 'grad_clip_norm': 1.6058202395089414, 'use_scheduler': False, 'channels_base': 32, 'se_reduction': 11, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 238, 'seed': 1947}
2025-09-25 17:37:40,736 - INFO - _models.training_function_executor - Model: 45,367 parameters, 194.9KB storage
2025-09-25 17:37:40,737 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.0074102656306456, 1.9811500954425594, 1.9570989159493914, 1.93667017161441, 1.9158288468186226, 1.9000371480926894, 1.8874258475767747, 1.8735056184413115, 1.8600241997462819, 1.8536648161820637, 1.8396584383061112, 1.8287355308749602, 1.8248031217532175, 1.8153414125972474, 1.8087346684726204, 1.8031236593001014, 1.8012978919701137, 1.7908941136339167, 1.7871400255146128, 1.7781937741487914, 1.7765079048669261, 1.7684500316396055, 1.7667790755022252, 1.7596578213578757, 1.7603016273907335, 1.7532093724940678, 1.7512314222967535, 1.7434846751579303, 1.7474058224093372, 1.7383735866261034, 1.738089216519869, 1.734672063668076, 1.7315264709744989, 1.7259185132169859, 1.7238206002914125, 1.7246349841114483, 1.7195237846753615, 1.7151278029691193, 1.7110530978489225], 'val_losses': [1.9893874254251778, 1.9642741065950804, 1.9436966371243496, 1.9229152946151926, 1.906322503146664, 1.8918423240099047, 1.8741223575955062, 1.8630832276417788, 1.853005843508469, 1.840054669874292, 1.830330965528212, 1.8208373320638918, 1.814945021334066, 1.8073610621666105, 1.797997531549119, 1.7914448101976703, 1.7891127884261009, 1.7799350669428513, 1.7767750557286595, 1.772598486677953, 1.7663371948621698, 1.7592974050856671, 1.7545636761921897, 1.7531000328996011, 1.7461804925103397, 1.7445381306986538, 1.7408086990984502, 1.7365700405293159, 1.7286354884843214, 1.728298991757995, 1.727162915129901, 1.7257655781472392, 1.7220636916002254, 1.7180470606540172, 1.7117089070767153, 1.7083821818279212, 1.7077473548402344, 1.7072624198596678, 1.6987646399487113], 'val_acc': [0.16150858288435033, 0.20736749780729233, 0.23656183435659692, 0.23154993108633004, 0.2272898133066032, 0.2272898133066032, 0.22503445683498308, 0.22566094474376644, 0.22979576494173662, 0.2349329657937602, 0.24257611828091719, 0.2452073674978073, 0.2528505199849643, 0.253727603057261, 0.26112016038090463, 0.26274902894374136, 0.2653802781606315, 0.2698909911038717, 0.27502819195589523, 0.2702668838491417, 0.2680115273775216, 0.26512968299711814, 0.2698909911038717, 0.2676356346322516, 0.27590527502819195, 0.2666332539781982, 0.26162135070793135, 0.2696403959403583, 0.26550557574238814, 0.2713945620849518, 0.26751033705049493, 0.2767823581004887, 0.2712692645031951, 0.2662573612329282, 0.26838742012279165, 0.26751033705049493, 0.27051747901265505, 0.2740258113018419, 0.2721463475754918], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.8972111403838395e-06, 'batch_size': 128, 'epochs': 39, 'weight_decay': 2.7090982735436847e-05, 'dropout': 0.03821467978475957, 'label_smoothing': 0.06306820244932744, 'grad_clip_norm': 1.6058202395089414, 'use_scheduler': False, 'channels_base': 32, 'se_reduction': 11, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 238, 'seed': 1947}, 'model_parameter_count': 45367, 'model_storage_size_kb': 194.93632812500002, 'model_size_validation': 'PASS'}
2025-09-25 17:37:40,737 - INFO - _models.training_function_executor - BO Objective: base=0.2721, size_penalty=0.0000, final=0.2721
2025-09-25 17:37:40,737 - INFO - _models.training_function_executor - Model: 45,367 parameters, 194.9KB (PASS 256KB limit)
2025-09-25 17:37:40,737 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 131.202s
2025-09-25 17:37:40,834 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2721
2025-09-25 17:37:40,834 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.097s
2025-09-25 17:37:40,834 - INFO - bo.run_bo - Recorded observation #32: hparams={'lr': 2.8972111403838395e-06, 'batch_size': np.int64(128), 'epochs': np.int64(39), 'weight_decay': 2.7090982735436847e-05, 'dropout': 0.03821467978475957, 'label_smoothing': 0.06306820244932744, 'grad_clip_norm': 1.6058202395089414, 'use_scheduler': np.False_, 'channels_base': np.int64(32), 'se_reduction': np.int64(11), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(238), 'seed': np.int64(1947)}, value=0.2721
2025-09-25 17:37:40,834 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 32: {'lr': 2.8972111403838395e-06, 'batch_size': np.int64(128), 'epochs': np.int64(39), 'weight_decay': 2.7090982735436847e-05, 'dropout': 0.03821467978475957, 'label_smoothing': 0.06306820244932744, 'grad_clip_norm': 1.6058202395089414, 'use_scheduler': np.False_, 'channels_base': np.int64(32), 'se_reduction': np.int64(11), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(238), 'seed': np.int64(1947)} -> 0.2721
2025-09-25 17:37:40,834 - INFO - bo.run_bo - üîçBO Trial 33: Using RF surrogate + Expected Improvement
2025-09-25 17:37:40,834 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:37:40,834 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:37:40,835 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:37:40,835 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.000964595145068849, 'batch_size': 256, 'epochs': 30, 'weight_decay': 0.00027792491631611536, 'dropout': 0.11075998711983695, 'label_smoothing': 0.06992532205008459, 'grad_clip_norm': 3.6929963618256285, 'use_scheduler': True, 'channels_base': 30, 'se_reduction': 9, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 219, 'seed': 497}
2025-09-25 17:37:40,836 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.000964595145068849, 'batch_size': 256, 'epochs': 30, 'weight_decay': 0.00027792491631611536, 'dropout': 0.11075998711983695, 'label_smoothing': 0.06992532205008459, 'grad_clip_norm': 3.6929963618256285, 'use_scheduler': True, 'channels_base': 30, 'se_reduction': 9, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 219, 'seed': 497}
2025-09-25 17:39:30,873 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB storage
2025-09-25 17:39:30,873 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.7348146567319123, 1.4941492015525362, 1.3347946124481191, 1.235641554820001, 1.190557426484393, 1.1638746122158818, 1.1441056265499538, 1.1302691849088458, 1.1204862842299013, 1.105476825528473, 1.098107313734881, 1.0857286186358373, 1.0856173015778268, 1.0788754194118817, 1.073603666503115, 1.0626617211116696, 1.0641057247155532, 1.0596928214368129, 1.0524793538115764, 1.0494634517320687, 1.0469708831463451, 1.0426926289913825, 1.0371861925580106, 1.035612659167852, 1.030732685705252, 1.0286640313037396, 1.0271197466669093, 1.0275192703851366, 1.0256948319831163, 1.0232441271247263], 'val_losses': [1.5915109357235202, 1.3977116076454246, 1.2830131062886543, 1.31167923584123, 1.2152791844769357, 1.1589754602840678, 1.2116104759917254, 1.1286142044177316, 1.1221549442247287, 1.1203056425248272, 1.105446584764511, 1.084503650306804, 1.103597219329985, 1.086043370156909, 1.0872811808530556, 1.074590793335263, 1.0803431353313913, 1.0524572483236487, 1.0622344936229369, 1.0664270878614839, 1.047957585100213, 1.0380928227812138, 1.0439975131888868, 1.036168190369763, 1.0423908278452485, 1.0357077890971358, 1.0322520818018461, 1.0297881579760575, 1.0352221378899504, 1.0298312025221947], 'val_acc': [0.40659065280040096, 0.41511088835985466, 0.5045733617341185, 0.4668587896253602, 0.5186066908908658, 0.6512968299711815, 0.5009397318631751, 0.661445934093472, 0.7133191329407342, 0.61633880466107, 0.7178298458839745, 0.7387545420373387, 0.5982959528881092, 0.7935095852650044, 0.7408846009272021, 0.839493797769703, 0.7501566219771958, 0.7873700037589274, 0.8425009397318631, 0.7760932214008269, 0.853652424508207, 0.7973938102994612, 0.7537902518481393, 0.8309735622102493, 0.8748277158250846, 0.8234557073048491, 0.7915048239568976, 0.8515223656183436, 0.8332289186818694, 0.8586643277784739], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.000964595145068849, 'batch_size': 256, 'epochs': 30, 'weight_decay': 0.00027792491631611536, 'dropout': 0.11075998711983695, 'label_smoothing': 0.06992532205008459, 'grad_clip_norm': 3.6929963618256285, 'use_scheduler': True, 'channels_base': 30, 'se_reduction': 9, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 219, 'seed': 497}, 'model_parameter_count': 0, 'model_storage_size_kb': 0.0, 'model_size_validation': 'PASS'}
2025-09-25 17:39:30,873 - INFO - _models.training_function_executor - BO Objective: base=0.8587, size_penalty=0.0000, final=0.8587
2025-09-25 17:39:30,873 - INFO - _models.training_function_executor - Model: 0 parameters, 0.0KB (PASS 256KB limit)
2025-09-25 17:39:30,873 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 110.039s
2025-09-25 17:39:30,972 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8587
2025-09-25 17:39:30,972 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.099s
2025-09-25 17:39:30,972 - INFO - bo.run_bo - Recorded observation #33: hparams={'lr': 0.000964595145068849, 'batch_size': np.int64(256), 'epochs': np.int64(30), 'weight_decay': 0.00027792491631611536, 'dropout': 0.11075998711983695, 'label_smoothing': 0.06992532205008459, 'grad_clip_norm': 3.6929963618256285, 'use_scheduler': np.True_, 'channels_base': np.int64(30), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(219), 'seed': np.int64(497)}, value=0.8587
2025-09-25 17:39:30,973 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 33: {'lr': 0.000964595145068849, 'batch_size': np.int64(256), 'epochs': np.int64(30), 'weight_decay': 0.00027792491631611536, 'dropout': 0.11075998711983695, 'label_smoothing': 0.06992532205008459, 'grad_clip_norm': 3.6929963618256285, 'use_scheduler': np.True_, 'channels_base': np.int64(30), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(219), 'seed': np.int64(497)} -> 0.8587
2025-09-25 17:39:30,973 - INFO - bo.run_bo - üîçBO Trial 34: Using RF surrogate + Expected Improvement
2025-09-25 17:39:30,973 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:39:30,973 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:39:30,973 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:39:30,973 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008997934234887387, 'batch_size': 128, 'epochs': 45, 'weight_decay': 0.0001991743416923527, 'dropout': 0.3831155655067214, 'label_smoothing': 0.02602567776791494, 'grad_clip_norm': 4.233180382104758, 'use_scheduler': False, 'channels_base': 27, 'se_reduction': 9, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 132, 'seed': 1816}
2025-09-25 17:39:30,974 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008997934234887387, 'batch_size': 128, 'epochs': 45, 'weight_decay': 0.0001991743416923527, 'dropout': 0.3831155655067214, 'label_smoothing': 0.02602567776791494, 'grad_clip_norm': 4.233180382104758, 'use_scheduler': False, 'channels_base': 27, 'se_reduction': 9, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 132, 'seed': 1816}
2025-09-25 17:41:45,349 - INFO - _models.training_function_executor - Model: 33,398 parameters, 143.5KB storage
2025-09-25 17:41:45,349 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.4253403139723815, 1.1148848085367893, 0.9687701465081023, 0.8848636713414227, 0.8183408954875965, 0.779694142110238, 0.7561489773346316, 0.7348560482974011, 0.7204741335854705, 0.7068749217092627, 0.6943653114934735, 0.6845406744633462, 0.6791155503977175, 0.6705994382353612, 0.6658753043252028, 0.6647654806813647, 0.6548210369914381, 0.6498867315378635, 0.6439067789573432, 0.6435760397721819, 0.6411672595322283, 0.6385363643470311, 0.6340542992706665, 0.627245956211764, 0.6309874479052111, 0.6253109994461313, 0.6174477412598857, 0.6160054803230677, 0.6149672690076576, 0.6133625864388855, 0.6071417741868421, 0.608868748251143, 0.6061552149555728, 0.60167088796668, 0.5980506540446399, 0.5987400613118132, 0.5932615715000327, 0.5953092733033113, 0.591435764687546, 0.5946977882947188, 0.5893362351771312, 0.5891864734966683, 0.5874959386907805, 0.5852254267097083, 0.5865374608837945], 'val_losses': [1.2253412171244218, 1.027905113908256, 0.8875553489642519, 0.993720804206591, 0.7693266390917699, 0.7442449434943821, 0.733561327076245, 0.8114120613406914, 0.7199544526926931, 0.7000085232386597, 0.6523807348799936, 0.6404803592210843, 0.6734691615272563, 0.6384848672156793, 0.6286519907453189, 0.6242517826327434, 0.637024389504042, 0.6129060399486015, 0.6449651408652557, 0.6672019276041209, 0.6240290130175021, 0.6136229885817919, 0.6128413339946343, 0.6041776724528466, 0.5933163275083583, 0.6101984817601434, 0.6090068899290169, 0.6048299582616053, 0.5945159953842335, 0.5959234317731266, 0.5918927355198584, 0.5823268986572794, 0.5898260698030503, 0.6047106058641478, 0.5777478696050956, 0.582166535859418, 0.5816722083937149, 0.5725665596131617, 0.5801563565107594, 0.5918758852920083, 0.5693930570620102, 0.5848529911507283, 0.5784374676078143, 0.5837799551836188, 0.5699325775296568], 'val_acc': [0.40671595038215763, 0.553690013782734, 0.5955394060894624, 0.5437915048239569, 0.7175792507204611, 0.8109259491291818, 0.8703170028818443, 0.7060518731988472, 0.8971306853777722, 0.8486405212379401, 0.891492294198722, 0.8759553940608946, 0.8396190953514597, 0.8990101491041222, 0.8869815812554818, 0.869314622227791, 0.8641774213757675, 0.8666833730109009, 0.8641774213757675, 0.8907405087081819, 0.8975065781230422, 0.9011402079939858, 0.9270768074176169, 0.861671469740634, 0.877333667460218, 0.8585390301967172, 0.9121663951885729, 0.891116401453452, 0.9255732364365368, 0.9046485402831725, 0.9241949630372134, 0.9293321638892369, 0.9126675855155996, 0.9347199599047739, 0.9072797895000626, 0.9234431775466734, 0.9161759178047864, 0.9265756170905901, 0.915925322641273, 0.9127928830973562, 0.9205613331662699, 0.9136699661696529, 0.9280791880716702, 0.9353464478135572, 0.9245708557824834], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008997934234887387, 'batch_size': 128, 'epochs': 45, 'weight_decay': 0.0001991743416923527, 'dropout': 0.3831155655067214, 'label_smoothing': 0.02602567776791494, 'grad_clip_norm': 4.233180382104758, 'use_scheduler': False, 'channels_base': 27, 'se_reduction': 9, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 4, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 132, 'seed': 1816}, 'model_parameter_count': 33398, 'model_storage_size_kb': 143.50703125, 'model_size_validation': 'PASS'}
2025-09-25 17:41:45,349 - INFO - _models.training_function_executor - BO Objective: base=0.9246, size_penalty=0.0000, final=0.9246
2025-09-25 17:41:45,349 - INFO - _models.training_function_executor - Model: 33,398 parameters, 143.5KB (PASS 256KB limit)
2025-09-25 17:41:45,349 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 134.376s
2025-09-25 17:41:45,448 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9246
2025-09-25 17:41:45,448 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-09-25 17:41:45,448 - INFO - bo.run_bo - Recorded observation #34: hparams={'lr': 0.0008997934234887387, 'batch_size': np.int64(128), 'epochs': np.int64(45), 'weight_decay': 0.0001991743416923527, 'dropout': 0.3831155655067214, 'label_smoothing': 0.02602567776791494, 'grad_clip_norm': 4.233180382104758, 'use_scheduler': np.False_, 'channels_base': np.int64(27), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(132), 'seed': np.int64(1816)}, value=0.9246
2025-09-25 17:41:45,448 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 34: {'lr': 0.0008997934234887387, 'batch_size': np.int64(128), 'epochs': np.int64(45), 'weight_decay': 0.0001991743416923527, 'dropout': 0.3831155655067214, 'label_smoothing': 0.02602567776791494, 'grad_clip_norm': 4.233180382104758, 'use_scheduler': np.False_, 'channels_base': np.int64(27), 'se_reduction': np.int64(9), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(132), 'seed': np.int64(1816)} -> 0.9246
2025-09-25 17:41:45,448 - INFO - bo.run_bo - üîçBO Trial 35: Using RF surrogate + Expected Improvement
2025-09-25 17:41:45,448 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:41:45,448 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:41:45,448 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:41:45,448 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0009174416923501391, 'batch_size': 128, 'epochs': 49, 'weight_decay': 0.004736161957374698, 'dropout': 0.10757149798712354, 'label_smoothing': 0.0019608039634582577, 'grad_clip_norm': 3.6824641371886337, 'use_scheduler': False, 'channels_base': 32, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 159, 'seed': 6198}
2025-09-25 17:41:45,450 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0009174416923501391, 'batch_size': 128, 'epochs': 49, 'weight_decay': 0.004736161957374698, 'dropout': 0.10757149798712354, 'label_smoothing': 0.0019608039634582577, 'grad_clip_norm': 3.6824641371886337, 'use_scheduler': False, 'channels_base': 32, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 159, 'seed': 6198}
2025-09-25 17:44:30,145 - INFO - _models.training_function_executor - Model: 47,169 parameters, 202.7KB storage
2025-09-25 17:44:30,145 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0939322408979908, 0.7292182007868988, 0.5715401256463097, 0.49371744942008616, 0.4260413190033398, 0.39502984950895936, 0.36826995114009303, 0.3453722397841125, 0.3319755581346471, 0.3130647818165323, 0.3041437552752951, 0.2857350125674059, 0.2863827008300417, 0.26621447387524116, 0.26873765223509416, 0.25763920326273315, 0.2535847322366794, 0.24764338123116117, 0.2379034779968445, 0.24029255300714525, 0.2340049569355016, 0.22780019275747707, 0.22256611332825257, 0.2259383090691127, 0.2234334172254158, 0.2167177790844104, 0.21454768717481287, 0.21188850324113495, 0.20891888860344823, 0.20701602164762, 0.20209640703101814, 0.2022879487520579, 0.20095148837706783, 0.19641664386434154, 0.19403948804235652, 0.19259952440453185, 0.18779619854884508, 0.1892947301791843, 0.1819428464826238, 0.1870225488502863, 0.18725842492570258, 0.18260347326439738, 0.18064913774378388, 0.17827808061598355, 0.17035749050600205, 0.1729853646349228, 0.1687305919475709, 0.17057016742459866, 0.16930176298646996], 'val_losses': [0.8757569669646558, 0.6229554110377852, 0.5343666118034637, 0.511346311598191, 0.4202819802441135, 0.4044099953102597, 0.3580522024083326, 0.352339732065907, 0.3472945676712057, 0.31151635020078833, 0.31646683229612627, 0.3246484995485771, 0.29033993016716053, 0.31561431436218457, 0.27029596265583256, 0.2738168610768903, 0.25366387964265863, 0.27525951664126047, 0.27098800553936153, 0.3371226029159461, 0.23684521146606882, 0.24594384800027058, 0.25572554990247803, 0.27385535142786477, 0.28440602080234384, 0.2725177092736503, 0.24195395034591202, 0.23327269144575632, 0.21759784773184443, 0.2164253651117445, 0.2370345117327056, 0.22737171831114253, 0.22905068476835508, 0.22586694420272732, 0.21424702442076224, 0.20827954439960944, 0.20862455756991985, 0.21004897452748042, 0.20430028922515112, 0.22801390941497632, 0.20666671185367405, 0.21057161747430922, 0.2399029244218699, 0.20629150154835268, 0.21522474772887426, 0.19516916775999177, 0.2094167034259462, 0.22603730633509456, 0.23556363811888442], 'val_acc': [0.4569602806665831, 0.6693396817441424, 0.7100613958150608, 0.8908658062899386, 0.8319759428643028, 0.8017792256609447, 0.8505199849642902, 0.808921187821075, 0.7961408344818945, 0.9223154993108633, 0.9224407968926199, 0.8604184939230672, 0.9194336549304598, 0.8739506327527878, 0.8972559829595289, 0.8891116401453452, 0.9259491291818068, 0.930083949379777, 0.8830973562210249, 0.7729607818569102, 0.9305851397068037, 0.9261997243453202, 0.8422503445683498, 0.9563964415486781, 0.9641648916175918, 0.9033955644656058, 0.8966294950507455, 0.9486279914797644, 0.9357223405588272, 0.9295827590527502, 0.9391053752662574, 0.945746147099361, 0.9418619220649042, 0.9480015035709811, 0.9398571607567974, 0.930083949379777, 0.9340934719959905, 0.9243202606189701, 0.914045858914923, 0.9649166771081318, 0.9417366244831474, 0.9447437664453076, 0.8928705675980454, 0.9221902017291066, 0.9226913920561333, 0.9555193584763814, 0.9152988347324896, 0.9585265004385415, 0.8884851522365619], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0009174416923501391, 'batch_size': 128, 'epochs': 49, 'weight_decay': 0.004736161957374698, 'dropout': 0.10757149798712354, 'label_smoothing': 0.0019608039634582577, 'grad_clip_norm': 3.6824641371886337, 'use_scheduler': False, 'channels_base': 32, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 1, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 159, 'seed': 6198}, 'model_parameter_count': 47169, 'model_storage_size_kb': 202.679296875, 'model_size_validation': 'PASS'}
2025-09-25 17:44:30,145 - INFO - _models.training_function_executor - BO Objective: base=0.8885, size_penalty=0.0000, final=0.8885
2025-09-25 17:44:30,145 - INFO - _models.training_function_executor - Model: 47,169 parameters, 202.7KB (PASS 256KB limit)
2025-09-25 17:44:30,145 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 164.697s
2025-09-25 17:44:30,243 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8885
2025-09-25 17:44:30,243 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-09-25 17:44:30,243 - INFO - bo.run_bo - Recorded observation #35: hparams={'lr': 0.0009174416923501391, 'batch_size': np.int64(128), 'epochs': np.int64(49), 'weight_decay': 0.004736161957374698, 'dropout': 0.10757149798712354, 'label_smoothing': 0.0019608039634582577, 'grad_clip_norm': 3.6824641371886337, 'use_scheduler': np.False_, 'channels_base': np.int64(32), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(159), 'seed': np.int64(6198)}, value=0.8885
2025-09-25 17:44:30,243 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 35: {'lr': 0.0009174416923501391, 'batch_size': np.int64(128), 'epochs': np.int64(49), 'weight_decay': 0.004736161957374698, 'dropout': 0.10757149798712354, 'label_smoothing': 0.0019608039634582577, 'grad_clip_norm': 3.6824641371886337, 'use_scheduler': np.False_, 'channels_base': np.int64(32), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(159), 'seed': np.int64(6198)} -> 0.8885
2025-09-25 17:44:30,244 - INFO - bo.run_bo - üîçBO Trial 36: Using RF surrogate + Expected Improvement
2025-09-25 17:44:30,244 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:44:30,244 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:44:30,244 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:44:30,244 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008598518586340472, 'batch_size': 256, 'epochs': 50, 'weight_decay': 1.029674924141322e-06, 'dropout': 0.04898115019219253, 'label_smoothing': 0.042360227018792995, 'grad_clip_norm': 0.05674472190130176, 'use_scheduler': True, 'channels_base': 32, 'se_reduction': 4, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 129, 'seed': 8225}
2025-09-25 17:44:30,245 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008598518586340472, 'batch_size': 256, 'epochs': 50, 'weight_decay': 1.029674924141322e-06, 'dropout': 0.04898115019219253, 'label_smoothing': 0.042360227018792995, 'grad_clip_norm': 0.05674472190130176, 'use_scheduler': True, 'channels_base': 32, 'se_reduction': 4, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 129, 'seed': 8225}
2025-09-25 17:47:09,206 - INFO - _models.training_function_executor - Model: 52,573 parameters, 225.9KB storage
2025-09-25 17:47:09,206 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.49836865466396, 1.2057996910660913, 1.067246983356241, 0.9944686394819539, 0.9413741332424936, 0.9068384150329794, 0.8850719154514033, 0.871198578559452, 0.8550094829273192, 0.8470866644931042, 0.8290091701921087, 0.821813707956936, 0.8090438429053011, 0.8028089296054867, 0.796114979428695, 0.7911332251135129, 0.7822388524687709, 0.7758256555708922, 0.7731197690267676, 0.7616258170361754, 0.7637681944591413, 0.7575672115276725, 0.7554686702672347, 0.7476134162175754, 0.7439437924301333, 0.7406690888949602, 0.7418283682002745, 0.7329150690559053, 0.7341139566381896, 0.7329375425220205, 0.726340987068577, 0.7239922904540314, 0.7233802433342233, 0.7210450645762458, 0.7173220823814377, 0.7174794227524337, 0.7153414049733496, 0.7135130603600313, 0.7090861806500411, 0.712209446541189, 0.7088403879048304, 0.7085580872708169, 0.7059982030070471, 0.7048760414631396, 0.7043652339443633, 0.7045539769262082, 0.703709094701687, 0.7015186961542689, 0.702415310023546, 0.6997844331848462], 'val_losses': [1.3148600441697635, 1.134649007007631, 1.1495875452503108, 1.0966524091553769, 1.0533380599237596, 0.9218857335560364, 0.9068603695760826, 0.8707022052139227, 0.8981368737088724, 0.8566102374542508, 0.856590028096524, 0.8262051530177694, 0.9243588878343374, 0.7967373025460645, 0.7979844758540762, 0.8133448569372235, 0.7979548904116389, 0.7786090008493445, 0.8287631202021901, 0.8021467545623693, 0.7730160179689046, 0.8059844314387352, 0.7599418049034298, 0.7662122807337248, 0.7895291187966054, 0.7643524867940734, 0.773480597362081, 0.7328602688576311, 0.7664938728965593, 0.7692912449112005, 0.7381785709507527, 0.7372184022299524, 0.7358899949681653, 0.7830643342590141, 0.7294584176785635, 0.7285429241065468, 0.7336679598305581, 0.7338074560139117, 0.7288484376499399, 0.7288275115876222, 0.7375092417685075, 0.7292114868003942, 0.7253254447988872, 0.7270729630502181, 0.724607581886757, 0.7277271502478919, 0.7237937561985605, 0.7204782662273962, 0.723588312318967, 0.7292003916433439], 'val_acc': [0.44468111765442925, 0.5345194837739632, 0.4412980829469991, 0.4859040220523744, 0.5206114521989725, 0.7433905525623355, 0.7027941360731738, 0.7262247838616714, 0.7105625861420874, 0.8323518356095727, 0.7516601929582759, 0.8234557073048491, 0.623230171657687, 0.8825961658939983, 0.9013908031574991, 0.8958777095602055, 0.8526500438541537, 0.8936223530885854, 0.9229419872196467, 0.84826462849267, 0.8950006264879088, 0.908658062899386, 0.9043979451196592, 0.9127928830973562, 0.9091592532264128, 0.9317128179426137, 0.901014910412229, 0.9180553815311364, 0.8897381280541286, 0.9627866182182684, 0.9179300839493798, 0.9219396065655933, 0.9134193710061396, 0.9573988222027315, 0.9061521112642525, 0.9397318631750408, 0.914045858914923, 0.940358351083824, 0.9353464478135572, 0.9354717453953139, 0.9408595414108508, 0.9443678737000376, 0.9330910913419371, 0.9259491291818068, 0.9468738253351711, 0.9378523994486906, 0.9436160882094976, 0.9372259115399073, 0.9248214509459968, 0.9426137075554442], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008598518586340472, 'batch_size': 256, 'epochs': 50, 'weight_decay': 1.029674924141322e-06, 'dropout': 0.04898115019219253, 'label_smoothing': 0.042360227018792995, 'grad_clip_norm': 0.05674472190130176, 'use_scheduler': True, 'channels_base': 32, 'se_reduction': 4, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 129, 'seed': 8225}, 'model_parameter_count': 52573, 'model_storage_size_kb': 225.899609375, 'model_size_validation': 'PASS'}
2025-09-25 17:47:09,206 - INFO - _models.training_function_executor - BO Objective: base=0.9426, size_penalty=0.0000, final=0.9426
2025-09-25 17:47:09,206 - INFO - _models.training_function_executor - Model: 52,573 parameters, 225.9KB (PASS 256KB limit)
2025-09-25 17:47:09,206 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 158.963s
2025-09-25 17:47:09,306 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9426
2025-09-25 17:47:09,306 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-25 17:47:09,306 - INFO - bo.run_bo - Recorded observation #36: hparams={'lr': 0.0008598518586340472, 'batch_size': np.int64(256), 'epochs': np.int64(50), 'weight_decay': 1.029674924141322e-06, 'dropout': 0.04898115019219253, 'label_smoothing': 0.042360227018792995, 'grad_clip_norm': 0.05674472190130176, 'use_scheduler': np.True_, 'channels_base': np.int64(32), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(129), 'seed': np.int64(8225)}, value=0.9426
2025-09-25 17:47:09,306 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 36: {'lr': 0.0008598518586340472, 'batch_size': np.int64(256), 'epochs': np.int64(50), 'weight_decay': 1.029674924141322e-06, 'dropout': 0.04898115019219253, 'label_smoothing': 0.042360227018792995, 'grad_clip_norm': 0.05674472190130176, 'use_scheduler': np.True_, 'channels_base': np.int64(32), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(129), 'seed': np.int64(8225)} -> 0.9426
2025-09-25 17:47:09,307 - INFO - bo.run_bo - üîçBO Trial 37: Using RF surrogate + Expected Improvement
2025-09-25 17:47:09,307 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:47:09,307 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:47:09,307 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:47:09,307 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008558398871419747, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': False, 'channels_base': 27, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 133, 'seed': 9542}
2025-09-25 17:47:09,308 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008558398871419747, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': False, 'channels_base': 27, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 133, 'seed': 9542}
2025-09-25 17:47:49,614 - INFO - _models.training_function_executor - Model: 34,158 parameters, 146.8KB storage
2025-09-25 17:47:49,614 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1032682941961687, 0.7210166662893166, 0.5369279849268987, 0.4459451559490045, 0.38440111546021555, 0.35206942432368793, 0.3302546619091835, 0.3077977648238223, 0.28730699770670465, 0.28306966883617024, 0.2732223775421595, 0.2650218590320555], 'val_losses': [0.8466957598054577, 0.5664575102857712, 0.4696988658436915, 0.3662151722602715, 0.331983366267691, 0.3342592346311258, 0.30086225746956824, 0.28178301066731526, 0.30962869980896557, 0.24598230277217878, 0.2570044611154859, 0.3044774211889938], 'val_acc': [0.61445934093472, 0.7779726851271771, 0.8500187946372635, 0.8497681994737502, 0.829971181556196, 0.9215637138203233, 0.8810925949129181, 0.9240696654554567, 0.92206490414735, 0.932339305851397, 0.9205613331662699, 0.9517604310236812], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008558398871419747, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': False, 'channels_base': 27, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 133, 'seed': 9542}, 'model_parameter_count': 34158, 'model_storage_size_kb': 146.77265625, 'model_size_validation': 'PASS'}
2025-09-25 17:47:49,614 - INFO - _models.training_function_executor - BO Objective: base=0.9518, size_penalty=0.0000, final=0.9518
2025-09-25 17:47:49,614 - INFO - _models.training_function_executor - Model: 34,158 parameters, 146.8KB (PASS 256KB limit)
2025-09-25 17:47:49,614 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 40.307s
2025-09-25 17:47:49,714 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9518
2025-09-25 17:47:49,714 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-25 17:47:49,714 - INFO - bo.run_bo - Recorded observation #37: hparams={'lr': 0.0008558398871419747, 'batch_size': np.int64(64), 'epochs': np.int64(12), 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': np.False_, 'channels_base': np.int64(27), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(133), 'seed': np.int64(9542)}, value=0.9518
2025-09-25 17:47:49,714 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 37: {'lr': 0.0008558398871419747, 'batch_size': np.int64(64), 'epochs': np.int64(12), 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': np.False_, 'channels_base': np.int64(27), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(133), 'seed': np.int64(9542)} -> 0.9518
2025-09-25 17:47:49,714 - INFO - bo.run_bo - üîçBO Trial 38: Using RF surrogate + Expected Improvement
2025-09-25 17:47:49,714 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:47:49,714 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:47:49,715 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:47:49,715 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008575905729525521, 'batch_size': 128, 'epochs': 13, 'weight_decay': 3.0228848008241565e-06, 'dropout': 0.39335578223154555, 'label_smoothing': 0.013339352935691643, 'grad_clip_norm': 2.02586117242237, 'use_scheduler': True, 'channels_base': 28, 'se_reduction': 8, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 162, 'seed': 1281}
2025-09-25 17:47:49,716 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008575905729525521, 'batch_size': 128, 'epochs': 13, 'weight_decay': 3.0228848008241565e-06, 'dropout': 0.39335578223154555, 'label_smoothing': 0.013339352935691643, 'grad_clip_norm': 2.02586117242237, 'use_scheduler': True, 'channels_base': 28, 'se_reduction': 8, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 162, 'seed': 1281}
2025-09-25 17:48:29,660 - INFO - _models.training_function_executor - Model: 36,289 parameters, 155.9KB storage
2025-09-25 17:48:29,661 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.3002274857612084, 0.9974174496972411, 0.8116984866344551, 0.7076797113342997, 0.6423597103879554, 0.6040276279762544, 0.5758720459531163, 0.5532629687292506, 0.5371112287580394, 0.5249160960350733, 0.5152186791303742, 0.5060318319868392, 0.5032151880425005], 'val_losses': [1.0989588763372982, 0.8418139149679336, 0.7101372993988675, 0.6854254027279886, 0.5645666961053935, 0.5766524529340586, 0.5236231541143684, 0.5470082138343766, 0.5027299979294382, 0.4988482856777374, 0.4891267289136708, 0.4857334880628109, 0.4905830210364413], 'val_acc': [0.5287557950131563, 0.6413983210124045, 0.7624357849893497, 0.8480140333291567, 0.8466357599298333, 0.8833479513845383, 0.8641774213757675, 0.8808419997494048, 0.907906277408846, 0.8661821826838741, 0.8728229545169778, 0.9000125297581757, 0.900263124921689], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008575905729525521, 'batch_size': 128, 'epochs': 13, 'weight_decay': 3.0228848008241565e-06, 'dropout': 0.39335578223154555, 'label_smoothing': 0.013339352935691643, 'grad_clip_norm': 2.02586117242237, 'use_scheduler': True, 'channels_base': 28, 'se_reduction': 8, 'dilation_b1': 4, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 162, 'seed': 1281}, 'model_parameter_count': 36289, 'model_storage_size_kb': 155.929296875, 'model_size_validation': 'PASS'}
2025-09-25 17:48:29,661 - INFO - _models.training_function_executor - BO Objective: base=0.9003, size_penalty=0.0000, final=0.9003
2025-09-25 17:48:29,661 - INFO - _models.training_function_executor - Model: 36,289 parameters, 155.9KB (PASS 256KB limit)
2025-09-25 17:48:29,661 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 39.946s
2025-09-25 17:48:29,761 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9003
2025-09-25 17:48:29,761 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.101s
2025-09-25 17:48:29,761 - INFO - bo.run_bo - Recorded observation #38: hparams={'lr': 0.0008575905729525521, 'batch_size': np.int64(128), 'epochs': np.int64(13), 'weight_decay': 3.0228848008241565e-06, 'dropout': 0.39335578223154555, 'label_smoothing': 0.013339352935691643, 'grad_clip_norm': 2.02586117242237, 'use_scheduler': np.True_, 'channels_base': np.int64(28), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(162), 'seed': np.int64(1281)}, value=0.9003
2025-09-25 17:48:29,761 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 38: {'lr': 0.0008575905729525521, 'batch_size': np.int64(128), 'epochs': np.int64(13), 'weight_decay': 3.0228848008241565e-06, 'dropout': 0.39335578223154555, 'label_smoothing': 0.013339352935691643, 'grad_clip_norm': 2.02586117242237, 'use_scheduler': np.True_, 'channels_base': np.int64(28), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(162), 'seed': np.int64(1281)} -> 0.9003
2025-09-25 17:48:29,762 - INFO - bo.run_bo - üîçBO Trial 39: Using RF surrogate + Expected Improvement
2025-09-25 17:48:29,762 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:48:29,762 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:48:29,762 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:48:29,762 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008493280907096744, 'batch_size': 128, 'epochs': 45, 'weight_decay': 1.606958635649543e-06, 'dropout': 0.49647891761579416, 'label_smoothing': 0.06032036955097427, 'grad_clip_norm': 3.6167549524326037, 'use_scheduler': False, 'channels_base': 25, 'se_reduction': 11, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 22, 'seed': 1363}
2025-09-25 17:48:29,763 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008493280907096744, 'batch_size': 128, 'epochs': 45, 'weight_decay': 1.606958635649543e-06, 'dropout': 0.49647891761579416, 'label_smoothing': 0.06032036955097427, 'grad_clip_norm': 3.6167549524326037, 'use_scheduler': False, 'channels_base': 25, 'se_reduction': 11, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 22, 'seed': 1363}
2025-09-25 17:50:36,625 - INFO - _models.training_function_executor - Model: 28,120 parameters, 120.8KB storage
2025-09-25 17:50:36,625 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.720982709904271, 1.4567493758725742, 1.2885607001857866, 1.2138212593083573, 1.171786486432206, 1.1393159937261845, 1.1104491628755233, 1.0990326684940968, 1.0829112963602812, 1.0663185084993498, 1.0566032242066197, 1.04701356994543, 1.0311345295384313, 1.0273502375313808, 1.0215282747516454, 1.0227245873005002, 1.0170084662751566, 1.0132044647300325, 1.003681908882983, 1.005924358421096, 0.9998960581817032, 1.0011624387323836, 0.9960947696365658, 0.9954626972400555, 0.98765520821634, 0.9885567802760444, 0.984592505707962, 0.977965554740537, 0.9785765643627374, 0.9750513278634776, 0.9773877809001618, 0.9774978615244846, 0.9755566112567124, 0.9727550941283917, 0.9669151192518328, 0.9690042128782652, 0.96317936589693, 0.9639799647084327, 0.9641914668957161, 0.9586281119569092, 0.9606410515128014, 0.9562773153760457, 0.9600715775748667, 0.9562012241328304, 0.9642571147919986], 'val_losses': [1.517126173680923, 1.2719403612869093, 1.225174278472692, 1.1176980296974866, 1.0926217708114454, 1.1141679203311627, 1.0492592118405082, 1.0408259989130604, 1.0194369249065454, 1.0141467992531419, 1.0045228481232977, 1.001797856281999, 0.9922610222135715, 0.9832575478032185, 0.9795613482010094, 0.9769458727883809, 0.9945414959226779, 1.011882794320083, 0.9721586233750126, 0.9650344009791352, 0.9601093834914176, 0.9637975176240161, 0.9502678592018937, 0.9601566702213342, 0.9617148813575809, 0.9523913364574106, 0.9596965935815472, 0.9629724901311904, 0.9382953167170783, 0.9447296352228743, 0.9456898574317671, 0.9350924676499142, 0.9392453818195163, 0.9360181139967612, 0.9434540971092317, 0.9375062518633035, 0.9333157065876324, 0.9343907513156628, 0.9327443191512548, 0.9296849216379448, 0.9306030010552712, 0.9331462766843068, 0.9243940284804524, 0.9293956206176771, 0.9264492563788507], 'val_acc': [0.31549931086330035, 0.4657311113895502, 0.4009522616213507, 0.5739882220273149, 0.6418995113394311, 0.5509334669840872, 0.7006640771833104, 0.7288560330785616, 0.7557950131562461, 0.7486530509961158, 0.761934594662323, 0.8188196967798521, 0.8156872572359354, 0.8195714822703922, 0.8725723593534644, 0.869314622227791, 0.8134319007643153, 0.8007768450068914, 0.8992607442676356, 0.8596667084325272, 0.8725723593534644, 0.8570354592156372, 0.861671469740634, 0.8731988472622478, 0.9061521112642525, 0.8893622353088585, 0.8625485528129307, 0.8886104498183185, 0.8975065781230422, 0.9085327653176294, 0.9003884225034456, 0.9057762185189826, 0.8902393183811552, 0.9095351459716827, 0.9030196717203358, 0.9047738378649292, 0.8942488409973688, 0.9208119283297832, 0.8844756296203483, 0.9160506202230297, 0.9317128179426137, 0.9066533015912793, 0.9060268136824959, 0.9084074677358727, 0.9050244330284426], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008493280907096744, 'batch_size': 128, 'epochs': 45, 'weight_decay': 1.606958635649543e-06, 'dropout': 0.49647891761579416, 'label_smoothing': 0.06032036955097427, 'grad_clip_norm': 3.6167549524326037, 'use_scheduler': False, 'channels_base': 25, 'se_reduction': 11, 'dilation_b1': 2, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 22, 'seed': 1363}, 'model_parameter_count': 28120, 'model_storage_size_kb': 120.82812500000001, 'model_size_validation': 'PASS'}
2025-09-25 17:50:36,625 - INFO - _models.training_function_executor - BO Objective: base=0.9050, size_penalty=0.0000, final=0.9050
2025-09-25 17:50:36,625 - INFO - _models.training_function_executor - Model: 28,120 parameters, 120.8KB (PASS 256KB limit)
2025-09-25 17:50:36,625 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 126.863s
2025-09-25 17:50:36,725 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9050
2025-09-25 17:50:36,725 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-25 17:50:36,725 - INFO - bo.run_bo - Recorded observation #39: hparams={'lr': 0.0008493280907096744, 'batch_size': np.int64(128), 'epochs': np.int64(45), 'weight_decay': 1.606958635649543e-06, 'dropout': 0.49647891761579416, 'label_smoothing': 0.06032036955097427, 'grad_clip_norm': 3.6167549524326037, 'use_scheduler': np.False_, 'channels_base': np.int64(25), 'se_reduction': np.int64(11), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(22), 'seed': np.int64(1363)}, value=0.9050
2025-09-25 17:50:36,725 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 39: {'lr': 0.0008493280907096744, 'batch_size': np.int64(128), 'epochs': np.int64(45), 'weight_decay': 1.606958635649543e-06, 'dropout': 0.49647891761579416, 'label_smoothing': 0.06032036955097427, 'grad_clip_norm': 3.6167549524326037, 'use_scheduler': np.False_, 'channels_base': np.int64(25), 'se_reduction': np.int64(11), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(22), 'seed': np.int64(1363)} -> 0.9050
2025-09-25 17:50:36,726 - INFO - bo.run_bo - üîçBO Trial 40: Using RF surrogate + Expected Improvement
2025-09-25 17:50:36,726 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:50:36,726 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:50:36,726 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:50:36,726 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008415781060132975, 'batch_size': 64, 'epochs': 25, 'weight_decay': 1.4343196111969493e-05, 'dropout': 0.34848941631205715, 'label_smoothing': 0.013562349103431107, 'grad_clip_norm': 0.08087825624193302, 'use_scheduler': True, 'channels_base': 32, 'se_reduction': 4, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 245, 'seed': 6330}
2025-09-25 17:50:36,727 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008415781060132975, 'batch_size': 64, 'epochs': 25, 'weight_decay': 1.4343196111969493e-05, 'dropout': 0.34848941631205715, 'label_smoothing': 0.013562349103431107, 'grad_clip_norm': 0.08087825624193302, 'use_scheduler': True, 'channels_base': 32, 'se_reduction': 4, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 245, 'seed': 6330}
2025-09-25 17:52:14,082 - INFO - _models.training_function_executor - Model: 52,573 parameters, 225.9KB storage
2025-09-25 17:52:14,082 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.208184179187818, 0.871719190387862, 0.7149711061520292, 0.6454091658643907, 0.6054195597036378, 0.5781113215060856, 0.5511675863432092, 0.5393013950428321, 0.5217017652460183, 0.5072671073580118, 0.49731214390479767, 0.48466481782662385, 0.48031545629762146, 0.4722077763103376, 0.4683261585445462, 0.4660549874451063, 0.45858522821958214, 0.44566842974309606, 0.4531590626578699, 0.4445767128571746, 0.43778014181177294, 0.43849352972215383, 0.43033889162713335, 0.4282186053999272, 0.4308181725804401], 'val_losses': [1.054973722295853, 0.7629411610919511, 0.6982125487263049, 0.5969010668430632, 0.6434565691881619, 0.6386264313837143, 0.530550889473872, 0.5115757937823273, 0.4808530277319233, 0.4974987097303904, 0.47703163601554227, 0.46201281222823926, 0.47166085680121694, 0.4478771835994995, 0.44790786165683855, 0.43260901466585194, 0.43478703534688257, 0.43554207821893687, 0.42699017815684903, 0.42420970482450426, 0.4208383973477374, 0.42127510027365045, 0.41837262860845315, 0.42389127678955396, 0.42082213477941405], 'val_acc': [0.43979451196591907, 0.6857536649542664, 0.7743390552562336, 0.8854780102744017, 0.77032953264002, 0.8503946873825335, 0.9180553815311364, 0.8485152236561835, 0.9332163889236937, 0.9467485277534143, 0.9190577621851899, 0.9411101365743642, 0.9013908031574991, 0.9426137075554442, 0.9433654930459842, 0.9383535897757174, 0.9387294825209873, 0.9452449567723343, 0.9446184688635509, 0.9350958526500438, 0.9522616213507079, 0.9359729357223405, 0.9350958526500438, 0.9592782859290816, 0.9468738253351711], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008415781060132975, 'batch_size': 64, 'epochs': 25, 'weight_decay': 1.4343196111969493e-05, 'dropout': 0.34848941631205715, 'label_smoothing': 0.013562349103431107, 'grad_clip_norm': 0.08087825624193302, 'use_scheduler': True, 'channels_base': 32, 'se_reduction': 4, 'dilation_b1': 1, 'dilation_b2': 2, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 245, 'seed': 6330}, 'model_parameter_count': 52573, 'model_storage_size_kb': 225.899609375, 'model_size_validation': 'PASS'}
2025-09-25 17:52:14,083 - INFO - _models.training_function_executor - BO Objective: base=0.9469, size_penalty=0.0000, final=0.9469
2025-09-25 17:52:14,083 - INFO - _models.training_function_executor - Model: 52,573 parameters, 225.9KB (PASS 256KB limit)
2025-09-25 17:52:14,083 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 97.357s
2025-09-25 17:52:14,182 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9469
2025-09-25 17:52:14,182 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-25 17:52:14,182 - INFO - bo.run_bo - Recorded observation #40: hparams={'lr': 0.0008415781060132975, 'batch_size': np.int64(64), 'epochs': np.int64(25), 'weight_decay': 1.4343196111969493e-05, 'dropout': 0.34848941631205715, 'label_smoothing': 0.013562349103431107, 'grad_clip_norm': 0.08087825624193302, 'use_scheduler': np.True_, 'channels_base': np.int64(32), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(245), 'seed': np.int64(6330)}, value=0.9469
2025-09-25 17:52:14,182 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 40: {'lr': 0.0008415781060132975, 'batch_size': np.int64(64), 'epochs': np.int64(25), 'weight_decay': 1.4343196111969493e-05, 'dropout': 0.34848941631205715, 'label_smoothing': 0.013562349103431107, 'grad_clip_norm': 0.08087825624193302, 'use_scheduler': np.True_, 'channels_base': np.int64(32), 'se_reduction': np.int64(4), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(245), 'seed': np.int64(6330)} -> 0.9469
2025-09-25 17:52:14,183 - INFO - bo.run_bo - üîçBO Trial 41: Using RF surrogate + Expected Improvement
2025-09-25 17:52:14,183 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:52:14,183 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:52:14,183 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:52:14,183 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008414105611781741, 'batch_size': 512, 'epochs': 10, 'weight_decay': 2.5713011489483383e-06, 'dropout': 0.07006936730689507, 'label_smoothing': 0.0569549877795004, 'grad_clip_norm': 3.9675670903427105, 'use_scheduler': False, 'channels_base': 30, 'se_reduction': 10, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 4, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 72, 'seed': 9896}
2025-09-25 17:52:14,184 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008414105611781741, 'batch_size': 512, 'epochs': 10, 'weight_decay': 2.5713011489483383e-06, 'dropout': 0.07006936730689507, 'label_smoothing': 0.0569549877795004, 'grad_clip_norm': 3.9675670903427105, 'use_scheduler': False, 'channels_base': 30, 'se_reduction': 10, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 4, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 72, 'seed': 9896}
2025-09-25 17:52:44,836 - INFO - _models.training_function_executor - Model: 40,616 parameters, 174.5KB storage
2025-09-25 17:52:44,836 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6806146187303255, 1.4206283665224675, 1.22697534355548, 1.1241801999787304, 1.061915739386302, 1.0349438877118857, 1.01227440753161, 1.005155288119687, 0.9870841528237645, 0.9794184083169039], 'val_losses': [1.579479298291686, 1.3575624406614066, 1.2314591168042877, 1.1673283873308185, 1.0992547667514947, 1.0423889100110975, 1.1378166463168666, 1.0189212164534107, 0.9831190161979254, 1.0026792644961497], 'val_acc': [0.3357975191078812, 0.3814058388673099, 0.5293822829219396, 0.6016789875955394, 0.5771206615712317, 0.782608695652174, 0.38541536148352334, 0.8334795138453828, 0.8493923067284801, 0.7075554441799273], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008414105611781741, 'batch_size': 512, 'epochs': 10, 'weight_decay': 2.5713011489483383e-06, 'dropout': 0.07006936730689507, 'label_smoothing': 0.0569549877795004, 'grad_clip_norm': 3.9675670903427105, 'use_scheduler': False, 'channels_base': 30, 'se_reduction': 10, 'dilation_b1': 4, 'dilation_b2': 4, 'dilation_b3': 4, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 72, 'seed': 9896}, 'model_parameter_count': 40616, 'model_storage_size_kb': 174.52187500000002, 'model_size_validation': 'PASS'}
2025-09-25 17:52:44,836 - INFO - _models.training_function_executor - BO Objective: base=0.7076, size_penalty=0.0000, final=0.7076
2025-09-25 17:52:44,837 - INFO - _models.training_function_executor - Model: 40,616 parameters, 174.5KB (PASS 256KB limit)
2025-09-25 17:52:44,837 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 30.654s
2025-09-25 17:52:44,936 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7076
2025-09-25 17:52:44,937 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-09-25 17:52:44,937 - INFO - bo.run_bo - Recorded observation #41: hparams={'lr': 0.0008414105611781741, 'batch_size': np.int64(512), 'epochs': np.int64(10), 'weight_decay': 2.5713011489483383e-06, 'dropout': 0.07006936730689507, 'label_smoothing': 0.0569549877795004, 'grad_clip_norm': 3.9675670903427105, 'use_scheduler': np.False_, 'channels_base': np.int64(30), 'se_reduction': np.int64(10), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(72), 'seed': np.int64(9896)}, value=0.7076
2025-09-25 17:52:44,937 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 41: {'lr': 0.0008414105611781741, 'batch_size': np.int64(512), 'epochs': np.int64(10), 'weight_decay': 2.5713011489483383e-06, 'dropout': 0.07006936730689507, 'label_smoothing': 0.0569549877795004, 'grad_clip_norm': 3.9675670903427105, 'use_scheduler': np.False_, 'channels_base': np.int64(30), 'se_reduction': np.int64(10), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(72), 'seed': np.int64(9896)} -> 0.7076
2025-09-25 17:52:44,937 - INFO - bo.run_bo - üîçBO Trial 42: Using RF surrogate + Expected Improvement
2025-09-25 17:52:44,937 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:52:44,937 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:52:44,937 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:52:44,937 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00443117805063839, 'batch_size': 256, 'epochs': 46, 'weight_decay': 0.000640997446172822, 'dropout': 0.32527137925476496, 'label_smoothing': 0.07978187687417682, 'grad_clip_norm': 0.8780812840982772, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 5, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 171, 'seed': 8144}
2025-09-25 17:52:44,938 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00443117805063839, 'batch_size': 256, 'epochs': 46, 'weight_decay': 0.000640997446172822, 'dropout': 0.32527137925476496, 'label_smoothing': 0.07978187687417682, 'grad_clip_norm': 0.8780812840982772, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 5, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 171, 'seed': 8144}
2025-09-25 17:54:41,530 - INFO - _models.training_function_executor - Model: 28,573 parameters, 122.8KB storage
2025-09-25 17:54:41,531 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6994213165355738, 1.441131142100434, 1.300466000886279, 1.2567517387573086, 1.2343408151724933, 1.207202233060673, 1.1948162104123532, 1.18375680538712, 1.1733743989990046, 1.1604806187905468, 1.1549878373008786, 1.1486250478828572, 1.1475566218073563, 1.1404593741502043, 1.1314359117980006, 1.1306961471976273, 1.1284172107629435, 1.1181069959753367, 1.1166659857632801, 1.1131667565930492, 1.1096497710641455, 1.1100806150431024, 1.1030249477825684, 1.1060693514913567, 1.1059180281871253, 1.1004014297546354, 1.1005555013589639, 1.0984062120825808, 1.0934244233093677, 1.0951427275370533, 1.0919406081756113, 1.0914815167856744, 1.0856262067989222, 1.0836430690948988, 1.079215203009538, 1.0829456201887397, 1.080885687720726, 1.0792042230172842, 1.079105924690658, 1.0805243433289213, 1.0757297287943877, 1.0752399128895596, 1.0779456116593549, 1.071952355158933, 1.070817539212668, 1.073302632778826], 'val_losses': [1.513479554642712, 1.345345799299488, 1.2447084780997641, 1.2865235616712398, 1.2270522614407249, 1.205777517597025, 1.1792403475829882, 1.1715504022009644, 1.1815220559842754, 1.1488463796629462, 1.1552520212471717, 1.1910424388869485, 1.1363893834709031, 1.1198147531440572, 1.1447956534558827, 1.1206465907718823, 1.1319467561344443, 1.1199107626568567, 1.1126771933332509, 1.0990473942086894, 1.095746364917332, 1.0964930683968137, 1.1029234907857428, 1.0958300228630327, 1.1052227444762859, 1.0931627909173165, 1.100719678104708, 1.1001078627220358, 1.0885796815795479, 1.0884267758671884, 1.0940709039092795, 1.0890782729618353, 1.087531703484266, 1.083882483828413, 1.083047298183568, 1.0889912109120479, 1.0838814283011775, 1.0835677407436421, 1.0831061448538277, 1.0830338263986643, 1.0824907496375498, 1.0799810364497004, 1.0826295752555801, 1.082532273529616, 1.0822528666712679, 1.0855540109597834], 'val_acc': [0.3677484024558326, 0.7267259741886981, 0.6749780729231926, 0.5762435784989349, 0.7203357975191079, 0.7784738754542038, 0.752787871194086, 0.8755795013156246, 0.7802280415987971, 0.8666833730109009, 0.7912542287933843, 0.6858789625360231, 0.8507705801278035, 0.8131813056008019, 0.821200350833229, 0.8149354717453953, 0.7966420248089212, 0.8599173035960406, 0.8450068913669966, 0.8748277158250846, 0.8673098609196842, 0.8749530134068413, 0.9097857411351961, 0.8820949755669716, 0.8800902142588648, 0.8422503445683498, 0.8744518230798145, 0.884600927202105, 0.8827214634757549, 0.9089086580628993, 0.8852274151108883, 0.8729482520987345, 0.8805914045858915, 0.9023931838115524, 0.9087833604811427, 0.8790878336048115, 0.8981330660318256, 0.9265756170905901, 0.901766695902769, 0.8858539030196717, 0.899135446685879, 0.8946247337426387, 0.8892369377271019, 0.9042726475379025, 0.9077809798270894, 0.9007643152487157], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00443117805063839, 'batch_size': 256, 'epochs': 46, 'weight_decay': 0.000640997446172822, 'dropout': 0.32527137925476496, 'label_smoothing': 0.07978187687417682, 'grad_clip_norm': 0.8780812840982772, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 5, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 171, 'seed': 8144}, 'model_parameter_count': 28573, 'model_storage_size_kb': 122.77460937500001, 'model_size_validation': 'PASS'}
2025-09-25 17:54:41,531 - INFO - _models.training_function_executor - BO Objective: base=0.9008, size_penalty=0.0000, final=0.9008
2025-09-25 17:54:41,531 - INFO - _models.training_function_executor - Model: 28,573 parameters, 122.8KB (PASS 256KB limit)
2025-09-25 17:54:41,531 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 116.593s
2025-09-25 17:54:41,633 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9008
2025-09-25 17:54:41,633 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.102s
2025-09-25 17:54:41,633 - INFO - bo.run_bo - Recorded observation #42: hparams={'lr': 0.00443117805063839, 'batch_size': np.int64(256), 'epochs': np.int64(46), 'weight_decay': 0.000640997446172822, 'dropout': 0.32527137925476496, 'label_smoothing': 0.07978187687417682, 'grad_clip_norm': 0.8780812840982772, 'use_scheduler': np.True_, 'channels_base': np.int64(24), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(171), 'seed': np.int64(8144)}, value=0.9008
2025-09-25 17:54:41,633 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 42: {'lr': 0.00443117805063839, 'batch_size': np.int64(256), 'epochs': np.int64(46), 'weight_decay': 0.000640997446172822, 'dropout': 0.32527137925476496, 'label_smoothing': 0.07978187687417682, 'grad_clip_norm': 0.8780812840982772, 'use_scheduler': np.True_, 'channels_base': np.int64(24), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(171), 'seed': np.int64(8144)} -> 0.9008
2025-09-25 17:54:41,634 - INFO - bo.run_bo - üîçBO Trial 43: Using RF surrogate + Expected Improvement
2025-09-25 17:54:41,634 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:54:41,634 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:54:41,634 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:54:41,634 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004082531240263393, 'batch_size': 256, 'epochs': 48, 'weight_decay': 3.217722332394048e-05, 'dropout': 0.13242046610680394, 'label_smoothing': 0.09348818735038253, 'grad_clip_norm': 1.1198526810563294, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 169, 'seed': 6682}
2025-09-25 17:54:41,635 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004082531240263393, 'batch_size': 256, 'epochs': 48, 'weight_decay': 3.217722332394048e-05, 'dropout': 0.13242046610680394, 'label_smoothing': 0.09348818735038253, 'grad_clip_norm': 1.1198526810563294, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 169, 'seed': 6682}
2025-09-25 17:56:43,242 - INFO - _models.training_function_executor - Model: 27,075 parameters, 116.3KB storage
2025-09-25 17:56:43,242 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.78213402078373, 1.4767663499091093, 1.3811247062291754, 1.3434935784394426, 1.307881694735058, 1.2982141322309584, 1.2807106723000605, 1.2767114051096429, 1.2596955055155798, 1.2526580224833168, 1.2485276747850742, 1.2474732522554006, 1.2312709706374003, 1.2290882039920799, 1.231609979215968, 1.2210891467715417, 1.2237966732994803, 1.218830166516386, 1.2113493119057486, 1.212823437691422, 1.2076741204511081, 1.2042782092809161, 1.2071733629405152, 1.199779655948093, 1.201927115518908, 1.1953465186260892, 1.1955444815538023, 1.1902602773536788, 1.1870715299674497, 1.1893361449536646, 1.1818003693241765, 1.1879954272649216, 1.1799212049333327, 1.1800322980916287, 1.1762345877670015, 1.178276273185021, 1.1745806180888974, 1.1747063123145072, 1.169622945803447, 1.1673537352380645, 1.1726970041701508, 1.166211321494844, 1.1669128178855595, 1.1645077294963975, 1.1651780168816652, 1.1656981870311247, 1.1673759123184357, 1.165375610926402], 'val_losses': [1.6285256417887115, 1.3923752738365687, 1.5428562890824602, 1.3668912491722522, 1.3157492402113407, 1.2852271759873, 1.272249567883607, 1.2604580373486813, 1.2519836835941267, 1.2632650263460936, 1.2766963744907358, 1.2361457779837377, 1.2656519628380671, 1.2494373876578586, 1.2172393077388621, 1.2193577408626286, 1.237476242199501, 1.2230175268887546, 1.205585061077366, 1.2272432638639617, 1.212339250534463, 1.2105961987404965, 1.2021702472584959, 1.2095570808932112, 1.2194967377142505, 1.204646218780467, 1.2037587411092079, 1.1988938837597476, 1.1975053243059934, 1.1985545787816059, 1.1887537344194208, 1.2167247391153346, 1.1961553319327112, 1.1907864794248986, 1.1866210708192768, 1.1869261295570495, 1.1864319908157386, 1.1797212869537577, 1.1825034884548533, 1.1808898347225603, 1.1801701254779542, 1.1822042678355753, 1.1797185188068093, 1.1798148603012741, 1.1863322001981669, 1.1805054814934117, 1.179385671727924, 1.1804569189656933], 'val_acc': [0.2726475379025185, 0.353840370880842, 0.3113644906653302, 0.5113394311489788, 0.492920686630748, 0.653050996115775, 0.7030447312366871, 0.6531762936975316, 0.7383786492920686, 0.8695652173913043, 0.7032953264002004, 0.7413857912542288, 0.753915549429896, 0.7710813181305601, 0.8095476757298584, 0.8919934845257487, 0.8234557073048491, 0.8690640270642777, 0.8800902142588648, 0.8694399198095477, 0.7965167272271645, 0.8646786117027941, 0.8477634381656434, 0.9127928830973562, 0.7571732865555695, 0.9152988347324896, 0.8957524119784488, 0.9204360355845133, 0.8764565843879213, 0.9353464478135572, 0.8951259240696654, 0.8288435033203859, 0.9285803783986969, 0.8661821826838741, 0.9048991354466859, 0.9023931838115524, 0.9347199599047739, 0.9173035960405964, 0.9382282921939606, 0.9210625234932965, 0.9219396065655933, 0.9145470492419496, 0.9226913920561333, 0.9229419872196467, 0.8977571732865556, 0.9051497306101992, 0.9097857411351961, 0.9408595414108508], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004082531240263393, 'batch_size': 256, 'epochs': 48, 'weight_decay': 3.217722332394048e-05, 'dropout': 0.13242046610680394, 'label_smoothing': 0.09348818735038253, 'grad_clip_norm': 1.1198526810563294, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_steps': 169, 'seed': 6682}, 'model_parameter_count': 27075, 'model_storage_size_kb': 116.33789062500001, 'model_size_validation': 'PASS'}
2025-09-25 17:56:43,243 - INFO - _models.training_function_executor - BO Objective: base=0.9409, size_penalty=0.0000, final=0.9409
2025-09-25 17:56:43,243 - INFO - _models.training_function_executor - Model: 27,075 parameters, 116.3KB (PASS 256KB limit)
2025-09-25 17:56:43,243 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 121.609s
2025-09-25 17:56:43,343 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9409
2025-09-25 17:56:43,343 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.101s
2025-09-25 17:56:43,343 - INFO - bo.run_bo - Recorded observation #43: hparams={'lr': 0.004082531240263393, 'batch_size': np.int64(256), 'epochs': np.int64(48), 'weight_decay': 3.217722332394048e-05, 'dropout': 0.13242046610680394, 'label_smoothing': 0.09348818735038253, 'grad_clip_norm': 1.1198526810563294, 'use_scheduler': np.True_, 'channels_base': np.int64(24), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(169), 'seed': np.int64(6682)}, value=0.9409
2025-09-25 17:56:43,344 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 43: {'lr': 0.004082531240263393, 'batch_size': np.int64(256), 'epochs': np.int64(48), 'weight_decay': 3.217722332394048e-05, 'dropout': 0.13242046610680394, 'label_smoothing': 0.09348818735038253, 'grad_clip_norm': 1.1198526810563294, 'use_scheduler': np.True_, 'channels_base': np.int64(24), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(169), 'seed': np.int64(6682)} -> 0.9409
2025-09-25 17:56:43,344 - INFO - bo.run_bo - üîçBO Trial 44: Using RF surrogate + Expected Improvement
2025-09-25 17:56:43,344 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:56:43,344 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:56:43,344 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:56:43,344 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003199959874555167, 'batch_size': 256, 'epochs': 45, 'weight_decay': 2.4233570222190365e-05, 'dropout': 0.16435647114441496, 'label_smoothing': 0.11305764754510639, 'grad_clip_norm': 2.079738857959389, 'use_scheduler': False, 'channels_base': 31, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 129, 'seed': 7696}
2025-09-25 17:56:43,345 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003199959874555167, 'batch_size': 256, 'epochs': 45, 'weight_decay': 2.4233570222190365e-05, 'dropout': 0.16435647114441496, 'label_smoothing': 0.11305764754510639, 'grad_clip_norm': 2.079738857959389, 'use_scheduler': False, 'channels_base': 31, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 129, 'seed': 7696}
2025-09-25 17:59:03,853 - INFO - _models.training_function_executor - Model: 44,050 parameters, 189.3KB storage
2025-09-25 17:59:03,854 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.9185403882420922, 1.6913345711454466, 1.568238101985279, 1.5214754279662175, 1.4876772746277107, 1.4782375449230283, 1.4566020273278764, 1.4433126202375897, 1.4345005962949713, 1.4318520986574155, 1.4230285648698702, 1.417964109030709, 1.4142570871273625, 1.404848850188285, 1.4012856335134034, 1.3969339578778988, 1.389947856375886, 1.3915716818026298, 1.3906198540968588, 1.3880566843312638, 1.3903991381140597, 1.38021658356617, 1.3831407305770316, 1.377356325807426, 1.3792167519192176, 1.3763139767145351, 1.3719034363922273, 1.370400954580425, 1.3672343389740662, 1.3698401952138637, 1.3655583457802185, 1.3685197849095632, 1.3648515219333361, 1.361353470703095, 1.3579788379933875, 1.359327448294567, 1.3561765372004422, 1.3637847810543946, 1.354292904487085, 1.3592424667617586, 1.3528390704989863, 1.3512024836586078, 1.3574402596319215, 1.3611139713237224, 1.3509968572424247], 'val_losses': [1.7445872172334262, 1.6802506494038147, 1.525260154174214, 1.5292629947072058, 1.552336044440576, 1.4623616192816136, 1.4462885594938562, 1.4405849698103397, 1.4659616959422113, 1.4242292107114993, 1.454664331975549, 1.4124454652257856, 1.4031848998285448, 1.39647202703263, 1.3961689936727857, 1.400149302021959, 1.4066263485367085, 1.3853318255402631, 1.3879865244387564, 1.3928178719538613, 1.380861543022799, 1.3875963560636593, 1.3855659822490995, 1.3734648895956731, 1.382282313630181, 1.3876389635339623, 1.3667104987180438, 1.3717438844851184, 1.3956960524671465, 1.3914141945037728, 1.3699000338805438, 1.3659948046950376, 1.377625604363525, 1.362290454260345, 1.3717300329661253, 1.3799888048983355, 1.3712211739612155, 1.3689781396763319, 1.3605448336637702, 1.3655637193332564, 1.3774447886212817, 1.3673584861215262, 1.3684139310708696, 1.381715550247611, 1.3666178918455407], 'val_acc': [0.24545796266132064, 0.31988472622478387, 0.43515850144092216, 0.3770204235058263, 0.3398070417240947, 0.4552061145219897, 0.4245082069916051, 0.41886981581255484, 0.3258990101491041, 0.46723468237063026, 0.4657311113895502, 0.683623606064403, 0.5103370504949254, 0.5435409096604435, 0.6164641022428267, 0.7259741886981581, 0.6276155870191705, 0.7015411602556071, 0.7574238817190828, 0.39982458338554067, 0.6055632126299962, 0.5847638140583886, 0.7304849016413983, 0.7858664327778474, 0.7755920310738003, 0.6987846134569603, 0.6689637889988723, 0.792131311865681, 0.6368876080691642, 0.8992607442676356, 0.7687006640771833, 0.7848640521237941, 0.8224533266507956, 0.7180804410474878, 0.5691016163388046, 0.7215887733366746, 0.8416238566595665, 0.7555444179927328, 0.7780979827089337, 0.6950256860042601, 0.6818694399198095, 0.7947625610825712, 0.7363738879839619, 0.44079689261997246, 0.6383911790502443], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003199959874555167, 'batch_size': 256, 'epochs': 45, 'weight_decay': 2.4233570222190365e-05, 'dropout': 0.16435647114441496, 'label_smoothing': 0.11305764754510639, 'grad_clip_norm': 2.079738857959389, 'use_scheduler': False, 'channels_base': 31, 'se_reduction': 8, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 129, 'seed': 7696}, 'model_parameter_count': 44050, 'model_storage_size_kb': 189.27734375000003, 'model_size_validation': 'PASS'}
2025-09-25 17:59:03,854 - INFO - _models.training_function_executor - BO Objective: base=0.6384, size_penalty=0.0000, final=0.6384
2025-09-25 17:59:03,854 - INFO - _models.training_function_executor - Model: 44,050 parameters, 189.3KB (PASS 256KB limit)
2025-09-25 17:59:03,854 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 140.510s
2025-09-25 17:59:03,956 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6384
2025-09-25 17:59:03,956 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.102s
2025-09-25 17:59:03,956 - INFO - bo.run_bo - Recorded observation #44: hparams={'lr': 0.003199959874555167, 'batch_size': np.int64(256), 'epochs': np.int64(45), 'weight_decay': 2.4233570222190365e-05, 'dropout': 0.16435647114441496, 'label_smoothing': 0.11305764754510639, 'grad_clip_norm': 2.079738857959389, 'use_scheduler': np.False_, 'channels_base': np.int64(31), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(129), 'seed': np.int64(7696)}, value=0.6384
2025-09-25 17:59:03,956 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 44: {'lr': 0.003199959874555167, 'batch_size': np.int64(256), 'epochs': np.int64(45), 'weight_decay': 2.4233570222190365e-05, 'dropout': 0.16435647114441496, 'label_smoothing': 0.11305764754510639, 'grad_clip_norm': 2.079738857959389, 'use_scheduler': np.False_, 'channels_base': np.int64(31), 'se_reduction': np.int64(8), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(129), 'seed': np.int64(7696)} -> 0.6384
2025-09-25 17:59:03,956 - INFO - bo.run_bo - üîçBO Trial 45: Using RF surrogate + Expected Improvement
2025-09-25 17:59:03,956 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 17:59:03,956 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 17:59:03,956 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 17:59:03,956 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008871951066319769, 'batch_size': 64, 'epochs': 46, 'weight_decay': 0.0012809897791404077, 'dropout': 0.15371391404730092, 'label_smoothing': 0.10743280618161614, 'grad_clip_norm': 1.598001124527528, 'use_scheduler': False, 'channels_base': 29, 'se_reduction': 7, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 205, 'seed': 6826}
2025-09-25 17:59:03,958 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008871951066319769, 'batch_size': 64, 'epochs': 46, 'weight_decay': 0.0012809897791404077, 'dropout': 0.15371391404730092, 'label_smoothing': 0.10743280618161614, 'grad_clip_norm': 1.598001124527528, 'use_scheduler': False, 'channels_base': 29, 'se_reduction': 7, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 205, 'seed': 6826}
2025-09-25 18:01:56,581 - INFO - _models.training_function_executor - Model: 39,357 parameters, 169.1KB storage
2025-09-25 18:01:56,581 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.8665737270715148, 1.5730995985214615, 1.5013133084247858, 1.4629553649239018, 1.4478645638386207, 1.4296338791234509, 1.4315277053102704, 1.4117181018656315, 1.414776084058824, 1.4016104484192207, 1.406781950200989, 1.4086180280870586, 1.3948329282659107, 1.3912535880370251, 1.3841306215553988, 1.3856444789761122, 1.3864193232858688, 1.3843171477104956, 1.3674705983164226, 1.3736154099691968, 1.3771148922486482, 1.37200162682442, 1.3640796410665137, 1.3715211185145169, 1.3666996714450645, 1.3660301792775837, 1.3679586387952498, 1.3657074039062977, 1.3635702080544092, 1.3535591495502743, 1.3596030179298795, 1.355686907182267, 1.3538320450930206, 1.3582012076031775, 1.358264180758997, 1.3538470469937753, 1.356916168664544, 1.3549728960716512, 1.35080531535731, 1.3537770377041025, 1.3551777708341097, 1.3507146988656518, 1.3431926455760783, 1.3375699730537531, 1.3541063758225547, 1.3431212727678408], 'val_losses': [1.6329699340820791, 1.5413529274383295, 1.4559637588182648, 1.437792094962786, 1.440656288989638, 1.4129973319132156, 1.404638117678018, 1.3982108254403403, 1.3859676723864993, 1.3835262779663147, 1.3897815302729442, 1.3778765235176034, 1.4011442581225033, 1.3796861141550407, 1.3651826348225287, 1.3758276656014237, 1.362361875217879, 1.367583113525643, 1.3585987873744163, 1.3574652911995067, 1.3642395858611698, 1.3538139295852838, 1.3544145973534412, 1.354737042947575, 1.3639638312492375, 1.3539779337706266, 1.3609258928419639, 1.3421745374976146, 1.34206158849563, 1.3482157960300711, 1.3393185681499733, 1.3450065805356077, 1.3445066564618726, 1.338276235430839, 1.344778366456906, 1.3457568830572084, 1.3410358558605076, 1.348388727021059, 1.3412863824051164, 1.3369204864930933, 1.338593732069465, 1.3434792755391272, 1.3509400808905287, 1.3390247729798186, 1.3396984390590503, 1.3425021772918777], 'val_acc': [0.2987094349079063, 0.3752662573612329, 0.4370379651672723, 0.631625109635384, 0.5315123418118031, 0.724846510462348, 0.4890364615962912, 0.8582884350332038, 0.6455331412103746, 0.6868813431900764, 0.7359979952386919, 0.7818569101616338, 0.6898884851522366, 0.8772083698784613, 0.8575366495426638, 0.7725848891116401, 0.6444054629745646, 0.8881092594912918, 0.8069164265129684, 0.8738253351710312, 0.8716952762811677, 0.8372384412980829, 0.8808419997494048, 0.9198095476757299, 0.7956396441548678, 0.9075303846635759, 0.8222027314872823, 0.8997619345946624, 0.9048991354466859, 0.8799649166771081, 0.8323518356095727, 0.8277158250845759, 0.8967547926325022, 0.8327277283548428, 0.9065280040095226, 0.8833479513845383, 0.9181806791128931, 0.9358476381405839, 0.8881092594912918, 0.8903646159629119, 0.8363613582257863, 0.899511339431149, 0.9203107380027565, 0.8534018293446937, 0.8549054003257737, 0.9265756170905901], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008871951066319769, 'batch_size': 64, 'epochs': 46, 'weight_decay': 0.0012809897791404077, 'dropout': 0.15371391404730092, 'label_smoothing': 0.10743280618161614, 'grad_clip_norm': 1.598001124527528, 'use_scheduler': False, 'channels_base': 29, 'se_reduction': 7, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 205, 'seed': 6826}, 'model_parameter_count': 39357, 'model_storage_size_kb': 169.11210937500002, 'model_size_validation': 'PASS'}
2025-09-25 18:01:56,581 - INFO - _models.training_function_executor - BO Objective: base=0.9266, size_penalty=0.0000, final=0.9266
2025-09-25 18:01:56,581 - INFO - _models.training_function_executor - Model: 39,357 parameters, 169.1KB (PASS 256KB limit)
2025-09-25 18:01:56,581 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 172.625s
2025-09-25 18:01:56,802 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9266
2025-09-25 18:01:56,802 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.221s
2025-09-25 18:01:56,802 - INFO - bo.run_bo - Recorded observation #45: hparams={'lr': 0.0008871951066319769, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'weight_decay': 0.0012809897791404077, 'dropout': 0.15371391404730092, 'label_smoothing': 0.10743280618161614, 'grad_clip_norm': 1.598001124527528, 'use_scheduler': np.False_, 'channels_base': np.int64(29), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(205), 'seed': np.int64(6826)}, value=0.9266
2025-09-25 18:01:56,802 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 45: {'lr': 0.0008871951066319769, 'batch_size': np.int64(64), 'epochs': np.int64(46), 'weight_decay': 0.0012809897791404077, 'dropout': 0.15371391404730092, 'label_smoothing': 0.10743280618161614, 'grad_clip_norm': 1.598001124527528, 'use_scheduler': np.False_, 'channels_base': np.int64(29), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(205), 'seed': np.int64(6826)} -> 0.9266
2025-09-25 18:01:56,803 - INFO - bo.run_bo - üîçBO Trial 46: Using RF surrogate + Expected Improvement
2025-09-25 18:01:56,803 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 18:01:56,803 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 18:01:56,803 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:01:56,803 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0018963611731961693, 'batch_size': 128, 'epochs': 29, 'weight_decay': 0.0014084515467485998, 'dropout': 0.03271206069892275, 'label_smoothing': 0.09313208053888444, 'grad_clip_norm': 1.6504562326202723, 'use_scheduler': True, 'channels_base': 29, 'se_reduction': 6, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 176, 'seed': 6644}
2025-09-25 18:01:56,804 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0018963611731961693, 'batch_size': 128, 'epochs': 29, 'weight_decay': 0.0014084515467485998, 'dropout': 0.03271206069892275, 'label_smoothing': 0.09313208053888444, 'grad_clip_norm': 1.6504562326202723, 'use_scheduler': True, 'channels_base': 29, 'se_reduction': 6, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 176, 'seed': 6644}
2025-09-25 18:03:29,335 - INFO - _models.training_function_executor - Model: 40,173 parameters, 172.6KB storage
2025-09-25 18:03:29,336 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.7265688979834448, 1.4158207730471264, 1.3402635206858198, 1.3061957925654248, 1.2854300784096773, 1.2829652808010774, 1.2616072869392705, 1.2470866526270692, 1.2477964333856375, 1.2442463328059155, 1.233861654488077, 1.2365379560353356, 1.2256312541255443, 1.2204248486513543, 1.216713427477857, 1.2117812573840676, 1.2094668594214923, 1.2035775097174606, 1.2053146783568351, 1.2013699190361855, 1.1942159492255595, 1.1943592315134957, 1.1896140596358222, 1.1942157354532013, 1.1920362072503607, 1.1848540565623484, 1.1811868139882138, 1.172351263222403, 1.1815685866877828], 'val_losses': [1.4990170185256162, 1.3770590648811243, 1.306786118556258, 1.2923402958324925, 1.268590843658103, 1.273744292507762, 1.3065843478642434, 1.2591138278731515, 1.24177409726984, 1.2508485952634707, 1.2306384243861788, 1.2820199637346705, 1.2383778799059635, 1.2188876146302907, 1.2250498741017504, 1.211052744001124, 1.2135021825016568, 1.2074960048837091, 1.2180990396682936, 1.202199452832414, 1.1942745383379378, 1.1971440133318358, 1.200057286679065, 1.19452210941525, 1.1942173076622946, 1.193027034108762, 1.1915685633581812, 1.1918010624589335, 1.1912853029822514], 'val_acc': [0.4158626738503947, 0.5376519233178799, 0.7188322265380278, 0.8174414233805287, 0.8209497556697156, 0.7881217892494675, 0.8869815812554818, 0.8342312993359228, 0.728354842751535, 0.8294699912291693, 0.8565342688886104, 0.9107881217892495, 0.8560330785615837, 0.9101616338804661, 0.7941360731737878, 0.8879839619095351, 0.8591655181055006, 0.8828467610575116, 0.92469615336424, 0.8879839619095351, 0.8757047988973813, 0.8838491417115649, 0.9337175792507204, 0.8942488409973688, 0.9089086580628993, 0.8956271143966922, 0.9077809798270894, 0.9135446685878963, 0.8932464603433153], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0018963611731961693, 'batch_size': 128, 'epochs': 29, 'weight_decay': 0.0014084515467485998, 'dropout': 0.03271206069892275, 'label_smoothing': 0.09313208053888444, 'grad_clip_norm': 1.6504562326202723, 'use_scheduler': True, 'channels_base': 29, 'se_reduction': 6, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 176, 'seed': 6644}, 'model_parameter_count': 40173, 'model_storage_size_kb': 172.618359375, 'model_size_validation': 'PASS'}
2025-09-25 18:03:29,336 - INFO - _models.training_function_executor - BO Objective: base=0.8932, size_penalty=0.0000, final=0.8932
2025-09-25 18:03:29,336 - INFO - _models.training_function_executor - Model: 40,173 parameters, 172.6KB (PASS 256KB limit)
2025-09-25 18:03:29,336 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 92.533s
2025-09-25 18:03:29,439 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8932
2025-09-25 18:03:29,439 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.103s
2025-09-25 18:03:29,439 - INFO - bo.run_bo - Recorded observation #46: hparams={'lr': 0.0018963611731961693, 'batch_size': np.int64(128), 'epochs': np.int64(29), 'weight_decay': 0.0014084515467485998, 'dropout': 0.03271206069892275, 'label_smoothing': 0.09313208053888444, 'grad_clip_norm': 1.6504562326202723, 'use_scheduler': np.True_, 'channels_base': np.int64(29), 'se_reduction': np.int64(6), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(176), 'seed': np.int64(6644)}, value=0.8932
2025-09-25 18:03:29,439 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 46: {'lr': 0.0018963611731961693, 'batch_size': np.int64(128), 'epochs': np.int64(29), 'weight_decay': 0.0014084515467485998, 'dropout': 0.03271206069892275, 'label_smoothing': 0.09313208053888444, 'grad_clip_norm': 1.6504562326202723, 'use_scheduler': np.True_, 'channels_base': np.int64(29), 'se_reduction': np.int64(6), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(176), 'seed': np.int64(6644)} -> 0.8932
2025-09-25 18:03:29,439 - INFO - bo.run_bo - üîçBO Trial 47: Using RF surrogate + Expected Improvement
2025-09-25 18:03:29,440 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 18:03:29,440 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 18:03:29,440 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:03:29,440 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0014317342151626059, 'batch_size': 256, 'epochs': 34, 'weight_decay': 0.008724546792166561, 'dropout': 0.0009327515765887309, 'label_smoothing': 0.12462639415282058, 'grad_clip_norm': 0.5468223055320788, 'use_scheduler': True, 'channels_base': 23, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 168, 'seed': 5346}
2025-09-25 18:03:29,441 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0014317342151626059, 'batch_size': 256, 'epochs': 34, 'weight_decay': 0.008724546792166561, 'dropout': 0.0009327515765887309, 'label_smoothing': 0.12462639415282058, 'grad_clip_norm': 0.5468223055320788, 'use_scheduler': True, 'channels_base': 23, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 168, 'seed': 5346}
2025-09-25 18:04:54,186 - INFO - _models.training_function_executor - Model: 25,051 parameters, 107.6KB storage
2025-09-25 18:04:54,187 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [2.025965864021363, 1.776750148263181, 1.662400186425951, 1.6018230516697027, 1.577826741132111, 1.5566071094630285, 1.552365789391644, 1.536073943537451, 1.5310322554178772, 1.521404419627042, 1.5113943276711397, 1.5070322705357795, 1.4970225603946508, 1.4921778370644505, 1.4901391547772496, 1.4892019975589816, 1.4787915089493149, 1.4714816282279688, 1.469577163755679, 1.4687149264857804, 1.4630313221570248, 1.4627086156607234, 1.4603748403276502, 1.4559446837084038, 1.457867693890606, 1.4538605254466375, 1.4482570143491034, 1.4494353423078112, 1.4425624687973466, 1.4453657501096306, 1.4440703541922644, 1.4399507090669188, 1.4420202556859019, 1.4420282777470126], 'val_losses': [1.89514124296375, 1.7700239715831887, 1.690449019300506, 1.6626482182555204, 1.5952424562719738, 1.5447011929289647, 1.5834348626549628, 1.5496010027826765, 1.5529140603376532, 1.541791607302661, 1.5779902393425667, 1.5004187820101424, 1.4990441138665787, 1.5035612755529177, 1.5664654125203585, 1.4941578824304127, 1.4851157631495888, 1.514124894527516, 1.4745406622337587, 1.50553107611175, 1.4769962629479074, 1.467001094837234, 1.4724237508096278, 1.4682304634215644, 1.4680577243065986, 1.4654313026999877, 1.463018126631721, 1.455891385590458, 1.4540082202969211, 1.4547558307587958, 1.4603174808792805, 1.4519058471782929, 1.4520284730440214, 1.4592595580765035], 'val_acc': [0.12980829469991229, 0.21350707931336926, 0.3227665706051873, 0.27866182182683874, 0.46360105249968675, 0.3946873825335171, 0.423756421501065, 0.6015536900137828, 0.38541536148352334, 0.4461846886355093, 0.36499185565718584, 0.37313619847136953, 0.4010775592031074, 0.323267760932214, 0.2604936724721213, 0.34770078937476506, 0.4454329031449693, 0.3205112141335672, 0.5326400200476131, 0.4265129682997118, 0.3672472121288059, 0.37426387670717953, 0.28004009522616213, 0.5395313870442301, 0.3092344317754667, 0.43039719333416865, 0.4116025560706678, 0.4966796140834482, 0.5120912166395188, 0.40195464227540406, 0.3770204235058263, 0.49993735120912164, 0.42814183686254853, 0.5090840746773587], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0014317342151626059, 'batch_size': 256, 'epochs': 34, 'weight_decay': 0.008724546792166561, 'dropout': 0.0009327515765887309, 'label_smoothing': 0.12462639415282058, 'grad_clip_norm': 0.5468223055320788, 'use_scheduler': True, 'channels_base': 23, 'se_reduction': 7, 'dilation_b1': 1, 'dilation_b2': 4, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_steps': 168, 'seed': 5346}, 'model_parameter_count': 25051, 'model_storage_size_kb': 107.64101562500001, 'model_size_validation': 'PASS'}
2025-09-25 18:04:54,187 - INFO - _models.training_function_executor - BO Objective: base=0.5091, size_penalty=0.0000, final=0.5091
2025-09-25 18:04:54,187 - INFO - _models.training_function_executor - Model: 25,051 parameters, 107.6KB (PASS 256KB limit)
2025-09-25 18:04:54,187 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 84.747s
2025-09-25 18:04:54,289 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5091
2025-09-25 18:04:54,289 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.102s
2025-09-25 18:04:54,289 - INFO - bo.run_bo - Recorded observation #47: hparams={'lr': 0.0014317342151626059, 'batch_size': np.int64(256), 'epochs': np.int64(34), 'weight_decay': 0.008724546792166561, 'dropout': 0.0009327515765887309, 'label_smoothing': 0.12462639415282058, 'grad_clip_norm': 0.5468223055320788, 'use_scheduler': np.True_, 'channels_base': np.int64(23), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(168), 'seed': np.int64(5346)}, value=0.5091
2025-09-25 18:04:54,289 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 47: {'lr': 0.0014317342151626059, 'batch_size': np.int64(256), 'epochs': np.int64(34), 'weight_decay': 0.008724546792166561, 'dropout': 0.0009327515765887309, 'label_smoothing': 0.12462639415282058, 'grad_clip_norm': 0.5468223055320788, 'use_scheduler': np.True_, 'channels_base': np.int64(23), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(4), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(168), 'seed': np.int64(5346)} -> 0.5091
2025-09-25 18:04:54,290 - INFO - bo.run_bo - üîçBO Trial 48: Using RF surrogate + Expected Improvement
2025-09-25 18:04:54,290 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 18:04:54,290 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 18:04:54,290 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:04:54,290 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.000840100953556839, 'batch_size': 256, 'epochs': 36, 'weight_decay': 0.008103934669559082, 'dropout': 0.2354520211719741, 'label_smoothing': 0.05278049728964996, 'grad_clip_norm': 4.636685357811771, 'use_scheduler': False, 'channels_base': 31, 'se_reduction': 5, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 113, 'seed': 2983}
2025-09-25 18:04:54,291 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.000840100953556839, 'batch_size': 256, 'epochs': 36, 'weight_decay': 0.008103934669559082, 'dropout': 0.2354520211719741, 'label_smoothing': 0.05278049728964996, 'grad_clip_norm': 4.636685357811771, 'use_scheduler': False, 'channels_base': 31, 'se_reduction': 5, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 113, 'seed': 2983}
2025-09-25 18:06:46,970 - INFO - _models.training_function_executor - Model: 47,105 parameters, 202.4KB storage
2025-09-25 18:06:46,970 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5981136257515602, 1.321366603693463, 1.155609528283422, 1.0710633135460315, 1.0281044339131387, 0.9996400148472803, 0.9767323580279041, 0.9565107244426989, 0.9502850870095343, 0.942555322866446, 0.9296366342815455, 0.9168119581518767, 0.9177845034747032, 0.9149939180955259, 0.903812615671571, 0.9051181836351245, 0.8895005612236976, 0.8917339745609884, 0.8904776680541584, 0.8910892744043121, 0.8838437135101646, 0.8825382915250574, 0.8812653522695645, 0.8784534314392511, 0.8709283198150586, 0.8697422124522225, 0.875153714697175, 0.8694711921209307, 0.8598307728584416, 0.8613675022149143, 0.8579950386601888, 0.8576240931388351, 0.8578802350917504, 0.8561679645054129, 0.8567054741768648, 0.853068703525598], 'val_losses': [1.4309679131328037, 1.3542800849130199, 1.085593041255202, 1.0340308609977877, 1.0108548926912801, 0.9853964204134581, 1.0177176433343844, 0.9467687553039635, 0.9930745301995267, 0.9712038243896843, 0.9078276798758288, 0.9277457805254035, 0.9114825026819602, 0.9025811092131509, 0.9332807588332309, 0.8999285722731827, 0.8898091533700561, 0.8967552421057438, 0.8917776021036106, 0.9346618682515754, 0.9589720124992837, 0.881475095566077, 0.8678586954551298, 0.8879253080712421, 0.8767486095069987, 0.9325633495019172, 0.8966134100715524, 0.9149828581654059, 0.8587273599629175, 0.858367895321236, 0.855311548272226, 0.885978640089237, 0.8695656920327771, 0.854667127677603, 0.8596168892557736, 0.9293091808430698], 'val_acc': [0.39355970429770704, 0.4800150357098108, 0.6411477258488911, 0.6986593158752036, 0.7551685252474627, 0.6814935471745396, 0.8284676105751159, 0.8352336799899762, 0.8627991479764441, 0.6728480140333292, 0.8706928956271144, 0.7614334043352964, 0.8411226663325397, 0.7720836987846135, 0.8956271143966922, 0.8435033203859166, 0.8873574740007518, 0.8844756296203483, 0.8878586643277785, 0.8877333667460218, 0.7858664327778474, 0.8889863425635885, 0.9137952637514096, 0.8488911164014534, 0.9248214509459968, 0.762310487407593, 0.8673098609196842, 0.8714446811176544, 0.9037714572108758, 0.9169277032953264, 0.8887357474000752, 0.9587770956020549, 0.8684375391554943, 0.9248214509459968, 0.9332163889236937, 0.7804786367623104], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.000840100953556839, 'batch_size': 256, 'epochs': 36, 'weight_decay': 0.008103934669559082, 'dropout': 0.2354520211719741, 'label_smoothing': 0.05278049728964996, 'grad_clip_norm': 4.636685357811771, 'use_scheduler': False, 'channels_base': 31, 'se_reduction': 5, 'dilation_b1': 4, 'dilation_b2': 2, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 113, 'seed': 2983}, 'model_parameter_count': 47105, 'model_storage_size_kb': 202.40429687500003, 'model_size_validation': 'PASS'}
2025-09-25 18:06:46,970 - INFO - _models.training_function_executor - BO Objective: base=0.7805, size_penalty=0.0000, final=0.7805
2025-09-25 18:06:46,970 - INFO - _models.training_function_executor - Model: 47,105 parameters, 202.4KB (PASS 256KB limit)
2025-09-25 18:06:46,970 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 112.680s
2025-09-25 18:06:47,074 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7805
2025-09-25 18:06:47,074 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.104s
2025-09-25 18:06:47,074 - INFO - bo.run_bo - Recorded observation #48: hparams={'lr': 0.000840100953556839, 'batch_size': np.int64(256), 'epochs': np.int64(36), 'weight_decay': 0.008103934669559082, 'dropout': 0.2354520211719741, 'label_smoothing': 0.05278049728964996, 'grad_clip_norm': 4.636685357811771, 'use_scheduler': np.False_, 'channels_base': np.int64(31), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(113), 'seed': np.int64(2983)}, value=0.7805
2025-09-25 18:06:47,074 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 48: {'lr': 0.000840100953556839, 'batch_size': np.int64(256), 'epochs': np.int64(36), 'weight_decay': 0.008103934669559082, 'dropout': 0.2354520211719741, 'label_smoothing': 0.05278049728964996, 'grad_clip_norm': 4.636685357811771, 'use_scheduler': np.False_, 'channels_base': np.int64(31), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(4), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(113), 'seed': np.int64(2983)} -> 0.7805
2025-09-25 18:06:47,075 - INFO - bo.run_bo - üîçBO Trial 49: Using RF surrogate + Expected Improvement
2025-09-25 18:06:47,075 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-25 18:06:47,075 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 18:06:47,075 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:06:47,075 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0024104802961855526, 'batch_size': 256, 'epochs': 37, 'weight_decay': 1.2588637173133993e-05, 'dropout': 0.2044999366191284, 'label_smoothing': 0.10032718709238928, 'grad_clip_norm': 1.0310778967297836, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 204, 'seed': 6085}
2025-09-25 18:06:47,076 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0024104802961855526, 'batch_size': 256, 'epochs': 37, 'weight_decay': 1.2588637173133993e-05, 'dropout': 0.2044999366191284, 'label_smoothing': 0.10032718709238928, 'grad_clip_norm': 1.0310778967297836, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 204, 'seed': 6085}
2025-09-25 18:08:20,664 - INFO - _models.training_function_executor - Model: 27,075 parameters, 116.3KB storage
2025-09-25 18:08:20,664 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.882985283728512, 1.6446128464453136, 1.538152701894364, 1.4646135832968166, 1.438097463769766, 1.4099437381242617, 1.3978801615849192, 1.3808843763505771, 1.3618211756821164, 1.3536829852753145, 1.3427301780982517, 1.3346929461257788, 1.3276067878639617, 1.3238124540081202, 1.311545160925598, 1.3115861401448732, 1.3052827552741875, 1.3044868900084097, 1.2979062021560988, 1.2951235407834154, 1.2880123531378118, 1.283510393600164, 1.276442974786227, 1.2865636375611151, 1.2775740442809853, 1.274134926302587, 1.2721543824041979, 1.2678031822951206, 1.2774161674009694, 1.2640742279923627, 1.2671195692471295, 1.2681916921400087, 1.2614650291088503, 1.2579906244488268, 1.2587994362138901, 1.264934989857322, 1.2580124323437456], 'val_losses': [1.734925881347446, 1.6864424196046601, 1.4626094630272104, 1.4310570550643027, 1.3934570174365992, 1.3894711054890485, 1.3727611096755796, 1.3627728112223991, 1.4040201844286013, 1.3761822524009202, 1.3257103724193489, 1.305610216316641, 1.3186037286811716, 1.3054500092780168, 1.3335520096730953, 1.3003126681411106, 1.2898643010243065, 1.3347550501666292, 1.288762917495912, 1.2757308077444753, 1.2763006859084984, 1.2870426562148787, 1.285787990809891, 1.2870446976198169, 1.2707322625913142, 1.273460814953149, 1.2700644617106978, 1.2641252125284903, 1.269766729706407, 1.2625799205216262, 1.2675952650583293, 1.2631129067638078, 1.2595413087558782, 1.2577512647824514, 1.258725437868838, 1.2590517711257025, 1.2599031276951427], 'val_acc': [0.21049993735120912, 0.2083698784613457, 0.43616088209497555, 0.5063275278787119, 0.49379776970304473, 0.5431650169151735, 0.645031950883348, 0.6480390928455081, 0.3444430522490916, 0.7327402581130185, 0.7075554441799273, 0.5722340558827215, 0.7289813306603182, 0.6346322515975442, 0.6277408846009273, 0.8205738629244456, 0.7644405462974565, 0.6858789625360231, 0.7947625610825712, 0.8411226663325397, 0.7905024433028442, 0.8289688009021426, 0.8300964791379526, 0.8477634381656434, 0.8452574865305099, 0.8757047988973813, 0.8358601678987595, 0.8262122541034959, 0.8090464854028317, 0.8243327903771457, 0.8477634381656434, 0.8633003383034707, 0.8371131437163263, 0.852900639017667, 0.862047362485904, 0.8627991479764441, 0.8650545044480642], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0024104802961855526, 'batch_size': 256, 'epochs': 37, 'weight_decay': 1.2588637173133993e-05, 'dropout': 0.2044999366191284, 'label_smoothing': 0.10032718709238928, 'grad_clip_norm': 1.0310778967297836, 'use_scheduler': True, 'channels_base': 24, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 1, 'dilation_b3': 1, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_steps': 204, 'seed': 6085}, 'model_parameter_count': 27075, 'model_storage_size_kb': 116.33789062500001, 'model_size_validation': 'PASS'}
2025-09-25 18:08:20,664 - INFO - _models.training_function_executor - BO Objective: base=0.8651, size_penalty=0.0000, final=0.8651
2025-09-25 18:08:20,664 - INFO - _models.training_function_executor - Model: 27,075 parameters, 116.3KB (PASS 256KB limit)
2025-09-25 18:08:20,664 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 93.589s
2025-09-25 18:08:20,886 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8651
2025-09-25 18:08:20,886 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.222s
2025-09-25 18:08:20,886 - INFO - bo.run_bo - Recorded observation #49: hparams={'lr': 0.0024104802961855526, 'batch_size': np.int64(256), 'epochs': np.int64(37), 'weight_decay': 1.2588637173133993e-05, 'dropout': 0.2044999366191284, 'label_smoothing': 0.10032718709238928, 'grad_clip_norm': 1.0310778967297836, 'use_scheduler': np.True_, 'channels_base': np.int64(24), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(204), 'seed': np.int64(6085)}, value=0.8651
2025-09-25 18:08:20,886 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 49: {'lr': 0.0024104802961855526, 'batch_size': np.int64(256), 'epochs': np.int64(37), 'weight_decay': 1.2588637173133993e-05, 'dropout': 0.2044999366191284, 'label_smoothing': 0.10032718709238928, 'grad_clip_norm': 1.0310778967297836, 'use_scheduler': np.True_, 'channels_base': np.int64(24), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(1), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_steps': np.int64(204), 'seed': np.int64(6085)} -> 0.8651
2025-09-25 18:08:20,887 - INFO - bo.run_bo - üîçBO Trial 50: Using RF surrogate + Expected Improvement
2025-09-25 18:08:20,887 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-25 18:08:20,887 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 18:08:20,887 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:08:20,887 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0016241915075124393, 'batch_size': 64, 'epochs': 50, 'weight_decay': 0.00011689530351992951, 'dropout': 0.15825999386433157, 'label_smoothing': 0.10440189576767157, 'grad_clip_norm': 1.0060016648496986, 'use_scheduler': True, 'channels_base': 28, 'se_reduction': 5, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 23, 'seed': 6921}
2025-09-25 18:08:20,888 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0016241915075124393, 'batch_size': 64, 'epochs': 50, 'weight_decay': 0.00011689530351992951, 'dropout': 0.15825999386433157, 'label_smoothing': 0.10440189576767157, 'grad_clip_norm': 1.0060016648496986, 'use_scheduler': True, 'channels_base': 28, 'se_reduction': 5, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 23, 'seed': 6921}
2025-09-25 18:11:16,610 - INFO - _models.training_function_executor - Model: 38,655 parameters, 166.1KB storage
2025-09-25 18:11:16,610 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.8223234616481163, 1.5544567866361676, 1.4947949698353509, 1.4632003085451855, 1.444012804945224, 1.4277015197875673, 1.4177897479701969, 1.4149709792191367, 1.399963598485407, 1.394161774812439, 1.3866186227109263, 1.377923022198415, 1.382883689051989, 1.374619382588303, 1.3740797477142968, 1.3691565934807288, 1.365198053386215, 1.3696721969337775, 1.3614793983657598, 1.3588022286441659, 1.3598156391638874, 1.3523127270966728, 1.3499615009433812, 1.353054194116086, 1.3406204657645295, 1.341771275015122, 1.341906437863235, 1.3335042090933227, 1.3435077360726961, 1.3434354213008313, 1.3317493286825297, 1.3322708037466953, 1.3339206462059836, 1.3234692116867826, 1.3240914061692142, 1.3284152239347293, 1.3316428303901546, 1.3263798673600364, 1.3182326469587786, 1.3245490580518058, 1.3229557469244928, 1.319756641377633, 1.3220070654806015, 1.3141784950802806, 1.320284907069417, 1.3105419351766086, 1.3079432822056507, 1.3206939791220007, 1.3133881111601795, 1.3190030116573057], 'val_losses': [1.5931771068909968, 1.5091862840590329, 1.450942593504501, 1.4284760102250644, 1.418891361222472, 1.4379020612530504, 1.4254832246551208, 1.3880832567951042, 1.3775525762837357, 1.3639475445806524, 1.3676381157149857, 1.373722916392661, 1.3659047011578684, 1.3719970850459975, 1.3557591094452104, 1.3505633202968512, 1.3645513407524748, 1.339794930203668, 1.3498806481432488, 1.3360296870783568, 1.3369068349607158, 1.3328538009388795, 1.331692475182806, 1.3393890757322342, 1.3334547741761738, 1.3242626419014807, 1.3297829020667116, 1.326902142623779, 1.3274490593997568, 1.3293991335321678, 1.3247603115738522, 1.3250312707490184, 1.3233746012236118, 1.3269703440492335, 1.320916607555867, 1.3228752251750409, 1.32195924640135, 1.3166742090622654, 1.323587858412532, 1.3168616134740105, 1.318667546293667, 1.3183502176324462, 1.323942523539746, 1.3176137543369872, 1.3174893450369558, 1.315230852364747, 1.3171815007809275, 1.3169527007678803, 1.3160660407280595, 1.3185393917218529], 'val_acc': [0.32577371256734744, 0.423380528755795, 0.5728605437915049, 0.6474126049367247, 0.3617341185315123, 0.4448064152361859, 0.5642150106502944, 0.7024182433279038, 0.5396566846259867, 0.76845006891367, 0.6299962410725473, 0.4935471745395314, 0.7194587144468112, 0.7694524495677233, 0.8109259491291818, 0.7192081192832979, 0.7096855030697907, 0.8353589775717328, 0.900639017666959, 0.8213256484149856, 0.8879839619095351, 0.8180679112893121, 0.8194461846886355, 0.8397443929332163, 0.9070291943365493, 0.8193208871068789, 0.8823455707304849, 0.8683122415737377, 0.8760806916426513, 0.8559077809798271, 0.8795890239318381, 0.8916175917804786, 0.8896128304723718, 0.9169277032953264, 0.9011402079939858, 0.8778348577872447, 0.8838491417115649, 0.8902393183811552, 0.9032702668838491, 0.8976318757047989, 0.8902393183811552, 0.92206490414735, 0.9244455582007267, 0.8986342563588523, 0.9106628242074928, 0.8799649166771081, 0.9106628242074928, 0.899511339431149, 0.9176794887858665, 0.9110387169527628], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0016241915075124393, 'batch_size': 64, 'epochs': 50, 'weight_decay': 0.00011689530351992951, 'dropout': 0.15825999386433157, 'label_smoothing': 0.10440189576767157, 'grad_clip_norm': 1.0060016648496986, 'use_scheduler': True, 'channels_base': 28, 'se_reduction': 5, 'dilation_b1': 1, 'dilation_b2': 1, 'dilation_b3': 2, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 23, 'seed': 6921}, 'model_parameter_count': 38655, 'model_storage_size_kb': 166.095703125, 'model_size_validation': 'PASS'}
2025-09-25 18:11:16,610 - INFO - _models.training_function_executor - BO Objective: base=0.9110, size_penalty=0.0000, final=0.9110
2025-09-25 18:11:16,610 - INFO - _models.training_function_executor - Model: 38,655 parameters, 166.1KB (PASS 256KB limit)
2025-09-25 18:11:16,610 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 175.723s
2025-09-25 18:11:16,714 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9110
2025-09-25 18:11:16,714 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.104s
2025-09-25 18:11:16,714 - INFO - bo.run_bo - Recorded observation #50: hparams={'lr': 0.0016241915075124393, 'batch_size': np.int64(64), 'epochs': np.int64(50), 'weight_decay': 0.00011689530351992951, 'dropout': 0.15825999386433157, 'label_smoothing': 0.10440189576767157, 'grad_clip_norm': 1.0060016648496986, 'use_scheduler': np.True_, 'channels_base': np.int64(28), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(23), 'seed': np.int64(6921)}, value=0.9110
2025-09-25 18:11:16,714 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 50: {'lr': 0.0016241915075124393, 'batch_size': np.int64(64), 'epochs': np.int64(50), 'weight_decay': 0.00011689530351992951, 'dropout': 0.15825999386433157, 'label_smoothing': 0.10440189576767157, 'grad_clip_norm': 1.0060016648496986, 'use_scheduler': np.True_, 'channels_base': np.int64(28), 'se_reduction': np.int64(5), 'dilation_b1': np.int64(1), 'dilation_b2': np.int64(1), 'dilation_b3': np.int64(2), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(23), 'seed': np.int64(6921)} -> 0.9110
2025-09-25 18:11:16,714 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.9518
2025-09-25 18:11:16,714 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.0008558398871419747, 'batch_size': np.int64(64), 'epochs': np.int64(12), 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': np.False_, 'channels_base': np.int64(27), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(133), 'seed': np.int64(9542)}
2025-09-25 18:11:16,714 - INFO - visualization - Generating BO visualization charts with 50 trials...
2025-09-25 18:11:18,028 - INFO - visualization - BO summary saved to: charts/20250925_181116_BO_TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)/bo_summary.txt
2025-09-25 18:11:18,028 - INFO - visualization - BO charts saved to: charts/20250925_181116_BO_TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:11:18,028 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä BO charts saved to: charts/20250925_181116_BO_TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:11:18,080 - INFO - evaluation.code_generation_pipeline_orchestrator - üöÄ STEP 4: Final Training Execution
2025-09-25 18:11:18,080 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-25 18:11:18,171 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-25 18:11:18,183 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-25 18:11:18,198 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-25 18:11:18,198 - INFO - _models.training_function_executor - Loaded training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:11:18,198 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-25 18:11:18,198 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.0008558398871419747, 'batch_size': np.int64(64), 'epochs': np.int64(12), 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': np.False_, 'channels_base': np.int64(27), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(133), 'seed': np.int64(9542)}
2025-09-25 18:11:18,198 - INFO - evaluation.code_generation_pipeline_orchestrator - Using test set for final training evaluation
2025-09-25 18:11:18,198 - INFO - _models.training_function_executor - Using device: cuda
2025-09-25 18:11:18,220 - INFO - _models.training_function_executor - Executing training function: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:11:18,220 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008558398871419747, 'batch_size': np.int64(64), 'epochs': np.int64(12), 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': np.False_, 'channels_base': np.int64(27), 'se_reduction': np.int64(7), 'dilation_b1': np.int64(2), 'dilation_b2': np.int64(2), 'dilation_b3': np.int64(4), 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_steps': np.int64(133), 'seed': np.int64(9542)}
2025-09-25 18:11:18,221 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008558398871419747, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': False, 'channels_base': 27, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 133, 'seed': 9542}
2025-09-25 18:12:09,150 - INFO - _models.training_function_executor - Model: 34,158 parameters, 146.8KB storage
2025-09-25 18:12:09,150 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0381493703689972, 0.6378882046021168, 0.4742030367530052, 0.41341588965825493, 0.3658428376282131, 0.3384798302732665, 0.3116006388925703, 0.29375972080775614, 0.28467739779134704, 0.25864305534215576, 0.2544018244721838, 0.253262420206569], 'val_losses': [0.8116614199059402, 0.5141520773854981, 0.38606673579926803, 0.34773130889463116, 0.40520817921295665, 0.2820355046408167, 0.29909267929578953, 0.31955372398161636, 0.24114010063927613, 0.2822799196259345, 0.2292497827963538, 0.25300071280885456], 'val_acc': [0.6773314088685751, 0.7952048753107209, 0.8992863443188197, 0.9067436452569962, 0.9044182503407906, 0.9198941544382968, 0.928874989976746, 0.916686713174565, 0.9291957341031193, 0.9122764814369337, 0.9282335017239997, 0.9522091251703954], 'model_name': 'TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008558398871419747, 'batch_size': 64, 'epochs': 12, 'weight_decay': 0.007411798255374396, 'dropout': 0.19438171582950015, 'label_smoothing': 0.0008425141906321933, 'grad_clip_norm': 2.087033752913602, 'use_scheduler': False, 'channels_base': 27, 'se_reduction': 7, 'dilation_b1': 2, 'dilation_b2': 2, 'dilation_b3': 4, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_steps': 133, 'seed': 9542}, 'model_parameter_count': 34158, 'model_storage_size_kb': 146.77265625, 'model_size_validation': 'PASS'}
2025-09-25 18:12:09,150 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final test metrics from training (avoids preprocessing mismatch)
2025-09-25 18:12:09,150 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 0.9522091251703954, 'macro_f1': None}
2025-09-25 18:12:09,158 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä STEP 5: Performance Analysis
2025-09-25 18:12:09,158 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:12:09,158 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.9518
2025-09-25 18:12:09,158 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: 0.9522
2025-09-25 18:12:09,158 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-09-25 18:12:09,158 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)
2025-09-25 18:12:09,158 - INFO - evaluation.code_generation_pipeline_orchestrator - Score: 0.9522
2025-09-25 18:12:09,158 - INFO - __main__ - AI-enhanced training completed!
2025-09-25 18:12:09,158 - INFO - __main__ - Final model achieved: {'acc': 0.9522091251703954, 'macro_f1': None}
2025-09-25 18:12:09,158 - INFO - __main__ - Pipeline completed successfully in single attempt
2025-09-25 18:12:09,158 - INFO - __main__ - Pipeline completed: TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class), metrics: {'acc': 0.9522091251703954, 'macro_f1': None}
2025-09-25 18:12:09,158 - INFO - __main__ - Pipeline summary saved to charts/pipeline_summary_20250925_181209.json
2025-09-25 18:12:09,163 - INFO - __main__ - Model saved: trained_models/best_model_TwoLead1D-CNN-SE-Residual-Dilated (MIT-BIH 5-class)_20250925_181209.pth, performance: {'acc': 0.9522091251703954, 'macro_f1': None}
2025-09-25 18:12:09,163 - INFO - __main__ - AI-enhanced processing completed successfully
