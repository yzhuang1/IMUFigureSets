2025-10-02 19:10:49,748 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 19:10:49,856 - INFO - __main__ - Logging system initialized successfully
2025-10-02 19:10:49,856 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-10-02 19:10:49,856 - INFO - __main__ - Starting real data processing from data/dataset1/ directory
2025-10-02 19:10:49,856 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-10-02 19:10:49,856 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-10-02 19:10:49,856 - INFO - __main__ - Attempting to load: X.npy
2025-10-02 19:10:49,899 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-10-02 19:10:49,939 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-10-02 19:10:49,939 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-10-02 19:10:49,939 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-10-02 19:10:49,939 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-10-02 19:10:49,941 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-02 19:10:49,941 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-10-02 19:10:49,941 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-10-02 19:10:49,941 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-10-02 19:10:49,941 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-10-02 19:10:49,941 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-10-02 19:10:49,941 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-10-02 19:10:49,941 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-10-02 19:10:49,941 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-10-02 19:10:50,039 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-10-02 19:10:50,039 - INFO - class_balancing - Class imbalance analysis:
2025-10-02 19:10:50,039 - INFO - class_balancing -   Strategy: severe_imbalance
2025-10-02 19:10:50,039 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-10-02 19:10:50,039 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-10-02 19:10:50,039 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-10-02 19:10:50,039 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-10-02 19:10:50,039 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-10-02 19:10:50,039 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-10-02 19:10:50,039 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-10-02 19:10:50,210 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-10-02 19:10:50,210 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-10-02 19:10:50,211 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-10-02 19:10:50,211 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-10-02 19:10:50,211 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-10-02 19:10:50,211 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-10-02 19:10:50,211 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-10-02 19:13:32,652 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 19:13:32,685 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-02 19:13:32,685 - INFO - _models.ai_code_generator - Prompt length: 4093 characters
2025-10-02 19:13:32,685 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-02 19:13:32,685 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-02 19:13:32,685 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-02 19:16:42,087 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 19:16:42,088 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-02 19:16:42,088 - INFO - _models.ai_code_generator - AI generated training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:16:42,088 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-10-02 19:16:42,088 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-10-02 19:16:42,088 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:16:42,089 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['epochs', 'batch_size', 'lr', 'weight_decay', 'patch_size', 'n_heads', 'head_dim', 'num_layers', 'mlp_ratio', 'dropout', 'attn_dropout', 'augment_noise_std', 'augment_scale_low', 'augment_scale_high', 'use_focal_loss', 'focal_gamma', 'class_weighted', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-10-02 19:16:42,089 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-10-02 19:16:42,089 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-10-02 19:16:42,089 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_Tiny1DTransformer-1D__Busia_et_al._2024,_adapted_for__1000,2_1759450602.json
2025-10-02 19:16:42,089 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_Tiny1DTransformer-1D__Busia_et_al._2024,_adapted_for__1000,2_1759450602.json
2025-10-02 19:16:42,089 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-10-02 19:16:42,089 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:16:42,089 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-10-02 19:16:42,090 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-02 19:16:42,092 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-02 19:16:42,092 - INFO - package_installer - Available packages: {'torch'}
2025-10-02 19:16:42,092 - INFO - package_installer - Missing packages: set()
2025-10-02 19:16:42,092 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-02 19:16:42,092 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-02 19:16:42,092 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 19:16:42,092 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-02 19:16:42,092 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['epochs', 'batch_size', 'lr', 'weight_decay', 'patch_size', 'n_heads', 'head_dim', 'num_layers', 'mlp_ratio', 'dropout', 'attn_dropout', 'augment_noise_std', 'augment_scale_low', 'augment_scale_high', 'use_focal_loss', 'focal_gamma', 'class_weighted', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-10-02 19:16:42,092 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 19:16:42,092 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 19:16:42,092 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-02 19:16:42,125 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-02 19:16:42,259 - INFO - bo.run_bo - Converted GPT search space: 21 parameters
2025-10-02 19:16:42,259 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-02 19:16:42,259 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-02 19:16:42,260 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-02 19:16:42,260 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 19:16:42,260 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:16:42,260 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:16:42,260 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 56, 'batch_size': 512, 'lr': 0.0015702970884055383, 'weight_decay': 0.0002481040974867812, 'patch_size': 10, 'n_heads': 3, 'head_dim': 26, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4330880728874677, 'attn_dropout': 0.18033450352296268, 'augment_noise_std': 0.03540362888980228, 'augment_scale_low': 0.8041168988591605, 'augment_scale_high': 1.1939819704323988, 'use_focal_loss': False, 'focal_gamma': 1.8493564427131048, 'class_weighted': True, 'grad_clip': 0.9170225492671692, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:16:42,261 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 56, 'batch_size': 512, 'lr': 0.0015702970884055383, 'weight_decay': 0.0002481040974867812, 'patch_size': 10, 'n_heads': 3, 'head_dim': 26, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4330880728874677, 'attn_dropout': 0.18033450352296268, 'augment_noise_std': 0.03540362888980228, 'augment_scale_low': 0.8041168988591605, 'augment_scale_high': 1.1939819704323988, 'use_focal_loss': False, 'focal_gamma': 1.8493564427131048, 'class_weighted': True, 'grad_clip': 0.9170225492671692, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:16:42,261 - ERROR - _models.training_function_executor - Training execution failed: train_model() got an unexpected keyword argument 'augment_scale_low'
2025-10-02 19:16:42,261 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-02 19:16:42,261 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-02 19:16:42,261 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-02 19:16:42,261 - INFO - _models.ai_code_generator - Prompt length: 17392 characters
2025-10-02 19:16:42,261 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-02 19:16:42,261 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-02 19:16:42,261 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-02 19:18:00,723 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 19:18:00,724 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-02 19:18:00,724 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251002_191800_attempt1.txt
2025-10-02 19:18:00,724 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code":"def train_model(\n    X_train, y_train, X_val, y_val, device,\n    epochs=20,\n    batch_size=256,\n    lr=3e-4,\n    weight_decay=1e-3,\n    patch_size=10,\n    n_heads=3,\n    head_dim=8,\n    num_layers=2,\n    mlp_ratio=2,\n    dropout=0.1,\n    attn_dropout=0.0,\n    augment_noise_std=0.01,\n    augment_scale_range=(0.9, 1.1),\n    augment_scale_low=None,\n    augment_scale_high=None,\n    use_focal_loss=False,\n    focal_gamma=2.0,\n    class_weighted=True,\n    grad_clip=1.0,\n    quantization_bits=8,\n    quantize_weights=True,\n    quantize_activations=True\n):\n    \"\"\"\n    Train a compact 1D Tiny-Transformer on ECG (MIT-BIH) and return a post-training quantized model.\n\n    Inputs:\n      - X_*: torch.Tensor of shape (N, 1000, 2) or (N, 2, 1000)\n      - y_*: torch.LongTensor labels in [0..4]\n      - device: string or torch.device (always training on GPU). Example: \"cuda\".\n\n    Notes:\n      - augment_scale_low / augment_scale_high are accepted for compatibility with external BO configs.\n        If provided, they override values from augment_scale_range (missing bound falls back to the tuple).\n\n    Returns:\n      quantized_model (on CPU), metrics_dict with lists: train_losses, val_losses, val_acc, and size_bytes.\n    \"\"\"\n    import io\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader\n\n    # Robust device handling\n    device = torch.device(device)\n    if device.type != 'cuda' or not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA device is required for training. Please pass device='cuda' and ensure a GPU is available.\")\n\n    # Validate dimensions\n    if X_train.dim() != 3 or X_val.dim() != 3:\n        raise ValueError(\"X_train and X_val must be 3-D tensors\")\n\n    # Resolve augmentation scale range with compatibility for separate low/high args\n    if augment_scale_low is not None or augment_scale_high is not None:\n        # Use provided bound(s), falling back to tuple for any missing side\n        base_low, base_high = augment_scale_range if augment_scale_range is not None else (0.9, 1.1)\n        scale_low = float(augment_scale_low) if augment_scale_low is not None else float(base_low)\n        scale_high = float(augment_scale_high) if augment_scale_high is not None else float(base_high)\n    else:\n        scale_low = float(augment_scale_range[0])\n        scale_high = float(augment_scale_range[1])\n    if scale_high < scale_low:\n        scale_low, scale_high = scale_high, scale_low\n\n    # Local imports for dataset and model dependencies from outer scope\n    # Expecting _ECGDataset, Tiny1DTransformer, FocalLoss, and _approx_model_size_bytes defined in module scope\n\n    # Dataset and loaders (pin_memory=False as requested)\n    train_ds = _ECGDataset(X_train, y_train, train=True, noise_std=augment_noise_std, scale_low=scale_low, scale_high=scale_high)\n    val_ds = _ECGDataset(X_val, y_val, train=False, noise_std=0.0, scale_low=1.0, scale_high=1.0)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n\n    # Hyper-parameters derived\n    d_model = int(head_dim) * int(n_heads)\n    if 1000 % int(patch_size) != 0:\n        raise ValueError(\"patch_size must divide the sequence length (1000)\")\n\n    # Build model\n    model = Tiny1DTransformer(\n        in_ch=2,\n        seq_len=1000,\n        patch_size=int(patch_size),\n        d_model=int(d_model),\n        n_heads=int(n_heads),\n        num_layers=int(num_layers),\n        mlp_ratio=int(mlp_ratio),\n        dropout=float(dropout),\n        attn_dropout=float(attn_dropout),\n        num_classes=5,\n    ).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n\n    # Class weights\n    if class_weighted:\n        with torch.no_grad():\n            binc = torch.bincount(y_train.long().cpu(), minlength=5).float()\n            inv = 1.0 / torch.clamp(binc, min=1.0)\n            weights = inv / inv.sum() * 5.0\n        class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=float(focal_gamma), weight=class_weights, reduction='mean')\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(int(epochs)):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n        train_loss = running_loss / max(1, total)\n        train_losses.append(train_loss)\n\n        # Validation\n        model.eval()\n        v_running = 0.0\n        correct = 0\n        v_total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                vloss = criterion(logits, yb)\n                v_running += vloss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n        val_loss = v_running / max(1, v_total)\n        val_acc = correct / max(1, v_total)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch+1}/{int(epochs)} - train_loss: {train_loss:.4f}  val_loss: {val_loss:.4f}  val_acc: {val_acc:.4f}\")\n\n    # Move to CPU for quantization\n    model = model.to('cpu')\n    quantized_model = model\n\n    # Post-Training Quantization strategy\n    if quantize_weights:\n        if int(quantization_bits) == 8:\n            try:\n                from torch.ao.quantization import quantize_dynamic\n            except Exception:\n                from torch.quantization import quantize_dynamic  # backward compat\n            quantized_model = quantize_dynamic(\n                model,\n                {nn.Linear},\n                dtype=torch.qint8,\n            )\n        elif int(quantization_bits) == 16:\n            quantized_model = model.half()\n        else:\n            quantized_model = model  # 32-bit\n    else:\n        quantized_model = model\n\n    # Size check and fallback to 8-bit dynamic if needed to meet <= 256KB\n    def _approx_model_size_bytes(model_):\n        buf = io.BytesIO()\n        try:\n            torch.save(model_.state_dict(), buf)\n            return len(buf.getvalue())\n        except Exception:\n            total_ = 0\n            for p in model_.parameters():\n                try:\n                    total_ += p.numel() * p.element_size()\n                except Exception:\n                    pass\n            return total_\n\n    size_bytes = _approx_model_size_bytes(quantized_model)\n    limit = 256 * 1024\n    if size_bytes > limit:\n        try:\n            from torch.ao.quantization import quantize_dynamic\n        except Exception:\n            from torch.quantization import quantize_dynamic\n        quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n        size_bytes = _approx_model_size_bytes(quantized_model)\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'size_bytes': int(size_bytes)\n    }\n\n    return quantized_model, metrics\n"}
2025-10-02 19:18:00,724 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-02 19:18:00,724 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code":"def train_model(\n    X_train, y_train, X_val, y_val, device,\n    epochs=20,\n    batch_size=256,\n    lr=3e-4,\n    weight_decay=1e-3,\n    patch_size=10,\n    n_heads=3,\n    head_dim=8,\n    num_layers=2,\n    mlp_ratio=2,\n    dropout=0.1,\n    attn_dropout=0.0,\n    augment_noise_std=0.01,\n    augment_scale_range=(0.9, 1.1),\n    augment_scale_low=None,\n    augment_scale_high=None,\n    use_focal_loss=False,\n    focal_gamma=2.0,\n    class_weighted=True,\n    grad_clip=1.0,\n    quantization_bits=8,\n    quantize_weights=True,\n    quantize_activations=True\n):\n    \"\"\"\n    Train a compact 1D Tiny-Transformer on ECG (MIT-BIH) and return a post-training quantized model.\n\n    Inputs:\n      - X_*: torch.Tensor of shape (N, 1000, 2) or (N, 2, 1000)\n      - y_*: torch.LongTensor labels in [0..4]\n      - device: string or torch.device (always training on GPU). Example: \"cuda\".\n\n    Notes:\n      - augment_scale_low / augment_scale_high are accepted for compatibility with external BO configs.\n        If provided, they override values from augment_scale_range (missing bound falls back to the tuple).\n\n    Returns:\n      quantized_model (on CPU), metrics_dict with lists: train_losses, val_losses, val_acc, and size_bytes.\n    \"\"\"\n    import io\n    import math\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader\n\n    # Robust device handling\n    device = torch.device(device)\n    if device.type != 'cuda' or not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA device is required for training. Please pass device='cuda' and ensure a GPU is available.\")\n\n    # Validate dimensions\n    if X_train.dim() != 3 or X_val.dim() != 3:\n        raise ValueError(\"X_train and X_val must be 3-D tensors\")\n\n    # Resolve augmentation scale range with compatibility for separate low/high args\n    if augment_scale_low is not None or augment_scale_high is not None:\n        # Use provided bound(s), falling back to tuple for any missing side\n        base_low, base_high = augment_scale_range if augment_scale_range is not None else (0.9, 1.1)\n        scale_low = float(augment_scale_low) if augment_scale_low is not None else float(base_low)\n        scale_high = float(augment_scale_high) if augment_scale_high is not None else float(base_high)\n    else:\n        scale_low = float(augment_scale_range[0])\n        scale_high = float(augment_scale_range[1])\n    if scale_high < scale_low:\n        scale_low, scale_high = scale_high, scale_low\n\n    # Local imports for dataset and model dependencies from outer scope\n    # Expecting _ECGDataset, Tiny1DTransformer, FocalLoss, and _approx_model_size_bytes defined in module scope\n\n    # Dataset and loaders (pin_memory=False as requested)\n    train_ds = _ECGDataset(X_train, y_train, train=True, noise_std=augment_noise_std, scale_low=scale_low, scale_high=scale_high)\n    val_ds = _ECGDataset(X_val, y_val, train=False, noise_std=0.0, scale_low=1.0, scale_high=1.0)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n\n    # Hyper-parameters derived\n    d_model = int(head_dim) * int(n_heads)\n    if 1000 % int(patch_size) != 0:\n        raise ValueError(\"patch_size must divide the sequence length (1000)\")\n\n    # Build model\n    model = Tiny1DTransformer(\n        in_ch=2,\n        seq_len=1000,\n        patch_size=int(patch_size),\n        d_model=int(d_model),\n        n_heads=int(n_heads),\n        num_layers=int(num_layers),\n        mlp_ratio=int(mlp_ratio),\n        dropout=float(dropout),\n        attn_dropout=float(attn_dropout),\n        num_classes=5,\n    ).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n\n    # Class weights\n    if class_weighted:\n        with torch.no_grad():\n            binc = torch.bincount(y_train.long().cpu(), minlength=5).float()\n            inv = 1.0 / torch.clamp(binc, min=1.0)\n            weights = inv / inv.sum() * 5.0\n        class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=float(focal_gamma), weight=class_weights, reduction='mean')\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(int(epochs)):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n        train_loss = running_loss / max(1, total)\n        train_losses.append(train_loss)\n\n        # Validation\n        model.eval()\n        v_running = 0.0\n        correct = 0\n        v_total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                vloss = criterion(logits, yb)\n                v_running += vloss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n        val_loss = v_running / max(1, v_total)\n        val_acc = correct / max(1, v_total)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch+1}/{int(epochs)} - train_loss: {train_loss:.4f}  val_loss: {val_loss:.4f}  val_acc: {val_acc:.4f}\")\n\n    # Move to CPU for quantization\n    model = model.to('cpu')\n    quantized_model = model\n\n    # Post-Training Quantization strategy\n    if quantize_weights:\n        if int(quantization_bits) == 8:\n            try:\n                from torch.ao.quantization import quantize_dynamic\n            except Exception:\n                from torch.quantization import quantize_dynamic  # backward compat\n            quantized_model = quantize_dynamic(\n                model,\n                {nn.Linear},\n                dtype=torch.qint8,\n            )\n        elif int(quantization_bits) == 16:\n            quantized_model = model.half()\n        else:\n            quantized_model = model  # 32-bit\n    else:\n        quantized_model = model\n\n    # Size check and fallback to 8-bit dynamic if needed to meet <= 256KB\n    def _approx_model_size_bytes(model_):\n        buf = io.BytesIO()\n        try:\n            torch.save(model_.state_dict(), buf)\n            return len(buf.getvalue())\n        except Exception:\n            total_ = 0\n            for p in model_.parameters():\n                try:\n                    total_ += p.numel() * p.element_size()\n                except Exception:\n                    pass\n            return total_\n\n    size_bytes = _approx_model_size_bytes(quantized_model)\n    limit = 256 * 1024\n    if size_bytes > limit:\n        try:\n            from torch.ao.quantization import quantize_dynamic\n        except Exception:\n            from torch.quantization import quantize_dynamic\n        quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n        size_bytes = _approx_model_size_bytes(quantized_model)\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'size_bytes': int(size_bytes)\n    }\n\n    return quantized_model, metrics\n"}
2025-10-02 19:18:00,724 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-02 19:18:00,724 - ERROR - _models.training_function_executor - BO training objective failed: train_model() got an unexpected keyword argument 'augment_scale_low'
2025-10-02 19:18:00,724 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 78.464s
2025-10-02 19:18:00,724 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: train_model() got an unexpected keyword argument 'augment_scale_low'
2025-10-02 19:18:00,724 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-10-02 19:18:03,727 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-10-02 19:18:03,727 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-10-02 19:18:03,727 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_Tiny1DTransformer-1D__Busia_et_al._2024,_adapted_for__1000,2_1759450602.json
2025-10-02 19:18:03,727 - INFO - _models.training_function_executor - Loaded training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:18:03,727 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-02 19:18:03,727 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-10-02 19:18:03,728 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_Tiny1DTransformer-1D__Busia_et_al._2024,_adapted_for__1000,2_1759450602.json
2025-10-02 19:18:03,728 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:18:03,728 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-10-02 19:18:03,728 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 1/4
2025-10-02 19:18:03,728 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 1: üì¶ Installing dependencies for GPT-generated training code...
2025-10-02 19:18:03,728 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-02 19:18:03,729 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-02 19:18:03,729 - INFO - package_installer - Available packages: {'torch'}
2025-10-02 19:18:03,729 - INFO - package_installer - Missing packages: set()
2025-10-02 19:18:03,729 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-02 19:18:03,729 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-02 19:18:03,729 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 19:18:03,729 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-02 19:18:03,729 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['epochs', 'batch_size', 'lr', 'weight_decay', 'patch_size', 'n_heads', 'head_dim', 'num_layers', 'mlp_ratio', 'dropout', 'attn_dropout', 'augment_noise_std', 'augment_scale_low', 'augment_scale_high', 'use_focal_loss', 'focal_gamma', 'class_weighted', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-10-02 19:18:03,729 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 19:18:03,729 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 19:18:03,729 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-02 19:18:03,762 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-02 19:18:03,795 - INFO - bo.run_bo - Converted GPT search space: 21 parameters
2025-10-02 19:18:03,795 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-02 19:18:03,795 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-02 19:18:03,796 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-02 19:18:03,796 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 19:18:03,796 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:18:03,796 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:18:03,796 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 56, 'batch_size': 512, 'lr': 0.0015702970884055383, 'weight_decay': 0.0002481040974867812, 'patch_size': 10, 'n_heads': 3, 'head_dim': 26, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4330880728874677, 'attn_dropout': 0.18033450352296268, 'augment_noise_std': 0.03540362888980228, 'augment_scale_low': 0.8041168988591605, 'augment_scale_high': 1.1939819704323988, 'use_focal_loss': False, 'focal_gamma': 1.8493564427131048, 'class_weighted': True, 'grad_clip': 0.9170225492671692, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:18:03,796 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 56, 'batch_size': 512, 'lr': 0.0015702970884055383, 'weight_decay': 0.0002481040974867812, 'patch_size': 10, 'n_heads': 3, 'head_dim': 26, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4330880728874677, 'attn_dropout': 0.18033450352296268, 'augment_noise_std': 0.03540362888980228, 'augment_scale_low': 0.8041168988591605, 'augment_scale_high': 1.1939819704323988, 'use_focal_loss': False, 'focal_gamma': 1.8493564427131048, 'class_weighted': True, 'grad_clip': 0.9170225492671692, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:18:03,797 - ERROR - _models.training_function_executor - Training execution failed: name '_ECGDataset' is not defined
2025-10-02 19:18:03,797 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-02 19:18:03,797 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-02 19:18:03,797 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-02 19:18:03,797 - INFO - _models.ai_code_generator - Prompt length: 11819 characters
2025-10-02 19:18:03,797 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-02 19:18:03,797 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-02 19:18:03,797 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-02 19:19:34,396 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-02 19:19:34,397 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-02 19:19:34,397 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251002_191934_attempt1.txt
2025-10-02 19:19:34,397 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(\n    X_train, y_train, X_val, y_val, device,\n    epochs=20,\n    batch_size=256,\n    lr=3e-4,\n    weight_decay=1e-3,\n    patch_size=10,\n    n_heads=3,\n    head_dim=8,\n    num_layers=2,\n    mlp_ratio=2,\n    dropout=0.1,\n    attn_dropout=0.0,\n    augment_noise_std=0.01,\n    augment_scale_range=(0.9, 1.1),\n    augment_scale_low=None,\n    augment_scale_high=None,\n    use_focal_loss=False,\n    focal_gamma=2.0,\n    class_weighted=True,\n    grad_clip=1.0,\n    quantization_bits=8,\n    quantize_weights=True,\n    quantize_activations=True\n):\n    \"\"\"\n    Train a compact 1D Tiny-Transformer on ECG (MIT-BIH) and return a post-training quantized model.\n\n    Inputs:\n      - X_*: torch.Tensor of shape (N, 1000, 2) or (N, 2, 1000)\n      - y_*: torch.LongTensor labels in [0..4]\n      - device: string or torch.device (always training on GPU). Example: \"cuda\".\n\n    Notes:\n      - augment_scale_low / augment_scale_high are accepted for compatibility with external BO configs.\n        If provided, they override values from augment_scale_range (missing bound falls back to the tuple).\n\n    Returns:\n      quantized_model (on CPU), metrics_dict with lists: train_losses, val_losses, val_acc, and size_bytes.\n    \"\"\"\n    import io\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n\n    # -------------------------------\n    # Local helper implementations\n    # -------------------------------\n    class _ECGDataset(Dataset):\n        def __init__(self, X, y, train=True, noise_std=0.0, scale_low=1.0, scale_high=1.0):\n            self.X = X\n            self.y = y\n            self.train = train\n            self.noise_std = float(noise_std)\n            self.scale_low = float(scale_low)\n            self.scale_high = float(scale_high)\n\n        def __len__(self):\n            return int(self.X.shape[0])\n\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            y = self.y[idx].long()\n            # Ensure channel-first shape (2, 1000)\n            if x.dim() != 2:\n                raise ValueError(\"Each sample in X must be 2-D: (1000,2) or (2,1000)\")\n            if x.shape == (1000, 2):\n                x = x.transpose(0, 1)\n            elif x.shape == (2, 1000):\n                pass\n            else:\n                # Try to coerce: if last dim is 1000 and first is 2\n                if x.shape[-1] == 1000 and x.shape[0] == 2:\n                    x = x\n                elif x.shape[0] == 1000 and x.shape[-1] == 2:\n                    x = x.transpose(0, 1)\n                else:\n                    raise ValueError(\"Unexpected sample shape in X: expected (1000,2) or (2,1000)\")\n            x = x.float()\n\n            if self.train:\n                # Amplitude scaling\n                if not (self.scale_low == 1.0 and self.scale_high == 1.0):\n                    scale = torch.empty(1).uniform_(self.scale_low, self.scale_high).item()\n                    x = x * scale\n                # Additive Gaussian noise\n                if self.noise_std > 0.0:\n                    noise = torch.randn_like(x) * self.noise_std\n                    x = x + noise\n\n            return x, y\n\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n            super().__init__()\n            self.gamma = float(gamma)\n            self.reduction = reduction\n            self.weight = weight  # tensor or None\n\n        def forward(self, logits, target):\n            logpt = F.log_softmax(logits, dim=1)\n            pt = logpt.exp()\n            logpt_t = logpt.gather(1, target.unsqueeze(1)).squeeze(1)\n            pt_t = pt.gather(1, target.unsqueeze(1)).squeeze(1)\n            if self.weight is not None:\n                alpha_t = self.weight.gather(0, target)\n            else:\n                alpha_t = 1.0\n            loss = -alpha_t * ((1.0 - pt_t) ** self.gamma) * logpt_t\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            else:\n                return loss\n\n    class Tiny1DTransformer(nn.Module):\n        def __init__(self, in_ch=2, seq_len=1000, patch_size=10, d_model=64, n_heads=2, num_layers=2, mlp_ratio=2, dropout=0.1, attn_dropout=0.0, num_classes=5):\n            super().__init__()\n            assert seq_len % patch_size == 0, \"patch_size must divide the sequence length\"\n            self.seq_len = int(seq_len)\n            self.patch_size = int(patch_size)\n            self.n_patches = self.seq_len // self.patch_size\n            self.d_model = int(d_model)\n\n            # Patch embedding: Conv1d with stride=patch_size to create tokens\n            self.patch_embed = nn.Conv1d(in_ch, self.d_model, kernel_size=self.patch_size, stride=self.patch_size, bias=False)\n\n            # Positional embedding\n            self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches, self.d_model))\n\n            # Transformer encoder\n            ff_dim = int(self.d_model * int(mlp_ratio))\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=self.d_model,\n                nhead=int(n_heads),\n                dim_feedforward=ff_dim,\n                dropout=float(dropout),\n                activation='gelu',\n                batch_first=True,\n            )\n            # Set separate attention dropout if requested\n            try:\n                encoder_layer.self_attn.dropout = float(attn_dropout)\n            except Exception:\n                pass\n            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=int(num_layers))\n\n            self.norm = nn.LayerNorm(self.d_model)\n            self.head = nn.Linear(self.d_model, int(num_classes))\n\n        def forward(self, x):\n            # x: (B, 2, 1000)\n            x = self.patch_embed(x)  # (B, d_model, n_patches)\n            x = x.transpose(1, 2)    # (B, n_patches, d_model)\n            x = x + self.pos_embed\n            x = self.encoder(x)      # (B, n_patches, d_model)\n            x = self.norm(x)\n            x = x.mean(dim=1)        # global average pooling over tokens\n            logits = self.head(x)\n            return logits\n\n    # -------------------------------\n    # Robust device handling\n    # -------------------------------\n    device = torch.device(device)\n    if device.type != 'cuda' or not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA device is required for training. Please pass device='cuda' and ensure a GPU is available.\")\n\n    # Validate dimensions\n    if X_train.dim() != 3 or X_val.dim() != 3:\n        raise ValueError(\"X_train and X_val must be 3-D tensors\")\n\n    # Resolve augmentation scale range with compatibility for separate low/high args\n    if augment_scale_low is not None or augment_scale_high is not None:\n        base_low, base_high = augment_scale_range if augment_scale_range is not None else (0.9, 1.1)\n        scale_low = float(augment_scale_low) if augment_scale_low is not None else float(base_low)\n        scale_high = float(augment_scale_high) if augment_scale_high is not None else float(base_high)\n    else:\n        scale_low = float(augment_scale_range[0])\n        scale_high = float(augment_scale_range[1])\n    if scale_high < scale_low:\n        scale_low, scale_high = scale_high, scale_low\n\n    # -------------------------------\n    # Dataset and loaders\n    # -------------------------------\n    train_ds = _ECGDataset(X_train, y_train, train=True, noise_std=augment_noise_std, scale_low=scale_low, scale_high=scale_high)\n    val_ds = _ECGDataset(X_val, y_val, train=False, noise_std=0.0, scale_low=1.0, scale_high=1.0)\n    train_loader = DataLoader(train_ds, batch_size=int(batch_size), shuffle=True, num_workers=0, pin_memory=False, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=int(batch_size), shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n\n    # Hyper-parameters derived\n    d_model = int(head_dim) * int(n_heads)\n    if 1000 % int(patch_size) != 0:\n        raise ValueError(\"patch_size must divide the sequence length (1000)\")\n\n    # Build model\n    model = Tiny1DTransformer(\n        in_ch=2,\n        seq_len=1000,\n        patch_size=int(patch_size),\n        d_model=int(d_model),\n        n_heads=int(n_heads),\n        num_layers=int(num_layers),\n        mlp_ratio=int(mlp_ratio),\n        dropout=float(dropout),\n        attn_dropout=float(attn_dropout),\n        num_classes=5,\n    ).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n\n    # Class weights\n    if class_weighted:\n        with torch.no_grad():\n            binc = torch.bincount(y_train.long().cpu(), minlength=5).float()\n            inv = 1.0 / torch.clamp(binc, min=1.0)\n            weights = inv / inv.sum() * 5.0\n        class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=float(focal_gamma), weight=class_weights, reduction='mean')\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(int(epochs)):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n        train_loss = running_loss / max(1, total)\n        train_losses.append(train_loss)\n\n        # Validation\n        model.eval()\n        v_running = 0.0\n        correct = 0\n        v_total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                vloss = criterion(logits, yb)\n                v_running += vloss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n        val_loss = v_running / max(1, v_total)\n        val_acc = correct / max(1, v_total)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch+1}/{int(epochs)} - train_loss: {train_loss:.4f}  val_loss: {val_loss:.4f}  val_acc: {val_acc:.4f}\")\n\n    # Move to CPU for quantization\n    model = model.to('cpu')\n    quantized_model = model\n\n    # Post-Training Quantization strategy\n    if quantize_weights:\n        if int(quantization_bits) == 8:\n            try:\n                from torch.ao.quantization import quantize_dynamic\n            except Exception:\n                from torch.quantization import quantize_dynamic  # backward compat\n            quantized_model = quantize_dynamic(\n                model,\n                {nn.Linear},\n                dtype=torch.qint8,\n            )\n        elif int(quantization_bits) == 16:\n            quantized_model = model.half()\n        else:\n            quantized_model = model  # 32-bit\n    else:\n        quantized_model = model\n\n    # Size check and fallback to 8-bit dynamic if needed to meet <= 256KB\n    def _approx_model_size_bytes(model_):\n        buf = io.BytesIO()\n        try:\n            torch.save(model_.state_dict(), buf)\n            return len(buf.getvalue())\n        except Exception:\n            total_ = 0\n            for p in model_.parameters():\n                try:\n                    total_ += p.numel() * p.element_size()\n                except Exception:\n                    pass\n            return total_\n\n    size_bytes = _approx_model_size_bytes(quantized_model)\n    limit = 256 * 1024\n    if size_bytes > limit:\n        try:\n            from torch.ao.quantization import quantize_dynamic\n        except Exception:\n            from torch.quantization import quantize_dynamic\n        quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n        size_bytes = _approx_model_size_bytes(quantized_model)\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'size_bytes': int(size_bytes)\n    }\n\n    return quantized_model, metrics\n"}
2025-10-02 19:19:34,397 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-02 19:19:34,397 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(\n    X_train, y_train, X_val, y_val, device,\n    epochs=20,\n    batch_size=256,\n    lr=3e-4,\n    weight_decay=1e-3,\n    patch_size=10,\n    n_heads=3,\n    head_dim=8,\n    num_layers=2,\n    mlp_ratio=2,\n    dropout=0.1,\n    attn_dropout=0.0,\n    augment_noise_std=0.01,\n    augment_scale_range=(0.9, 1.1),\n    augment_scale_low=None,\n    augment_scale_high=None,\n    use_focal_loss=False,\n    focal_gamma=2.0,\n    class_weighted=True,\n    grad_clip=1.0,\n    quantization_bits=8,\n    quantize_weights=True,\n    quantize_activations=True\n):\n    \"\"\"\n    Train a compact 1D Tiny-Transformer on ECG (MIT-BIH) and return a post-training quantized model.\n\n    Inputs:\n      - X_*: torch.Tensor of shape (N, 1000, 2) or (N, 2, 1000)\n      - y_*: torch.LongTensor labels in [0..4]\n      - device: string or torch.device (always training on GPU). Example: \"cuda\".\n\n    Notes:\n      - augment_scale_low / augment_scale_high are accepted for compatibility with external BO configs.\n        If provided, they override values from augment_scale_range (missing bound falls back to the tuple).\n\n    Returns:\n      quantized_model (on CPU), metrics_dict with lists: train_losses, val_losses, val_acc, and size_bytes.\n    \"\"\"\n    import io\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import DataLoader, Dataset\n\n    # -------------------------------\n    # Local helper implementations\n    # -------------------------------\n    class _ECGDataset(Dataset):\n        def __init__(self, X, y, train=True, noise_std=0.0, scale_low=1.0, scale_high=1.0):\n            self.X = X\n            self.y = y\n            self.train = train\n            self.noise_std = float(noise_std)\n            self.scale_low = float(scale_low)\n            self.scale_high = float(scale_high)\n\n        def __len__(self):\n            return int(self.X.shape[0])\n\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            y = self.y[idx].long()\n            # Ensure channel-first shape (2, 1000)\n            if x.dim() != 2:\n                raise ValueError(\"Each sample in X must be 2-D: (1000,2) or (2,1000)\")\n            if x.shape == (1000, 2):\n                x = x.transpose(0, 1)\n            elif x.shape == (2, 1000):\n                pass\n            else:\n                # Try to coerce: if last dim is 1000 and first is 2\n                if x.shape[-1] == 1000 and x.shape[0] == 2:\n                    x = x\n                elif x.shape[0] == 1000 and x.shape[-1] == 2:\n                    x = x.transpose(0, 1)\n                else:\n                    raise ValueError(\"Unexpected sample shape in X: expected (1000,2) or (2,1000)\")\n            x = x.float()\n\n            if self.train:\n                # Amplitude scaling\n                if not (self.scale_low == 1.0 and self.scale_high == 1.0):\n                    scale = torch.empty(1).uniform_(self.scale_low, self.scale_high).item()\n                    x = x * scale\n                # Additive Gaussian noise\n                if self.noise_std > 0.0:\n                    noise = torch.randn_like(x) * self.noise_std\n                    x = x + noise\n\n            return x, y\n\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n            super().__init__()\n            self.gamma = float(gamma)\n            self.reduction = reduction\n            self.weight = weight  # tensor or None\n\n        def forward(self, logits, target):\n            logpt = F.log_softmax(logits, dim=1)\n            pt = logpt.exp()\n            logpt_t = logpt.gather(1, target.unsqueeze(1)).squeeze(1)\n            pt_t = pt.gather(1, target.unsqueeze(1)).squeeze(1)\n            if self.weight is not None:\n                alpha_t = self.weight.gather(0, target)\n            else:\n                alpha_t = 1.0\n            loss = -alpha_t * ((1.0 - pt_t) ** self.gamma) * logpt_t\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            else:\n                return loss\n\n    class Tiny1DTransformer(nn.Module):\n        def __init__(self, in_ch=2, seq_len=1000, patch_size=10, d_model=64, n_heads=2, num_layers=2, mlp_ratio=2, dropout=0.1, attn_dropout=0.0, num_classes=5):\n            super().__init__()\n            assert seq_len % patch_size == 0, \"patch_size must divide the sequence length\"\n            self.seq_len = int(seq_len)\n            self.patch_size = int(patch_size)\n            self.n_patches = self.seq_len // self.patch_size\n            self.d_model = int(d_model)\n\n            # Patch embedding: Conv1d with stride=patch_size to create tokens\n            self.patch_embed = nn.Conv1d(in_ch, self.d_model, kernel_size=self.patch_size, stride=self.patch_size, bias=False)\n\n            # Positional embedding\n            self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches, self.d_model))\n\n            # Transformer encoder\n            ff_dim = int(self.d_model * int(mlp_ratio))\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=self.d_model,\n                nhead=int(n_heads),\n                dim_feedforward=ff_dim,\n                dropout=float(dropout),\n                activation='gelu',\n                batch_first=True,\n            )\n            # Set separate attention dropout if requested\n            try:\n                encoder_layer.self_attn.dropout = float(attn_dropout)\n            except Exception:\n                pass\n            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=int(num_layers))\n\n            self.norm = nn.LayerNorm(self.d_model)\n            self.head = nn.Linear(self.d_model, int(num_classes))\n\n        def forward(self, x):\n            # x: (B, 2, 1000)\n            x = self.patch_embed(x)  # (B, d_model, n_patches)\n            x = x.transpose(1, 2)    # (B, n_patches, d_model)\n            x = x + self.pos_embed\n            x = self.encoder(x)      # (B, n_patches, d_model)\n            x = self.norm(x)\n            x = x.mean(dim=1)        # global average pooling over tokens\n            logits = self.head(x)\n            return logits\n\n    # -------------------------------\n    # Robust device handling\n    # -------------------------------\n    device = torch.device(device)\n    if device.type != 'cuda' or not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA device is required for training. Please pass device='cuda' and ensure a GPU is available.\")\n\n    # Validate dimensions\n    if X_train.dim() != 3 or X_val.dim() != 3:\n        raise ValueError(\"X_train and X_val must be 3-D tensors\")\n\n    # Resolve augmentation scale range with compatibility for separate low/high args\n    if augment_scale_low is not None or augment_scale_high is not None:\n        base_low, base_high = augment_scale_range if augment_scale_range is not None else (0.9, 1.1)\n        scale_low = float(augment_scale_low) if augment_scale_low is not None else float(base_low)\n        scale_high = float(augment_scale_high) if augment_scale_high is not None else float(base_high)\n    else:\n        scale_low = float(augment_scale_range[0])\n        scale_high = float(augment_scale_range[1])\n    if scale_high < scale_low:\n        scale_low, scale_high = scale_high, scale_low\n\n    # -------------------------------\n    # Dataset and loaders\n    # -------------------------------\n    train_ds = _ECGDataset(X_train, y_train, train=True, noise_std=augment_noise_std, scale_low=scale_low, scale_high=scale_high)\n    val_ds = _ECGDataset(X_val, y_val, train=False, noise_std=0.0, scale_low=1.0, scale_high=1.0)\n    train_loader = DataLoader(train_ds, batch_size=int(batch_size), shuffle=True, num_workers=0, pin_memory=False, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=int(batch_size), shuffle=False, num_workers=0, pin_memory=False, drop_last=False)\n\n    # Hyper-parameters derived\n    d_model = int(head_dim) * int(n_heads)\n    if 1000 % int(patch_size) != 0:\n        raise ValueError(\"patch_size must divide the sequence length (1000)\")\n\n    # Build model\n    model = Tiny1DTransformer(\n        in_ch=2,\n        seq_len=1000,\n        patch_size=int(patch_size),\n        d_model=int(d_model),\n        n_heads=int(n_heads),\n        num_layers=int(num_layers),\n        mlp_ratio=int(mlp_ratio),\n        dropout=float(dropout),\n        attn_dropout=float(attn_dropout),\n        num_classes=5,\n    ).to(device)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n\n    # Class weights\n    if class_weighted:\n        with torch.no_grad():\n            binc = torch.bincount(y_train.long().cpu(), minlength=5).float()\n            inv = 1.0 / torch.clamp(binc, min=1.0)\n            weights = inv / inv.sum() * 5.0\n        class_weights = weights.to(device)\n    else:\n        class_weights = None\n\n    # Loss\n    if use_focal_loss:\n        criterion = FocalLoss(gamma=float(focal_gamma), weight=class_weights, reduction='mean')\n    else:\n        criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n    train_losses = []\n    val_losses = []\n    val_accs = []\n\n    for epoch in range(int(epochs)):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False)\n            yb = yb.to(device, non_blocking=False)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if grad_clip and grad_clip > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n            optimizer.step()\n            running_loss += loss.detach().item() * xb.size(0)\n            total += xb.size(0)\n        train_loss = running_loss / max(1, total)\n        train_losses.append(train_loss)\n\n        # Validation\n        model.eval()\n        v_running = 0.0\n        correct = 0\n        v_total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False)\n                yb = yb.to(device, non_blocking=False)\n                logits = model(xb)\n                vloss = criterion(logits, yb)\n                v_running += vloss.detach().item() * xb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                v_total += xb.size(0)\n        val_loss = v_running / max(1, v_total)\n        val_acc = correct / max(1, v_total)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch+1}/{int(epochs)} - train_loss: {train_loss:.4f}  val_loss: {val_loss:.4f}  val_acc: {val_acc:.4f}\")\n\n    # Move to CPU for quantization\n    model = model.to('cpu')\n    quantized_model = model\n\n    # Post-Training Quantization strategy\n    if quantize_weights:\n        if int(quantization_bits) == 8:\n            try:\n                from torch.ao.quantization import quantize_dynamic\n            except Exception:\n                from torch.quantization import quantize_dynamic  # backward compat\n            quantized_model = quantize_dynamic(\n                model,\n                {nn.Linear},\n                dtype=torch.qint8,\n            )\n        elif int(quantization_bits) == 16:\n            quantized_model = model.half()\n        else:\n            quantized_model = model  # 32-bit\n    else:\n        quantized_model = model\n\n    # Size check and fallback to 8-bit dynamic if needed to meet <= 256KB\n    def _approx_model_size_bytes(model_):\n        buf = io.BytesIO()\n        try:\n            torch.save(model_.state_dict(), buf)\n            return len(buf.getvalue())\n        except Exception:\n            total_ = 0\n            for p in model_.parameters():\n                try:\n                    total_ += p.numel() * p.element_size()\n                except Exception:\n                    pass\n            return total_\n\n    size_bytes = _approx_model_size_bytes(quantized_model)\n    limit = 256 * 1024\n    if size_bytes > limit:\n        try:\n            from torch.ao.quantization import quantize_dynamic\n        except Exception:\n            from torch.quantization import quantize_dynamic\n        quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n        size_bytes = _approx_model_size_bytes(quantized_model)\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'size_bytes': int(size_bytes)\n    }\n\n    return quantized_model, metrics\n"}
2025-10-02 19:19:34,397 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-02 19:19:34,397 - ERROR - _models.training_function_executor - BO training objective failed: name '_ECGDataset' is not defined
2025-10-02 19:19:34,397 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 90.602s
2025-10-02 19:19:34,398 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: name '_ECGDataset' is not defined
2025-10-02 19:19:34,398 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-10-02 19:19:37,400 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-10-02 19:19:37,400 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-10-02 19:19:37,400 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_Tiny1DTransformer-1D__Busia_et_al._2024,_adapted_for__1000,2_1759450602.json
2025-10-02 19:19:37,400 - INFO - _models.training_function_executor - Loaded training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:19:37,401 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-02 19:19:37,401 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-10-02 19:19:37,401 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_Tiny1DTransformer-1D__Busia_et_al._2024,_adapted_for__1000,2_1759450602.json
2025-10-02 19:19:37,401 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:19:37,401 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-10-02 19:19:37,401 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 2/4
2025-10-02 19:19:37,401 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 2: üì¶ Installing dependencies for GPT-generated training code...
2025-10-02 19:19:37,401 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-02 19:19:37,403 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-02 19:19:37,403 - INFO - package_installer - Available packages: {'torch'}
2025-10-02 19:19:37,403 - INFO - package_installer - Missing packages: set()
2025-10-02 19:19:37,403 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-02 19:19:37,403 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-02 19:19:37,403 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 19:19:37,403 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-10-02 19:19:37,403 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['epochs', 'batch_size', 'lr', 'weight_decay', 'patch_size', 'n_heads', 'head_dim', 'num_layers', 'mlp_ratio', 'dropout', 'attn_dropout', 'augment_noise_std', 'augment_scale_low', 'augment_scale_high', 'use_focal_loss', 'focal_gamma', 'class_weighted', 'grad_clip', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-10-02 19:19:37,403 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-02 19:19:37,403 - INFO - data_splitting - Using all 39904 training samples for BO
2025-10-02 19:19:37,403 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-10-02 19:19:37,435 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-10-02 19:19:37,469 - INFO - bo.run_bo - Converted GPT search space: 21 parameters
2025-10-02 19:19:37,469 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-02 19:19:37,470 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-02 19:19:37,471 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-02 19:19:37,471 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 19:19:37,471 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:19:37,471 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:19:37,471 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 56, 'batch_size': 512, 'lr': 0.0015702970884055383, 'weight_decay': 0.0002481040974867812, 'patch_size': 10, 'n_heads': 3, 'head_dim': 26, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4330880728874677, 'attn_dropout': 0.18033450352296268, 'augment_noise_std': 0.03540362888980228, 'augment_scale_low': 0.8041168988591605, 'augment_scale_high': 1.1939819704323988, 'use_focal_loss': False, 'focal_gamma': 1.8493564427131048, 'class_weighted': True, 'grad_clip': 0.9170225492671692, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:19:37,472 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 56, 'batch_size': 512, 'lr': 0.0015702970884055383, 'weight_decay': 0.0002481040974867812, 'patch_size': 10, 'n_heads': 3, 'head_dim': 26, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4330880728874677, 'attn_dropout': 0.18033450352296268, 'augment_noise_std': 0.03540362888980228, 'augment_scale_low': 0.8041168988591605, 'augment_scale_high': 1.1939819704323988, 'use_focal_loss': False, 'focal_gamma': 1.8493564427131048, 'class_weighted': True, 'grad_clip': 0.9170225492671692, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:23:07,086 - INFO - _models.training_function_executor - Model: 84,396 parameters, 362.6KB storage
2025-10-02 19:23:07,086 - WARNING - _models.training_function_executor - Model storage 362.6KB exceeds 256KB limit!
2025-10-02 19:23:07,087 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0009550403845482, 0.6202355098280733, 0.5049653350597403, 0.4652212302138385, 0.41568497527782, 0.3905694168258818, 0.35099377978271146, 0.3403390153476522, 0.3246434235506158, 0.3183038443316681, 0.30781609568217083, 0.29659452967090616, 0.27913369192024917, 0.2674521095809178, 0.2739753721202708, 0.2701295956722332, 0.24844585940896713, 0.24185774631872253, 0.24971972283099494, 0.2318576239827798, 0.22961358113790095, 0.23260037812400805, 0.21253299934390613, 0.21422434166128412, 0.20602169265482273, 0.22037351350594525, 0.21238428312065988, 0.2007297416038876, 0.19965937952805524, 0.19607823701322227, 0.17423098145724203, 0.17693468762395212, 0.17697963754220278, 0.1780497315147633, 0.16504511416716464, 0.1804994138014465, 0.17922525473306142, 0.1635869729599831, 0.16382661891318545, 0.15287383728277093, 0.1599791830343919, 0.14851420246202598, 0.16013250592620482, 0.14471894188027026, 0.14759945482108033, 0.15383237555608584, 0.14568727664131123, 0.1322578064269023, 0.1271916702158659, 0.1354153328542665, 0.12743773540964523, 0.13795464208030248, 0.1294944750695475, 0.12895120766430582, 0.12693680713481667, 0.12161110366101784], 'val_losses': [0.7906171674642358, 0.5834965809382467, 0.5309108367630629, 0.5449185564791017, 0.48811473611631634, 0.44059294852797004, 0.40736167162541026, 0.4539451656440433, 0.42858096930694195, 0.5364229065286622, 0.4372843954515642, 0.4874850368569958, 0.3852471042890743, 0.5053005462757195, 0.4221730099900177, 0.5049165837105437, 0.4055963365218496, 0.4327496368512222, 0.4539772206388788, 0.455521096129896, 0.45452443028867523, 0.41539445808471226, 0.48700030007438244, 0.4570779852991997, 0.40114746465970963, 0.44971600347048596, 0.43532488812795034, 0.48059474404720626, 0.4229033383461575, 0.4864831851652251, 0.4659250450752654, 0.4731717321929769, 0.5108911185735988, 0.4935722420305525, 0.4634715841798133, 0.48475834112644733, 0.4765483849053992, 0.4439969076546881, 0.5148013693552062, 0.49209838524197536, 0.4168334574332558, 0.4141200902154012, 0.5529969913135598, 0.4504828594962004, 0.5034068459713762, 0.5198308375750758, 0.49338885617501127, 0.5293504205648948, 0.4879355643748819, 0.4745939177665297, 0.5142376297862439, 0.49500471027165394, 0.4835773723389239, 0.5305045767134255, 0.47942261914086065, 0.5444718169293716], 'val_acc': [0.6184688635509334, 0.8165643403082321, 0.7900012529758176, 0.7576744768825961, 0.7808545295075805, 0.7387545420373387, 0.7992732740258113, 0.7871194085954141, 0.7891241699035209, 0.7480265630873324, 0.8026563087332415, 0.7902518481393309, 0.8007768450068914, 0.7889988723217642, 0.8348577872447062, 0.7268512717704548, 0.807041724094725, 0.8442551058764566, 0.8006515474251347, 0.8225786242325523, 0.794010775592031, 0.8151860669089087, 0.8190702919433654, 0.8057887482771583, 0.7856158376143341, 0.7966420248089212, 0.798646786117028, 0.8348577872447062, 0.8417491542413231, 0.7980202982082446, 0.8531512341811803, 0.8531512341811803, 0.8378649292068663, 0.8525247462723969, 0.8356095727352462, 0.8016539280791881, 0.8496429018919934, 0.868562836737251, 0.8377396316251097, 0.8321012404460594, 0.8331036211001128, 0.8483899260744268, 0.8164390427264754, 0.8601678987595539, 0.8690640270642777, 0.8477634381656434, 0.8625485528129307, 0.8554065906528004, 0.8468863550933466, 0.8655556947750909, 0.8752036085703546, 0.8778348577872447, 0.8878586643277785, 0.8698158125548177, 0.8630497431399574, 0.8792131311865681], 'size_bytes': 507131, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 56, 'batch_size': 512, 'lr': 0.0015702970884055383, 'weight_decay': 0.0002481040974867812, 'patch_size': 10, 'n_heads': 3, 'head_dim': 26, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.4330880728874677, 'attn_dropout': 0.18033450352296268, 'augment_noise_std': 0.03540362888980228, 'augment_scale_low': 0.8041168988591605, 'augment_scale_high': 1.1939819704323988, 'use_focal_loss': False, 'focal_gamma': 1.8493564427131048, 'class_weighted': True, 'grad_clip': 0.9170225492671692, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 84396, 'model_storage_size_kb': 362.6390625, 'model_size_validation': 'FAIL'}
2025-10-02 19:23:07,087 - INFO - _models.training_function_executor - BO Objective: base=0.8792, size_penalty=0.2083, final=0.6709
2025-10-02 19:23:07,087 - INFO - _models.training_function_executor - Model: 84,396 parameters, 362.6KB (FAIL 256KB limit)
2025-10-02 19:23:07,087 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 209.616s
2025-10-02 19:23:07,087 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6709
2025-10-02 19:23:07,087 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-02 19:23:07,087 - INFO - bo.run_bo - Recorded observation #1: hparams={'epochs': np.int64(56), 'batch_size': 512, 'lr': 0.0015702970884055383, 'weight_decay': 0.0002481040974867812, 'patch_size': 10, 'n_heads': np.int64(3), 'head_dim': np.int64(26), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.4330880728874677, 'attn_dropout': 0.18033450352296268, 'augment_noise_std': 0.03540362888980228, 'augment_scale_low': 0.8041168988591605, 'augment_scale_high': 1.1939819704323988, 'use_focal_loss': False, 'focal_gamma': 1.8493564427131048, 'class_weighted': True, 'grad_clip': 0.9170225492671692, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, value=0.6709
2025-10-02 19:23:07,087 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'epochs': np.int64(56), 'batch_size': 512, 'lr': 0.0015702970884055383, 'weight_decay': 0.0002481040974867812, 'patch_size': 10, 'n_heads': np.int64(3), 'head_dim': np.int64(26), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.4330880728874677, 'attn_dropout': 0.18033450352296268, 'augment_noise_std': 0.03540362888980228, 'augment_scale_low': 0.8041168988591605, 'augment_scale_high': 1.1939819704323988, 'use_focal_loss': False, 'focal_gamma': 1.8493564427131048, 'class_weighted': True, 'grad_clip': 0.9170225492671692, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True} -> 0.6709
2025-10-02 19:23:07,088 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-10-02 19:23:07,088 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 19:23:07,088 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:23:07,088 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:23:07,088 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 53, 'batch_size': 256, 'lr': 0.00015833718339012065, 'weight_decay': 1.536960311060885e-06, 'patch_size': 500, 'n_heads': 3, 'head_dim': 18, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.19123099563358142, 'attn_dropout': 0.2949692657420365, 'augment_noise_std': 0.023338144662399002, 'augment_scale_low': 0.9719880813472641, 'augment_scale_high': 1.136061507717556, 'use_focal_loss': True, 'focal_gamma': 1.0530598446394661, 'class_weighted': False, 'grad_clip': 2.816441089227697, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:23:07,089 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 53, 'batch_size': 256, 'lr': 0.00015833718339012065, 'weight_decay': 1.536960311060885e-06, 'patch_size': 500, 'n_heads': 3, 'head_dim': 18, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.19123099563358142, 'attn_dropout': 0.2949692657420365, 'augment_noise_std': 0.023338144662399002, 'augment_scale_low': 0.9719880813472641, 'augment_scale_high': 1.136061507717556, 'use_focal_loss': True, 'focal_gamma': 1.0530598446394661, 'class_weighted': False, 'grad_clip': 2.816441089227697, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:23:49,749 - INFO - _models.training_function_executor - Model: 125,879 parameters, 270.4KB storage
2025-10-02 19:23:49,749 - WARNING - _models.training_function_executor - Model storage 270.4KB exceeds 256KB limit!
2025-10-02 19:23:49,749 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6361720398942715, 0.4726231541424262, 0.4213342513838078, 0.3889738399126631, 0.35665243575026495, 0.32557722582119664, 0.3011754227865348, 0.2801327633218419, 0.25988343057710717, 0.24280647571725145, 0.23141310253730116, 0.21726809442603445, 0.20667831641769654, 0.19794161947165703, 0.19318330509609302, 0.18351381756016377, 0.17865127105715953, 0.17338491058609515, 0.16653225069800642, 0.16001818672952633, 0.15796726084625953, 0.15328221911560327, 0.14826652709609064, 0.14559478887645902, 0.14094851185871582, 0.13538929365552674, 0.13277563077215862, 0.12955265154178866, 0.12586722067602407, 0.12255210928235911, 0.12020176460536035, 0.11552861311071197, 0.11507241170326726, 0.11330807564984743, 0.10916612188372364, 0.1073549907503169, 0.10425484693820018, 0.10232753829659291, 0.09995930874488282, 0.09579112546264806, 0.09502766706303606, 0.092920184003337, 0.08907133552312284, 0.08925490423346605, 0.0913898903668007, 0.08704800748147862, 0.08411838795020124, 0.08284483333551931, 0.08036499404702402, 0.08117043927040696, 0.07751912979882127, 0.07526756605844735, 0.07420411050558666], 'val_losses': [0.4993507795062912, 0.4229991293359768, 0.39183175231098694, 0.36082316542602305, 0.3284784415404221, 0.29698192724315553, 0.2806537393795609, 0.2534062494861648, 0.23273473148872614, 0.21995224946517722, 0.21095559766098096, 0.19974511250799393, 0.19239808685116983, 0.184625424798487, 0.18342570365296765, 0.17493919398630492, 0.17239228499636228, 0.17263877789596466, 0.16280432271469716, 0.1601231520835162, 0.15713138509737326, 0.15641655648203515, 0.15647423379482625, 0.15306578900783138, 0.15016288607995204, 0.14836238350314523, 0.14577293843161626, 0.14467075375249372, 0.14134707484082043, 0.13916109588481237, 0.1386956990627758, 0.13863037576925305, 0.1370002316410657, 0.13734694788191137, 0.1358411553087218, 0.13609448499121532, 0.1304482879459507, 0.12966220153388547, 0.12706599756841924, 0.12714846060566848, 0.12565828335810209, 0.130913526784125, 0.12824853771709138, 0.12605922052382587, 0.12282545742288357, 0.12469498456726844, 0.12379164782036159, 0.12193709237636621, 0.12470582553265223, 0.12086297562603752, 0.12692924568754227, 0.12271906151786931, 0.12350692537464664], 'val_acc': [0.7576744768825961, 0.7804786367623104, 0.7883723844129809, 0.8146848765818819, 0.829971181556196, 0.8416238566595665, 0.8512717704548303, 0.8635509334669841, 0.8730735496804911, 0.8795890239318381, 0.8866056885102117, 0.8965041974689888, 0.9001378273399323, 0.9033955644656058, 0.907906277408846, 0.9117905024433028, 0.9136699661696529, 0.9119158000250596, 0.9198095476757299, 0.9185565718581631, 0.9199348452574865, 0.9234431775466734, 0.92281668963789, 0.92469615336424, 0.9263250219270768, 0.9278285929081569, 0.9284550808169403, 0.9265756170905901, 0.9303345445432903, 0.9313369251973437, 0.9313369251973437, 0.9322140082696404, 0.9335922816689638, 0.9317128179426137, 0.9373512091216639, 0.9353464478135572, 0.9382282921939606, 0.940358351083824, 0.9374765067034206, 0.9382282921939606, 0.9421125172284175, 0.9371006139581506, 0.9401077559203107, 0.9408595414108508, 0.9424884099736875, 0.9424884099736875, 0.9429896003007142, 0.9444931712817942, 0.9428643027189575, 0.9456208495176043, 0.9436160882094976, 0.9438666833730109, 0.9438666833730109], 'size_bytes': 261449, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 53, 'batch_size': 256, 'lr': 0.00015833718339012065, 'weight_decay': 1.536960311060885e-06, 'patch_size': 500, 'n_heads': 3, 'head_dim': 18, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.19123099563358142, 'attn_dropout': 0.2949692657420365, 'augment_noise_std': 0.023338144662399002, 'augment_scale_low': 0.9719880813472641, 'augment_scale_high': 1.136061507717556, 'use_focal_loss': True, 'focal_gamma': 1.0530598446394661, 'class_weighted': False, 'grad_clip': 2.816441089227697, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 125879, 'model_storage_size_kb': 270.44316406250005, 'model_size_validation': 'FAIL'}
2025-10-02 19:23:49,749 - INFO - _models.training_function_executor - BO Objective: base=0.9439, size_penalty=0.0282, final=0.9157
2025-10-02 19:23:49,749 - INFO - _models.training_function_executor - Model: 125,879 parameters, 270.4KB (FAIL 256KB limit)
2025-10-02 19:23:49,749 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 42.662s
2025-10-02 19:23:49,750 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9157
2025-10-02 19:23:49,750 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-02 19:23:49,750 - INFO - bo.run_bo - Recorded observation #2: hparams={'epochs': np.int64(53), 'batch_size': 256, 'lr': 0.00015833718339012065, 'weight_decay': 1.536960311060885e-06, 'patch_size': 500, 'n_heads': np.int64(3), 'head_dim': np.int64(18), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.19123099563358142, 'attn_dropout': 0.2949692657420365, 'augment_noise_std': 0.023338144662399002, 'augment_scale_low': 0.9719880813472641, 'augment_scale_high': 1.136061507717556, 'use_focal_loss': True, 'focal_gamma': 1.0530598446394661, 'class_weighted': False, 'grad_clip': 2.816441089227697, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, value=0.9157
2025-10-02 19:23:49,750 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'epochs': np.int64(53), 'batch_size': 256, 'lr': 0.00015833718339012065, 'weight_decay': 1.536960311060885e-06, 'patch_size': 500, 'n_heads': np.int64(3), 'head_dim': np.int64(18), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.19123099563358142, 'attn_dropout': 0.2949692657420365, 'augment_noise_std': 0.023338144662399002, 'augment_scale_low': 0.9719880813472641, 'augment_scale_high': 1.136061507717556, 'use_focal_loss': True, 'focal_gamma': 1.0530598446394661, 'class_weighted': False, 'grad_clip': 2.816441089227697, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True} -> 0.9157
2025-10-02 19:23:49,750 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-10-02 19:23:49,751 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 19:23:49,751 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:23:49,751 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:23:49,751 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 96, 'batch_size': 128, 'lr': 2.3233503515390097e-05, 'weight_decay': 9.565499215943817e-05, 'patch_size': 5, 'n_heads': 2, 'head_dim': 20, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.33126114217699104, 'attn_dropout': 0.0935133228268233, 'augment_noise_std': 0.026003401058890544, 'augment_scale_low': 0.909342055868656, 'augment_scale_high': 1.0369708911051054, 'use_focal_loss': False, 'focal_gamma': 4.100531293444459, 'class_weighted': False, 'grad_clip': 4.474136752138245, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:23:49,752 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 96, 'batch_size': 128, 'lr': 2.3233503515390097e-05, 'weight_decay': 9.565499215943817e-05, 'patch_size': 5, 'n_heads': 2, 'head_dim': 20, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.33126114217699104, 'attn_dropout': 0.0935133228268233, 'augment_noise_std': 0.026003401058890544, 'augment_scale_low': 0.909342055868656, 'augment_scale_high': 1.0369708911051054, 'use_focal_loss': False, 'focal_gamma': 4.100531293444459, 'class_weighted': False, 'grad_clip': 4.474136752138245, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:33:16,246 - INFO - _models.training_function_executor - Model: 35,360 parameters, 151.9KB storage
2025-10-02 19:33:16,246 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1228780017118536, 0.8613392020591544, 0.7793782810364526, 0.7119123688583755, 0.6666949783688078, 0.6336372416389805, 0.6121415215391035, 0.5931631491338311, 0.5723815292910712, 0.5458715338962096, 0.521627267417396, 0.5017695928015394, 0.4824571214392009, 0.4636058084473247, 0.4461037578237492, 0.42881222761738247, 0.41371300434519526, 0.39790608705115177, 0.38568700331272315, 0.375864442399961, 0.3648053785559117, 0.3560922844461177, 0.3497396498335986, 0.3396427851691116, 0.33383920895751956, 0.3268658274952318, 0.31777897464733496, 0.3131594303846583, 0.30762958657824446, 0.30226810669027404, 0.2957127567373548, 0.2906313164050136, 0.2869140633334904, 0.2835945475184745, 0.27798131586131275, 0.2763583242633687, 0.2722999122435708, 0.26627476964673014, 0.2643016929193924, 0.26140794358325775, 0.2592076585408616, 0.255788998815853, 0.25297280913590825, 0.24999603227304157, 0.2487289299120361, 0.24596870137141155, 0.2436851553379214, 0.24170675111263948, 0.23965493744448738, 0.23828281847616486, 0.23515942376090057, 0.23611412118768288, 0.23365336311780072, 0.23026717458606136, 0.23014416399269225, 0.22720602740172854, 0.22585608915002842, 0.2232655693478426, 0.22321845267166512, 0.22202179756927987, 0.2200123802132241, 0.2202865718731892, 0.21654325681824182, 0.21745622482325722, 0.2153702388752322, 0.21400334014781447, 0.21174872210548923, 0.21139620582875096, 0.21168668454005501, 0.20891831265230607, 0.20629906258711378, 0.20684242514695672, 0.2051156881760338, 0.20547524841716855, 0.2032458463951717, 0.20304413701455193, 0.20142283739437902, 0.20091708882496262, 0.19874062190495223, 0.1995971862706229, 0.19834628775522112, 0.19786321750752733, 0.19600006457939484, 0.19633092615544875, 0.19297734960338112, 0.19415552518496076, 0.19207879708056963, 0.19305133482935105, 0.19095244653098428, 0.19091853317366586, 0.18869344492090512, 0.18808622578292533, 0.1860463008836775, 0.18468977525609478, 0.18653994551637929, 0.18732848740406727], 'val_losses': [0.8871645788145072, 0.817606006076709, 0.7311834010985573, 0.6762175603313803, 0.6385792645165652, 0.620959904883652, 0.600645796859224, 0.5821636105267838, 0.5569138196032681, 0.5317781720450013, 0.5136809565193178, 0.4992292577940156, 0.4870853811603279, 0.4744175909519524, 0.45914467860023983, 0.4439996522988402, 0.44441786025299673, 0.4304398290552512, 0.39722835638591514, 0.4061103030626613, 0.41138815676496826, 0.4027857731165645, 0.40102469242241967, 0.39249787722639445, 0.38568914763731194, 0.3999952852330162, 0.3774018439601618, 0.39264738157553986, 0.3659753777294914, 0.35684399967906977, 0.36287290031129354, 0.36517862760696534, 0.35667428227911435, 0.3655549992058394, 0.35423799507929526, 0.340326837550248, 0.336155989608166, 0.35188661576019403, 0.3332256599037797, 0.34199932619020046, 0.3427143346216278, 0.3299624943827316, 0.31113152630331226, 0.3069137850510956, 0.30527913041273735, 0.31765594312592327, 0.32671199988607863, 0.30574576883234783, 0.29394415311833666, 0.2991488148374167, 0.30179527748827495, 0.2950850228642838, 0.27664856842146407, 0.2842639705700439, 0.2867171389576248, 0.28599190704133365, 0.30330052424412207, 0.2825612901311936, 0.26475066771828104, 0.2729946059020685, 0.2723856677971393, 0.2701175383660537, 0.2800000915475364, 0.26772321522228404, 0.2617174932053328, 0.260906900077696, 0.2656470598201826, 0.25860974131379116, 0.2696596494878299, 0.2725039318814461, 0.2826707793975919, 0.26014621628808016, 0.25875316231849127, 0.271999066664064, 0.2603836336420855, 0.2597434251675285, 0.2607532829744537, 0.268223097850589, 0.25128944210130283, 0.25718765848188707, 0.24193806130550602, 0.26543104137383733, 0.2580889995430305, 0.26272277046513576, 0.2591578062950945, 0.2431864777056888, 0.24434098406044735, 0.2418675360121295, 0.24131438988609252, 0.25098785154204994, 0.2248216210720369, 0.26396360940733676, 0.24529181219531904, 0.2411383903966842, 0.24014157854611526, 0.22444045870969684], 'val_acc': [0.7200852023555946, 0.7207116902643779, 0.7663200100238066, 0.7678235810048866, 0.776469114146097, 0.7733366746021801, 0.7770956020548804, 0.7803533391805538, 0.7898759553940609, 0.8075429144217516, 0.8151860669089087, 0.8214509459967423, 0.8280917178298459, 0.8349830848264629, 0.8423756421501065, 0.8493923067284801, 0.8461345696028066, 0.8541536148352337, 0.8671845633379276, 0.8636762310487408, 0.8630497431399574, 0.8664327778473876, 0.8663074802656309, 0.8698158125548177, 0.8734494424257612, 0.8695652173913043, 0.8770830722967047, 0.8713193835358978, 0.8840997368750783, 0.8886104498183185, 0.8894875328906152, 0.8893622353088585, 0.8916175917804786, 0.8897381280541286, 0.8939982458338555, 0.900263124921689, 0.9012655055757424, 0.8957524119784488, 0.9041473499561459, 0.8997619345946624, 0.9012655055757424, 0.9047738378649292, 0.9122916927703295, 0.9141711564966796, 0.9120410976068162, 0.9084074677358727, 0.9054003257737125, 0.914045858914923, 0.9169277032953264, 0.9158000250595163, 0.914797644405463, 0.9158000250595163, 0.9224407968926199, 0.9193083573487032, 0.9180553815311364, 0.9183059766946498, 0.9122916927703295, 0.9198095476757299, 0.9255732364365368, 0.92356847512843, 0.9226913920561333, 0.9250720461095101, 0.9195589525122165, 0.9230672848014033, 0.9255732364365368, 0.9245708557824834, 0.9238190702919433, 0.9260744267635634, 0.9211878210750533, 0.9203107380027565, 0.9173035960405964, 0.9265756170905901, 0.9261997243453202, 0.9189324646034331, 0.9230672848014033, 0.9248214509459968, 0.9253226412730234, 0.9214384162385666, 0.9283297832351836, 0.9234431775466734, 0.9298333542162637, 0.9203107380027565, 0.9226913920561333, 0.9198095476757299, 0.9226913920561333, 0.9272021049993735, 0.9274527001628868, 0.9277032953264002, 0.928956271143967, 0.9243202606189701, 0.9344693647412605, 0.9173035960405964, 0.9258238316000501, 0.9283297832351836, 0.9261997243453202, 0.9347199599047739], 'size_bytes': 206923, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 96, 'batch_size': 128, 'lr': 2.3233503515390097e-05, 'weight_decay': 9.565499215943817e-05, 'patch_size': 5, 'n_heads': 2, 'head_dim': 20, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.33126114217699104, 'attn_dropout': 0.0935133228268233, 'augment_noise_std': 0.026003401058890544, 'augment_scale_low': 0.909342055868656, 'augment_scale_high': 1.0369708911051054, 'use_focal_loss': False, 'focal_gamma': 4.100531293444459, 'class_weighted': False, 'grad_clip': 4.474136752138245, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 35360, 'model_storage_size_kb': 151.9375, 'model_size_validation': 'PASS'}
2025-10-02 19:33:16,246 - INFO - _models.training_function_executor - BO Objective: base=0.9347, size_penalty=0.0000, final=0.9347
2025-10-02 19:33:16,246 - INFO - _models.training_function_executor - Model: 35,360 parameters, 151.9KB (PASS 256KB limit)
2025-10-02 19:33:16,246 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 566.496s
2025-10-02 19:33:16,457 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9347
2025-10-02 19:33:16,457 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.211s
2025-10-02 19:33:16,457 - INFO - bo.run_bo - Recorded observation #3: hparams={'epochs': np.int64(96), 'batch_size': 128, 'lr': 2.3233503515390097e-05, 'weight_decay': 9.565499215943817e-05, 'patch_size': 5, 'n_heads': np.int64(2), 'head_dim': np.int64(20), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.33126114217699104, 'attn_dropout': 0.0935133228268233, 'augment_noise_std': 0.026003401058890544, 'augment_scale_low': 0.909342055868656, 'augment_scale_high': 1.0369708911051054, 'use_focal_loss': False, 'focal_gamma': 4.100531293444459, 'class_weighted': False, 'grad_clip': 4.474136752138245, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, value=0.9347
2025-10-02 19:33:16,457 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'epochs': np.int64(96), 'batch_size': 128, 'lr': 2.3233503515390097e-05, 'weight_decay': 9.565499215943817e-05, 'patch_size': 5, 'n_heads': np.int64(2), 'head_dim': np.int64(20), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.33126114217699104, 'attn_dropout': 0.0935133228268233, 'augment_noise_std': 0.026003401058890544, 'augment_scale_low': 0.909342055868656, 'augment_scale_high': 1.0369708911051054, 'use_focal_loss': False, 'focal_gamma': 4.100531293444459, 'class_weighted': False, 'grad_clip': 4.474136752138245, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True} -> 0.9347
2025-10-02 19:33:16,457 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-10-02 19:33:16,457 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 19:33:16,457 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:33:16,457 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:33:16,457 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 91, 'batch_size': 128, 'lr': 0.0008223617738669272, 'weight_decay': 0.0017145859743373732, 'patch_size': 50, 'n_heads': 4, 'head_dim': 21, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.47374803272034915, 'attn_dropout': 0.20903142410910758, 'augment_noise_std': 0.011275499191008278, 'augment_scale_low': 0.8571213976390165, 'augment_scale_high': 1.1933193533102644, 'use_focal_loss': True, 'focal_gamma': 4.598486908381274, 'class_weighted': False, 'grad_clip': 1.5934116211006624, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:33:16,458 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 91, 'batch_size': 128, 'lr': 0.0008223617738669272, 'weight_decay': 0.0017145859743373732, 'patch_size': 50, 'n_heads': 4, 'head_dim': 21, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.47374803272034915, 'attn_dropout': 0.20903142410910758, 'augment_noise_std': 0.011275499191008278, 'augment_scale_low': 0.8571213976390165, 'augment_scale_high': 1.1933193533102644, 'use_focal_loss': True, 'focal_gamma': 4.598486908381274, 'class_weighted': False, 'grad_clip': 1.5934116211006624, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:35:24,731 - INFO - _models.training_function_executor - Model: 96,936 parameters, 208.3KB storage
2025-10-02 19:35:24,731 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.11954599878443918, 0.06378642285112969, 0.0481525629236503, 0.041066808973168334, 0.03611117436479694, 0.0322562080581856, 0.03134371433468334, 0.028835650699846397, 0.02648620052429491, 0.024307674427053737, 0.02354714525909915, 0.023190743843698524, 0.022148395666755562, 0.020512722082860507, 0.01946037676558447, 0.020373987335447635, 0.017948231946697858, 0.0183889502066332, 0.01898451752805943, 0.017327008901234946, 0.015500418274345597, 0.016885199952855787, 0.016805194409288324, 0.01573616467539751, 0.015214568881508597, 0.015204197786571264, 0.014849131995241648, 0.014021758615160857, 0.014179229993380508, 0.013175473192752973, 0.01418824888186649, 0.011884116045222761, 0.014736888680063393, 0.015309930454741338, 0.012237706605599525, 0.011259511320214903, 0.012778945675170112, 0.013387187748102517, 0.011705241098954933, 0.014049476127708271, 0.012642155712695577, 0.011676213619940857, 0.010984218436538453, 0.012419362338917399, 0.011214602125030643, 0.012503900513539796, 0.010658220120425622, 0.01182925697732372, 0.010319176890636685, 0.01220590287843836, 0.010217292856875377, 0.010693257079935176, 0.011154684807350324, 0.011493104801574549, 0.009140593879506519, 0.012727609337717554, 0.009701124880254233, 0.010047712452225152, 0.010911168798721897, 0.009873555629910545, 0.008521203822193587, 0.007972879632931367, 0.009927357303440344, 0.011332900730489072, 0.010473980896207116, 0.008722976487032718, 0.009268643884948961, 0.008860099226200109, 0.007825529013596108, 0.00921075981212677, 0.00949328076913304, 0.008372159226781854, 0.008909262747111164, 0.009544057324617383, 0.012035070082314362, 0.010050417848351024, 0.008987724150775209, 0.007613988070538781, 0.007770026379560103, 0.009123274640372166, 0.00958050124673259, 0.010313657590625726, 0.008853205917838962, 0.008624355651272994, 0.0069166960943341715, 0.008346067483250147, 0.008671833812009262, 0.008137012984753248, 0.007097743675753969, 0.006999988306673294, 0.008566188251105052], 'val_losses': [0.07119259349310993, 0.06647472619432047, 0.04214052680276298, 0.04838519587752485, 0.03733157295098417, 0.04090294005985136, 0.03500180878474479, 0.03448249480413205, 0.028833577604088015, 0.028721179528183587, 0.031105335950447684, 0.027991089645760102, 0.029863204312967174, 0.030791028566348563, 0.0381197542500121, 0.025811641911902872, 0.026237816136862952, 0.02549435318126862, 0.02660624873672134, 0.023357291656778863, 0.022831863466621773, 0.02834016827068492, 0.025031607432363028, 0.027612366028715384, 0.025997586596172313, 0.025844690431123644, 0.024682049851319856, 0.024158608319597007, 0.029260083039025962, 0.02883357413242879, 0.02802119658373753, 0.027584476532038105, 0.023194489425778494, 0.022974688605047262, 0.020778682530056995, 0.027406141453183035, 0.02384418972436833, 0.021794201658454967, 0.027357693866610362, 0.03155692964568202, 0.030864299286533576, 0.02612227779369563, 0.028675648694781636, 0.028390962053699467, 0.028557249147325003, 0.027169940383821752, 0.02970755597453505, 0.025852395659187746, 0.026507928540848234, 0.026670867965314312, 0.028094273895611515, 0.028928920634108637, 0.02355122007592969, 0.02762700011632807, 0.03325455627664261, 0.027018460044734176, 0.023087915970842695, 0.024094470841381846, 0.021733609775809206, 0.026278503966382268, 0.026244653580466872, 0.02510363956155551, 0.028591667723796104, 0.025529011787141627, 0.024594679837044043, 0.02358441953819776, 0.02263590452620565, 0.02269372415899588, 0.023364903363066052, 0.026427608644763654, 0.025999363913747967, 0.03213111826417859, 0.028297926810970434, 0.030983904256899543, 0.03258829513351626, 0.02535096515152571, 0.020058177306066256, 0.023972187730367134, 0.02259949435194159, 0.029504886756130305, 0.02350206665863684, 0.02762318937260321, 0.02381233640025394, 0.026699867963514565, 0.02577980487741499, 0.029771532761155923, 0.03207395045496799, 0.02750267830577325, 0.040020762384124546, 0.03434631588233173, 0.029663648221549824], 'val_acc': [0.8617967673223906, 0.8491417115649668, 0.9105375266257362, 0.8797143215135947, 0.9240696654554567, 0.914045858914923, 0.916677108131813, 0.9163012153865431, 0.9250720461095101, 0.9349705550682872, 0.9360982333040972, 0.9371006139581506, 0.9302092469615336, 0.9393559704297707, 0.9072797895000626, 0.9347199599047739, 0.9273274025811302, 0.940358351083824, 0.9397318631750408, 0.9505074552061146, 0.9437413857912542, 0.913294073424383, 0.951885728605438, 0.9174288936223531, 0.9419872196466608, 0.9160506202230297, 0.9439919809547676, 0.9485026938980078, 0.9096604435534394, 0.9398571607567974, 0.9312116276155871, 0.930835734870317, 0.9345946623230171, 0.9453702543540909, 0.9348452574865305, 0.9047738378649292, 0.9502568600426011, 0.939606565593284, 0.9185565718581631, 0.9409848389926074, 0.9001378273399323, 0.938478887357474, 0.92281668963789, 0.92469615336424, 0.939606565593284, 0.9513845382784112, 0.9184312742764065, 0.9325899010149105, 0.9397318631750408, 0.9332163889236937, 0.945746147099361, 0.9333416865054505, 0.9516351334419245, 0.9468738253351711, 0.9001378273399323, 0.9261997243453202, 0.9508833479513845, 0.9363488284676106, 0.9557699536398947, 0.9180553815311364, 0.9391053752662574, 0.9542663826588147, 0.9432401954642275, 0.9412354341561208, 0.9452449567723343, 0.9278285929081569, 0.9168024057135697, 0.9270768074176169, 0.9586517980202982, 0.9319634131061271, 0.9429896003007142, 0.8955018168149355, 0.9294574614709936, 0.9358476381405839, 0.9000125297581757, 0.9407342438290941, 0.9557699536398947, 0.9448690640270643, 0.9520110261871946, 0.9099110387169528, 0.9447437664453076, 0.9487532890615211, 0.9516351334419245, 0.946122039844631, 0.9463726350081444, 0.9398571607567974, 0.908282170154116, 0.9442425761182809, 0.9422378148101742, 0.9273274025811302, 0.9215637138203233], 'size_bytes': 384891, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 91, 'batch_size': 128, 'lr': 0.0008223617738669272, 'weight_decay': 0.0017145859743373732, 'patch_size': 50, 'n_heads': 4, 'head_dim': 21, 'num_layers': 3, 'mlp_ratio': 4, 'dropout': 0.47374803272034915, 'attn_dropout': 0.20903142410910758, 'augment_noise_std': 0.011275499191008278, 'augment_scale_low': 0.8571213976390165, 'augment_scale_high': 1.1933193533102644, 'use_focal_loss': True, 'focal_gamma': 4.598486908381274, 'class_weighted': False, 'grad_clip': 1.5934116211006624, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 96936, 'model_storage_size_kb': 208.2609375, 'model_size_validation': 'PASS'}
2025-10-02 19:35:24,731 - INFO - _models.training_function_executor - BO Objective: base=0.9216, size_penalty=0.0000, final=0.9216
2025-10-02 19:35:24,731 - INFO - _models.training_function_executor - Model: 96,936 parameters, 208.3KB (PASS 256KB limit)
2025-10-02 19:35:24,731 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 128.274s
2025-10-02 19:35:24,815 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9216
2025-10-02 19:35:24,816 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-10-02 19:35:24,816 - INFO - bo.run_bo - Recorded observation #4: hparams={'epochs': np.int64(91), 'batch_size': np.int64(128), 'lr': 0.0008223617738669272, 'weight_decay': 0.0017145859743373732, 'patch_size': np.int64(50), 'n_heads': np.int64(4), 'head_dim': np.int64(21), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.47374803272034915, 'attn_dropout': 0.20903142410910758, 'augment_noise_std': 0.011275499191008278, 'augment_scale_low': 0.8571213976390165, 'augment_scale_high': 1.1933193533102644, 'use_focal_loss': np.True_, 'focal_gamma': 4.598486908381274, 'class_weighted': np.False_, 'grad_clip': 1.5934116211006624, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9216
2025-10-02 19:35:24,816 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'epochs': np.int64(91), 'batch_size': np.int64(128), 'lr': 0.0008223617738669272, 'weight_decay': 0.0017145859743373732, 'patch_size': np.int64(50), 'n_heads': np.int64(4), 'head_dim': np.int64(21), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'dropout': 0.47374803272034915, 'attn_dropout': 0.20903142410910758, 'augment_noise_std': 0.011275499191008278, 'augment_scale_low': 0.8571213976390165, 'augment_scale_high': 1.1933193533102644, 'use_focal_loss': np.True_, 'focal_gamma': 4.598486908381274, 'class_weighted': np.False_, 'grad_clip': 1.5934116211006624, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9216
2025-10-02 19:35:24,816 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-10-02 19:35:24,816 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 19:35:24,816 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:35:24,816 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:35:24,816 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 80, 'batch_size': 256, 'lr': 0.00015370980818978325, 'weight_decay': 1.5790274199287825e-06, 'patch_size': 5, 'n_heads': 4, 'head_dim': 28, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.3132333167962661, 'attn_dropout': 0.10839120999439625, 'augment_noise_std': 0.019110292383172477, 'augment_scale_low': 0.8119453642686089, 'augment_scale_high': 1.17203825479743, 'use_focal_loss': False, 'focal_gamma': 1.623218482140139, 'class_weighted': True, 'grad_clip': 1.3692462110939239, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 19:35:24,817 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 80, 'batch_size': 256, 'lr': 0.00015370980818978325, 'weight_decay': 1.5790274199287825e-06, 'patch_size': 5, 'n_heads': 4, 'head_dim': 28, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.3132333167962661, 'attn_dropout': 0.10839120999439625, 'augment_noise_std': 0.019110292383172477, 'augment_scale_low': 0.8119453642686089, 'augment_scale_high': 1.17203825479743, 'use_focal_loss': False, 'focal_gamma': 1.623218482140139, 'class_weighted': True, 'grad_clip': 1.3692462110939239, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 19:46:52,949 - INFO - _models.training_function_executor - Model: 176,960 parameters, 760.4KB storage
2025-10-02 19:46:52,949 - WARNING - _models.training_function_executor - Model storage 760.4KB exceeds 256KB limit!
2025-10-02 19:46:52,950 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1794979158052916, 0.8481173635022038, 0.6720258142608108, 0.5781992342643629, 0.5050352841342142, 0.45714033329538756, 0.42131746303440315, 0.37928618121422747, 0.37087628814830775, 0.3479277176602018, 0.3454592976928844, 0.3178149888695315, 0.316294308987431, 0.30321874357467044, 0.2883889469321097, 0.279442813974109, 0.2727947022879187, 0.26781394536888875, 0.2538455818409169, 0.2524327242919251, 0.24056587796081216, 0.23246651174366909, 0.22912308868964026, 0.229485404625205, 0.21553139049955783, 0.2184359677313556, 0.21448445678153097, 0.20670696195056631, 0.20183381203626088, 0.20220785764680635, 0.18831728791038901, 0.19184267187737114, 0.18670380016737437, 0.1809297967513686, 0.17835787308640683, 0.18040703123348031, 0.17225748268624308, 0.16941410043583668, 0.15796134125436528, 0.15789486859400453, 0.16431377460715116, 0.1610435283611066, 0.15196046982926428, 0.14723810881233432, 0.1517348671793243, 0.13703956907516515, 0.13657573206822302, 0.13945701771917243, 0.1425377094519339, 0.13461005566746423, 0.13289641834991758, 0.12439253657666027, 0.12906162657154774, 0.12420998645845266, 0.12487975777460979, 0.12489163468403726, 0.11935884352615071, 0.11074739509281896, 0.12049919033155657, 0.11425325708390328, 0.11057998355243139, 0.11143236626521338, 0.11440996931935188, 0.10833077131861661, 0.10117808039759431, 0.10100865532397194, 0.09572290291453246, 0.09786868277910683, 0.10147001914145769, 0.09342812128343801, 0.09306174814638064, 0.08746338379033279, 0.10193597476077428, 0.08897647472116033, 0.08811589167036889, 0.08582424820757777, 0.08539596377320746, 0.08035314802881045, 0.07809716969118383, 0.0881544526607938], 'val_losses': [0.9317677275845616, 0.726322516446095, 0.6264675073404985, 0.5964218495095557, 0.5144480363257082, 0.49425664430213684, 0.466218483073119, 0.4545805840739599, 0.4422698974788615, 0.4436483485718715, 0.406455241144637, 0.42714829753177413, 0.39822551108828586, 0.39361794328588007, 0.42113744912328555, 0.4382547476867912, 0.38537516972278085, 0.4196501001874362, 0.4004530892412759, 0.41416030356705064, 0.3845829651657762, 0.39226183171649276, 0.3704677682340541, 0.3610373759550523, 0.3821252976170669, 0.3715878039310935, 0.375584414243967, 0.3910469058431968, 0.3854676206717439, 0.34164089721749713, 0.3840563134462399, 0.3852550936527919, 0.3613986521940634, 0.3730517695786854, 0.37093385843059024, 0.37210753228846327, 0.3629347296783372, 0.3897886276245117, 0.39462183494493785, 0.3716599413841055, 0.3928141019470096, 0.4334818338589058, 0.39832122645391854, 0.38145070644477125, 0.37500166355286485, 0.41557572499177103, 0.3972050781358141, 0.44191704485264355, 0.4432191789157706, 0.40806275824230276, 0.4368865869798363, 0.4122881059837198, 0.4411311706061531, 0.42911911382664414, 0.44451813427611203, 0.46178674567180417, 0.461676971563763, 0.4523631951187864, 0.4236745712318272, 0.4214800933431734, 0.43454690296060533, 0.4344768813865971, 0.46734439967464586, 0.41360156684871185, 0.46190446338891356, 0.4703171933120003, 0.4567814936107329, 0.4523602351182207, 0.4580313649605934, 0.4571362922341285, 0.4326334686121566, 0.4483179166239017, 0.4687781103152318, 0.4943733346102277, 0.49073267065183723, 0.4760996190217963, 0.48006409160207497, 0.4687116743179659, 0.5042911464671207, 0.5149751905360745], 'val_acc': [0.4823956897631876, 0.6512968299711815, 0.6717203357975191, 0.7058012780353339, 0.7074301465981706, 0.7347450194211251, 0.7400075178549054, 0.7238441298082947, 0.8104247588021551, 0.8057887482771583, 0.791755419120411, 0.7490289437413858, 0.8293446936474126, 0.8184438040345822, 0.8278411226663326, 0.7996491667710813, 0.8362360606440296, 0.8338554065906528, 0.8240821952136324, 0.8285929081568726, 0.8022804159879714, 0.807793509585265, 0.8263375516852525, 0.8541536148352337, 0.800902142588648, 0.8710687883723844, 0.837614334043353, 0.8511464728730735, 0.8545295075805037, 0.8523994486906402, 0.84563337927578, 0.84751284300213, 0.8283423129933593, 0.8733241448440046, 0.8617967673223906, 0.8461345696028066, 0.8462598671845634, 0.8406214760055131, 0.8546548051622603, 0.8708181932088711, 0.84600927202105, 0.791379526375141, 0.8626738503946874, 0.8758300964791379, 0.8607943866683373, 0.8688134319007643, 0.8713193835358978, 0.8778348577872447, 0.8884851522365619, 0.84488159378524, 0.8805914045858915, 0.870066407718331, 0.8691893246460344, 0.8743265254980579, 0.8744518230798145, 0.8735747400075179, 0.9046485402831725, 0.8784613456960281, 0.8782107505325147, 0.8921187821075053, 0.8863550933466984, 0.8990101491041222, 0.9052750281919559, 0.8785866432777847, 0.8787119408595414, 0.8565342688886104, 0.8946247337426387, 0.9003884225034456, 0.8891116401453452, 0.9099110387169528, 0.8695652173913043, 0.9106628242074928, 0.8783360481142714, 0.8803408094223781, 0.8676857536649543, 0.8663074802656309, 0.8901140207993986, 0.9099110387169528, 0.9008896128304724, 0.9163012153865431], 'size_bytes': 882619, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 80, 'batch_size': 256, 'lr': 0.00015370980818978325, 'weight_decay': 1.5790274199287825e-06, 'patch_size': 5, 'n_heads': 4, 'head_dim': 28, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.3132333167962661, 'attn_dropout': 0.10839120999439625, 'augment_noise_std': 0.019110292383172477, 'augment_scale_low': 0.8119453642686089, 'augment_scale_high': 1.17203825479743, 'use_focal_loss': False, 'focal_gamma': 1.623218482140139, 'class_weighted': True, 'grad_clip': 1.3692462110939239, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 176960, 'model_storage_size_kb': 760.3750000000001, 'model_size_validation': 'FAIL'}
2025-10-02 19:46:52,950 - INFO - _models.training_function_executor - BO Objective: base=0.9163, size_penalty=0.8000, final=0.1163
2025-10-02 19:46:52,950 - INFO - _models.training_function_executor - Model: 176,960 parameters, 760.4KB (FAIL 256KB limit)
2025-10-02 19:46:52,950 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 688.134s
2025-10-02 19:46:53,035 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1163
2025-10-02 19:46:53,035 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.085s
2025-10-02 19:46:53,035 - INFO - bo.run_bo - Recorded observation #5: hparams={'epochs': np.int64(80), 'batch_size': np.int64(256), 'lr': 0.00015370980818978325, 'weight_decay': 1.5790274199287825e-06, 'patch_size': np.int64(5), 'n_heads': np.int64(4), 'head_dim': np.int64(28), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.3132333167962661, 'attn_dropout': 0.10839120999439625, 'augment_noise_std': 0.019110292383172477, 'augment_scale_low': 0.8119453642686089, 'augment_scale_high': 1.17203825479743, 'use_focal_loss': np.False_, 'focal_gamma': 1.623218482140139, 'class_weighted': np.True_, 'grad_clip': 1.3692462110939239, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.1163
2025-10-02 19:46:53,035 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'epochs': np.int64(80), 'batch_size': np.int64(256), 'lr': 0.00015370980818978325, 'weight_decay': 1.5790274199287825e-06, 'patch_size': np.int64(5), 'n_heads': np.int64(4), 'head_dim': np.int64(28), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.3132333167962661, 'attn_dropout': 0.10839120999439625, 'augment_noise_std': 0.019110292383172477, 'augment_scale_low': 0.8119453642686089, 'augment_scale_high': 1.17203825479743, 'use_focal_loss': np.False_, 'focal_gamma': 1.623218482140139, 'class_weighted': np.True_, 'grad_clip': 1.3692462110939239, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.1163
2025-10-02 19:46:53,035 - INFO - bo.run_bo - üîçBO Trial 6: Using RF surrogate + Expected Improvement
2025-10-02 19:46:53,035 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 19:46:53,036 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:46:53,036 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:46:53,036 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 76, 'batch_size': 64, 'lr': 3.484014869142792e-05, 'weight_decay': 1.766206646836706e-05, 'patch_size': 200, 'n_heads': 4, 'head_dim': 6, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.3261980222306933, 'attn_dropout': 0.10736236838165304, 'augment_noise_std': 0.004239407333070411, 'augment_scale_low': 0.9566291110304341, 'augment_scale_high': 1.1239495556872914, 'use_focal_loss': False, 'focal_gamma': 1.142780621747404, 'class_weighted': True, 'grad_clip': 1.9187354467022704, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 19:46:53,037 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 76, 'batch_size': 64, 'lr': 3.484014869142792e-05, 'weight_decay': 1.766206646836706e-05, 'patch_size': 200, 'n_heads': 4, 'head_dim': 6, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.3261980222306933, 'attn_dropout': 0.10736236838165304, 'augment_noise_std': 0.004239407333070411, 'augment_scale_low': 0.9566291110304341, 'augment_scale_high': 1.1239495556872914, 'use_focal_loss': False, 'focal_gamma': 1.142780621747404, 'class_weighted': True, 'grad_clip': 1.9187354467022704, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 19:48:22,440 - INFO - _models.training_function_executor - Model: 15,941 parameters, 68.5KB storage
2025-10-02 19:48:22,440 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5468747851783102, 1.4921687372689094, 1.4469611886128466, 1.3988183358705713, 1.3339782452719988, 1.285677340325454, 1.2346173192057899, 1.1957220581171377, 1.1550547117107446, 1.1313539030811592, 1.1017855598402775, 1.079345808239265, 1.0579788634450917, 1.0420304452200766, 1.017718211763489, 1.0058371798457335, 0.9897356758895003, 0.9720749606693485, 0.956440708194203, 0.9470910132426097, 0.9305968371466191, 0.9113269089472434, 0.9074199409542819, 0.8967580998077939, 0.8863177879768643, 0.8729996541570132, 0.8653528925933601, 0.8601841743427145, 0.8474419798161815, 0.8435083454150333, 0.8345169283873723, 0.8401642304048149, 0.8260013859354829, 0.8167500140837021, 0.8204948340849788, 0.8103645741855523, 0.8110599910182634, 0.7992285279441521, 0.8020793571323693, 0.7937393336577945, 0.7892934827955729, 0.7848014399375219, 0.7771019514306903, 0.7789253575388166, 0.7823230216985463, 0.7692403333906997, 0.7646375879142621, 0.7632985709800056, 0.760138331144685, 0.7597880262377507, 0.7546197256938716, 0.7519050490022307, 0.7566900556033646, 0.7468650717262233, 0.744864905201298, 0.7422503166560507, 0.7375731774784735, 0.7444991236364423, 0.7341530855723022, 0.7323840293796746, 0.7299028322139383, 0.7294296280832789, 0.7324595326247402, 0.7278175792940031, 0.723830730437157, 0.7175327549937285, 0.7138586807037376, 0.7110242005320333, 0.7152709121808363, 0.7100531609626065, 0.7100471059630674, 0.716412873158041, 0.7093130785249414, 0.6964620609755161, 0.706392752099365, 0.6942911147063034], 'val_losses': [1.5127459309510667, 1.4663566926146732, 1.4191519789282891, 1.360843042385726, 1.304493889789536, 1.2491982594093214, 1.202422485227887, 1.1623499184499602, 1.1267762168683648, 1.0999452910198035, 1.0693232246679616, 1.0445568482389787, 1.023721464496405, 1.0109312089817997, 0.985780281054766, 0.9716986460339797, 0.951899465134741, 0.9353876515911151, 0.9211731760398202, 0.9108597909862005, 0.893247782467093, 0.8818478569875697, 0.8748359261725932, 0.8663697113460235, 0.8540265497416276, 0.8500062810674136, 0.8394586794538585, 0.8374332832208687, 0.8308876249862186, 0.8309018229007302, 0.8164937031431883, 0.8172130321085177, 0.8112239422109279, 0.8124616842732336, 0.8048497204821804, 0.8070717663592286, 0.8012580137969169, 0.8010220803677953, 0.7962208827356545, 0.7933881992445959, 0.7891700302594109, 0.7865535934128837, 0.7853509208759021, 0.7825285831484755, 0.7839908903365953, 0.7799606311815661, 0.7805155022687115, 0.7731175357245993, 0.7810390436952537, 0.7733183198816987, 0.771857990490854, 0.7698744225884392, 0.7667706759408249, 0.7660944026419579, 0.7612640869869891, 0.7584043627409032, 0.758382276542622, 0.7541165963298858, 0.7542971426973724, 0.7545267639042456, 0.7502520149206102, 0.7459064517653298, 0.7456862039172311, 0.7439373306516507, 0.7439600261675805, 0.7445805536417596, 0.7415095523714855, 0.7384877785482049, 0.7359127893272819, 0.7367124157566696, 0.7320893394694623, 0.7303855957027188, 0.7334360772170394, 0.7279666086707612, 0.7262375312048067, 0.7245237999520435], 'val_acc': [0.2608695652173913, 0.3165016915173537, 0.3567222152612455, 0.39280791880716703, 0.38466357599298334, 0.4132314246335046, 0.4376644530760556, 0.44931712817942615, 0.4508206991605062, 0.43378022804159877, 0.4758802155118406, 0.491793008394938, 0.48703170028818443, 0.4697406340057637, 0.5095852650043854, 0.4905400325773713, 0.5123418118030322, 0.5415361483523368, 0.5236185941611327, 0.49342187695777473, 0.5293822829219396, 0.5367748402455833, 0.5271269264503196, 0.5363989475003133, 0.5663450695401578, 0.5655932840496178, 0.5654679864678611, 0.5461721588773336, 0.553314121037464, 0.5469239443678737, 0.5764941736624483, 0.5665956647036712, 0.5883974439293321, 0.5850144092219021, 0.5831349454955519, 0.5660944743766445, 0.585390301967172, 0.5865179802029821, 0.5887733366746022, 0.5956647036712192, 0.600300714196216, 0.6187194587144468, 0.6118280917178298, 0.6025560706678361, 0.6254855281293071, 0.6068161884475629, 0.6237313619847137, 0.6258614208745771, 0.6014283924320261, 0.6333792757799774, 0.6252349329657938, 0.6439042726475379, 0.6253602305475504, 0.6251096353840371, 0.6286179676732239, 0.6211001127678236, 0.630121538654304, 0.6501691517353715, 0.6487908783360481, 0.6477884976819948, 0.6704673599799524, 0.6548051622603683, 0.6350081443428142, 0.6616965292569853, 0.6581881969677985, 0.6792381907029195, 0.6634506954015787, 0.677484024558326, 0.6555569477509084, 0.6802405713569728, 0.6535521864428017, 0.6695902769076557, 0.6761057511590026, 0.6720962285427892, 0.6522992106252349, 0.674852775341436], 'size_bytes': 69869, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 76, 'batch_size': 64, 'lr': 3.484014869142792e-05, 'weight_decay': 1.766206646836706e-05, 'patch_size': 200, 'n_heads': 4, 'head_dim': 6, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.3261980222306933, 'attn_dropout': 0.10736236838165304, 'augment_noise_std': 0.004239407333070411, 'augment_scale_low': 0.9566291110304341, 'augment_scale_high': 1.1239495556872914, 'use_focal_loss': False, 'focal_gamma': 1.142780621747404, 'class_weighted': True, 'grad_clip': 1.9187354467022704, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 15941, 'model_storage_size_kb': 68.49648437500001, 'model_size_validation': 'PASS'}
2025-10-02 19:48:22,440 - INFO - _models.training_function_executor - BO Objective: base=0.6749, size_penalty=0.0000, final=0.6749
2025-10-02 19:48:22,440 - INFO - _models.training_function_executor - Model: 15,941 parameters, 68.5KB (PASS 256KB limit)
2025-10-02 19:48:22,440 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 89.405s
2025-10-02 19:48:22,524 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6749
2025-10-02 19:48:22,524 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-10-02 19:48:22,524 - INFO - bo.run_bo - Recorded observation #6: hparams={'epochs': np.int64(76), 'batch_size': np.int64(64), 'lr': 3.484014869142792e-05, 'weight_decay': 1.766206646836706e-05, 'patch_size': np.int64(200), 'n_heads': np.int64(4), 'head_dim': np.int64(6), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(3), 'dropout': 0.3261980222306933, 'attn_dropout': 0.10736236838165304, 'augment_noise_std': 0.004239407333070411, 'augment_scale_low': 0.9566291110304341, 'augment_scale_high': 1.1239495556872914, 'use_focal_loss': np.False_, 'focal_gamma': 1.142780621747404, 'class_weighted': np.True_, 'grad_clip': 1.9187354467022704, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.6749
2025-10-02 19:48:22,524 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'epochs': np.int64(76), 'batch_size': np.int64(64), 'lr': 3.484014869142792e-05, 'weight_decay': 1.766206646836706e-05, 'patch_size': np.int64(200), 'n_heads': np.int64(4), 'head_dim': np.int64(6), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(3), 'dropout': 0.3261980222306933, 'attn_dropout': 0.10736236838165304, 'augment_noise_std': 0.004239407333070411, 'augment_scale_low': 0.9566291110304341, 'augment_scale_high': 1.1239495556872914, 'use_focal_loss': np.False_, 'focal_gamma': 1.142780621747404, 'class_weighted': np.True_, 'grad_clip': 1.9187354467022704, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.6749
2025-10-02 19:48:22,524 - INFO - bo.run_bo - üîçBO Trial 7: Using RF surrogate + Expected Improvement
2025-10-02 19:48:22,524 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 19:48:22,524 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:48:22,525 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:48:22,525 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 97, 'batch_size': 128, 'lr': 0.003594152955888722, 'weight_decay': 0.0057855317043995595, 'patch_size': 20, 'n_heads': 2, 'head_dim': 11, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.09455431302900538, 'attn_dropout': 0.2709477116888663, 'augment_noise_std': 0.016158391484836315, 'augment_scale_low': 0.8827392814128312, 'augment_scale_high': 1.0434382943568896, 'use_focal_loss': False, 'focal_gamma': 3.4520011898119205, 'class_weighted': False, 'grad_clip': 4.899763191449977, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 19:48:22,526 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 97, 'batch_size': 128, 'lr': 0.003594152955888722, 'weight_decay': 0.0057855317043995595, 'patch_size': 20, 'n_heads': 2, 'head_dim': 11, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.09455431302900538, 'attn_dropout': 0.2709477116888663, 'augment_noise_std': 0.016158391484836315, 'augment_scale_low': 0.8827392814128312, 'augment_scale_high': 1.0434382943568896, 'use_focal_loss': False, 'focal_gamma': 3.4520011898119205, 'class_weighted': False, 'grad_clip': 4.899763191449977, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 19:50:19,163 - INFO - _models.training_function_executor - Model: 10,367 parameters, 22.3KB storage
2025-10-02 19:50:19,163 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5266242092819257, 0.2941110724896539, 0.24132042752431643, 0.22177978807849827, 0.20175836336600378, 0.19342642810617075, 0.18125974321884047, 0.17554498052328552, 0.1670110860093623, 0.15831107138435377, 0.15415770998696943, 0.15243915526452334, 0.15023300414007268, 0.1403311623979052, 0.14044219104629319, 0.13800860995557065, 0.1373861803704803, 0.1346130757376585, 0.12969212056175344, 0.1269594610893388, 0.12547172316771696, 0.12203933106576, 0.11916482180690263, 0.11855294930708703, 0.1264444352114921, 0.11823999961074408, 0.11650697239046949, 0.11257807859107728, 0.11219578728098933, 0.11385932490329966, 0.11184914966264665, 0.11212220167715448, 0.10769633562923882, 0.10854816649387912, 0.10722764260130778, 0.10644843583879354, 0.10445855708363189, 0.10970705627500721, 0.10222075810956502, 0.10315854160791202, 0.10368348710551446, 0.10188346449206975, 0.0979274437230575, 0.10083964698661273, 0.09981860478400728, 0.0986135247964681, 0.09933116015114402, 0.0966453017644822, 0.09530042281969894, 0.09696714679810896, 0.09607420583179066, 0.09530942128893091, 0.09204813364452204, 0.09386607956211135, 0.09677137183515895, 0.09868988353240103, 0.09146384773192162, 0.08989668638132604, 0.09336721758046806, 0.08919746217718176, 0.09138777300342486, 0.0843403435744308, 0.0893206473157994, 0.08741140602540391, 0.09028250957026024, 0.09097305584619679, 0.0890131768550923, 0.08772993698019255, 0.0887375109944917, 0.08599764175221304, 0.08965239059797928, 0.0817890769988135, 0.0915949965855145, 0.08229388943987562, 0.08309440355263299, 0.08760853787647865, 0.0894069259094898, 0.07957123460500935, 0.08111801859876758, 0.08468802614492929, 0.08087050970288419, 0.08208614622203335, 0.08585230637502428, 0.08109255696904505, 0.08141380474958616, 0.08110934127422889, 0.07802045985944767, 0.07747437449877742, 0.08338154003040905, 0.07958593039736617, 0.07818344611368454, 0.08173401903963118, 0.08171633070254447, 0.07725717592368866, 0.08153336750118743, 0.07898058884584026, 0.08282822679073722], 'val_losses': [0.4637063207475184, 0.3089675764148209, 0.24194670275046015, 0.21939640790653503, 0.20057419616093747, 0.21574512564692339, 0.21132149618287505, 0.1974697044874706, 0.1856625402721303, 0.18734772135187938, 0.19090087709197154, 0.17263416538164142, 0.1781988525888015, 0.16723090237340985, 0.1906455208300842, 0.16262894465841607, 0.17245600883665874, 0.16551081271058649, 0.15193289578476282, 0.15572948595972202, 0.16045356021703686, 0.15505761229824622, 0.18446826595566115, 0.15091113380872131, 0.14381136153817042, 0.13584232610570893, 0.13584737567439473, 0.13691789058385195, 0.16524192188791637, 0.14671348237540247, 0.13732349638644054, 0.1336091283576715, 0.14158891351274375, 0.15420615493732204, 0.18473331814137278, 0.15859886016680355, 0.14148828462223678, 0.1326851122484569, 0.16677155063655977, 0.14899443605906076, 0.12773162971861365, 0.1406853498139263, 0.16151010322895298, 0.1321333342029582, 0.16991503070678168, 0.1364172124830744, 0.13399482798309736, 0.13199920094910533, 0.12538283700378985, 0.1479078005140937, 0.13508754664029174, 0.12184200763642646, 0.1513062430716759, 0.12309703481699975, 0.1425860993813384, 0.13478054525394845, 0.15813480730543922, 0.12969474399609607, 0.13902604229124643, 0.14605838071394797, 0.1604896678394758, 0.13180341510702653, 0.1255472085202133, 0.1439671663833716, 0.13303590120901487, 0.11723095826387346, 0.15452127886155162, 0.12775284012546442, 0.1094954989610368, 0.14228123604370052, 0.13664230658937526, 0.1446290882023082, 0.1356279371352277, 0.12394237311531496, 0.12143092679321699, 0.13378027922048738, 0.1347229088544696, 0.1419388988118186, 0.1615831210804141, 0.12689444774479633, 0.11812505240381406, 0.13783258964214135, 0.13513679132696352, 0.11882255981513962, 0.12290762105620366, 0.143793144458787, 0.12218162951303177, 0.12587049158723893, 0.12737361086298388, 0.16661342372936933, 0.13961785054213569, 0.1439873879835385, 0.13035753727950647, 0.1257335830925859, 0.1246562977901991, 0.15193001062187644, 0.1293373419086955], 'val_acc': [0.8668086705926575, 0.9030196717203358, 0.9280791880716702, 0.9314622227791004, 0.9382282921939606, 0.930459842125047, 0.9340934719959905, 0.9417366244831474, 0.9423631123919308, 0.9459967422628743, 0.9428643027189575, 0.9466232301716577, 0.9439919809547676, 0.9482520987344945, 0.9367247212128806, 0.9497556697155745, 0.944994361608821, 0.9515098358601679, 0.9512592406966546, 0.952637514095978, 0.9487532890615211, 0.9508833479513845, 0.9431148978824708, 0.9555193584763814, 0.9565217391304348, 0.9581506076932715, 0.9591529883473249, 0.9576494173662449, 0.9520110261871946, 0.9550181681493547, 0.9581506076932715, 0.960656559328405, 0.9551434657311114, 0.9532640020047614, 0.9355970429770706, 0.9473750156621977, 0.9527628116777346, 0.9558952512216514, 0.9426137075554442, 0.9508833479513845, 0.9595288810925949, 0.9563964415486781, 0.954141085077058, 0.9579000125297582, 0.953765192331788, 0.9602806665831349, 0.9563964415486781, 0.9576494173662449, 0.9624107254729983, 0.9542663826588147, 0.9594035835108382, 0.9631625109635384, 0.9538904899135446, 0.9615336424007017, 0.9540157874953014, 0.9592782859290816, 0.9451196591905776, 0.9631625109635384, 0.9597794762561083, 0.9551434657311114, 0.952637514095978, 0.9596541786743515, 0.9639142964540784, 0.9556446560581381, 0.9572735246209748, 0.9636637012905651, 0.9538904899135446, 0.9641648916175918, 0.968299711815562, 0.9540157874953014, 0.9551434657311114, 0.9566470367121914, 0.9530134068412479, 0.960656559328405, 0.9627866182182684, 0.9626613206365117, 0.9579000125297582, 0.9558952512216514, 0.9469991229169277, 0.9636637012905651, 0.9660443553439418, 0.9597794762561083, 0.9609071544919183, 0.9631625109635384, 0.9642901891993485, 0.9601553690013783, 0.9620348327277284, 0.9651672722716451, 0.9634131061270518, 0.9490038842250345, 0.9557699536398947, 0.9553940608946248, 0.9585265004385415, 0.9609071544919183, 0.9645407843628618, 0.9556446560581381, 0.9629119158000251], 'size_bytes': 30537, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 97, 'batch_size': 128, 'lr': 0.003594152955888722, 'weight_decay': 0.0057855317043995595, 'patch_size': 20, 'n_heads': 2, 'head_dim': 11, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.09455431302900538, 'attn_dropout': 0.2709477116888663, 'augment_noise_std': 0.016158391484836315, 'augment_scale_low': 0.8827392814128312, 'augment_scale_high': 1.0434382943568896, 'use_focal_loss': False, 'focal_gamma': 3.4520011898119205, 'class_weighted': False, 'grad_clip': 4.899763191449977, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 10367, 'model_storage_size_kb': 22.2728515625, 'model_size_validation': 'PASS'}
2025-10-02 19:50:19,163 - INFO - _models.training_function_executor - BO Objective: base=0.9629, size_penalty=0.0000, final=0.9629
2025-10-02 19:50:19,163 - INFO - _models.training_function_executor - Model: 10,367 parameters, 22.3KB (PASS 256KB limit)
2025-10-02 19:50:19,163 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 116.639s
2025-10-02 19:50:19,246 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9629
2025-10-02 19:50:19,246 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.083s
2025-10-02 19:50:19,247 - INFO - bo.run_bo - Recorded observation #7: hparams={'epochs': np.int64(97), 'batch_size': np.int64(128), 'lr': 0.003594152955888722, 'weight_decay': 0.0057855317043995595, 'patch_size': np.int64(20), 'n_heads': np.int64(2), 'head_dim': np.int64(11), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.09455431302900538, 'attn_dropout': 0.2709477116888663, 'augment_noise_std': 0.016158391484836315, 'augment_scale_low': 0.8827392814128312, 'augment_scale_high': 1.0434382943568896, 'use_focal_loss': np.False_, 'focal_gamma': 3.4520011898119205, 'class_weighted': np.False_, 'grad_clip': 4.899763191449977, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9629
2025-10-02 19:50:19,247 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'epochs': np.int64(97), 'batch_size': np.int64(128), 'lr': 0.003594152955888722, 'weight_decay': 0.0057855317043995595, 'patch_size': np.int64(20), 'n_heads': np.int64(2), 'head_dim': np.int64(11), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.09455431302900538, 'attn_dropout': 0.2709477116888663, 'augment_noise_std': 0.016158391484836315, 'augment_scale_low': 0.8827392814128312, 'augment_scale_high': 1.0434382943568896, 'use_focal_loss': np.False_, 'focal_gamma': 3.4520011898119205, 'class_weighted': np.False_, 'grad_clip': 4.899763191449977, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9629
2025-10-02 19:50:19,247 - INFO - bo.run_bo - üîçBO Trial 8: Using RF surrogate + Expected Improvement
2025-10-02 19:50:19,247 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 19:50:19,247 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:50:19,247 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:50:19,247 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 94, 'batch_size': 128, 'lr': 0.0003574140069927762, 'weight_decay': 0.00046709431723535125, 'patch_size': 10, 'n_heads': 4, 'head_dim': 4, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.28126948328755336, 'attn_dropout': 0.23469959032321136, 'augment_noise_std': 0.03316451235143541, 'augment_scale_low': 0.8993273872538305, 'augment_scale_high': 1.0854996959182426, 'use_focal_loss': True, 'focal_gamma': 4.191550592012252, 'class_weighted': False, 'grad_clip': 0.39095139089052383, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:50:19,248 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 94, 'batch_size': 128, 'lr': 0.0003574140069927762, 'weight_decay': 0.00046709431723535125, 'patch_size': 10, 'n_heads': 4, 'head_dim': 4, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.28126948328755336, 'attn_dropout': 0.23469959032321136, 'augment_noise_std': 0.03316451235143541, 'augment_scale_low': 0.8993273872538305, 'augment_scale_high': 1.0854996959182426, 'use_focal_loss': True, 'focal_gamma': 4.191550592012252, 'class_weighted': False, 'grad_clip': 0.39095139089052383, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:53:46,600 - INFO - _models.training_function_executor - Model: 8,709 parameters, 18.7KB storage
2025-10-02 19:53:46,600 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.242034245285329, 0.15372861427229098, 0.11982384279705927, 0.10346834161302647, 0.09339154876318842, 0.08414600531741893, 0.07659853657594552, 0.07110574909241428, 0.06694079418633549, 0.062168607678139164, 0.06001897852622663, 0.05715110762965461, 0.05450994267775459, 0.05305180850163108, 0.050590712306252866, 0.04922532888083667, 0.048196690918673096, 0.04643589332412703, 0.04393471973955387, 0.0438911837746484, 0.0421413343967023, 0.04087393460097149, 0.04053106537137239, 0.03895875282780037, 0.03877756594423606, 0.03757733546373742, 0.03716773439049478, 0.03635687044197443, 0.035603005328065034, 0.035145493862766025, 0.03358782034559755, 0.03340884077238756, 0.032838937715797545, 0.03321423382413048, 0.03206446848908211, 0.03159850608172716, 0.03126224092343304, 0.03083342113370064, 0.030347798853616433, 0.03055865422487063, 0.02907522464080554, 0.028815148146916696, 0.027341246991883773, 0.02876973566758626, 0.02783307578681971, 0.02761085791024309, 0.02633956878377513, 0.02614404528827704, 0.02674553250993784, 0.025871698952926802, 0.02534732030632791, 0.025196932195073005, 0.025001152380890155, 0.02507364067355748, 0.02439051560600819, 0.024072911704802063, 0.02420622756034794, 0.023185569397386065, 0.02346504487698715, 0.023165737076846014, 0.02280764122307814, 0.021953697522492252, 0.022217792149616715, 0.02259458429763712, 0.021622744343862816, 0.021136035811464927, 0.0212281041590189, 0.02110826246225923, 0.020217758730767848, 0.020626680466114187, 0.02056637422389064, 0.02006877531580697, 0.01967965108280182, 0.019783741716019262, 0.019238936237628398, 0.01874528014711788, 0.018811481464897518, 0.018913753847087888, 0.018300890274124387, 0.01811910346309105, 0.018253109047958207, 0.017940293132180665, 0.018398697177744117, 0.018120962979267165, 0.01781442204667653, 0.017586298805789543, 0.017080799593375683, 0.017335845393288595, 0.017105574482160397, 0.01674779510057309, 0.01651584256973819, 0.01667884594586345, 0.01640074916527881, 0.016330794102973806], 'val_losses': [0.16656454973498797, 0.12541619541146615, 0.10295064473386292, 0.09056277422622068, 0.08361531657494335, 0.07406297424343009, 0.07019937748051858, 0.06506019674919293, 0.0654680251714849, 0.059664204530908024, 0.06089091247589221, 0.05856522649938287, 0.05627220023604073, 0.056941481321852186, 0.053181363510773336, 0.05157591963518033, 0.05022474243941014, 0.06186244019657624, 0.053561398453516644, 0.05425853279085853, 0.05118549179601768, 0.049411676227094206, 0.04741385739257933, 0.047110905281965064, 0.05082858321786136, 0.04493849257527923, 0.05006706239645112, 0.04418138208215718, 0.05703502330056648, 0.04427237543485738, 0.04928020497952289, 0.045367024427233386, 0.0447328139407834, 0.04489690055768452, 0.04996556136698789, 0.044825835709150025, 0.04604833665243323, 0.04990630288714679, 0.04676568251891537, 0.05315122095142864, 0.04637188188635516, 0.04699090346105862, 0.043279235366890115, 0.041402483843252964, 0.04250667913730634, 0.04838023352758909, 0.04670929704680656, 0.04570483928365436, 0.04059017126425931, 0.04821458386644476, 0.0403503725017436, 0.049642133315640755, 0.045692477114344585, 0.04294155816650379, 0.04409461438118134, 0.04608511207785138, 0.04600459267965849, 0.0410753515762668, 0.04406238268314485, 0.04142562951857607, 0.04280019965820512, 0.04584823086224856, 0.0419498319033841, 0.04042872287358217, 0.04684291769819381, 0.039839955840981885, 0.037848764483215606, 0.044019555315690864, 0.04458700489103906, 0.03975278420576609, 0.03999930589946794, 0.04315981124090739, 0.04184503002679672, 0.039849952915388155, 0.043172031964944756, 0.04291496567485058, 0.03573649605067955, 0.03885818443013349, 0.041077284743807965, 0.04003082553634009, 0.038491400256295535, 0.038686091490832054, 0.04040540878955859, 0.039433696366404994, 0.041084708778269086, 0.038257718505402015, 0.04250785617407305, 0.04111469323145777, 0.037305666198409033, 0.036959668477889125, 0.03804776686584572, 0.04469540750131976, 0.03467480291991347, 0.04055378463480082], 'val_acc': [0.7733366746021801, 0.7792256609447438, 0.8059140458589149, 0.8201979701791755, 0.8418744518230799, 0.868562836737251, 0.8701917053000877, 0.8943741385791254, 0.8867309860919684, 0.9057762185189826, 0.8973812805412855, 0.8933717579250721, 0.906402706427766, 0.8926199724345321, 0.9054003257737125, 0.9160506202230297, 0.9119158000250596, 0.8630497431399574, 0.8924946748527753, 0.8939982458338555, 0.892244079689262, 0.9040220523743892, 0.9092845508081694, 0.9062774088460093, 0.9022678862297958, 0.9164265129682997, 0.8975065781230422, 0.915549429896003, 0.876205989224408, 0.9124169903520862, 0.8907405087081819, 0.900639017666959, 0.9062774088460093, 0.9055256233554693, 0.8942488409973688, 0.899511339431149, 0.8972559829595289, 0.8872321764189951, 0.9022678862297958, 0.8644280165392808, 0.8891116401453452, 0.8961283047237188, 0.9050244330284426, 0.9174288936223531, 0.9042726475379025, 0.8792131311865681, 0.9070291943365493, 0.8874827715825084, 0.9089086580628993, 0.8902393183811552, 0.9115399072797895, 0.8835985465480516, 0.8793384287683248, 0.8924946748527753, 0.8946247337426387, 0.8783360481142714, 0.8889863425635885, 0.907154491918306, 0.8967547926325022, 0.8952512216514221, 0.886856283673725, 0.884225034456835, 0.8928705675980454, 0.9047738378649292, 0.886856283673725, 0.9106628242074928, 0.9178047863676231, 0.8946247337426387, 0.8819696779852149, 0.9135446685878963, 0.906402706427766, 0.8863550933466984, 0.8814684876581882, 0.8886104498183185, 0.8953765192331788, 0.8825961658939983, 0.914045858914923, 0.8901140207993986, 0.8839744392933216, 0.8815937852399449, 0.9000125297581757, 0.8958777095602055, 0.9059015161007392, 0.9015161007392557, 0.8772083698784613, 0.9025184813933091, 0.8884851522365619, 0.8881092594912918, 0.9091592532264128, 0.9047738378649292, 0.8886104498183185, 0.8817190828217015, 0.9219396065655933, 0.8948753289061521], 'size_bytes': 31141, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 94, 'batch_size': 128, 'lr': 0.0003574140069927762, 'weight_decay': 0.00046709431723535125, 'patch_size': 10, 'n_heads': 4, 'head_dim': 4, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.28126948328755336, 'attn_dropout': 0.23469959032321136, 'augment_noise_std': 0.03316451235143541, 'augment_scale_low': 0.8993273872538305, 'augment_scale_high': 1.0854996959182426, 'use_focal_loss': True, 'focal_gamma': 4.191550592012252, 'class_weighted': False, 'grad_clip': 0.39095139089052383, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 8709, 'model_storage_size_kb': 18.710742187500003, 'model_size_validation': 'PASS'}
2025-10-02 19:53:46,600 - INFO - _models.training_function_executor - BO Objective: base=0.8949, size_penalty=0.0000, final=0.8949
2025-10-02 19:53:46,600 - INFO - _models.training_function_executor - Model: 8,709 parameters, 18.7KB (PASS 256KB limit)
2025-10-02 19:53:46,601 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 207.354s
2025-10-02 19:53:46,687 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8949
2025-10-02 19:53:46,687 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.086s
2025-10-02 19:53:46,687 - INFO - bo.run_bo - Recorded observation #8: hparams={'epochs': np.int64(94), 'batch_size': np.int64(128), 'lr': 0.0003574140069927762, 'weight_decay': 0.00046709431723535125, 'patch_size': np.int64(10), 'n_heads': np.int64(4), 'head_dim': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.28126948328755336, 'attn_dropout': 0.23469959032321136, 'augment_noise_std': 0.03316451235143541, 'augment_scale_low': 0.8993273872538305, 'augment_scale_high': 1.0854996959182426, 'use_focal_loss': np.True_, 'focal_gamma': 4.191550592012252, 'class_weighted': np.False_, 'grad_clip': 0.39095139089052383, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.8949
2025-10-02 19:53:46,687 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'epochs': np.int64(94), 'batch_size': np.int64(128), 'lr': 0.0003574140069927762, 'weight_decay': 0.00046709431723535125, 'patch_size': np.int64(10), 'n_heads': np.int64(4), 'head_dim': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.28126948328755336, 'attn_dropout': 0.23469959032321136, 'augment_noise_std': 0.03316451235143541, 'augment_scale_low': 0.8993273872538305, 'augment_scale_high': 1.0854996959182426, 'use_focal_loss': np.True_, 'focal_gamma': 4.191550592012252, 'class_weighted': np.False_, 'grad_clip': 0.39095139089052383, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.8949
2025-10-02 19:53:46,687 - INFO - bo.run_bo - üîçBO Trial 9: Using RF surrogate + Expected Improvement
2025-10-02 19:53:46,687 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 19:53:46,687 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:53:46,687 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:53:46,687 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 95, 'batch_size': 128, 'lr': 0.006044748704311133, 'weight_decay': 0.0023108388474443083, 'patch_size': 5, 'n_heads': 1, 'head_dim': 15, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.42597447327912635, 'attn_dropout': 0.20453572911336576, 'augment_noise_std': 0.02133138880680528, 'augment_scale_low': 0.9093461200797417, 'augment_scale_high': 1.0597230021057606, 'use_focal_loss': False, 'focal_gamma': 4.758645348831133, 'class_weighted': False, 'grad_clip': 4.987183503499593, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:53:46,688 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 95, 'batch_size': 128, 'lr': 0.006044748704311133, 'weight_decay': 0.0023108388474443083, 'patch_size': 5, 'n_heads': 1, 'head_dim': 15, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.42597447327912635, 'attn_dropout': 0.20453572911336576, 'augment_noise_std': 0.02133138880680528, 'augment_scale_low': 0.9093461200797417, 'augment_scale_high': 1.0597230021057606, 'use_focal_loss': False, 'focal_gamma': 4.758645348831133, 'class_weighted': False, 'grad_clip': 4.987183503499593, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 19:56:34,018 - INFO - _models.training_function_executor - Model: 5,690 parameters, 24.4KB storage
2025-10-02 19:56:34,019 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7200239841058895, 0.5380552487604056, 0.40806073128674836, 0.3486563004344432, 0.3287205903885736, 0.30226053189459073, 0.2888459873386924, 0.2821549724007827, 0.27810300210073574, 0.27098384903971945, 0.26710757698323556, 0.26094471514402384, 0.254225849035649, 0.2515266432140352, 0.24853769721096255, 0.24414481552132716, 0.2418165632836021, 0.23941408928060234, 0.23336696429266515, 0.23391450714472, 0.23271415910393553, 0.22940418522599473, 0.22937251923059837, 0.225005937300204, 0.22566724254669515, 0.22068167223845367, 0.21757804623273172, 0.2173423452813184, 0.21587582007930403, 0.21684238381612175, 0.21002242077869368, 0.20803445953009544, 0.21350127489753593, 0.20945213933440335, 0.20825229005634466, 0.20435751789479284, 0.20373379641093173, 0.19714221502722376, 0.2026350588680968, 0.19889947329234298, 0.20109931359368227, 0.19644656972777882, 0.1919891586139739, 0.19719289200510504, 0.19467413572669384, 0.19246747634885522, 0.19059265041104484, 0.19468732978513215, 0.18950046335691512, 0.187466071882606, 0.19215342496258533, 0.18586603750883215, 0.18764555173156533, 0.1827906941222438, 0.18789345420542483, 0.18393193901050092, 0.18770173102588322, 0.18519889877211296, 0.18277455216666172, 0.1808192285636469, 0.18453214569320409, 0.18627723804966637, 0.17947346626913116, 0.17837229283081987, 0.17881347903671918, 0.1761654646751468, 0.1761745082791712, 0.18162628669407613, 0.17995254464123783, 0.177703057325023, 0.17636540294052033, 0.17857519116754336, 0.17327937545403088, 0.17521295358264924, 0.17762429965594334, 0.17454391133916694, 0.17180567131373467, 0.173391810579452, 0.17852412848027208, 0.17193567525910855, 0.17059523544273658, 0.1737158751625807, 0.16812426385846693, 0.16908700108907165, 0.17122515016132864, 0.16878231832863144, 0.16900090548452779, 0.170091785118719, 0.16650410079410477, 0.17297539536201748, 0.16917322637778237, 0.16695764132995367, 0.16531680411574082, 0.16378586165016726, 0.16590689000659384], 'val_losses': [0.6453340043028022, 0.7088771338541098, 0.5355120101842556, 0.4771299980783385, 0.414233675331359, 0.3887694208241991, 0.4598112235226408, 0.44612226883072464, 0.4402277881811472, 0.48764309872960765, 0.6138464534078052, 0.43811583416292565, 0.5622564177661892, 0.48167990917117504, 0.48971992053344515, 0.5090347273414587, 0.4282424906496266, 0.44483394555438505, 0.5100017257791879, 0.5503301279069248, 0.5094366545419379, 0.46859923648126234, 0.4317664051097731, 0.4616701092999048, 0.629990558974681, 0.5159022081370283, 0.39981233440325564, 0.5997098088787247, 0.49675025149494645, 0.47978826042494815, 0.5578165643048786, 0.54089346023195, 0.42920664722811025, 0.47668620102717124, 0.5239190460857092, 0.420212450555924, 0.5196878996622502, 0.5662311442049803, 0.6073534856641999, 0.49054568712461294, 0.4858652546454784, 0.564360405068367, 0.7106453958250439, 0.6463653825381214, 0.5858750488282803, 0.5337434732671997, 0.6143191908291894, 0.5330918533747364, 0.6258145926321618, 0.5424576725798164, 0.6066107603074494, 0.620521275314707, 0.5711110991249256, 0.5530765783296105, 0.5384972307216314, 0.6620571252816065, 0.5135026824547524, 0.6375734683804428, 0.49990890664185605, 0.6745791243940737, 0.5200574291206485, 0.48840662031570425, 0.4837495217069679, 0.5189461432002478, 0.5139869162088169, 0.7123329484392535, 0.6786695990014563, 0.6001338239615909, 0.5324952321801772, 0.6391133334258493, 0.49827833231895613, 0.5931858435271303, 0.5031756891551254, 0.5263207759299743, 0.6475526502804265, 0.5384449434189282, 0.5313991752035739, 0.6224474997960181, 0.7108068284314313, 0.6243491235244335, 0.5965441062910158, 0.5309026308442668, 0.749476396522939, 0.7333802087588382, 0.8621564017114686, 0.7002111424641447, 0.6391234486686482, 0.5468112957621708, 0.7787616255923838, 0.8930813915119212, 0.6647339040900573, 0.6946986369296672, 0.5478708966175516, 1.0252431837512561, 0.8116493457138292], 'val_acc': [0.7789750657812304, 0.7901265505575742, 0.862423255231174, 0.8854780102744017, 0.8990101491041222, 0.9033955644656058, 0.8813431900764315, 0.8907405087081819, 0.8852274151108883, 0.8768324771331913, 0.8458839744392933, 0.8966294950507455, 0.8599173035960406, 0.8768324771331913, 0.9042726475379025, 0.8828467610575116, 0.9008896128304724, 0.8874827715825084, 0.8714446811176544, 0.8673098609196842, 0.8835985465480516, 0.8863550933466984, 0.8980077684500689, 0.8931211627615587, 0.853652424508207, 0.8709434907906277, 0.9052750281919559, 0.8517729607818569, 0.8718205738629244, 0.8944994361608821, 0.8635509334669841, 0.8675604560831975, 0.8997619345946624, 0.8817190828217015, 0.8665580754291442, 0.9060268136824959, 0.8764565843879213, 0.8641774213757675, 0.8341060017541662, 0.869314622227791, 0.8813431900764315, 0.8714446811176544, 0.8344818944994361, 0.8414985590778098, 0.8523994486906402, 0.8822202731487282, 0.8594161132690139, 0.8683122415737377, 0.8574113519609071, 0.8686881343190076, 0.8592908156872572, 0.8649292068663075, 0.8547801027440171, 0.8779601553690014, 0.8699411101365744, 0.84751284300213, 0.8802155118406215, 0.8584137326149606, 0.8884851522365619, 0.8582884350332038, 0.8869815812554818, 0.8977571732865556, 0.8992607442676356, 0.8923693772710186, 0.8894875328906152, 0.8610449818318506, 0.8567848640521238, 0.8743265254980579, 0.8908658062899386, 0.8669339681744143, 0.8986342563588523, 0.8673098609196842, 0.891868186943992, 0.8881092594912918, 0.8552812930710437, 0.8814684876581882, 0.8936223530885854, 0.8706928956271144, 0.8661821826838741, 0.8799649166771081, 0.8684375391554943, 0.8958777095602055, 0.8498934970555069, 0.8581631374514472, 0.8542789124169904, 0.8643027189575241, 0.8689387294825209, 0.884225034456835, 0.8462598671845634, 0.8438792131311865, 0.8825961658939983, 0.8638015286304974, 0.8960030071419621, 0.8347324896629496, 0.8458839744392933], 'size_bytes': 28845, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 95, 'batch_size': 128, 'lr': 0.006044748704311133, 'weight_decay': 0.0023108388474443083, 'patch_size': 5, 'n_heads': 1, 'head_dim': 15, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.42597447327912635, 'attn_dropout': 0.20453572911336576, 'augment_noise_std': 0.02133138880680528, 'augment_scale_low': 0.9093461200797417, 'augment_scale_high': 1.0597230021057606, 'use_focal_loss': False, 'focal_gamma': 4.758645348831133, 'class_weighted': False, 'grad_clip': 4.987183503499593, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 5690, 'model_storage_size_kb': 24.449218750000004, 'model_size_validation': 'PASS'}
2025-10-02 19:56:34,019 - INFO - _models.training_function_executor - BO Objective: base=0.8459, size_penalty=0.0000, final=0.8459
2025-10-02 19:56:34,019 - INFO - _models.training_function_executor - Model: 5,690 parameters, 24.4KB (PASS 256KB limit)
2025-10-02 19:56:34,019 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 167.332s
2025-10-02 19:56:34,107 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8459
2025-10-02 19:56:34,107 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.089s
2025-10-02 19:56:34,107 - INFO - bo.run_bo - Recorded observation #9: hparams={'epochs': np.int64(95), 'batch_size': np.int64(128), 'lr': 0.006044748704311133, 'weight_decay': 0.0023108388474443083, 'patch_size': np.int64(5), 'n_heads': np.int64(1), 'head_dim': np.int64(15), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(3), 'dropout': 0.42597447327912635, 'attn_dropout': 0.20453572911336576, 'augment_noise_std': 0.02133138880680528, 'augment_scale_low': 0.9093461200797417, 'augment_scale_high': 1.0597230021057606, 'use_focal_loss': np.False_, 'focal_gamma': 4.758645348831133, 'class_weighted': np.False_, 'grad_clip': 4.987183503499593, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.8459
2025-10-02 19:56:34,107 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'epochs': np.int64(95), 'batch_size': np.int64(128), 'lr': 0.006044748704311133, 'weight_decay': 0.0023108388474443083, 'patch_size': np.int64(5), 'n_heads': np.int64(1), 'head_dim': np.int64(15), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(3), 'dropout': 0.42597447327912635, 'attn_dropout': 0.20453572911336576, 'augment_noise_std': 0.02133138880680528, 'augment_scale_low': 0.9093461200797417, 'augment_scale_high': 1.0597230021057606, 'use_focal_loss': np.False_, 'focal_gamma': 4.758645348831133, 'class_weighted': np.False_, 'grad_clip': 4.987183503499593, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.8459
2025-10-02 19:56:34,108 - INFO - bo.run_bo - üîçBO Trial 10: Using RF surrogate + Expected Improvement
2025-10-02 19:56:34,108 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 19:56:34,108 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:56:34,108 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:56:34,108 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 26, 'batch_size': 128, 'lr': 0.00010419721302468964, 'weight_decay': 0.000503418779148111, 'patch_size': 100, 'n_heads': 3, 'head_dim': 19, 'num_layers': 1, 'mlp_ratio': 4, 'dropout': 0.30087597564119134, 'attn_dropout': 0.2929636314752339, 'augment_noise_std': 0.04934756168064717, 'augment_scale_low': 0.9127425292594538, 'augment_scale_high': 1.061867724578755, 'use_focal_loss': False, 'focal_gamma': 3.0914784944404894, 'class_weighted': True, 'grad_clip': 1.6744246935170624, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:56:34,109 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 26, 'batch_size': 128, 'lr': 0.00010419721302468964, 'weight_decay': 0.000503418779148111, 'patch_size': 100, 'n_heads': 3, 'head_dim': 19, 'num_layers': 1, 'mlp_ratio': 4, 'dropout': 0.30087597564119134, 'attn_dropout': 0.2929636314752339, 'augment_noise_std': 0.04934756168064717, 'augment_scale_low': 0.9127425292594538, 'augment_scale_high': 1.061867724578755, 'use_focal_loss': False, 'focal_gamma': 3.0914784944404894, 'class_weighted': True, 'grad_clip': 1.6744246935170624, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:56:57,029 - INFO - _models.training_function_executor - Model: 52,103 parameters, 111.9KB storage
2025-10-02 19:56:57,029 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.466433867406521, 1.2300961754455964, 1.0639085177023735, 0.9786968730809944, 0.9218620856080189, 0.8787572662831025, 0.8485736252333748, 0.8264129725374293, 0.8003512678191024, 0.782556681186675, 0.7594848570397813, 0.7464712662598701, 0.730585076142884, 0.7207203514910624, 0.7027055837193688, 0.6980117196480037, 0.6767189603280115, 0.6671502873760355, 0.6590134105287884, 0.6528278798032022, 0.6459658997356789, 0.6252336356690499, 0.6196685363836868, 0.6117535766621518, 0.6070748278212137, 0.5913513165829799], 'val_losses': [1.3398931450957927, 1.0740759915060425, 0.9522999600798934, 0.8873131067781815, 0.847862868870041, 0.8127153687960461, 0.8129591556991077, 0.7743193160664571, 0.7526845382786741, 0.7483886196385976, 0.7349886428501773, 0.7229068172805486, 0.7151686385705229, 0.6990084887342545, 0.6842608715474882, 0.676775488850766, 0.6683242518775461, 0.6708327648693749, 0.6541761652581779, 0.6505196049628349, 0.6451227184398598, 0.6307862118519483, 0.6310723130901033, 0.6124298381799446, 0.6137671245995089, 0.6174042299641895], 'val_acc': [0.37388798396190953, 0.3613582257862423, 0.42425761182809174, 0.4787620598922441, 0.500187946372635, 0.49818318506452824, 0.49718080441047485, 0.5275028191955895, 0.567096855030698, 0.5535647162009774, 0.5972935722340559, 0.6059391053752663, 0.6033078561583761, 0.61709059015161, 0.6487908783360481, 0.6273649918556572, 0.631625109635384, 0.6637012905650921, 0.668713193835359, 0.6583134945495552, 0.6522992106252349, 0.6801152737752162, 0.6520486154617215, 0.6877584262623732, 0.6870066407718332, 0.7081819320887107], 'size_bytes': 110381, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 26, 'batch_size': 128, 'lr': 0.00010419721302468964, 'weight_decay': 0.000503418779148111, 'patch_size': 100, 'n_heads': 3, 'head_dim': 19, 'num_layers': 1, 'mlp_ratio': 4, 'dropout': 0.30087597564119134, 'attn_dropout': 0.2929636314752339, 'augment_noise_std': 0.04934756168064717, 'augment_scale_low': 0.9127425292594538, 'augment_scale_high': 1.061867724578755, 'use_focal_loss': False, 'focal_gamma': 3.0914784944404894, 'class_weighted': True, 'grad_clip': 1.6744246935170624, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 52103, 'model_storage_size_kb': 111.9400390625, 'model_size_validation': 'PASS'}
2025-10-02 19:56:57,029 - INFO - _models.training_function_executor - BO Objective: base=0.7082, size_penalty=0.0000, final=0.7082
2025-10-02 19:56:57,029 - INFO - _models.training_function_executor - Model: 52,103 parameters, 111.9KB (PASS 256KB limit)
2025-10-02 19:56:57,029 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 22.922s
2025-10-02 19:56:57,121 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7082
2025-10-02 19:56:57,121 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.091s
2025-10-02 19:56:57,121 - INFO - bo.run_bo - Recorded observation #10: hparams={'epochs': np.int64(26), 'batch_size': np.int64(128), 'lr': 0.00010419721302468964, 'weight_decay': 0.000503418779148111, 'patch_size': np.int64(100), 'n_heads': np.int64(3), 'head_dim': np.int64(19), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'dropout': 0.30087597564119134, 'attn_dropout': 0.2929636314752339, 'augment_noise_std': 0.04934756168064717, 'augment_scale_low': 0.9127425292594538, 'augment_scale_high': 1.061867724578755, 'use_focal_loss': np.False_, 'focal_gamma': 3.0914784944404894, 'class_weighted': np.True_, 'grad_clip': 1.6744246935170624, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7082
2025-10-02 19:56:57,121 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'epochs': np.int64(26), 'batch_size': np.int64(128), 'lr': 0.00010419721302468964, 'weight_decay': 0.000503418779148111, 'patch_size': np.int64(100), 'n_heads': np.int64(3), 'head_dim': np.int64(19), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'dropout': 0.30087597564119134, 'attn_dropout': 0.2929636314752339, 'augment_noise_std': 0.04934756168064717, 'augment_scale_low': 0.9127425292594538, 'augment_scale_high': 1.061867724578755, 'use_focal_loss': np.False_, 'focal_gamma': 3.0914784944404894, 'class_weighted': np.True_, 'grad_clip': 1.6744246935170624, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7082
2025-10-02 19:56:57,121 - INFO - bo.run_bo - üîçBO Trial 11: Using RF surrogate + Expected Improvement
2025-10-02 19:56:57,121 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 19:56:57,121 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:56:57,121 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:56:57,121 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 78, 'batch_size': 512, 'lr': 0.0008356098776240092, 'weight_decay': 3.5586923213250475e-06, 'patch_size': 25, 'n_heads': 3, 'head_dim': 24, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.1078934795184818, 'attn_dropout': 0.10151156939795246, 'augment_noise_std': 0.006890426305091936, 'augment_scale_low': 0.8196137355392269, 'augment_scale_high': 1.1890694545733382, 'use_focal_loss': True, 'focal_gamma': 3.6233115079275393, 'class_weighted': False, 'grad_clip': 0.5870589619196049, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:56:57,122 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 78, 'batch_size': 512, 'lr': 0.0008356098776240092, 'weight_decay': 3.5586923213250475e-06, 'patch_size': 25, 'n_heads': 3, 'head_dim': 24, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.1078934795184818, 'attn_dropout': 0.10151156939795246, 'augment_noise_std': 0.006890426305091936, 'augment_scale_low': 0.8196137355392269, 'augment_scale_high': 1.1890694545733382, 'use_focal_loss': True, 'focal_gamma': 3.6233115079275393, 'class_weighted': False, 'grad_clip': 0.5870589619196049, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 19:59:10,524 - INFO - _models.training_function_executor - Model: 91,872 parameters, 197.4KB storage
2025-10-02 19:59:10,524 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.19935377769626605, 0.08524942580443988, 0.06051754164723467, 0.04554518159927366, 0.03872361318228262, 0.03421561504358472, 0.028447438335778497, 0.0261647688587895, 0.024526017747978832, 0.023375458592345973, 0.019209028157688164, 0.018319958060780527, 0.018190810652863123, 0.016758076710269333, 0.01384062446282516, 0.013092016839047604, 0.015240724852718643, 0.012616078488159852, 0.011491838619098966, 0.010618533844686816, 0.009642381307936248, 0.012474730207924938, 0.010156537217244679, 0.010022970068342902, 0.01001579498320655, 0.007917305790419457, 0.008524313658808227, 0.00842866137406382, 0.008278318991292881, 0.008020961040062802, 0.007684301363265572, 0.0061716380297989, 0.006269668570318779, 0.007892067865981003, 0.005825585103101455, 0.005210196902672088, 0.006512615390330706, 0.006717405589863681, 0.007285791906026586, 0.006889015532971918, 0.00538001458658259, 0.005165327608050773, 0.007206862828840242, 0.005438184503652488, 0.003958028560754105, 0.004656415964621671, 0.004514156298148429, 0.004652083148769688, 0.005944731105931195, 0.004971976957582627, 0.00452183118250549, 0.005004656578673528, 0.005596077048999533, 0.0048273593409259, 0.0052154610762096335, 0.004004267617285016, 0.0038842377003876407, 0.005703334612102908, 0.00615692655542338, 0.005204528039937167, 0.004717350055465975, 0.005015532380428628, 0.0031899851072171243, 0.0025413335235909325, 0.0021191198923611776, 0.002302822070031875, 0.004024089375615313, 0.0042092508137072295, 0.00444065057955913, 0.003716042012065398, 0.0030437320353216713, 0.003263655134617019, 0.003320828123967527, 0.005267207161979246, 0.004008995443674725, 0.002771732183480793, 0.0030462341924204885, 0.0023262355001423913], 'val_losses': [0.11173057647825946, 0.0783576366783697, 0.05891642547735907, 0.05314422556340448, 0.04180242851201028, 0.03882881717180163, 0.03204063118011032, 0.03295610239198334, 0.03793548838857525, 0.029913290076460954, 0.03014140108187401, 0.030328730364675406, 0.03331881807434381, 0.028488367215982656, 0.026802467014474866, 0.0326391292950415, 0.02615111426187404, 0.025408979537040872, 0.02664543403871054, 0.027139726539616552, 0.02556975143885175, 0.024884470649900972, 0.02678217361600062, 0.026570015388885516, 0.024553254506235776, 0.027460487241334087, 0.0288964207485447, 0.027113113709866753, 0.025688931317466676, 0.02871715803949804, 0.02681637162480598, 0.02532361905245595, 0.029101640976990632, 0.030285882610580763, 0.029453150662895194, 0.033782567412768326, 0.03036318018812163, 0.027553886934823597, 0.029953348597978654, 0.026278620642978527, 0.030379520081074193, 0.03155107435612448, 0.029859912594694674, 0.02752007689392816, 0.029746894362336965, 0.028002284168020822, 0.0350567733481771, 0.030918351447499347, 0.029031415199568034, 0.030684451235705876, 0.02995276843430912, 0.03079292026920555, 0.030186321355786156, 0.02915896468640285, 0.03458688601892539, 0.030474884458172816, 0.03096363092556198, 0.029702661708118683, 0.033376406727699676, 0.029633163712202735, 0.028534819742963662, 0.029364859895581606, 0.029042236505071855, 0.028542596813678264, 0.032932798989287984, 0.03940253253326487, 0.03498979298335585, 0.03485683582339456, 0.03288823358370104, 0.03026926341420979, 0.032668704387090394, 0.032859530888498766, 0.03361868599560189, 0.03553822791338519, 0.03300978705266442, 0.03189609416716978, 0.031135391882731494, 0.03125812839625653], 'val_acc': [0.8560330785615837, 0.9000125297581757, 0.92319258238316, 0.9230672848014033, 0.945746147099361, 0.9458714446811176, 0.9520110261871946, 0.9382282921939606, 0.9215637138203233, 0.9527628116777346, 0.9540157874953014, 0.9522616213507079, 0.940358351083824, 0.9555193584763814, 0.9616589399824583, 0.9413607317378775, 0.9615336424007017, 0.9601553690013783, 0.9582759052750282, 0.9600300714196216, 0.9589023931838115, 0.9546422754040848, 0.9551434657311114, 0.9629119158000251, 0.9585265004385415, 0.963287808545295, 0.963287808545295, 0.962536023054755, 0.9604059641648917, 0.9594035835108382, 0.9587770956020549, 0.9585265004385415, 0.9619095351459717, 0.9575241197844881, 0.9654178674351584, 0.946497932589901, 0.9604059641648917, 0.9627866182182684, 0.9615336424007017, 0.9620348327277284, 0.9650419746898885, 0.9652925698534018, 0.9655431650169152, 0.9645407843628618, 0.9566470367121914, 0.9619095351459717, 0.9622854278912417, 0.962536023054755, 0.9637889988723217, 0.963287808545295, 0.9679238190702919, 0.9631625109635384, 0.9650419746898885, 0.9665455456709685, 0.9647913795263752, 0.9634131061270518, 0.9649166771081318, 0.9629119158000251, 0.9660443553439418, 0.9637889988723217, 0.968299711815562, 0.9661696529256986, 0.9664202480892119, 0.963287808545295, 0.9624107254729983, 0.9666708432527252, 0.9655431650169152, 0.9630372133817817, 0.9652925698534018, 0.9677985214885353, 0.9651672722716451, 0.9645407843628618, 0.9640395940358351, 0.9622854278912417, 0.9660443553439418, 0.9664202480892119, 0.9684250093973187, 0.9695526876331286], 'size_bytes': 335435, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 78, 'batch_size': 512, 'lr': 0.0008356098776240092, 'weight_decay': 3.5586923213250475e-06, 'patch_size': 25, 'n_heads': 3, 'head_dim': 24, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.1078934795184818, 'attn_dropout': 0.10151156939795246, 'augment_noise_std': 0.006890426305091936, 'augment_scale_low': 0.8196137355392269, 'augment_scale_high': 1.1890694545733382, 'use_focal_loss': True, 'focal_gamma': 3.6233115079275393, 'class_weighted': False, 'grad_clip': 0.5870589619196049, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 91872, 'model_storage_size_kb': 197.38125000000002, 'model_size_validation': 'PASS'}
2025-10-02 19:59:10,524 - INFO - _models.training_function_executor - BO Objective: base=0.9696, size_penalty=0.0000, final=0.9696
2025-10-02 19:59:10,524 - INFO - _models.training_function_executor - Model: 91,872 parameters, 197.4KB (PASS 256KB limit)
2025-10-02 19:59:10,524 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 133.403s
2025-10-02 19:59:10,616 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9696
2025-10-02 19:59:10,616 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.092s
2025-10-02 19:59:10,616 - INFO - bo.run_bo - Recorded observation #11: hparams={'epochs': np.int64(78), 'batch_size': np.int64(512), 'lr': 0.0008356098776240092, 'weight_decay': 3.5586923213250475e-06, 'patch_size': np.int64(25), 'n_heads': np.int64(3), 'head_dim': np.int64(24), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.1078934795184818, 'attn_dropout': 0.10151156939795246, 'augment_noise_std': 0.006890426305091936, 'augment_scale_low': 0.8196137355392269, 'augment_scale_high': 1.1890694545733382, 'use_focal_loss': np.True_, 'focal_gamma': 3.6233115079275393, 'class_weighted': np.False_, 'grad_clip': 0.5870589619196049, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9696
2025-10-02 19:59:10,616 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 11: {'epochs': np.int64(78), 'batch_size': np.int64(512), 'lr': 0.0008356098776240092, 'weight_decay': 3.5586923213250475e-06, 'patch_size': np.int64(25), 'n_heads': np.int64(3), 'head_dim': np.int64(24), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.1078934795184818, 'attn_dropout': 0.10151156939795246, 'augment_noise_std': 0.006890426305091936, 'augment_scale_low': 0.8196137355392269, 'augment_scale_high': 1.1890694545733382, 'use_focal_loss': np.True_, 'focal_gamma': 3.6233115079275393, 'class_weighted': np.False_, 'grad_clip': 0.5870589619196049, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9696
2025-10-02 19:59:10,616 - INFO - bo.run_bo - üîçBO Trial 12: Using RF surrogate + Expected Improvement
2025-10-02 19:59:10,616 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 19:59:10,616 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 19:59:10,616 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 19:59:10,616 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 82, 'batch_size': 512, 'lr': 0.0005821349344707373, 'weight_decay': 1.4321368479220654e-06, 'patch_size': 250, 'n_heads': 2, 'head_dim': 31, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.023765069214410135, 'attn_dropout': 0.2652242710488922, 'augment_noise_std': 0.020666739956162736, 'augment_scale_low': 0.889988240950007, 'augment_scale_high': 1.1745141846938907, 'use_focal_loss': True, 'focal_gamma': 1.2615536235679103, 'class_weighted': False, 'grad_clip': 4.440311439064104, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 19:59:10,618 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 82, 'batch_size': 512, 'lr': 0.0005821349344707373, 'weight_decay': 1.4321368479220654e-06, 'patch_size': 250, 'n_heads': 2, 'head_dim': 31, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.023765069214410135, 'attn_dropout': 0.2652242710488922, 'augment_noise_std': 0.020666739956162736, 'augment_scale_low': 0.889988240950007, 'augment_scale_high': 1.1745141846938907, 'use_focal_loss': True, 'focal_gamma': 1.2615536235679103, 'class_weighted': False, 'grad_clip': 4.440311439064104, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:00:13,051 - INFO - _models.training_function_executor - Model: 94,860 parameters, 407.6KB storage
2025-10-02 20:00:13,051 - WARNING - _models.training_function_executor - Model storage 407.6KB exceeds 256KB limit!
2025-10-02 20:00:13,051 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5347765926332942, 0.3797131099449387, 0.31610249731488627, 0.2580347953513799, 0.20418740502426402, 0.16563080599484795, 0.14157633537883632, 0.12205691303911616, 0.1103689499803643, 0.09853697752184622, 0.09069296627671028, 0.083003280153793, 0.07711575941652637, 0.0707247079121709, 0.06638455700851736, 0.0628889206958742, 0.05818960985764373, 0.05533342185036, 0.051104692749288624, 0.04689668778928622, 0.045299685488136156, 0.0457810235469091, 0.037993185241753845, 0.036796437331409644, 0.03593077581173136, 0.03449344001023075, 0.03309972876804767, 0.03164917879763266, 0.029121756762856874, 0.028131256301934915, 0.026173568377004353, 0.024816499791774027, 0.024170385190695257, 0.02233210261584988, 0.022126335552365966, 0.023308212360103386, 0.01947701686371076, 0.018844176361598294, 0.018507092655278258, 0.018160840729054578, 0.01642509397357694, 0.016899329828026007, 0.015977024137567682, 0.016489995641337636, 0.01537298427083265, 0.016160301503206083, 0.013136555220080703, 0.012383847073242393, 0.013656623791721795, 0.013768102183812695, 0.010987591992764245, 0.009903490843347447, 0.010273004702909093, 0.011478914478017419, 0.00857310194481999, 0.011070662418113905, 0.012027296180116345, 0.009318969835892296, 0.010664252729494388, 0.009608821203986487, 0.009584193435446732, 0.012009655003024935, 0.009174958917792849, 0.009139035618125168, 0.006705957620517243, 0.006994924146364727, 0.009342400476886585, 0.008924185331510348, 0.007044174929390105, 0.006885836194743583, 0.007872547365195123, 0.0077223270337310876, 0.006478799881915489, 0.0077172667320673126, 0.0073877948705092155, 0.009496374272973049, 0.010463247309886035, 0.00961150528649927, 0.007466326154699266, 0.005207547288072876, 0.004966999877426309, 0.005123623902418605], 'val_losses': [0.40581535342686337, 0.34518089746341507, 0.27762836011754677, 0.21190505307422158, 0.17374508851356577, 0.15603808442343298, 0.1354508847520997, 0.11913034775258305, 0.11813135890883172, 0.10582574436578994, 0.10248711510471437, 0.10035502051757461, 0.0990633270619821, 0.09699993220513484, 0.09530909023847575, 0.08684741515494009, 0.08768266850382056, 0.09054224566792116, 0.09444450802215133, 0.08442871445828909, 0.08442397822754646, 0.08374840648845538, 0.08409334547969874, 0.08559054693692819, 0.08523300257620545, 0.09052318504390344, 0.08567685386229691, 0.0846765876234571, 0.09257107946679581, 0.08836483721531598, 0.0849224952701981, 0.08536020864177925, 0.0824833513706523, 0.08227675676891012, 0.09186138921377504, 0.08525622591830005, 0.08425368171648757, 0.09464706679253691, 0.09381644773186038, 0.0894535901947975, 0.08821788709248954, 0.09544324185178776, 0.09674122847234855, 0.09108061469179275, 0.0928369151499991, 0.08555968200920698, 0.09489324582581263, 0.0906122248321529, 0.09498874996314445, 0.09589092949793432, 0.08993504543962252, 0.09032801440030676, 0.09863119194504164, 0.09386644579443085, 0.10293568722374412, 0.10822145548989758, 0.09257285980828169, 0.09513234983032291, 0.0920886200758288, 0.09655706750436947, 0.09225026418871188, 0.0917755686630732, 0.09449154282890306, 0.09264869268298852, 0.0950217388141814, 0.10175733770505331, 0.10008157122599692, 0.107155903492367, 0.09440540802865649, 0.09795834374463644, 0.08921004165056848, 0.09684703691201227, 0.10170608261163007, 0.10411964586456682, 0.11000614933592083, 0.09984365723445561, 0.10150486009227888, 0.09891489662520346, 0.0876866081871196, 0.09622099414204553, 0.1021363250831885, 0.09975607058855096], 'val_acc': [0.7767197093096103, 0.8110512467109385, 0.8381155243703796, 0.8825961658939983, 0.9030196717203358, 0.9152988347324896, 0.929708056634507, 0.9394812680115274, 0.9360982333040972, 0.9427390051372009, 0.9492544793885478, 0.9458714446811176, 0.9469991229169277, 0.947249718080441, 0.9501315624608445, 0.9515098358601679, 0.955268763312868, 0.9507580503696279, 0.9501315624608445, 0.9570229294574615, 0.9575241197844881, 0.9562711439669215, 0.9558952512216514, 0.9571482270392181, 0.9579000125297582, 0.9566470367121914, 0.9576494173662449, 0.9595288810925949, 0.9568976318757048, 0.9590276907655683, 0.9600300714196216, 0.9604059641648917, 0.9624107254729983, 0.962536023054755, 0.9558952512216514, 0.9627866182182684, 0.9612830472371883, 0.961408344818945, 0.9601553690013783, 0.9595288810925949, 0.9626613206365117, 0.9639142964540784, 0.9611577496554317, 0.962536023054755, 0.9634131061270518, 0.9630372133817817, 0.9641648916175918, 0.962160130309485, 0.9616589399824583, 0.9615336424007017, 0.962160130309485, 0.9655431650169152, 0.9610324520736749, 0.9656684625986719, 0.9620348327277284, 0.9591529883473249, 0.9650419746898885, 0.9645407843628618, 0.963287808545295, 0.963287808545295, 0.9645407843628618, 0.9672973311615086, 0.9641648916175918, 0.9667961408344818, 0.9655431650169152, 0.962536023054755, 0.9672973311615086, 0.9651672722716451, 0.9647913795263752, 0.9655431650169152, 0.9657937601804285, 0.9689261997243453, 0.9664202480892119, 0.9619095351459717, 0.9654178674351584, 0.9646660819446184, 0.963287808545295, 0.9642901891993485, 0.9669214384162386, 0.9662949505074552, 0.9665455456709685, 0.9676732239067786], 'size_bytes': 499467, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 82, 'batch_size': 512, 'lr': 0.0005821349344707373, 'weight_decay': 1.4321368479220654e-06, 'patch_size': 250, 'n_heads': 2, 'head_dim': 31, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.023765069214410135, 'attn_dropout': 0.2652242710488922, 'augment_noise_std': 0.020666739956162736, 'augment_scale_low': 0.889988240950007, 'augment_scale_high': 1.1745141846938907, 'use_focal_loss': True, 'focal_gamma': 1.2615536235679103, 'class_weighted': False, 'grad_clip': 4.440311439064104, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 94860, 'model_storage_size_kb': 407.60156250000006, 'model_size_validation': 'FAIL'}
2025-10-02 20:00:13,051 - INFO - _models.training_function_executor - BO Objective: base=0.9677, size_penalty=0.2961, final=0.6716
2025-10-02 20:00:13,051 - INFO - _models.training_function_executor - Model: 94,860 parameters, 407.6KB (FAIL 256KB limit)
2025-10-02 20:00:13,051 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 62.435s
2025-10-02 20:00:13,270 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6716
2025-10-02 20:00:13,270 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.219s
2025-10-02 20:00:13,270 - INFO - bo.run_bo - Recorded observation #12: hparams={'epochs': np.int64(82), 'batch_size': np.int64(512), 'lr': 0.0005821349344707373, 'weight_decay': 1.4321368479220654e-06, 'patch_size': np.int64(250), 'n_heads': np.int64(2), 'head_dim': np.int64(31), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.023765069214410135, 'attn_dropout': 0.2652242710488922, 'augment_noise_std': 0.020666739956162736, 'augment_scale_low': 0.889988240950007, 'augment_scale_high': 1.1745141846938907, 'use_focal_loss': np.True_, 'focal_gamma': 1.2615536235679103, 'class_weighted': np.False_, 'grad_clip': 4.440311439064104, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.6716
2025-10-02 20:00:13,270 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 12: {'epochs': np.int64(82), 'batch_size': np.int64(512), 'lr': 0.0005821349344707373, 'weight_decay': 1.4321368479220654e-06, 'patch_size': np.int64(250), 'n_heads': np.int64(2), 'head_dim': np.int64(31), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.023765069214410135, 'attn_dropout': 0.2652242710488922, 'augment_noise_std': 0.020666739956162736, 'augment_scale_low': 0.889988240950007, 'augment_scale_high': 1.1745141846938907, 'use_focal_loss': np.True_, 'focal_gamma': 1.2615536235679103, 'class_weighted': np.False_, 'grad_clip': 4.440311439064104, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.6716
2025-10-02 20:00:13,270 - INFO - bo.run_bo - üîçBO Trial 13: Using RF surrogate + Expected Improvement
2025-10-02 20:00:13,270 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:00:13,270 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:00:13,271 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:00:13,271 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 90, 'batch_size': 64, 'lr': 0.0027169424077498985, 'weight_decay': 6.816485408575476e-05, 'patch_size': 5, 'n_heads': 4, 'head_dim': 31, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.0373002162935251, 'attn_dropout': 0.07306465988281743, 'augment_noise_std': 0.04142446551489526, 'augment_scale_low': 0.9593826065568889, 'augment_scale_high': 1.1600936734408291, 'use_focal_loss': True, 'focal_gamma': 3.6172219033251585, 'class_weighted': False, 'grad_clip': 3.0790391017054612, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 20:00:13,272 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 90, 'batch_size': 64, 'lr': 0.0027169424077498985, 'weight_decay': 6.816485408575476e-05, 'patch_size': 5, 'n_heads': 4, 'head_dim': 31, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.0373002162935251, 'attn_dropout': 0.07306465988281743, 'augment_noise_std': 0.04142446551489526, 'augment_scale_low': 0.9593826065568889, 'augment_scale_high': 1.1600936734408291, 'use_focal_loss': True, 'focal_gamma': 3.6172219033251585, 'class_weighted': False, 'grad_clip': 3.0790391017054612, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 20:19:49,189 - INFO - _models.training_function_executor - Model: 276,272 parameters, 1187.1KB storage
2025-10-02 20:19:49,189 - WARNING - _models.training_function_executor - Model storage 1187.1KB exceeds 256KB limit!
2025-10-02 20:19:49,189 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.2773599478296852, 0.2861390136236222, 0.2943085956788394, 0.2953305219519509, 0.2939395720431878, 0.297779448293293, 0.3011086809994773, 0.3019437243564421, 0.301261011039285, 0.30127415204393204, 0.3012025381060309, 0.3010821905979831, 0.3010726976158647, 0.301163603209562, 0.30091953250701103, 0.3009697740617874, 0.30118468379683466, 0.3009063168672587, 0.30100461914988347, 0.300611894605632, 0.30090097132506005, 0.3007475500252747, 0.3007759056434339, 0.3007079763557067, 0.30074821038528915, 0.30078289458770946, 0.30098273332296066, 0.30069637176659875, 0.30085418448148543, 0.30070347052285856, 0.30076068679898715, 0.3009635394613153, 0.3004318533492515, 0.3006577427082595, 0.3005324487064699, 0.30060660804632117, 0.30067834342194094, 0.3008841974543766, 0.30068671450154716, 0.30072096827125555, 0.3006205397026695, 0.3005205786868982, 0.3006041037214059, 0.30060346642427943, 0.30060195431140785, 0.3005859650811751, 0.3005677715993684, 0.30060563854987776, 0.3007885438801109, 0.300539131424251, 0.3006438981543633, 0.3005404349801475, 0.30068364775453793, 0.3006641291462568, 0.30052979396958923, 0.3004763173294468, 0.30055530378664136, 0.3005845144979675, 0.30073372802967246, 0.30062669417958426, 0.3006166435701822, 0.30056627576233835, 0.3008706158453169, 0.3004775594901132, 0.3005227694432658, 0.3007133467215044, 0.3004040323160265, 0.30064738293302107, 0.3005040385240048, 0.30051561946674415, 0.3007040331377345, 0.30073702547837966, 0.3005393042218001, 0.300530694294173, 0.3007374547249922, 0.30055194129711726, 0.30044820096727803, 0.3004952457260862, 0.30049668473397206, 0.300665482523462, 0.3005647245459967, 0.300583439854584, 0.30047642940907004, 0.30069264987215744, 0.3005275630282119, 0.3005309622980197, 0.30066449962563596, 0.3005790898656724, 0.3003693498841198, 0.3003888317685981], 'val_losses': [0.31489230383384975, 0.2882995619210455, 0.2939212701308251, 0.28984711725017864, 0.29351154035485905, 0.30119166815509446, 0.3005757049676425, 0.3003982149308195, 0.30077661786173804, 0.3005061204115888, 0.30066985702390675, 0.30036193590021687, 0.3004964098209218, 0.3008745250031548, 0.30075626573964714, 0.3015374712344474, 0.3004181655524085, 0.300576971331092, 0.302726639150199, 0.3010675635811022, 0.3013431875665539, 0.3005553143949172, 0.30080943930576265, 0.3013534990674107, 0.30066001578995555, 0.3021881638120222, 0.300933288948725, 0.30037977748191413, 0.3006560548238177, 0.30051778679039826, 0.30071837777572236, 0.3003887294800624, 0.3006868050979233, 0.3014303529647908, 0.30062008385012406, 0.3009300464478341, 0.3028234365780003, 0.30057945222786503, 0.3013817179757183, 0.3015245031241023, 0.30085101783977625, 0.30063532475256527, 0.300524252171042, 0.300872193500745, 0.301155944860694, 0.3003358943818461, 0.3002631639014538, 0.30098811624126615, 0.30047522659454706, 0.30089361592613806, 0.3015635910954724, 0.30045704148516533, 0.30070784958535723, 0.30178281070556934, 0.3002984778050745, 0.30059708422055714, 0.30044700949357245, 0.3008674570207473, 0.3015546654436825, 0.30064448788752757, 0.3009874494530446, 0.3015952417387757, 0.3003178047885155, 0.3005920681323403, 0.3005889626228037, 0.3003044689960788, 0.30030849824691386, 0.30069979097166244, 0.300714490669381, 0.3004444358458929, 0.300296227395527, 0.3007710323124179, 0.3003465733799536, 0.3019918063658996, 0.3005152984831868, 0.3004143544175305, 0.3007237285547366, 0.30068011643511355, 0.30096817840917145, 0.300332926711318, 0.3005018589800872, 0.30025032174055505, 0.3004274532719821, 0.30063068081996236, 0.30037170085463666, 0.30122548962928436, 0.30034699976115636, 0.3005075862706118, 0.3005904017788562, 0.300722874169839], 'val_acc': [0.7200852023555946, 0.7119408595414108, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946], 'size_bytes': 1506379, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 90, 'batch_size': 64, 'lr': 0.0027169424077498985, 'weight_decay': 6.816485408575476e-05, 'patch_size': 5, 'n_heads': 4, 'head_dim': 31, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.0373002162935251, 'attn_dropout': 0.07306465988281743, 'augment_noise_std': 0.04142446551489526, 'augment_scale_low': 0.9593826065568889, 'augment_scale_high': 1.1600936734408291, 'use_focal_loss': True, 'focal_gamma': 3.6172219033251585, 'class_weighted': False, 'grad_clip': 3.0790391017054612, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 276272, 'model_storage_size_kb': 1187.10625, 'model_size_validation': 'FAIL'}
2025-10-02 20:19:49,189 - INFO - _models.training_function_executor - BO Objective: base=0.7201, size_penalty=0.8000, final=-0.0799
2025-10-02 20:19:49,189 - INFO - _models.training_function_executor - Model: 276,272 parameters, 1187.1KB (FAIL 256KB limit)
2025-10-02 20:19:49,189 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 1175.919s
2025-10-02 20:19:49,285 - INFO - bo.run_bo - Updated RF surrogate model with observation: -0.0799
2025-10-02 20:19:49,286 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.096s
2025-10-02 20:19:49,286 - INFO - bo.run_bo - Recorded observation #13: hparams={'epochs': np.int64(90), 'batch_size': np.int64(64), 'lr': 0.0027169424077498985, 'weight_decay': 6.816485408575476e-05, 'patch_size': np.int64(5), 'n_heads': np.int64(4), 'head_dim': np.int64(31), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.0373002162935251, 'attn_dropout': 0.07306465988281743, 'augment_noise_std': 0.04142446551489526, 'augment_scale_low': 0.9593826065568889, 'augment_scale_high': 1.1600936734408291, 'use_focal_loss': np.True_, 'focal_gamma': 3.6172219033251585, 'class_weighted': np.False_, 'grad_clip': 3.0790391017054612, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=-0.0799
2025-10-02 20:19:49,286 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 13: {'epochs': np.int64(90), 'batch_size': np.int64(64), 'lr': 0.0027169424077498985, 'weight_decay': 6.816485408575476e-05, 'patch_size': np.int64(5), 'n_heads': np.int64(4), 'head_dim': np.int64(31), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.0373002162935251, 'attn_dropout': 0.07306465988281743, 'augment_noise_std': 0.04142446551489526, 'augment_scale_low': 0.9593826065568889, 'augment_scale_high': 1.1600936734408291, 'use_focal_loss': np.True_, 'focal_gamma': 3.6172219033251585, 'class_weighted': np.False_, 'grad_clip': 3.0790391017054612, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> -0.0799
2025-10-02 20:19:49,286 - INFO - bo.run_bo - üîçBO Trial 14: Using RF surrogate + Expected Improvement
2025-10-02 20:19:49,286 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:19:49,286 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:19:49,286 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:19:49,286 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 29, 'batch_size': 128, 'lr': 0.007795435217012224, 'weight_decay': 2.6436603416682245e-06, 'patch_size': 125, 'n_heads': 4, 'head_dim': 24, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.4386618059431215, 'attn_dropout': 0.016218332735073327, 'augment_noise_std': 0.011605059010567855, 'augment_scale_low': 0.8654631310855724, 'augment_scale_high': 1.1777564489067995, 'use_focal_loss': True, 'focal_gamma': 3.7665367947964707, 'class_weighted': False, 'grad_clip': 0.9236200135997403, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:19:49,288 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 29, 'batch_size': 128, 'lr': 0.007795435217012224, 'weight_decay': 2.6436603416682245e-06, 'patch_size': 125, 'n_heads': 4, 'head_dim': 24, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.4386618059431215, 'attn_dropout': 0.016218332735073327, 'augment_noise_std': 0.011605059010567855, 'augment_scale_low': 0.8654631310855724, 'augment_scale_high': 1.1777564489067995, 'use_focal_loss': True, 'focal_gamma': 3.7665367947964707, 'class_weighted': False, 'grad_clip': 0.9236200135997403, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:20:23,504 - INFO - _models.training_function_executor - Model: 137,856 parameters, 296.2KB storage
2025-10-02 20:20:23,504 - WARNING - _models.training_function_executor - Model storage 296.2KB exceeds 256KB limit!
2025-10-02 20:20:23,505 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.28862843072694844, 0.2736491624885449, 0.2918428669551506, 0.29079357133329026, 0.2909938845619747, 0.29065047372957, 0.27415792432243774, 0.26343302478248215, 0.27022232694828324, 0.2640210538206328, 0.2525512900378394, 0.25316933821183685, 0.2436816986277151, 0.24385257713426434, 0.24418452199901722, 0.23886074569352053, 0.2438509736016509, 0.2478118615171102, 0.2514091802777471, 0.24826104431635856, 0.24376280388038035, 0.23801143495199978, 0.23718584712890106, 0.2360884930664363, 0.23494311219315084, 0.2359533882130284, 0.2365462183098116, 0.23601875236222378, 0.2336885052290349], 'val_losses': [0.2829344039527325, 0.2736001427931499, 0.3011374597191109, 0.302250160994035, 0.3028189819709891, 0.295921730223701, 0.29752832382092154, 0.28807277607411436, 0.2736908265607367, 0.2499706300428048, 0.2567169928876339, 0.25718103043567925, 0.2468049385903578, 0.24892501309355344, 0.2419299319892461, 0.24679826912485917, 0.2601001395040729, 0.24928228356118728, 0.2619634258177268, 0.25016813928464676, 0.2449367505657749, 0.2454483843700373, 0.2501231687582909, 0.2479508513170008, 0.24493044485496618, 0.24748556149075346, 0.2391080369967713, 0.250499268778321, 0.23503511387454792], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7101866933968174, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.6928956271143967, 0.6902643778975066, 0.7200852023555946, 0.7089337175792507, 0.7017917554191204, 0.7200852023555946, 0.6838742012279163, 0.6630748026563087, 0.6193459466232302, 0.637012905650921, 0.6360105249968676, 0.7200852023555946, 0.7200852023555946, 0.6302468362360606, 0.6350081443428142], 'size_bytes': 408187, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 29, 'batch_size': 128, 'lr': 0.007795435217012224, 'weight_decay': 2.6436603416682245e-06, 'patch_size': 125, 'n_heads': 4, 'head_dim': 24, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.4386618059431215, 'attn_dropout': 0.016218332735073327, 'augment_noise_std': 0.011605059010567855, 'augment_scale_low': 0.8654631310855724, 'augment_scale_high': 1.1777564489067995, 'use_focal_loss': True, 'focal_gamma': 3.7665367947964707, 'class_weighted': False, 'grad_clip': 0.9236200135997403, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 137856, 'model_storage_size_kb': 296.175, 'model_size_validation': 'FAIL'}
2025-10-02 20:20:23,505 - INFO - _models.training_function_executor - BO Objective: base=0.6350, size_penalty=0.0785, final=0.5565
2025-10-02 20:20:23,505 - INFO - _models.training_function_executor - Model: 137,856 parameters, 296.2KB (FAIL 256KB limit)
2025-10-02 20:20:23,505 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 34.218s
2025-10-02 20:20:23,600 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5565
2025-10-02 20:20:23,600 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.095s
2025-10-02 20:20:23,600 - INFO - bo.run_bo - Recorded observation #14: hparams={'epochs': np.int64(29), 'batch_size': np.int64(128), 'lr': 0.007795435217012224, 'weight_decay': 2.6436603416682245e-06, 'patch_size': np.int64(125), 'n_heads': np.int64(4), 'head_dim': np.int64(24), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.4386618059431215, 'attn_dropout': 0.016218332735073327, 'augment_noise_std': 0.011605059010567855, 'augment_scale_low': 0.8654631310855724, 'augment_scale_high': 1.1777564489067995, 'use_focal_loss': np.True_, 'focal_gamma': 3.7665367947964707, 'class_weighted': np.False_, 'grad_clip': 0.9236200135997403, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.5565
2025-10-02 20:20:23,600 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 14: {'epochs': np.int64(29), 'batch_size': np.int64(128), 'lr': 0.007795435217012224, 'weight_decay': 2.6436603416682245e-06, 'patch_size': np.int64(125), 'n_heads': np.int64(4), 'head_dim': np.int64(24), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.4386618059431215, 'attn_dropout': 0.016218332735073327, 'augment_noise_std': 0.011605059010567855, 'augment_scale_low': 0.8654631310855724, 'augment_scale_high': 1.1777564489067995, 'use_focal_loss': np.True_, 'focal_gamma': 3.7665367947964707, 'class_weighted': np.False_, 'grad_clip': 0.9236200135997403, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.5565
2025-10-02 20:20:23,600 - INFO - bo.run_bo - üîçBO Trial 15: Using RF surrogate + Expected Improvement
2025-10-02 20:20:23,600 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:20:23,601 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:20:23,601 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:20:23,601 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 97, 'batch_size': 64, 'lr': 3.642727676398785e-05, 'weight_decay': 1.7770007710078548e-05, 'patch_size': 5, 'n_heads': 1, 'head_dim': 23, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.07370640728238982, 'attn_dropout': 0.07622503612127832, 'augment_noise_std': 0.013507659182021606, 'augment_scale_low': 0.8920814459475123, 'augment_scale_high': 1.043520648475631, 'use_focal_loss': True, 'focal_gamma': 3.9937084816410597, 'class_weighted': False, 'grad_clip': 1.3263987925298824, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 20:20:23,602 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 97, 'batch_size': 64, 'lr': 3.642727676398785e-05, 'weight_decay': 1.7770007710078548e-05, 'patch_size': 5, 'n_heads': 1, 'head_dim': 23, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.07370640728238982, 'attn_dropout': 0.07622503612127832, 'augment_noise_std': 0.013507659182021606, 'augment_scale_low': 0.8920814459475123, 'augment_scale_high': 1.043520648475631, 'use_focal_loss': True, 'focal_gamma': 3.9937084816410597, 'class_weighted': False, 'grad_clip': 1.3263987925298824, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 20:25:16,706 - INFO - _models.training_function_executor - Model: 13,966 parameters, 60.0KB storage
2025-10-02 20:25:16,707 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4200394321796093, 0.2644882883691132, 0.23049491321824342, 0.19585105478231446, 0.17836537949769593, 0.16926576501817606, 0.16196862202255222, 0.15504465500327, 0.1491978657035863, 0.14357545404517216, 0.1398114323349027, 0.1359477245860021, 0.13241719458657944, 0.12932997746948266, 0.12614593834256266, 0.12288973363405148, 0.11999506649283752, 0.1171979310443608, 0.11419503260369446, 0.11203383648917964, 0.10980808847248825, 0.10805145150732434, 0.10646837114722038, 0.10464540719416485, 0.1034778890033113, 0.10293067416748419, 0.10121886193037175, 0.09989147175841996, 0.09957365573285289, 0.0982252627431915, 0.09722275124401898, 0.09598452959077697, 0.09537721999229742, 0.09392731840677422, 0.09339802202776822, 0.0930840486621915, 0.09160593923547604, 0.09090731845332452, 0.08972942384473177, 0.08942424225942511, 0.0888978110309685, 0.08815028180883137, 0.08681777032441193, 0.08673790432527931, 0.08565311221751226, 0.08504935577980906, 0.0844399297855979, 0.08349948307368064, 0.08288368151626711, 0.08214366691592485, 0.08177631914044287, 0.0811310317602458, 0.08049050608769871, 0.08010516051171855, 0.07916645431253981, 0.07872273525582502, 0.0778013730098239, 0.07746450677810485, 0.0774486766410954, 0.07658958498097838, 0.0763176386196312, 0.07554479152158136, 0.07494417148820291, 0.0740300584586225, 0.07433001328898851, 0.07377587906094145, 0.07303181720698414, 0.07246179062165596, 0.07230711625968589, 0.07206976323754097, 0.07115263525104028, 0.07112944597336421, 0.07026116761474606, 0.07004208250990436, 0.07013388302363788, 0.06966857390643787, 0.06880964274876653, 0.06877009025277625, 0.06798093805207281, 0.06813437987637207, 0.0674222287865264, 0.06780162263845187, 0.06661234642405096, 0.06637690321492756, 0.06632366400401447, 0.06671032028667939, 0.06607516158325602, 0.0653697756370465, 0.06509822034179143, 0.0645310631172337, 0.06463841288472907, 0.06434267083419182, 0.06394215743505981, 0.06384277714586936, 0.06369243515764801, 0.06252413475385653, 0.06307663230490752], 'val_losses': [0.28801525747801426, 0.24919291577935443, 0.21000086936514833, 0.1859779511430765, 0.1743014232575139, 0.16681720748452253, 0.15985114259680117, 0.1538409623320338, 0.14799290913012583, 0.14327231640094892, 0.13893662440983698, 0.13540699834076628, 0.13181088166795657, 0.128364727473781, 0.1255719813333502, 0.12143529729296608, 0.11926137214362464, 0.11625152784906717, 0.11332699816448544, 0.11180481078800306, 0.11001184746873229, 0.1081752323427306, 0.10616322663374494, 0.10590782518340014, 0.1040526513090081, 0.10382465230665804, 0.10166214896692038, 0.102560363995123, 0.10024552434835013, 0.099006998943287, 0.09891329963278792, 0.09776356516485524, 0.09684732835518135, 0.09621629655844749, 0.09502644409434775, 0.09432159101268676, 0.09436240046936353, 0.09302106952041951, 0.09259321185008818, 0.09144240828078577, 0.09155187309824529, 0.09101133618787138, 0.08954532562849919, 0.08941591023472732, 0.08894332684039725, 0.08877100975447215, 0.08757072850658532, 0.08715855007270736, 0.08690186599991835, 0.08637096540218919, 0.08551734488407498, 0.08509414545867754, 0.0841010253395208, 0.08407744305700067, 0.08336854787129624, 0.08325015375143961, 0.08118175518837971, 0.08217612037974527, 0.08256170014100346, 0.08155457506906402, 0.08127736456411223, 0.07929950244346667, 0.07876048911007674, 0.07835454550613632, 0.07783313491623842, 0.07754732565196994, 0.07771247191488362, 0.07698038902776445, 0.07669697069551186, 0.07632474894586952, 0.07609322921278079, 0.07597779407593155, 0.07494074538725388, 0.0745618594106711, 0.07403927032348358, 0.07421939955087715, 0.07399165583150143, 0.07414376476349918, 0.07480012210964215, 0.07334562728237261, 0.07371837250648428, 0.07297153286416494, 0.07273478501311462, 0.07174936944450551, 0.07277340956042545, 0.07183344603189773, 0.07344368580612544, 0.07113039262369676, 0.07129559840396926, 0.0704577966316528, 0.07072122240117687, 0.06987944419863693, 0.0697125821979874, 0.0703654143277333, 0.0704560551664955, 0.0689866351473191, 0.0691774483397183], 'val_acc': [0.7198346071920811, 0.7188322265380278, 0.7507831098859792, 0.762686380152863, 0.7663200100238066, 0.785239944869064, 0.7882470868312241, 0.7920060142839244, 0.7931336925197344, 0.7923819070291943, 0.7953890489913544, 0.7995238691893246, 0.8000250595163514, 0.8101741636386418, 0.8109259491291818, 0.8228292193960657, 0.8293446936474126, 0.8342312993359228, 0.8381155243703796, 0.8391179050244331, 0.84638516476632, 0.8493923067284801, 0.8510211752913168, 0.854028317253477, 0.8554065906528004, 0.8579125422879338, 0.8636762310487408, 0.8626738503946874, 0.8650545044480642, 0.8669339681744143, 0.8684375391554943, 0.8674351585014409, 0.8738253351710312, 0.8748277158250846, 0.8748277158250846, 0.8772083698784613, 0.8760806916426513, 0.8784613456960281, 0.8824708683122415, 0.8814684876581882, 0.8817190828217015, 0.8830973562210249, 0.8832226538027816, 0.8829720586392683, 0.8864803909284551, 0.8852274151108883, 0.886856283673725, 0.8864803909284551, 0.8869815812554818, 0.8871068788372385, 0.8909911038716952, 0.891492294198722, 0.8907405087081819, 0.8919934845257487, 0.8926199724345321, 0.8916175917804786, 0.8943741385791254, 0.8927452700162887, 0.8923693772710186, 0.8953765192331788, 0.8943741385791254, 0.8976318757047989, 0.8985089587770956, 0.9008896128304724, 0.9008896128304724, 0.9005137200852024, 0.8985089587770956, 0.9012655055757424, 0.9030196717203358, 0.9021425886480391, 0.9023931838115524, 0.9027690765568225, 0.9043979451196592, 0.9021425886480391, 0.9055256233554693, 0.9036461596291192, 0.9052750281919559, 0.9028943741385791, 0.9033955644656058, 0.9042726475379025, 0.9036461596291192, 0.9045232427014158, 0.9060268136824959, 0.9065280040095226, 0.9036461596291192, 0.9042726475379025, 0.9030196717203358, 0.9059015161007392, 0.9048991354466859, 0.906402706427766, 0.9056509209372259, 0.9060268136824959, 0.9048991354466859, 0.9054003257737125, 0.9056509209372259, 0.9056509209372259, 0.9072797895000626], 'size_bytes': 65865, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 97, 'batch_size': 64, 'lr': 3.642727676398785e-05, 'weight_decay': 1.7770007710078548e-05, 'patch_size': 5, 'n_heads': 1, 'head_dim': 23, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.07370640728238982, 'attn_dropout': 0.07622503612127832, 'augment_noise_std': 0.013507659182021606, 'augment_scale_low': 0.8920814459475123, 'augment_scale_high': 1.043520648475631, 'use_focal_loss': True, 'focal_gamma': 3.9937084816410597, 'class_weighted': False, 'grad_clip': 1.3263987925298824, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 13966, 'model_storage_size_kb': 60.01015625, 'model_size_validation': 'PASS'}
2025-10-02 20:25:16,707 - INFO - _models.training_function_executor - BO Objective: base=0.9073, size_penalty=0.0000, final=0.9073
2025-10-02 20:25:16,707 - INFO - _models.training_function_executor - Model: 13,966 parameters, 60.0KB (PASS 256KB limit)
2025-10-02 20:25:16,707 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 293.106s
2025-10-02 20:25:16,803 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9073
2025-10-02 20:25:16,803 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.096s
2025-10-02 20:25:16,803 - INFO - bo.run_bo - Recorded observation #15: hparams={'epochs': np.int64(97), 'batch_size': np.int64(64), 'lr': 3.642727676398785e-05, 'weight_decay': 1.7770007710078548e-05, 'patch_size': np.int64(5), 'n_heads': np.int64(1), 'head_dim': np.int64(23), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.07370640728238982, 'attn_dropout': 0.07622503612127832, 'augment_noise_std': 0.013507659182021606, 'augment_scale_low': 0.8920814459475123, 'augment_scale_high': 1.043520648475631, 'use_focal_loss': np.True_, 'focal_gamma': 3.9937084816410597, 'class_weighted': np.False_, 'grad_clip': 1.3263987925298824, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.9073
2025-10-02 20:25:16,803 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 15: {'epochs': np.int64(97), 'batch_size': np.int64(64), 'lr': 3.642727676398785e-05, 'weight_decay': 1.7770007710078548e-05, 'patch_size': np.int64(5), 'n_heads': np.int64(1), 'head_dim': np.int64(23), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.07370640728238982, 'attn_dropout': 0.07622503612127832, 'augment_noise_std': 0.013507659182021606, 'augment_scale_low': 0.8920814459475123, 'augment_scale_high': 1.043520648475631, 'use_focal_loss': np.True_, 'focal_gamma': 3.9937084816410597, 'class_weighted': np.False_, 'grad_clip': 1.3263987925298824, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.9073
2025-10-02 20:25:16,803 - INFO - bo.run_bo - üîçBO Trial 16: Using RF surrogate + Expected Improvement
2025-10-02 20:25:16,803 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:25:16,803 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:25:16,803 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:25:16,803 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 95, 'batch_size': 512, 'lr': 2.3586003099220876e-05, 'weight_decay': 0.0005283258515347151, 'patch_size': 25, 'n_heads': 2, 'head_dim': 27, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.11512654528367058, 'attn_dropout': 0.1691997970010451, 'augment_noise_std': 0.0012433101501611813, 'augment_scale_low': 0.9826080520896139, 'augment_scale_high': 1.0463380438499246, 'use_focal_loss': False, 'focal_gamma': 4.012637503415206, 'class_weighted': True, 'grad_clip': 3.749691672625172, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 20:25:16,804 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 95, 'batch_size': 512, 'lr': 2.3586003099220876e-05, 'weight_decay': 0.0005283258515347151, 'patch_size': 25, 'n_heads': 2, 'head_dim': 27, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.11512654528367058, 'attn_dropout': 0.1691997970010451, 'augment_noise_std': 0.0012433101501611813, 'augment_scale_low': 0.9826080520896139, 'augment_scale_high': 1.0463380438499246, 'use_focal_loss': False, 'focal_gamma': 4.012637503415206, 'class_weighted': True, 'grad_clip': 3.749691672625172, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 20:27:08,404 - INFO - _models.training_function_executor - Model: 29,160 parameters, 125.3KB storage
2025-10-02 20:27:08,404 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6220391346272105, 1.518611495272952, 1.487844171318812, 1.4615464613102538, 1.4284333857035714, 1.3876586554474137, 1.340014792614905, 1.2913545507336568, 1.2448397264875528, 1.2036727348657208, 1.1621615507696572, 1.1246116949203966, 1.089472413503694, 1.0564107036835544, 1.0264203116935817, 0.9993113385862872, 0.9744885915805906, 0.9536490949145072, 0.9317638705936854, 0.9151332559686546, 0.8989631796627794, 0.8856482332695981, 0.8677368085101236, 0.8575549785667309, 0.8456436520257928, 0.8346171176392859, 0.8222723783091324, 0.8143076549006394, 0.8012776143142417, 0.7926528356773074, 0.7835971769154925, 0.7750943291868045, 0.7684365844299361, 0.7612911420418453, 0.7518245812791505, 0.7475328213255966, 0.7389061638642959, 0.7331291988682926, 0.7249040574043559, 0.7208729993616269, 0.7115411543888701, 0.7069776184079463, 0.7023619853759011, 0.6987401170042439, 0.6855655890230629, 0.6839640102294462, 0.6774511207476843, 0.6706150562767023, 0.6657747829953362, 0.6616720597057554, 0.6557075887095998, 0.6496811722238175, 0.6434610525819289, 0.6383280321454821, 0.6351048128107408, 0.6296552201169993, 0.6212314503978145, 0.6224777783284908, 0.6145602389616958, 0.608834715647659, 0.6068846467342441, 0.6033409284203998, 0.5962615204597556, 0.5891025866817772, 0.5911097818122555, 0.5874589290347938, 0.5816023093030495, 0.5774418707741302, 0.5733192212837881, 0.5720837630525932, 0.5639251709130012, 0.564394306494987, 0.5592912293022312, 0.5585080730694001, 0.5566157473939906, 0.5525240366119373, 0.5484783445926543, 0.542581957046338, 0.5428949939852852, 0.540994153469983, 0.5393096547308555, 0.5338195534112874, 0.5336350594920793, 0.5271904467546794, 0.5238415598757296, 0.522180709402181, 0.5211208054353408, 0.5200107386714701, 0.516871568264939, 0.5158338431550518, 0.512636056018571, 0.5105913252248033, 0.5136250502199166, 0.5048706226396974, 0.5039963614336438], 'val_losses': [1.550833016062661, 1.5039068042686776, 1.4758179403011578, 1.4453634247865284, 1.4071387916022922, 1.3612110037326275, 1.3131890152946868, 1.2656700886695431, 1.2212361208165652, 1.1800182854348056, 1.1402545826370265, 1.102900118505786, 1.068759800273215, 1.0384901200839263, 1.0089170373318086, 0.982446637507325, 0.9599005806910604, 0.9413198127496841, 0.9211023082232478, 0.9060707859804182, 0.8908488635072253, 0.876799099398442, 0.8642429585460433, 0.852583865312209, 0.8410099615370804, 0.8311993553665046, 0.8219112494194691, 0.8126205471117325, 0.8040412249249663, 0.7948307920601595, 0.7855665403086953, 0.7776161105228596, 0.7742289390768298, 0.7654817865286505, 0.7570625952111347, 0.74798175309807, 0.7425815338345193, 0.736037647727139, 0.7330551329015744, 0.7253429321265283, 0.7185270561563716, 0.7149693150969021, 0.7081315678367787, 0.702658190656733, 0.6950979091188779, 0.6894503170917273, 0.6873016939129153, 0.6814362217408795, 0.6735951581958421, 0.6711554593526694, 0.6641558787978363, 0.660635527498276, 0.6570459204857667, 0.6491503891916804, 0.6449237858990711, 0.6394516334559383, 0.6354353229603124, 0.6319268860026509, 0.6287876121263847, 0.6225569272919595, 0.6185566304979012, 0.6145699374688596, 0.6122022064283906, 0.6102470935190132, 0.6041601591952058, 0.5979512485402103, 0.597717533395667, 0.5923259064243366, 0.5904818485963466, 0.5874429509508596, 0.5881753394275422, 0.5811900802310466, 0.5797900528764982, 0.5766214785189306, 0.5731801286166122, 0.5728658521523408, 0.5708142607795491, 0.5650575327747762, 0.5634505087429234, 0.5611724271509615, 0.5578987761079153, 0.5569565325747634, 0.5524423969014518, 0.5509519928037306, 0.5537138903683222, 0.5469437432369781, 0.5457921053678854, 0.5421794914180601, 0.5416399820931438, 0.5399692959615782, 0.5372672004069978, 0.5353219742957788, 0.5357891912000876, 0.5339027854482448, 0.5325436192845778], 'val_acc': [0.29219396065655934, 0.3380528755795013, 0.3345445432903145, 0.33479513845382786, 0.33078561583761434, 0.35446685878962536, 0.35897757173286554, 0.36611953389299584, 0.37514095977947626, 0.3979451196591906, 0.40433529632878085, 0.4092219020172911, 0.41348201979701793, 0.42425761182809174, 0.4270141586267385, 0.4431775466733492, 0.45583260243077306, 0.4668587896253602, 0.4752537276030573, 0.48465104623480765, 0.4881593785239945, 0.4976819947375016, 0.5063275278787119, 0.5125924069665455, 0.5193584763814059, 0.5209873449442426, 0.5394060894624734, 0.5327653176293697, 0.5360230547550432, 0.5480516226036838, 0.5471745395313871, 0.5600801904523243, 0.5500563839117905, 0.5594537025435409, 0.5576995363989475, 0.5645909033955645, 0.5639644154867811, 0.5731111389550182, 0.569352211502318, 0.5891492294198722, 0.5921563713820324, 0.5823831600050119, 0.5920310738002756, 0.5986718456333793, 0.5984212504698659, 0.6113269013908031, 0.5990477383786493, 0.6081944618468863, 0.6173411853151234, 0.6392682621225411, 0.6238566595664704, 0.6223530885853903, 0.6292444555820073, 0.6356346322515976, 0.6243578498934971, 0.6375140959779476, 0.6345069540157875, 0.6431524871569979, 0.6403959403583511, 0.6543039719333417, 0.639644154867811, 0.6609447437664453, 0.6596917679488786, 0.6683373010900889, 0.6600676606941486, 0.6799899761934595, 0.6786117027941361, 0.677108131813056, 0.6848765818819696, 0.6809923568475128, 0.6902643778975066, 0.7029194336549305, 0.7086831224157374, 0.6955268763312868, 0.6933968174414233, 0.7006640771833104, 0.6895125924069665, 0.7075554441799273, 0.6905149730610199, 0.698534018293447, 0.730610199223155, 0.7150732990853277, 0.7051747901265506, 0.7159503821576244, 0.7308607943866683, 0.7140709184312742, 0.7099360982333041, 0.7239694273900513, 0.7289813306603182, 0.7332414484400451, 0.7299837113143717, 0.7486530509961158, 0.7354968049116652, 0.7304849016413983, 0.7494048364866558], 'size_bytes': 167403, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 95, 'batch_size': 512, 'lr': 2.3586003099220876e-05, 'weight_decay': 0.0005283258515347151, 'patch_size': 25, 'n_heads': 2, 'head_dim': 27, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.11512654528367058, 'attn_dropout': 0.1691997970010451, 'augment_noise_std': 0.0012433101501611813, 'augment_scale_low': 0.9826080520896139, 'augment_scale_high': 1.0463380438499246, 'use_focal_loss': False, 'focal_gamma': 4.012637503415206, 'class_weighted': True, 'grad_clip': 3.749691672625172, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 29160, 'model_storage_size_kb': 125.29687500000001, 'model_size_validation': 'PASS'}
2025-10-02 20:27:08,404 - INFO - _models.training_function_executor - BO Objective: base=0.7494, size_penalty=0.0000, final=0.7494
2025-10-02 20:27:08,404 - INFO - _models.training_function_executor - Model: 29,160 parameters, 125.3KB (PASS 256KB limit)
2025-10-02 20:27:08,404 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 111.601s
2025-10-02 20:27:08,503 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7494
2025-10-02 20:27:08,503 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-10-02 20:27:08,503 - INFO - bo.run_bo - Recorded observation #16: hparams={'epochs': np.int64(95), 'batch_size': np.int64(512), 'lr': 2.3586003099220876e-05, 'weight_decay': 0.0005283258515347151, 'patch_size': np.int64(25), 'n_heads': np.int64(2), 'head_dim': np.int64(27), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.11512654528367058, 'attn_dropout': 0.1691997970010451, 'augment_noise_std': 0.0012433101501611813, 'augment_scale_low': 0.9826080520896139, 'augment_scale_high': 1.0463380438499246, 'use_focal_loss': np.False_, 'focal_gamma': 4.012637503415206, 'class_weighted': np.True_, 'grad_clip': 3.749691672625172, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7494
2025-10-02 20:27:08,503 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 16: {'epochs': np.int64(95), 'batch_size': np.int64(512), 'lr': 2.3586003099220876e-05, 'weight_decay': 0.0005283258515347151, 'patch_size': np.int64(25), 'n_heads': np.int64(2), 'head_dim': np.int64(27), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.11512654528367058, 'attn_dropout': 0.1691997970010451, 'augment_noise_std': 0.0012433101501611813, 'augment_scale_low': 0.9826080520896139, 'augment_scale_high': 1.0463380438499246, 'use_focal_loss': np.False_, 'focal_gamma': 4.012637503415206, 'class_weighted': np.True_, 'grad_clip': 3.749691672625172, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7494
2025-10-02 20:27:08,503 - INFO - bo.run_bo - üîçBO Trial 17: Using RF surrogate + Expected Improvement
2025-10-02 20:27:08,503 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:27:08,503 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:27:08,503 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:27:08,503 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 94, 'batch_size': 256, 'lr': 0.004616300446473096, 'weight_decay': 0.009823256192893271, 'patch_size': 25, 'n_heads': 1, 'head_dim': 21, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.12900004904564596, 'attn_dropout': 0.13090307746849317, 'augment_noise_std': 0.03632954738078504, 'augment_scale_low': 0.8132640447742766, 'augment_scale_high': 1.1783661722762786, 'use_focal_loss': True, 'focal_gamma': 4.590008240619511, 'class_weighted': False, 'grad_clip': 3.292256854404989, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:27:08,504 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 94, 'batch_size': 256, 'lr': 0.004616300446473096, 'weight_decay': 0.009823256192893271, 'patch_size': 25, 'n_heads': 1, 'head_dim': 21, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.12900004904564596, 'attn_dropout': 0.13090307746849317, 'augment_noise_std': 0.03632954738078504, 'augment_scale_low': 0.8132640447742766, 'augment_scale_high': 1.1783661722762786, 'use_focal_loss': True, 'focal_gamma': 4.590008240619511, 'class_weighted': False, 'grad_clip': 3.292256854404989, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:28:39,865 - INFO - _models.training_function_executor - Model: 11,366 parameters, 48.8KB storage
2025-10-02 20:28:39,865 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.16090767804820433, 0.09255234207906053, 0.07782559451779465, 0.06715118464210729, 0.058526913742763975, 0.0548244146968295, 0.04942237502041868, 0.04907408241009038, 0.04317988660190614, 0.03962295636858173, 0.039278441666833246, 0.03669460878395569, 0.0346951344089356, 0.033974518771491484, 0.03316714714037337, 0.03132493416989234, 0.030333230925783834, 0.029847086658636827, 0.029408029115332048, 0.027343067324069478, 0.028509813244961463, 0.026705696881148832, 0.027534256325946267, 0.02703829367987267, 0.02486223956241588, 0.023524323168629596, 0.023330062466868407, 0.02335520526831567, 0.02502468423394799, 0.022408566402506942, 0.022955705540068757, 0.022237646363807928, 0.021047547192018148, 0.02188838788599389, 0.020895069195531468, 0.02119855711464912, 0.01837302766280773, 0.021449258873607976, 0.02068531615480758, 0.019260162594811465, 0.020598266907513593, 0.02108280536314694, 0.01946273432622171, 0.01852848600396989, 0.01824653580114612, 0.019931806705062845, 0.016518296565599926, 0.01978175720120064, 0.020075559217221522, 0.018801798252288185, 0.019930589452950245, 0.018304223832229273, 0.01946415528384317, 0.017392796827335202, 0.017718637479431313, 0.01826690514190806, 0.017680525594293985, 0.017516605365942392, 0.016107694278987405, 0.018166137000925297, 0.016647668521595386, 0.016289576189016292, 0.015364693015895749, 0.016173338712837614, 0.016225509614661893, 0.015013431567450504, 0.01692170407359903, 0.018598213678462358, 0.016710659627641424, 0.01718455122691182, 0.01893402676751868, 0.01510213058183629, 0.015985926658022943, 0.014444119245023621, 0.01655380496746432, 0.016192967834738406, 0.01577257819351971, 0.014357813533298788, 0.01436388753204595, 0.016082963465496463, 0.016723259705955635, 0.015553526598246565, 0.015330498990021441, 0.016957155325930003, 0.01579726738639063, 0.014537238707440997, 0.014083557126065866, 0.014559979184377191, 0.014149368641047263, 0.015319341517261584, 0.013573566015692028, 0.01488721134810398, 0.014332945159994783, 0.015221303910397676], 'val_losses': [0.10507020892736652, 0.07906697520956092, 0.07445025629875184, 0.06408723992247836, 0.05731641950699328, 0.055413037163945626, 0.050188857987812474, 0.052675235664800826, 0.04308195377898058, 0.0394901432721963, 0.03849760924491977, 0.04376944364220662, 0.040664497023555006, 0.03815558202851313, 0.03512452418807754, 0.040317943057120255, 0.036971246809432, 0.0407799125774246, 0.034771852685371266, 0.03412625715351242, 0.03124911255846046, 0.033008452786335504, 0.037310811250938866, 0.03430874375030146, 0.035544922569986724, 0.03865035787334927, 0.03476028971662499, 0.04049203892085221, 0.03626338294474437, 0.027964294265758242, 0.03274168981998401, 0.029373326930108336, 0.0291834972180821, 0.03389157563337795, 0.02949552061268447, 0.025883371450820074, 0.03221257132392989, 0.025946948663765082, 0.02865563245095073, 0.02909724383728499, 0.028829581780685187, 0.02710636309199264, 0.031053375753823446, 0.026522882298604094, 0.02797474564727484, 0.026497536292072077, 0.032144131736152826, 0.03519556614341719, 0.02652654520230938, 0.03436738605051019, 0.030891036442304193, 0.03257476126822056, 0.02740363006140113, 0.026325449024491285, 0.02648508335352728, 0.027990222127533784, 0.030059421793244383, 0.02808254639515526, 0.026766110134126997, 0.02584194486906857, 0.025339519428834927, 0.03265027948589257, 0.024300881578739747, 0.02452409911894691, 0.028243237591289425, 0.027163475186151753, 0.02517812116784106, 0.02886557114324449, 0.026588871601776226, 0.027453629124492816, 0.021686519601282775, 0.027601279804706873, 0.027031435107149705, 0.024003364442621404, 0.03351892891357812, 0.025577558237438944, 0.02715559973948702, 0.023068987226115778, 0.024909798531992485, 0.02546457986442715, 0.025150988157150098, 0.02811681733138429, 0.030784597601930593, 0.02492887003361514, 0.026573006684036875, 0.024267514150269034, 0.02739564070421836, 0.025105667643014716, 0.026287859173410144, 0.0268882748004212, 0.020786400455428372, 0.02686674330864779, 0.02653643203920969, 0.02531917272034093], 'val_acc': [0.7982708933717579, 0.8493923067284801, 0.8602931963413106, 0.8549054003257737, 0.8658062899386042, 0.8888610449818318, 0.9015161007392557, 0.8654303971933341, 0.9016413983210124, 0.9214384162385666, 0.9249467485277534, 0.8962536023054755, 0.9038967547926325, 0.9201854404209999, 0.9284550808169403, 0.9269515098358602, 0.915925322641273, 0.9111640145345195, 0.9234431775466734, 0.9146723468237064, 0.938854780102744, 0.9345946623230171, 0.9178047863676231, 0.9154241323142464, 0.9111640145345195, 0.9129181806791129, 0.9269515098358602, 0.8851021175291317, 0.9295827590527502, 0.938102994612204, 0.9264503195088335, 0.9303345445432903, 0.9282044856534268, 0.9099110387169528, 0.9365994236311239, 0.9406089462473374, 0.9327151985966671, 0.9411101365743642, 0.9372259115399073, 0.9312116276155871, 0.9338428768324771, 0.9382282921939606, 0.9216890114020799, 0.9422378148101742, 0.9342187695777472, 0.9335922816689638, 0.9317128179426137, 0.8944994361608821, 0.9393559704297707, 0.8934970555068287, 0.9251973436912667, 0.9054003257737125, 0.9418619220649042, 0.9352211502318005, 0.9282044856534268, 0.9240696654554567, 0.9391053752662574, 0.9248214509459968, 0.9376018042851773, 0.9272021049993735, 0.9285803783986969, 0.9085327653176294, 0.9429896003007142, 0.9453702543540909, 0.9198095476757299, 0.9269515098358602, 0.9378523994486906, 0.9249467485277534, 0.9399824583385541, 0.9337175792507204, 0.9487532890615211, 0.9209372259115399, 0.9359729357223405, 0.9338428768324771, 0.900263124921689, 0.939230672848014, 0.936975316376394, 0.9448690640270643, 0.9354717453953139, 0.9394812680115274, 0.9421125172284175, 0.9433654930459842, 0.9364741260493672, 0.9475003132439543, 0.9249467485277534, 0.9312116276155871, 0.9382282921939606, 0.9500062648790878, 0.9394812680115274, 0.9114146096980328, 0.9513845382784112, 0.9213131186568099, 0.9508833479513845, 0.939230672848014], 'size_bytes': 55177, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 94, 'batch_size': 256, 'lr': 0.004616300446473096, 'weight_decay': 0.009823256192893271, 'patch_size': 25, 'n_heads': 1, 'head_dim': 21, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.12900004904564596, 'attn_dropout': 0.13090307746849317, 'augment_noise_std': 0.03632954738078504, 'augment_scale_low': 0.8132640447742766, 'augment_scale_high': 1.1783661722762786, 'use_focal_loss': True, 'focal_gamma': 4.590008240619511, 'class_weighted': False, 'grad_clip': 3.292256854404989, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 11366, 'model_storage_size_kb': 48.83828125, 'model_size_validation': 'PASS'}
2025-10-02 20:28:39,865 - INFO - _models.training_function_executor - BO Objective: base=0.9392, size_penalty=0.0000, final=0.9392
2025-10-02 20:28:39,865 - INFO - _models.training_function_executor - Model: 11,366 parameters, 48.8KB (PASS 256KB limit)
2025-10-02 20:28:39,865 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 91.362s
2025-10-02 20:28:39,963 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9392
2025-10-02 20:28:39,963 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-10-02 20:28:39,963 - INFO - bo.run_bo - Recorded observation #17: hparams={'epochs': np.int64(94), 'batch_size': np.int64(256), 'lr': 0.004616300446473096, 'weight_decay': 0.009823256192893271, 'patch_size': np.int64(25), 'n_heads': np.int64(1), 'head_dim': np.int64(21), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.12900004904564596, 'attn_dropout': 0.13090307746849317, 'augment_noise_std': 0.03632954738078504, 'augment_scale_low': 0.8132640447742766, 'augment_scale_high': 1.1783661722762786, 'use_focal_loss': np.True_, 'focal_gamma': 4.590008240619511, 'class_weighted': np.False_, 'grad_clip': 3.292256854404989, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9392
2025-10-02 20:28:39,963 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 17: {'epochs': np.int64(94), 'batch_size': np.int64(256), 'lr': 0.004616300446473096, 'weight_decay': 0.009823256192893271, 'patch_size': np.int64(25), 'n_heads': np.int64(1), 'head_dim': np.int64(21), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.12900004904564596, 'attn_dropout': 0.13090307746849317, 'augment_noise_std': 0.03632954738078504, 'augment_scale_low': 0.8132640447742766, 'augment_scale_high': 1.1783661722762786, 'use_focal_loss': np.True_, 'focal_gamma': 4.590008240619511, 'class_weighted': np.False_, 'grad_clip': 3.292256854404989, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9392
2025-10-02 20:28:39,964 - INFO - bo.run_bo - üîçBO Trial 18: Using RF surrogate + Expected Improvement
2025-10-02 20:28:39,964 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:28:39,964 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:28:39,964 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:28:39,964 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 94, 'batch_size': 64, 'lr': 0.00017162563498530662, 'weight_decay': 1.6656608232656036e-06, 'patch_size': 250, 'n_heads': 4, 'head_dim': 17, 'num_layers': 4, 'mlp_ratio': 4, 'dropout': 0.036854571873314375, 'attn_dropout': 0.03785920438360317, 'augment_noise_std': 0.011756388140362369, 'augment_scale_low': 0.8044194874343102, 'augment_scale_high': 1.1220904223976729, 'use_focal_loss': True, 'focal_gamma': 4.412321138984504, 'class_weighted': False, 'grad_clip': 3.8007527247818387, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 20:28:39,965 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 94, 'batch_size': 64, 'lr': 0.00017162563498530662, 'weight_decay': 1.6656608232656036e-06, 'patch_size': 250, 'n_heads': 4, 'head_dim': 17, 'num_layers': 4, 'mlp_ratio': 4, 'dropout': 0.036854571873314375, 'attn_dropout': 0.03785920438360317, 'augment_noise_std': 0.011756388140362369, 'augment_scale_low': 0.8044194874343102, 'augment_scale_high': 1.1220904223976729, 'use_focal_loss': True, 'focal_gamma': 4.412321138984504, 'class_weighted': False, 'grad_clip': 3.8007527247818387, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 20:32:06,054 - INFO - _models.training_function_executor - Model: 110,568 parameters, 475.1KB storage
2025-10-02 20:32:06,054 - WARNING - _models.training_function_executor - Model storage 475.1KB exceeds 256KB limit!
2025-10-02 20:32:06,054 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.16071665868818008, 0.09633021649223307, 0.06770169320216816, 0.05122433974988847, 0.040765539482102875, 0.03362746090602446, 0.027502057688197606, 0.024786014693607694, 0.01967044579519194, 0.01733176403012648, 0.014483017005358692, 0.01308591578218947, 0.01262817388358629, 0.009795738936946321, 0.008696566719291197, 0.00858914379353831, 0.008109742956117499, 0.006856219173165062, 0.005350398512299425, 0.007180757968431044, 0.005668872951736063, 0.005838438754583468, 0.005070100629338877, 0.0036071809101928566, 0.003307957994096004, 0.005943233531354225, 0.002840357617116631, 0.002689710045170353, 0.00650903416544259, 0.0025644352394916287, 0.0023479848572413053, 0.00562266175604322, 0.0030949041145605194, 0.0016756135359245665, 0.004535691451007773, 0.0031806124985552077, 0.002522211072613042, 0.002587420255951909, 0.002098180713897025, 0.0028681636637660976, 0.002593633308904436, 0.0018150346690960233, 0.0015972744838867164, 0.0016887810752238923, 0.004687628741031702, 0.003164887955192209, 0.0015028733518001381, 0.002003635172569338, 0.0019749834563944193, 0.0018159382970552563, 0.0015276869113976827, 0.0014511083558351482, 0.0028625085617448747, 0.0015004725060791943, 0.0015433095969746421, 0.0018963582707043054, 0.002877441075139684, 0.0011364987851195509, 0.001656663104330447, 0.0012647897233207508, 0.0010850327803007453, 0.002710802939064722, 0.0007484995869801824, 0.0008265619261238002, 0.003969702104959514, 0.0008323361425291733, 0.0004304019689042761, 0.00032814245701841635, 0.000766755781441665, 0.001645462328199786, 0.002955517873514003, 0.0006679918087910128, 0.0003654022321610706, 0.0003536627198642955, 0.0006315988940024099, 0.0053326582938728485, 0.0013000754967092867, 0.0004667963825616862, 0.0011753248092979968, 0.0009874395574646312, 0.0009968862520051695, 0.0008607133404470421, 0.000821356353812589, 0.0022533399716809033, 0.00044555755871459907, 0.0007010432155268086, 0.0015051496753611508, 0.0013886468276451752, 0.0006093868540366056, 0.0016423621909939172, 0.000996661215435324, 0.002205798726156433, 0.0009493567935181307, 0.0004721132539951441], 'val_losses': [0.10929460672162451, 0.07825809577334093, 0.059937428745122916, 0.049957123314668894, 0.04605920552038606, 0.042639450402434435, 0.04281768136407002, 0.040688395779311486, 0.03949388253735116, 0.0368740936922573, 0.0416021838493887, 0.03889758220039772, 0.039806757366398854, 0.04152765386894134, 0.04086332720684656, 0.04053616739390354, 0.04174338255295582, 0.04049281696833893, 0.042310633003345754, 0.044359081335503, 0.04259409805506098, 0.04089449114314283, 0.04184829219363564, 0.04464435362585086, 0.05217279204131368, 0.045595863822587616, 0.04646466067300048, 0.0487172200887473, 0.04389396871376749, 0.04669995874618322, 0.05678767895578307, 0.05308367564943765, 0.04527582020821234, 0.049029641540723624, 0.04719649033139947, 0.046565787682395866, 0.04791477806712135, 0.04916406467161599, 0.05096957461807871, 0.05258279599836559, 0.04880052260273143, 0.05147250026908827, 0.04829441767235871, 0.056325360274348674, 0.04720727445868349, 0.047670135901401386, 0.0463200619653364, 0.04882035008436187, 0.049009314921362845, 0.05373440254439971, 0.04743918941322751, 0.059623282884927103, 0.04865553924810832, 0.047669939959813605, 0.050165749761785776, 0.051591006755855, 0.047874718416954114, 0.051239824762233904, 0.04784904920146319, 0.050011367968259904, 0.060356651329305996, 0.046313912672610445, 0.04862053754853005, 0.04940017367899394, 0.04758619282302694, 0.046681486228041004, 0.049130217423079665, 0.05164942839876358, 0.052762547876105195, 0.06489822047680538, 0.04881305327572936, 0.05085676843346555, 0.05062324786398295, 0.05361814776941006, 0.0642266757351011, 0.05217191647521637, 0.04958034777256229, 0.045850167019188155, 0.056117019815144754, 0.05398762960088681, 0.05947782131973381, 0.0519069066769346, 0.06101271409191058, 0.05093390030919191, 0.0500298103903648, 0.05535428805372206, 0.06046080542524093, 0.05201481054868989, 0.05051208631991579, 0.055676751141340124, 0.05965404046336841, 0.04758932897553843, 0.04808491084254628, 0.04850789080897309], 'val_acc': [0.822328029069039, 0.8635509334669841, 0.8957524119784488, 0.914045858914923, 0.9196842500939731, 0.9263250219270768, 0.9287056759804536, 0.930835734870317, 0.9378523994486906, 0.9367247212128806, 0.9406089462473374, 0.9431148978824708, 0.9426137075554442, 0.9407342438290941, 0.9419872196466608, 0.9475003132439543, 0.9421125172284175, 0.9475003132439543, 0.9478762059892244, 0.9442425761182809, 0.9387294825209873, 0.9463726350081444, 0.9438666833730109, 0.9507580503696279, 0.9414860293196341, 0.9487532890615211, 0.9447437664453076, 0.9469991229169277, 0.9510086455331412, 0.9487532890615211, 0.9438666833730109, 0.9467485277534143, 0.9543916802405713, 0.9508833479513845, 0.9532640020047614, 0.9551434657311114, 0.9522616213507079, 0.951885728605438, 0.952637514095978, 0.952637514095978, 0.9555193584763814, 0.9491291818067912, 0.953389299586518, 0.9486279914797644, 0.9520110261871946, 0.9563964415486781, 0.9592782859290816, 0.9595288810925949, 0.9586517980202982, 0.9572735246209748, 0.9573988222027315, 0.9451196591905776, 0.9558952512216514, 0.9573988222027315, 0.9561458463851648, 0.952637514095978, 0.9586517980202982, 0.9560205488034081, 0.9581506076932715, 0.953389299586518, 0.9576494173662449, 0.9528881092594913, 0.962536023054755, 0.9607818569101616, 0.9599047738378649, 0.9602806665831349, 0.9616589399824583, 0.9624107254729983, 0.962160130309485, 0.9497556697155745, 0.9597794762561083, 0.9580253101115148, 0.962160130309485, 0.961408344818945, 0.9506327527878712, 0.9575241197844881, 0.9610324520736749, 0.9620348327277284, 0.9540157874953014, 0.9584012028567849, 0.9602806665831349, 0.9620348327277284, 0.9558952512216514, 0.961408344818945, 0.9610324520736749, 0.9634131061270518, 0.9550181681493547, 0.9627866182182684, 0.9654178674351584, 0.9615336424007017, 0.9581506076932715, 0.9581506076932715, 0.962536023054755, 0.963287808545295], 'size_bytes': 620811, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 94, 'batch_size': 64, 'lr': 0.00017162563498530662, 'weight_decay': 1.6656608232656036e-06, 'patch_size': 250, 'n_heads': 4, 'head_dim': 17, 'num_layers': 4, 'mlp_ratio': 4, 'dropout': 0.036854571873314375, 'attn_dropout': 0.03785920438360317, 'augment_noise_std': 0.011756388140362369, 'augment_scale_low': 0.8044194874343102, 'augment_scale_high': 1.1220904223976729, 'use_focal_loss': True, 'focal_gamma': 4.412321138984504, 'class_weighted': False, 'grad_clip': 3.8007527247818387, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 110568, 'model_storage_size_kb': 475.096875, 'model_size_validation': 'FAIL'}
2025-10-02 20:32:06,054 - INFO - _models.training_function_executor - BO Objective: base=0.9633, size_penalty=0.4279, final=0.5354
2025-10-02 20:32:06,054 - INFO - _models.training_function_executor - Model: 110,568 parameters, 475.1KB (FAIL 256KB limit)
2025-10-02 20:32:06,054 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 206.091s
2025-10-02 20:32:06,154 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5354
2025-10-02 20:32:06,154 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.099s
2025-10-02 20:32:06,154 - INFO - bo.run_bo - Recorded observation #18: hparams={'epochs': np.int64(94), 'batch_size': np.int64(64), 'lr': 0.00017162563498530662, 'weight_decay': 1.6656608232656036e-06, 'patch_size': np.int64(250), 'n_heads': np.int64(4), 'head_dim': np.int64(17), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(4), 'dropout': 0.036854571873314375, 'attn_dropout': 0.03785920438360317, 'augment_noise_std': 0.011756388140362369, 'augment_scale_low': 0.8044194874343102, 'augment_scale_high': 1.1220904223976729, 'use_focal_loss': np.True_, 'focal_gamma': 4.412321138984504, 'class_weighted': np.False_, 'grad_clip': 3.8007527247818387, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.5354
2025-10-02 20:32:06,154 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 18: {'epochs': np.int64(94), 'batch_size': np.int64(64), 'lr': 0.00017162563498530662, 'weight_decay': 1.6656608232656036e-06, 'patch_size': np.int64(250), 'n_heads': np.int64(4), 'head_dim': np.int64(17), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(4), 'dropout': 0.036854571873314375, 'attn_dropout': 0.03785920438360317, 'augment_noise_std': 0.011756388140362369, 'augment_scale_low': 0.8044194874343102, 'augment_scale_high': 1.1220904223976729, 'use_focal_loss': np.True_, 'focal_gamma': 4.412321138984504, 'class_weighted': np.False_, 'grad_clip': 3.8007527247818387, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.5354
2025-10-02 20:32:06,154 - INFO - bo.run_bo - üîçBO Trial 19: Using RF surrogate + Expected Improvement
2025-10-02 20:32:06,154 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:32:06,154 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:32:06,154 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:32:06,154 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 87, 'batch_size': 256, 'lr': 0.002244798923101265, 'weight_decay': 0.0006059075420307161, 'patch_size': 5, 'n_heads': 2, 'head_dim': 9, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.039956005130565415, 'attn_dropout': 0.22610018185134506, 'augment_noise_std': 0.025213343670804633, 'augment_scale_low': 0.8500208784813386, 'augment_scale_high': 1.0142988419062484, 'use_focal_loss': True, 'focal_gamma': 4.661557650492023, 'class_weighted': False, 'grad_clip': 0.32418214625748293, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:32:06,155 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 87, 'batch_size': 256, 'lr': 0.002244798923101265, 'weight_decay': 0.0006059075420307161, 'patch_size': 5, 'n_heads': 2, 'head_dim': 9, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.039956005130565415, 'attn_dropout': 0.22610018185134506, 'augment_noise_std': 0.025213343670804633, 'augment_scale_low': 0.8500208784813386, 'augment_scale_high': 1.0142988419062484, 'use_focal_loss': True, 'focal_gamma': 4.661557650492023, 'class_weighted': False, 'grad_clip': 0.32418214625748293, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:36:54,202 - INFO - _models.training_function_executor - Model: 10,823 parameters, 46.5KB storage
2025-10-02 20:36:54,202 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.1670048817323775, 0.08867777104593103, 0.06537123359591576, 0.05363366895022146, 0.04469000645839811, 0.040616444224368074, 0.03692829312135659, 0.03507082142558749, 0.030775595779785073, 0.02949732537527659, 0.027467118995049743, 0.027357613309588632, 0.024881668587343638, 0.024302430400736957, 0.023146285789157256, 0.021597424958369076, 0.020787361906010812, 0.019271171522306257, 0.017907454678055053, 0.018024495910646945, 0.017519091429519157, 0.016874318664165732, 0.015717973986654168, 0.016852752335441472, 0.015124711327250571, 0.014649983931713139, 0.013311566706908208, 0.014010864339607378, 0.013607910552665092, 0.013392672070755696, 0.013020232214542772, 0.012144822278282471, 0.011726163483308218, 0.011438970624267658, 0.01190263713088224, 0.010853585414944375, 0.010553421926544439, 0.009970841431578387, 0.010518584607463514, 0.009286191279041574, 0.009237141100340206, 0.00954959006358713, 0.008994752914203579, 0.008999170799661269, 0.008912731777306322, 0.009521705166279804, 0.009231447052087805, 0.008514882829861245, 0.008646824981137856, 0.008237513634797938, 0.008532348582369668, 0.0077065200167323435, 0.007759626206684266, 0.007712403270752484, 0.00709799587108337, 0.008459046986525274, 0.006430830933196573, 0.007438110831921722, 0.007152259599815265, 0.006702018753983039, 0.006603999797092185, 0.007512038742773732, 0.006987715660566485, 0.00716571275629717, 0.006711426077632476, 0.00635088157516504, 0.006274841844170718, 0.00609648729912374, 0.006447672046344287, 0.006567875370440444, 0.006669561588273227, 0.00583273925875188, 0.006073989915306961, 0.00561903040858807, 0.00646267315432398, 0.0059250643439150985, 0.006780082569452485, 0.005419099740445681, 0.00470297841982417, 0.0048196909678915245, 0.007323006181340383, 0.006356834359360253, 0.0050749427033834815, 0.005268972948194951, 0.005007668227740119, 0.005784052749933809, 0.005349686890202203], 'val_losses': [0.11052771209139466, 0.07337992106710325, 0.059764173204379635, 0.05237034957749983, 0.05004988093703496, 0.044980832649930025, 0.04215172436282033, 0.03889778748298405, 0.03934122618769041, 0.03871788318078629, 0.0356216273544658, 0.03903277020711466, 0.03839396487824855, 0.03684532963642201, 0.03927313962379743, 0.0376550294150798, 0.031842381502397224, 0.03849754737270698, 0.03758319030969667, 0.036959619365297396, 0.03416677113389509, 0.03404119901537388, 0.031312064683470205, 0.03371984858237148, 0.03428618390523406, 0.03223919963036813, 0.031887118500988666, 0.03554859103992266, 0.03142878302669274, 0.0351023083939437, 0.032969523267464565, 0.03274706870507901, 0.03257599932674537, 0.03284957017079733, 0.03613294958377479, 0.03252253038305759, 0.035287597202079184, 0.03229239746011479, 0.03210330872797993, 0.0322128131625318, 0.03056157426739408, 0.03485207508910107, 0.03353847805053545, 0.03425774227301738, 0.03241995994368975, 0.03244609138060745, 0.030803666241787202, 0.037791588793062596, 0.0315254098379721, 0.033557111738827756, 0.034007206903657376, 0.0353207008971739, 0.032261878202551816, 0.030688634640089685, 0.03921767847644941, 0.030900393515477995, 0.03315352860596855, 0.032987632480999715, 0.03690810109395429, 0.033090236849869094, 0.032603005150583, 0.03273239184174229, 0.03612212451283846, 0.03214817792139859, 0.03144772896325397, 0.034399128938741426, 0.03190136813336806, 0.03760131462790467, 0.03184261557721596, 0.029984919466577727, 0.03454127740089255, 0.033770205253027326, 0.03342691481266535, 0.03397358628334131, 0.03133765439848421, 0.02917746517304534, 0.033515043447466066, 0.029937703253451842, 0.032757094739754834, 0.03687424981090721, 0.032564479572486794, 0.029523968581768853, 0.028635826087389325, 0.031945652021880495, 0.031956240234182615, 0.03294328677932088, 0.03633896322227065], 'val_acc': [0.7916301215386543, 0.861671469740634, 0.8960030071419621, 0.8968800902142589, 0.8986342563588523, 0.9165518105500564, 0.9179300839493798, 0.930459842125047, 0.9267009146723468, 0.9240696654554567, 0.9382282921939606, 0.9215637138203233, 0.9135446685878963, 0.9111640145345195, 0.9250720461095101, 0.916677108131813, 0.930083949379777, 0.9145470492419496, 0.9181806791128931, 0.930459842125047, 0.9364741260493672, 0.9264503195088335, 0.9348452574865305, 0.9364741260493672, 0.9285803783986969, 0.9352211502318005, 0.932339305851397, 0.9210625234932965, 0.9367247212128806, 0.9196842500939731, 0.9394812680115274, 0.9417366244831474, 0.9293321638892369, 0.9358476381405839, 0.9248214509459968, 0.9158000250595163, 0.9211878210750533, 0.9408595414108508, 0.930083949379777, 0.9205613331662699, 0.9419872196466608, 0.92544793885478, 0.9382282921939606, 0.9180553815311364, 0.9431148978824708, 0.9372259115399073, 0.9398571607567974, 0.9269515098358602, 0.9454955519358477, 0.9201854404209999, 0.9349705550682872, 0.9292068663074803, 0.9419872196466608, 0.9350958526500438, 0.9278285929081569, 0.9402330535020674, 0.9338428768324771, 0.9194336549304598, 0.9280791880716702, 0.92544793885478, 0.9491291818067912, 0.9373512091216639, 0.9379776970304473, 0.9439919809547676, 0.9340934719959905, 0.930459842125047, 0.936975316376394, 0.9213131186568099, 0.9374765067034206, 0.9365994236311239, 0.9270768074176169, 0.9260744267635634, 0.946122039844631, 0.9372259115399073, 0.9379776970304473, 0.9441172785365243, 0.9328404961784238, 0.9471244204986844, 0.92544793885478, 0.9190577621851899, 0.9279538904899135, 0.9448690640270643, 0.9488785866432777, 0.9411101365743642, 0.9329657937601804, 0.9330910913419371, 0.92206490414735], 'size_bytes': 53321, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 87, 'batch_size': 256, 'lr': 0.002244798923101265, 'weight_decay': 0.0006059075420307161, 'patch_size': 5, 'n_heads': 2, 'head_dim': 9, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.039956005130565415, 'attn_dropout': 0.22610018185134506, 'augment_noise_std': 0.025213343670804633, 'augment_scale_low': 0.8500208784813386, 'augment_scale_high': 1.0142988419062484, 'use_focal_loss': True, 'focal_gamma': 4.661557650492023, 'class_weighted': False, 'grad_clip': 0.32418214625748293, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 10823, 'model_storage_size_kb': 46.505078125000004, 'model_size_validation': 'PASS'}
2025-10-02 20:36:54,203 - INFO - _models.training_function_executor - BO Objective: base=0.9221, size_penalty=0.0000, final=0.9221
2025-10-02 20:36:54,203 - INFO - _models.training_function_executor - Model: 10,823 parameters, 46.5KB (PASS 256KB limit)
2025-10-02 20:36:54,203 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 288.048s
2025-10-02 20:36:54,303 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9221
2025-10-02 20:36:54,303 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.101s
2025-10-02 20:36:54,303 - INFO - bo.run_bo - Recorded observation #19: hparams={'epochs': np.int64(87), 'batch_size': np.int64(256), 'lr': 0.002244798923101265, 'weight_decay': 0.0006059075420307161, 'patch_size': np.int64(5), 'n_heads': np.int64(2), 'head_dim': np.int64(9), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.039956005130565415, 'attn_dropout': 0.22610018185134506, 'augment_noise_std': 0.025213343670804633, 'augment_scale_low': 0.8500208784813386, 'augment_scale_high': 1.0142988419062484, 'use_focal_loss': np.True_, 'focal_gamma': 4.661557650492023, 'class_weighted': np.False_, 'grad_clip': 0.32418214625748293, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9221
2025-10-02 20:36:54,303 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 19: {'epochs': np.int64(87), 'batch_size': np.int64(256), 'lr': 0.002244798923101265, 'weight_decay': 0.0006059075420307161, 'patch_size': np.int64(5), 'n_heads': np.int64(2), 'head_dim': np.int64(9), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.039956005130565415, 'attn_dropout': 0.22610018185134506, 'augment_noise_std': 0.025213343670804633, 'augment_scale_low': 0.8500208784813386, 'augment_scale_high': 1.0142988419062484, 'use_focal_loss': np.True_, 'focal_gamma': 4.661557650492023, 'class_weighted': np.False_, 'grad_clip': 0.32418214625748293, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9221
2025-10-02 20:36:54,304 - INFO - bo.run_bo - üîçBO Trial 20: Using RF surrogate + Expected Improvement
2025-10-02 20:36:54,304 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:36:54,304 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:36:54,304 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:36:54,304 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 80, 'batch_size': 512, 'lr': 0.007054261152131292, 'weight_decay': 8.262588105083553e-06, 'patch_size': 5, 'n_heads': 2, 'head_dim': 14, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.02465526267887231, 'attn_dropout': 0.033894016974286505, 'augment_noise_std': 0.01961315578040348, 'augment_scale_low': 0.8505475177760333, 'augment_scale_high': 1.1845564391868173, 'use_focal_loss': False, 'focal_gamma': 4.629161621542339, 'class_weighted': False, 'grad_clip': 3.8312064466183506, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:36:54,305 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 80, 'batch_size': 512, 'lr': 0.007054261152131292, 'weight_decay': 8.262588105083553e-06, 'patch_size': 5, 'n_heads': 2, 'head_dim': 14, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.02465526267887231, 'attn_dropout': 0.033894016974286505, 'augment_noise_std': 0.01961315578040348, 'augment_scale_low': 0.8505475177760333, 'augment_scale_high': 1.1845564391868173, 'use_focal_loss': False, 'focal_gamma': 4.629161621542339, 'class_weighted': False, 'grad_clip': 3.8312064466183506, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:40:26,159 - INFO - _models.training_function_executor - Model: 12,656 parameters, 13.6KB storage
2025-10-02 20:40:26,159 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6686602265872226, 0.35647062956119874, 0.26049431499183745, 0.2267175552625463, 0.1954383882156415, 0.1909919004451249, 0.17186854224200088, 0.16434914201028447, 0.14587859073279513, 0.15051534301073738, 0.14358102547675414, 0.12814948890495095, 0.12496248865478211, 0.12559828456687633, 0.11495626177339444, 0.11293529202617268, 0.10749682807653121, 0.10709118009424858, 0.09863187851885434, 0.10468452534648935, 0.10550674390480382, 0.09144546597107488, 0.09352541495891412, 0.08890775133919619, 0.086717490241255, 0.08366856257029279, 0.08309508107374572, 0.0875774066667339, 0.08252602338574305, 0.08282702067703605, 0.06970391433129494, 0.07056909245409262, 0.07871659256475176, 0.0829094186769696, 0.0756044623031253, 0.07261473836531127, 0.06733962824833695, 0.07573392844216845, 0.06648243549890627, 0.06675551933574635, 0.06086059382052715, 0.05904977224546419, 0.058850585921179345, 0.06567045336890469, 0.06231711888887999, 0.0658329953621142, 0.06580139102369731, 0.06142056998666791, 0.05475154166279565, 0.05475287378502756, 0.06086502117348286, 0.05178536844538092, 0.047079102646185936, 0.053904875022529365, 0.06360815461179209, 0.05799865616392762, 0.05304881949261864, 0.051234353740864394, 0.0533573191556478, 0.04949030906257932, 0.0555017133809154, 0.05716744195214191, 0.05303226924747209, 0.04617789314213559, 0.05241333351813062, 0.04810250890467641, 0.03950047207180485, 0.05075165698570198, 0.038849337159685485, 0.049548553980713526, 0.04445502961814895, 0.04730731853049762, 0.05100533791654029, 0.050589167934022065, 0.04175640967886736, 0.04431948976436782, 0.04212885953634637, 0.04059128924273392, 0.04474780278596215, 0.04336681322086721], 'val_losses': [0.4110470151351577, 0.2945041214070991, 0.22896118587553463, 0.2102552717613436, 0.19002235741778703, 0.17634037374150646, 0.17872828386874295, 0.15752634517741135, 0.1667549174436276, 0.16767088608094408, 0.14392640484505587, 0.15963071215953345, 0.14412015963468527, 0.15314961053863263, 0.14029030915027185, 0.15079350810782263, 0.13634968935965355, 0.14062796185457563, 0.1391708443743919, 0.13617980219176382, 0.12457574580586833, 0.14726536824894107, 0.1415292552529145, 0.13225144358106072, 0.14615665747548476, 0.13747023725820326, 0.148982641926506, 0.13256728546076618, 0.15525399165454626, 0.13103983971450042, 0.13173418655541588, 0.15090020681671773, 0.155852387767659, 0.17333958633388735, 0.12469691465132787, 0.13656333755422184, 0.13897426853411748, 0.14016607032953207, 0.13401238073226937, 0.14728981492297122, 0.13972173905091811, 0.14265832514440843, 0.15828403507150096, 0.13813123547587236, 0.13156571375249757, 0.14143697224028082, 0.14917936684560723, 0.14454829190036078, 0.1502428362317558, 0.14036063449064035, 0.16182150908608944, 0.152646281374993, 0.1712669584591606, 0.1966263956517866, 0.1431053417668637, 0.16080630779669647, 0.15112364378590554, 0.16162559197572163, 0.15905001226291218, 0.17732618288346483, 0.15662720077446476, 0.14711882486004024, 0.1614795177204091, 0.1575534578224663, 0.1474759915171224, 0.14487122206322875, 0.18173637992717945, 0.14971968293929366, 0.157605238513695, 0.16416841517966307, 0.14937660052892543, 0.16257326174030295, 0.18168525733291738, 0.1773892143210766, 0.14871302620463317, 0.16581693600479067, 0.15905094199566863, 0.1809468457587096, 0.17226305691360336, 0.16474839721036993], 'val_acc': [0.8758300964791379, 0.9183059766946498, 0.9374765067034206, 0.9389800776845006, 0.9436160882094976, 0.9469991229169277, 0.9469991229169277, 0.9500062648790878, 0.9506327527878712, 0.9503821576243578, 0.955268763312868, 0.9471244204986844, 0.9540157874953014, 0.9511339431148979, 0.9577747149480015, 0.9581506076932715, 0.9597794762561083, 0.9579000125297582, 0.9589023931838115, 0.9592782859290816, 0.9610324520736749, 0.9590276907655683, 0.9576494173662449, 0.9597794762561083, 0.9560205488034081, 0.9595288810925949, 0.9572735246209748, 0.9615336424007017, 0.9566470367121914, 0.9627866182182684, 0.9612830472371883, 0.9589023931838115, 0.9611577496554317, 0.9548928705675981, 0.9635384037088084, 0.9595288810925949, 0.9592782859290816, 0.9579000125297582, 0.9594035835108382, 0.9600300714196216, 0.962536023054755, 0.9634131061270518, 0.9547675729858414, 0.962536023054755, 0.962160130309485, 0.9597794762561083, 0.9591529883473249, 0.9590276907655683, 0.962536023054755, 0.9630372133817817, 0.9582759052750282, 0.9626613206365117, 0.9577747149480015, 0.9560205488034081, 0.9601553690013783, 0.9577747149480015, 0.9592782859290816, 0.9615336424007017, 0.9599047738378649, 0.9496303721338178, 0.9610324520736749, 0.9594035835108382, 0.9566470367121914, 0.9602806665831349, 0.9609071544919183, 0.962160130309485, 0.954516977822328, 0.9650419746898885, 0.9637889988723217, 0.9605312617466483, 0.9650419746898885, 0.963287808545295, 0.9553940608946248, 0.961408344818945, 0.9630372133817817, 0.9576494173662449, 0.9620348327277284, 0.9594035835108382, 0.9517604310236812, 0.9590276907655683], 'size_bytes': 71147, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 80, 'batch_size': 512, 'lr': 0.007054261152131292, 'weight_decay': 8.262588105083553e-06, 'patch_size': 5, 'n_heads': 2, 'head_dim': 14, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.02465526267887231, 'attn_dropout': 0.033894016974286505, 'augment_noise_std': 0.01961315578040348, 'augment_scale_low': 0.8505475177760333, 'augment_scale_high': 1.1845564391868173, 'use_focal_loss': False, 'focal_gamma': 4.629161621542339, 'class_weighted': False, 'grad_clip': 3.8312064466183506, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 12656, 'model_storage_size_kb': 13.5953125, 'model_size_validation': 'PASS'}
2025-10-02 20:40:26,159 - INFO - _models.training_function_executor - BO Objective: base=0.9590, size_penalty=0.0000, final=0.9590
2025-10-02 20:40:26,159 - INFO - _models.training_function_executor - Model: 12,656 parameters, 13.6KB (PASS 256KB limit)
2025-10-02 20:40:26,159 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 211.855s
2025-10-02 20:40:26,261 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9590
2025-10-02 20:40:26,261 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.102s
2025-10-02 20:40:26,262 - INFO - bo.run_bo - Recorded observation #20: hparams={'epochs': np.int64(80), 'batch_size': np.int64(512), 'lr': 0.007054261152131292, 'weight_decay': 8.262588105083553e-06, 'patch_size': np.int64(5), 'n_heads': np.int64(2), 'head_dim': np.int64(14), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.02465526267887231, 'attn_dropout': 0.033894016974286505, 'augment_noise_std': 0.01961315578040348, 'augment_scale_low': 0.8505475177760333, 'augment_scale_high': 1.1845564391868173, 'use_focal_loss': np.False_, 'focal_gamma': 4.629161621542339, 'class_weighted': np.False_, 'grad_clip': 3.8312064466183506, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9590
2025-10-02 20:40:26,262 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 20: {'epochs': np.int64(80), 'batch_size': np.int64(512), 'lr': 0.007054261152131292, 'weight_decay': 8.262588105083553e-06, 'patch_size': np.int64(5), 'n_heads': np.int64(2), 'head_dim': np.int64(14), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.02465526267887231, 'attn_dropout': 0.033894016974286505, 'augment_noise_std': 0.01961315578040348, 'augment_scale_low': 0.8505475177760333, 'augment_scale_high': 1.1845564391868173, 'use_focal_loss': np.False_, 'focal_gamma': 4.629161621542339, 'class_weighted': np.False_, 'grad_clip': 3.8312064466183506, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9590
2025-10-02 20:40:26,262 - INFO - bo.run_bo - üîçBO Trial 21: Using RF surrogate + Expected Improvement
2025-10-02 20:40:26,262 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:40:26,262 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:40:26,262 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:40:26,262 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 91, 'batch_size': 512, 'lr': 0.0012522446417439028, 'weight_decay': 0.00012049319490218728, 'patch_size': 250, 'n_heads': 2, 'head_dim': 9, 'num_layers': 1, 'mlp_ratio': 2, 'dropout': 0.014453577313641889, 'attn_dropout': 0.030294703809428496, 'augment_noise_std': 0.03885344250443289, 'augment_scale_low': 0.8133256096158054, 'augment_scale_high': 1.0282057151709247, 'use_focal_loss': True, 'focal_gamma': 4.5709943964727175, 'class_weighted': False, 'grad_clip': 3.606185757576213, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:40:26,263 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 91, 'batch_size': 512, 'lr': 0.0012522446417439028, 'weight_decay': 0.00012049319490218728, 'patch_size': 250, 'n_heads': 2, 'head_dim': 9, 'num_layers': 1, 'mlp_ratio': 2, 'dropout': 0.014453577313641889, 'attn_dropout': 0.030294703809428496, 'augment_noise_std': 0.03885344250443289, 'augment_scale_low': 0.8133256096158054, 'augment_scale_high': 1.0282057151709247, 'use_focal_loss': True, 'focal_gamma': 4.5709943964727175, 'class_weighted': False, 'grad_clip': 3.606185757576213, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:41:24,482 - INFO - _models.training_function_executor - Model: 11,993 parameters, 51.5KB storage
2025-10-02 20:41:24,483 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.32392923699951715, 0.18841846361982714, 0.16590463161875543, 0.15070409816212066, 0.13944223923267615, 0.1292167837655892, 0.12106535238197295, 0.11463881186650585, 0.10689251770001697, 0.10192075447734764, 0.09681458062113137, 0.09382150674831209, 0.08867503382683106, 0.08652356637147171, 0.08242834970218266, 0.07936571446981913, 0.0761581921604387, 0.0743569190956516, 0.07071502542511972, 0.06948830125681818, 0.06695306025107275, 0.06499025463384352, 0.06346038132985314, 0.06251192824813584, 0.060047996717075454, 0.058631484550123576, 0.05662893713140489, 0.05559414060965437, 0.05380537046643945, 0.05302909390596923, 0.05273034483151407, 0.05031651885406873, 0.04968778465678473, 0.04973402081136024, 0.04763746735592987, 0.04759315366026782, 0.047130960369781906, 0.046653716061726976, 0.0443332433765678, 0.04423517589249176, 0.04359742374332485, 0.042548250383991376, 0.0428433084965879, 0.04193195430838178, 0.04135149249554528, 0.041483652385841695, 0.03970678567013932, 0.03972247035523534, 0.038819762716116556, 0.037563423103355406, 0.03837273469547134, 0.03801399089424707, 0.0371186400398169, 0.03715465455340367, 0.036149923777214675, 0.03546542358733402, 0.03581521550856088, 0.03471210486876497, 0.034377128447710306, 0.034280488443810424, 0.03429971556863502, 0.03452700665823646, 0.032771294139911665, 0.03280686136671569, 0.0327370917799796, 0.03193687541014202, 0.03292984111543265, 0.03111785828483283, 0.03112310165549245, 0.030350138934283233, 0.030457840103442383, 0.0305332521464044, 0.03053233103882481, 0.029178519624051744, 0.02970092668097778, 0.02994540298447287, 0.029909952014721554, 0.029202858950819408, 0.028791627157538455, 0.02803952371454144, 0.027697820707766333, 0.027593816870031894, 0.02718217812312367, 0.028504962374705987, 0.02761151130046495, 0.02699405479319311, 0.02640610469004973, 0.02725640868559385, 0.027006563990215938, 0.026599999257320112, 0.026599415754363455], 'val_losses': [0.20825827827677365, 0.1730420812955104, 0.15666142026518629, 0.14466099211027295, 0.1349390743874679, 0.12340878514702973, 0.11916577491324402, 0.11049895511225731, 0.1062263010543266, 0.10133394819462961, 0.09526028454326146, 0.09405082599954817, 0.09022996025718912, 0.08603027380621085, 0.08484708317545568, 0.08081107428573962, 0.08165761648062671, 0.07644800333029166, 0.07639971203787436, 0.07695636523182926, 0.07434704225025385, 0.07077747977436404, 0.07156734347022307, 0.06881372376953415, 0.06898208778940142, 0.06842838161280843, 0.06850447898125936, 0.06609375620779649, 0.06664931036409631, 0.06571066874789946, 0.06695239759151715, 0.0651561560233185, 0.06415427006643871, 0.06321178655009523, 0.06443255259567313, 0.06431394921775122, 0.06451281616617124, 0.06311189972378604, 0.0640418751632274, 0.061143149816311027, 0.06145797722585861, 0.0625709994246698, 0.06242020914508218, 0.060397764892618305, 0.06155526594061284, 0.06161982102382379, 0.06273177548143263, 0.06070821261405198, 0.06126722229053959, 0.059034242762082086, 0.06389010257065826, 0.059953879153776285, 0.060458891768680155, 0.06016889937122115, 0.062421629551455964, 0.05991511586379055, 0.06170704675813379, 0.058985126739590256, 0.05896735407366309, 0.06263307551086106, 0.05906915614915184, 0.05906431148963649, 0.060548833783871395, 0.0594910223066411, 0.05830897740733129, 0.06121740842702738, 0.06127593396895512, 0.06089222179220488, 0.0598670410900438, 0.05880593250358244, 0.061081215855009106, 0.05840012514323104, 0.05783662064704153, 0.05856565726892256, 0.05771382243867626, 0.058113453741017444, 0.05965106239195322, 0.058833063020322006, 0.05717762292940027, 0.05756290571019847, 0.057765296871927115, 0.05870189036963473, 0.05884872591912993, 0.05771719505166727, 0.05698536584930243, 0.05794878956548941, 0.0606748819297051, 0.059726284705536536, 0.05784987953727578, 0.05878182761805232, 0.058356744278203185], 'val_acc': [0.7284801403332916, 0.7540408470116526, 0.7629369753163764, 0.776469114146097, 0.783360481142714, 0.7818569101616338, 0.7916301215386543, 0.7947625610825712, 0.7956396441548678, 0.8056634506954016, 0.8149354717453953, 0.8102994612203984, 0.8163137451447187, 0.8264628492670092, 0.832602430773086, 0.8362360606440296, 0.837990226788623, 0.8532765317629369, 0.8505199849642902, 0.8539030196717203, 0.8617967673223906, 0.870066407718331, 0.8668086705926575, 0.8706928956271144, 0.8675604560831975, 0.8715699786994111, 0.8699411101365744, 0.8774589650419747, 0.8740759303345446, 0.8790878336048115, 0.8709434907906277, 0.8833479513845383, 0.8878586643277785, 0.8899887232176419, 0.8829720586392683, 0.8809672973311615, 0.8819696779852149, 0.8888610449818318, 0.883473248966295, 0.8931211627615587, 0.8951259240696654, 0.8901140207993986, 0.8901140207993986, 0.8941235434156121, 0.8944994361608821, 0.8934970555068287, 0.8963788998872322, 0.8972559829595289, 0.8943741385791254, 0.9013908031574991, 0.8901140207993986, 0.8982583636135822, 0.899135446685879, 0.8952512216514221, 0.8936223530885854, 0.8980077684500689, 0.8932464603433153, 0.8987595539406089, 0.9050244330284426, 0.8992607442676356, 0.9005137200852024, 0.9011402079939858, 0.8993860418493923, 0.9028943741385791, 0.9008896128304724, 0.900639017666959, 0.9036461596291192, 0.900639017666959, 0.9005137200852024, 0.9048991354466859, 0.899511339431149, 0.909033955644656, 0.9062774088460093, 0.9056509209372259, 0.908658062899386, 0.9020172910662824, 0.9061521112642525, 0.9008896128304724, 0.9095351459716827, 0.9094098483899261, 0.9107881217892495, 0.9126675855155996, 0.9081568725723593, 0.9038967547926325, 0.9109134193710061, 0.9114146096980328, 0.9074050870818193, 0.9075303846635759, 0.9070291943365493, 0.9099110387169528, 0.9065280040095226], 'size_bytes': 54445, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 91, 'batch_size': 512, 'lr': 0.0012522446417439028, 'weight_decay': 0.00012049319490218728, 'patch_size': 250, 'n_heads': 2, 'head_dim': 9, 'num_layers': 1, 'mlp_ratio': 2, 'dropout': 0.014453577313641889, 'attn_dropout': 0.030294703809428496, 'augment_noise_std': 0.03885344250443289, 'augment_scale_low': 0.8133256096158054, 'augment_scale_high': 1.0282057151709247, 'use_focal_loss': True, 'focal_gamma': 4.5709943964727175, 'class_weighted': False, 'grad_clip': 3.606185757576213, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 11993, 'model_storage_size_kb': 51.532421875000004, 'model_size_validation': 'PASS'}
2025-10-02 20:41:24,483 - INFO - _models.training_function_executor - BO Objective: base=0.9065, size_penalty=0.0000, final=0.9065
2025-10-02 20:41:24,483 - INFO - _models.training_function_executor - Model: 11,993 parameters, 51.5KB (PASS 256KB limit)
2025-10-02 20:41:24,483 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 58.221s
2025-10-02 20:41:24,587 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9065
2025-10-02 20:41:24,587 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.104s
2025-10-02 20:41:24,587 - INFO - bo.run_bo - Recorded observation #21: hparams={'epochs': np.int64(91), 'batch_size': np.int64(512), 'lr': 0.0012522446417439028, 'weight_decay': 0.00012049319490218728, 'patch_size': np.int64(250), 'n_heads': np.int64(2), 'head_dim': np.int64(9), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(2), 'dropout': 0.014453577313641889, 'attn_dropout': 0.030294703809428496, 'augment_noise_std': 0.03885344250443289, 'augment_scale_low': 0.8133256096158054, 'augment_scale_high': 1.0282057151709247, 'use_focal_loss': np.True_, 'focal_gamma': 4.5709943964727175, 'class_weighted': np.False_, 'grad_clip': 3.606185757576213, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9065
2025-10-02 20:41:24,587 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 21: {'epochs': np.int64(91), 'batch_size': np.int64(512), 'lr': 0.0012522446417439028, 'weight_decay': 0.00012049319490218728, 'patch_size': np.int64(250), 'n_heads': np.int64(2), 'head_dim': np.int64(9), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(2), 'dropout': 0.014453577313641889, 'attn_dropout': 0.030294703809428496, 'augment_noise_std': 0.03885344250443289, 'augment_scale_low': 0.8133256096158054, 'augment_scale_high': 1.0282057151709247, 'use_focal_loss': np.True_, 'focal_gamma': 4.5709943964727175, 'class_weighted': np.False_, 'grad_clip': 3.606185757576213, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9065
2025-10-02 20:41:24,587 - INFO - bo.run_bo - üîçBO Trial 22: Using RF surrogate + Expected Improvement
2025-10-02 20:41:24,587 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:41:24,588 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:41:24,588 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:41:24,588 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 93, 'batch_size': 256, 'lr': 0.0007904134943829129, 'weight_decay': 0.004615867609481725, 'patch_size': 200, 'n_heads': 2, 'head_dim': 26, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.000335798247279584, 'attn_dropout': 0.17562220581120216, 'augment_noise_std': 0.04282071328394219, 'augment_scale_low': 0.831384632173375, 'augment_scale_high': 1.1862910788589283, 'use_focal_loss': False, 'focal_gamma': 3.901695652136759, 'class_weighted': False, 'grad_clip': 4.640279207335707, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:41:24,589 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 93, 'batch_size': 256, 'lr': 0.0007904134943829129, 'weight_decay': 0.004615867609481725, 'patch_size': 200, 'n_heads': 2, 'head_dim': 26, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.000335798247279584, 'attn_dropout': 0.17562220581120216, 'augment_noise_std': 0.04282071328394219, 'augment_scale_low': 0.831384632173375, 'augment_scale_high': 1.1862910788589283, 'use_focal_loss': False, 'focal_gamma': 3.901695652136759, 'class_weighted': False, 'grad_clip': 4.640279207335707, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:42:39,059 - INFO - _models.training_function_executor - Model: 76,757 parameters, 164.9KB storage
2025-10-02 20:42:39,059 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7422519378172471, 0.4950993526757886, 0.3459625787701825, 0.2788652883258465, 0.23530198617403367, 0.20929445806416122, 0.19556018120400892, 0.18143898202647535, 0.16522597558069388, 0.15418728908601345, 0.14528628608830774, 0.13729206664530372, 0.13040781409529537, 0.1190310249569138, 0.11684797128861259, 0.11564969425063547, 0.10566102741796995, 0.09877395593006838, 0.09257981698295248, 0.08807677919227662, 0.08345129994595406, 0.08122329140775235, 0.07933271591813965, 0.07005789979061876, 0.06797076179502273, 0.06756731866063119, 0.06421599455481894, 0.0602610126802668, 0.05789721673619115, 0.0572424030036861, 0.05385253532376164, 0.05227130746791391, 0.04872174943693635, 0.04860201627702241, 0.046259048514793275, 0.04333646022920205, 0.037575646605691196, 0.03747557423591308, 0.036976085720493414, 0.03853661334700772, 0.036749232122260604, 0.03235195613215394, 0.033163549680115416, 0.0365394680491437, 0.034606344567740935, 0.0272360981120799, 0.027175570666464192, 0.027981342474563534, 0.02527732547209555, 0.03244213404256038, 0.03512373745426383, 0.025194595335277518, 0.02085903525974235, 0.021635723020353364, 0.026034014112506373, 0.02611617783855698, 0.019748414728458053, 0.01991449952423358, 0.026031431264754535, 0.02386543588505151, 0.024172181747213094, 0.01941154053441673, 0.0176146264281261, 0.014857595730204762, 0.018472548536574714, 0.02038535812504234, 0.0223284457691689, 0.01735991150272279, 0.020281051106844788, 0.024889924409107707, 0.0171684346106673, 0.017579739646343656, 0.014685172815949095, 0.011371692061831291, 0.018094610876564176, 0.021187143549449113, 0.020540032460580747, 0.01516365641015322, 0.014198616534472446, 0.011635417432289383, 0.012042581003004196, 0.018111819133099016, 0.018475357443765255, 0.014989213230011275, 0.013262705501720122, 0.013463667247338081, 0.012803796452449381, 0.015999564779280245, 0.020568533814137452, 0.015217011721717041, 0.012867733590470326, 0.012893558283664756, 0.008634226868241612], 'val_losses': [0.5854288674755632, 0.417721159142848, 0.3020533476575188, 0.2517121846668255, 0.22419643571257009, 0.21683869241749548, 0.20408040098589236, 0.19955862056132084, 0.2013468654767237, 0.19222266035911328, 0.18660094109995742, 0.19107411910665767, 0.1765404442653607, 0.1911943690689595, 0.16878832344751343, 0.17626046197125525, 0.17030851533987695, 0.1713312283287131, 0.1617624659430576, 0.1599243344773387, 0.1634090917721501, 0.15640190076990812, 0.16277252186018337, 0.1684855101901985, 0.15873287915959422, 0.15656579858026562, 0.1582165446691145, 0.1553042834656622, 0.16576238004743957, 0.15331459180773419, 0.16555760321559174, 0.17167536035887715, 0.16084370733636794, 0.16099903469186666, 0.17014688928351937, 0.17650663004104034, 0.17281564120555667, 0.1751144153292713, 0.17915920777296068, 0.1786049703371105, 0.16854675774354114, 0.17880323922263697, 0.18239156782530017, 0.19193554854366118, 0.18171849603186035, 0.1776571622984912, 0.1830477471463499, 0.18406247037564763, 0.19657098662956304, 0.19923211545472813, 0.18642592364953672, 0.19431550569655887, 0.17358806516843067, 0.1815923189492531, 0.19060274302892227, 0.18901321995065648, 0.18192010112699597, 0.19392705599611348, 0.18313696778458383, 0.1936501098906183, 0.18863964633466068, 0.1780005537515863, 0.1788495264993158, 0.2023927068825339, 0.19922413068776024, 0.207489970273772, 0.19628604478427394, 0.20202308954817358, 0.1970790543363232, 0.1953061467169401, 0.1855632924537673, 0.18980692020634812, 0.20051846162834616, 0.19881700006751335, 0.22606767989044396, 0.21077370343389346, 0.1983414935114957, 0.19405082550029112, 0.2085098236325002, 0.20090545470621066, 0.19209847610377082, 0.19510277509764015, 0.1954672516863562, 0.2123211349420508, 0.19753849334233448, 0.19219789282000913, 0.20712505047773302, 0.20046511809638592, 0.2096291802447327, 0.20730655436385262, 0.2088744569513347, 0.21585937992789486, 0.2070124121577709], 'val_acc': [0.8104247588021551, 0.8670592657561709, 0.9116652048615461, 0.9260744267635634, 0.9342187695777472, 0.9373512091216639, 0.9408595414108508, 0.940358351083824, 0.9423631123919308, 0.9417366244831474, 0.9444931712817942, 0.9459967422628743, 0.947249718080441, 0.9437413857912542, 0.9502568600426011, 0.9488785866432777, 0.9483773963162511, 0.9513845382784112, 0.953765192331788, 0.954516977822328, 0.9561458463851648, 0.9555193584763814, 0.9530134068412479, 0.9532640020047614, 0.955268763312868, 0.9551434657311114, 0.9579000125297582, 0.9555193584763814, 0.9543916802405713, 0.9600300714196216, 0.955268763312868, 0.9580253101115148, 0.9582759052750282, 0.9595288810925949, 0.9570229294574615, 0.9582759052750282, 0.9585265004385415, 0.9580253101115148, 0.9560205488034081, 0.9580253101115148, 0.9616589399824583, 0.9572735246209748, 0.9590276907655683, 0.9571482270392181, 0.9610324520736749, 0.962160130309485, 0.9602806665831349, 0.9604059641648917, 0.9553940608946248, 0.9570229294574615, 0.9620348327277284, 0.9594035835108382, 0.9622854278912417, 0.9635384037088084, 0.9590276907655683, 0.9629119158000251, 0.9624107254729983, 0.9591529883473249, 0.9620348327277284, 0.9619095351459717, 0.9587770956020549, 0.9615336424007017, 0.9635384037088084, 0.9631625109635384, 0.9611577496554317, 0.9615336424007017, 0.9631625109635384, 0.9660443553439418, 0.9622854278912417, 0.9607818569101616, 0.9622854278912417, 0.9630372133817817, 0.9619095351459717, 0.963287808545295, 0.9586517980202982, 0.9624107254729983, 0.9612830472371883, 0.9610324520736749, 0.9649166771081318, 0.9649166771081318, 0.9611577496554317, 0.9640395940358351, 0.9652925698534018, 0.9654178674351584, 0.9622854278912417, 0.963287808545295, 0.9631625109635384, 0.9646660819446184, 0.9629119158000251, 0.9651672722716451, 0.9631625109635384, 0.9627866182182684, 0.9656684625986719], 'size_bytes': 163081, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 93, 'batch_size': 256, 'lr': 0.0007904134943829129, 'weight_decay': 0.004615867609481725, 'patch_size': 200, 'n_heads': 2, 'head_dim': 26, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.000335798247279584, 'attn_dropout': 0.17562220581120216, 'augment_noise_std': 0.04282071328394219, 'augment_scale_low': 0.831384632173375, 'augment_scale_high': 1.1862910788589283, 'use_focal_loss': False, 'focal_gamma': 3.901695652136759, 'class_weighted': False, 'grad_clip': 4.640279207335707, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 76757, 'model_storage_size_kb': 164.90761718750002, 'model_size_validation': 'PASS'}
2025-10-02 20:42:39,059 - INFO - _models.training_function_executor - BO Objective: base=0.9657, size_penalty=0.0000, final=0.9657
2025-10-02 20:42:39,060 - INFO - _models.training_function_executor - Model: 76,757 parameters, 164.9KB (PASS 256KB limit)
2025-10-02 20:42:39,060 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 74.472s
2025-10-02 20:42:39,162 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9657
2025-10-02 20:42:39,162 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.102s
2025-10-02 20:42:39,162 - INFO - bo.run_bo - Recorded observation #22: hparams={'epochs': np.int64(93), 'batch_size': np.int64(256), 'lr': 0.0007904134943829129, 'weight_decay': 0.004615867609481725, 'patch_size': np.int64(200), 'n_heads': np.int64(2), 'head_dim': np.int64(26), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.000335798247279584, 'attn_dropout': 0.17562220581120216, 'augment_noise_std': 0.04282071328394219, 'augment_scale_low': 0.831384632173375, 'augment_scale_high': 1.1862910788589283, 'use_focal_loss': np.False_, 'focal_gamma': 3.901695652136759, 'class_weighted': np.False_, 'grad_clip': 4.640279207335707, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9657
2025-10-02 20:42:39,162 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 22: {'epochs': np.int64(93), 'batch_size': np.int64(256), 'lr': 0.0007904134943829129, 'weight_decay': 0.004615867609481725, 'patch_size': np.int64(200), 'n_heads': np.int64(2), 'head_dim': np.int64(26), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.000335798247279584, 'attn_dropout': 0.17562220581120216, 'augment_noise_std': 0.04282071328394219, 'augment_scale_low': 0.831384632173375, 'augment_scale_high': 1.1862910788589283, 'use_focal_loss': np.False_, 'focal_gamma': 3.901695652136759, 'class_weighted': np.False_, 'grad_clip': 4.640279207335707, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9657
2025-10-02 20:42:39,162 - INFO - bo.run_bo - üîçBO Trial 23: Using RF surrogate + Expected Improvement
2025-10-02 20:42:39,162 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:42:39,162 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:42:39,163 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:42:39,163 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 78, 'batch_size': 64, 'lr': 0.000649459897808419, 'weight_decay': 6.768981604577105e-05, 'patch_size': 50, 'n_heads': 1, 'head_dim': 27, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.06153051428280533, 'attn_dropout': 0.2697241029675703, 'augment_noise_std': 0.02643085976386707, 'augment_scale_low': 0.8348774996506171, 'augment_scale_high': 1.1668529493335449, 'use_focal_loss': True, 'focal_gamma': 4.641260799262154, 'class_weighted': False, 'grad_clip': 1.188688832612316, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 20:42:39,164 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 78, 'batch_size': 64, 'lr': 0.000649459897808419, 'weight_decay': 6.768981604577105e-05, 'patch_size': 50, 'n_heads': 1, 'head_dim': 27, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.06153051428280533, 'attn_dropout': 0.2697241029675703, 'augment_noise_std': 0.02643085976386707, 'augment_scale_low': 0.8348774996506171, 'augment_scale_high': 1.1668529493335449, 'use_focal_loss': True, 'focal_gamma': 4.641260799262154, 'class_weighted': False, 'grad_clip': 1.188688832612316, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 20:44:17,552 - INFO - _models.training_function_executor - Model: 6,426 parameters, 6.9KB storage
2025-10-02 20:44:17,554 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.17653275441788074, 0.11259941712800892, 0.09368051699303268, 0.08234501714532479, 0.07367926511586906, 0.06778837254078537, 0.06347391381906457, 0.058692435026299526, 0.05670277373786185, 0.0529022484124351, 0.05120613890123067, 0.0489574670474796, 0.04704981011815848, 0.04583793985974078, 0.044464545281092165, 0.04312946822589842, 0.042467692507322324, 0.04147425655213618, 0.04003484728352704, 0.03876993594930326, 0.038608491468941346, 0.037669170313816926, 0.03687980735962565, 0.03618654948825679, 0.03473949422074614, 0.03547951130869964, 0.03271636763377599, 0.03308465847114153, 0.03237565544656081, 0.031749722486242776, 0.031202640700915538, 0.030842693530245286, 0.030748118718006787, 0.029688487653566423, 0.029461449498257998, 0.029317325374709425, 0.028616224263049293, 0.028535829785807133, 0.02819459164515196, 0.02730873888202976, 0.026807302655122656, 0.026354176796258805, 0.026346827005752253, 0.0268295340179203, 0.026075206626561043, 0.025358941779191194, 0.024884542387584634, 0.02436872009569883, 0.02491532355610901, 0.024244220467866694, 0.023631613638974487, 0.023445747321385157, 0.02287541852773092, 0.02308258118916444, 0.022618499621313647, 0.022094612235367753, 0.023163647798900858, 0.021813268701941814, 0.0219126239155717, 0.021186360436959123, 0.02176080883316913, 0.020512130631570532, 0.020152796104249752, 0.021127221003257403, 0.020532946069150276, 0.02001027576747878, 0.01992449948460137, 0.020221415530216072, 0.019164985635806264, 0.01938949547586904, 0.018899938184059033, 0.01927619236260753, 0.018567998357617452, 0.01794881833288748, 0.018254676890010934, 0.019119951105311186, 0.017585052107584057, 0.017483577514463184], 'val_losses': [0.12588770875501656, 0.1006048523909973, 0.0877373269325606, 0.07531958162997424, 0.06642742277586419, 0.06393956375009485, 0.05925757373483494, 0.058344525813565355, 0.05315004539642287, 0.05123820258991795, 0.05148796009873256, 0.048387089068094094, 0.04744198997086667, 0.0480000554181261, 0.045881551321534764, 0.04459847507713851, 0.04002839554595538, 0.040795469362988404, 0.04378920477063356, 0.041600915560513534, 0.04085486067316389, 0.038469520099101664, 0.041703690281012964, 0.0377517555640886, 0.038436316728569515, 0.041116031319231576, 0.0397324345216142, 0.03785992220539846, 0.034284223662240064, 0.035928992292920896, 0.03445272373478748, 0.036351861830116024, 0.03265840876994538, 0.033617756462156166, 0.03504303665083686, 0.03460046001102613, 0.032182445658853726, 0.03365406594902572, 0.03261623786226272, 0.03370788724001466, 0.03139408939347547, 0.033385714064443815, 0.03355184044838773, 0.030973966651308017, 0.031652665772304814, 0.03358439996683602, 0.03059295445863592, 0.031328876677733916, 0.029742687169493714, 0.03244421698265977, 0.03173108434948971, 0.031670505165784915, 0.029515959400869884, 0.03191728181758929, 0.028712898272030964, 0.029807070388506864, 0.03019326081124408, 0.029729038174010326, 0.03194998269028987, 0.026949734217105108, 0.030685039902988823, 0.029996783890706705, 0.03089736231453712, 0.0315516602092519, 0.026369427895933564, 0.029473515669701467, 0.03461556022249507, 0.02822856400981163, 0.029769000239635735, 0.028543054841877958, 0.02802888534999539, 0.030111901556365606, 0.027382430158272644, 0.02536641874372429, 0.028522934062601764, 0.02669831255842698, 0.026196087620122707, 0.026074867506994965], 'val_acc': [0.7749655431650169, 0.8116777346197218, 0.8344818944994361, 0.8346071920811928, 0.868562836737251, 0.877333667460218, 0.8804661070041349, 0.8863550933466984, 0.891492294198722, 0.898383661195339, 0.9003884225034456, 0.909033955644656, 0.9070291943365493, 0.9020172910662824, 0.9092845508081694, 0.9146723468237064, 0.9238190702919433, 0.9240696654554567, 0.9179300839493798, 0.9126675855155996, 0.9295827590527502, 0.9261997243453202, 0.9174288936223531, 0.9312116276155871, 0.9299586517980203, 0.9205613331662699, 0.9170530008770831, 0.9279538904899135, 0.939606565593284, 0.932339305851397, 0.9327151985966671, 0.9251973436912667, 0.9376018042851773, 0.9284550808169403, 0.9329657937601804, 0.9302092469615336, 0.9414860293196341, 0.9357223405588272, 0.9397318631750408, 0.9360982333040972, 0.9402330535020674, 0.937727101866934, 0.9307104372885603, 0.9365994236311239, 0.9389800776845006, 0.9293321638892369, 0.9373512091216639, 0.9353464478135572, 0.9354717453953139, 0.9371006139581506, 0.9345946623230171, 0.9404836486655808, 0.9393559704297707, 0.9427390051372009, 0.9437413857912542, 0.9416113269013908, 0.9417366244831474, 0.9393559704297707, 0.9379776970304473, 0.946497932589901, 0.939606565593284, 0.9488785866432777, 0.9309610324520736, 0.9416113269013908, 0.9476256108257111, 0.9397318631750408, 0.928956271143967, 0.9428643027189575, 0.9462473374263877, 0.9431148978824708, 0.9517604310236812, 0.9426137075554442, 0.9451196591905776, 0.9462473374263877, 0.9393559704297707, 0.944994361608821, 0.9500062648790878, 0.9506327527878712], 'size_bytes': 39003, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 78, 'batch_size': 64, 'lr': 0.000649459897808419, 'weight_decay': 6.768981604577105e-05, 'patch_size': 50, 'n_heads': 1, 'head_dim': 27, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.06153051428280533, 'attn_dropout': 0.2697241029675703, 'augment_noise_std': 0.02643085976386707, 'augment_scale_low': 0.8348774996506171, 'augment_scale_high': 1.1668529493335449, 'use_focal_loss': True, 'focal_gamma': 4.641260799262154, 'class_weighted': False, 'grad_clip': 1.188688832612316, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 6426, 'model_storage_size_kb': 6.9029296875, 'model_size_validation': 'PASS'}
2025-10-02 20:44:17,554 - INFO - _models.training_function_executor - BO Objective: base=0.9506, size_penalty=0.0000, final=0.9506
2025-10-02 20:44:17,554 - INFO - _models.training_function_executor - Model: 6,426 parameters, 6.9KB (PASS 256KB limit)
2025-10-02 20:44:17,554 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 98.392s
2025-10-02 20:44:17,658 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9506
2025-10-02 20:44:17,658 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.104s
2025-10-02 20:44:17,658 - INFO - bo.run_bo - Recorded observation #23: hparams={'epochs': np.int64(78), 'batch_size': np.int64(64), 'lr': 0.000649459897808419, 'weight_decay': 6.768981604577105e-05, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(27), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(3), 'dropout': 0.06153051428280533, 'attn_dropout': 0.2697241029675703, 'augment_noise_std': 0.02643085976386707, 'augment_scale_low': 0.8348774996506171, 'augment_scale_high': 1.1668529493335449, 'use_focal_loss': np.True_, 'focal_gamma': 4.641260799262154, 'class_weighted': np.False_, 'grad_clip': 1.188688832612316, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9506
2025-10-02 20:44:17,658 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 23: {'epochs': np.int64(78), 'batch_size': np.int64(64), 'lr': 0.000649459897808419, 'weight_decay': 6.768981604577105e-05, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(27), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(3), 'dropout': 0.06153051428280533, 'attn_dropout': 0.2697241029675703, 'augment_noise_std': 0.02643085976386707, 'augment_scale_low': 0.8348774996506171, 'augment_scale_high': 1.1668529493335449, 'use_focal_loss': np.True_, 'focal_gamma': 4.641260799262154, 'class_weighted': np.False_, 'grad_clip': 1.188688832612316, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9506
2025-10-02 20:44:17,659 - INFO - bo.run_bo - üîçBO Trial 24: Using RF surrogate + Expected Improvement
2025-10-02 20:44:17,659 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:44:17,659 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:44:17,659 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:44:17,659 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 97, 'batch_size': 128, 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': 50, 'n_heads': 1, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': False, 'focal_gamma': 4.864309655312979, 'class_weighted': False, 'grad_clip': 0.8737514784734614, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:44:17,660 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 97, 'batch_size': 128, 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': 50, 'n_heads': 1, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': False, 'focal_gamma': 4.864309655312979, 'class_weighted': False, 'grad_clip': 0.8737514784734614, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:46:30,087 - INFO - _models.training_function_executor - Model: 19,020 parameters, 20.4KB storage
2025-10-02 20:46:30,088 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.680262739218346, 0.43603165270100447, 0.3491636782009844, 0.3064436631042236, 0.27459475134890565, 0.25639428185407626, 0.23741919024217376, 0.22702565552749696, 0.21553734297688598, 0.2010119138976067, 0.19651236352782214, 0.19037544796712955, 0.18162329361966256, 0.17493472485652384, 0.16807239212001682, 0.159674902711264, 0.15840097664115654, 0.15393026600187573, 0.15293266163285923, 0.14566416973440965, 0.14425138994611206, 0.13792275194012985, 0.13389101452432067, 0.1330250841610504, 0.1307360071862651, 0.13042135964642043, 0.1275271570977907, 0.12377136453220175, 0.1228187386912406, 0.11877059502203811, 0.1169766088777219, 0.11815082360107686, 0.11503112282569787, 0.11345288317756867, 0.10908105200121503, 0.10934105664010865, 0.11143548282454573, 0.10331778490137357, 0.10156740525706655, 0.10408511963095786, 0.10085183507039187, 0.09772980667381598, 0.09981150644481943, 0.09981987918267755, 0.09477490527583995, 0.09620894017279441, 0.0914607601494033, 0.09603154653483112, 0.09211567191528623, 0.09172151266853563, 0.09139980485417348, 0.09004635577511154, 0.08799975724696003, 0.08712427031432138, 0.08854404287160991, 0.0856287333934251, 0.08349346870014596, 0.0840131156458105, 0.08118331474306455, 0.08204135921186057, 0.08190838217896798, 0.08356860909029024, 0.08136481785619819, 0.08065838960639723, 0.0810765740789126, 0.07982485417580147, 0.07662343395512088, 0.0787091270905789, 0.07705112860303186, 0.07593635498288953, 0.07438364725522491, 0.07374941840377162, 0.0772123438894097, 0.07279642805465518, 0.07594439331213977, 0.07194072890043454, 0.06984895385768942, 0.07204969201564997, 0.07111704093159782, 0.07073748967037409, 0.07020204824746762, 0.0730805153265412, 0.07348931633403136, 0.06797594269054637, 0.06838477105315371, 0.06850930339254256, 0.07086347216737975, 0.06795296419384007, 0.0687233715841263, 0.06584419663063686, 0.06547239544331333, 0.06725204583221611, 0.06448067079655259, 0.06712856991440305, 0.06629388758699888, 0.06443796125362718, 0.06052427341064534], 'val_losses': [0.4878883590944398, 0.3692434411638289, 0.3283987685771025, 0.28579369361740264, 0.24946334907478623, 0.21911863012916385, 0.2199876527916353, 0.22751009069843378, 0.18209158295158484, 0.17057714247771513, 0.18708181331001147, 0.17085050735022844, 0.1613742921166724, 0.15939422827267682, 0.1491025321390034, 0.14070429800802, 0.14926024627553358, 0.13185557934818837, 0.13124841761486733, 0.13460573352301067, 0.1358610705566233, 0.12438519703439715, 0.1351831500763777, 0.12834859430106596, 0.1274672938407386, 0.12958423493276217, 0.11479403402027578, 0.11791023762042235, 0.11215522463922736, 0.11308921421082781, 0.10758470026842709, 0.11729719811230581, 0.10274717796772677, 0.1288674041244219, 0.11003619703362572, 0.11103650617701803, 0.1018912242500275, 0.10295573486531681, 0.11554717029239611, 0.10462310706713253, 0.109502773597984, 0.0992471073726009, 0.10093939519159299, 0.09286726495609772, 0.10922984577423199, 0.10240954862432124, 0.10189739978338838, 0.10432335726338929, 0.1030796767504065, 0.09828195097586548, 0.092279423782064, 0.09714590236516603, 0.10490229503001773, 0.09837522649246713, 0.1034651312959805, 0.09601621526574569, 0.11321361019764907, 0.09752719499550731, 0.09908423640460333, 0.09881722367249729, 0.09741732877136666, 0.10864258392041465, 0.08844310953669822, 0.09073171614779071, 0.0931520729850011, 0.09157031002296809, 0.0880579765040748, 0.08619692601807777, 0.10095596210133714, 0.09098562502447279, 0.09087004789716394, 0.09682690629929532, 0.0876977030809116, 0.08956191492826027, 0.08658354311468248, 0.09205798260693024, 0.08813373828412985, 0.0963885014837748, 0.09972019997784673, 0.09240386966378329, 0.1037686478653254, 0.11260448073201156, 0.09365059645040608, 0.09059578527683244, 0.08929288405035839, 0.08831321907109178, 0.0938565936240617, 0.09264184199035534, 0.09444236743048369, 0.0905881005299299, 0.09173729211790282, 0.09431210771723957, 0.0974210300407543, 0.09033508452425444, 0.10117794762442187, 0.09181091012478682, 0.09116247014217606], 'val_acc': [0.8382408219521363, 0.8847262247838616, 0.898383661195339, 0.9142964540784363, 0.9239443678737, 0.9329657937601804, 0.9329657937601804, 0.9269515098358602, 0.9429896003007142, 0.9493797769703045, 0.9418619220649042, 0.9485026938980078, 0.9550181681493547, 0.9515098358601679, 0.955268763312868, 0.9563964415486781, 0.9557699536398947, 0.9607818569101616, 0.9573988222027315, 0.9595288810925949, 0.9612830472371883, 0.960656559328405, 0.9585265004385415, 0.9615336424007017, 0.9596541786743515, 0.9604059641648917, 0.9649166771081318, 0.9635384037088084, 0.9647913795263752, 0.9679238190702919, 0.9670467359979953, 0.9627866182182684, 0.9688009021425886, 0.9634131061270518, 0.9670467359979953, 0.9672973311615086, 0.9696779852148854, 0.969427390051372, 0.9651672722716451, 0.9672973311615086, 0.967547926325022, 0.9716827465229921, 0.9689261997243453, 0.9738128054128555, 0.9662949505074552, 0.9679238190702919, 0.9672973311615086, 0.9681744142338052, 0.9689261997243453, 0.970930961032452, 0.9726851271770455, 0.9708056634506954, 0.9695526876331286, 0.9715574489412354, 0.968675604560832, 0.9684250093973187, 0.9646660819446184, 0.9714321513594788, 0.9698032827966421, 0.9705550682871821, 0.970930961032452, 0.9666708432527252, 0.9728104247588022, 0.9719333416865055, 0.9711815561959655, 0.9695526876331286, 0.9728104247588022, 0.9745645909033955, 0.968675604560832, 0.9705550682871821, 0.9726851271770455, 0.970179175541912, 0.9730610199223155, 0.9730610199223155, 0.9749404836486656, 0.9753163763939356, 0.9746898884851523, 0.9720586392682621, 0.9704297707054254, 0.9716827465229921, 0.9700538779601554, 0.9664202480892119, 0.9750657812304222, 0.9748151860669089, 0.9746898884851523, 0.9750657812304222, 0.9735622102493422, 0.9741886981581256, 0.9705550682871821, 0.9746898884851523, 0.9744392933216389, 0.9706803658689387, 0.9696779852148854, 0.9761934594662323, 0.9724345320135321, 0.9740634005763689, 0.9765693522115023], 'size_bytes': 123275, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 97, 'batch_size': 128, 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': 50, 'n_heads': 1, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': False, 'focal_gamma': 4.864309655312979, 'class_weighted': False, 'grad_clip': 0.8737514784734614, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 19020, 'model_storage_size_kb': 20.431640625, 'model_size_validation': 'PASS'}
2025-10-02 20:46:30,088 - INFO - _models.training_function_executor - BO Objective: base=0.9766, size_penalty=0.0000, final=0.9766
2025-10-02 20:46:30,088 - INFO - _models.training_function_executor - Model: 19,020 parameters, 20.4KB (PASS 256KB limit)
2025-10-02 20:46:30,088 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 132.429s
2025-10-02 20:46:30,193 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9766
2025-10-02 20:46:30,193 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-10-02 20:46:30,193 - INFO - bo.run_bo - Recorded observation #24: hparams={'epochs': np.int64(97), 'batch_size': np.int64(128), 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(30), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': np.False_, 'focal_gamma': 4.864309655312979, 'class_weighted': np.False_, 'grad_clip': 0.8737514784734614, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9766
2025-10-02 20:46:30,193 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 24: {'epochs': np.int64(97), 'batch_size': np.int64(128), 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(30), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': np.False_, 'focal_gamma': 4.864309655312979, 'class_weighted': np.False_, 'grad_clip': 0.8737514784734614, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9766
2025-10-02 20:46:30,194 - INFO - bo.run_bo - üîçBO Trial 25: Using RF surrogate + Expected Improvement
2025-10-02 20:46:30,194 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 20:46:30,194 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:46:30,194 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:46:30,194 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 92, 'batch_size': 512, 'lr': 0.0004023161801174178, 'weight_decay': 0.0032391289637576743, 'patch_size': 125, 'n_heads': 1, 'head_dim': 32, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.032198843528460704, 'attn_dropout': 0.15577258381020787, 'augment_noise_std': 0.013985797556621318, 'augment_scale_low': 0.8274821885379214, 'augment_scale_high': 1.1431263608358209, 'use_focal_loss': False, 'focal_gamma': 4.760300052378337, 'class_weighted': False, 'grad_clip': 1.697112565168659, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 20:46:30,196 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 92, 'batch_size': 512, 'lr': 0.0004023161801174178, 'weight_decay': 0.0032391289637576743, 'patch_size': 125, 'n_heads': 1, 'head_dim': 32, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.032198843528460704, 'attn_dropout': 0.15577258381020787, 'augment_noise_std': 0.013985797556621318, 'augment_scale_low': 0.8274821885379214, 'augment_scale_high': 1.1431263608358209, 'use_focal_loss': False, 'focal_gamma': 4.760300052378337, 'class_weighted': False, 'grad_clip': 1.697112565168659, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 20:47:34,868 - INFO - _models.training_function_executor - Model: 40,357 parameters, 173.4KB storage
2025-10-02 20:47:34,869 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9079037672355391, 0.6746805853101017, 0.5795179306275152, 0.5092350634035078, 0.4366586145168893, 0.38598212327618975, 0.35010845835161913, 0.31888828467139735, 0.29405588284379924, 0.2752014604366106, 0.26085913397052723, 0.2495195760660608, 0.23885239018024337, 0.2298037953422963, 0.21995618118885948, 0.21247022703191495, 0.20436425853749474, 0.19918719866881263, 0.19447396170854742, 0.19108837527815076, 0.1839822495205489, 0.17904419237674563, 0.17563248005779458, 0.16952110594697187, 0.16546835104101631, 0.1600713228256456, 0.15843296864490716, 0.15403005349086393, 0.14983466654433397, 0.14908983974204648, 0.14520961250320388, 0.14447897722132355, 0.14206252547587622, 0.13761241870562324, 0.1356620199506199, 0.13146154377955496, 0.13220872976595613, 0.12966945534920635, 0.12583639491231013, 0.12479721075781297, 0.12241802566775485, 0.12175187748996207, 0.11890007466562327, 0.11626284111539616, 0.1130551080057911, 0.11257068561717083, 0.11223447152935906, 0.10900573306973856, 0.10978291758251785, 0.10915685240245233, 0.10856378466405281, 0.10176667989119997, 0.10095216224988637, 0.09924278509137745, 0.09872246107247988, 0.09913404493860234, 0.09795235954611327, 0.09497985272014424, 0.09350639576545669, 0.09552015872772618, 0.09470841601148822, 0.09185000907680958, 0.09223976595979531, 0.08991965266975085, 0.08864850894163281, 0.08498760575712001, 0.08411221648839931, 0.08385616371022599, 0.0830635308481374, 0.08331810580949581, 0.08000956651222339, 0.08004319643718413, 0.07740503696612919, 0.07943552006979532, 0.07777457516772875, 0.07593692214331285, 0.075841680491887, 0.07451445607358967, 0.07415536337733433, 0.07282907005498775, 0.07076176992947418, 0.06853746897620194, 0.07110183874237165, 0.06734406494375078, 0.0699905380767692, 0.06992836946462422, 0.06442824083403478, 0.06468226058196873, 0.06434333052145312, 0.0641169890437427, 0.06324480330528996, 0.060517406195828786], 'val_losses': [0.7464510757826636, 0.61817224562549, 0.5430276047136383, 0.46491891015757447, 0.4174947177141983, 0.36684388242677823, 0.32951840940550625, 0.30615923383334215, 0.2786992068032904, 0.2690991208997231, 0.2581342896629793, 0.2465748016569283, 0.23926114273868906, 0.2290787605650218, 0.22594688020580292, 0.21405605382376217, 0.2199739229371413, 0.20774736671112576, 0.20289824295650372, 0.1968981347284495, 0.19024793699322123, 0.20331843648432132, 0.18704288520171908, 0.18365852911360894, 0.1776446930867152, 0.17632677926894433, 0.16947084988489916, 0.16957033386827636, 0.17318191000384925, 0.16800832860251147, 0.16428821887449876, 0.16782312799427626, 0.16139328772338196, 0.15859814831755273, 0.15771076957163185, 0.1534938718192396, 0.1520522203515338, 0.14762965860955027, 0.15201691465932854, 0.14962719830261587, 0.1452107454747309, 0.15042147858894764, 0.14821219889760062, 0.14230849144004995, 0.14593512908906928, 0.1420073280256354, 0.1434711701883258, 0.14577211729940384, 0.14604861939473193, 0.14389448281428432, 0.14355040937680796, 0.14001828929286392, 0.13558873749663547, 0.1436254107859511, 0.13624199006793764, 0.13562275731067672, 0.13601072840957382, 0.13723758863799332, 0.13670674400503752, 0.13153384957131609, 0.13734294039282102, 0.13094844867667524, 0.13357353927221025, 0.13039437745570717, 0.13098269688494596, 0.1369001699030183, 0.1317553562473316, 0.12989061803667487, 0.134319122457337, 0.13044110579816878, 0.13159347871777527, 0.13247390513005763, 0.13297363013951435, 0.1294052795391605, 0.13432481191523046, 0.12906938921597172, 0.13163075524304926, 0.12804836967007557, 0.13590151679469534, 0.12800366303859803, 0.1340508573546085, 0.1372625267105344, 0.132616929783661, 0.12839968482543765, 0.1401217159524685, 0.1241783505748379, 0.12908057443720106, 0.13706104234262392, 0.1261631940752772, 0.13181970373697857, 0.1298207047458072, 0.13307140448677138], 'val_acc': [0.7639393559704297, 0.8027816063149982, 0.8290940984838993, 0.8549054003257737, 0.8715699786994111, 0.8867309860919684, 0.9007643152487157, 0.9047738378649292, 0.9194336549304598, 0.9205613331662699, 0.9263250219270768, 0.9269515098358602, 0.9320887106878837, 0.9332163889236937, 0.9333416865054505, 0.9386041849392307, 0.9354717453953139, 0.939606565593284, 0.9416113269013908, 0.9419872196466608, 0.9439919809547676, 0.9371006139581506, 0.9441172785365243, 0.9452449567723343, 0.9468738253351711, 0.9482520987344945, 0.9488785866432777, 0.9487532890615211, 0.9471244204986844, 0.9517604310236812, 0.9493797769703045, 0.9488785866432777, 0.9501315624608445, 0.9513845382784112, 0.951885728605438, 0.9530134068412479, 0.954516977822328, 0.9567723342939481, 0.9546422754040848, 0.9560205488034081, 0.9557699536398947, 0.9548928705675981, 0.955268763312868, 0.9587770956020549, 0.9575241197844881, 0.9582759052750282, 0.9586517980202982, 0.9581506076932715, 0.9565217391304348, 0.9582759052750282, 0.9595288810925949, 0.9589023931838115, 0.9609071544919183, 0.9599047738378649, 0.9590276907655683, 0.9612830472371883, 0.9607818569101616, 0.9604059641648917, 0.9612830472371883, 0.9612830472371883, 0.9616589399824583, 0.9616589399824583, 0.9615336424007017, 0.9627866182182684, 0.9634131061270518, 0.9615336424007017, 0.9626613206365117, 0.9630372133817817, 0.963287808545295, 0.9635384037088084, 0.9637889988723217, 0.9630372133817817, 0.9627866182182684, 0.9647913795263752, 0.9630372133817817, 0.9650419746898885, 0.9624107254729983, 0.9646660819446184, 0.9642901891993485, 0.9661696529256986, 0.961784237564215, 0.9635384037088084, 0.962536023054755, 0.9647913795263752, 0.9600300714196216, 0.9654178674351584, 0.9634131061270518, 0.9612830472371883, 0.9644154867811051, 0.9647913795263752, 0.9646660819446184, 0.9652925698534018], 'size_bytes': 175781, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 92, 'batch_size': 512, 'lr': 0.0004023161801174178, 'weight_decay': 0.0032391289637576743, 'patch_size': 125, 'n_heads': 1, 'head_dim': 32, 'num_layers': 3, 'mlp_ratio': 3, 'dropout': 0.032198843528460704, 'attn_dropout': 0.15577258381020787, 'augment_noise_std': 0.013985797556621318, 'augment_scale_low': 0.8274821885379214, 'augment_scale_high': 1.1431263608358209, 'use_focal_loss': False, 'focal_gamma': 4.760300052378337, 'class_weighted': False, 'grad_clip': 1.697112565168659, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 40357, 'model_storage_size_kb': 173.40898437500002, 'model_size_validation': 'PASS'}
2025-10-02 20:47:34,869 - INFO - _models.training_function_executor - BO Objective: base=0.9653, size_penalty=0.0000, final=0.9653
2025-10-02 20:47:34,869 - INFO - _models.training_function_executor - Model: 40,357 parameters, 173.4KB (PASS 256KB limit)
2025-10-02 20:47:34,869 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 64.675s
2025-10-02 20:47:34,974 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9653
2025-10-02 20:47:34,974 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-10-02 20:47:34,974 - INFO - bo.run_bo - Recorded observation #25: hparams={'epochs': np.int64(92), 'batch_size': np.int64(512), 'lr': 0.0004023161801174178, 'weight_decay': 0.0032391289637576743, 'patch_size': np.int64(125), 'n_heads': np.int64(1), 'head_dim': np.int64(32), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.032198843528460704, 'attn_dropout': 0.15577258381020787, 'augment_noise_std': 0.013985797556621318, 'augment_scale_low': 0.8274821885379214, 'augment_scale_high': 1.1431263608358209, 'use_focal_loss': np.False_, 'focal_gamma': 4.760300052378337, 'class_weighted': np.False_, 'grad_clip': 1.697112565168659, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.9653
2025-10-02 20:47:34,974 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 25: {'epochs': np.int64(92), 'batch_size': np.int64(512), 'lr': 0.0004023161801174178, 'weight_decay': 0.0032391289637576743, 'patch_size': np.int64(125), 'n_heads': np.int64(1), 'head_dim': np.int64(32), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'dropout': 0.032198843528460704, 'attn_dropout': 0.15577258381020787, 'augment_noise_std': 0.013985797556621318, 'augment_scale_low': 0.8274821885379214, 'augment_scale_high': 1.1431263608358209, 'use_focal_loss': np.False_, 'focal_gamma': 4.760300052378337, 'class_weighted': np.False_, 'grad_clip': 1.697112565168659, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.9653
2025-10-02 20:47:34,975 - INFO - bo.run_bo - üîçBO Trial 26: Using RF surrogate + Expected Improvement
2025-10-02 20:47:34,975 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:47:34,975 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:47:34,975 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:47:34,975 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 78, 'batch_size': 256, 'lr': 0.004905598184852152, 'weight_decay': 0.0022193287586951462, 'patch_size': 200, 'n_heads': 3, 'head_dim': 32, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.09231157370680838, 'attn_dropout': 0.09936268410418007, 'augment_noise_std': 0.01624465634978635, 'augment_scale_low': 0.8195892909246384, 'augment_scale_high': 1.1492976242666, 'use_focal_loss': False, 'focal_gamma': 4.621915567398555, 'class_weighted': False, 'grad_clip': 3.545308022465073, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:47:34,976 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 78, 'batch_size': 256, 'lr': 0.004905598184852152, 'weight_decay': 0.0022193287586951462, 'patch_size': 200, 'n_heads': 3, 'head_dim': 32, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.09231157370680838, 'attn_dropout': 0.09936268410418007, 'augment_noise_std': 0.01624465634978635, 'augment_scale_low': 0.8195892909246384, 'augment_scale_high': 1.1492976242666, 'use_focal_loss': False, 'focal_gamma': 4.621915567398555, 'class_weighted': False, 'grad_clip': 3.545308022465073, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:48:38,423 - INFO - _models.training_function_executor - Model: 114,336 parameters, 491.3KB storage
2025-10-02 20:48:38,423 - WARNING - _models.training_function_executor - Model storage 491.3KB exceeds 256KB limit!
2025-10-02 20:48:38,423 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7916312722269091, 0.6844568328331935, 0.6489690809035082, 0.6498784185878631, 0.6294322249525133, 0.6299751275102473, 0.6190530045869801, 0.6060927043514391, 0.593563212876899, 0.5660259962292896, 0.550537631103756, 0.5532046407269785, 0.529172242917285, 0.5362351910514468, 0.5188118582720656, 0.5234335673574972, 0.5160941201874335, 0.5337625456230498, 0.5197080443044192, 0.5180581483768348, 0.49961959804108547, 0.47865361609625623, 0.4771664064786333, 0.4824526486131507, 0.4509346411815432, 0.4264294965919634, 0.4429543823040178, 0.4392630760531839, 0.4221364284926211, 0.4466691603376181, 0.4463381763613359, 0.42651126066238715, 0.41252003730891273, 0.40860829697109957, 0.4102582523296849, 0.4010498936050903, 0.3970887989960596, 0.41386509005879935, 0.44854806191945956, 0.4023656797695477, 0.40950796273523704, 0.38600887805414846, 0.37531632182088237, 0.40162519969311483, 0.39867126019408117, 0.39015554378841755, 0.3802068121583077, 0.3726146106985403, 0.3701951952286876, 0.38249244024176027, 0.3802573344475441, 0.3884344004290417, 0.4015119218051058, 0.40279074199806947, 0.39589189434508587, 0.3909517930376497, 0.3975208976973202, 0.3536922417059917, 0.3783088497063893, 0.3656512854825441, 0.34594895188302743, 0.34265695144235203, 0.3444609733191819, 0.3446179561509982, 0.34657872420737784, 0.3592108128397431, 0.3461309430811917, 0.3746920555491116, 0.4130563921163327, 0.381182124599181, 0.3510262103482259, 0.3425982204118184, 0.352487055085504, 0.35003973518577625, 0.34624312347111513, 0.3626175668114271, 0.39514645594752595, 0.3559255027464412], 'val_losses': [0.6884900225521762, 0.6358418580344888, 0.602931379976913, 0.6192410547114034, 0.6482394955881465, 0.6228504034305123, 0.6183237526339407, 0.6038447782376732, 0.5491119105443069, 0.5214345019243964, 0.5533232955522756, 0.5101705860477718, 0.5240180409502795, 0.518338590277838, 0.48869209869241614, 0.4914631991588954, 0.5241217586931736, 0.49331112203928423, 0.5054917061647038, 0.49071256240019745, 0.4685243774724028, 0.4608309188312254, 0.4358791105487475, 0.4222789049977543, 0.4101204422725321, 0.39550947946401904, 0.3999535732812383, 0.4090414374260179, 0.39806677812663493, 0.39031830615618524, 0.39625960876545563, 0.3979998194834162, 0.38973628842633495, 0.3817771771186845, 0.3626264771732482, 0.35959767560965733, 0.369263624705448, 0.46241477101223466, 0.3903064418240099, 0.3674488786228497, 0.3888582479194238, 0.3751803726917549, 0.3599479012681404, 0.3551865181720073, 0.36585264348726854, 0.35532736225977174, 0.3568954076545607, 0.3395390892638223, 0.35165068844762487, 0.36147452877243275, 0.3846654369708306, 0.3654087546598103, 0.3859559204879821, 0.3486544696583573, 0.3638349657962532, 0.36335897142465007, 0.347739935470186, 0.34097920061502074, 0.3808190007434425, 0.33379713457010995, 0.31891611001124404, 0.319439651034497, 0.3382946154838546, 0.33857468040556166, 0.32012080750045974, 0.33146437607885765, 0.3367817746474172, 0.40709995837337676, 0.36165202065019614, 0.3265324570663351, 0.3201587822182318, 0.3426722705069229, 0.3402102890879181, 0.30985285582406197, 0.32821812282871027, 0.34674145280007895, 0.35478868701713095, 0.33153408925614847], 'val_acc': [0.76769828342313, 0.792131311865681, 0.8017792256609447, 0.7900012529758176, 0.7871194085954141, 0.7983961909535146, 0.7874953013406841, 0.7972685127177046, 0.8156872572359354, 0.8295952888109259, 0.8190702919433654, 0.8328530259365994, 0.8305976694649794, 0.8277158250845759, 0.8409973687507831, 0.831474752537276, 0.8199473750156622, 0.8280917178298459, 0.8297205863926826, 0.8337301090088961, 0.8404961784237565, 0.84751284300213, 0.8498934970555069, 0.8634256358852274, 0.8648039092845508, 0.870066407718331, 0.8723217641899511, 0.8668086705926575, 0.8676857536649543, 0.8734494424257612, 0.8676857536649543, 0.8706928956271144, 0.8676857536649543, 0.8785866432777847, 0.8857286054379151, 0.884600927202105, 0.8783360481142714, 0.8501440922190202, 0.8844756296203483, 0.8830973562210249, 0.8790878336048115, 0.8804661070041349, 0.8876080691642652, 0.8909911038716952, 0.8857286054379151, 0.8913669966169653, 0.891868186943992, 0.898383661195339, 0.8917428893622353, 0.8864803909284551, 0.8803408094223781, 0.8847262247838616, 0.8764565843879213, 0.8919934845257487, 0.8867309860919684, 0.886856283673725, 0.8902393183811552, 0.8942488409973688, 0.8820949755669716, 0.8996366370129056, 0.9021425886480391, 0.9020172910662824, 0.8985089587770956, 0.8955018168149355, 0.9026437789750658, 0.8972559829595289, 0.8942488409973688, 0.8752036085703546, 0.8912416990352087, 0.900639017666959, 0.9035208620473625, 0.8962536023054755, 0.8957524119784488, 0.9046485402831725, 0.8968800902142589, 0.8897381280541286, 0.8913669966169653, 0.8980077684500689], 'size_bytes': 585579, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 78, 'batch_size': 256, 'lr': 0.004905598184852152, 'weight_decay': 0.0022193287586951462, 'patch_size': 200, 'n_heads': 3, 'head_dim': 32, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.09231157370680838, 'attn_dropout': 0.09936268410418007, 'augment_noise_std': 0.01624465634978635, 'augment_scale_low': 0.8195892909246384, 'augment_scale_high': 1.1492976242666, 'use_focal_loss': False, 'focal_gamma': 4.621915567398555, 'class_weighted': False, 'grad_clip': 3.545308022465073, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 114336, 'model_storage_size_kb': 491.2875, 'model_size_validation': 'FAIL'}
2025-10-02 20:48:38,423 - INFO - _models.training_function_executor - BO Objective: base=0.8980, size_penalty=0.4595, final=0.4385
2025-10-02 20:48:38,423 - INFO - _models.training_function_executor - Model: 114,336 parameters, 491.3KB (FAIL 256KB limit)
2025-10-02 20:48:38,423 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 63.448s
2025-10-02 20:48:38,529 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.4385
2025-10-02 20:48:38,529 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.106s
2025-10-02 20:48:38,529 - INFO - bo.run_bo - Recorded observation #26: hparams={'epochs': np.int64(78), 'batch_size': np.int64(256), 'lr': 0.004905598184852152, 'weight_decay': 0.0022193287586951462, 'patch_size': np.int64(200), 'n_heads': np.int64(3), 'head_dim': np.int64(32), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.09231157370680838, 'attn_dropout': 0.09936268410418007, 'augment_noise_std': 0.01624465634978635, 'augment_scale_low': 0.8195892909246384, 'augment_scale_high': 1.1492976242666, 'use_focal_loss': np.False_, 'focal_gamma': 4.621915567398555, 'class_weighted': np.False_, 'grad_clip': 3.545308022465073, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.4385
2025-10-02 20:48:38,529 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 26: {'epochs': np.int64(78), 'batch_size': np.int64(256), 'lr': 0.004905598184852152, 'weight_decay': 0.0022193287586951462, 'patch_size': np.int64(200), 'n_heads': np.int64(3), 'head_dim': np.int64(32), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.09231157370680838, 'attn_dropout': 0.09936268410418007, 'augment_noise_std': 0.01624465634978635, 'augment_scale_low': 0.8195892909246384, 'augment_scale_high': 1.1492976242666, 'use_focal_loss': np.False_, 'focal_gamma': 4.621915567398555, 'class_weighted': np.False_, 'grad_clip': 3.545308022465073, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.4385
2025-10-02 20:48:38,530 - INFO - bo.run_bo - üîçBO Trial 27: Using RF surrogate + Expected Improvement
2025-10-02 20:48:38,530 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:48:38,530 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:48:38,530 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:48:38,530 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 90, 'batch_size': 64, 'lr': 0.006949006191536314, 'weight_decay': 0.0005838117627567651, 'patch_size': 5, 'n_heads': 1, 'head_dim': 28, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.2575649185465322, 'attn_dropout': 0.11646001331394003, 'augment_noise_std': 0.04156075261260389, 'augment_scale_low': 0.8588055775190687, 'augment_scale_high': 1.057966939585782, 'use_focal_loss': True, 'focal_gamma': 3.7330142882210957, 'class_weighted': False, 'grad_clip': 0.5187633632241296, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:48:38,531 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 90, 'batch_size': 64, 'lr': 0.006949006191536314, 'weight_decay': 0.0005838117627567651, 'patch_size': 5, 'n_heads': 1, 'head_dim': 28, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.2575649185465322, 'attn_dropout': 0.11646001331394003, 'augment_noise_std': 0.04156075261260389, 'augment_scale_low': 0.8588055775190687, 'augment_scale_high': 1.057966939585782, 'use_focal_loss': True, 'focal_gamma': 3.7330142882210957, 'class_weighted': False, 'grad_clip': 0.5187633632241296, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 20:52:29,803 - INFO - _models.training_function_executor - Model: 22,433 parameters, 48.2KB storage
2025-10-02 20:52:29,804 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.16504444086648384, 0.11215412342517189, 0.0957470111187376, 0.09101723172864015, 0.0759390707406391, 0.07026135649938066, 0.06652085462909098, 0.06247358140513043, 0.061621172558485336, 0.057591831599764595, 0.058634939473712254, 0.05535815995350759, 0.055734644270972404, 0.05167180083707711, 0.05086534524110825, 0.05242193899683474, 0.04835049639568673, 0.04980843909164967, 0.05174287036907334, 0.05038651944941358, 0.049085703045848625, 0.04671226585583143, 0.0448635282572643, 0.04790824246969311, 0.0454667964817462, 0.047063720352541374, 0.0443612288717378, 0.04483333936076832, 0.041668897228839964, 0.05154307876347644, 0.04786617015165927, 0.0462752411146251, 0.044499535489567976, 0.04752206421524535, 0.044627982592213386, 0.04193655356954541, 0.040565667888695464, 0.0432482015901545, 0.04601207792887191, 0.045830061643929686, 0.04103189800182605, 0.04242943890591864, 0.047774359276477094, 0.04199476888963379, 0.04231317643909042, 0.03814534270375126, 0.04033252420363949, 0.03799209489830752, 0.03782547405594961, 0.040238833272946875, 0.03718891119739589, 0.039218377635404325, 0.03794767989442432, 0.037736610038148745, 0.03824261221904576, 0.04105643396733552, 0.039301971088906644, 0.04349992757510434, 0.040776440894572365, 0.03541286567918819, 0.037189452984172984, 0.041192488511944056, 0.03969433338511495, 0.038913926794036, 0.03697980084883524, 0.036876193647915674, 0.037548598704637136, 0.037435175793456246, 0.04099360360310807, 0.04444197102967119, 0.03749995649535902, 0.03453183031157212, 0.03712340750945461, 0.04353725764725052, 0.043087398516548046, 0.03953962893048254, 0.03585739791404016, 0.03397317756534172, 0.035466203181844375, 0.03995024815617898, 0.03838986578406993, 0.03507554464866528, 0.034448384528084776, 0.034178668238262044, 0.03303703587669467, 0.0340746347296634, 0.033071553627267644, 0.03212149996998746, 0.03063835965684959, 0.03037695867376383], 'val_losses': [0.1311937762218271, 0.0960951151190702, 0.13330884467889276, 0.09974728464098177, 0.08808940224832139, 0.0683544568349665, 0.08750382933871606, 0.08005476410456012, 0.07364251115088057, 0.07690035576338518, 0.08579491181916393, 0.09883262147402766, 0.07701385583013927, 0.10236314325113223, 0.11087526054836995, 0.09710823892501494, 0.07102776632411201, 0.09825503017509242, 0.07805225149668961, 0.0919284237225004, 0.0945606260610514, 0.08653201866330636, 0.0751230866958542, 0.062373665568804235, 0.07145068079677534, 0.07397292494351905, 0.07296725660245111, 0.06774417472642222, 0.08815472115700801, 0.1328402939283479, 0.07763393961902311, 0.06752905092760968, 0.08505528380814507, 0.09320646306167732, 0.08484190608430904, 0.09445573761230695, 0.07295208357208251, 0.16988962948464195, 0.12875577925425874, 0.0695031067761857, 0.07855049580570869, 0.07835303763735864, 0.11954766577638311, 0.08226522572162143, 0.10047167986201966, 0.06432022897242126, 0.0785370495297783, 0.07723846570984819, 0.09941426693810629, 0.09366270492375905, 0.08283382691531653, 0.07983431534657516, 0.06552522207259415, 0.12660571552095162, 0.09551528912579531, 0.08162046033903525, 0.08311464151517145, 0.0914770292356011, 0.07602230924261108, 0.07526506424648483, 0.12630047208981143, 0.12349139044919927, 0.08603356895034078, 0.08047541334936642, 0.07355333766107078, 0.12821287702679887, 0.07568529995069395, 0.10112128974822003, 0.10519134430169252, 0.13167431924663425, 0.10325324716849997, 0.10301150185148335, 0.09922740310094452, 0.10156811524148483, 0.14008364041240542, 0.1509932951698057, 0.08830761987342077, 0.082859001180878, 0.10122507828936707, 0.10540418788676811, 0.08996293874607544, 0.07742157426114639, 0.12783205459148062, 0.12175415131609894, 0.07763945506884481, 0.11467867531419144, 0.11179322638751839, 0.08582862230668226, 0.07249148066607562, 0.09235838016028572], 'val_acc': [0.8164390427264754, 0.8665580754291442, 0.8248339807041725, 0.8275905275028193, 0.852900639017667, 0.9096604435534394, 0.8878586643277785, 0.8818443804034583, 0.8893622353088585, 0.9042726475379025, 0.8739506327527878, 0.8625485528129307, 0.8695652173913043, 0.8691893246460344, 0.8622979576494174, 0.8653050996115775, 0.8907405087081819, 0.9055256233554693, 0.8919934845257487, 0.8534018293446937, 0.8523994486906402, 0.8755795013156246, 0.9142964540784363, 0.891492294198722, 0.8715699786994111, 0.8641774213757675, 0.8975065781230422, 0.8947500313243955, 0.8522741511088836, 0.8453827841122666, 0.8546548051622603, 0.8595414108507706, 0.8744518230798145, 0.862423255231174, 0.8649292068663075, 0.8807167021676482, 0.8909911038716952, 0.821200350833229, 0.8309735622102493, 0.8750783109885979, 0.878085452950758, 0.8889863425635885, 0.8337301090088961, 0.8770830722967047, 0.8541536148352337, 0.8894875328906152, 0.8534018293446937, 0.8361107630622729, 0.8443804034582133, 0.8902393183811552, 0.8824708683122415, 0.8960030071419621, 0.9085327653176294, 0.829971181556196, 0.8397443929332163, 0.8719458714446812, 0.876205989224408, 0.8892369377271019, 0.8651798020298208, 0.8690640270642777, 0.7963914296454079, 0.8341060017541662, 0.8645533141210374, 0.8675604560831975, 0.8814684876581882, 0.8188196967798521, 0.8347324896629496, 0.7968926199724345, 0.8442551058764566, 0.8339807041724094, 0.8254604686129557, 0.8607943866683373, 0.8274652299210625, 0.8317253477007894, 0.7997744643528379, 0.8220774339055257, 0.8596667084325272, 0.8825961658939983, 0.8232051121413356, 0.8408720711690264, 0.883473248966295, 0.886104498183185, 0.8046610700413482, 0.8004009522616213, 0.8502693898007768, 0.7907530384663576, 0.8562836737250971, 0.8537777220899636, 0.8901140207993986, 0.8268387420122791], 'size_bytes': 54217, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 90, 'batch_size': 64, 'lr': 0.006949006191536314, 'weight_decay': 0.0005838117627567651, 'patch_size': 5, 'n_heads': 1, 'head_dim': 28, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.2575649185465322, 'attn_dropout': 0.11646001331394003, 'augment_noise_std': 0.04156075261260389, 'augment_scale_low': 0.8588055775190687, 'augment_scale_high': 1.057966939585782, 'use_focal_loss': True, 'focal_gamma': 3.7330142882210957, 'class_weighted': False, 'grad_clip': 0.5187633632241296, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 22433, 'model_storage_size_kb': 48.195898437500006, 'model_size_validation': 'PASS'}
2025-10-02 20:52:29,804 - INFO - _models.training_function_executor - BO Objective: base=0.8268, size_penalty=0.0000, final=0.8268
2025-10-02 20:52:29,804 - INFO - _models.training_function_executor - Model: 22,433 parameters, 48.2KB (PASS 256KB limit)
2025-10-02 20:52:29,804 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 231.274s
2025-10-02 20:52:29,909 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8268
2025-10-02 20:52:29,909 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-10-02 20:52:29,909 - INFO - bo.run_bo - Recorded observation #27: hparams={'epochs': np.int64(90), 'batch_size': np.int64(64), 'lr': 0.006949006191536314, 'weight_decay': 0.0005838117627567651, 'patch_size': np.int64(5), 'n_heads': np.int64(1), 'head_dim': np.int64(28), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.2575649185465322, 'attn_dropout': 0.11646001331394003, 'augment_noise_std': 0.04156075261260389, 'augment_scale_low': 0.8588055775190687, 'augment_scale_high': 1.057966939585782, 'use_focal_loss': np.True_, 'focal_gamma': 3.7330142882210957, 'class_weighted': np.False_, 'grad_clip': 0.5187633632241296, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.8268
2025-10-02 20:52:29,909 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 27: {'epochs': np.int64(90), 'batch_size': np.int64(64), 'lr': 0.006949006191536314, 'weight_decay': 0.0005838117627567651, 'patch_size': np.int64(5), 'n_heads': np.int64(1), 'head_dim': np.int64(28), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.2575649185465322, 'attn_dropout': 0.11646001331394003, 'augment_noise_std': 0.04156075261260389, 'augment_scale_low': 0.8588055775190687, 'augment_scale_high': 1.057966939585782, 'use_focal_loss': np.True_, 'focal_gamma': 3.7330142882210957, 'class_weighted': np.False_, 'grad_clip': 0.5187633632241296, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.8268
2025-10-02 20:52:29,910 - INFO - bo.run_bo - üîçBO Trial 28: Using RF surrogate + Expected Improvement
2025-10-02 20:52:29,910 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:52:29,910 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:52:29,910 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:52:29,910 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 88, 'batch_size': 64, 'lr': 0.00011014598449447628, 'weight_decay': 0.0002527900807709807, 'patch_size': 20, 'n_heads': 1, 'head_dim': 30, 'num_layers': 1, 'mlp_ratio': 2, 'dropout': 0.3506840996131593, 'attn_dropout': 0.08701244589255584, 'augment_noise_std': 0.03746001331179031, 'augment_scale_low': 0.8218154070516586, 'augment_scale_high': 1.121735357561923, 'use_focal_loss': False, 'focal_gamma': 4.761739793216449, 'class_weighted': False, 'grad_clip': 1.650117589624561, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 20:52:29,911 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 88, 'batch_size': 64, 'lr': 0.00011014598449447628, 'weight_decay': 0.0002527900807709807, 'patch_size': 20, 'n_heads': 1, 'head_dim': 30, 'num_layers': 1, 'mlp_ratio': 2, 'dropout': 0.3506840996131593, 'attn_dropout': 0.08701244589255584, 'augment_noise_std': 0.03746001331179031, 'augment_scale_low': 0.8218154070516586, 'augment_scale_high': 1.121735357561923, 'use_focal_loss': False, 'focal_gamma': 4.761739793216449, 'class_weighted': False, 'grad_clip': 1.650117589624561, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 20:54:16,547 - INFO - _models.training_function_executor - Model: 10,445 parameters, 22.4KB storage
2025-10-02 20:54:16,548 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9429978749897159, 0.7262438014478749, 0.6777748221521767, 0.6455697687527434, 0.6114399640873754, 0.5739916880018068, 0.5396810257580108, 0.5095531928687901, 0.48937793823313647, 0.4759018626632956, 0.46453108417525896, 0.45406987323586373, 0.4462710219797356, 0.43956566791902923, 0.43098272523901887, 0.42585179508171167, 0.4210847990416802, 0.41381051040411226, 0.4100698808466765, 0.40544000253792234, 0.4003630280681404, 0.39602482833368036, 0.3927458827581762, 0.3879899618177378, 0.3844171506948602, 0.3811924501231741, 0.37693283242009445, 0.3743648252529604, 0.3723280487095873, 0.3680202134582097, 0.3656507476992025, 0.36057181983809944, 0.3575686449597243, 0.35381023464759137, 0.35183431162435497, 0.35028162981093136, 0.34532808726680003, 0.34404717755742276, 0.34079450580054527, 0.33826299296212453, 0.33536166323682803, 0.33283018150212484, 0.32833860687559324, 0.3269677232710882, 0.3253481065450603, 0.3233752707377997, 0.32066099973322476, 0.3178917338528236, 0.3169427329042862, 0.31594405213409255, 0.31061092272171664, 0.31016011909595637, 0.3101960265075929, 0.3079842234716131, 0.3051976079969438, 0.30247713228710926, 0.30367656615568134, 0.3001016065507732, 0.2985678996173219, 0.29961918994699865, 0.2976055539057069, 0.29561064464889913, 0.29422246189925977, 0.2887276766811648, 0.29067920331725594, 0.2880682338440967, 0.28669104977396126, 0.28475556593348994, 0.2828941036875683, 0.2825662682720792, 0.2779349383555255, 0.2786202112228706, 0.2782162134173415, 0.27871289796254595, 0.27884022245865153, 0.2727967836033957, 0.27251748798694375, 0.27214994505330203, 0.2684607525860737, 0.269618207264435, 0.26767398870057024, 0.2672991984835791, 0.26817182003490175, 0.2644559094234281, 0.2651596645740094, 0.263167217752877, 0.2632898950350485, 0.2627045482082467], 'val_losses': [0.7510677228564234, 0.6932443504927377, 0.6554941661584021, 0.6268097112881124, 0.5903967150081266, 0.5474370753439488, 0.5156878872281282, 0.491832959168, 0.47898668701629893, 0.47452642915467247, 0.4618701804156232, 0.4491430226236847, 0.4422401034687115, 0.4371045452171871, 0.43060732351599923, 0.4265315763156615, 0.4204926990078081, 0.41228507445533985, 0.41242613294357316, 0.4087396657357851, 0.40842413012432643, 0.40071929200910533, 0.39953574438155437, 0.39812433168846567, 0.39790844809006753, 0.39154766844472344, 0.3890972096338082, 0.3864707442553864, 0.38896124382139136, 0.3810569523547679, 0.38188895857240035, 0.3717088053256643, 0.3756066246329058, 0.37061319150223854, 0.36693890080791325, 0.3625881565706038, 0.36303538151245013, 0.3586509507067773, 0.3483615422432561, 0.34927560913504524, 0.3481600393240441, 0.3438963629506074, 0.3473148963315761, 0.3365395184246434, 0.3383141322055118, 0.33582647332249244, 0.3384440582623205, 0.33256240107670804, 0.323898508556653, 0.334702805530426, 0.32667313806844256, 0.3214850046516376, 0.3268093013357151, 0.31970798475986406, 0.319427023292738, 0.31828744301908285, 0.3125282213075047, 0.3208956708922719, 0.31558856510336336, 0.3209940417583478, 0.31211676212491407, 0.31327134713412913, 0.30734504794226064, 0.3077953181299197, 0.30587749922168, 0.3080797531322166, 0.3053343150866926, 0.30636331041517034, 0.29795914849359995, 0.29964241740765124, 0.3035556258541079, 0.2907433859418619, 0.29184944252183065, 0.2895517810803878, 0.29556901422849113, 0.29479857257002207, 0.28502748127293426, 0.28702783276713323, 0.29057542421647203, 0.28580779275967655, 0.28972401972470346, 0.2885932138451298, 0.28085238507159627, 0.2789388960139851, 0.27595366022817147, 0.2826696396034592, 0.2783313512320418, 0.2767100309353994], 'val_acc': [0.7625610825711063, 0.7646911414609698, 0.7848640521237941, 0.7897506578123042, 0.8099235684751285, 0.8262122541034959, 0.8352336799899762, 0.84488159378524, 0.8507705801278035, 0.8565342688886104, 0.8582884350332038, 0.861671469740634, 0.8643027189575241, 0.8676857536649543, 0.868562836737251, 0.870066407718331, 0.8763312868061647, 0.8783360481142714, 0.8798396190953515, 0.8817190828217015, 0.8822202731487282, 0.8840997368750783, 0.883473248966295, 0.8859792006014284, 0.8838491417115649, 0.8876080691642652, 0.8896128304723718, 0.8912416990352087, 0.8906152111264253, 0.891492294198722, 0.8876080691642652, 0.893747650670342, 0.8952512216514221, 0.8972559829595289, 0.8982583636135822, 0.8982583636135822, 0.8988848515223656, 0.9013908031574991, 0.9046485402831725, 0.9052750281919559, 0.9045232427014158, 0.9060268136824959, 0.906402706427766, 0.9109134193710061, 0.9096604435534394, 0.9107881217892495, 0.9101616338804661, 0.9114146096980328, 0.915549429896003, 0.9111640145345195, 0.9136699661696529, 0.9164265129682997, 0.9146723468237064, 0.9152988347324896, 0.9149229419872197, 0.9181806791128931, 0.9183059766946498, 0.9163012153865431, 0.9185565718581631, 0.9141711564966796, 0.9200601428392432, 0.9201854404209999, 0.9199348452574865, 0.9209372259115399, 0.9180553815311364, 0.9210625234932965, 0.9239443678737, 0.9201854404209999, 0.9226913920561333, 0.9245708557824834, 0.9191830597669465, 0.9238190702919433, 0.9261997243453202, 0.9240696654554567, 0.9233178799649167, 0.92356847512843, 0.9260744267635634, 0.92469615336424, 0.9245708557824834, 0.92469615336424, 0.9261997243453202, 0.9248214509459968, 0.9268262122541034, 0.9275779977446436, 0.9269515098358602, 0.9264503195088335, 0.9278285929081569, 0.9277032953264002], 'size_bytes': 26925, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 88, 'batch_size': 64, 'lr': 0.00011014598449447628, 'weight_decay': 0.0002527900807709807, 'patch_size': 20, 'n_heads': 1, 'head_dim': 30, 'num_layers': 1, 'mlp_ratio': 2, 'dropout': 0.3506840996131593, 'attn_dropout': 0.08701244589255584, 'augment_noise_std': 0.03746001331179031, 'augment_scale_low': 0.8218154070516586, 'augment_scale_high': 1.121735357561923, 'use_focal_loss': False, 'focal_gamma': 4.761739793216449, 'class_weighted': False, 'grad_clip': 1.650117589624561, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 10445, 'model_storage_size_kb': 22.440429687500004, 'model_size_validation': 'PASS'}
2025-10-02 20:54:16,548 - INFO - _models.training_function_executor - BO Objective: base=0.9277, size_penalty=0.0000, final=0.9277
2025-10-02 20:54:16,548 - INFO - _models.training_function_executor - Model: 10,445 parameters, 22.4KB (PASS 256KB limit)
2025-10-02 20:54:16,548 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 106.638s
2025-10-02 20:54:16,654 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9277
2025-10-02 20:54:16,655 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.107s
2025-10-02 20:54:16,655 - INFO - bo.run_bo - Recorded observation #28: hparams={'epochs': np.int64(88), 'batch_size': np.int64(64), 'lr': 0.00011014598449447628, 'weight_decay': 0.0002527900807709807, 'patch_size': np.int64(20), 'n_heads': np.int64(1), 'head_dim': np.int64(30), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(2), 'dropout': 0.3506840996131593, 'attn_dropout': 0.08701244589255584, 'augment_noise_std': 0.03746001331179031, 'augment_scale_low': 0.8218154070516586, 'augment_scale_high': 1.121735357561923, 'use_focal_loss': np.False_, 'focal_gamma': 4.761739793216449, 'class_weighted': np.False_, 'grad_clip': 1.650117589624561, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9277
2025-10-02 20:54:16,655 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 28: {'epochs': np.int64(88), 'batch_size': np.int64(64), 'lr': 0.00011014598449447628, 'weight_decay': 0.0002527900807709807, 'patch_size': np.int64(20), 'n_heads': np.int64(1), 'head_dim': np.int64(30), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(2), 'dropout': 0.3506840996131593, 'attn_dropout': 0.08701244589255584, 'augment_noise_std': 0.03746001331179031, 'augment_scale_low': 0.8218154070516586, 'augment_scale_high': 1.121735357561923, 'use_focal_loss': np.False_, 'focal_gamma': 4.761739793216449, 'class_weighted': np.False_, 'grad_clip': 1.650117589624561, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9277
2025-10-02 20:54:16,655 - INFO - bo.run_bo - üîçBO Trial 29: Using RF surrogate + Expected Improvement
2025-10-02 20:54:16,655 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:54:16,655 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:54:16,655 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:54:16,655 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 92, 'batch_size': 64, 'lr': 0.0044277074826261165, 'weight_decay': 0.003950839055901104, 'patch_size': 100, 'n_heads': 1, 'head_dim': 32, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.1825573732767237, 'attn_dropout': 0.17812375162452537, 'augment_noise_std': 0.04290697119114691, 'augment_scale_low': 0.8395754083518261, 'augment_scale_high': 1.1794400850505293, 'use_focal_loss': False, 'focal_gamma': 4.513670459947833, 'class_weighted': False, 'grad_clip': 1.994972157823313, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:54:16,657 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 92, 'batch_size': 64, 'lr': 0.0044277074826261165, 'weight_decay': 0.003950839055901104, 'patch_size': 100, 'n_heads': 1, 'head_dim': 32, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.1825573732767237, 'attn_dropout': 0.17812375162452537, 'augment_noise_std': 0.04290697119114691, 'augment_scale_low': 0.8395754083518261, 'augment_scale_high': 1.1794400850505293, 'use_focal_loss': False, 'focal_gamma': 4.513670459947833, 'class_weighted': False, 'grad_clip': 1.994972157823313, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:56:46,184 - INFO - _models.training_function_executor - Model: 32,581 parameters, 140.0KB storage
2025-10-02 20:56:46,184 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.673256330594224, 0.4867432042029785, 0.3624618743091688, 0.2952844756488957, 0.25172016217371534, 0.2364401374576691, 0.2197038158716028, 0.2049897778684161, 0.1992592325456436, 0.1882458258959015, 0.19262998363086165, 0.18333448323535279, 0.17575019857632734, 0.18190842014875858, 0.16836447519784478, 0.1684696228763788, 0.16297121134240075, 0.16352576036821334, 0.1628664461678477, 0.16603660798298644, 0.16469227889854313, 0.15432322333143442, 0.15086095261606652, 0.1584900516839388, 0.14862418090349017, 0.15157198785047643, 0.14156143113266378, 0.15030654760423776, 0.1459595962070087, 0.146610298550527, 0.15031812194991193, 0.14509918963879648, 0.1536427524548905, 0.14450499055460125, 0.14108267036614874, 0.14229418459736015, 0.13646255788461498, 0.14770141825734817, 0.1447873683440061, 0.13719599652119688, 0.1414892237167966, 0.1376815598228277, 0.13532972252634434, 0.134059366939794, 0.1368282710633843, 0.13473083664395696, 0.13842202637558035, 0.13590304846135953, 0.13688868812578825, 0.13355116861653657, 0.13229305584955622, 0.1315897856525818, 0.12912415385365775, 0.13515064665496945, 0.142362512076528, 0.12926369995666234, 0.1282512941829648, 0.13002868939457288, 0.12858773810332183, 0.12883199559411984, 0.12647208209629365, 0.13207909345175917, 0.12892510055705866, 0.13131798543387238, 0.13062144027849534, 0.13128441341924543, 0.13021696262598165, 0.1286245935372148, 0.13211025961887748, 0.12421980980332437, 0.13422178495136772, 0.13172657657216874, 0.1262002369052328, 0.12694484087720914, 0.12705980063780534, 0.12711578293599957, 0.12452420222474694, 0.13408976247057977, 0.12820259019108385, 0.1264782688704939, 0.12879062914793807, 0.12062806798734432, 0.12497403980593512, 0.12675526633665432, 0.12892928352534605, 0.12824257926325341, 0.12380153131244871, 0.12441993476778919, 0.12831567861266974, 0.12229885965566104, 0.12536215451308683, 0.12536425855910124], 'val_losses': [0.5373614136250511, 0.37942688784538364, 0.2534282336074776, 0.2264047349631495, 0.21858801252653598, 0.19613442871411063, 0.17981110192912814, 0.1794140789974768, 0.1634446501156812, 0.2283896832683603, 0.17533159520266545, 0.1440754991093348, 0.15594168092325925, 0.16450727078165123, 0.1489613936744213, 0.12249928085879565, 0.13883822363594872, 0.1489428407709232, 0.1302044282137205, 0.13115942509733575, 0.13407208461931977, 0.1280805380750518, 0.12090378672784469, 0.11440620719247586, 0.11646257547967163, 0.11286819250496867, 0.11895358043421392, 0.11180463977239287, 0.13068849751822648, 0.12690789892274384, 0.11666996542552467, 0.1197641128061514, 0.14243907646668494, 0.11024403731157689, 0.11883012924001206, 0.10799693254607615, 0.11307673123774262, 0.10316055154437455, 0.12979058400421958, 0.11717238357761363, 0.10767023155670867, 0.11636901198566714, 0.11955564703790933, 0.11573139194232482, 0.12170656717909471, 0.1229144311818678, 0.12588846165387646, 0.1066130691939257, 0.11996432110273275, 0.12750715887564462, 0.11719342207358963, 0.0968789099104765, 0.10456956942518945, 0.09734728953105258, 0.12196978211007073, 0.09436050576832701, 0.11804847133225359, 0.11203247713327198, 0.10123351989100413, 0.10995387264383032, 0.10198597855144328, 0.11333718657579477, 0.11065659327078935, 0.11622920526592315, 0.10691720143645767, 0.10054416286382979, 0.11587886100588489, 0.1234494661666101, 0.10739395554696628, 0.10664174440898568, 0.11385227574574411, 0.10028340568811041, 0.10371969305207565, 0.10788181762829145, 0.10413588960899504, 0.11189131999322756, 0.10820718745534962, 0.11141741549127787, 0.10617956621256139, 0.10079622834775251, 0.10584510374399314, 0.11391208379855805, 0.10104316453962991, 0.09761202486964518, 0.11305592858190898, 0.09744296873135609, 0.11593320419739518, 0.10780768977801851, 0.11334405453051112, 0.10497956157529761, 0.10934231668064699, 0.11762794577376001], 'val_acc': [0.8131813056008019, 0.8830973562210249, 0.9208119283297832, 0.930083949379777, 0.9312116276155871, 0.9399824583385541, 0.9490038842250345, 0.946122039844631, 0.9510086455331412, 0.9292068663074803, 0.9488785866432777, 0.954141085077058, 0.9521363237689513, 0.9475003132439543, 0.9566470367121914, 0.962160130309485, 0.9592782859290816, 0.9558952512216514, 0.9620348327277284, 0.9624107254729983, 0.9604059641648917, 0.9616589399824583, 0.9619095351459717, 0.9666708432527252, 0.9636637012905651, 0.9665455456709685, 0.9647913795263752, 0.9639142964540784, 0.9607818569101616, 0.9629119158000251, 0.9674226287432652, 0.963287808545295, 0.9585265004385415, 0.9681744142338052, 0.9640395940358351, 0.968675604560832, 0.9670467359979953, 0.9699285803783987, 0.9634131061270518, 0.9649166771081318, 0.9681744142338052, 0.9637889988723217, 0.9665455456709685, 0.9647913795263752, 0.9651672722716451, 0.9610324520736749, 0.9610324520736749, 0.9680491166520486, 0.9639142964540784, 0.9626613206365117, 0.9651672722716451, 0.9695526876331286, 0.9679238190702919, 0.9710562586142087, 0.9649166771081318, 0.9714321513594788, 0.9616589399824583, 0.9652925698534018, 0.968299711815562, 0.9680491166520486, 0.9689261997243453, 0.9667961408344818, 0.9666708432527252, 0.9664202480892119, 0.9674226287432652, 0.9695526876331286, 0.9651672722716451, 0.9604059641648917, 0.9684250093973187, 0.968299711815562, 0.9652925698534018, 0.9680491166520486, 0.969051497306102, 0.969427390051372, 0.969427390051372, 0.9662949505074552, 0.9676732239067786, 0.9651672722716451, 0.969427390051372, 0.970179175541912, 0.9685503069790753, 0.9671720335797519, 0.9695526876331286, 0.9714321513594788, 0.9664202480892119, 0.9688009021425886, 0.967547926325022, 0.9674226287432652, 0.9691767948878587, 0.9672973311615086, 0.9700538779601554, 0.9649166771081318], 'size_bytes': 144677, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 92, 'batch_size': 64, 'lr': 0.0044277074826261165, 'weight_decay': 0.003950839055901104, 'patch_size': 100, 'n_heads': 1, 'head_dim': 32, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.1825573732767237, 'attn_dropout': 0.17812375162452537, 'augment_noise_std': 0.04290697119114691, 'augment_scale_low': 0.8395754083518261, 'augment_scale_high': 1.1794400850505293, 'use_focal_loss': False, 'focal_gamma': 4.513670459947833, 'class_weighted': False, 'grad_clip': 1.994972157823313, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 32581, 'model_storage_size_kb': 139.99648437500002, 'model_size_validation': 'PASS'}
2025-10-02 20:56:46,184 - INFO - _models.training_function_executor - BO Objective: base=0.9649, size_penalty=0.0000, final=0.9649
2025-10-02 20:56:46,184 - INFO - _models.training_function_executor - Model: 32,581 parameters, 140.0KB (PASS 256KB limit)
2025-10-02 20:56:46,184 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 149.529s
2025-10-02 20:56:46,290 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9649
2025-10-02 20:56:46,290 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.106s
2025-10-02 20:56:46,291 - INFO - bo.run_bo - Recorded observation #29: hparams={'epochs': np.int64(92), 'batch_size': np.int64(64), 'lr': 0.0044277074826261165, 'weight_decay': 0.003950839055901104, 'patch_size': np.int64(100), 'n_heads': np.int64(1), 'head_dim': np.int64(32), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.1825573732767237, 'attn_dropout': 0.17812375162452537, 'augment_noise_std': 0.04290697119114691, 'augment_scale_low': 0.8395754083518261, 'augment_scale_high': 1.1794400850505293, 'use_focal_loss': np.False_, 'focal_gamma': 4.513670459947833, 'class_weighted': np.False_, 'grad_clip': 1.994972157823313, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9649
2025-10-02 20:56:46,291 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 29: {'epochs': np.int64(92), 'batch_size': np.int64(64), 'lr': 0.0044277074826261165, 'weight_decay': 0.003950839055901104, 'patch_size': np.int64(100), 'n_heads': np.int64(1), 'head_dim': np.int64(32), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.1825573732767237, 'attn_dropout': 0.17812375162452537, 'augment_noise_std': 0.04290697119114691, 'augment_scale_low': 0.8395754083518261, 'augment_scale_high': 1.1794400850505293, 'use_focal_loss': np.False_, 'focal_gamma': 4.513670459947833, 'class_weighted': np.False_, 'grad_clip': 1.994972157823313, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9649
2025-10-02 20:56:46,291 - INFO - bo.run_bo - üîçBO Trial 30: Using RF surrogate + Expected Improvement
2025-10-02 20:56:46,291 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 20:56:46,291 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:56:46,291 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:56:46,291 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 79, 'batch_size': 256, 'lr': 0.001462587316144684, 'weight_decay': 0.00011335466079183592, 'patch_size': 500, 'n_heads': 1, 'head_dim': 29, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.11374931351980053, 'attn_dropout': 0.00962169532386873, 'augment_noise_std': 0.015393534690090136, 'augment_scale_low': 0.8496543258267686, 'augment_scale_high': 1.1166762126756438, 'use_focal_loss': False, 'focal_gamma': 4.478721580192662, 'class_weighted': False, 'grad_clip': 0.0038503344248036484, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:56:46,292 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 79, 'batch_size': 256, 'lr': 0.001462587316144684, 'weight_decay': 0.00011335466079183592, 'patch_size': 500, 'n_heads': 1, 'head_dim': 29, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.11374931351980053, 'attn_dropout': 0.00962169532386873, 'augment_noise_std': 0.015393534690090136, 'augment_scale_low': 0.8496543258267686, 'augment_scale_high': 1.1166762126756438, 'use_focal_loss': False, 'focal_gamma': 4.478721580192662, 'class_weighted': False, 'grad_clip': 0.0038503344248036484, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 20:58:01,234 - INFO - _models.training_function_executor - Model: 57,454 parameters, 246.9KB storage
2025-10-02 20:58:01,234 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7819729206037227, 0.6361497920101621, 0.5175416951174202, 0.4365661772641869, 0.37545446026139634, 0.3391486108275063, 0.3068361498460515, 0.2840912707797179, 0.26030895529569914, 0.24591706874985012, 0.23451833162658986, 0.22020488203923677, 0.20926064272551698, 0.2017150780577164, 0.19031014271236632, 0.1856907442096809, 0.17726610036699245, 0.1736074999510202, 0.16868187024682005, 0.15879026457750905, 0.1549972600230626, 0.15031047932787528, 0.14972014232470576, 0.1401653370471619, 0.13849799811403146, 0.13667070918471108, 0.12757344485305736, 0.12653943700524226, 0.12446537905282269, 0.12278320690691592, 0.11951865406720757, 0.11621829591563361, 0.11093760039936841, 0.10802040054530154, 0.10985632251718344, 0.10456105730525608, 0.10365173608275638, 0.098167181094368, 0.09558296950048355, 0.09315805923082482, 0.0949619572179171, 0.09476778747944091, 0.09183054128642205, 0.0887326941636781, 0.08271720172602876, 0.0881131701025528, 0.08800708422360816, 0.08159560066939364, 0.08338440626652413, 0.07830738474066476, 0.07657177501304091, 0.07781748031540998, 0.0744782949858812, 0.0766610614240078, 0.0739666929245798, 0.07337129085262244, 0.07136727141396867, 0.06610034017038592, 0.06864238263863473, 0.06816451159235182, 0.06685515234462563, 0.06643220702669873, 0.06379206312817223, 0.06458652384666237, 0.06512543979868948, 0.06392673779486363, 0.06171933879747138, 0.0642296725687622, 0.0616530029945881, 0.05835637257471216, 0.06158769555690724, 0.057297802148953685, 0.058558236565557004, 0.0574479096286162, 0.05738115368246812, 0.057609115135625645, 0.0554116010219881, 0.056566390871354684, 0.05634785377307669], 'val_losses': [0.6860326071600167, 0.5613573053444467, 0.4531755804335947, 0.39163019197161314, 0.35634559289492873, 0.33692515459503986, 0.2861892291327315, 0.27937408497183863, 0.25275355669889304, 0.24750587736349658, 0.23349399793257825, 0.23137477153995972, 0.22243844706818597, 0.20749783104799277, 0.2151217001844223, 0.20941210689755732, 0.2052597198894397, 0.2117420081728354, 0.20517530572614548, 0.21143621603598078, 0.19326181081307442, 0.19241799834064038, 0.19414702587435528, 0.19082032083202882, 0.19978508550583637, 0.2040749268862795, 0.1845036231111112, 0.1866467535633922, 0.18418201329362766, 0.18181881438317626, 0.19100426053428782, 0.18607531617703765, 0.1978941000395887, 0.19708429652412887, 0.18798544241168896, 0.18717027075897705, 0.19200680963438327, 0.1898846625217503, 0.1807526628048165, 0.19252056106336882, 0.18490437321787773, 0.18322809267209864, 0.19034762146172407, 0.19773916879806763, 0.1862023324577182, 0.19145729876807302, 0.18097143966875912, 0.2018924591617766, 0.19200678328989204, 0.1920922326535274, 0.1879474800857233, 0.19443401135480906, 0.19465314931752106, 0.19442260348061963, 0.19424182860127756, 0.19830207421193818, 0.19754508012982033, 0.2044397652141732, 0.19982888934838117, 0.20190853233916048, 0.19459365923114028, 0.19584226485049122, 0.20907770841100165, 0.1986066369115074, 0.19828035351126552, 0.20486356349550622, 0.19746118460572643, 0.19250950834876895, 0.20264306066254598, 0.1992997360541668, 0.20295716699106833, 0.20217484503502745, 0.20081050904408657, 0.20098310511275919, 0.21329702480240223, 0.1993891863469088, 0.21349023966812547, 0.19763602026422106, 0.2048226101978607], 'val_acc': [0.775717328655557, 0.8205738629244456, 0.8518982583636135, 0.875454203733868, 0.8882345570730484, 0.8921187821075053, 0.9100363362987094, 0.9120410976068162, 0.9210625234932965, 0.92319258238316, 0.9294574614709936, 0.9290815687257236, 0.9288309735622102, 0.9362235308858539, 0.9347199599047739, 0.9350958526500438, 0.938478887357474, 0.9343440671595038, 0.9339681744142339, 0.9371006139581506, 0.9429896003007142, 0.9422378148101742, 0.9437413857912542, 0.9439919809547676, 0.9432401954642275, 0.9411101365743642, 0.9476256108257111, 0.9462473374263877, 0.9454955519358477, 0.9475003132439543, 0.9441172785365243, 0.9469991229169277, 0.9439919809547676, 0.9447437664453076, 0.9439919809547676, 0.9462473374263877, 0.9459967422628743, 0.9462473374263877, 0.9492544793885478, 0.9482520987344945, 0.9508833479513845, 0.9487532890615211, 0.9459967422628743, 0.944994361608821, 0.9496303721338178, 0.9517604310236812, 0.9497556697155745, 0.946122039844631, 0.9500062648790878, 0.9495050745520611, 0.9522616213507079, 0.9466232301716577, 0.9507580503696279, 0.9473750156621977, 0.9516351334419245, 0.9498809672973312, 0.9517604310236812, 0.9497556697155745, 0.9497556697155745, 0.9486279914797644, 0.9475003132439543, 0.9522616213507079, 0.9486279914797644, 0.9510086455331412, 0.9497556697155745, 0.9521363237689513, 0.9517604310236812, 0.9505074552061146, 0.9503821576243578, 0.953389299586518, 0.9536398947500313, 0.9503821576243578, 0.9516351334419245, 0.9530134068412479, 0.9467485277534143, 0.9502568600426011, 0.952637514095978, 0.9547675729858414, 0.9510086455331412], 'size_bytes': 246785, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 79, 'batch_size': 256, 'lr': 0.001462587316144684, 'weight_decay': 0.00011335466079183592, 'patch_size': 500, 'n_heads': 1, 'head_dim': 29, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.11374931351980053, 'attn_dropout': 0.00962169532386873, 'augment_noise_std': 0.015393534690090136, 'augment_scale_low': 0.8496543258267686, 'augment_scale_high': 1.1166762126756438, 'use_focal_loss': False, 'focal_gamma': 4.478721580192662, 'class_weighted': False, 'grad_clip': 0.0038503344248036484, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 57454, 'model_storage_size_kb': 246.87265625000003, 'model_size_validation': 'PASS'}
2025-10-02 20:58:01,234 - INFO - _models.training_function_executor - BO Objective: base=0.9510, size_penalty=0.0000, final=0.9510
2025-10-02 20:58:01,234 - INFO - _models.training_function_executor - Model: 57,454 parameters, 246.9KB (PASS 256KB limit)
2025-10-02 20:58:01,234 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 74.943s
2025-10-02 20:58:01,345 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9510
2025-10-02 20:58:01,345 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-10-02 20:58:01,345 - INFO - bo.run_bo - Recorded observation #30: hparams={'epochs': np.int64(79), 'batch_size': np.int64(256), 'lr': 0.001462587316144684, 'weight_decay': 0.00011335466079183592, 'patch_size': np.int64(500), 'n_heads': np.int64(1), 'head_dim': np.int64(29), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(2), 'dropout': 0.11374931351980053, 'attn_dropout': 0.00962169532386873, 'augment_noise_std': 0.015393534690090136, 'augment_scale_low': 0.8496543258267686, 'augment_scale_high': 1.1166762126756438, 'use_focal_loss': np.False_, 'focal_gamma': 4.478721580192662, 'class_weighted': np.False_, 'grad_clip': 0.0038503344248036484, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9510
2025-10-02 20:58:01,345 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 30: {'epochs': np.int64(79), 'batch_size': np.int64(256), 'lr': 0.001462587316144684, 'weight_decay': 0.00011335466079183592, 'patch_size': np.int64(500), 'n_heads': np.int64(1), 'head_dim': np.int64(29), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(2), 'dropout': 0.11374931351980053, 'attn_dropout': 0.00962169532386873, 'augment_noise_std': 0.015393534690090136, 'augment_scale_low': 0.8496543258267686, 'augment_scale_high': 1.1166762126756438, 'use_focal_loss': np.False_, 'focal_gamma': 4.478721580192662, 'class_weighted': np.False_, 'grad_clip': 0.0038503344248036484, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9510
2025-10-02 20:58:01,345 - INFO - bo.run_bo - üîçBO Trial 31: Using RF surrogate + Expected Improvement
2025-10-02 20:58:01,345 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 20:58:01,346 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 20:58:01,346 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 20:58:01,346 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 97, 'batch_size': 256, 'lr': 0.003038957801965804, 'weight_decay': 0.0010539072297286452, 'patch_size': 20, 'n_heads': 3, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.303205171076286, 'attn_dropout': 0.15719420676058288, 'augment_noise_std': 0.008175290499400946, 'augment_scale_low': 0.8199304515903392, 'augment_scale_high': 1.118935702306287, 'use_focal_loss': True, 'focal_gamma': 4.621597221019115, 'class_weighted': False, 'grad_clip': 4.0985819757765665, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 20:58:01,347 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 97, 'batch_size': 256, 'lr': 0.003038957801965804, 'weight_decay': 0.0010539072297286452, 'patch_size': 20, 'n_heads': 3, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.303205171076286, 'attn_dropout': 0.15719420676058288, 'augment_noise_std': 0.008175290499400946, 'augment_scale_low': 0.8199304515903392, 'augment_scale_high': 1.118935702306287, 'use_focal_loss': True, 'focal_gamma': 4.621597221019115, 'class_weighted': False, 'grad_clip': 4.0985819757765665, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:02:17,783 - INFO - _models.training_function_executor - Model: 140,760 parameters, 151.2KB storage
2025-10-02 21:02:17,783 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.14428358985520662, 0.0756057587514111, 0.05998493364782772, 0.05370574322304876, 0.05409379682461017, 0.04885171742998552, 0.049320192481580614, 0.04418451864378199, 0.04463711359521479, 0.04185727185348302, 0.04129305905365923, 0.04636987780086869, 0.046748406608628, 0.04404118202253978, 0.04728469827291426, 0.04399875275452506, 0.05142468059692743, 0.04555972179952422, 0.04154949418714264, 0.040461774576279495, 0.04512900283875645, 0.0399847900276653, 0.03734651996147844, 0.040439721223379896, 0.0439732297193032, 0.038905737764697634, 0.04214650930769871, 0.04370126907954368, 0.043918211606245074, 0.04221520232101418, 0.03780762731017906, 0.037746986073358045, 0.03869441401634564, 0.03976011588640636, 0.03860863872380987, 0.04209787763287126, 0.0461337980333636, 0.04456887198554621, 0.037380554135575485, 0.03729489281086461, 0.03510203313818109, 0.03600341322106631, 0.03539179511879637, 0.037631566518722154, 0.040500313366667574, 0.038982147176038186, 0.038788783946165716, 0.04185908756152655, 0.0363823426925611, 0.03732049749789615, 0.039698262669570133, 0.03265765478846588, 0.03979362582196397, 0.04915998245060539, 0.052826242744586555, 0.04590840547547784, 0.051067195957944275, 0.040859566711880554, 0.0380170978039771, 0.03927343802478339, 0.03651100409783184, 0.03636251927214253, 0.035682900956343, 0.03907855064903544, 0.03366013074059932, 0.03850735089746403, 0.03675904096763917, 0.038717031456592874, 0.03811699854118354, 0.03512974718740641, 0.03502678557732341, 0.03124377179634256, 0.02975393487336294, 0.026702170023386616, 0.029076225138740672, 0.028597477708258445, 0.027052011343919824, 0.02740210791638937, 0.024852747230411615, 0.03204952015779875, 0.032918205032346194, 0.04724275181853293, 0.03322799952184762, 0.031557558416966426, 0.030374774499057497, 0.029203779455721873, 0.030530710129925427, 0.03614402431248405, 0.030203603743700232, 0.026737791632042134, 0.036095275705225076, 0.030009430131029486, 0.030959355464148606, 0.03176040257471466, 0.030907571497461244, 0.03202713769359502, 0.035148688083484175], 'val_losses': [0.07328572836930353, 0.06257519529507641, 0.06844829235498043, 0.05093667054540224, 0.05857556165450274, 0.043004709862703785, 0.0514922033145402, 0.04938513010482623, 0.046531419231407106, 0.048900239408117024, 0.04261880483193111, 0.05174140231664012, 0.04703266405632451, 0.06286374765568815, 0.06827250359209538, 0.05698431845553207, 0.06061858911701588, 0.07416815955325813, 0.05074227596496606, 0.050981787867846906, 0.05110050476980141, 0.04699623081464376, 0.057852946356258185, 0.06947152715751871, 0.06423985502345268, 0.05829982101894395, 0.05816060267953612, 0.0605914762927213, 0.062924701611252, 0.057037378949448214, 0.04687978147661278, 0.061749634834149025, 0.07323931747235991, 0.07291475430881525, 0.08764997499161399, 0.0801560144776719, 0.052559357508503546, 0.06629026124141611, 0.048001513806030634, 0.05084411832954991, 0.05303766693354639, 0.05588372535756202, 0.04650698799176438, 0.05797065584398148, 0.06123709037214753, 0.05302969816511562, 0.0502343529660538, 0.05675939052467126, 0.06003443194130129, 0.051311888311048014, 0.059508906221797914, 0.04368669171111028, 0.06488800102308283, 0.10516238976127595, 0.07721928213187969, 0.06625686247605873, 0.08029993716834488, 0.051133112530315626, 0.05265773793921909, 0.06210018402016144, 0.05966461097496955, 0.04798296695373707, 0.09604886247438157, 0.06292848168679638, 0.0629961433838047, 0.054851204113232625, 0.0627126159904229, 0.05842067099302025, 0.04177741097376664, 0.05171407502839596, 0.04823923159459303, 0.05391677820586184, 0.05231751147468591, 0.05242555532984874, 0.07022881382347448, 0.05759191929700306, 0.05717812730131285, 0.050283314636619655, 0.06925223757395124, 0.060606813310546216, 0.08369081001133642, 0.05743466495831383, 0.049749707921923084, 0.042403993136455445, 0.060484047716383355, 0.05210142770483147, 0.05119096108078266, 0.06673983857980846, 0.04181795120388624, 0.05313512088693356, 0.0670317270364876, 0.058464033985896724, 0.07540395146786845, 0.06304936535413032, 0.07416503743604974, 0.05010611621800438, 0.07467374766459965], 'val_acc': [0.8740759303345446, 0.8263375516852525, 0.8729482520987345, 0.8986342563588523, 0.9092845508081694, 0.9198095476757299, 0.9059015161007392, 0.9236937727101867, 0.9149229419872197, 0.9144217516601929, 0.9273274025811302, 0.8950006264879088, 0.8913669966169653, 0.884225034456835, 0.8580378398696905, 0.8856033078561584, 0.8882345570730484, 0.8772083698784613, 0.8944994361608821, 0.876581881969678, 0.8961283047237188, 0.9188071670216765, 0.9027690765568225, 0.8923693772710186, 0.9055256233554693, 0.9066533015912793, 0.8803408094223781, 0.8858539030196717, 0.8629244455582007, 0.8990101491041222, 0.9066533015912793, 0.8803408094223781, 0.8892369377271019, 0.8778348577872447, 0.8784613456960281, 0.8532765317629369, 0.9041473499561459, 0.8622979576494174, 0.9056509209372259, 0.9003884225034456, 0.8975065781230422, 0.9091592532264128, 0.899887232176419, 0.9048991354466859, 0.900639017666959, 0.8966294950507455, 0.8894875328906152, 0.8733241448440046, 0.9051497306101992, 0.9106628242074928, 0.9003884225034456, 0.900639017666959, 0.8640521237940108, 0.8329783235183561, 0.8840997368750783, 0.8807167021676482, 0.8734494424257612, 0.8967547926325022, 0.891868186943992, 0.8886104498183185, 0.8645533141210374, 0.9081568725723593, 0.8453827841122666, 0.8799649166771081, 0.9057762185189826, 0.884225034456835, 0.9042726475379025, 0.9031449693020924, 0.9101616338804661, 0.8970053877960156, 0.9022678862297958, 0.8988848515223656, 0.8967547926325022, 0.909033955644656, 0.8871068788372385, 0.8912416990352087, 0.8864803909284551, 0.901766695902769, 0.9072797895000626, 0.8977571732865556, 0.8607943866683373, 0.8873574740007518, 0.9052750281919559, 0.9161759178047864, 0.9041473499561459, 0.9139205613331662, 0.9200601428392432, 0.8884851522365619, 0.9206866307480266, 0.9015161007392557, 0.8852274151108883, 0.9116652048615461, 0.8833479513845383, 0.901014910412229, 0.84826462849267, 0.899887232176419, 0.8719458714446812], 'size_bytes': 721419, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 97, 'batch_size': 256, 'lr': 0.003038957801965804, 'weight_decay': 0.0010539072297286452, 'patch_size': 20, 'n_heads': 3, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.303205171076286, 'attn_dropout': 0.15719420676058288, 'augment_noise_std': 0.008175290499400946, 'augment_scale_low': 0.8199304515903392, 'augment_scale_high': 1.118935702306287, 'use_focal_loss': True, 'focal_gamma': 4.621597221019115, 'class_weighted': False, 'grad_clip': 4.0985819757765665, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 140760, 'model_storage_size_kb': 151.20703125, 'model_size_validation': 'PASS'}
2025-10-02 21:02:17,783 - INFO - _models.training_function_executor - BO Objective: base=0.8719, size_penalty=0.0000, final=0.8719
2025-10-02 21:02:17,783 - INFO - _models.training_function_executor - Model: 140,760 parameters, 151.2KB (PASS 256KB limit)
2025-10-02 21:02:17,783 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 256.438s
2025-10-02 21:02:17,892 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8719
2025-10-02 21:02:17,892 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.109s
2025-10-02 21:02:17,892 - INFO - bo.run_bo - Recorded observation #31: hparams={'epochs': np.int64(97), 'batch_size': np.int64(256), 'lr': 0.003038957801965804, 'weight_decay': 0.0010539072297286452, 'patch_size': np.int64(20), 'n_heads': np.int64(3), 'head_dim': np.int64(30), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(2), 'dropout': 0.303205171076286, 'attn_dropout': 0.15719420676058288, 'augment_noise_std': 0.008175290499400946, 'augment_scale_low': 0.8199304515903392, 'augment_scale_high': 1.118935702306287, 'use_focal_loss': np.True_, 'focal_gamma': 4.621597221019115, 'class_weighted': np.False_, 'grad_clip': 4.0985819757765665, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.8719
2025-10-02 21:02:17,892 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 31: {'epochs': np.int64(97), 'batch_size': np.int64(256), 'lr': 0.003038957801965804, 'weight_decay': 0.0010539072297286452, 'patch_size': np.int64(20), 'n_heads': np.int64(3), 'head_dim': np.int64(30), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(2), 'dropout': 0.303205171076286, 'attn_dropout': 0.15719420676058288, 'augment_noise_std': 0.008175290499400946, 'augment_scale_low': 0.8199304515903392, 'augment_scale_high': 1.118935702306287, 'use_focal_loss': np.True_, 'focal_gamma': 4.621597221019115, 'class_weighted': np.False_, 'grad_clip': 4.0985819757765665, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.8719
2025-10-02 21:02:17,893 - INFO - bo.run_bo - üîçBO Trial 32: Using RF surrogate + Expected Improvement
2025-10-02 21:02:17,893 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 21:02:17,893 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:02:17,893 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:02:17,893 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 86, 'batch_size': 128, 'lr': 0.00016684328629588887, 'weight_decay': 5.688218016220795e-06, 'patch_size': 200, 'n_heads': 1, 'head_dim': 20, 'num_layers': 1, 'mlp_ratio': 2, 'dropout': 0.2098124029734263, 'attn_dropout': 0.02042851252483621, 'augment_noise_std': 0.03337435574011033, 'augment_scale_low': 0.8187339342477863, 'augment_scale_high': 1.1655504890598227, 'use_focal_loss': False, 'focal_gamma': 4.296655645135667, 'class_weighted': False, 'grad_clip': 2.9663873859849472, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:02:17,894 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 86, 'batch_size': 128, 'lr': 0.00016684328629588887, 'weight_decay': 5.688218016220795e-06, 'patch_size': 200, 'n_heads': 1, 'head_dim': 20, 'num_layers': 1, 'mlp_ratio': 2, 'dropout': 0.2098124029734263, 'attn_dropout': 0.02042851252483621, 'augment_noise_std': 0.03337435574011033, 'augment_scale_low': 0.8187339342477863, 'augment_scale_high': 1.1655504890598227, 'use_focal_loss': False, 'focal_gamma': 4.296655645135667, 'class_weighted': False, 'grad_clip': 2.9663873859849472, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:03:29,897 - INFO - _models.training_function_executor - Model: 11,665 parameters, 50.1KB storage
2025-10-02 21:03:29,897 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1347588424701587, 0.7747689210315122, 0.7032798167193357, 0.6715141044849628, 0.6495866011965302, 0.6342657856451585, 0.619446053748568, 0.6028285557732368, 0.5866749270967653, 0.5719112612320375, 0.5586551392440251, 0.5463641908726843, 0.531044986906892, 0.5235252028359576, 0.5127342472723043, 0.5056022615770766, 0.4947176358335184, 0.4864301637024657, 0.47625340438574715, 0.46877507234289617, 0.45846068460933176, 0.45244901340766513, 0.44455221574867304, 0.43806135175715294, 0.4301351732298855, 0.4235556210623653, 0.41844435128278984, 0.41574769830530467, 0.4082637849810189, 0.403084591034342, 0.4007957599765618, 0.394222781408947, 0.3895641215645281, 0.38856087972267683, 0.38205832798812667, 0.37603475541764214, 0.373704757329848, 0.36956567571481563, 0.3658886573777194, 0.3635384288437029, 0.36070343179500064, 0.3606080001478181, 0.3511212725028117, 0.3502131368930719, 0.34876025108383574, 0.3450705112287082, 0.3462773884595219, 0.3398252829250692, 0.3374452588244368, 0.3357011852447234, 0.33241364306828936, 0.33148783005647287, 0.32667197955378025, 0.32525528444721935, 0.32271456481878447, 0.3220668682371155, 0.3186111743746741, 0.3147214545194388, 0.3133338627950465, 0.3129997404534779, 0.3097943562575608, 0.30884680610135046, 0.3075019861676955, 0.307203219722089, 0.30111881864738205, 0.3003909774219146, 0.29726456822176583, 0.29835476966384167, 0.2988340206393151, 0.2943909527574107, 0.2928563263910858, 0.2930777392075055, 0.29064119037102926, 0.2870680008250341, 0.2884767974666145, 0.28264591306951176, 0.28361460073409556, 0.2824312700163775, 0.283230289449497, 0.2801623123193547, 0.28100042690819405, 0.2780557991295266, 0.27690446694307347, 0.27642995826646777, 0.27055238319273056, 0.2741179105245772], 'val_losses': [0.832564932558265, 0.7215576841932477, 0.6789085427078593, 0.6582845083724107, 0.6384397462942056, 0.6212602622369195, 0.607195074024124, 0.5976930080261047, 0.57604139242655, 0.5633319947761696, 0.5484299766936707, 0.5369337382635935, 0.5290677362053843, 0.5147369376341362, 0.5075340562484539, 0.49579337492814834, 0.4814771028400558, 0.4747258544139972, 0.46445807488979424, 0.4533967950965151, 0.44777543526695956, 0.4413421367541424, 0.4306738891333898, 0.42037178722184904, 0.41872532647992267, 0.4117082243664135, 0.4052214819362417, 0.39770179453342425, 0.3927754128566288, 0.3865053750655807, 0.3800781614394165, 0.3796739213233035, 0.3747775892336974, 0.3689577925244066, 0.370621703678079, 0.3681593341922091, 0.3554537364742299, 0.35789224898631433, 0.354592502023803, 0.34957729919096264, 0.34285039982241866, 0.3437190013464464, 0.3407767774123414, 0.3340164048564776, 0.33479860226338165, 0.32950631975231426, 0.32681984205605047, 0.32299241490074904, 0.3246096374434108, 0.3220029722149099, 0.31989349038166864, 0.3143942416490969, 0.31662637954247563, 0.3100140055473728, 0.30792870079495677, 0.30987153971438686, 0.303547567520475, 0.30172282108954657, 0.30158318984539884, 0.2975105430238633, 0.2986420365768094, 0.2985491446871579, 0.2965544051334473, 0.2897787289748797, 0.29220482468410863, 0.2960948562012656, 0.28678182005329605, 0.2874472986231351, 0.2862938434583862, 0.28524249262221846, 0.2821023634405845, 0.282641618189216, 0.2793758981561858, 0.2857226474387292, 0.2791339241984852, 0.2739649390781483, 0.27313570439434637, 0.2726941332678466, 0.2723832171918635, 0.2705392722549661, 0.2673920218082108, 0.268614998479607, 0.2734611769126421, 0.26796492442064845, 0.2662868744268586, 0.2681094913080273], 'val_acc': [0.7252224032076181, 0.76957774714948, 0.784488159378524, 0.7915048239568976, 0.7957649417366245, 0.7985214885352713, 0.8045357724595915, 0.808921187821075, 0.8159378523994487, 0.8190702919433654, 0.8257110637764691, 0.8287182057386292, 0.831474752537276, 0.8348577872447062, 0.8371131437163263, 0.8421250469865932, 0.8457586768575367, 0.84751284300213, 0.8512717704548303, 0.8559077809798271, 0.8566595664703671, 0.8572860543791505, 0.8601678987595539, 0.868562836737251, 0.8703170028818443, 0.8716952762811677, 0.8733241448440046, 0.8740759303345446, 0.8760806916426513, 0.8778348577872447, 0.8813431900764315, 0.8795890239318381, 0.8817190828217015, 0.8825961658939983, 0.8832226538027816, 0.8840997368750783, 0.8896128304723718, 0.8859792006014284, 0.8894875328906152, 0.8896128304723718, 0.8928705675980454, 0.8924946748527753, 0.893747650670342, 0.8968800902142589, 0.8962536023054755, 0.8977571732865556, 0.8968800902142589, 0.9025184813933091, 0.9018919934845258, 0.9018919934845258, 0.9033955644656058, 0.9048991354466859, 0.900639017666959, 0.9052750281919559, 0.9067785991730359, 0.9080315749906027, 0.9095351459716827, 0.9080315749906027, 0.9115399072797895, 0.9104122290439794, 0.9115399072797895, 0.9081568725723593, 0.9081568725723593, 0.9131687758426262, 0.9121663951885729, 0.9101616338804661, 0.9150482395689763, 0.9142964540784363, 0.9142964540784363, 0.9129181806791129, 0.9149229419872197, 0.9169277032953264, 0.9173035960405964, 0.9175541912041097, 0.9158000250595163, 0.9168024057135697, 0.9205613331662699, 0.9198095476757299, 0.9183059766946498, 0.9178047863676231, 0.9199348452574865, 0.9209372259115399, 0.9185565718581631, 0.9199348452574865, 0.9221902017291066, 0.9185565718581631], 'size_bytes': 52909, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 86, 'batch_size': 128, 'lr': 0.00016684328629588887, 'weight_decay': 5.688218016220795e-06, 'patch_size': 200, 'n_heads': 1, 'head_dim': 20, 'num_layers': 1, 'mlp_ratio': 2, 'dropout': 0.2098124029734263, 'attn_dropout': 0.02042851252483621, 'augment_noise_std': 0.03337435574011033, 'augment_scale_low': 0.8187339342477863, 'augment_scale_high': 1.1655504890598227, 'use_focal_loss': False, 'focal_gamma': 4.296655645135667, 'class_weighted': False, 'grad_clip': 2.9663873859849472, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 11665, 'model_storage_size_kb': 50.12304687500001, 'model_size_validation': 'PASS'}
2025-10-02 21:03:29,897 - INFO - _models.training_function_executor - BO Objective: base=0.9186, size_penalty=0.0000, final=0.9186
2025-10-02 21:03:29,897 - INFO - _models.training_function_executor - Model: 11,665 parameters, 50.1KB (PASS 256KB limit)
2025-10-02 21:03:29,897 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 72.004s
2025-10-02 21:03:30,005 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9186
2025-10-02 21:03:30,006 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.108s
2025-10-02 21:03:30,006 - INFO - bo.run_bo - Recorded observation #32: hparams={'epochs': np.int64(86), 'batch_size': np.int64(128), 'lr': 0.00016684328629588887, 'weight_decay': 5.688218016220795e-06, 'patch_size': np.int64(200), 'n_heads': np.int64(1), 'head_dim': np.int64(20), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(2), 'dropout': 0.2098124029734263, 'attn_dropout': 0.02042851252483621, 'augment_noise_std': 0.03337435574011033, 'augment_scale_low': 0.8187339342477863, 'augment_scale_high': 1.1655504890598227, 'use_focal_loss': np.False_, 'focal_gamma': 4.296655645135667, 'class_weighted': np.False_, 'grad_clip': 2.9663873859849472, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9186
2025-10-02 21:03:30,006 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 32: {'epochs': np.int64(86), 'batch_size': np.int64(128), 'lr': 0.00016684328629588887, 'weight_decay': 5.688218016220795e-06, 'patch_size': np.int64(200), 'n_heads': np.int64(1), 'head_dim': np.int64(20), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(2), 'dropout': 0.2098124029734263, 'attn_dropout': 0.02042851252483621, 'augment_noise_std': 0.03337435574011033, 'augment_scale_low': 0.8187339342477863, 'augment_scale_high': 1.1655504890598227, 'use_focal_loss': np.False_, 'focal_gamma': 4.296655645135667, 'class_weighted': np.False_, 'grad_clip': 2.9663873859849472, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9186
2025-10-02 21:03:30,006 - INFO - bo.run_bo - üîçBO Trial 33: Using RF surrogate + Expected Improvement
2025-10-02 21:03:30,006 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 21:03:30,006 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:03:30,006 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:03:30,006 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 81, 'batch_size': 256, 'lr': 0.0015054448725399662, 'weight_decay': 6.827600167798092e-05, 'patch_size': 5, 'n_heads': 1, 'head_dim': 28, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.1416616771682256, 'attn_dropout': 0.06473486295278376, 'augment_noise_std': 0.0315070476565121, 'augment_scale_low': 0.8819959379098423, 'augment_scale_high': 1.1799151329208288, 'use_focal_loss': False, 'focal_gamma': 3.6104978133450656, 'class_weighted': False, 'grad_clip': 2.0212269322843226, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:03:30,007 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 81, 'batch_size': 256, 'lr': 0.0015054448725399662, 'weight_decay': 6.827600167798092e-05, 'patch_size': 5, 'n_heads': 1, 'head_dim': 28, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.1416616771682256, 'attn_dropout': 0.06473486295278376, 'augment_noise_std': 0.0315070476565121, 'augment_scale_low': 0.8819959379098423, 'augment_scale_high': 1.1799151329208288, 'use_focal_loss': False, 'focal_gamma': 3.6104978133450656, 'class_weighted': False, 'grad_clip': 2.0212269322843226, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:06:37,229 - INFO - _models.training_function_executor - Model: 12,656 parameters, 13.6KB storage
2025-10-02 21:06:37,229 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.7879800275409259, 0.589720244989902, 0.4566381260679393, 0.3194300404647439, 0.2744544289610094, 0.24811037918198875, 0.22794897231871042, 0.2121544129423221, 0.20105829725850516, 0.18891329827896952, 0.18525944671878666, 0.1755331926843903, 0.17708225766115684, 0.16693831048013125, 0.16101702296882853, 0.15552021763093624, 0.15622035083006944, 0.14739999185667724, 0.14392552110580514, 0.14176441054821903, 0.1403260657901816, 0.13742304483000148, 0.13153298770684377, 0.12852559684754852, 0.12801346116586462, 0.12228737167888717, 0.12476525892156767, 0.11844669330048388, 0.11768221601279491, 0.11349400181323406, 0.1156830594998062, 0.10867378422226373, 0.10708388216641598, 0.10803296101731667, 0.10763276197513132, 0.1052474920113269, 0.1012978153612802, 0.10097600788338286, 0.10074356817173725, 0.0944806138376843, 0.09586451018111089, 0.08825819264672956, 0.09163382963240903, 0.08765413534874471, 0.08773916901881297, 0.08806112762437303, 0.0888870398757605, 0.08329685669061747, 0.08034497347037468, 0.08615488085058015, 0.08259301743714115, 0.08052687689671782, 0.07911245922163547, 0.07790332106881037, 0.07920863746679976, 0.07929184430342466, 0.07730453189319937, 0.07578221458783108, 0.07572205809363035, 0.07558220390201127, 0.07462377058098729, 0.06827301394515396, 0.07147669719380252, 0.07212053081595733, 0.07157112102935391, 0.06984002472787812, 0.06817790228783985, 0.06958462002371814, 0.06438470672809853, 0.06605702228532587, 0.0665811322907422, 0.0628026994444992, 0.06598548442314368, 0.06494546183221726, 0.06136110860511184, 0.0625501000212853, 0.06414394409733619, 0.05924170272638896, 0.06052110460237487, 0.05498680585090746, 0.06070368705322688], 'val_losses': [0.6534361257176399, 0.5255849991334947, 0.3564464575761005, 0.31412295094223164, 0.2699690473444135, 0.23840264349246051, 0.22639180040982354, 0.21351198202131863, 0.19635688482803923, 0.19352251184291372, 0.18333489807524966, 0.1833791041296536, 0.19039623059921434, 0.19334935064662923, 0.1714370765735774, 0.17582850591377006, 0.15835244048845648, 0.19018326037667357, 0.1560482464421738, 0.2337128388345458, 0.17845285681723175, 0.15118775849088573, 0.1802478969534034, 0.13890540041752947, 0.14300398293510594, 0.1773923986773789, 0.20722236211602213, 0.1401216346934361, 0.1571405171888721, 0.1409840888041868, 0.1604697131327169, 0.15041105081945147, 0.16453185553741012, 0.14461809804887346, 0.15140287566716742, 0.15009862897188767, 0.14636045404393755, 0.1335101248051137, 0.1439535454010817, 0.13618268516783338, 0.1458785552109054, 0.1346987815275779, 0.13520025219285178, 0.14341531099644, 0.1462021647013006, 0.13646930462882986, 0.13728073562144516, 0.1428526254442458, 0.12814262526709846, 0.12608064482750084, 0.13718214395641906, 0.14424986516700444, 0.1446399369356761, 0.13196076319982736, 0.14056135187724408, 0.14337374600622083, 0.13548057362140384, 0.14047171095942304, 0.12488586391822809, 0.12845776035080367, 0.1453435464754333, 0.13237482119086033, 0.14942520798791128, 0.13447877765822092, 0.1307680675643919, 0.1318797136313514, 0.13143356051260988, 0.13872386247961463, 0.13873871072217525, 0.15961680756693958, 0.14677923659366587, 0.1435056745561647, 0.1428432250072179, 0.14011378829386012, 0.13668926599225578, 0.13621900927346836, 0.1343551082089646, 0.14054904412489877, 0.1441278371681263, 0.14670396218389783, 0.1383952805458521], 'val_acc': [0.7907530384663576, 0.8160631499812053, 0.8833479513845383, 0.9142964540784363, 0.9239443678737, 0.9322140082696404, 0.936975316376394, 0.9382282921939606, 0.9429896003007142, 0.9453702543540909, 0.9468738253351711, 0.9476256108257111, 0.9434907906277409, 0.9437413857912542, 0.9485026938980078, 0.9480015035709811, 0.9517604310236812, 0.9419872196466608, 0.9531387044230046, 0.9350958526500438, 0.9468738253351711, 0.9535145971682747, 0.9442425761182809, 0.9573988222027315, 0.9557699536398947, 0.9485026938980078, 0.9376018042851773, 0.9573988222027315, 0.9548928705675981, 0.9580253101115148, 0.9538904899135446, 0.9546422754040848, 0.9535145971682747, 0.9597794762561083, 0.9540157874953014, 0.9530134068412479, 0.9567723342939481, 0.9586517980202982, 0.9595288810925949, 0.961408344818945, 0.9580253101115148, 0.9624107254729983, 0.9610324520736749, 0.9586517980202982, 0.961408344818945, 0.9587770956020549, 0.9611577496554317, 0.9582759052750282, 0.9624107254729983, 0.9636637012905651, 0.961784237564215, 0.9587770956020549, 0.961784237564215, 0.9637889988723217, 0.9581506076932715, 0.9611577496554317, 0.9620348327277284, 0.9622854278912417, 0.9661696529256986, 0.9649166771081318, 0.9610324520736749, 0.9637889988723217, 0.9581506076932715, 0.9620348327277284, 0.962536023054755, 0.9646660819446184, 0.9645407843628618, 0.9615336424007017, 0.9630372133817817, 0.9607818569101616, 0.9597794762561083, 0.9627866182182684, 0.9582759052750282, 0.9631625109635384, 0.9636637012905651, 0.9636637012905651, 0.9642901891993485, 0.9612830472371883, 0.9595288810925949, 0.9582759052750282, 0.9660443553439418], 'size_bytes': 74475, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 81, 'batch_size': 256, 'lr': 0.0015054448725399662, 'weight_decay': 6.827600167798092e-05, 'patch_size': 5, 'n_heads': 1, 'head_dim': 28, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.1416616771682256, 'attn_dropout': 0.06473486295278376, 'augment_noise_std': 0.0315070476565121, 'augment_scale_low': 0.8819959379098423, 'augment_scale_high': 1.1799151329208288, 'use_focal_loss': False, 'focal_gamma': 3.6104978133450656, 'class_weighted': False, 'grad_clip': 2.0212269322843226, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 12656, 'model_storage_size_kb': 13.5953125, 'model_size_validation': 'PASS'}
2025-10-02 21:06:37,229 - INFO - _models.training_function_executor - BO Objective: base=0.9660, size_penalty=0.0000, final=0.9660
2025-10-02 21:06:37,229 - INFO - _models.training_function_executor - Model: 12,656 parameters, 13.6KB (PASS 256KB limit)
2025-10-02 21:06:37,229 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 187.223s
2025-10-02 21:06:37,340 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9660
2025-10-02 21:06:37,340 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-10-02 21:06:37,340 - INFO - bo.run_bo - Recorded observation #33: hparams={'epochs': np.int64(81), 'batch_size': np.int64(256), 'lr': 0.0015054448725399662, 'weight_decay': 6.827600167798092e-05, 'patch_size': np.int64(5), 'n_heads': np.int64(1), 'head_dim': np.int64(28), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.1416616771682256, 'attn_dropout': 0.06473486295278376, 'augment_noise_std': 0.0315070476565121, 'augment_scale_low': 0.8819959379098423, 'augment_scale_high': 1.1799151329208288, 'use_focal_loss': np.False_, 'focal_gamma': 3.6104978133450656, 'class_weighted': np.False_, 'grad_clip': 2.0212269322843226, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9660
2025-10-02 21:06:37,340 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 33: {'epochs': np.int64(81), 'batch_size': np.int64(256), 'lr': 0.0015054448725399662, 'weight_decay': 6.827600167798092e-05, 'patch_size': np.int64(5), 'n_heads': np.int64(1), 'head_dim': np.int64(28), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.1416616771682256, 'attn_dropout': 0.06473486295278376, 'augment_noise_std': 0.0315070476565121, 'augment_scale_low': 0.8819959379098423, 'augment_scale_high': 1.1799151329208288, 'use_focal_loss': np.False_, 'focal_gamma': 3.6104978133450656, 'class_weighted': np.False_, 'grad_clip': 2.0212269322843226, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9660
2025-10-02 21:06:37,340 - INFO - bo.run_bo - üîçBO Trial 34: Using RF surrogate + Expected Improvement
2025-10-02 21:06:37,340 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-02 21:06:37,341 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:06:37,341 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:06:37,341 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 85, 'batch_size': 128, 'lr': 0.00025625383843802546, 'weight_decay': 0.0001918411010858463, 'patch_size': 25, 'n_heads': 1, 'head_dim': 31, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.014597955005731803, 'attn_dropout': 0.271187628663866, 'augment_noise_std': 0.03650288661478387, 'augment_scale_low': 0.8001864296153092, 'augment_scale_high': 1.1986169564195175, 'use_focal_loss': True, 'focal_gamma': 4.424690319590942, 'class_weighted': False, 'grad_clip': 2.375764107461036, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:06:37,342 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 85, 'batch_size': 128, 'lr': 0.00025625383843802546, 'weight_decay': 0.0001918411010858463, 'patch_size': 25, 'n_heads': 1, 'head_dim': 31, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.014597955005731803, 'attn_dropout': 0.271187628663866, 'augment_noise_std': 0.03650288661478387, 'augment_scale_low': 0.8001864296153092, 'augment_scale_high': 1.1986169564195175, 'use_focal_loss': True, 'focal_gamma': 4.424690319590942, 'class_weighted': False, 'grad_clip': 2.375764107461036, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:08:29,652 - INFO - _models.training_function_executor - Model: 15,128 parameters, 16.3KB storage
2025-10-02 21:08:29,653 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.21522688459932546, 0.12791082043954238, 0.09736648229015944, 0.08381630389103109, 0.07432225309724282, 0.0665854755427867, 0.06130525061483577, 0.05672310431349953, 0.05203602398310643, 0.04845006249165156, 0.045624024345660964, 0.042636977251290835, 0.04046640089466429, 0.03820074984783783, 0.03619651166630047, 0.035191439816317054, 0.03402609912345957, 0.0325022565267996, 0.03062608463206738, 0.029846370027052243, 0.028209528392623955, 0.02711218981637641, 0.025930416841434867, 0.026468435178299596, 0.025417007619964165, 0.02454266061168651, 0.023677072451974975, 0.02314257168587417, 0.021858468526381562, 0.021153312744075926, 0.02113245680168072, 0.020294870188865798, 0.0200399626298191, 0.01876466320439297, 0.019035146160546194, 0.017784605002919895, 0.017857419785517196, 0.0175151017512325, 0.017298233921120645, 0.016758620875430095, 0.016530305265311258, 0.016361447489069685, 0.01628078450163635, 0.01587783648704585, 0.015295375790269036, 0.015370872640212134, 0.014774012940306535, 0.01457945736127387, 0.014022895552319952, 0.013609551663851225, 0.014083730964661036, 0.01376810785689255, 0.013382089268223034, 0.013390976593530396, 0.01300011771828724, 0.011939851842814576, 0.01249811530418789, 0.012753690269550495, 0.012240446892582179, 0.011477745776553213, 0.011686704807805362, 0.011395057520864704, 0.012206758513425378, 0.011362818432768491, 0.010850559250674427, 0.010664518639987708, 0.011063413201328524, 0.010971821766070474, 0.00993765137155621, 0.010569965958847806, 0.010582259731299604, 0.009813389537016179, 0.010196905864847672, 0.0090849838535742, 0.009432001009657458, 0.009285670273509856, 0.009686984205528937, 0.008947728999240944, 0.0085953390478634, 0.009385308910176311, 0.008122226889830298, 0.00875245863114606, 0.008307088711985905, 0.008229365598371159, 0.008247560857357656], 'val_losses': [0.14804494743631114, 0.10606159463799279, 0.08892553942407556, 0.07562128684424306, 0.06745594216851629, 0.06028872051508134, 0.055997691320622045, 0.05164481573672226, 0.04797264832390414, 0.046512888930948015, 0.04627900425885692, 0.04217081742944417, 0.0397302960808221, 0.03869636244432413, 0.04031482817717863, 0.03955556484808182, 0.037964997098247535, 0.03452850794593518, 0.03539447141177612, 0.03511916908770781, 0.03661829600797502, 0.035579773709560725, 0.03669366338896492, 0.03356610782899111, 0.03200747148966882, 0.030923204437456487, 0.030435581624231576, 0.029404729910922103, 0.02875563066722068, 0.029112877445350297, 0.02940844936753078, 0.028416974960361083, 0.029574395976461214, 0.02755927852656127, 0.028253670551733272, 0.028040866263079143, 0.02562412838879242, 0.028569520859435423, 0.029206050890584798, 0.029297519187224626, 0.025495594236922735, 0.024800973514120854, 0.025226056923051927, 0.026155379728601175, 0.025169251672666842, 0.02462982065493026, 0.0265688223899897, 0.027788494685481564, 0.02678686141680243, 0.02584539908370104, 0.024466599426559602, 0.025275850019506038, 0.02691031924100347, 0.023680878361803055, 0.02338337900588244, 0.026067209270286943, 0.025993788494209108, 0.02392224823434525, 0.027111321889342947, 0.025193864744552888, 0.02323913529315489, 0.024005219404738643, 0.024135017434975033, 0.023802295504559135, 0.022835546297666168, 0.022883084217918917, 0.022068911411643938, 0.021727563856971725, 0.02484290674655543, 0.023721665065549336, 0.024735093367882443, 0.024554576166977694, 0.024248539346558844, 0.025547127770706907, 0.024495207437027846, 0.024307134142198025, 0.022775791131745094, 0.02330895862626695, 0.023738226819353555, 0.023609287384279, 0.023975038215519866, 0.025120245491393363, 0.022786841738412233, 0.022442880708454935, 0.025078609421459033], 'val_acc': [0.7814810174163639, 0.8381155243703796, 0.8673098609196842, 0.8882345570730484, 0.8958777095602055, 0.9070291943365493, 0.9110387169527628, 0.9150482395689763, 0.92281668963789, 0.9196842500939731, 0.9205613331662699, 0.9278285929081569, 0.9353464478135572, 0.9339681744142339, 0.9294574614709936, 0.9327151985966671, 0.9236937727101867, 0.937727101866934, 0.939230672848014, 0.9334669840872071, 0.9349705550682872, 0.9345946623230171, 0.9261997243453202, 0.938854780102744, 0.9349705550682872, 0.945746147099361, 0.9417366244831474, 0.9463726350081444, 0.9496303721338178, 0.9439919809547676, 0.9383535897757174, 0.9491291818067912, 0.9453702543540909, 0.9443678737000376, 0.9456208495176043, 0.9441172785365243, 0.9508833479513845, 0.9453702543540909, 0.9357223405588272, 0.9371006139581506, 0.9517604310236812, 0.9528881092594913, 0.9476256108257111, 0.9510086455331412, 0.9503821576243578, 0.9505074552061146, 0.9416113269013908, 0.931587520360857, 0.9446184688635509, 0.9463726350081444, 0.9522616213507079, 0.9432401954642275, 0.9462473374263877, 0.9521363237689513, 0.9543916802405713, 0.9496303721338178, 0.938478887357474, 0.9568976318757048, 0.9360982333040972, 0.9480015035709811, 0.9563964415486781, 0.9478762059892244, 0.946497932589901, 0.9480015035709811, 0.953389299586518, 0.9497556697155745, 0.9540157874953014, 0.9531387044230046, 0.9515098358601679, 0.9498809672973312, 0.954141085077058, 0.9478762059892244, 0.946122039844631, 0.9456208495176043, 0.9505074552061146, 0.9486279914797644, 0.9511339431148979, 0.9567723342939481, 0.9503821576243578, 0.9429896003007142, 0.9469991229169277, 0.936975316376394, 0.953765192331788, 0.9502568600426011, 0.9447437664453076], 'size_bytes': 91771, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 85, 'batch_size': 128, 'lr': 0.00025625383843802546, 'weight_decay': 0.0001918411010858463, 'patch_size': 25, 'n_heads': 1, 'head_dim': 31, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.014597955005731803, 'attn_dropout': 0.271187628663866, 'augment_noise_std': 0.03650288661478387, 'augment_scale_low': 0.8001864296153092, 'augment_scale_high': 1.1986169564195175, 'use_focal_loss': True, 'focal_gamma': 4.424690319590942, 'class_weighted': False, 'grad_clip': 2.375764107461036, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 15128, 'model_storage_size_kb': 16.250781250000003, 'model_size_validation': 'PASS'}
2025-10-02 21:08:29,653 - INFO - _models.training_function_executor - BO Objective: base=0.9447, size_penalty=0.0000, final=0.9447
2025-10-02 21:08:29,653 - INFO - _models.training_function_executor - Model: 15,128 parameters, 16.3KB (PASS 256KB limit)
2025-10-02 21:08:29,653 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 112.312s
2025-10-02 21:08:29,768 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9447
2025-10-02 21:08:29,768 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.115s
2025-10-02 21:08:29,768 - INFO - bo.run_bo - Recorded observation #34: hparams={'epochs': np.int64(85), 'batch_size': np.int64(128), 'lr': 0.00025625383843802546, 'weight_decay': 0.0001918411010858463, 'patch_size': np.int64(25), 'n_heads': np.int64(1), 'head_dim': np.int64(31), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.014597955005731803, 'attn_dropout': 0.271187628663866, 'augment_noise_std': 0.03650288661478387, 'augment_scale_low': 0.8001864296153092, 'augment_scale_high': 1.1986169564195175, 'use_focal_loss': np.True_, 'focal_gamma': 4.424690319590942, 'class_weighted': np.False_, 'grad_clip': 2.375764107461036, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9447
2025-10-02 21:08:29,768 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 34: {'epochs': np.int64(85), 'batch_size': np.int64(128), 'lr': 0.00025625383843802546, 'weight_decay': 0.0001918411010858463, 'patch_size': np.int64(25), 'n_heads': np.int64(1), 'head_dim': np.int64(31), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.014597955005731803, 'attn_dropout': 0.271187628663866, 'augment_noise_std': 0.03650288661478387, 'augment_scale_low': 0.8001864296153092, 'augment_scale_high': 1.1986169564195175, 'use_focal_loss': np.True_, 'focal_gamma': 4.424690319590942, 'class_weighted': np.False_, 'grad_clip': 2.375764107461036, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9447
2025-10-02 21:08:29,769 - INFO - bo.run_bo - üîçBO Trial 35: Using RF surrogate + Expected Improvement
2025-10-02 21:08:29,769 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:08:29,769 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:08:29,769 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:08:29,769 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 62, 'batch_size': 128, 'lr': 2.337322367385048e-05, 'weight_decay': 1.274469052850468e-05, 'patch_size': 40, 'n_heads': 3, 'head_dim': 29, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.09453284467855169, 'attn_dropout': 0.01204219277577593, 'augment_noise_std': 0.03740461635564207, 'augment_scale_low': 0.8331062423902127, 'augment_scale_high': 1.1213985373271855, 'use_focal_loss': False, 'focal_gamma': 3.974890025173333, 'class_weighted': False, 'grad_clip': 3.168131174229874, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:08:29,770 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 62, 'batch_size': 128, 'lr': 2.337322367385048e-05, 'weight_decay': 1.274469052850468e-05, 'patch_size': 40, 'n_heads': 3, 'head_dim': 29, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.09453284467855169, 'attn_dropout': 0.01204219277577593, 'augment_noise_std': 0.03740461635564207, 'augment_scale_low': 0.8331062423902127, 'augment_scale_high': 1.1213985373271855, 'use_focal_loss': False, 'focal_gamma': 3.974890025173333, 'class_weighted': False, 'grad_clip': 3.168131174229874, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:10:27,233 - INFO - _models.training_function_executor - Model: 133,197 parameters, 572.3KB storage
2025-10-02 21:10:27,233 - WARNING - _models.training_function_executor - Model storage 572.3KB exceeds 256KB limit!
2025-10-02 21:10:27,233 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9723687977027248, 0.6734761971618225, 0.5756013677487138, 0.49964650710625436, 0.42873120599382597, 0.37796172299126063, 0.3424681304416811, 0.31533905914734767, 0.2964176989806362, 0.28077244964691306, 0.267034622153214, 0.2562667737167846, 0.24710033528283398, 0.23864502285677205, 0.23177774855109962, 0.2251815777922567, 0.22025868023600984, 0.21340606628091505, 0.2081983460120583, 0.20436242700493193, 0.19943736016860497, 0.19468494021078103, 0.19251946449565457, 0.1879779101258303, 0.18469282654016772, 0.18130736994307556, 0.17734576725920195, 0.17519412158237446, 0.17187906419147364, 0.16908271531383484, 0.16464296960294447, 0.16303490636479276, 0.16148896067839788, 0.15755280935498805, 0.15582971046921887, 0.15308289352587484, 0.1508677677554721, 0.15003414717256225, 0.1464608520492517, 0.14482416748754087, 0.14383734134049941, 0.14017864517489237, 0.13991366497325303, 0.13704737994243518, 0.1354368208375031, 0.13362488788457622, 0.1330551733998752, 0.12993888193145414, 0.12862936514618067, 0.12641705994718344, 0.12635117998992657, 0.12459446012010342, 0.1234997871767118, 0.12095120940647437, 0.11944080645481817, 0.11900377136920082, 0.11841644595393455, 0.11507452866007382, 0.11393373321794449, 0.11386156344906308, 0.1117573712879179, 0.11016948146641878], 'val_losses': [0.7609773964724166, 0.612929818516252, 0.5408739557602011, 0.45790068721572585, 0.3969037354705687, 0.35901492572139937, 0.3293877769830591, 0.3039035142147665, 0.2854353673994175, 0.27126025310764784, 0.2608245849896905, 0.25248934379900745, 0.2450280560941613, 0.24256153910317796, 0.23210133852542394, 0.2269416053112759, 0.21875198876636695, 0.21445077906021154, 0.20944937204064812, 0.20500428790161, 0.2008328293844014, 0.2020687952930804, 0.19897739212717752, 0.19150465253267712, 0.18948287207643993, 0.18894569114057838, 0.1842594906349795, 0.1807364590793695, 0.17818509224116572, 0.1755928800159746, 0.1752433563354932, 0.170726555191534, 0.1675053704508996, 0.16897194343015165, 0.16581904861827257, 0.1609037517306351, 0.16623590861200585, 0.16905676490709964, 0.1599950092677678, 0.1547828101143549, 0.15611787625810075, 0.15474011792625464, 0.15266210833687394, 0.14842718510852693, 0.15116215485679432, 0.15138921682890788, 0.14559251243474264, 0.14711527562880036, 0.14369358430136836, 0.14531704817793897, 0.14418451369178428, 0.1447370022202984, 0.14200498130839506, 0.14340423125959909, 0.13939660944790488, 0.1433300441435563, 0.14131883216925903, 0.1383637671431583, 0.1361594406941334, 0.13706250267135756, 0.13738607026075544, 0.13871181455004739], 'val_acc': [0.7475253727603057, 0.7936348828467611, 0.8149354717453953, 0.8517729607818569, 0.8772083698784613, 0.8897381280541286, 0.8966294950507455, 0.9036461596291192, 0.9124169903520862, 0.9188071670216765, 0.9218143089838366, 0.9255732364365368, 0.9260744267635634, 0.9285803783986969, 0.9310863300338303, 0.9335922816689638, 0.9352211502318005, 0.9363488284676106, 0.9393559704297707, 0.9407342438290941, 0.9407342438290941, 0.9397318631750408, 0.9414860293196341, 0.9441172785365243, 0.9427390051372009, 0.9429896003007142, 0.945746147099361, 0.9466232301716577, 0.9463726350081444, 0.9473750156621977, 0.946497932589901, 0.9488785866432777, 0.9482520987344945, 0.9485026938980078, 0.9480015035709811, 0.9528881092594913, 0.9496303721338178, 0.9477509084074678, 0.9528881092594913, 0.9542663826588147, 0.9522616213507079, 0.9543916802405713, 0.9543916802405713, 0.9551434657311114, 0.9568976318757048, 0.9561458463851648, 0.9571482270392181, 0.9551434657311114, 0.9579000125297582, 0.9567723342939481, 0.9556446560581381, 0.9551434657311114, 0.9575241197844881, 0.9572735246209748, 0.9582759052750282, 0.9562711439669215, 0.9582759052750282, 0.9566470367121914, 0.9577747149480015, 0.9585265004385415, 0.9602806665831349, 0.9587770956020549], 'size_bytes': 744651, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 62, 'batch_size': 128, 'lr': 2.337322367385048e-05, 'weight_decay': 1.274469052850468e-05, 'patch_size': 40, 'n_heads': 3, 'head_dim': 29, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.09453284467855169, 'attn_dropout': 0.01204219277577593, 'augment_noise_std': 0.03740461635564207, 'augment_scale_low': 0.8331062423902127, 'augment_scale_high': 1.1213985373271855, 'use_focal_loss': False, 'focal_gamma': 3.974890025173333, 'class_weighted': False, 'grad_clip': 3.168131174229874, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 133197, 'model_storage_size_kb': 572.330859375, 'model_size_validation': 'FAIL'}
2025-10-02 21:10:27,233 - INFO - _models.training_function_executor - BO Objective: base=0.9588, size_penalty=0.6178, final=0.3409
2025-10-02 21:10:27,233 - INFO - _models.training_function_executor - Model: 133,197 parameters, 572.3KB (FAIL 256KB limit)
2025-10-02 21:10:27,233 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 117.464s
2025-10-02 21:10:27,344 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.3409
2025-10-02 21:10:27,344 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-10-02 21:10:27,345 - INFO - bo.run_bo - Recorded observation #35: hparams={'epochs': np.int64(62), 'batch_size': np.int64(128), 'lr': 2.337322367385048e-05, 'weight_decay': 1.274469052850468e-05, 'patch_size': np.int64(40), 'n_heads': np.int64(3), 'head_dim': np.int64(29), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.09453284467855169, 'attn_dropout': 0.01204219277577593, 'augment_noise_std': 0.03740461635564207, 'augment_scale_low': 0.8331062423902127, 'augment_scale_high': 1.1213985373271855, 'use_focal_loss': np.False_, 'focal_gamma': 3.974890025173333, 'class_weighted': np.False_, 'grad_clip': 3.168131174229874, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.3409
2025-10-02 21:10:27,345 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 35: {'epochs': np.int64(62), 'batch_size': np.int64(128), 'lr': 2.337322367385048e-05, 'weight_decay': 1.274469052850468e-05, 'patch_size': np.int64(40), 'n_heads': np.int64(3), 'head_dim': np.int64(29), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.09453284467855169, 'attn_dropout': 0.01204219277577593, 'augment_noise_std': 0.03740461635564207, 'augment_scale_low': 0.8331062423902127, 'augment_scale_high': 1.1213985373271855, 'use_focal_loss': np.False_, 'focal_gamma': 3.974890025173333, 'class_weighted': np.False_, 'grad_clip': 3.168131174229874, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.3409
2025-10-02 21:10:27,345 - INFO - bo.run_bo - üîçBO Trial 36: Using RF surrogate + Expected Improvement
2025-10-02 21:10:27,345 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:10:27,345 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:10:27,345 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:10:27,345 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 96, 'batch_size': 64, 'lr': 3.607351122892423e-05, 'weight_decay': 8.11640122319438e-05, 'patch_size': 25, 'n_heads': 1, 'head_dim': 32, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.05399246409678578, 'attn_dropout': 0.1778698855864461, 'augment_noise_std': 0.019959733046464555, 'augment_scale_low': 0.8385945439578247, 'augment_scale_high': 1.1734569298067927, 'use_focal_loss': False, 'focal_gamma': 1.97262615029951, 'class_weighted': False, 'grad_clip': 0.04261900448122192, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:10:27,346 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 96, 'batch_size': 64, 'lr': 3.607351122892423e-05, 'weight_decay': 8.11640122319438e-05, 'patch_size': 25, 'n_heads': 1, 'head_dim': 32, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.05399246409678578, 'attn_dropout': 0.1778698855864461, 'augment_noise_std': 0.019959733046464555, 'augment_scale_low': 0.8385945439578247, 'augment_scale_high': 1.1734569298067927, 'use_focal_loss': False, 'focal_gamma': 1.97262615029951, 'class_weighted': False, 'grad_clip': 0.04261900448122192, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:13:06,620 - INFO - _models.training_function_executor - Model: 28,741 parameters, 123.5KB storage
2025-10-02 21:13:06,620 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0141090527963388, 0.7599663806563575, 0.6725153381340606, 0.6237723776613251, 0.5754653679000007, 0.5291112759741193, 0.4967756589606633, 0.4726307705855551, 0.44946558058637165, 0.4280294932811753, 0.4144239602533158, 0.3997820035125873, 0.38819775976296883, 0.37782868192230473, 0.3715261709543337, 0.3639104031748712, 0.3543291009419455, 0.3473732540036706, 0.3416374669968475, 0.3348579143929279, 0.3293944209342853, 0.3246333053223461, 0.3180843813232983, 0.31259096930511193, 0.3067107710136498, 0.3009279862345451, 0.2970298138262055, 0.29038195225247926, 0.2882693578844377, 0.28190019428851376, 0.278221524595333, 0.27570021972916753, 0.2700228724749234, 0.2674498283267077, 0.262653457307354, 0.26061807323065267, 0.2577868166425113, 0.25449095889578205, 0.252159036498274, 0.24791156856991223, 0.24682321871206045, 0.244217287717194, 0.24058171691398023, 0.23911306825611886, 0.2373363821976244, 0.23380594696910617, 0.23280782730654137, 0.22904799700052192, 0.22669735674858393, 0.22295951141684633, 0.22162671086306307, 0.2202845289922801, 0.2205542826221778, 0.2171605602856683, 0.2154599312143592, 0.2144237610662924, 0.2117968105260275, 0.20936536809759018, 0.20827694849373385, 0.20767248363442667, 0.2072082234133613, 0.2034751175191025, 0.2003151699571544, 0.20081177076588963, 0.19760756585943637, 0.19827706667503325, 0.1949285607314351, 0.19542040696627094, 0.19390732239876624, 0.190751608770503, 0.19154106449945318, 0.18956779273298566, 0.18855354925548434, 0.18544097003411952, 0.18760471463673015, 0.18670784258405185, 0.18436979063148617, 0.18366174009154138, 0.18146314148641793, 0.1800658944177922, 0.17920213798470908, 0.1785238809318552, 0.17837299602904708, 0.17656689277045676, 0.17671100893713862, 0.17451132863598487, 0.17494753073418165, 0.17318592947725028, 0.17045423919635208, 0.17195594402577008, 0.17089516584393644, 0.16905489125911394, 0.168102511813612, 0.16570921427410498, 0.16829078919835244, 0.16737839602734725], 'val_losses': [0.8181353526117807, 0.7088941322608544, 0.6435175318345138, 0.6029815059932397, 0.5485536570821455, 0.5107558015264854, 0.48338264061303504, 0.45745892006828737, 0.4338039042223457, 0.4122983923443516, 0.4005836718089672, 0.38684558185430534, 0.3762846230095736, 0.3728611868870481, 0.36539415363724137, 0.3551422896566374, 0.3475189994539388, 0.3400877005077666, 0.33679598831447827, 0.33175772789419034, 0.3271778465799783, 0.3185214014720863, 0.3123968700267904, 0.3132659689743945, 0.30729780723305217, 0.3014565806589902, 0.2940255427725362, 0.2917513432084595, 0.28573564202146057, 0.28410294657773116, 0.28301055691580834, 0.2763341259336863, 0.26905086496512315, 0.26988847882989475, 0.26450699483309453, 0.2672810671122178, 0.26222878065888644, 0.26281521745775743, 0.25232190299532253, 0.2565742391416594, 0.24954933242688215, 0.2525907213585058, 0.2550617737730349, 0.2417996898928302, 0.24546466600030845, 0.23651375071795688, 0.23318093774361562, 0.23696396169399142, 0.2285617922197352, 0.23171773876086046, 0.23307213099091562, 0.22652228656899973, 0.2256188069419699, 0.22415972723688732, 0.22087732077488817, 0.21557711161436882, 0.21448022120056587, 0.2156623362740573, 0.21428521379852725, 0.21295735010331832, 0.21247439687263397, 0.20804089292288275, 0.21242565612004463, 0.21261887423541967, 0.2128173914938594, 0.20868690844399784, 0.2015928830477681, 0.20290347865714178, 0.20067498407433645, 0.20018259808467812, 0.2019288532399486, 0.20347050432236688, 0.20044824290661686, 0.2022421792272771, 0.1970822900718517, 0.1972795269926857, 0.19456491242758597, 0.19860660673576913, 0.19108761670766894, 0.1910689327249236, 0.19001673494510343, 0.1930198005976242, 0.19121756456263037, 0.18777390217784054, 0.1904444450258372, 0.18608834571056296, 0.18776421632504287, 0.18639562522919909, 0.1843566682244315, 0.1814740953236247, 0.18138731394722532, 0.18338005827727466, 0.18465694399379487, 0.1825751224293504, 0.18228881677586756, 0.18342746432383786], 'val_acc': [0.7190828217015411, 0.7534143591028694, 0.7827339932339306, 0.806289938604185, 0.8329783235183561, 0.8450068913669966, 0.8527753414359103, 0.855155995489287, 0.8646786117027941, 0.8724470617717078, 0.8797143215135947, 0.8837238441298083, 0.8886104498183185, 0.886104498183185, 0.8908658062899386, 0.8943741385791254, 0.8985089587770956, 0.9003884225034456, 0.9005137200852024, 0.9031449693020924, 0.9031449693020924, 0.9051497306101992, 0.9089086580628993, 0.9084074677358727, 0.9102869314622227, 0.9129181806791129, 0.9146723468237064, 0.9150482395689763, 0.9171782984588397, 0.9165518105500564, 0.9156747274777597, 0.9184312742764065, 0.9214384162385666, 0.9209372259115399, 0.92356847512843, 0.9219396065655933, 0.92469615336424, 0.9243202606189701, 0.9284550808169403, 0.9273274025811302, 0.9293321638892369, 0.9279538904899135, 0.9234431775466734, 0.9314622227791004, 0.931587520360857, 0.9320887106878837, 0.9325899010149105, 0.9309610324520736, 0.9354717453953139, 0.9324646034331537, 0.9309610324520736, 0.9355970429770706, 0.9353464478135572, 0.9334669840872071, 0.9353464478135572, 0.9378523994486906, 0.939230672848014, 0.9402330535020674, 0.9389800776845006, 0.9374765067034206, 0.9391053752662574, 0.9391053752662574, 0.9372259115399073, 0.9406089462473374, 0.936975316376394, 0.9379776970304473, 0.9429896003007142, 0.9408595414108508, 0.9439919809547676, 0.9404836486655808, 0.940358351083824, 0.9422378148101742, 0.9427390051372009, 0.939230672848014, 0.9416113269013908, 0.9412354341561208, 0.9434907906277409, 0.9409848389926074, 0.9428643027189575, 0.9432401954642275, 0.9438666833730109, 0.9428643027189575, 0.9443678737000376, 0.9442425761182809, 0.9454955519358477, 0.945746147099361, 0.945746147099361, 0.9456208495176043, 0.9467485277534143, 0.9468738253351711, 0.9480015035709811, 0.9462473374263877, 0.9454955519358477, 0.947249718080441, 0.9471244204986844, 0.9447437664453076], 'size_bytes': 129317, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 96, 'batch_size': 64, 'lr': 3.607351122892423e-05, 'weight_decay': 8.11640122319438e-05, 'patch_size': 25, 'n_heads': 1, 'head_dim': 32, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.05399246409678578, 'attn_dropout': 0.1778698855864461, 'augment_noise_std': 0.019959733046464555, 'augment_scale_low': 0.8385945439578247, 'augment_scale_high': 1.1734569298067927, 'use_focal_loss': False, 'focal_gamma': 1.97262615029951, 'class_weighted': False, 'grad_clip': 0.04261900448122192, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 28741, 'model_storage_size_kb': 123.49648437500001, 'model_size_validation': 'PASS'}
2025-10-02 21:13:06,620 - INFO - _models.training_function_executor - BO Objective: base=0.9447, size_penalty=0.0000, final=0.9447
2025-10-02 21:13:06,620 - INFO - _models.training_function_executor - Model: 28,741 parameters, 123.5KB (PASS 256KB limit)
2025-10-02 21:13:06,620 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 159.275s
2025-10-02 21:13:06,732 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9447
2025-10-02 21:13:06,732 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.111s
2025-10-02 21:13:06,732 - INFO - bo.run_bo - Recorded observation #36: hparams={'epochs': np.int64(96), 'batch_size': np.int64(64), 'lr': 3.607351122892423e-05, 'weight_decay': 8.11640122319438e-05, 'patch_size': np.int64(25), 'n_heads': np.int64(1), 'head_dim': np.int64(32), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.05399246409678578, 'attn_dropout': 0.1778698855864461, 'augment_noise_std': 0.019959733046464555, 'augment_scale_low': 0.8385945439578247, 'augment_scale_high': 1.1734569298067927, 'use_focal_loss': np.False_, 'focal_gamma': 1.97262615029951, 'class_weighted': np.False_, 'grad_clip': 0.04261900448122192, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9447
2025-10-02 21:13:06,732 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 36: {'epochs': np.int64(96), 'batch_size': np.int64(64), 'lr': 3.607351122892423e-05, 'weight_decay': 8.11640122319438e-05, 'patch_size': np.int64(25), 'n_heads': np.int64(1), 'head_dim': np.int64(32), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.05399246409678578, 'attn_dropout': 0.1778698855864461, 'augment_noise_std': 0.019959733046464555, 'augment_scale_low': 0.8385945439578247, 'augment_scale_high': 1.1734569298067927, 'use_focal_loss': np.False_, 'focal_gamma': 1.97262615029951, 'class_weighted': np.False_, 'grad_clip': 0.04261900448122192, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9447
2025-10-02 21:13:06,733 - INFO - bo.run_bo - üîçBO Trial 37: Using RF surrogate + Expected Improvement
2025-10-02 21:13:06,733 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:13:06,733 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:13:06,733 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:13:06,733 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 39, 'batch_size': 256, 'lr': 0.0006016871537512024, 'weight_decay': 1.8610885639584018e-05, 'patch_size': 100, 'n_heads': 2, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.2079246267965562, 'attn_dropout': 0.08241873053328731, 'augment_noise_std': 0.049234117185589975, 'augment_scale_low': 0.8049704509172394, 'augment_scale_high': 1.0857832237962126, 'use_focal_loss': False, 'focal_gamma': 4.7810467448854785, 'class_weighted': False, 'grad_clip': 0.2031284053404986, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:13:06,734 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 39, 'batch_size': 256, 'lr': 0.0006016871537512024, 'weight_decay': 1.8610885639584018e-05, 'patch_size': 100, 'n_heads': 2, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.2079246267965562, 'attn_dropout': 0.08241873053328731, 'augment_noise_std': 0.049234117185589975, 'augment_scale_low': 0.8049704509172394, 'augment_scale_high': 1.0857832237962126, 'use_focal_loss': False, 'focal_gamma': 4.7810467448854785, 'class_weighted': False, 'grad_clip': 0.2031284053404986, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:13:44,027 - INFO - _models.training_function_executor - Model: 72,240 parameters, 310.4KB storage
2025-10-02 21:13:44,027 - WARNING - _models.training_function_executor - Model storage 310.4KB exceeds 256KB limit!
2025-10-02 21:13:44,027 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6643978734075495, 0.37748485474427707, 0.2837251545579251, 0.23771447849125057, 0.20865581682708012, 0.18859725713373235, 0.17107057448973562, 0.16248139177494822, 0.15211339582426336, 0.14189424788035482, 0.13457225931647487, 0.1281116187975781, 0.12407605367872054, 0.12054929133542322, 0.11400368106185772, 0.109603887327657, 0.10618989642403633, 0.10125025572646244, 0.09712617549662138, 0.09464294951395914, 0.08671095890670112, 0.08753128803588915, 0.08325882733714941, 0.08065048573345587, 0.07998742599111337, 0.0746970992862452, 0.07318415052245196, 0.06961656068444264, 0.06918374497396876, 0.06575907230390267, 0.065196617644, 0.06231603970041596, 0.06037519360849501, 0.058517672729044824, 0.05674220041886191, 0.05755792463797714, 0.05595729851193944, 0.054993770515037815, 0.05276967406541756], 'val_losses': [0.46168624068349096, 0.29138902465366745, 0.22324756660989137, 0.20399326548333185, 0.18734236075762833, 0.1783758626139645, 0.166950827677075, 0.15333563406960946, 0.14600035950005644, 0.14103405534672178, 0.13953761096220837, 0.14911503359804235, 0.1418531299974219, 0.1258814951677756, 0.12140465719615318, 0.1333385246244906, 0.143971434411265, 0.12109888659512663, 0.12605148760907378, 0.11050423873148979, 0.11527866119878989, 0.11619735951001536, 0.1318012443750951, 0.1082147473347783, 0.10678068561230428, 0.10854720177432078, 0.11655758874874309, 0.12514825485009118, 0.1131335342793727, 0.11215669130669577, 0.11029792886683404, 0.13367377614977963, 0.11558514733934983, 0.12272170552643838, 0.11287831516847502, 0.11527566092718725, 0.11332327549790039, 0.1214707286629577, 0.13950316470721777], 'val_acc': [0.8487658188196968, 0.9126675855155996, 0.9330910913419371, 0.9382282921939606, 0.9433654930459842, 0.945746147099361, 0.9490038842250345, 0.9543916802405713, 0.9558952512216514, 0.9562711439669215, 0.955268763312868, 0.9500062648790878, 0.9557699536398947, 0.9596541786743515, 0.9612830472371883, 0.9584012028567849, 0.9527628116777346, 0.9620348327277284, 0.9612830472371883, 0.9651672722716451, 0.9626613206365117, 0.9641648916175918, 0.9575241197844881, 0.9664202480892119, 0.9669214384162386, 0.9670467359979953, 0.9645407843628618, 0.961784237564215, 0.9676732239067786, 0.9671720335797519, 0.9689261997243453, 0.9647913795263752, 0.9639142964540784, 0.9672973311615086, 0.9698032827966421, 0.9659190577621852, 0.9672973311615086, 0.9664202480892119, 0.9605312617466483], 'size_bytes': 403211, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 39, 'batch_size': 256, 'lr': 0.0006016871537512024, 'weight_decay': 1.8610885639584018e-05, 'patch_size': 100, 'n_heads': 2, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.2079246267965562, 'attn_dropout': 0.08241873053328731, 'augment_noise_std': 0.049234117185589975, 'augment_scale_low': 0.8049704509172394, 'augment_scale_high': 1.0857832237962126, 'use_focal_loss': False, 'focal_gamma': 4.7810467448854785, 'class_weighted': False, 'grad_clip': 0.2031284053404986, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 72240, 'model_storage_size_kb': 310.40625, 'model_size_validation': 'FAIL'}
2025-10-02 21:13:44,027 - INFO - _models.training_function_executor - BO Objective: base=0.9605, size_penalty=0.1063, final=0.8543
2025-10-02 21:13:44,027 - INFO - _models.training_function_executor - Model: 72,240 parameters, 310.4KB (FAIL 256KB limit)
2025-10-02 21:13:44,027 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 37.295s
2025-10-02 21:13:44,141 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8543
2025-10-02 21:13:44,141 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.113s
2025-10-02 21:13:44,141 - INFO - bo.run_bo - Recorded observation #37: hparams={'epochs': np.int64(39), 'batch_size': np.int64(256), 'lr': 0.0006016871537512024, 'weight_decay': 1.8610885639584018e-05, 'patch_size': np.int64(100), 'n_heads': np.int64(2), 'head_dim': np.int64(30), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.2079246267965562, 'attn_dropout': 0.08241873053328731, 'augment_noise_std': 0.049234117185589975, 'augment_scale_low': 0.8049704509172394, 'augment_scale_high': 1.0857832237962126, 'use_focal_loss': np.False_, 'focal_gamma': 4.7810467448854785, 'class_weighted': np.False_, 'grad_clip': 0.2031284053404986, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.8543
2025-10-02 21:13:44,141 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 37: {'epochs': np.int64(39), 'batch_size': np.int64(256), 'lr': 0.0006016871537512024, 'weight_decay': 1.8610885639584018e-05, 'patch_size': np.int64(100), 'n_heads': np.int64(2), 'head_dim': np.int64(30), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.2079246267965562, 'attn_dropout': 0.08241873053328731, 'augment_noise_std': 0.049234117185589975, 'augment_scale_low': 0.8049704509172394, 'augment_scale_high': 1.0857832237962126, 'use_focal_loss': np.False_, 'focal_gamma': 4.7810467448854785, 'class_weighted': np.False_, 'grad_clip': 0.2031284053404986, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.8543
2025-10-02 21:13:44,141 - INFO - bo.run_bo - üîçBO Trial 38: Using RF surrogate + Expected Improvement
2025-10-02 21:13:44,141 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:13:44,141 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:13:44,141 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:13:44,141 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 87, 'batch_size': 128, 'lr': 0.0013072019696764524, 'weight_decay': 0.005537099703708741, 'patch_size': 50, 'n_heads': 1, 'head_dim': 32, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.025585869142677638, 'attn_dropout': 0.09346010632929455, 'augment_noise_std': 0.032671198069491716, 'augment_scale_low': 0.8779527518628633, 'augment_scale_high': 1.1581334822974894, 'use_focal_loss': False, 'focal_gamma': 1.7956706406215, 'class_weighted': True, 'grad_clip': 2.49601699419338, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:13:44,143 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 87, 'batch_size': 128, 'lr': 0.0013072019696764524, 'weight_decay': 0.005537099703708741, 'patch_size': 50, 'n_heads': 1, 'head_dim': 32, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.025585869142677638, 'attn_dropout': 0.09346010632929455, 'augment_noise_std': 0.032671198069491716, 'augment_scale_low': 0.8779527518628633, 'augment_scale_high': 1.1581334822974894, 'use_focal_loss': False, 'focal_gamma': 1.7956706406215, 'class_weighted': True, 'grad_clip': 2.49601699419338, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:15:09,703 - INFO - _models.training_function_executor - Model: 25,317 parameters, 108.8KB storage
2025-10-02 21:15:09,704 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.063604859986431, 0.7661429887068427, 0.6530424899863064, 0.5857248440036177, 0.5218642949600535, 0.4731455205983209, 0.4368765998132373, 0.41057096677843247, 0.3819727689565522, 0.3586594154989122, 0.3342663916343959, 0.32348826396242364, 0.30359219606807425, 0.3001181566613108, 0.281339906850405, 0.26912855711477845, 0.265636997481611, 0.25544383235144347, 0.24027955232118264, 0.2301122419599363, 0.22873207105489557, 0.21668113070771391, 0.21551592949572107, 0.20902129033657968, 0.2115281082633048, 0.19272592014374373, 0.19205944325468113, 0.1878012404976806, 0.1858150959570366, 0.18132973316076298, 0.17360245005961322, 0.1659913282340001, 0.1555112400225364, 0.16016704300200565, 0.15694523185722153, 0.1501490128798185, 0.15288841754176602, 0.15628057775831505, 0.14311207873710052, 0.14566662310443007, 0.14614464568646204, 0.14051391981300315, 0.13150329646165967, 0.13174762584217, 0.12796805528435787, 0.11887987322270491, 0.12636004218026048, 0.11559100804261552, 0.13398173234205485, 0.12219936706815877, 0.11147663737565985, 0.11643656697767822, 0.10966939679917477, 0.1066327261351719, 0.12326966376942329, 0.10467546893623252, 0.10983247584042362, 0.1061921166527097, 0.09792711332646228, 0.1003168710083758, 0.10021586598568728, 0.09918808188140384, 0.09130242789007871, 0.10244170157558297, 0.09352186248238548, 0.09053229355132995, 0.09706488560639079, 0.09920697510898295, 0.09637797930783558, 0.09704013079329211, 0.09032211902632778, 0.09517422751261134, 0.08287938326866548, 0.08527938343466207, 0.07705060768286334, 0.07845065541873729, 0.0861017138867029, 0.07689245086332142, 0.08248848089911479, 0.07886055301632965, 0.08522595609162933, 0.07475885754126216, 0.076383476451922, 0.08205371818468206, 0.0768780970683213, 0.07910053334336448, 0.07951471449616664], 'val_losses': [0.8252419950056608, 0.6911567959805178, 0.6202501937263428, 0.5976643204659152, 0.49430779774988703, 0.4728304259446372, 0.4530361746878362, 0.4172232564626937, 0.39943116498708936, 0.3906658187876398, 0.40467378240960417, 0.38422880307831364, 0.3684082837790986, 0.3442950123511913, 0.3075060926199467, 0.3660392597114184, 0.4005908778086893, 0.36758364453051057, 0.32535356977368013, 0.36140121063064656, 0.3146397215451323, 0.34288337651970185, 0.32779199075406096, 0.3395312660776453, 0.31118845986686633, 0.34163619684121493, 0.3125567409687063, 0.3605396038472331, 0.33135516927917713, 0.30987853353453104, 0.307355841599585, 0.37780256398734463, 0.3621639444884494, 0.3402646858506059, 0.325406030520597, 0.3224004345301055, 0.328502462716378, 0.3492116389560186, 0.3148692147188984, 0.3468939117800695, 0.3398457461506662, 0.3306657570172575, 0.3830744557072384, 0.3171531684064668, 0.3529003351094086, 0.36053196007245586, 0.35978968334502703, 0.4294197200466675, 0.31470418784451387, 0.33935187009996737, 0.40543656344850504, 0.3850717523121592, 0.5067353378582263, 0.34239766604394606, 0.3816804427645616, 0.35418660836983945, 0.4026723306566802, 0.3741606455625351, 0.4614583167003937, 0.36935450995331376, 0.3949001049087275, 0.396817529075293, 0.40877271874807425, 0.4144389878731147, 0.40640288427081894, 0.38444034578417524, 0.41386553704133683, 0.3520338645914625, 0.3642767736927009, 0.37625922716555626, 0.4216469452727963, 0.38917840209360965, 0.4128948429326577, 0.4018132030075168, 0.46352764054836, 0.4050830695999847, 0.41938328704145705, 0.4198956670595013, 0.3971802122968433, 0.38766923778682344, 0.4355969267737789, 0.38009665471959897, 0.3446146991300398, 0.3696627075578646, 0.42939988115111355, 0.4691840073747483, 0.3255229530522973], 'val_acc': [0.4905400325773713, 0.5421626362611202, 0.7323643653677484, 0.6391429645407843, 0.7312366871319383, 0.7440170404711189, 0.8173161257987721, 0.777596792381907, 0.8317253477007894, 0.8274652299210625, 0.8264628492670092, 0.8100488660568851, 0.783736373887984, 0.8317253477007894, 0.8435033203859166, 0.8496429018919934, 0.861671469740634, 0.7809798270893372, 0.8755795013156246, 0.8476381405838868, 0.8738253351710312, 0.8760806916426513, 0.8948753289061521, 0.8815937852399449, 0.8614208745771207, 0.8663074802656309, 0.899511339431149, 0.8487658188196968, 0.8635509334669841, 0.84751284300213, 0.9051497306101992, 0.8782107505325147, 0.9233178799649167, 0.9100363362987094, 0.8835985465480516, 0.9008896128304724, 0.8602931963413106, 0.9117905024433028, 0.9041473499561459, 0.9180553815311364, 0.900639017666959, 0.9288309735622102, 0.9239443678737, 0.8970053877960156, 0.9137952637514096, 0.9045232427014158, 0.9165518105500564, 0.9330910913419371, 0.9106628242074928, 0.9267009146723468, 0.930083949379777, 0.9349705550682872, 0.9386041849392307, 0.9179300839493798, 0.9249467485277534, 0.9275779977446436, 0.9293321638892369, 0.938478887357474, 0.9353464478135572, 0.9219396065655933, 0.9107881217892495, 0.9307104372885603, 0.9408595414108508, 0.9350958526500438, 0.9146723468237064, 0.9290815687257236, 0.9313369251973437, 0.9256985340182935, 0.9249467485277534, 0.9322140082696404, 0.9418619220649042, 0.930459842125047, 0.9393559704297707, 0.9443678737000376, 0.9360982333040972, 0.9447437664453076, 0.9347199599047739, 0.9265756170905901, 0.9335922816689638, 0.9320887106878837, 0.930459842125047, 0.9399824583385541, 0.9382282921939606, 0.9163012153865431, 0.9424884099736875, 0.9414860293196341, 0.9365994236311239], 'size_bytes': 111369, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 87, 'batch_size': 128, 'lr': 0.0013072019696764524, 'weight_decay': 0.005537099703708741, 'patch_size': 50, 'n_heads': 1, 'head_dim': 32, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.025585869142677638, 'attn_dropout': 0.09346010632929455, 'augment_noise_std': 0.032671198069491716, 'augment_scale_low': 0.8779527518628633, 'augment_scale_high': 1.1581334822974894, 'use_focal_loss': False, 'focal_gamma': 1.7956706406215, 'class_weighted': True, 'grad_clip': 2.49601699419338, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 25317, 'model_storage_size_kb': 108.783984375, 'model_size_validation': 'PASS'}
2025-10-02 21:15:09,704 - INFO - _models.training_function_executor - BO Objective: base=0.9366, size_penalty=0.0000, final=0.9366
2025-10-02 21:15:09,704 - INFO - _models.training_function_executor - Model: 25,317 parameters, 108.8KB (PASS 256KB limit)
2025-10-02 21:15:09,704 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 85.562s
2025-10-02 21:15:09,817 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9366
2025-10-02 21:15:09,817 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.113s
2025-10-02 21:15:09,817 - INFO - bo.run_bo - Recorded observation #38: hparams={'epochs': np.int64(87), 'batch_size': np.int64(128), 'lr': 0.0013072019696764524, 'weight_decay': 0.005537099703708741, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(32), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.025585869142677638, 'attn_dropout': 0.09346010632929455, 'augment_noise_std': 0.032671198069491716, 'augment_scale_low': 0.8779527518628633, 'augment_scale_high': 1.1581334822974894, 'use_focal_loss': np.False_, 'focal_gamma': 1.7956706406215, 'class_weighted': np.True_, 'grad_clip': 2.49601699419338, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.9366
2025-10-02 21:15:09,817 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 38: {'epochs': np.int64(87), 'batch_size': np.int64(128), 'lr': 0.0013072019696764524, 'weight_decay': 0.005537099703708741, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(32), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.025585869142677638, 'attn_dropout': 0.09346010632929455, 'augment_noise_std': 0.032671198069491716, 'augment_scale_low': 0.8779527518628633, 'augment_scale_high': 1.1581334822974894, 'use_focal_loss': np.False_, 'focal_gamma': 1.7956706406215, 'class_weighted': np.True_, 'grad_clip': 2.49601699419338, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.9366
2025-10-02 21:15:09,817 - INFO - bo.run_bo - üîçBO Trial 39: Using RF surrogate + Expected Improvement
2025-10-02 21:15:09,817 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:15:09,818 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:15:09,818 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:15:09,818 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 91, 'batch_size': 128, 'lr': 0.00029593533455609067, 'weight_decay': 0.00012781186328979046, 'patch_size': 250, 'n_heads': 3, 'head_dim': 29, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.16797385420195912, 'attn_dropout': 0.21369804280690435, 'augment_noise_std': 0.03533406556328753, 'augment_scale_low': 0.8852638015439146, 'augment_scale_high': 1.1918316121184296, 'use_focal_loss': False, 'focal_gamma': 4.316016374004612, 'class_weighted': False, 'grad_clip': 0.358945688382833, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:15:09,819 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 91, 'batch_size': 128, 'lr': 0.00029593533455609067, 'weight_decay': 0.00012781186328979046, 'patch_size': 250, 'n_heads': 3, 'head_dim': 29, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.16797385420195912, 'attn_dropout': 0.21369804280690435, 'augment_noise_std': 0.03533406556328753, 'augment_scale_low': 0.8852638015439146, 'augment_scale_high': 1.1918316121184296, 'use_focal_loss': False, 'focal_gamma': 4.316016374004612, 'class_weighted': False, 'grad_clip': 0.358945688382833, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:17:12,486 - INFO - _models.training_function_executor - Model: 167,910 parameters, 721.5KB storage
2025-10-02 21:17:12,486 - WARNING - _models.training_function_executor - Model storage 721.5KB exceeds 256KB limit!
2025-10-02 21:17:12,486 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6773721876215509, 0.4189454234481377, 0.3124902230887486, 0.25864709605466146, 0.22404231028127494, 0.20063690971197448, 0.18039789157711136, 0.1711866814665667, 0.1548405447915578, 0.1478454021937872, 0.13477376590181553, 0.12743351719026014, 0.12096241085996037, 0.1144144643069292, 0.10676158628759164, 0.10116840020758958, 0.0963242214844198, 0.0898654240682729, 0.08542138582380651, 0.0813399172262773, 0.07962319235176878, 0.07645877651398565, 0.07305631273098578, 0.06621320392287988, 0.06334989772441411, 0.06131135123649031, 0.058132982587569174, 0.05480585752432386, 0.05540271066226066, 0.05144580703037214, 0.04922515644206983, 0.04508870200251135, 0.046369601211804994, 0.04363840801927659, 0.04465358987140494, 0.04132655867646329, 0.03947946584856692, 0.03722521754937042, 0.0407023440054676, 0.03359300829896098, 0.03434641073025158, 0.03384974398324793, 0.031268485109426654, 0.03079686906654694, 0.03133698111951456, 0.029900961860401885, 0.031381999049461165, 0.02827381466639089, 0.02674198747853426, 0.029163819741320818, 0.02706750833558608, 0.02548028139473763, 0.024628474031573068, 0.02417858937610815, 0.024305695482432756, 0.02629977268830637, 0.022818638453673913, 0.021307507906958293, 0.02379909330585782, 0.024157237946423278, 0.02148061842059033, 0.021277631657777508, 0.019911951343902827, 0.019958779334960337, 0.023316394196774167, 0.01885903697777669, 0.020777434125750893, 0.019141835233583347, 0.02200986227343928, 0.018578794359515732, 0.016742161092562914, 0.018059760414611704, 0.018851792515554164, 0.018743959599828438, 0.01835897420248992, 0.017667019604040397, 0.017717711238681065, 0.018175064274695715, 0.02049491108666974, 0.015954403445721584, 0.018455359583606938, 0.014280328469620093, 0.013427742724957609, 0.014299599304183303, 0.013214201743282838, 0.01434804866400531, 0.015978624478440086, 0.01792022289898661, 0.014097602902585875, 0.016408634454778494, 0.01528581594084982], 'val_losses': [0.4960313762448632, 0.31984579330502766, 0.2718632028651348, 0.22630952851296726, 0.2025248526933124, 0.18280339815341476, 0.17107850231595273, 0.17343522609587136, 0.14966448272003474, 0.1722627469709442, 0.16304288112591447, 0.16146684988685275, 0.14823247289548555, 0.16822269417381933, 0.14639731277657428, 0.14479508980513067, 0.13927937004668062, 0.13492837870513835, 0.13916927586334, 0.1430036328875799, 0.14840025020943684, 0.14436819278989246, 0.1457359269158671, 0.14041997483538113, 0.13991714275924355, 0.13619098142340225, 0.13607839407657654, 0.13780096132397399, 0.1537079459581141, 0.15432582568209685, 0.14084913701569582, 0.14401663602511247, 0.14852506277809435, 0.1540672652345898, 0.15486762035895285, 0.16494753725650302, 0.14674813112449622, 0.15389072725519234, 0.15094778049994406, 0.1529595184902726, 0.14958793570890475, 0.17101961630819554, 0.1584360186519845, 0.15952619909119925, 0.16956412691010103, 0.16060590270153788, 0.15884796110195498, 0.16241693997454815, 0.16616062112521324, 0.1709343920172553, 0.17194347879131133, 0.1669845569180062, 0.17924649635363216, 0.19141705240645873, 0.18765837795049412, 0.18043832578929112, 0.17643865737576275, 0.19307269603518104, 0.1894301871931683, 0.17569151059933855, 0.179531227689446, 0.1705483554143264, 0.19179143846341204, 0.19394920438681162, 0.1986197072668581, 0.19468501407749714, 0.1958247324179444, 0.21487726399420856, 0.18282125955416048, 0.19002128883406508, 0.19839406500049678, 0.1897529565276784, 0.21145062450477764, 0.21587998403103006, 0.2003529107207688, 0.20075818348492525, 0.1995733626212053, 0.19630058165399253, 0.19534834935213746, 0.20753175321937936, 0.20510084857439997, 0.19496756972428358, 0.20885527889735417, 0.20671412668764674, 0.2025072155642847, 0.22057550973408863, 0.2091045903578511, 0.21353449647908462, 0.21155462724988344, 0.2024747665447335, 0.22129210718456865], 'val_acc': [0.8367372509710562, 0.9012655055757424, 0.9152988347324896, 0.9327151985966671, 0.9365994236311239, 0.9412354341561208, 0.9467485277534143, 0.9493797769703045, 0.9550181681493547, 0.9471244204986844, 0.9495050745520611, 0.9501315624608445, 0.9546422754040848, 0.9505074552061146, 0.9596541786743515, 0.9563964415486781, 0.9590276907655683, 0.9611577496554317, 0.9590276907655683, 0.9624107254729983, 0.960656559328405, 0.9626613206365117, 0.9599047738378649, 0.9631625109635384, 0.9622854278912417, 0.9657937601804285, 0.9642901891993485, 0.9635384037088084, 0.9585265004385415, 0.9595288810925949, 0.9649166771081318, 0.9649166771081318, 0.9667961408344818, 0.9650419746898885, 0.9646660819446184, 0.961408344818945, 0.9669214384162386, 0.9700538779601554, 0.967547926325022, 0.9671720335797519, 0.9670467359979953, 0.9666708432527252, 0.9666708432527252, 0.9676732239067786, 0.9671720335797519, 0.9674226287432652, 0.9654178674351584, 0.9655431650169152, 0.9656684625986719, 0.9652925698534018, 0.9669214384162386, 0.9691767948878587, 0.9689261997243453, 0.9664202480892119, 0.9664202480892119, 0.9672973311615086, 0.9699285803783987, 0.9666708432527252, 0.9639142964540784, 0.9693020924696153, 0.967547926325022, 0.9689261997243453, 0.9667961408344818, 0.969051497306102, 0.9680491166520486, 0.9672973311615086, 0.9685503069790753, 0.9680491166520486, 0.968675604560832, 0.9695526876331286, 0.9684250093973187, 0.969051497306102, 0.9679238190702919, 0.9641648916175918, 0.9699285803783987, 0.968299711815562, 0.9676732239067786, 0.969051497306102, 0.9693020924696153, 0.968299711815562, 0.9698032827966421, 0.9704297707054254, 0.9703044731236687, 0.9716827465229921, 0.9710562586142087, 0.9700538779601554, 0.9700538779601554, 0.9685503069790753, 0.9699285803783987, 0.9699285803783987, 0.9704297707054254], 'size_bytes': 883595, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 91, 'batch_size': 128, 'lr': 0.00029593533455609067, 'weight_decay': 0.00012781186328979046, 'patch_size': 250, 'n_heads': 3, 'head_dim': 29, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.16797385420195912, 'attn_dropout': 0.21369804280690435, 'augment_noise_std': 0.03533406556328753, 'augment_scale_low': 0.8852638015439146, 'augment_scale_high': 1.1918316121184296, 'use_focal_loss': False, 'focal_gamma': 4.316016374004612, 'class_weighted': False, 'grad_clip': 0.358945688382833, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 167910, 'model_storage_size_kb': 721.4882812500001, 'model_size_validation': 'FAIL'}
2025-10-02 21:17:12,486 - INFO - _models.training_function_executor - BO Objective: base=0.9704, size_penalty=0.8000, final=0.1704
2025-10-02 21:17:12,486 - INFO - _models.training_function_executor - Model: 167,910 parameters, 721.5KB (FAIL 256KB limit)
2025-10-02 21:17:12,486 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 122.668s
2025-10-02 21:17:12,604 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.1704
2025-10-02 21:17:12,604 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.118s
2025-10-02 21:17:12,604 - INFO - bo.run_bo - Recorded observation #39: hparams={'epochs': np.int64(91), 'batch_size': np.int64(128), 'lr': 0.00029593533455609067, 'weight_decay': 0.00012781186328979046, 'patch_size': np.int64(250), 'n_heads': np.int64(3), 'head_dim': np.int64(29), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.16797385420195912, 'attn_dropout': 0.21369804280690435, 'augment_noise_std': 0.03533406556328753, 'augment_scale_low': 0.8852638015439146, 'augment_scale_high': 1.1918316121184296, 'use_focal_loss': np.False_, 'focal_gamma': 4.316016374004612, 'class_weighted': np.False_, 'grad_clip': 0.358945688382833, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.1704
2025-10-02 21:17:12,604 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 39: {'epochs': np.int64(91), 'batch_size': np.int64(128), 'lr': 0.00029593533455609067, 'weight_decay': 0.00012781186328979046, 'patch_size': np.int64(250), 'n_heads': np.int64(3), 'head_dim': np.int64(29), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.16797385420195912, 'attn_dropout': 0.21369804280690435, 'augment_noise_std': 0.03533406556328753, 'augment_scale_low': 0.8852638015439146, 'augment_scale_high': 1.1918316121184296, 'use_focal_loss': np.False_, 'focal_gamma': 4.316016374004612, 'class_weighted': np.False_, 'grad_clip': 0.358945688382833, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.1704
2025-10-02 21:17:12,604 - INFO - bo.run_bo - üîçBO Trial 40: Using RF surrogate + Expected Improvement
2025-10-02 21:17:12,605 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:17:12,605 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:17:12,605 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:17:12,605 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 55, 'batch_size': 128, 'lr': 0.00022405472815006954, 'weight_decay': 5.8005758515819436e-06, 'patch_size': 40, 'n_heads': 1, 'head_dim': 14, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.12220613527026561, 'attn_dropout': 0.02218781868599097, 'augment_noise_std': 0.027004299066321902, 'augment_scale_low': 0.808275706965858, 'augment_scale_high': 1.1906129347363898, 'use_focal_loss': False, 'focal_gamma': 2.6605140791071404, 'class_weighted': True, 'grad_clip': 1.0927328748755285, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:17:12,606 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 55, 'batch_size': 128, 'lr': 0.00022405472815006954, 'weight_decay': 5.8005758515819436e-06, 'patch_size': 40, 'n_heads': 1, 'head_dim': 14, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.12220613527026561, 'attn_dropout': 0.02218781868599097, 'augment_noise_std': 0.027004299066321902, 'augment_scale_low': 0.808275706965858, 'augment_scale_high': 1.1906129347363898, 'use_focal_loss': False, 'focal_gamma': 2.6605140791071404, 'class_weighted': True, 'grad_clip': 1.0927328748755285, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:18:10,514 - INFO - _models.training_function_executor - Model: 6,641 parameters, 28.5KB storage
2025-10-02 21:18:10,514 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5011798568571495, 1.2678782996410483, 1.0832076132049187, 0.9941585778170606, 0.9325326721579293, 0.8880985321580507, 0.8502658383666216, 0.8240837613203821, 0.801663284227371, 0.7784973104932988, 0.7571488780824925, 0.7416451226644369, 0.7243234203266059, 0.7156989116292735, 0.6975177442116707, 0.6883896223208108, 0.6781432518955909, 0.6682325266967967, 0.6595977356763755, 0.6533082387906151, 0.6461564994443125, 0.6319658106641468, 0.6293001228775016, 0.6171639306508736, 0.6087114107956829, 0.6021445992092058, 0.5954176604167092, 0.5912921359073517, 0.584793747972584, 0.5719597047329681, 0.5721384400262445, 0.5598547504860346, 0.5596579418099322, 0.54504239430061, 0.5461430735672543, 0.5419039429102319, 0.535683874088104, 0.5332721323162318, 0.5255830557305342, 0.5249565609297283, 0.5175979920264918, 0.5216842200438435, 0.50957855156063, 0.49999976362033555, 0.501240325936383, 0.4955727691653826, 0.4939423191537311, 0.4839650151820924, 0.49128726735410955, 0.4840197199490257, 0.4726930282625142, 0.4840875641522101, 0.4776831507921794, 0.4683293375616268, 0.460963130255852], 'val_losses': [1.394852719425244, 1.1209861256397842, 1.003140619241449, 0.9443809364152991, 0.8965846256659512, 0.8542071895078973, 0.8155001412207524, 0.7888306582717874, 0.7721581468754224, 0.7415450325235958, 0.7347146658435261, 0.7173011919868394, 0.7179146842197165, 0.6828926692532054, 0.6732936638210549, 0.6741387523956248, 0.6649419900513938, 0.6393214327984176, 0.6395071497474782, 0.6286119672756508, 0.6152108204856191, 0.6217791099787924, 0.6063901313846981, 0.5990733048615935, 0.6008552289381763, 0.5925160700807952, 0.586000835459762, 0.5746232457902404, 0.5703811902996742, 0.5602459735424353, 0.5713315121460761, 0.5645545158260314, 0.5629255777592946, 0.5533082420373976, 0.5677089027378018, 0.5471968721729549, 0.5636369937767347, 0.5351560150765936, 0.5423692196155756, 0.5319216915474785, 0.534713773465055, 0.5434507402321731, 0.519878675399635, 0.5347738498293237, 0.5225896489544032, 0.5203425635234438, 0.5138681610471368, 0.5108478438763104, 0.4987597283456218, 0.5052071940800313, 0.5055509897240898, 0.4936463114608426, 0.5103058969297888, 0.5152210022569405, 0.48968442899961845], 'val_acc': [0.36436536774840245, 0.47801027440170407, 0.5370254354090966, 0.5627114396692143, 0.5821325648414986, 0.6100739255732365, 0.5883974439293321, 0.6371382032326776, 0.644280165392808, 0.6520486154617215, 0.6920185440421, 0.6954015787495301, 0.715699786994111, 0.6715950382157624, 0.6708432527252224, 0.6903896754792632, 0.7210875830096479, 0.7030447312366871, 0.7005387796015536, 0.7104372885603308, 0.7100613958150608, 0.7274777596792382, 0.7136950256860043, 0.6969051497306102, 0.754667334920436, 0.714196216013031, 0.7470241824332791, 0.7311113895501817, 0.7475253727603057, 0.753163763939356, 0.7461470993609823, 0.7594286430271896, 0.7670717955143466, 0.7680741761683999, 0.7670717955143466, 0.7433905525623355, 0.7821075053251473, 0.7328655556947751, 0.7898759553940609, 0.7843628617967673, 0.7828592908156873, 0.7992732740258113, 0.792131311865681, 0.7955143465731112, 0.7648164390427264, 0.7831098859792006, 0.7564215010650295, 0.7816063149981205, 0.7817316125798772, 0.7995238691893246, 0.7814810174163639, 0.7767197093096103, 0.7996491667710813, 0.8165643403082321, 0.7780979827089337], 'size_bytes': 35913, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 55, 'batch_size': 128, 'lr': 0.00022405472815006954, 'weight_decay': 5.8005758515819436e-06, 'patch_size': 40, 'n_heads': 1, 'head_dim': 14, 'num_layers': 2, 'mlp_ratio': 4, 'dropout': 0.12220613527026561, 'attn_dropout': 0.02218781868599097, 'augment_noise_std': 0.027004299066321902, 'augment_scale_low': 0.808275706965858, 'augment_scale_high': 1.1906129347363898, 'use_focal_loss': False, 'focal_gamma': 2.6605140791071404, 'class_weighted': True, 'grad_clip': 1.0927328748755285, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 6641, 'model_storage_size_kb': 28.535546875, 'model_size_validation': 'PASS'}
2025-10-02 21:18:10,514 - INFO - _models.training_function_executor - BO Objective: base=0.7781, size_penalty=0.0000, final=0.7781
2025-10-02 21:18:10,514 - INFO - _models.training_function_executor - Model: 6,641 parameters, 28.5KB (PASS 256KB limit)
2025-10-02 21:18:10,514 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 57.910s
2025-10-02 21:18:10,630 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7781
2025-10-02 21:18:10,630 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.116s
2025-10-02 21:18:10,630 - INFO - bo.run_bo - Recorded observation #40: hparams={'epochs': np.int64(55), 'batch_size': np.int64(128), 'lr': 0.00022405472815006954, 'weight_decay': 5.8005758515819436e-06, 'patch_size': np.int64(40), 'n_heads': np.int64(1), 'head_dim': np.int64(14), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.12220613527026561, 'attn_dropout': 0.02218781868599097, 'augment_noise_std': 0.027004299066321902, 'augment_scale_low': 0.808275706965858, 'augment_scale_high': 1.1906129347363898, 'use_focal_loss': np.False_, 'focal_gamma': 2.6605140791071404, 'class_weighted': np.True_, 'grad_clip': 1.0927328748755285, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.7781
2025-10-02 21:18:10,630 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 40: {'epochs': np.int64(55), 'batch_size': np.int64(128), 'lr': 0.00022405472815006954, 'weight_decay': 5.8005758515819436e-06, 'patch_size': np.int64(40), 'n_heads': np.int64(1), 'head_dim': np.int64(14), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(4), 'dropout': 0.12220613527026561, 'attn_dropout': 0.02218781868599097, 'augment_noise_std': 0.027004299066321902, 'augment_scale_low': 0.808275706965858, 'augment_scale_high': 1.1906129347363898, 'use_focal_loss': np.False_, 'focal_gamma': 2.6605140791071404, 'class_weighted': np.True_, 'grad_clip': 1.0927328748755285, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.7781
2025-10-02 21:18:10,631 - INFO - bo.run_bo - üîçBO Trial 41: Using RF surrogate + Expected Improvement
2025-10-02 21:18:10,631 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:18:10,631 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:18:10,631 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:18:10,631 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 97, 'batch_size': 256, 'lr': 1.6676901184466323e-05, 'weight_decay': 0.00046583526506532223, 'patch_size': 50, 'n_heads': 1, 'head_dim': 31, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.08251093988729348, 'attn_dropout': 0.2714265107019044, 'augment_noise_std': 0.03705475632635027, 'augment_scale_low': 0.8788871070852688, 'augment_scale_high': 1.1856206695186138, 'use_focal_loss': False, 'focal_gamma': 3.3847283064732783, 'class_weighted': False, 'grad_clip': 2.239739282436849, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 21:18:10,632 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 97, 'batch_size': 256, 'lr': 1.6676901184466323e-05, 'weight_decay': 0.00046583526506532223, 'patch_size': 50, 'n_heads': 1, 'head_dim': 31, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.08251093988729348, 'attn_dropout': 0.2714265107019044, 'augment_noise_std': 0.03705475632635027, 'augment_scale_low': 0.8788871070852688, 'augment_scale_high': 1.1856206695186138, 'use_focal_loss': False, 'focal_gamma': 3.3847283064732783, 'class_weighted': False, 'grad_clip': 2.239739282436849, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 21:19:56,272 - INFO - _models.training_function_executor - Model: 43,870 parameters, 188.5KB storage
2025-10-02 21:19:56,272 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.5872150025966565, 1.2435681645525087, 1.035554075059603, 0.9296474159781476, 0.8686016358457793, 0.8246304869662997, 0.7955705710559456, 0.7759936027922241, 0.762210091408813, 0.751217904541011, 0.7420349733160241, 0.7342985462482176, 0.7266565946130359, 0.7186690920935468, 0.7114920165572173, 0.7023953003262426, 0.6925005261168289, 0.6830584191814445, 0.6720100486502217, 0.6602611782814227, 0.6476405459612723, 0.6369140083933297, 0.6263782773027443, 0.6161680570021573, 0.605668539272433, 0.5961767954408937, 0.5860208389219311, 0.5749857348240816, 0.5636717645767148, 0.5527163712492833, 0.540742495794275, 0.5308859833752986, 0.5211330542243842, 0.5120263075752373, 0.5020515203552505, 0.49416801399990895, 0.4869740488210074, 0.47895636298683125, 0.47248697500272496, 0.4672027256486709, 0.4609793699878471, 0.45490975488499247, 0.44905619136306446, 0.4460757307565194, 0.44001647536215305, 0.43529906288721515, 0.43176947932298165, 0.426681902323421, 0.42207788677493857, 0.41864006856167985, 0.41326015646664627, 0.40924971307671704, 0.40538310121202353, 0.40344398191066083, 0.39872043447506755, 0.3942771568771622, 0.3911977191114472, 0.388206719981546, 0.38493066333430126, 0.3799006039814046, 0.3764272829111561, 0.37483282860529415, 0.37145802504219505, 0.367224401569238, 0.36363267948972294, 0.3624448064669509, 0.3580836232062984, 0.35481973482590584, 0.353517799502465, 0.3516687879956942, 0.3503298338822276, 0.34458801326766497, 0.34317372456396955, 0.34084078460428585, 0.3379669751489655, 0.33782197295624233, 0.33374770545210375, 0.3313829749127108, 0.32948449656870965, 0.32683414744728845, 0.32432158590782123, 0.32373386131538223, 0.3214758892773768, 0.3220420521579694, 0.319502828703583, 0.3155885658745359, 0.31365739892095423, 0.30939674574517817, 0.31200043536276645, 0.31017990926358163, 0.30855579946052536, 0.3048209132707698, 0.30468145914216377, 0.30226701012108964, 0.30042096504280674, 0.30064669975984837, 0.2988932359279466], 'val_losses': [1.3822628734345097, 1.1100919612322426, 0.9633621512954806, 0.8898589079981384, 0.8389607585995527, 0.8034772071574299, 0.7817636029550925, 0.7662944032530217, 0.7541468881482305, 0.7450468767601929, 0.7367837605821714, 0.7300259757498992, 0.7219361596844756, 0.7171355831437863, 0.7065005308142462, 0.6967157867076985, 0.6859184695603091, 0.6749390606152177, 0.6632662069123874, 0.6524595669181914, 0.6410983293079789, 0.6301646158780658, 0.6193216666028458, 0.6092247219405935, 0.5987946720570494, 0.5880343405812952, 0.5776988401394741, 0.5641830333035597, 0.5520990164361192, 0.5389957837279675, 0.5276814450253939, 0.5151445109613287, 0.5061030744453254, 0.4945137815104438, 0.4855877216478261, 0.4775099197346351, 0.4697346678877695, 0.46356220664931425, 0.4564868964552655, 0.450067767966527, 0.4449800009409925, 0.4420496868453906, 0.4346108354881075, 0.4274098633442766, 0.4225449054675239, 0.41799359691425336, 0.4147431196283389, 0.40869687424986423, 0.40495101314576226, 0.40000708331187673, 0.39518675781658547, 0.3919050711211876, 0.38661521642582414, 0.38451315448212753, 0.3804657712098934, 0.3768976907236596, 0.3703263705049507, 0.3682469899695315, 0.3643779098136191, 0.35914706601682517, 0.35527442360399303, 0.3527322196935057, 0.3485371651334013, 0.34692332334513054, 0.3421729652001257, 0.3394490399018309, 0.3378547005688632, 0.3352668941125522, 0.33124068812564594, 0.33270549593063214, 0.3258831962511782, 0.3227665174905108, 0.32104383879340603, 0.3171820183129496, 0.31577373241036927, 0.31275777495037327, 0.3113567987385735, 0.3102431515339786, 0.3103991840965929, 0.30393303061260524, 0.30350194506411593, 0.3013397117647487, 0.3009336043115159, 0.29782400861925806, 0.29553452411206976, 0.29405093245332425, 0.2916001764332318, 0.2904769704800742, 0.2876346966457343, 0.28808774586728214, 0.2850839095357038, 0.28336390227829955, 0.28219637977884254, 0.281180897626553, 0.2792426263489919, 0.2784043701948819, 0.2768930829851389], 'val_acc': [0.453953138704423, 0.7200852023555946, 0.7200852023555946, 0.7200852023555946, 0.7327402581130185, 0.7504072171407092, 0.7552938228292194, 0.7562962034832728, 0.7562962034832728, 0.7567973938102994, 0.7564215010650295, 0.7572985841373262, 0.7583009647913795, 0.7596792381907029, 0.7596792381907029, 0.762310487407593, 0.7640646535521864, 0.7658188196967799, 0.7707054253852901, 0.7744643528379902, 0.7792256609447438, 0.7861170279413607, 0.7901265505575742, 0.7973938102994612, 0.8021551184062148, 0.8060393434406716, 0.8075429144217516, 0.8125548176920185, 0.8149354717453953, 0.8224533266507956, 0.8268387420122791, 0.8323518356095727, 0.8346071920811928, 0.8418744518230799, 0.84488159378524, 0.8488911164014534, 0.8511464728730735, 0.8546548051622603, 0.8559077809798271, 0.859792006014284, 0.860919684250094, 0.8602931963413106, 0.8643027189575241, 0.8671845633379276, 0.8688134319007643, 0.870442300463601, 0.8713193835358978, 0.8738253351710312, 0.8757047988973813, 0.876581881969678, 0.8789625360230547, 0.8785866432777847, 0.8815937852399449, 0.8803408094223781, 0.8815937852399449, 0.8818443804034583, 0.8857286054379151, 0.8859792006014284, 0.8879839619095351, 0.8896128304723718, 0.8924946748527753, 0.893747650670342, 0.8957524119784488, 0.8942488409973688, 0.8978824708683122, 0.8985089587770956, 0.8981330660318256, 0.9005137200852024, 0.9018919934845258, 0.8992607442676356, 0.9026437789750658, 0.9048991354466859, 0.9041473499561459, 0.9060268136824959, 0.9065280040095226, 0.9084074677358727, 0.9085327653176294, 0.9084074677358727, 0.9066533015912793, 0.9112893121162762, 0.9107881217892495, 0.9109134193710061, 0.9117905024433028, 0.9125422879338428, 0.9122916927703295, 0.9136699661696529, 0.9141711564966796, 0.9145470492419496, 0.9154241323142464, 0.9145470492419496, 0.9168024057135697, 0.9184312742764065, 0.9169277032953264, 0.9175541912041097, 0.9188071670216765, 0.9183059766946498, 0.9196842500939731], 'size_bytes': 192257, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 97, 'batch_size': 256, 'lr': 1.6676901184466323e-05, 'weight_decay': 0.00046583526506532223, 'patch_size': 50, 'n_heads': 1, 'head_dim': 31, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.08251093988729348, 'attn_dropout': 0.2714265107019044, 'augment_noise_std': 0.03705475632635027, 'augment_scale_low': 0.8788871070852688, 'augment_scale_high': 1.1856206695186138, 'use_focal_loss': False, 'focal_gamma': 3.3847283064732783, 'class_weighted': False, 'grad_clip': 2.239739282436849, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 43870, 'model_storage_size_kb': 188.50390625000003, 'model_size_validation': 'PASS'}
2025-10-02 21:19:56,272 - INFO - _models.training_function_executor - BO Objective: base=0.9197, size_penalty=0.0000, final=0.9197
2025-10-02 21:19:56,272 - INFO - _models.training_function_executor - Model: 43,870 parameters, 188.5KB (PASS 256KB limit)
2025-10-02 21:19:56,272 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 105.641s
2025-10-02 21:19:56,391 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9197
2025-10-02 21:19:56,391 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.119s
2025-10-02 21:19:56,391 - INFO - bo.run_bo - Recorded observation #41: hparams={'epochs': np.int64(97), 'batch_size': np.int64(256), 'lr': 1.6676901184466323e-05, 'weight_decay': 0.00046583526506532223, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(31), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.08251093988729348, 'attn_dropout': 0.2714265107019044, 'augment_noise_std': 0.03705475632635027, 'augment_scale_low': 0.8788871070852688, 'augment_scale_high': 1.1856206695186138, 'use_focal_loss': np.False_, 'focal_gamma': 3.3847283064732783, 'class_weighted': np.False_, 'grad_clip': 2.239739282436849, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9197
2025-10-02 21:19:56,391 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 41: {'epochs': np.int64(97), 'batch_size': np.int64(256), 'lr': 1.6676901184466323e-05, 'weight_decay': 0.00046583526506532223, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(31), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.08251093988729348, 'attn_dropout': 0.2714265107019044, 'augment_noise_std': 0.03705475632635027, 'augment_scale_low': 0.8788871070852688, 'augment_scale_high': 1.1856206695186138, 'use_focal_loss': np.False_, 'focal_gamma': 3.3847283064732783, 'class_weighted': np.False_, 'grad_clip': 2.239739282436849, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9197
2025-10-02 21:19:56,392 - INFO - bo.run_bo - üîçBO Trial 42: Using RF surrogate + Expected Improvement
2025-10-02 21:19:56,392 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:19:56,392 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:19:56,392 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:19:56,392 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 80, 'batch_size': 128, 'lr': 0.0002645771733165373, 'weight_decay': 0.0005921528836381434, 'patch_size': 20, 'n_heads': 1, 'head_dim': 5, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.17798726110357369, 'attn_dropout': 0.009100345503975961, 'augment_noise_std': 0.013799913144144485, 'augment_scale_low': 0.8078381698752839, 'augment_scale_high': 1.19254801426843, 'use_focal_loss': False, 'focal_gamma': 1.9273220803843831, 'class_weighted': False, 'grad_clip': 1.4645610338260127, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:19:56,393 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 80, 'batch_size': 128, 'lr': 0.0002645771733165373, 'weight_decay': 0.0005921528836381434, 'patch_size': 20, 'n_heads': 1, 'head_dim': 5, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.17798726110357369, 'attn_dropout': 0.009100345503975961, 'augment_noise_std': 0.013799913144144485, 'augment_scale_low': 0.8078381698752839, 'augment_scale_high': 1.19254801426843, 'use_focal_loss': False, 'focal_gamma': 1.9273220803843831, 'class_weighted': False, 'grad_clip': 1.4645610338260127, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:21:54,111 - INFO - _models.training_function_executor - Model: 1,020 parameters, 1.1KB storage
2025-10-02 21:21:54,111 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.491838771586721, 1.0482360681966034, 0.9018040153368058, 0.8125926459137705, 0.7435355391828905, 0.700798475381609, 0.6672392200397466, 0.6390768156625818, 0.6129361090831722, 0.5950220607502831, 0.5787618874894853, 0.5662620426693742, 0.5542028401854179, 0.5467739645705032, 0.5394802292934864, 0.5296977452383683, 0.5219564408841552, 0.5114639541340499, 0.49833241031794484, 0.49374846998463634, 0.48395805960310523, 0.47902910059822484, 0.4742804100159972, 0.46912975766562215, 0.46429068210843966, 0.4611829090866384, 0.456679141518093, 0.45718217540406436, 0.4490486272875537, 0.44766047083281807, 0.4439714088407229, 0.437042621761412, 0.4348859314340004, 0.4308512327840811, 0.4278346939759905, 0.4244436247584866, 0.42017441311713966, 0.4201012490383684, 0.4158069873066753, 0.41283069985053955, 0.40982461035161993, 0.407666944562726, 0.4014606464067109, 0.3999570195016095, 0.3994883237258349, 0.3970067258629588, 0.3953075519516792, 0.39144602239727544, 0.39160993978037134, 0.38918076736363977, 0.38512405624769946, 0.38514631661642684, 0.3828117114254897, 0.3755588118427212, 0.3787965101470245, 0.3759816392704915, 0.37449517020225687, 0.3741437485073681, 0.3703855342764613, 0.37065197416267215, 0.3668343030317488, 0.3665380968785176, 0.3651311939507952, 0.3638534704959919, 0.3637560604845182, 0.3624190152414019, 0.3602596689774735, 0.3568841970041947, 0.3593857362915414, 0.3567231444782158, 0.35808534031131195, 0.35653887744147506, 0.35463596447073226, 0.35600010287181755, 0.3507964784771982, 0.35490928432384505, 0.3532334853300821, 0.35146223373518093, 0.35114914445895673, 0.35244795224090997], 'val_losses': [1.167858079536982, 0.9488395931218564, 0.8504790624510509, 0.7649519391607154, 0.710855223999303, 0.6760407616404749, 0.6474307736960916, 0.6195784522027483, 0.6048320186248685, 0.5916103434964478, 0.5716330612711016, 0.5740463870119263, 0.5515723376887407, 0.543994045548086, 0.547185984142232, 0.5206389430695167, 0.5099366503413975, 0.4976124145709517, 0.4880800612206418, 0.48370997860697246, 0.4842728761081602, 0.47174845957402045, 0.46501133763734864, 0.4573805383565865, 0.4553261218983911, 0.4606515095748783, 0.4511101582906641, 0.44900076734498395, 0.4402951402068063, 0.43582996036710336, 0.4280164489757595, 0.42537663856472474, 0.42573801015174983, 0.4267878271672021, 0.4168309681234227, 0.4174074524738872, 0.41625468809493216, 0.41119401864152016, 0.40622161081217834, 0.4099416181775595, 0.39781021614030493, 0.403960043883476, 0.39473052933772934, 0.39464964199605373, 0.38806127203338325, 0.3859222242854888, 0.383391197847851, 0.3875192410888237, 0.3868288541479437, 0.3871390324754832, 0.37838484896161384, 0.3824432312568437, 0.3751769463134102, 0.37694400470144185, 0.3713037773001509, 0.3782931445718424, 0.3697880901682871, 0.36438982965780103, 0.36863247775378766, 0.3671996641645992, 0.3693620456465951, 0.3720719947012545, 0.36348061909570745, 0.36284584081638965, 0.3619258442463963, 0.3759734582750141, 0.36457509521239534, 0.36726436825522113, 0.36496715725043294, 0.3624033528213109, 0.36439062056573107, 0.36239715865823113, 0.35725891055013437, 0.35595247854289963, 0.3627856119322666, 0.3540904711320009, 0.35506858752382053, 0.3539659690829301, 0.36196410654108263, 0.36045869912304773], 'val_acc': [0.7200852023555946, 0.7200852023555946, 0.7199599047738379, 0.7702042350582634, 0.7755920310738003, 0.7777220899636637, 0.7778473875454204, 0.7832351835609572, 0.8005262498433781, 0.8034080942237815, 0.8092970805663451, 0.8026563087332415, 0.815812554817692, 0.8130560080190452, 0.8115524370379652, 0.8272146347575492, 0.8272146347575492, 0.8341060017541662, 0.8418744518230799, 0.8465104623480767, 0.8458839744392933, 0.8532765317629369, 0.8567848640521238, 0.8575366495426638, 0.8582884350332038, 0.8587896253602305, 0.8604184939230672, 0.8604184939230672, 0.8648039092845508, 0.8690640270642777, 0.8723217641899511, 0.8731988472622478, 0.8695652173913043, 0.871194085954141, 0.8774589650419747, 0.8733241448440046, 0.8752036085703546, 0.8782107505325147, 0.8819696779852149, 0.876581881969678, 0.8833479513845383, 0.8803408094223781, 0.883473248966295, 0.884225034456835, 0.8877333667460218, 0.8869815812554818, 0.8906152111264253, 0.8857286054379151, 0.8889863425635885, 0.8871068788372385, 0.891492294198722, 0.8898634256358853, 0.8928705675980454, 0.8912416990352087, 0.8938729482520987, 0.8921187821075053, 0.8960030071419621, 0.8956271143966922, 0.8957524119784488, 0.8948753289061521, 0.8947500313243955, 0.892995865179802, 0.8952512216514221, 0.8990101491041222, 0.8976318757047989, 0.8894875328906152, 0.8973812805412855, 0.8963788998872322, 0.8950006264879088, 0.899511339431149, 0.8958777095602055, 0.8958777095602055, 0.9001378273399323, 0.8993860418493923, 0.8972559829595289, 0.8997619345946624, 0.8997619345946624, 0.9003884225034456, 0.8976318757047989, 0.8977571732865556], 'size_bytes': 28747, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 80, 'batch_size': 128, 'lr': 0.0002645771733165373, 'weight_decay': 0.0005921528836381434, 'patch_size': 20, 'n_heads': 1, 'head_dim': 5, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.17798726110357369, 'attn_dropout': 0.009100345503975961, 'augment_noise_std': 0.013799913144144485, 'augment_scale_low': 0.8078381698752839, 'augment_scale_high': 1.19254801426843, 'use_focal_loss': False, 'focal_gamma': 1.9273220803843831, 'class_weighted': False, 'grad_clip': 1.4645610338260127, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 1020, 'model_storage_size_kb': 1.095703125, 'model_size_validation': 'PASS'}
2025-10-02 21:21:54,111 - INFO - _models.training_function_executor - BO Objective: base=0.8978, size_penalty=0.0000, final=0.8978
2025-10-02 21:21:54,111 - INFO - _models.training_function_executor - Model: 1,020 parameters, 1.1KB (PASS 256KB limit)
2025-10-02 21:21:54,111 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 117.720s
2025-10-02 21:21:54,227 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8978
2025-10-02 21:21:54,227 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.116s
2025-10-02 21:21:54,227 - INFO - bo.run_bo - Recorded observation #42: hparams={'epochs': np.int64(80), 'batch_size': np.int64(128), 'lr': 0.0002645771733165373, 'weight_decay': 0.0005921528836381434, 'patch_size': np.int64(20), 'n_heads': np.int64(1), 'head_dim': np.int64(5), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(2), 'dropout': 0.17798726110357369, 'attn_dropout': 0.009100345503975961, 'augment_noise_std': 0.013799913144144485, 'augment_scale_low': 0.8078381698752839, 'augment_scale_high': 1.19254801426843, 'use_focal_loss': np.False_, 'focal_gamma': 1.9273220803843831, 'class_weighted': np.False_, 'grad_clip': 1.4645610338260127, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.8978
2025-10-02 21:21:54,227 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 42: {'epochs': np.int64(80), 'batch_size': np.int64(128), 'lr': 0.0002645771733165373, 'weight_decay': 0.0005921528836381434, 'patch_size': np.int64(20), 'n_heads': np.int64(1), 'head_dim': np.int64(5), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(2), 'dropout': 0.17798726110357369, 'attn_dropout': 0.009100345503975961, 'augment_noise_std': 0.013799913144144485, 'augment_scale_low': 0.8078381698752839, 'augment_scale_high': 1.19254801426843, 'use_focal_loss': np.False_, 'focal_gamma': 1.9273220803843831, 'class_weighted': np.False_, 'grad_clip': 1.4645610338260127, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.8978
2025-10-02 21:21:54,228 - INFO - bo.run_bo - üîçBO Trial 43: Using RF surrogate + Expected Improvement
2025-10-02 21:21:54,228 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:21:54,228 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:21:54,228 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:21:54,228 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 82, 'batch_size': 128, 'lr': 0.003339952488844921, 'weight_decay': 0.008325410566280878, 'patch_size': 100, 'n_heads': 4, 'head_dim': 20, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.05588120569436496, 'attn_dropout': 0.29523348961431206, 'augment_noise_std': 0.006203877468326281, 'augment_scale_low': 0.8679643432695774, 'augment_scale_high': 1.185452489813183, 'use_focal_loss': True, 'focal_gamma': 1.3616547952137026, 'class_weighted': False, 'grad_clip': 1.6070974345456348, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:21:54,229 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 82, 'batch_size': 128, 'lr': 0.003339952488844921, 'weight_decay': 0.008325410566280878, 'patch_size': 100, 'n_heads': 4, 'head_dim': 20, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.05588120569436496, 'attn_dropout': 0.29523348961431206, 'augment_noise_std': 0.006203877468326281, 'augment_scale_low': 0.8679643432695774, 'augment_scale_high': 1.185452489813183, 'use_focal_loss': True, 'focal_gamma': 1.3616547952137026, 'class_weighted': False, 'grad_clip': 1.6070974345456348, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:23:19,142 - INFO - _models.training_function_executor - Model: 69,440 parameters, 74.6KB storage
2025-10-02 21:23:19,142 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3436703275469869, 0.25117886029560793, 0.22665404826724744, 0.2027409970802356, 0.17196424576874175, 0.14817078764164796, 0.13306900475135564, 0.11943217449531338, 0.10543493358168324, 0.09346397478823464, 0.09058657048484069, 0.08361323462833092, 0.079728471430136, 0.08101079223106114, 0.07188305790628499, 0.07064844948962469, 0.06641021268615276, 0.06356325814246579, 0.06147577762753131, 0.06353767979713314, 0.05894910723612325, 0.056054735424524105, 0.058862132100844164, 0.05928302378314736, 0.06008444920194365, 0.0519219213253886, 0.05458237050884834, 0.05513193322647799, 0.049644072171373815, 0.05094690044093609, 0.0498228420928824, 0.047893368022458783, 0.04628975802232974, 0.048296162722596056, 0.04648773106485414, 0.0476162281350999, 0.04601857079597873, 0.04498389990192116, 0.042461433002730906, 0.0441749499421013, 0.04016038348931375, 0.04087045760439888, 0.04268609651308985, 0.04251361829741725, 0.04100482899855392, 0.0362322230258558, 0.04093899140589861, 0.04460885728323807, 0.04272561269569762, 0.03953726503312918, 0.04245041677166345, 0.03578534649299753, 0.03859468255590792, 0.03504302381560814, 0.035518550809996885, 0.03531550311554073, 0.036986233921286825, 0.03720601111733555, 0.04189948830768152, 0.03520066561916351, 0.03649190075141675, 0.03500788655531506, 0.036202343482843555, 0.03355962402575507, 0.03227245832083873, 0.03895656768501218, 0.033912098222688654, 0.02925671811495621, 0.032137629857060826, 0.028338448711463595, 0.034024411945633284, 0.033778154743322505, 0.030408843076571622, 0.033291869829516624, 0.03093151450710305, 0.028226196918188097, 0.028928894363594613, 0.03294536308879202, 0.03314512483587735, 0.030899263209704264, 0.028306303681346005, 0.028870080334461413], 'val_losses': [0.2583896830209349, 0.2252850921836477, 0.19135092045934915, 0.18441425169780878, 0.14918853228034462, 0.14457543432253253, 0.11752981011348625, 0.10448350633152997, 0.09046165415051662, 0.08778895897765818, 0.08450604525910868, 0.07759721311622135, 0.08866876530204407, 0.07532173261520647, 0.06809413147878564, 0.06724221565836638, 0.07452406066093749, 0.06520976277240728, 0.0723940602283985, 0.061609339611734336, 0.06665165155058053, 0.05368471378548046, 0.0645827478804455, 0.06480176900879302, 0.059597870547129776, 0.061793410119713434, 0.0651768817797503, 0.059445989098693475, 0.05843224350207582, 0.055741361994564705, 0.0605834554407086, 0.05579048483563073, 0.060637302596816114, 0.05183855921978521, 0.05402148564930414, 0.05044169579288921, 0.04813945728082613, 0.051890119776743994, 0.049364618354909566, 0.05335679710613368, 0.049568197104270975, 0.048641696155945054, 0.05049788592274275, 0.0511749195374428, 0.052078019622692324, 0.04464422019968474, 0.05880930064025043, 0.06357177562125717, 0.05180961010790218, 0.05168419023897546, 0.05381155351232637, 0.05287081748713916, 0.047612315768467244, 0.05509846508988401, 0.04455264489605319, 0.05327586620549423, 0.050138992710066325, 0.056255134295631566, 0.05384514220465008, 0.04568374935150624, 0.04879387206428407, 0.04901202952648192, 0.06726372854749357, 0.045093601209420395, 0.0528359286017532, 0.049297139666579656, 0.052956641181536056, 0.055151743645611866, 0.04327081063571632, 0.05733768548221311, 0.05474526314803524, 0.04663403548627939, 0.04478863208586075, 0.04347074035750701, 0.04741002338084343, 0.05328869459014715, 0.05527946857630258, 0.05519191775031742, 0.05421362709370252, 0.04562970853444852, 0.054224004478192304, 0.054130235893910666], 'val_acc': [0.8331036211001128, 0.8604184939230672, 0.8788372384412981, 0.8882345570730484, 0.9165518105500564, 0.9100363362987094, 0.9314622227791004, 0.9412354341561208, 0.9478762059892244, 0.9444931712817942, 0.9481268011527377, 0.9525122165142212, 0.9471244204986844, 0.9561458463851648, 0.9573988222027315, 0.9589023931838115, 0.9568976318757048, 0.9594035835108382, 0.9579000125297582, 0.9620348327277284, 0.9634131061270518, 0.9672973311615086, 0.9604059641648917, 0.9567723342939481, 0.960656559328405, 0.9642901891993485, 0.9611577496554317, 0.9652925698534018, 0.9667961408344818, 0.9659190577621852, 0.9640395940358351, 0.9646660819446184, 0.9627866182182684, 0.9659190577621852, 0.9635384037088084, 0.9700538779601554, 0.9699285803783987, 0.9689261997243453, 0.9685503069790753, 0.968675604560832, 0.970930961032452, 0.9715574489412354, 0.9698032827966421, 0.9660443553439418, 0.9666708432527252, 0.9721839368500188, 0.9620348327277284, 0.9636637012905651, 0.9676732239067786, 0.9685503069790753, 0.9627866182182684, 0.9693020924696153, 0.9700538779601554, 0.9669214384162386, 0.9724345320135321, 0.9700538779601554, 0.9713068537777221, 0.9655431650169152, 0.9677985214885353, 0.9715574489412354, 0.9698032827966421, 0.9724345320135321, 0.9513845382784112, 0.9720586392682621, 0.9721839368500188, 0.9699285803783987, 0.9655431650169152, 0.9705550682871821, 0.9745645909033955, 0.970179175541912, 0.9693020924696153, 0.9716827465229921, 0.9703044731236687, 0.9744392933216389, 0.9715574489412354, 0.970179175541912, 0.9688009021425886, 0.9637889988723217, 0.968675604560832, 0.9716827465229921, 0.9695526876331286, 0.9671720335797519], 'size_bytes': 371627, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 82, 'batch_size': 128, 'lr': 0.003339952488844921, 'weight_decay': 0.008325410566280878, 'patch_size': 100, 'n_heads': 4, 'head_dim': 20, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.05588120569436496, 'attn_dropout': 0.29523348961431206, 'augment_noise_std': 0.006203877468326281, 'augment_scale_low': 0.8679643432695774, 'augment_scale_high': 1.185452489813183, 'use_focal_loss': True, 'focal_gamma': 1.3616547952137026, 'class_weighted': False, 'grad_clip': 1.6070974345456348, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 69440, 'model_storage_size_kb': 74.59375, 'model_size_validation': 'PASS'}
2025-10-02 21:23:19,142 - INFO - _models.training_function_executor - BO Objective: base=0.9672, size_penalty=0.0000, final=0.9672
2025-10-02 21:23:19,143 - INFO - _models.training_function_executor - Model: 69,440 parameters, 74.6KB (PASS 256KB limit)
2025-10-02 21:23:19,143 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 84.914s
2025-10-02 21:23:19,266 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9672
2025-10-02 21:23:19,267 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.124s
2025-10-02 21:23:19,267 - INFO - bo.run_bo - Recorded observation #43: hparams={'epochs': np.int64(82), 'batch_size': np.int64(128), 'lr': 0.003339952488844921, 'weight_decay': 0.008325410566280878, 'patch_size': np.int64(100), 'n_heads': np.int64(4), 'head_dim': np.int64(20), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.05588120569436496, 'attn_dropout': 0.29523348961431206, 'augment_noise_std': 0.006203877468326281, 'augment_scale_low': 0.8679643432695774, 'augment_scale_high': 1.185452489813183, 'use_focal_loss': np.True_, 'focal_gamma': 1.3616547952137026, 'class_weighted': np.False_, 'grad_clip': 1.6070974345456348, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9672
2025-10-02 21:23:19,267 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 43: {'epochs': np.int64(82), 'batch_size': np.int64(128), 'lr': 0.003339952488844921, 'weight_decay': 0.008325410566280878, 'patch_size': np.int64(100), 'n_heads': np.int64(4), 'head_dim': np.int64(20), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.05588120569436496, 'attn_dropout': 0.29523348961431206, 'augment_noise_std': 0.006203877468326281, 'augment_scale_low': 0.8679643432695774, 'augment_scale_high': 1.185452489813183, 'use_focal_loss': np.True_, 'focal_gamma': 1.3616547952137026, 'class_weighted': np.False_, 'grad_clip': 1.6070974345456348, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9672
2025-10-02 21:23:19,267 - INFO - bo.run_bo - üîçBO Trial 44: Using RF surrogate + Expected Improvement
2025-10-02 21:23:19,267 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:23:19,267 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:23:19,267 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:23:19,267 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 92, 'batch_size': 64, 'lr': 0.00035130181889329507, 'weight_decay': 0.007652702984178144, 'patch_size': 125, 'n_heads': 3, 'head_dim': 24, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.048812349192774065, 'attn_dropout': 0.20963102632497863, 'augment_noise_std': 0.033245652549935355, 'augment_scale_low': 0.9363750664559771, 'augment_scale_high': 1.172625933100774, 'use_focal_loss': False, 'focal_gamma': 3.6006100133526906, 'class_weighted': False, 'grad_clip': 1.6357025099207259, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:23:19,269 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 92, 'batch_size': 64, 'lr': 0.00035130181889329507, 'weight_decay': 0.007652702984178144, 'patch_size': 125, 'n_heads': 3, 'head_dim': 24, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.048812349192774065, 'attn_dropout': 0.20963102632497863, 'augment_noise_std': 0.033245652549935355, 'augment_scale_low': 0.9363750664559771, 'augment_scale_high': 1.172625933100774, 'use_focal_loss': False, 'focal_gamma': 3.6006100133526906, 'class_weighted': False, 'grad_clip': 1.6357025099207259, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:25:04,498 - INFO - _models.training_function_executor - Model: 40,032 parameters, 172.0KB storage
2025-10-02 21:25:04,498 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6753845147561434, 0.43975611700844514, 0.33700330944317153, 0.28284488655296525, 0.25074001499524196, 0.23007415370013307, 0.2115609381739177, 0.19835310576348414, 0.18636921053112282, 0.17942689038507137, 0.1715128726519032, 0.16501918416845468, 0.1554779983797651, 0.1507998346447799, 0.1470422334960875, 0.14332504933175708, 0.13659937482224413, 0.13113342399545522, 0.12554989331173305, 0.12425015497413214, 0.11948842769188492, 0.11439146709928341, 0.11301720174385918, 0.11004981737421318, 0.10744276626718652, 0.10248055143313733, 0.10187507341891697, 0.09717458995445455, 0.09432509076363711, 0.0959112968396638, 0.09246340386878278, 0.08667636882600467, 0.0870834903064705, 0.0841588179946097, 0.08458015513856829, 0.07769241592899412, 0.0797551146041248, 0.07686526829955222, 0.07653606890664903, 0.07221985581160784, 0.07191168241914911, 0.06698055469993137, 0.068642697266083, 0.0687512026138911, 0.0645299063684427, 0.06394079826715965, 0.06346059353614934, 0.060030619735161064, 0.05943508556393768, 0.05946432890660653, 0.058471862550264625, 0.055045677049937555, 0.05646916370578298, 0.0532149922705892, 0.05457678607833949, 0.04886194921636006, 0.050074374623027426, 0.04857142200247184, 0.05019540684603716, 0.0467066794983487, 0.04621959957377458, 0.04522700826990307, 0.04377559541907222, 0.046548535610647965, 0.04227543582940402, 0.04249588545581385, 0.04098387879673341, 0.040656067195349, 0.04066970421114394, 0.04092699153977737, 0.039779189321679925, 0.03951821567849421, 0.038847583251865915, 0.038898612882276674, 0.03799540962988542, 0.039172357930371056, 0.037645875649857025, 0.0328248834213942, 0.032095039231597136, 0.03374305164821137, 0.0348456495744352, 0.03297838465194754, 0.031111872418526962, 0.03384312060235235, 0.032188764937559025, 0.032847609686350976, 0.030922199502091127, 0.033400918750486064, 0.032653901396749355, 0.03262524962784306, 0.029915486800687147, 0.029007404513168838], 'val_losses': [0.5115639399450056, 0.36478500736042035, 0.307797110615824, 0.26482989751535346, 0.23154450717568023, 0.2225467699849804, 0.21669402934191684, 0.22456475496273443, 0.19908096059986918, 0.19274524498211382, 0.18296691644066274, 0.19032580786637981, 0.17868336054775713, 0.17330284969517798, 0.1731475677731909, 0.17776828959291105, 0.16508224276601036, 0.15503710689639974, 0.1697101995055799, 0.16185826101617964, 0.17173524446810656, 0.15380101029309184, 0.16039184216807142, 0.14272994302654757, 0.15409074900346267, 0.15405612716458522, 0.15316260623507685, 0.14616711191434398, 0.15797206719870735, 0.1480829770620896, 0.1496884768849521, 0.14875139734631806, 0.14272202301736375, 0.158815281143345, 0.1396527470181332, 0.14638789597272783, 0.149148500037737, 0.13630093124602585, 0.14250913879063595, 0.1452556464878513, 0.14356832547364656, 0.14103646115049237, 0.1442018290922137, 0.14305784007262023, 0.15128079070556194, 0.14220396646535005, 0.1469665995319749, 0.14427771034761713, 0.14013790118636052, 0.14254537135149778, 0.14237735141057029, 0.15626630172471387, 0.1459274667872625, 0.14789507162517182, 0.16123955635300943, 0.1462998647555987, 0.14612056516589803, 0.153658802309381, 0.1480364516002166, 0.1409441537015227, 0.16135200684851175, 0.15365213181512158, 0.14152226846792573, 0.1513961474708172, 0.15161531408946477, 0.1504352098922953, 0.15658655934257026, 0.16350528912434614, 0.16750274808831628, 0.15011294601727154, 0.15833874239298412, 0.16081982003374365, 0.15939676988290644, 0.167041516602002, 0.15933426517081611, 0.15045496853703977, 0.157672844436022, 0.1591867834139163, 0.1557072270313849, 0.15859892293428093, 0.17339769917470355, 0.1645143378155824, 0.15844530281290467, 0.16172639290919924, 0.16025417125280333, 0.1606076816697868, 0.1579340442300007, 0.16620641238049239, 0.1539699962161236, 0.16638868884729005, 0.16511428372787662, 0.1617250534677928], 'val_acc': [0.815060769327152, 0.8879839619095351, 0.9023931838115524, 0.92469615336424, 0.9325899010149105, 0.9325899010149105, 0.9340934719959905, 0.9324646034331537, 0.9409848389926074, 0.9423631123919308, 0.9451196591905776, 0.9426137075554442, 0.9456208495176043, 0.9463726350081444, 0.9512592406966546, 0.9467485277534143, 0.9523869189324646, 0.9535145971682747, 0.9490038842250345, 0.9507580503696279, 0.9473750156621977, 0.9532640020047614, 0.9513845382784112, 0.9575241197844881, 0.9532640020047614, 0.9531387044230046, 0.9547675729858414, 0.9556446560581381, 0.9527628116777346, 0.9572735246209748, 0.9540157874953014, 0.9590276907655683, 0.9586517980202982, 0.9522616213507079, 0.9595288810925949, 0.9563964415486781, 0.9587770956020549, 0.9600300714196216, 0.9599047738378649, 0.9573988222027315, 0.9570229294574615, 0.9610324520736749, 0.9576494173662449, 0.9570229294574615, 0.9587770956020549, 0.960656559328405, 0.9581506076932715, 0.9616589399824583, 0.9631625109635384, 0.9600300714196216, 0.9615336424007017, 0.9563964415486781, 0.9607818569101616, 0.9582759052750282, 0.9596541786743515, 0.9610324520736749, 0.9602806665831349, 0.9600300714196216, 0.9615336424007017, 0.9611577496554317, 0.9579000125297582, 0.9590276907655683, 0.9659190577621852, 0.9599047738378649, 0.9599047738378649, 0.9622854278912417, 0.9581506076932715, 0.9620348327277284, 0.9612830472371883, 0.9631625109635384, 0.9622854278912417, 0.9615336424007017, 0.9610324520736749, 0.9645407843628618, 0.9615336424007017, 0.9642901891993485, 0.9630372133817817, 0.9636637012905651, 0.9652925698534018, 0.9624107254729983, 0.9611577496554317, 0.9642901891993485, 0.9659190577621852, 0.9635384037088084, 0.9671720335797519, 0.9644154867811051, 0.9651672722716451, 0.960656559328405, 0.9650419746898885, 0.9629119158000251, 0.9645407843628618, 0.9657937601804285], 'size_bytes': 200987, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 92, 'batch_size': 64, 'lr': 0.00035130181889329507, 'weight_decay': 0.007652702984178144, 'patch_size': 125, 'n_heads': 3, 'head_dim': 24, 'num_layers': 1, 'mlp_ratio': 3, 'dropout': 0.048812349192774065, 'attn_dropout': 0.20963102632497863, 'augment_noise_std': 0.033245652549935355, 'augment_scale_low': 0.9363750664559771, 'augment_scale_high': 1.172625933100774, 'use_focal_loss': False, 'focal_gamma': 3.6006100133526906, 'class_weighted': False, 'grad_clip': 1.6357025099207259, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 40032, 'model_storage_size_kb': 172.01250000000002, 'model_size_validation': 'PASS'}
2025-10-02 21:25:04,498 - INFO - _models.training_function_executor - BO Objective: base=0.9658, size_penalty=0.0000, final=0.9658
2025-10-02 21:25:04,498 - INFO - _models.training_function_executor - Model: 40,032 parameters, 172.0KB (PASS 256KB limit)
2025-10-02 21:25:04,498 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 105.231s
2025-10-02 21:25:04,617 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9658
2025-10-02 21:25:04,617 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.119s
2025-10-02 21:25:04,617 - INFO - bo.run_bo - Recorded observation #44: hparams={'epochs': np.int64(92), 'batch_size': np.int64(64), 'lr': 0.00035130181889329507, 'weight_decay': 0.007652702984178144, 'patch_size': np.int64(125), 'n_heads': np.int64(3), 'head_dim': np.int64(24), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(3), 'dropout': 0.048812349192774065, 'attn_dropout': 0.20963102632497863, 'augment_noise_std': 0.033245652549935355, 'augment_scale_low': 0.9363750664559771, 'augment_scale_high': 1.172625933100774, 'use_focal_loss': np.False_, 'focal_gamma': 3.6006100133526906, 'class_weighted': np.False_, 'grad_clip': 1.6357025099207259, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9658
2025-10-02 21:25:04,617 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 44: {'epochs': np.int64(92), 'batch_size': np.int64(64), 'lr': 0.00035130181889329507, 'weight_decay': 0.007652702984178144, 'patch_size': np.int64(125), 'n_heads': np.int64(3), 'head_dim': np.int64(24), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(3), 'dropout': 0.048812349192774065, 'attn_dropout': 0.20963102632497863, 'augment_noise_std': 0.033245652549935355, 'augment_scale_low': 0.9363750664559771, 'augment_scale_high': 1.172625933100774, 'use_focal_loss': np.False_, 'focal_gamma': 3.6006100133526906, 'class_weighted': np.False_, 'grad_clip': 1.6357025099207259, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9658
2025-10-02 21:25:04,618 - INFO - bo.run_bo - üîçBO Trial 45: Using RF surrogate + Expected Improvement
2025-10-02 21:25:04,618 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:25:04,618 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:25:04,618 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:25:04,618 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 94, 'batch_size': 256, 'lr': 0.0007435172623216507, 'weight_decay': 0.0003940781285146156, 'patch_size': 10, 'n_heads': 4, 'head_dim': 26, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.0748812600250602, 'attn_dropout': 0.2948975124461465, 'augment_noise_std': 0.013105117352154786, 'augment_scale_low': 0.9065223333329082, 'augment_scale_high': 1.1404999574503445, 'use_focal_loss': True, 'focal_gamma': 2.455853823491135, 'class_weighted': False, 'grad_clip': 3.0599438043880833, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:25:04,619 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 94, 'batch_size': 256, 'lr': 0.0007435172623216507, 'weight_decay': 0.0003940781285146156, 'patch_size': 10, 'n_heads': 4, 'head_dim': 26, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.0748812600250602, 'attn_dropout': 0.2948975124461465, 'augment_noise_std': 0.013105117352154786, 'augment_scale_low': 0.9065223333329082, 'augment_scale_high': 1.1404999574503445, 'use_focal_loss': True, 'focal_gamma': 2.455853823491135, 'class_weighted': False, 'grad_clip': 3.0599438043880833, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:29:11,499 - INFO - _models.training_function_executor - Model: 100,880 parameters, 108.4KB storage
2025-10-02 21:29:11,499 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.23022689440969768, 0.1008509602089019, 0.07334848455061303, 0.06021720534261429, 0.05397580680788204, 0.047202806804283994, 0.04583719863456521, 0.041564338565578986, 0.039389112317367546, 0.03524502483769097, 0.034100891562294317, 0.030431102918678544, 0.03022486131435354, 0.029396789020842688, 0.026951947312167133, 0.02679320784067501, 0.02619571070725434, 0.02367220555204846, 0.02126367877000343, 0.021118408381526763, 0.02112079774745863, 0.021003665087131182, 0.02120634580580501, 0.019428940866291534, 0.01758027158242185, 0.016578054081186355, 0.019465422435509222, 0.015792430743376246, 0.01453000376196408, 0.014738509089882638, 0.014476612750357982, 0.015168126713349975, 0.014417084027481411, 0.01303431551840587, 0.012968493838375386, 0.014579793125190522, 0.012337779551944011, 0.011986032040374343, 0.011734537059803025, 0.011763286489388965, 0.009545931050180081, 0.011135223719273273, 0.010617771314329451, 0.011431045555809682, 0.009175737889087562, 0.009151012995736754, 0.009822443173639587, 0.00973395258161991, 0.010892171602570966, 0.009531915918324241, 0.007371596960088509, 0.010397016563306414, 0.007883964505482934, 0.012315817458489578, 0.008334150547300858, 0.0062229712869260375, 0.0061365462275465195, 0.005658547112448248, 0.006172427695680527, 0.0083538769302846, 0.009420676467056764, 0.006924172563542708, 0.0086706847599584, 0.0051014789234656846, 0.005743103772181254, 0.005808627988793745, 0.0053869770166094485, 0.008590767834284453, 0.0074410050260426165, 0.006080431373921131, 0.005162825367707506, 0.006653378850080572, 0.006561850218443497, 0.005565280584534632, 0.00427391329521068, 0.0044851967677123, 0.007115530523928721, 0.007237559946253487, 0.0033766276209105765, 0.004846412840840516, 0.005966164242048884, 0.003709212619825825, 0.004747248002959101, 0.007692108519978176, 0.0062850441982808545, 0.004106781838169841, 0.003350035874217098, 0.003877323889261819, 0.006306211822247435, 0.005018874397995849, 0.0037485589444434335, 0.004627031557375956, 0.005709545352740014, 0.005252401608328062], 'val_losses': [0.12607536741163988, 0.08120141521725435, 0.0694489224824731, 0.07843026938447825, 0.05988535429134567, 0.06313842922251202, 0.05667110550632872, 0.056126832047167195, 0.05664561210834297, 0.050084588131783916, 0.05188985350974116, 0.05513355423074485, 0.05097422547066156, 0.04950721255147566, 0.05654820489892385, 0.057360727581303006, 0.051666735109119635, 0.051761432541162056, 0.05370319079686609, 0.05033929167482701, 0.051835410953195396, 0.05266936715422022, 0.05774167050299318, 0.05826322993693264, 0.049445236663504095, 0.04701899515030573, 0.04681057534022463, 0.05535803435029817, 0.051460433664992256, 0.05165877569148959, 0.051834444519257936, 0.05467583256068945, 0.05477623907923176, 0.05410361576014601, 0.0635384815820789, 0.052849094973532365, 0.05330660195185448, 0.07126160316069373, 0.05255165942972009, 0.051281540531022345, 0.048125385931897115, 0.05491034797283211, 0.052734737284364834, 0.0491921185659236, 0.06646676832251554, 0.0632747801947513, 0.05700176998519192, 0.057746804448734056, 0.05576276844335878, 0.05096152033845877, 0.0658953579005226, 0.057184353095519336, 0.057833647105482014, 0.052096333963846206, 0.05574238339905571, 0.0514511874692659, 0.05722016723453812, 0.0543060640563733, 0.0707907017972472, 0.06774229301372398, 0.05325206496963195, 0.06198290328739908, 0.05375865998759244, 0.0586317196785246, 0.056230608229648496, 0.06331943603793327, 0.05963686408255426, 0.0637068104088986, 0.05390492051022048, 0.05299150079671728, 0.06280420341560108, 0.05899427173960972, 0.05417532545591772, 0.05897940959664249, 0.06569997231967167, 0.06383810691510268, 0.06943619946551613, 0.05320999484293115, 0.06171817071395117, 0.06162213411169563, 0.0568155114094486, 0.06094605110183334, 0.07440623912487453, 0.05922103524880217, 0.056852367817571386, 0.06063013424208373, 0.06042201828984984, 0.0596943135182969, 0.05942784498290839, 0.05963191309566412, 0.05657290353906168, 0.06974755669690182, 0.06352364862716492, 0.060194071845786644], 'val_acc': [0.8967547926325022, 0.9345946623230171, 0.9404836486655808, 0.9238190702919433, 0.9371006139581506, 0.9408595414108508, 0.9329657937601804, 0.9452449567723343, 0.9452449567723343, 0.9523869189324646, 0.9481268011527377, 0.9459967422628743, 0.9486279914797644, 0.955268763312868, 0.9371006139581506, 0.9452449567723343, 0.9527628116777346, 0.9538904899135446, 0.9456208495176043, 0.9511339431148979, 0.9563964415486781, 0.9506327527878712, 0.9360982333040972, 0.939606565593284, 0.953765192331788, 0.9587770956020549, 0.9590276907655683, 0.9424884099736875, 0.9521363237689513, 0.9561458463851648, 0.9573988222027315, 0.9508833479513845, 0.9488785866432777, 0.953765192331788, 0.9398571607567974, 0.9482520987344945, 0.9585265004385415, 0.9239443678737, 0.9566470367121914, 0.9592782859290816, 0.9602806665831349, 0.9568976318757048, 0.9560205488034081, 0.9599047738378649, 0.9502568600426011, 0.9525122165142212, 0.945746147099361, 0.9503821576243578, 0.9565217391304348, 0.9547675729858414, 0.9542663826588147, 0.9556446560581381, 0.9557699536398947, 0.9592782859290816, 0.9610324520736749, 0.9651672722716451, 0.9510086455331412, 0.9619095351459717, 0.9473750156621977, 0.9495050745520611, 0.9599047738378649, 0.952637514095978, 0.9624107254729983, 0.9590276907655683, 0.9644154867811051, 0.9576494173662449, 0.961784237564215, 0.953389299586518, 0.9619095351459717, 0.9616589399824583, 0.9553940608946248, 0.9546422754040848, 0.9586517980202982, 0.960656559328405, 0.9604059641648917, 0.9555193584763814, 0.9523869189324646, 0.9660443553439418, 0.9662949505074552, 0.9563964415486781, 0.9581506076932715, 0.9561458463851648, 0.9565217391304348, 0.9626613206365117, 0.9585265004385415, 0.9589023931838115, 0.9567723342939481, 0.9651672722716451, 0.9535145971682747, 0.963287808545295, 0.9644154867811051, 0.9596541786743515, 0.9576494173662449, 0.9631625109635384], 'size_bytes': 506795, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 94, 'batch_size': 256, 'lr': 0.0007435172623216507, 'weight_decay': 0.0003940781285146156, 'patch_size': 10, 'n_heads': 4, 'head_dim': 26, 'num_layers': 2, 'mlp_ratio': 2, 'dropout': 0.0748812600250602, 'attn_dropout': 0.2948975124461465, 'augment_noise_std': 0.013105117352154786, 'augment_scale_low': 0.9065223333329082, 'augment_scale_high': 1.1404999574503445, 'use_focal_loss': True, 'focal_gamma': 2.455853823491135, 'class_weighted': False, 'grad_clip': 3.0599438043880833, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 100880, 'model_storage_size_kb': 108.36718750000001, 'model_size_validation': 'PASS'}
2025-10-02 21:29:11,499 - INFO - _models.training_function_executor - BO Objective: base=0.9632, size_penalty=0.0000, final=0.9632
2025-10-02 21:29:11,499 - INFO - _models.training_function_executor - Model: 100,880 parameters, 108.4KB (PASS 256KB limit)
2025-10-02 21:29:11,499 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 246.881s
2025-10-02 21:29:11,616 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9632
2025-10-02 21:29:11,617 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.117s
2025-10-02 21:29:11,617 - INFO - bo.run_bo - Recorded observation #45: hparams={'epochs': np.int64(94), 'batch_size': np.int64(256), 'lr': 0.0007435172623216507, 'weight_decay': 0.0003940781285146156, 'patch_size': np.int64(10), 'n_heads': np.int64(4), 'head_dim': np.int64(26), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.0748812600250602, 'attn_dropout': 0.2948975124461465, 'augment_noise_std': 0.013105117352154786, 'augment_scale_low': 0.9065223333329082, 'augment_scale_high': 1.1404999574503445, 'use_focal_loss': np.True_, 'focal_gamma': 2.455853823491135, 'class_weighted': np.False_, 'grad_clip': 3.0599438043880833, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9632
2025-10-02 21:29:11,617 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 45: {'epochs': np.int64(94), 'batch_size': np.int64(256), 'lr': 0.0007435172623216507, 'weight_decay': 0.0003940781285146156, 'patch_size': np.int64(10), 'n_heads': np.int64(4), 'head_dim': np.int64(26), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'dropout': 0.0748812600250602, 'attn_dropout': 0.2948975124461465, 'augment_noise_std': 0.013105117352154786, 'augment_scale_low': 0.9065223333329082, 'augment_scale_high': 1.1404999574503445, 'use_focal_loss': np.True_, 'focal_gamma': 2.455853823491135, 'class_weighted': np.False_, 'grad_clip': 3.0599438043880833, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9632
2025-10-02 21:29:11,617 - INFO - bo.run_bo - üîçBO Trial 46: Using RF surrogate + Expected Improvement
2025-10-02 21:29:11,617 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:29:11,617 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:29:11,617 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:29:11,617 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 87, 'batch_size': 512, 'lr': 0.00041788216889369033, 'weight_decay': 0.0013947051117803747, 'patch_size': 100, 'n_heads': 4, 'head_dim': 11, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.061020738914162945, 'attn_dropout': 0.275780784558357, 'augment_noise_std': 0.04011333687945228, 'augment_scale_low': 0.8581093640631616, 'augment_scale_high': 1.184645136171719, 'use_focal_loss': True, 'focal_gamma': 3.0789761049501148, 'class_weighted': False, 'grad_clip': 0.9547740981619975, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:29:11,618 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 87, 'batch_size': 512, 'lr': 0.00041788216889369033, 'weight_decay': 0.0013947051117803747, 'patch_size': 100, 'n_heads': 4, 'head_dim': 11, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.061020738914162945, 'attn_dropout': 0.275780784558357, 'augment_noise_std': 0.04011333687945228, 'augment_scale_low': 0.8581093640631616, 'augment_scale_high': 1.184645136171719, 'use_focal_loss': True, 'focal_gamma': 3.0789761049501148, 'class_weighted': False, 'grad_clip': 0.9547740981619975, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:30:15,210 - INFO - _models.training_function_executor - Model: 49,329 parameters, 212.0KB storage
2025-10-02 21:30:15,210 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3459217785608909, 0.20609000310359196, 0.16573970307730024, 0.1382596036709554, 0.1181444880428221, 0.1040923159708487, 0.09392899991503613, 0.08575414747108968, 0.07869403636925494, 0.07414999730770433, 0.06821151198189412, 0.06602208802248793, 0.06341969669009907, 0.06034219496964351, 0.0563137574238694, 0.05374314995486123, 0.05277523870691898, 0.05095159883930622, 0.04931135471446138, 0.04613628250237538, 0.04454991607143609, 0.04404276853175636, 0.042825507097704996, 0.04104175719703005, 0.04091169057078493, 0.03910712616854983, 0.03786056792897467, 0.03708932820622488, 0.03653318026388897, 0.035450768948097976, 0.03373600513283766, 0.032629005376617025, 0.03259724791841792, 0.0319818487801121, 0.032003818933403, 0.030726048795308148, 0.030592885071551355, 0.029489900260515813, 0.02924147976177692, 0.0281124845831789, 0.027693000500758885, 0.027068960945863637, 0.02562570227867577, 0.025118741479631137, 0.025621477837753644, 0.025931088879813703, 0.02458566058978718, 0.023947526669782702, 0.022825773870846392, 0.024823788242947953, 0.022876554668074308, 0.021692758591837055, 0.02190618158993452, 0.021718244603463808, 0.02101085282106229, 0.020711776393213373, 0.020434199747454217, 0.01991309104487008, 0.019077058697374746, 0.01866045228670626, 0.018283674545478516, 0.018642049712785266, 0.019445190606204386, 0.01890506290428683, 0.017594679872556217, 0.017795538704564476, 0.017182321191388118, 0.017206795363004394, 0.016695428945044097, 0.01670871908164595, 0.01651405886910697, 0.01626512011130876, 0.015925231768934588, 0.016038390788056626, 0.01565892154961956, 0.014568798421377586, 0.014560840957463525, 0.014016720108729655, 0.014205523650785517, 0.01391637437052576, 0.014189791486471965, 0.014338538124111759, 0.01443969365346459, 0.013345940161909622, 0.01388356520536171, 0.013343621968709092, 0.013157556298843034], 'val_losses': [0.23400167593224397, 0.17996746107101322, 0.14393863491942718, 0.12086044909037409, 0.11017209949510287, 0.09143812060188103, 0.08607831944511612, 0.0778284553266187, 0.0746129660208659, 0.06697350942123999, 0.06586638356719664, 0.06193519169141469, 0.06001452590568653, 0.059760077934548336, 0.056006036331470326, 0.05548997776032749, 0.06058128794115851, 0.054388465089780645, 0.05094298591017873, 0.047795731629375796, 0.05039506079833275, 0.04764455736949321, 0.04778054982793464, 0.04711454971138584, 0.04393515885604412, 0.04815708785547139, 0.0444678929832186, 0.04486771975384292, 0.04265866756706201, 0.04114709639273675, 0.04228812362636977, 0.04368981844438092, 0.042746328199724556, 0.04389361279380977, 0.04151641337935928, 0.04204275244663733, 0.039585463036131315, 0.038291478169926306, 0.04048327953715609, 0.03951549468742414, 0.040579821120225505, 0.03787167443462204, 0.039809964398848, 0.03929621795263077, 0.03771432955786558, 0.0394004555828678, 0.036937897262214685, 0.03833817276982769, 0.036486426374190375, 0.03602585887057734, 0.03713419190818595, 0.03802532721915321, 0.03776656355848402, 0.037166627698779, 0.03747520272317439, 0.038323662848600525, 0.035920373950507865, 0.03422312053939653, 0.0378480978340915, 0.03720153368544592, 0.03611084321076642, 0.03924596111284672, 0.0375033781896773, 0.04076320597804619, 0.03661022955418014, 0.03576166498246176, 0.03536414672909875, 0.03854617015037967, 0.034489247125482785, 0.03630585391613538, 0.04026167199847401, 0.035099002692879716, 0.037404750404365306, 0.0363993794073711, 0.03987613473264455, 0.035018862101071205, 0.03617633558933918, 0.035035684797901166, 0.03361665514969815, 0.0367414613833513, 0.036623899892015484, 0.039126110149393656, 0.036929531938475396, 0.03636475569604632, 0.03459428097549096, 0.03468591372355634, 0.03487760177641349], 'val_acc': [0.7760932214008269, 0.8087958902393184, 0.8341060017541662, 0.869690514973061, 0.875454203733868, 0.9035208620473625, 0.9094098483899261, 0.9203107380027565, 0.9258238316000501, 0.9270768074176169, 0.9357223405588272, 0.939230672848014, 0.9376018042851773, 0.936975316376394, 0.9418619220649042, 0.9426137075554442, 0.9357223405588272, 0.9412354341561208, 0.9458714446811176, 0.9506327527878712, 0.9468738253351711, 0.9512592406966546, 0.9491291818067912, 0.9512592406966546, 0.9532640020047614, 0.9488785866432777, 0.9507580503696279, 0.9522616213507079, 0.9528881092594913, 0.9542663826588147, 0.9567723342939481, 0.9497556697155745, 0.9531387044230046, 0.9522616213507079, 0.9568976318757048, 0.951885728605438, 0.9523869189324646, 0.9571482270392181, 0.955268763312868, 0.9547675729858414, 0.9517604310236812, 0.9570229294574615, 0.9496303721338178, 0.9542663826588147, 0.9558952512216514, 0.9525122165142212, 0.9551434657311114, 0.9538904899135446, 0.9611577496554317, 0.9611577496554317, 0.9551434657311114, 0.9540157874953014, 0.9512592406966546, 0.9571482270392181, 0.9546422754040848, 0.9562711439669215, 0.960656559328405, 0.9586517980202982, 0.9571482270392181, 0.9586517980202982, 0.9585265004385415, 0.9585265004385415, 0.9566470367121914, 0.9506327527878712, 0.9567723342939481, 0.9616589399824583, 0.9563964415486781, 0.9551434657311114, 0.9619095351459717, 0.9594035835108382, 0.9556446560581381, 0.9630372133817817, 0.9587770956020549, 0.9594035835108382, 0.9585265004385415, 0.9636637012905651, 0.961784237564215, 0.9645407843628618, 0.9639142964540784, 0.9607818569101616, 0.9610324520736749, 0.9575241197844881, 0.9585265004385415, 0.9604059641648917, 0.9620348327277284, 0.9615336424007017, 0.9604059641648917], 'size_bytes': 206857, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 87, 'batch_size': 512, 'lr': 0.00041788216889369033, 'weight_decay': 0.0013947051117803747, 'patch_size': 100, 'n_heads': 4, 'head_dim': 11, 'num_layers': 2, 'mlp_ratio': 3, 'dropout': 0.061020738914162945, 'attn_dropout': 0.275780784558357, 'augment_noise_std': 0.04011333687945228, 'augment_scale_low': 0.8581093640631616, 'augment_scale_high': 1.184645136171719, 'use_focal_loss': True, 'focal_gamma': 3.0789761049501148, 'class_weighted': False, 'grad_clip': 0.9547740981619975, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 49329, 'model_storage_size_kb': 211.960546875, 'model_size_validation': 'PASS'}
2025-10-02 21:30:15,210 - INFO - _models.training_function_executor - BO Objective: base=0.9604, size_penalty=0.0000, final=0.9604
2025-10-02 21:30:15,210 - INFO - _models.training_function_executor - Model: 49,329 parameters, 212.0KB (PASS 256KB limit)
2025-10-02 21:30:15,210 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 63.593s
2025-10-02 21:30:15,336 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9604
2025-10-02 21:30:15,336 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.126s
2025-10-02 21:30:15,336 - INFO - bo.run_bo - Recorded observation #46: hparams={'epochs': np.int64(87), 'batch_size': np.int64(512), 'lr': 0.00041788216889369033, 'weight_decay': 0.0013947051117803747, 'patch_size': np.int64(100), 'n_heads': np.int64(4), 'head_dim': np.int64(11), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.061020738914162945, 'attn_dropout': 0.275780784558357, 'augment_noise_std': 0.04011333687945228, 'augment_scale_low': 0.8581093640631616, 'augment_scale_high': 1.184645136171719, 'use_focal_loss': np.True_, 'focal_gamma': 3.0789761049501148, 'class_weighted': np.False_, 'grad_clip': 0.9547740981619975, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.9604
2025-10-02 21:30:15,336 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 46: {'epochs': np.int64(87), 'batch_size': np.int64(512), 'lr': 0.00041788216889369033, 'weight_decay': 0.0013947051117803747, 'patch_size': np.int64(100), 'n_heads': np.int64(4), 'head_dim': np.int64(11), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'dropout': 0.061020738914162945, 'attn_dropout': 0.275780784558357, 'augment_noise_std': 0.04011333687945228, 'augment_scale_low': 0.8581093640631616, 'augment_scale_high': 1.184645136171719, 'use_focal_loss': np.True_, 'focal_gamma': 3.0789761049501148, 'class_weighted': np.False_, 'grad_clip': 0.9547740981619975, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.9604
2025-10-02 21:30:15,337 - INFO - bo.run_bo - üîçBO Trial 47: Using RF surrogate + Expected Improvement
2025-10-02 21:30:15,337 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:30:15,337 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:30:15,337 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:30:15,337 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 89, 'batch_size': 128, 'lr': 0.0014293053249612915, 'weight_decay': 0.0005455831808434975, 'patch_size': 25, 'n_heads': 4, 'head_dim': 25, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.11665280264751174, 'attn_dropout': 0.2817122470474022, 'augment_noise_std': 0.04653740727914974, 'augment_scale_low': 0.9924608530999173, 'augment_scale_high': 1.1981566583996934, 'use_focal_loss': False, 'focal_gamma': 1.9386913038878493, 'class_weighted': False, 'grad_clip': 1.5714632690098338, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:30:15,338 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 89, 'batch_size': 128, 'lr': 0.0014293053249612915, 'weight_decay': 0.0005455831808434975, 'patch_size': 25, 'n_heads': 4, 'head_dim': 25, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.11665280264751174, 'attn_dropout': 0.2817122470474022, 'augment_noise_std': 0.04653740727914974, 'augment_scale_low': 0.9924608530999173, 'augment_scale_high': 1.1981566583996934, 'use_focal_loss': False, 'focal_gamma': 1.9386913038878493, 'class_weighted': False, 'grad_clip': 1.5714632690098338, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-10-02 21:33:05,060 - INFO - _models.training_function_executor - Model: 131,600 parameters, 565.5KB storage
2025-10-02 21:33:05,060 - WARNING - _models.training_function_executor - Model storage 565.5KB exceeds 256KB limit!
2025-10-02 21:33:05,060 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.38782304566078973, 0.20775998580206703, 0.17617879730649555, 0.15816708651621292, 0.15197657383853502, 0.13784169564102092, 0.1350865744270672, 0.12696106815819483, 0.11936468946079484, 0.11811172335682252, 0.11650467115694974, 0.11569898450329419, 0.10813248550253689, 0.10573369913059671, 0.10635193276110513, 0.09704578179669208, 0.09862853785733981, 0.09592123101478095, 0.09718936526588273, 0.09167521874336995, 0.09251393649822913, 0.08918728916673449, 0.09210872437852899, 0.08818493801351815, 0.08398050545517784, 0.08300187244520739, 0.08089651782521329, 0.08303701993204841, 0.08036296174881942, 0.07867209825570118, 0.07996410625720891, 0.0756501219223195, 0.07373809544861744, 0.07118304715674256, 0.07162781043264481, 0.07245031513370107, 0.06883688708014313, 0.0695855322611088, 0.06829489915930541, 0.066964749284937, 0.06667469374533715, 0.06637419512014552, 0.06780849966677044, 0.062315600434274365, 0.06710220406390499, 0.06230042112343959, 0.059431224155504635, 0.06767496650596425, 0.0571650153622477, 0.05926367730800594, 0.057388600901283164, 0.06198938478219555, 0.06101709597494931, 0.05683638890027436, 0.057814624485526885, 0.054736059788003374, 0.05654870783950707, 0.054896705055398884, 0.05593535885636312, 0.051557877419497015, 0.05288450973375003, 0.05046277996239578, 0.05437891275396406, 0.052493595145590245, 0.051174626253173955, 0.0477590881979126, 0.05176647140828608, 0.0491020110805871, 0.04842958302283393, 0.04714106745368869, 0.04437883872105512, 0.049193107605790806, 0.044210284410846665, 0.04428534704239727, 0.04885813584015288, 0.0467201018771839, 0.043771343860655385, 0.052723940430015964, 0.045210995191009025, 0.040893496971348565, 0.04211306616608385, 0.04441040482814499, 0.04262297902248899, 0.04314896830751781, 0.04668706455566854, 0.037724708510303184, 0.04319570350697572, 0.041254323332847384, 0.039334323488557935], 'val_losses': [0.23907551519472847, 0.18113961209406637, 0.17980088578655432, 0.18203648153443697, 0.16583674234968127, 0.1383131095973147, 0.15185226326769358, 0.1573376140750122, 0.1365603367112929, 0.12846182836967668, 0.14959778913488486, 0.1282293799788025, 0.1215972967120045, 0.13526205707688244, 0.12925430657798345, 0.13685694429218254, 0.13599384521843572, 0.1275276744904158, 0.10918648932500227, 0.11496895067267753, 0.11927355906372156, 0.12468127549150178, 0.11619209030090216, 0.12387565051481668, 0.11743100266821954, 0.10507962614477107, 0.09737834426793908, 0.12908931521825093, 0.12775499104908303, 0.12437565354640853, 0.11360775943821705, 0.1109161282404221, 0.11918589034100371, 0.1089940649041734, 0.12385177928027824, 0.11921383234182256, 0.11962171684207959, 0.10730834073581846, 0.10707728046064552, 0.11614228397759037, 0.11396729280896062, 0.10461611622498009, 0.10685025769440008, 0.12398259941796105, 0.11144896697399088, 0.11176946560006559, 0.10804943365084918, 0.11428503046327329, 0.12014936937035332, 0.11848424415386137, 0.13223991949797842, 0.12337524477378899, 0.1041407951054276, 0.12684585477434712, 0.11458804842344637, 0.10653864383667637, 0.13425714473589337, 0.12287215548878459, 0.11205702785217228, 0.10677960440899223, 0.1253122574325463, 0.11146768084401072, 0.1276654072601594, 0.11432586912784103, 0.11122346650484645, 0.10325645208694839, 0.1217315630853557, 0.11185148066333804, 0.11542000216867762, 0.12091842154455192, 0.11440015052952962, 0.09950919776628406, 0.10902844889393098, 0.1201694454018028, 0.10241249661303123, 0.11838132800979433, 0.11355268627931683, 0.12913016770667501, 0.12450413272212343, 0.10870990882487752, 0.11626050652544714, 0.10940835392201788, 0.11956957461518491, 0.11720912820462251, 0.12213435621581838, 0.11734465682824712, 0.12005039486068335, 0.12046915536393737, 0.13842478834765698], 'val_acc': [0.9241949630372134, 0.9456208495176043, 0.9471244204986844, 0.9442425761182809, 0.9491291818067912, 0.9565217391304348, 0.9513845382784112, 0.9536398947500313, 0.9590276907655683, 0.960656559328405, 0.9528881092594913, 0.9601553690013783, 0.9630372133817817, 0.9582759052750282, 0.9609071544919183, 0.9587770956020549, 0.9610324520736749, 0.9601553690013783, 0.9665455456709685, 0.9624107254729983, 0.9664202480892119, 0.9619095351459717, 0.9639142964540784, 0.9586517980202982, 0.9665455456709685, 0.968675604560832, 0.9689261997243453, 0.9579000125297582, 0.9626613206365117, 0.9605312617466483, 0.9671720335797519, 0.9672973311615086, 0.9627866182182684, 0.9657937601804285, 0.9601553690013783, 0.9650419746898885, 0.961784237564215, 0.9688009021425886, 0.967547926325022, 0.9662949505074552, 0.9671720335797519, 0.9696779852148854, 0.9679238190702919, 0.9629119158000251, 0.9672973311615086, 0.9684250093973187, 0.967547926325022, 0.9647913795263752, 0.9646660819446184, 0.968299711815562, 0.9661696529256986, 0.9635384037088084, 0.9705550682871821, 0.9620348327277284, 0.9655431650169152, 0.9714321513594788, 0.9586517980202982, 0.9684250093973187, 0.969427390051372, 0.9691767948878587, 0.9703044731236687, 0.968675604560832, 0.9605312617466483, 0.9719333416865055, 0.9710562586142087, 0.969427390051372, 0.9671720335797519, 0.9696779852148854, 0.9736875078310988, 0.9719333416865055, 0.9705550682871821, 0.9723092344317754, 0.9724345320135321, 0.969051497306102, 0.9705550682871821, 0.962160130309485, 0.969051497306102, 0.9637889988723217, 0.9641648916175918, 0.970179175541912, 0.9660443553439418, 0.9695526876331286, 0.9693020924696153, 0.967547926325022, 0.9718080441047487, 0.9689261997243453, 0.9676732239067786, 0.969427390051372, 0.9662949505074552], 'size_bytes': 669627, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 89, 'batch_size': 128, 'lr': 0.0014293053249612915, 'weight_decay': 0.0005455831808434975, 'patch_size': 25, 'n_heads': 4, 'head_dim': 25, 'num_layers': 3, 'mlp_ratio': 2, 'dropout': 0.11665280264751174, 'attn_dropout': 0.2817122470474022, 'augment_noise_std': 0.04653740727914974, 'augment_scale_low': 0.9924608530999173, 'augment_scale_high': 1.1981566583996934, 'use_focal_loss': False, 'focal_gamma': 1.9386913038878493, 'class_weighted': False, 'grad_clip': 1.5714632690098338, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 131600, 'model_storage_size_kb': 565.46875, 'model_size_validation': 'FAIL'}
2025-10-02 21:33:05,060 - INFO - _models.training_function_executor - BO Objective: base=0.9663, size_penalty=0.6044, final=0.3619
2025-10-02 21:33:05,060 - INFO - _models.training_function_executor - Model: 131,600 parameters, 565.5KB (FAIL 256KB limit)
2025-10-02 21:33:05,060 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 169.724s
2025-10-02 21:33:05,179 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.3619
2025-10-02 21:33:05,179 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.119s
2025-10-02 21:33:05,179 - INFO - bo.run_bo - Recorded observation #47: hparams={'epochs': np.int64(89), 'batch_size': np.int64(128), 'lr': 0.0014293053249612915, 'weight_decay': 0.0005455831808434975, 'patch_size': np.int64(25), 'n_heads': np.int64(4), 'head_dim': np.int64(25), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.11665280264751174, 'attn_dropout': 0.2817122470474022, 'augment_noise_std': 0.04653740727914974, 'augment_scale_low': 0.9924608530999173, 'augment_scale_high': 1.1981566583996934, 'use_focal_loss': np.False_, 'focal_gamma': 1.9386913038878493, 'class_weighted': np.False_, 'grad_clip': 1.5714632690098338, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.3619
2025-10-02 21:33:05,179 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 47: {'epochs': np.int64(89), 'batch_size': np.int64(128), 'lr': 0.0014293053249612915, 'weight_decay': 0.0005455831808434975, 'patch_size': np.int64(25), 'n_heads': np.int64(4), 'head_dim': np.int64(25), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'dropout': 0.11665280264751174, 'attn_dropout': 0.2817122470474022, 'augment_noise_std': 0.04653740727914974, 'augment_scale_low': 0.9924608530999173, 'augment_scale_high': 1.1981566583996934, 'use_focal_loss': np.False_, 'focal_gamma': 1.9386913038878493, 'class_weighted': np.False_, 'grad_clip': 1.5714632690098338, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.3619
2025-10-02 21:33:05,180 - INFO - bo.run_bo - üîçBO Trial 48: Using RF surrogate + Expected Improvement
2025-10-02 21:33:05,180 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:33:05,180 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:33:05,180 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:33:05,180 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 92, 'batch_size': 128, 'lr': 0.0010797080314285743, 'weight_decay': 0.0006134050443355251, 'patch_size': 25, 'n_heads': 3, 'head_dim': 21, 'num_layers': 1, 'mlp_ratio': 4, 'dropout': 0.060334667975597125, 'attn_dropout': 0.24843835308886908, 'augment_noise_std': 0.048499244446687885, 'augment_scale_low': 0.9531678957424407, 'augment_scale_high': 1.177993538169219, 'use_focal_loss': True, 'focal_gamma': 1.7765779527720287, 'class_weighted': False, 'grad_clip': 2.4130789635007432, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:33:05,181 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 92, 'batch_size': 128, 'lr': 0.0010797080314285743, 'weight_decay': 0.0006134050443355251, 'patch_size': 25, 'n_heads': 3, 'head_dim': 21, 'num_layers': 1, 'mlp_ratio': 4, 'dropout': 0.060334667975597125, 'attn_dropout': 0.24843835308886908, 'augment_noise_std': 0.048499244446687885, 'augment_scale_low': 0.9531678957424407, 'augment_scale_high': 1.177993538169219, 'use_focal_loss': True, 'focal_gamma': 1.7765779527720287, 'class_weighted': False, 'grad_clip': 2.4130789635007432, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-10-02 21:34:41,376 - INFO - _models.training_function_executor - Model: 22,176 parameters, 23.8KB storage
2025-10-02 21:34:41,376 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.29327243904558387, 0.16760701321795243, 0.1382667397158272, 0.11954819128424685, 0.10382797852211294, 0.09548576708160815, 0.08696996219426088, 0.07902086386773624, 0.0750502900924744, 0.07055768360102367, 0.07131321094394537, 0.0662303448773187, 0.0635076071457285, 0.06409423468632794, 0.05966523866929518, 0.0552377139855575, 0.055138885886194984, 0.05241049671009348, 0.051244905837146505, 0.05003735138699625, 0.049548849853227876, 0.048139995971032745, 0.04488942270286698, 0.044687016376306475, 0.046029136971531256, 0.04195125437370892, 0.04154660733222319, 0.041124825747466884, 0.040053948584871775, 0.03742603136433262, 0.03862590727542209, 0.03632346138589641, 0.035522519051060075, 0.03416587912668614, 0.03522712345783025, 0.033297416801124656, 0.03413780670469164, 0.03138086894417442, 0.03197559162933981, 0.030671190840383784, 0.03025539751653776, 0.030119491319729862, 0.028306367826782752, 0.02786407159893361, 0.027981902064909425, 0.026290266995383848, 0.027715406777593984, 0.02767338793937979, 0.02464156671423777, 0.02314842096088644, 0.027586421017057074, 0.024610238348320605, 0.02535575574422462, 0.02652694240956495, 0.0212875162327292, 0.022160139435827144, 0.02202463904698234, 0.022060011148068128, 0.023447143148000817, 0.023494109529652956, 0.019850605349290253, 0.02084760292709276, 0.021954338210924733, 0.02004422311044216, 0.020194411459758598, 0.02048721442948576, 0.01872241562097883, 0.01941376891567079, 0.01872303853554168, 0.017500407985653933, 0.016263059745331047, 0.018053048146061297, 0.018257047699100547, 0.020188178336951187, 0.017362650144844432, 0.016373087955466697, 0.01755697417426512, 0.016551544077194256, 0.0165550105824646, 0.01657238838475272, 0.01566031400120833, 0.016367874832952413, 0.01580457913466163, 0.01608549557006941, 0.014901977736042286, 0.014129680376735034, 0.01601917281983699, 0.01618690940614213, 0.015872465109077017, 0.013866110133407494, 0.014629162699441594, 0.01782797110206214], 'val_losses': [0.2023604475358512, 0.15288525873737038, 0.1421405591454337, 0.13035905666256323, 0.122408574995257, 0.11028424636334229, 0.09906154514748176, 0.1023106329827332, 0.09790664916311256, 0.1016586418924677, 0.09594453496340596, 0.08378768800595039, 0.08319672365514068, 0.08786319185768568, 0.07285604095689756, 0.08146790864734688, 0.0767827339917552, 0.07816101626274005, 0.07712342035571013, 0.09067250810056653, 0.08435332152958412, 0.06782157351040569, 0.07339398740794169, 0.06723097549739032, 0.06828065933173065, 0.07612540869934041, 0.07115167129237301, 0.07149834826527017, 0.07953255830451429, 0.0701913729682518, 0.07856199715243084, 0.07181662029592917, 0.07152956544087057, 0.06936980751172117, 0.0654963784301629, 0.06851393550768814, 0.07569886855113597, 0.0735378126134073, 0.06539179202388094, 0.0716661054717092, 0.06577481035982856, 0.07159115720898297, 0.07274315089207666, 0.06686334781625966, 0.06895683962250096, 0.0745657814813428, 0.08303498502229333, 0.0739748761365455, 0.06232149096441783, 0.09368170386693962, 0.062092307538201136, 0.0773672499206039, 0.07566915323558479, 0.06373113119602951, 0.07211147680880506, 0.07540430828075692, 0.06622034633832533, 0.0698985476520572, 0.06843780070196012, 0.06869249522794295, 0.06768552379797252, 0.07285042587971571, 0.07085946283496093, 0.07633363584782557, 0.07541756573894122, 0.07467049459883958, 0.07292463380062494, 0.06959884971298082, 0.06751246901681557, 0.06657449884767028, 0.07921161228885867, 0.07375114373400642, 0.09350217346514157, 0.08337689704036387, 0.06577376586283198, 0.0936609189434598, 0.07734451170923447, 0.07949124372045792, 0.07888422495921353, 0.07123457840093618, 0.07519661987011571, 0.073905002890098, 0.07775040663934683, 0.07655354280875688, 0.08176030308034334, 0.07987646988580556, 0.07388240467701442, 0.06894437513070184, 0.06963574451199153, 0.0795636924352208, 0.08766149050246771, 0.07730784684852794], 'val_acc': [0.8690640270642777, 0.8996366370129056, 0.9026437789750658, 0.9145470492419496, 0.9189324646034331, 0.9216890114020799, 0.9312116276155871, 0.9267009146723468, 0.9260744267635634, 0.9218143089838366, 0.9302092469615336, 0.9414860293196341, 0.9411101365743642, 0.9327151985966671, 0.9481268011527377, 0.9350958526500438, 0.9452449567723343, 0.9419872196466608, 0.9463726350081444, 0.9277032953264002, 0.932339305851397, 0.9478762059892244, 0.9439919809547676, 0.9505074552061146, 0.9451196591905776, 0.9438666833730109, 0.9520110261871946, 0.946497932589901, 0.9397318631750408, 0.9463726350081444, 0.9382282921939606, 0.9444931712817942, 0.9482520987344945, 0.9502568600426011, 0.9536398947500313, 0.9501315624608445, 0.9428643027189575, 0.9466232301716577, 0.9521363237689513, 0.9495050745520611, 0.9511339431148979, 0.9442425761182809, 0.9487532890615211, 0.9547675729858414, 0.9508833479513845, 0.9483773963162511, 0.9418619220649042, 0.939606565593284, 0.9565217391304348, 0.939230672848014, 0.9535145971682747, 0.9492544793885478, 0.946497932589901, 0.9538904899135446, 0.953765192331788, 0.9422378148101742, 0.9540157874953014, 0.9512592406966546, 0.9516351334419245, 0.9513845382784112, 0.9551434657311114, 0.952637514095978, 0.9495050745520611, 0.9503821576243578, 0.9492544793885478, 0.9476256108257111, 0.9505074552061146, 0.9542663826588147, 0.9582759052750282, 0.9581506076932715, 0.951885728605438, 0.9475003132439543, 0.9353464478135572, 0.9475003132439543, 0.9597794762561083, 0.9389800776845006, 0.947249718080441, 0.9454955519358477, 0.9516351334419245, 0.9505074552061146, 0.954141085077058, 0.9512592406966546, 0.9478762059892244, 0.9561458463851648, 0.9513845382784112, 0.9566470367121914, 0.9481268011527377, 0.9582759052750282, 0.9581506076932715, 0.9487532890615211, 0.9535145971682747, 0.9528881092594913], 'size_bytes': 130203, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 92, 'batch_size': 128, 'lr': 0.0010797080314285743, 'weight_decay': 0.0006134050443355251, 'patch_size': 25, 'n_heads': 3, 'head_dim': 21, 'num_layers': 1, 'mlp_ratio': 4, 'dropout': 0.060334667975597125, 'attn_dropout': 0.24843835308886908, 'augment_noise_std': 0.048499244446687885, 'augment_scale_low': 0.9531678957424407, 'augment_scale_high': 1.177993538169219, 'use_focal_loss': True, 'focal_gamma': 1.7765779527720287, 'class_weighted': False, 'grad_clip': 2.4130789635007432, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 22176, 'model_storage_size_kb': 23.821875000000002, 'model_size_validation': 'PASS'}
2025-10-02 21:34:41,376 - INFO - _models.training_function_executor - BO Objective: base=0.9529, size_penalty=0.0000, final=0.9529
2025-10-02 21:34:41,376 - INFO - _models.training_function_executor - Model: 22,176 parameters, 23.8KB (PASS 256KB limit)
2025-10-02 21:34:41,376 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 96.196s
2025-10-02 21:34:41,499 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9529
2025-10-02 21:34:41,499 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.123s
2025-10-02 21:34:41,499 - INFO - bo.run_bo - Recorded observation #48: hparams={'epochs': np.int64(92), 'batch_size': np.int64(128), 'lr': 0.0010797080314285743, 'weight_decay': 0.0006134050443355251, 'patch_size': np.int64(25), 'n_heads': np.int64(3), 'head_dim': np.int64(21), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'dropout': 0.060334667975597125, 'attn_dropout': 0.24843835308886908, 'augment_noise_std': 0.048499244446687885, 'augment_scale_low': 0.9531678957424407, 'augment_scale_high': 1.177993538169219, 'use_focal_loss': np.True_, 'focal_gamma': 1.7765779527720287, 'class_weighted': np.False_, 'grad_clip': 2.4130789635007432, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.9529
2025-10-02 21:34:41,499 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 48: {'epochs': np.int64(92), 'batch_size': np.int64(128), 'lr': 0.0010797080314285743, 'weight_decay': 0.0006134050443355251, 'patch_size': np.int64(25), 'n_heads': np.int64(3), 'head_dim': np.int64(21), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'dropout': 0.060334667975597125, 'attn_dropout': 0.24843835308886908, 'augment_noise_std': 0.048499244446687885, 'augment_scale_low': 0.9531678957424407, 'augment_scale_high': 1.177993538169219, 'use_focal_loss': np.True_, 'focal_gamma': 1.7765779527720287, 'class_weighted': np.False_, 'grad_clip': 2.4130789635007432, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.9529
2025-10-02 21:34:41,500 - INFO - bo.run_bo - üîçBO Trial 49: Using RF surrogate + Expected Improvement
2025-10-02 21:34:41,500 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:34:41,500 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:34:41,500 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:34:41,500 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 90, 'batch_size': 512, 'lr': 0.00015180667873451257, 'weight_decay': 0.0014476774828075412, 'patch_size': 50, 'n_heads': 3, 'head_dim': 7, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.08121979259381175, 'attn_dropout': 0.21920171516967976, 'augment_noise_std': 0.024511026729739347, 'augment_scale_low': 0.9588685131182845, 'augment_scale_high': 1.1422643539329314, 'use_focal_loss': False, 'focal_gamma': 4.595018291428653, 'class_weighted': False, 'grad_clip': 2.5834769857286486, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 21:34:41,501 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 90, 'batch_size': 512, 'lr': 0.00015180667873451257, 'weight_decay': 0.0014476774828075412, 'patch_size': 50, 'n_heads': 3, 'head_dim': 7, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.08121979259381175, 'attn_dropout': 0.21920171516967976, 'augment_noise_std': 0.024511026729739347, 'augment_scale_low': 0.9588685131182845, 'augment_scale_high': 1.1422643539329314, 'use_focal_loss': False, 'focal_gamma': 4.595018291428653, 'class_weighted': False, 'grad_clip': 2.5834769857286486, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}
2025-10-02 21:36:19,246 - INFO - _models.training_function_executor - Model: 17,708 parameters, 76.1KB storage
2025-10-02 21:36:19,246 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.2735657749426073, 0.8800544568561236, 0.7909263426288194, 0.7411724106842678, 0.7008268983689062, 0.6618537326390532, 0.6300157236973751, 0.600689979721773, 0.5722171432862241, 0.5423211776940994, 0.517067133573737, 0.49236193624082164, 0.47105918153856713, 0.45426592606278254, 0.43696061672750014, 0.4222270861182622, 0.4054357263532396, 0.38937383884550564, 0.37578525140060465, 0.3628525140440614, 0.35129677339222737, 0.3406458054003596, 0.33277801243736005, 0.3223950134654043, 0.3166424596344764, 0.3067445718810414, 0.30722015862162727, 0.3005280891970219, 0.29057022563045837, 0.28443154883284627, 0.2845528679313327, 0.2762348279942771, 0.27259496367467256, 0.2695552267231514, 0.2629387561214697, 0.2601059884830917, 0.25777297736211285, 0.25350664741479934, 0.24899661918660437, 0.24693073118289036, 0.2458740483870375, 0.2442788574518433, 0.24041707321282144, 0.23649151799553556, 0.23315712824266394, 0.23151736234366116, 0.22886236109213173, 0.22545291898280304, 0.2234433484192096, 0.22128197742808758, 0.21924747643015033, 0.21640503804598468, 0.2163594834149655, 0.21491952458875532, 0.21118403200720745, 0.20918296992963625, 0.20903394536209433, 0.2073876464211567, 0.2057271325314317, 0.20375414530208408, 0.20097579459452006, 0.20049289943768753, 0.20131967919404145, 0.19800672948470605, 0.19564389411600536, 0.19491179759492777, 0.19295604451470882, 0.19010592800931286, 0.1888185404636685, 0.18850855201158825, 0.1854766052553193, 0.18372981336501198, 0.1866551448377762, 0.18263668164503968, 0.181898019164852, 0.18185787766315778, 0.17881404695753442, 0.17686355608541318, 0.17699542025250795, 0.17516524920974821, 0.17495968612805124, 0.1701740911922161, 0.1734060710565311, 0.17253831256659105, 0.17309092931631226, 0.16788491694467825, 0.16830635677620615, 0.16719710747660452, 0.16900046406884361, 0.16318690335340183], 'val_losses': [0.9432846056819036, 0.829173492692255, 0.7623752479997737, 0.7207243278083041, 0.6822457774631542, 0.6476363477185919, 0.6170938287279, 0.5875057305866668, 0.5576268845954166, 0.5312222107776259, 0.5020402458627904, 0.47846145518455513, 0.45722932073425016, 0.4443574165875206, 0.4242587202310174, 0.41159838847297875, 0.38796426341088847, 0.3750297277736389, 0.35622898573162054, 0.34278698609655267, 0.3338347773080839, 0.3241123286401579, 0.31405648649254175, 0.3108430758198914, 0.306035166455307, 0.29354315232161127, 0.29453100400555926, 0.28681036582373687, 0.2864949095370163, 0.2751853669358889, 0.2674259499953274, 0.2670718406976696, 0.2650682346375122, 0.26046926895189876, 0.2532161483077862, 0.25239896496097153, 0.24972951088822765, 0.24475637544681667, 0.23941470304350704, 0.23859514472680557, 0.23874644177656934, 0.23945819569618357, 0.23829600951707747, 0.22815157447881737, 0.22455177533530962, 0.22281652105891186, 0.219360199401125, 0.22627001813540645, 0.22058651515929983, 0.21641776930410572, 0.21430062414776516, 0.21207050744632241, 0.2092640698590892, 0.20652746355312537, 0.2130947984272399, 0.20525697273966692, 0.20374932387780015, 0.20365511429868474, 0.20004355474837873, 0.2008293515017361, 0.1962743049022802, 0.1982924610417434, 0.19517878995004745, 0.19226448306791197, 0.19307283585889493, 0.1927392277229461, 0.19086606652683488, 0.18848630269881314, 0.18568145930244276, 0.18556715455057926, 0.18771095656800457, 0.18560802803991314, 0.1814283787897992, 0.1823690864055053, 0.18034816652913543, 0.17808016050640108, 0.17875995418172838, 0.17759111974527925, 0.175499778620403, 0.1769819651823596, 0.17400155441718596, 0.1732604446872376, 0.17374630367609512, 0.17072777380816037, 0.16994169197043982, 0.17059644628043102, 0.17480303747599293, 0.1685695546213479, 0.1688456706923958, 0.16534405248070433], 'val_acc': [0.7200852023555946, 0.730986091968425, 0.761558701917053, 0.7653176293697531, 0.7702042350582634, 0.7862423255231175, 0.7980202982082446, 0.8108006515474251, 0.8151860669089087, 0.8268387420122791, 0.8412479639142965, 0.8490164139832101, 0.8581631374514472, 0.8645533141210374, 0.868562836737251, 0.8737000375892745, 0.8809672973311615, 0.8866056885102117, 0.8948753289061521, 0.901766695902769, 0.9018919934845258, 0.9059015161007392, 0.9097857411351961, 0.9092845508081694, 0.9112893121162762, 0.9139205613331662, 0.9178047863676231, 0.9190577621851899, 0.916677108131813, 0.9199348452574865, 0.9240696654554567, 0.9229419872196467, 0.9219396065655933, 0.9233178799649167, 0.9298333542162637, 0.9310863300338303, 0.9292068663074803, 0.9322140082696404, 0.9338428768324771, 0.9343440671595038, 0.932339305851397, 0.930835734870317, 0.9313369251973437, 0.9357223405588272, 0.9367247212128806, 0.9372259115399073, 0.9379776970304473, 0.9358476381405839, 0.9376018042851773, 0.9394812680115274, 0.939606565593284, 0.9408595414108508, 0.9414860293196341, 0.940358351083824, 0.9391053752662574, 0.9417366244831474, 0.9414860293196341, 0.9422378148101742, 0.9414860293196341, 0.9414860293196341, 0.9424884099736875, 0.9428643027189575, 0.9447437664453076, 0.9436160882094976, 0.9436160882094976, 0.9433654930459842, 0.9439919809547676, 0.944994361608821, 0.9441172785365243, 0.944994361608821, 0.9438666833730109, 0.9458714446811176, 0.946122039844631, 0.945746147099361, 0.946497932589901, 0.9475003132439543, 0.9469991229169277, 0.945746147099361, 0.9482520987344945, 0.9471244204986844, 0.9476256108257111, 0.9481268011527377, 0.9486279914797644, 0.9493797769703045, 0.9487532890615211, 0.9498809672973312, 0.9485026938980078, 0.9492544793885478, 0.9502568600426011, 0.9508833479513845], 'size_bytes': 88513, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 90, 'batch_size': 512, 'lr': 0.00015180667873451257, 'weight_decay': 0.0014476774828075412, 'patch_size': 50, 'n_heads': 3, 'head_dim': 7, 'num_layers': 4, 'mlp_ratio': 2, 'dropout': 0.08121979259381175, 'attn_dropout': 0.21920171516967976, 'augment_noise_std': 0.024511026729739347, 'augment_scale_low': 0.9588685131182845, 'augment_scale_high': 1.1422643539329314, 'use_focal_loss': False, 'focal_gamma': 4.595018291428653, 'class_weighted': False, 'grad_clip': 2.5834769857286486, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 17708, 'model_storage_size_kb': 76.08906250000001, 'model_size_validation': 'PASS'}
2025-10-02 21:36:19,246 - INFO - _models.training_function_executor - BO Objective: base=0.9509, size_penalty=0.0000, final=0.9509
2025-10-02 21:36:19,246 - INFO - _models.training_function_executor - Model: 17,708 parameters, 76.1KB (PASS 256KB limit)
2025-10-02 21:36:19,246 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 97.746s
2025-10-02 21:36:19,366 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9509
2025-10-02 21:36:19,366 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.120s
2025-10-02 21:36:19,366 - INFO - bo.run_bo - Recorded observation #49: hparams={'epochs': np.int64(90), 'batch_size': np.int64(512), 'lr': 0.00015180667873451257, 'weight_decay': 0.0014476774828075412, 'patch_size': np.int64(50), 'n_heads': np.int64(3), 'head_dim': np.int64(7), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(2), 'dropout': 0.08121979259381175, 'attn_dropout': 0.21920171516967976, 'augment_noise_std': 0.024511026729739347, 'augment_scale_low': 0.9588685131182845, 'augment_scale_high': 1.1422643539329314, 'use_focal_loss': np.False_, 'focal_gamma': 4.595018291428653, 'class_weighted': np.False_, 'grad_clip': 2.5834769857286486, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.9509
2025-10-02 21:36:19,366 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 49: {'epochs': np.int64(90), 'batch_size': np.int64(512), 'lr': 0.00015180667873451257, 'weight_decay': 0.0014476774828075412, 'patch_size': np.int64(50), 'n_heads': np.int64(3), 'head_dim': np.int64(7), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(2), 'dropout': 0.08121979259381175, 'attn_dropout': 0.21920171516967976, 'augment_noise_std': 0.024511026729739347, 'augment_scale_low': 0.9588685131182845, 'augment_scale_high': 1.1422643539329314, 'use_focal_loss': np.False_, 'focal_gamma': 4.595018291428653, 'class_weighted': np.False_, 'grad_clip': 2.5834769857286486, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.9509
2025-10-02 21:36:19,367 - INFO - bo.run_bo - üîçBO Trial 50: Using RF surrogate + Expected Improvement
2025-10-02 21:36:19,367 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-02 21:36:19,367 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:36:19,367 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:36:19,367 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': 83, 'batch_size': 512, 'lr': 0.0005835822346194008, 'weight_decay': 0.001736642102037435, 'patch_size': 20, 'n_heads': 3, 'head_dim': 21, 'num_layers': 1, 'mlp_ratio': 4, 'dropout': 0.11421688454030807, 'attn_dropout': 0.2029891803811636, 'augment_noise_std': 0.04560292489237265, 'augment_scale_low': 0.9818552447030413, 'augment_scale_high': 1.1816857829933687, 'use_focal_loss': True, 'focal_gamma': 2.5118600332802092, 'class_weighted': False, 'grad_clip': 0.27662951060956903, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:36:19,368 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 83, 'batch_size': 512, 'lr': 0.0005835822346194008, 'weight_decay': 0.001736642102037435, 'patch_size': 20, 'n_heads': 3, 'head_dim': 21, 'num_layers': 1, 'mlp_ratio': 4, 'dropout': 0.11421688454030807, 'attn_dropout': 0.2029891803811636, 'augment_noise_std': 0.04560292489237265, 'augment_scale_low': 0.9818552447030413, 'augment_scale_high': 1.1816857829933687, 'use_focal_loss': True, 'focal_gamma': 2.5118600332802092, 'class_weighted': False, 'grad_clip': 0.27662951060956903, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:37:47,434 - INFO - _models.training_function_executor - Model: 54,563 parameters, 234.5KB storage
2025-10-02 21:37:47,434 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.3737416270961253, 0.2055767502469892, 0.15444581514116523, 0.13529001249385703, 0.12245203618673112, 0.11098378428318363, 0.10290418519528405, 0.09638210443919103, 0.0907024525349148, 0.08632631700425146, 0.08281820918299838, 0.07840919484849772, 0.07711131037256232, 0.07430108630456106, 0.07159413142051067, 0.06997275809693433, 0.06714586195927852, 0.0651701327360924, 0.06351101819671, 0.06210170045303185, 0.06140155260981901, 0.05940570902084362, 0.0576369387648725, 0.05551694835323299, 0.053021939360911145, 0.05273371459727478, 0.05046380330995671, 0.04995454295209837, 0.04870204055376139, 0.048427259245796904, 0.04621480711121176, 0.04577018953683595, 0.045972097655619104, 0.04406003139419924, 0.043150294527653725, 0.042606488096606845, 0.04155489809694886, 0.041261728325404125, 0.04108764136856172, 0.03890719743281499, 0.03813073823182792, 0.03855287803096759, 0.03672814903471515, 0.035891115854250116, 0.036525207283533416, 0.03550888930905111, 0.034313132497177865, 0.03539918381252247, 0.032840098556129606, 0.03351092800101008, 0.03295599429360854, 0.03229217963145049, 0.0329443557962252, 0.031017201319297215, 0.031229320409017554, 0.030506291306200316, 0.030320377592072654, 0.029435846735078715, 0.029573489235279325, 0.02948586079893422, 0.028465630499677517, 0.02837484034942762, 0.028215141802339094, 0.028309797159145302, 0.028122510219435865, 0.027310116746282382, 0.02638623908334127, 0.02646818284484575, 0.025944621477027408, 0.026074151660769377, 0.025680983436520553, 0.025605202748904977, 0.02501424668514958, 0.024790161077500407, 0.02465149758554183, 0.023301261071040698, 0.023077603520637216, 0.022678558562078472, 0.02237944790328934, 0.022472879888613365, 0.022532097810609608, 0.020943610244500765, 0.022475493782843293], 'val_losses': [0.25140532141728983, 0.16693560383833975, 0.1508183313941854, 0.13694793742830152, 0.12706118994242505, 0.12146587343297198, 0.10976054945202374, 0.10715293239761513, 0.0996261900287346, 0.09763974689769649, 0.09604621781280145, 0.09945917068743165, 0.10547179757966207, 0.0926502589666146, 0.08832767069929749, 0.09643959539596217, 0.09362174689960187, 0.09136343451632009, 0.09201794774434334, 0.08457736749948377, 0.08184714163791833, 0.07883736336609129, 0.07936041796753363, 0.07647834517703052, 0.07925604545014078, 0.07214101357834782, 0.07919916984913508, 0.07354012563546458, 0.0685996557285412, 0.08209678008937699, 0.07267083708500746, 0.07868131896152546, 0.07647323695030925, 0.0724169427976843, 0.07494900748086983, 0.07190764303132564, 0.07666821755415634, 0.07015796413029246, 0.07693750998561893, 0.0728927436987793, 0.06967826185370131, 0.07384907557408384, 0.07170872147923318, 0.07042776514393183, 0.07738135403864456, 0.07786538383481556, 0.08677998168914185, 0.06691643330196796, 0.07568771142851902, 0.06950076378376886, 0.06909772011924845, 0.07363827657171575, 0.07319642961223148, 0.07617514926271321, 0.07045689200738098, 0.06490648417074181, 0.07292484641803296, 0.07207198801094869, 0.0722000492241221, 0.07493657548745583, 0.06600739789153233, 0.07063442993665933, 0.08373428682417756, 0.06868199189774092, 0.06759424847519767, 0.06886345416665929, 0.06573049934895292, 0.07306293063281159, 0.07630585851454671, 0.06407398679638518, 0.06222438124983909, 0.07208736573203944, 0.07367228853562141, 0.07467443425880012, 0.0659571223986909, 0.06445022847640187, 0.06855199516824038, 0.07520845889776306, 0.07530326190941496, 0.06906696984794351, 0.07802001590672598, 0.06786953554335969, 0.07764713500615066], 'val_acc': [0.7665706051873199, 0.8321012404460594, 0.8472622478386167, 0.8715699786994111, 0.8800902142588648, 0.8888610449818318, 0.9035208620473625, 0.9104122290439794, 0.9139205613331662, 0.9139205613331662, 0.915925322641273, 0.9152988347324896, 0.8962536023054755, 0.9077809798270894, 0.9131687758426262, 0.9046485402831725, 0.913294073424383, 0.9076556822453327, 0.9144217516601929, 0.9183059766946498, 0.9238190702919433, 0.92544793885478, 0.914797644405463, 0.9278285929081569, 0.9205613331662699, 0.92356847512843, 0.9198095476757299, 0.9310863300338303, 0.932339305851397, 0.908658062899386, 0.9332163889236937, 0.9193083573487032, 0.9243202606189701, 0.9274527001628868, 0.9209372259115399, 0.9269515098358602, 0.9224407968926199, 0.9283297832351836, 0.9135446685878963, 0.92469615336424, 0.9325899010149105, 0.9196842500939731, 0.9293321638892369, 0.9295827590527502, 0.9236937727101867, 0.9263250219270768, 0.9100363362987094, 0.9290815687257236, 0.9173035960405964, 0.9386041849392307, 0.9274527001628868, 0.9273274025811302, 0.9277032953264002, 0.9251973436912667, 0.9344693647412605, 0.940358351083824, 0.9233178799649167, 0.9293321638892369, 0.930083949379777, 0.9265756170905901, 0.9355970429770706, 0.9333416865054505, 0.9114146096980328, 0.9348452574865305, 0.9416113269013908, 0.9368500187946373, 0.936975316376394, 0.9288309735622102, 0.9256985340182935, 0.940358351083824, 0.9454955519358477, 0.9318381155243703, 0.9318381155243703, 0.9230672848014033, 0.9407342438290941, 0.9393559704297707, 0.9374765067034206, 0.9278285929081569, 0.9293321638892369, 0.9413607317378775, 0.9234431775466734, 0.9387294825209873, 0.9248214509459968], 'size_bytes': 224301, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 83, 'batch_size': 512, 'lr': 0.0005835822346194008, 'weight_decay': 0.001736642102037435, 'patch_size': 20, 'n_heads': 3, 'head_dim': 21, 'num_layers': 1, 'mlp_ratio': 4, 'dropout': 0.11421688454030807, 'attn_dropout': 0.2029891803811636, 'augment_noise_std': 0.04560292489237265, 'augment_scale_low': 0.9818552447030413, 'augment_scale_high': 1.1816857829933687, 'use_focal_loss': True, 'focal_gamma': 2.5118600332802092, 'class_weighted': False, 'grad_clip': 0.27662951060956903, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 54563, 'model_storage_size_kb': 234.450390625, 'model_size_validation': 'PASS'}
2025-10-02 21:37:47,434 - INFO - _models.training_function_executor - BO Objective: base=0.9248, size_penalty=0.0000, final=0.9248
2025-10-02 21:37:47,435 - INFO - _models.training_function_executor - Model: 54,563 parameters, 234.5KB (PASS 256KB limit)
2025-10-02 21:37:47,435 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 88.068s
2025-10-02 21:37:47,556 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.9248
2025-10-02 21:37:47,556 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.122s
2025-10-02 21:37:47,556 - INFO - bo.run_bo - Recorded observation #50: hparams={'epochs': np.int64(83), 'batch_size': np.int64(512), 'lr': 0.0005835822346194008, 'weight_decay': 0.001736642102037435, 'patch_size': np.int64(20), 'n_heads': np.int64(3), 'head_dim': np.int64(21), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'dropout': 0.11421688454030807, 'attn_dropout': 0.2029891803811636, 'augment_noise_std': 0.04560292489237265, 'augment_scale_low': 0.9818552447030413, 'augment_scale_high': 1.1816857829933687, 'use_focal_loss': np.True_, 'focal_gamma': 2.5118600332802092, 'class_weighted': np.False_, 'grad_clip': 0.27662951060956903, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.9248
2025-10-02 21:37:47,556 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 50: {'epochs': np.int64(83), 'batch_size': np.int64(512), 'lr': 0.0005835822346194008, 'weight_decay': 0.001736642102037435, 'patch_size': np.int64(20), 'n_heads': np.int64(3), 'head_dim': np.int64(21), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'dropout': 0.11421688454030807, 'attn_dropout': 0.2029891803811636, 'augment_noise_std': 0.04560292489237265, 'augment_scale_low': 0.9818552447030413, 'augment_scale_high': 1.1816857829933687, 'use_focal_loss': np.True_, 'focal_gamma': 2.5118600332802092, 'class_weighted': np.False_, 'grad_clip': 0.27662951060956903, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.9248
2025-10-02 21:37:47,557 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.9766
2025-10-02 21:37:47,557 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'epochs': np.int64(97), 'batch_size': np.int64(128), 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(30), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': np.False_, 'focal_gamma': 4.864309655312979, 'class_weighted': np.False_, 'grad_clip': 0.8737514784734614, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}
2025-10-02 21:37:47,557 - INFO - visualization - Generating BO visualization charts with 50 trials...
2025-10-02 21:37:49,018 - INFO - visualization - BO summary saved to: charts/20251002_213747_BO_Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))/bo_summary.txt
2025-10-02 21:37:49,018 - INFO - visualization - BO charts saved to: charts/20251002_213747_BO_Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:37:49,018 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä BO charts saved to: charts/20251002_213747_BO_Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:37:49,082 - INFO - evaluation.code_generation_pipeline_orchestrator - üöÄ STEP 4: Final Training Execution
2025-10-02 21:37:49,083 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-10-02 21:37:49,175 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-02 21:37:49,187 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-02 21:37:49,202 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-02 21:37:49,203 - INFO - _models.training_function_executor - Loaded training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:37:49,203 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-02 21:37:49,203 - INFO - _models.training_function_executor - Loaded training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:37:49,203 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-02 21:37:49,203 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'epochs': np.int64(97), 'batch_size': np.int64(128), 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(30), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': np.False_, 'focal_gamma': 4.864309655312979, 'class_weighted': np.False_, 'grad_clip': 0.8737514784734614, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}
2025-10-02 21:37:49,203 - INFO - evaluation.code_generation_pipeline_orchestrator - Using test set for final training evaluation
2025-10-02 21:37:49,203 - INFO - _models.training_function_executor - Using device: cuda
2025-10-02 21:37:49,225 - INFO - _models.training_function_executor - Executing training function: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:37:49,225 - INFO - _models.training_function_executor - Hyperparameters: {'epochs': np.int64(97), 'batch_size': np.int64(128), 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': np.int64(50), 'n_heads': np.int64(1), 'head_dim': np.int64(30), 'num_layers': np.int64(4), 'mlp_ratio': np.int64(3), 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': np.False_, 'focal_gamma': 4.864309655312979, 'class_weighted': np.False_, 'grad_clip': 0.8737514784734614, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}
2025-10-02 21:37:49,226 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'epochs': 97, 'batch_size': 128, 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': 50, 'n_heads': 1, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': False, 'focal_gamma': 4.864309655312979, 'class_weighted': False, 'grad_clip': 0.8737514784734614, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-10-02 21:40:35,918 - INFO - _models.training_function_executor - Model: 19,020 parameters, 20.4KB storage
2025-10-02 21:40:35,919 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6461842751493431, 0.4253345547003467, 0.3430753407854984, 0.29618289024713235, 0.2707967645374411, 0.245709416729121, 0.22524399255979702, 0.2099430176527288, 0.19972551074521294, 0.19384658701102447, 0.18361123048408373, 0.17618158342557233, 0.16608550217310333, 0.16081516848579827, 0.15648245889134854, 0.15230076285056715, 0.14800336499307856, 0.1430365466710655, 0.13739740716498092, 0.13654914305342994, 0.13105078962868275, 0.12778533315816304, 0.12667534871395816, 0.12229138211240363, 0.11993611283725801, 0.11788886682740574, 0.11811738335785144, 0.11165913841937337, 0.11453809291602328, 0.11007068427496178, 0.1089198363955344, 0.10743517719012026, 0.10478342403661468, 0.10401557634473325, 0.10319107460827472, 0.10182061348089526, 0.10139985469789053, 0.09840877783733458, 0.09714956019960982, 0.09435905903314913, 0.09485513674515003, 0.09564163709700155, 0.09220722043153373, 0.09277939611826311, 0.09150106854193098, 0.09051719645475902, 0.08853687723439842, 0.09215006734145194, 0.08911043062877827, 0.08887110703117673, 0.08325973099042343, 0.08387226774487958, 0.08660355980157756, 0.08344978951784067, 0.08270322525785652, 0.08234227007761323, 0.08348786461659212, 0.08057136658307258, 0.07967542991364776, 0.08001264635512326, 0.07920588649444897, 0.07987951428749701, 0.07568750549899451, 0.07661606712414679, 0.07594554610221789, 0.07739695406846744, 0.07297043722502308, 0.077724682713867, 0.07600104792974142, 0.07279405558112868, 0.07331512390270697, 0.07355658435610025, 0.07227603268735679, 0.07164282458083765, 0.07151110795623031, 0.06824996514782107, 0.0724085257855433, 0.07032281413686305, 0.07186253810691279, 0.07029829370552479, 0.06843301491347331, 0.06900812547471682, 0.06633323928118708, 0.06991951797327808, 0.06767975530305574, 0.06694989286082309, 0.06805930513539071, 0.06570546259564357, 0.06701563621789766, 0.06665565121587315, 0.06682218433938078, 0.06479831496978054, 0.0649105739012755, 0.062071848409765706, 0.06705038323878955, 0.06483242344917803, 0.06343193948806623], 'val_losses': [0.4549778360867785, 0.37722180556843765, 0.28767873672167305, 0.27469551880380105, 0.2566945326900857, 0.24045323692283083, 0.2385711165808339, 0.1931019436647779, 0.2007093417712369, 0.16827709391198156, 0.1611620507361885, 0.17266329720636653, 0.15919513851296652, 0.16032292957261363, 0.15046104317525463, 0.14672374988809525, 0.13203317643875165, 0.14581370346051595, 0.128410450638024, 0.1287984814853009, 0.12574803852134522, 0.12218044503266279, 0.11915554562315468, 0.1366557789581568, 0.1159066156182558, 0.10907257663504409, 0.1037611506730569, 0.1295032305203103, 0.1207416967129843, 0.11036953273224148, 0.09937014583733926, 0.13729154112576367, 0.10824581543205448, 0.1053248326375599, 0.10993769887433612, 0.10892491987035265, 0.11263579860952153, 0.11721635893645622, 0.10040678632190306, 0.11359957042699464, 0.10080716879050376, 0.10467313556773519, 0.10397121149614344, 0.09937823435903867, 0.09544706568432575, 0.09506731709116932, 0.1008576101047476, 0.09712573845919559, 0.10183293715916336, 0.10605203722661943, 0.09219390526264559, 0.09731285862380296, 0.09601863714389215, 0.09812745324823423, 0.09143385727105781, 0.09293213975729685, 0.09278096992167902, 0.08588933431089436, 0.09225005098538586, 0.0939699917381686, 0.09262670972819519, 0.10119646227371243, 0.09030557783514263, 0.09123625947731115, 0.08808321260777924, 0.08872611935106106, 0.08753592817876173, 0.09203791285218811, 0.0943182486553552, 0.08887142865891617, 0.08819022722839209, 0.09023337536113951, 0.08517473032165573, 0.08513473311969229, 0.09083137399005523, 0.09009837056112484, 0.08821816258589493, 0.09654995539360732, 0.08723507279808124, 0.08390463233022756, 0.09264112477798755, 0.10241004991680491, 0.07900028956976203, 0.09202251553824618, 0.093059397433331, 0.08720026197986264, 0.08714506637447508, 0.09323359764028061, 0.08279000649369238, 0.08559053505790802, 0.08057408395531221, 0.08440536754860758, 0.08010804828006343, 0.08537901470381039, 0.08029129923676853, 0.08155182672740845, 0.08517221550097749], 'val_acc': [0.8458824472776842, 0.881244487210328, 0.9103520166786946, 0.9170876433325315, 0.9236628979231818, 0.9287146179135595, 0.929917408387459, 0.9437895918530992, 0.9417047550316735, 0.9500441023173763, 0.9498837302541897, 0.9484403816855104, 0.9508459626333092, 0.9514874508860557, 0.9532515435811082, 0.9554165664341272, 0.9608692165824714, 0.9530911715179216, 0.9617512629299976, 0.9623125651511507, 0.9638360997514233, 0.9634351695934569, 0.9651190762569161, 0.9602277283297249, 0.9668831689519686, 0.9688878197418009, 0.9700104241841071, 0.9623125651511507, 0.9648785181621361, 0.9671237270467484, 0.9702509822788871, 0.9578221473819261, 0.9665624248255954, 0.9673642851415284, 0.9674444711731216, 0.9682463314890546, 0.9656001924464758, 0.9625531232459306, 0.9702509822788871, 0.9673642851415284, 0.9701707962472937, 0.9663218667308154, 0.9691283778365809, 0.9711330286264133, 0.9715339587843798, 0.9708924705316334, 0.9696094940261406, 0.9700906102157004, 0.9708924705316334, 0.9673642851415284, 0.9732980514794323, 0.9716141448159731, 0.9712934006895999, 0.9688878197418009, 0.9711330286264133, 0.9711330286264133, 0.9720952610055328, 0.9758640044904178, 0.9733782375110256, 0.9725763771950926, 0.9720952610055328, 0.9698500521209206, 0.9747414000481116, 0.973217865447839, 0.9731376794162457, 0.9745008419533318, 0.9739395397321786, 0.9742602838585518, 0.9729773073530591, 0.9758640044904178, 0.9736989816373988, 0.9736187956058054, 0.9743404698901451, 0.9746612140165183, 0.9722556330687194, 0.975382888300858, 0.9744206559217384, 0.9737791676689921, 0.9739395397321786, 0.9759441905220111, 0.9738593537005854, 0.9704915403736669, 0.9765054927431641, 0.9715339587843798, 0.9702509822788871, 0.9740999117953653, 0.9762649346483843, 0.9726565632266859, 0.9758640044904178, 0.9765054927431641, 0.975382888300858, 0.9739395397321786, 0.9767460508379441, 0.9767460508379441, 0.977547911153877, 0.9773073530590971, 0.975382888300858], 'size_bytes': 123275, 'model_name': 'Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'epochs': 97, 'batch_size': 128, 'lr': 0.0007797834355049807, 'weight_decay': 0.0010599408122337636, 'patch_size': 50, 'n_heads': 1, 'head_dim': 30, 'num_layers': 4, 'mlp_ratio': 3, 'dropout': 0.27997860731607316, 'attn_dropout': 0.27857271741558665, 'augment_noise_std': 0.04245469404792365, 'augment_scale_low': 0.8191338467034233, 'augment_scale_high': 1.107769437201049, 'use_focal_loss': False, 'focal_gamma': 4.864309655312979, 'class_weighted': False, 'grad_clip': 0.8737514784734614, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 19020, 'model_storage_size_kb': 20.431640625, 'model_size_validation': 'PASS'}
2025-10-02 21:40:35,919 - INFO - evaluation.code_generation_pipeline_orchestrator - Using final test metrics from training (avoids preprocessing mismatch)
2025-10-02 21:40:35,919 - INFO - evaluation.code_generation_pipeline_orchestrator - Final test metrics: {'acc': 0.975382888300858, 'macro_f1': None}
2025-10-02 21:40:36,081 - INFO - evaluation.code_generation_pipeline_orchestrator - Model and test tensors saved to: trained_models/20251002_213749_Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:40:36,087 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä STEP 5: Performance Analysis
2025-10-02 21:40:36,087 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated Model: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:40:36,087 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Score: 0.9766
2025-10-02 21:40:36,087 - INFO - evaluation.code_generation_pipeline_orchestrator - Final Score: 0.9754
2025-10-02 21:40:36,087 - INFO - evaluation.code_generation_pipeline_orchestrator - CODE GENERATION PIPELINE COMPLETE!
2025-10-02 21:40:36,087 - INFO - evaluation.code_generation_pipeline_orchestrator - Model: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))
2025-10-02 21:40:36,087 - INFO - evaluation.code_generation_pipeline_orchestrator - Score: 0.9754
2025-10-02 21:40:36,087 - INFO - __main__ - AI-enhanced training completed!
2025-10-02 21:40:36,087 - INFO - __main__ - Final model achieved: {'acc': 0.975382888300858, 'macro_f1': None}
2025-10-02 21:40:36,087 - INFO - __main__ - Pipeline completed successfully in single attempt
2025-10-02 21:40:36,087 - INFO - __main__ - Pipeline completed: Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2)), metrics: {'acc': 0.975382888300858, 'macro_f1': None}
2025-10-02 21:40:36,088 - INFO - __main__ - Pipeline summary saved to charts/pipeline_summary_20251002_214036.json
2025-10-02 21:40:36,091 - INFO - __main__ - Model saved: trained_models/best_model_Tiny1DTransformer-1D (Busia et al. 2024, adapted for (1000,2))_20251002_214036.pth, performance: {'acc': 0.975382888300858, 'macro_f1': None}
2025-10-02 21:40:36,091 - INFO - __main__ - AI-enhanced processing completed successfully
