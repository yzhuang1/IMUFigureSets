2025-10-12 09:21:02,898 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-12 09:21:03,039 - INFO - __main__ - Logging system initialized successfully
2025-10-12 09:21:03,039 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-10-12 09:21:03,039 - INFO - __main__ - Starting real data processing from data/dataset3/ directory
2025-10-12 09:21:03,040 - INFO - __main__ - Found 4 data files: ['sleep_sample.csv', 'X.npy', 'y.npy', 'sleep_metadata.json']
2025-10-12 09:21:03,040 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-10-12 09:21:03,040 - INFO - __main__ - Attempting to load: X.npy
2025-10-12 09:21:08,182 - INFO - __main__ - Successfully loaded NPY data: X(89283, 6, 6000), y(89283,)
2025-10-12 09:21:14,700 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (89283, 6, 6000), device: cuda
2025-10-12 09:21:14,700 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-10-12 09:21:14,700 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-10-12 09:21:14,700 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-10-12 09:21:14,704 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-10-12 09:21:14,704 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (89283, 6, 6000), 'dtype': 'float32', 'feature_count': 6000, 'sample_count': 89283, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-10-12 09:21:14,705 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-10-12 09:21:14,705 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-10-12 09:21:14,705 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-10-12 09:21:14,705 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-10-12 09:21:14,705 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-10-12 09:21:14,705 - INFO - data_splitting - Input data shape: X=(89283, 6, 6000), y=(89283,)
2025-10-12 09:21:14,705 - INFO - data_splitting - Class distribution: [20758 11387 28006 17266 11866]
2025-10-12 09:21:24,714 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.8602182913059842), np.int64(1): np.float64(1.5682722656786057), np.int64(2): np.float64(0.6375808971211783), np.int64(3): np.float64(1.03420814479638), np.int64(4): np.float64(1.5048722675796682)}
2025-10-12 09:21:24,721 - INFO - class_balancing - Class imbalance analysis:
2025-10-12 09:21:24,721 - INFO - class_balancing -   Strategy: mild_imbalance
2025-10-12 09:21:24,721 - INFO - class_balancing -   Imbalance ratio: 2.46
2025-10-12 09:21:24,721 - INFO - class_balancing -   Recommendations: Standard training should work, Consider class_weight='balanced'
2025-10-12 09:21:24,721 - INFO - data_splitting - Final splits - Train: 57140, Val: 14286, Test: 17857
2025-10-12 09:21:24,723 - INFO - data_splitting - Train class distribution: [13285  7287 17924 11050  7594]
2025-10-12 09:21:24,723 - INFO - data_splitting - Val class distribution: [3321 1822 4481 2763 1899]
2025-10-12 09:21:24,724 - INFO - data_splitting - Test class distribution: [4152 2278 5601 3453 2373]
2025-10-12 09:21:24,724 - INFO - data_splitting - Recommended balancing strategy: mild_imbalance
2025-10-12 09:21:28,043 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 6000]), std shape: torch.Size([1, 6000])
2025-10-12 09:21:28,054 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-10-12 09:21:28,054 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-10-12 09:21:28,066 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-10-12 09:21:28,066 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-10-12 09:21:28,066 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-10-12 09:21:28,068 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-10-12 09:25:15,264 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-12 09:25:15,301 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-12 09:25:15,301 - INFO - _models.ai_code_generator - Prompt length: 4931 characters
2025-10-12 09:25:15,301 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-12 09:25:15,301 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-12 09:25:15,301 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-12 09:28:02,313 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-12 09:28:02,314 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-12 09:28:02,314 - INFO - _models.ai_code_generator - AI generated training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:28:02,314 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-10-12 09:28:02,314 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-10-12 09:28:02,314 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:28:02,314 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'weight_decay', 'epochs', 'batch_size', 'dropout', 'base_channels', 'eca_kernel', 'pool_stride', 'label_smoothing', 'grad_clip_norm', 'scheduler_step_size', 'scheduler_gamma', 'num_workers', 'use_amp', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_batches']
2025-10-12 09:28:02,314 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-10-12 09:28:02,316 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-10-12 09:28:02,317 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_CNN-ECA-BiGRU-ISRUC_1760279282.json
2025-10-12 09:28:02,317 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_CNN-ECA-BiGRU-ISRUC_1760279282.json
2025-10-12 09:28:02,317 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-10-12 09:28:02,317 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:28:02,317 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-10-12 09:28:02,329 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-12 09:28:02,332 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-12 09:28:02,332 - INFO - package_installer - Available packages: {'torch'}
2025-10-12 09:28:02,332 - INFO - package_installer - Missing packages: set()
2025-10-12 09:28:02,332 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-12 09:28:02,332 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-12 09:28:02,332 - INFO - data_splitting - Using all 57140 training samples for BO
2025-10-12 09:28:02,332 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 57140 samples (using bo_sample_num=100000000000000)
2025-10-12 09:28:02,332 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'weight_decay', 'epochs', 'batch_size', 'dropout', 'base_channels', 'eca_kernel', 'pool_stride', 'label_smoothing', 'grad_clip_norm', 'scheduler_step_size', 'scheduler_gamma', 'num_workers', 'use_amp', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_batches']
2025-10-12 09:28:02,333 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-12 09:28:02,333 - INFO - data_splitting - Using all 57140 training samples for BO
2025-10-12 09:28:02,333 - INFO - _models.training_function_executor - Using BO subset for optimization: 57140 samples (bo_sample_num=100000000000000)
2025-10-12 09:28:02,909 - INFO - _models.training_function_executor - BO splits - Train: 45712, Val: 11428
2025-10-12 09:28:04,105 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-10-12 09:28:04,105 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-12 09:28:04,106 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-12 09:28:04,107 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-12 09:28:04,107 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-12 09:28:04,107 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 09:28:04,107 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:28:04,107 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001412035543062636, 'weight_decay': 5.416754583247461e-06, 'epochs': 12, 'batch_size': 12, 'dropout': 0.07800932022121827, 'base_channels': 26, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03337086111390219, 'grad_clip_norm': 1.1429006806487336, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11832019992326419, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 09:28:04,108 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001412035543062636, 'weight_decay': 5.416754583247461e-06, 'epochs': 12, 'batch_size': 12, 'dropout': 0.07800932022121827, 'base_channels': 26, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03337086111390219, 'grad_clip_norm': 1.1429006806487336, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11832019992326419, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 09:28:23,434 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9356 val_loss=0.7972 val_acc=0.7113 time=18.5s
2025-10-12 09:28:38,566 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7297 val_loss=0.6983 val_acc=0.7565 time=15.1s
2025-10-12 09:28:53,720 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.6210 val_loss=0.6434 val_acc=0.7817 time=15.2s
2025-10-12 09:29:08,945 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6039 val_loss=0.6566 val_acc=0.7834 time=15.2s
2025-10-12 09:29:24,123 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.5889 val_loss=0.6303 val_acc=0.7912 time=15.2s
2025-10-12 09:29:39,342 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.5833 val_loss=0.6878 val_acc=0.7574 time=15.2s
2025-10-12 09:29:54,534 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.5822 val_loss=0.6294 val_acc=0.7910 time=15.2s
2025-10-12 09:30:09,699 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.5840 val_loss=0.6380 val_acc=0.7861 time=15.2s
2025-10-12 09:30:24,889 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.5810 val_loss=0.6462 val_acc=0.7873 time=15.2s
2025-10-12 09:30:40,071 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.5796 val_loss=0.7408 val_acc=0.7475 time=15.2s
2025-10-12 09:30:55,267 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.5817 val_loss=0.6296 val_acc=0.7926 time=15.2s
2025-10-12 09:31:10,513 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.5804 val_loss=0.6403 val_acc=0.7880 time=15.2s
2025-10-12 09:31:12,538 - ERROR - _models.training_function_executor - Training execution failed: 'tuple' object has no attribute 'numel'
2025-10-12 09:31:12,538 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-12 09:31:12,538 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-12 09:31:12,539 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-12 09:31:12,539 - INFO - _models.ai_code_generator - Prompt length: 16121 characters
2025-10-12 09:31:12,539 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-12 09:31:12,539 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-12 09:31:12,539 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-12 09:33:44,003 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-12 09:33:44,003 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-12 09:33:44,004 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251012_093344_attempt1.txt
2025-10-12 09:33:44,004 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    # Required imports\n    import torch\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch import nn, optim\n    import torch.nn.functional as F\n    import torch.ao.quantization as tq\n    import math\n    import time\n    import copy\n    import torch.multiprocessing as mp\n\n    # --------------------\n    # Hyperparameters with defaults (can be overridden via kwargs)\n    # --------------------\n    hp = {\n        'lr': 1e-3,\n        'weight_decay': 1e-4,\n        'epochs': 15,\n        'batch_size': 8,\n        'dropout': 0.1,\n        'base_channels': 16,\n        'eca_kernel': 3,\n        'pool_stride': 4,\n        'label_smoothing': 0.05,\n        'grad_clip_norm': 1.0,\n        'scheduler_step_size': 0,\n        'scheduler_gamma': 0.5,\n        'num_workers': 4,\n        'use_amp': True,\n        # Quantization params\n        'quantization_bits': 8,\n        'quantize_weights': True,\n        'quantize_activations': True,\n        'calibrate_batches': 4,\n    }\n    hp.update(kwargs or {})\n\n    # --------------------\n    # Helpers\n    # --------------------\n    def _to_tensor(x, dtype=None):\n        if isinstance(x, torch.Tensor):\n            return x.to(dtype=dtype) if dtype is not None else x\n        return torch.as_tensor(x, dtype=dtype)\n\n    def _normalize_targets(y, num_classes):\n        # Accept tensor, numpy, or (tensor,) tuples/lists\n        if isinstance(y, (list, tuple)):\n            # If tuple/list, take the first element (most loaders put labels here)\n            y = y[0]\n        if not isinstance(y, torch.Tensor):\n            y = torch.as_tensor(y)\n        # If one-hot or extra dims, convert/squeeze\n        if y.ndim > 1:\n            if y.size(-1) == num_classes:\n                y = y.argmax(dim=-1)\n            else:\n                y = y.squeeze()\n        return y.long()\n\n    # --------------------\n    # Device handling and checks\n    # --------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        raise RuntimeError('This training function requires a CUDA device. Pass device=\"cuda\" or torch.device(\"cuda\").')\n\n    torch.backends.cudnn.benchmark = True\n\n    num_classes = 5\n\n    # --------------------\n    # Convert inputs to tensors and normalize shapes/dtypes\n    # Expect X_*: (N, 6, 6000), y_*: (N,) with class indices 0..4 or 1-hot (N,5)\n    # --------------------\n    X_train = _to_tensor(X_train, dtype=torch.float32)\n    X_val = _to_tensor(X_val, dtype=torch.float32)\n    y_train = _normalize_targets(y_train, num_classes)\n    y_val = _normalize_targets(y_val, num_classes)\n\n    if X_train.ndim != 3:\n        raise ValueError(f'X_train must be 3D (N, C=6, L=6000). Got shape {tuple(X_train.shape)}')\n    if X_val.ndim != 3:\n        raise ValueError(f'X_val must be 3D (N, C=6, L=6000). Got shape {tuple(X_val.shape)}')\n    if X_train.shape[1] != 6 or X_val.shape[1] != 6:\n        raise ValueError('Expected 6 EEG channels.')\n    if X_train.shape[2] != 6000 or X_val.shape[2] != 6000:\n        raise ValueError('Expected sequence length 6000 per epoch.')\n    if y_train.ndim != 1 or y_val.ndim != 1:\n        raise ValueError('y_train and y_val must be 1D label tensors of shape (N,) after normalization.')\n    if X_train.shape[0] != y_train.shape[0] or X_val.shape[0] != y_val.shape[0]:\n        raise ValueError('Mismatched number of samples between X and y.')\n\n    # --------------------\n    # Datasets and DataLoaders (with spawn context for CUDA safety)\n    # --------------------\n    mp_ctx = torch.multiprocessing.get_context('spawn')\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=int(hp['batch_size']),\n        shuffle=True,\n        num_workers=int(hp['num_workers']),\n        pin_memory=True,\n        multiprocessing_context=mp_ctx,\n        drop_last=False,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=max(1, int(hp['batch_size']) // 2),\n        shuffle=False,\n        num_workers=int(hp['num_workers']),\n        pin_memory=True,\n        multiprocessing_context=mp_ctx,\n        drop_last=False,\n    )\n\n    # --------------------\n    # Model definition\n    # --------------------\n    class ChannelECA(nn.Module):\n        def __init__(self, channels: int, k: int = 3):\n            super().__init__()\n            k = max(1, k)\n            if k % 2 == 0:\n                k += 1\n            self.avg_pool = nn.AdaptiveAvgPool1d(1)\n            self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)\n            self.sig = nn.Sigmoid()\n\n        def forward(self, x):  # x: (B, C, L)\n            b, c, l = x.shape\n            y = self.avg_pool(x)            # (B, C, 1)\n            y = y.permute(0, 2, 1)          # (B, 1, C)\n            y = self.conv(y)                # (B, 1, C)\n            y = self.sig(y).permute(0, 2, 1)  # (B, C, 1)\n            return x * y                    # broadcast over L\n\n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, in_ch: int, out_ch: int, k: int = 7, stride: int = 1, p: float = 0.0):\n            super().__init__()\n            pad = (k - 1) // 2\n            self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=stride, padding=pad, groups=in_ch, bias=False)\n            self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n        def forward(self, x):\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class ConvBlock(nn.Module):\n        def __init__(self, in_ch, out_ch, k=5, s=2, p: float = 0.0):\n            super().__init__()\n            pad = (k - 1) // 2\n            self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=pad, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n        def forward(self, x):\n            x = self.conv(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class ClassifierHead(nn.Module):\n        def __init__(self, in_dim, hidden, out_dim, p: float = 0.1):\n            super().__init__()\n            self.quant = tq.QuantStub()\n            self.fc1 = nn.Linear(in_dim, hidden)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n            self.fc2 = nn.Linear(hidden, out_dim)\n            self.dequant = tq.DeQuantStub()\n        def forward(self, x):  # x: (B, in_dim)\n            x = self.quant(x)\n            x = self.fc1(x)\n            x = self.act(x)\n            x = self.drop(x)\n            x = self.fc2(x)\n            x = self.dequant(x)\n            return x\n\n    class EEGSleepNet(nn.Module):\n        def __init__(self, base_channels=16, eca_k=3, pool_stride=4, dropout=0.1, num_classes=5):\n            super().__init__()\n            self.eca = ChannelECA(6, k=eca_k)\n            c1 = base_channels\n            c2 = base_channels * 2\n            c3 = base_channels * 4\n            self.conv1 = ConvBlock(6, c1, k=7, s=2, p=dropout)\n            self.conv2 = ConvBlock(c1, c2, k=5, s=2, p=dropout)\n            self.conv3 = ConvBlock(c2, c3, k=5, s=2, p=dropout)\n            self.ds = DepthwiseSeparableConv1d(c3, c3, k=7, stride=1, p=dropout)\n            self.pool = nn.AvgPool1d(kernel_size=pool_stride, stride=pool_stride)\n\n            d_model = c3\n            self.bigru = nn.GRU(input_size=d_model, hidden_size=d_model, num_layers=1, batch_first=False, bidirectional=True)\n\n            head_in = d_model * 2\n            head_hidden = max(32, d_model)\n            self.head = ClassifierHead(head_in, head_hidden, num_classes, p=dropout)\n\n        def forward(self, x):  # x: (B, 6, 6000)\n            x = self.eca(x)\n            x = self.conv1(x)\n            x = self.conv2(x)\n            x = self.conv3(x)\n            x = self.ds(x)\n            x = self.pool(x)  # (B, C, L')\n            x = x.transpose(1, 2)  # (B, L', C)\n            x = x.transpose(0, 1)  # (L', B, C)\n            out, _ = self.bigru(x)  # (L', B, 2C)\n            out = out.mean(dim=0)   # (B, 2C)\n            logits = self.head(out) # (B, num_classes)\n            return logits\n\n    # --------------------\n    # Instantiate model\n    # --------------------\n    model = EEGSleepNet(\n        base_channels=int(hp['base_channels']),\n        eca_k=int(hp['eca_kernel']),\n        pool_stride=int(hp['pool_stride']),\n        dropout=float(hp['dropout']),\n        num_classes=num_classes,\n    ).to(device)\n\n    # --------------------\n    # Optimizer, loss, scheduler\n    # --------------------\n    optimizer = optim.AdamW(model.parameters(), lr=float(hp['lr']), weight_decay=float(hp['weight_decay']))\n    criterion = nn.CrossEntropyLoss(label_smoothing=float(hp['label_smoothing'])).to(device)\n\n    scheduler = None\n    if int(hp['scheduler_step_size']) > 0:\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=int(hp['scheduler_step_size']), gamma=float(hp['scheduler_gamma']))\n\n    scaler = torch.cuda.amp.GradScaler(enabled=bool(hp['use_amp']))\n\n    # --------------------\n    # Training loop\n    # --------------------\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, int(hp['epochs']) + 1):\n        model.train()\n        epoch_loss = 0.0\n        n_train = 0\n        t0 = time.time()\n\n        for xb, yb in train_loader:\n            # Normalize potential tuple/list targets coming from dataset/collate\n            if isinstance(yb, (list, tuple)):\n                yb = yb[0]\n            if yb.ndim > 1:\n                if yb.size(-1) == num_classes:\n                    yb = yb.argmax(dim=-1)\n                else:\n                    yb = yb.squeeze()\n            yb = yb.long()\n\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=bool(hp['use_amp'])):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n\n            scaler.scale(loss).backward()\n            if hp['grad_clip_norm'] is not None and float(hp['grad_clip_norm']) > 0:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(hp['grad_clip_norm']))\n            scaler.step(optimizer)\n            scaler.update()\n\n            batch_size_now = xb.size(0)\n            epoch_loss += loss.detach().item() * batch_size_now\n            n_train += batch_size_now\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_loss = epoch_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if isinstance(yb, (list, tuple)):\n                    yb = yb[0]\n                if yb.ndim > 1:\n                    if yb.size(-1) == num_classes:\n                        yb = yb.argmax(dim=-1)\n                    else:\n                        yb = yb.squeeze()\n                yb = yb.long()\n\n                xb = xb.to(device, non_blocking=True)\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                pred = logits.argmax(dim=1)\n                correct += (pred == yb).sum().item()\n\n        val_loss = val_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} time={time.time()-t0:.1f}s')\n\n    # --------------------\n    # Post-Training Quantization (ensure final model <= 256KB)\n    # --------------------\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n\n    qb = int(hp['quantization_bits'])\n    qw = bool(hp['quantize_weights'])\n    qa = bool(hp['quantize_activations'])\n\n    if qw and qb == 8:\n        if qa:\n            torch.backends.quantized.engine = 'fbgemm'\n            model_cpu.qconfig = tq.get_default_qconfig('fbgemm')\n            tq.prepare(model_cpu, inplace=True)\n            model_cpu.eval()\n            with torch.inference_mode():\n                steps = 0\n                for xb, _ in train_loader:\n                    xb = xb.to('cpu', non_blocking=False)\n                    _ = model_cpu(xb)\n                    steps += 1\n                    if steps >= int(hp['calibrate_batches']):\n                        break\n            tq.convert(model_cpu, inplace=True)\n        else:\n            model_cpu = tq.quantize_dynamic(\n                model_cpu,\n                {nn.Linear, nn.GRU},\n                dtype=torch.qint8,\n                inplace=True\n            )\n    elif qw and qb == 16:\n        model_cpu = model_cpu.half()\n    else:\n        pass\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return model_cpu, metrics\n"}
2025-10-12 09:33:44,004 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-12 09:33:44,004 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    # Required imports\n    import torch\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch import nn, optim\n    import torch.nn.functional as F\n    import torch.ao.quantization as tq\n    import math\n    import time\n    import copy\n    import torch.multiprocessing as mp\n\n    # --------------------\n    # Hyperparameters with defaults (can be overridden via kwargs)\n    # --------------------\n    hp = {\n        'lr': 1e-3,\n        'weight_decay': 1e-4,\n        'epochs': 15,\n        'batch_size': 8,\n        'dropout': 0.1,\n        'base_channels': 16,\n        'eca_kernel': 3,\n        'pool_stride': 4,\n        'label_smoothing': 0.05,\n        'grad_clip_norm': 1.0,\n        'scheduler_step_size': 0,\n        'scheduler_gamma': 0.5,\n        'num_workers': 4,\n        'use_amp': True,\n        # Quantization params\n        'quantization_bits': 8,\n        'quantize_weights': True,\n        'quantize_activations': True,\n        'calibrate_batches': 4,\n    }\n    hp.update(kwargs or {})\n\n    # --------------------\n    # Helpers\n    # --------------------\n    def _to_tensor(x, dtype=None):\n        if isinstance(x, torch.Tensor):\n            return x.to(dtype=dtype) if dtype is not None else x\n        return torch.as_tensor(x, dtype=dtype)\n\n    def _normalize_targets(y, num_classes):\n        # Accept tensor, numpy, or (tensor,) tuples/lists\n        if isinstance(y, (list, tuple)):\n            # If tuple/list, take the first element (most loaders put labels here)\n            y = y[0]\n        if not isinstance(y, torch.Tensor):\n            y = torch.as_tensor(y)\n        # If one-hot or extra dims, convert/squeeze\n        if y.ndim > 1:\n            if y.size(-1) == num_classes:\n                y = y.argmax(dim=-1)\n            else:\n                y = y.squeeze()\n        return y.long()\n\n    # --------------------\n    # Device handling and checks\n    # --------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        raise RuntimeError('This training function requires a CUDA device. Pass device=\"cuda\" or torch.device(\"cuda\").')\n\n    torch.backends.cudnn.benchmark = True\n\n    num_classes = 5\n\n    # --------------------\n    # Convert inputs to tensors and normalize shapes/dtypes\n    # Expect X_*: (N, 6, 6000), y_*: (N,) with class indices 0..4 or 1-hot (N,5)\n    # --------------------\n    X_train = _to_tensor(X_train, dtype=torch.float32)\n    X_val = _to_tensor(X_val, dtype=torch.float32)\n    y_train = _normalize_targets(y_train, num_classes)\n    y_val = _normalize_targets(y_val, num_classes)\n\n    if X_train.ndim != 3:\n        raise ValueError(f'X_train must be 3D (N, C=6, L=6000). Got shape {tuple(X_train.shape)}')\n    if X_val.ndim != 3:\n        raise ValueError(f'X_val must be 3D (N, C=6, L=6000). Got shape {tuple(X_val.shape)}')\n    if X_train.shape[1] != 6 or X_val.shape[1] != 6:\n        raise ValueError('Expected 6 EEG channels.')\n    if X_train.shape[2] != 6000 or X_val.shape[2] != 6000:\n        raise ValueError('Expected sequence length 6000 per epoch.')\n    if y_train.ndim != 1 or y_val.ndim != 1:\n        raise ValueError('y_train and y_val must be 1D label tensors of shape (N,) after normalization.')\n    if X_train.shape[0] != y_train.shape[0] or X_val.shape[0] != y_val.shape[0]:\n        raise ValueError('Mismatched number of samples between X and y.')\n\n    # --------------------\n    # Datasets and DataLoaders (with spawn context for CUDA safety)\n    # --------------------\n    mp_ctx = torch.multiprocessing.get_context('spawn')\n\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=int(hp['batch_size']),\n        shuffle=True,\n        num_workers=int(hp['num_workers']),\n        pin_memory=True,\n        multiprocessing_context=mp_ctx,\n        drop_last=False,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=max(1, int(hp['batch_size']) // 2),\n        shuffle=False,\n        num_workers=int(hp['num_workers']),\n        pin_memory=True,\n        multiprocessing_context=mp_ctx,\n        drop_last=False,\n    )\n\n    # --------------------\n    # Model definition\n    # --------------------\n    class ChannelECA(nn.Module):\n        def __init__(self, channels: int, k: int = 3):\n            super().__init__()\n            k = max(1, k)\n            if k % 2 == 0:\n                k += 1\n            self.avg_pool = nn.AdaptiveAvgPool1d(1)\n            self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)\n            self.sig = nn.Sigmoid()\n\n        def forward(self, x):  # x: (B, C, L)\n            b, c, l = x.shape\n            y = self.avg_pool(x)            # (B, C, 1)\n            y = y.permute(0, 2, 1)          # (B, 1, C)\n            y = self.conv(y)                # (B, 1, C)\n            y = self.sig(y).permute(0, 2, 1)  # (B, C, 1)\n            return x * y                    # broadcast over L\n\n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, in_ch: int, out_ch: int, k: int = 7, stride: int = 1, p: float = 0.0):\n            super().__init__()\n            pad = (k - 1) // 2\n            self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=stride, padding=pad, groups=in_ch, bias=False)\n            self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n        def forward(self, x):\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class ConvBlock(nn.Module):\n        def __init__(self, in_ch, out_ch, k=5, s=2, p: float = 0.0):\n            super().__init__()\n            pad = (k - 1) // 2\n            self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=pad, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n        def forward(self, x):\n            x = self.conv(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class ClassifierHead(nn.Module):\n        def __init__(self, in_dim, hidden, out_dim, p: float = 0.1):\n            super().__init__()\n            self.quant = tq.QuantStub()\n            self.fc1 = nn.Linear(in_dim, hidden)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n            self.fc2 = nn.Linear(hidden, out_dim)\n            self.dequant = tq.DeQuantStub()\n        def forward(self, x):  # x: (B, in_dim)\n            x = self.quant(x)\n            x = self.fc1(x)\n            x = self.act(x)\n            x = self.drop(x)\n            x = self.fc2(x)\n            x = self.dequant(x)\n            return x\n\n    class EEGSleepNet(nn.Module):\n        def __init__(self, base_channels=16, eca_k=3, pool_stride=4, dropout=0.1, num_classes=5):\n            super().__init__()\n            self.eca = ChannelECA(6, k=eca_k)\n            c1 = base_channels\n            c2 = base_channels * 2\n            c3 = base_channels * 4\n            self.conv1 = ConvBlock(6, c1, k=7, s=2, p=dropout)\n            self.conv2 = ConvBlock(c1, c2, k=5, s=2, p=dropout)\n            self.conv3 = ConvBlock(c2, c3, k=5, s=2, p=dropout)\n            self.ds = DepthwiseSeparableConv1d(c3, c3, k=7, stride=1, p=dropout)\n            self.pool = nn.AvgPool1d(kernel_size=pool_stride, stride=pool_stride)\n\n            d_model = c3\n            self.bigru = nn.GRU(input_size=d_model, hidden_size=d_model, num_layers=1, batch_first=False, bidirectional=True)\n\n            head_in = d_model * 2\n            head_hidden = max(32, d_model)\n            self.head = ClassifierHead(head_in, head_hidden, num_classes, p=dropout)\n\n        def forward(self, x):  # x: (B, 6, 6000)\n            x = self.eca(x)\n            x = self.conv1(x)\n            x = self.conv2(x)\n            x = self.conv3(x)\n            x = self.ds(x)\n            x = self.pool(x)  # (B, C, L')\n            x = x.transpose(1, 2)  # (B, L', C)\n            x = x.transpose(0, 1)  # (L', B, C)\n            out, _ = self.bigru(x)  # (L', B, 2C)\n            out = out.mean(dim=0)   # (B, 2C)\n            logits = self.head(out) # (B, num_classes)\n            return logits\n\n    # --------------------\n    # Instantiate model\n    # --------------------\n    model = EEGSleepNet(\n        base_channels=int(hp['base_channels']),\n        eca_k=int(hp['eca_kernel']),\n        pool_stride=int(hp['pool_stride']),\n        dropout=float(hp['dropout']),\n        num_classes=num_classes,\n    ).to(device)\n\n    # --------------------\n    # Optimizer, loss, scheduler\n    # --------------------\n    optimizer = optim.AdamW(model.parameters(), lr=float(hp['lr']), weight_decay=float(hp['weight_decay']))\n    criterion = nn.CrossEntropyLoss(label_smoothing=float(hp['label_smoothing'])).to(device)\n\n    scheduler = None\n    if int(hp['scheduler_step_size']) > 0:\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=int(hp['scheduler_step_size']), gamma=float(hp['scheduler_gamma']))\n\n    scaler = torch.cuda.amp.GradScaler(enabled=bool(hp['use_amp']))\n\n    # --------------------\n    # Training loop\n    # --------------------\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, int(hp['epochs']) + 1):\n        model.train()\n        epoch_loss = 0.0\n        n_train = 0\n        t0 = time.time()\n\n        for xb, yb in train_loader:\n            # Normalize potential tuple/list targets coming from dataset/collate\n            if isinstance(yb, (list, tuple)):\n                yb = yb[0]\n            if yb.ndim > 1:\n                if yb.size(-1) == num_classes:\n                    yb = yb.argmax(dim=-1)\n                else:\n                    yb = yb.squeeze()\n            yb = yb.long()\n\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=bool(hp['use_amp'])):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n\n            scaler.scale(loss).backward()\n            if hp['grad_clip_norm'] is not None and float(hp['grad_clip_norm']) > 0:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(hp['grad_clip_norm']))\n            scaler.step(optimizer)\n            scaler.update()\n\n            batch_size_now = xb.size(0)\n            epoch_loss += loss.detach().item() * batch_size_now\n            n_train += batch_size_now\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_loss = epoch_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                if isinstance(yb, (list, tuple)):\n                    yb = yb[0]\n                if yb.ndim > 1:\n                    if yb.size(-1) == num_classes:\n                        yb = yb.argmax(dim=-1)\n                    else:\n                        yb = yb.squeeze()\n                yb = yb.long()\n\n                xb = xb.to(device, non_blocking=True)\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                pred = logits.argmax(dim=1)\n                correct += (pred == yb).sum().item()\n\n        val_loss = val_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} time={time.time()-t0:.1f}s')\n\n    # --------------------\n    # Post-Training Quantization (ensure final model <= 256KB)\n    # --------------------\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n\n    qb = int(hp['quantization_bits'])\n    qw = bool(hp['quantize_weights'])\n    qa = bool(hp['quantize_activations'])\n\n    if qw and qb == 8:\n        if qa:\n            torch.backends.quantized.engine = 'fbgemm'\n            model_cpu.qconfig = tq.get_default_qconfig('fbgemm')\n            tq.prepare(model_cpu, inplace=True)\n            model_cpu.eval()\n            with torch.inference_mode():\n                steps = 0\n                for xb, _ in train_loader:\n                    xb = xb.to('cpu', non_blocking=False)\n                    _ = model_cpu(xb)\n                    steps += 1\n                    if steps >= int(hp['calibrate_batches']):\n                        break\n            tq.convert(model_cpu, inplace=True)\n        else:\n            model_cpu = tq.quantize_dynamic(\n                model_cpu,\n                {nn.Linear, nn.GRU},\n                dtype=torch.qint8,\n                inplace=True\n            )\n    elif qw and qb == 16:\n        model_cpu = model_cpu.half()\n    else:\n        pass\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return model_cpu, metrics\n"}
2025-10-12 09:33:44,004 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-12 09:33:44,004 - ERROR - _models.training_function_executor - BO training objective failed: 'tuple' object has no attribute 'numel'
2025-10-12 09:33:44,004 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 339.897s
2025-10-12 09:33:44,011 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: 'tuple' object has no attribute 'numel'
2025-10-12 09:33:44,011 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-10-12 09:33:47,014 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-10-12 09:33:47,014 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-10-12 09:33:47,028 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CNN-ECA-BiGRU-ISRUC_1760279282.json
2025-10-12 09:33:47,028 - INFO - _models.training_function_executor - Loaded training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:33:47,028 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-12 09:33:47,028 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-10-12 09:33:47,029 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CNN-ECA-BiGRU-ISRUC_1760279282.json
2025-10-12 09:33:47,029 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:33:47,029 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-10-12 09:33:47,339 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 1/4
2025-10-12 09:33:47,339 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 1: üì¶ Installing dependencies for GPT-generated training code...
2025-10-12 09:33:47,339 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-12 09:33:47,342 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-12 09:33:47,342 - INFO - package_installer - Available packages: {'torch'}
2025-10-12 09:33:47,342 - INFO - package_installer - Missing packages: set()
2025-10-12 09:33:47,342 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-12 09:33:47,342 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-12 09:33:47,342 - INFO - data_splitting - Using all 57140 training samples for BO
2025-10-12 09:33:47,342 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 57140 samples (using bo_sample_num=100000000000000)
2025-10-12 09:33:47,342 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'weight_decay', 'epochs', 'batch_size', 'dropout', 'base_channels', 'eca_kernel', 'pool_stride', 'label_smoothing', 'grad_clip_norm', 'scheduler_step_size', 'scheduler_gamma', 'num_workers', 'use_amp', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_batches']
2025-10-12 09:33:47,342 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-12 09:33:47,342 - INFO - data_splitting - Using all 57140 training samples for BO
2025-10-12 09:33:47,342 - INFO - _models.training_function_executor - Using BO subset for optimization: 57140 samples (bo_sample_num=100000000000000)
2025-10-12 09:33:49,337 - INFO - _models.training_function_executor - BO splits - Train: 45712, Val: 11428
2025-10-12 09:33:50,268 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-10-12 09:33:50,268 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-12 09:33:50,269 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-12 09:33:50,270 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-12 09:33:50,270 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-12 09:33:50,270 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 09:33:50,270 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:33:50,270 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001412035543062636, 'weight_decay': 5.416754583247461e-06, 'epochs': 12, 'batch_size': 12, 'dropout': 0.07800932022121827, 'base_channels': 26, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03337086111390219, 'grad_clip_norm': 1.1429006806487336, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11832019992326419, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 09:33:50,272 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001412035543062636, 'weight_decay': 5.416754583247461e-06, 'epochs': 12, 'batch_size': 12, 'dropout': 0.07800932022121827, 'base_channels': 26, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03337086111390219, 'grad_clip_norm': 1.1429006806487336, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11832019992326419, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 09:34:06,342 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9431 val_loss=0.7519 val_acc=0.7287 time=16.0s
2025-10-12 09:34:21,510 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7390 val_loss=1.0273 val_acc=0.6529 time=15.2s
2025-10-12 09:34:36,715 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.6257 val_loss=0.6268 val_acc=0.7931 time=15.2s
2025-10-12 09:34:51,944 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6021 val_loss=0.6429 val_acc=0.7871 time=15.2s
2025-10-12 09:35:07,135 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.5863 val_loss=0.6304 val_acc=0.7881 time=15.2s
2025-10-12 09:35:22,306 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.5833 val_loss=0.6272 val_acc=0.7890 time=15.2s
2025-10-12 09:35:37,470 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.5815 val_loss=0.7796 val_acc=0.7174 time=15.2s
2025-10-12 09:35:52,680 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.5816 val_loss=0.7212 val_acc=0.7454 time=15.2s
2025-10-12 09:36:07,885 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.5820 val_loss=0.6193 val_acc=0.7934 time=15.2s
2025-10-12 09:36:23,116 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.5779 val_loss=0.6245 val_acc=0.7903 time=15.2s
2025-10-12 09:36:38,362 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.5823 val_loss=0.6238 val_acc=0.7910 time=15.2s
2025-10-12 09:36:53,664 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.5808 val_loss=0.6189 val_acc=0.7948 time=15.3s
2025-10-12 09:36:55,672 - ERROR - _models.training_function_executor - Training execution failed: 'tuple' object has no attribute 'numel'
2025-10-12 09:36:55,674 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-10-12 09:36:55,674 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-10-12 09:36:55,675 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-10-12 09:36:55,675 - INFO - _models.ai_code_generator - Prompt length: 16788 characters
2025-10-12 09:36:55,675 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-10-12 09:36:55,675 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-10-12 09:36:55,675 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-10-12 09:40:42,457 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-10-12 09:40:42,458 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-10-12 09:40:42,471 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20251012_094042_attempt1.txt
2025-10-12 09:40:42,471 - INFO - _models.ai_code_generator - GPT suggested correction: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    import torch\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch import nn, optim\n    import torch.nn.functional as F\n    import torch.ao.quantization as tq\n    import math\n    import time\n    import copy\n\n    # --------------------\n    # Hyperparameters with defaults (can be overridden via kwargs)\n    # --------------------\n    hp = {\n        'lr': 1e-3,\n        'weight_decay': 1e-4,\n        'epochs': 15,\n        'batch_size': 8,\n        'dropout': 0.1,\n        'base_channels': 16,\n        'eca_kernel': 3,\n        'pool_stride': 4,\n        'label_smoothing': 0.05,\n        'grad_clip_norm': 1.0,\n        'scheduler_step_size': 0,\n        'scheduler_gamma': 0.5,\n        'num_workers': 4,\n        'use_amp': True,\n        'quantization_bits': 8,\n        'quantize_weights': True,\n        'quantize_activations': True,  # ignored in dynamic path\n        'calibrate_batches': 4,        # ignored in dynamic path\n    }\n    hp.update(kwargs or {})\n\n    # --------------------\n    # Helpers\n    # --------------------\n    def _to_tensor(x, dtype=None):\n        if isinstance(x, torch.Tensor):\n            return x.to(dtype=dtype) if dtype is not None else x\n        return torch.as_tensor(x, dtype=dtype)\n\n    def _normalize_targets(y, num_classes):\n        # Robustly extract label tensor from possibly nested structures\n        while isinstance(y, (list, tuple)):\n            y = y[0]\n        if not isinstance(y, torch.Tensor):\n            y = torch.as_tensor(y)\n        if y.ndim > 1:\n            if y.size(-1) == num_classes:\n                y = y.argmax(dim=-1)\n            else:\n                y = y.squeeze()\n        return y.long()\n\n    def _extract_xy(batch):\n        # Robustly unpack (xb, yb) from possibly nested tuples/lists\n        if isinstance(batch, (list, tuple)):\n            xb = batch[0]\n            yb = batch[1] if len(batch) > 1 else None\n        else:\n            xb, yb = batch, None\n        while isinstance(xb, (list, tuple)):\n            xb = xb[0]\n        while isinstance(yb, (list, tuple)):\n            yb = yb[0]\n        return xb, yb\n\n    # --------------------\n    # Device handling and checks\n    # --------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        raise RuntimeError('This training function requires a CUDA device. Pass device=\"cuda\" or torch.device(\"cuda\").')\n\n    torch.backends.cudnn.benchmark = True\n\n    num_classes = 5\n\n    # --------------------\n    # Convert inputs to tensors and normalize shapes/dtypes\n    # Expect X_*: (N, 6, 6000), y_*: (N,) with class indices 0..4 or 1-hot (N,5)\n    # --------------------\n    X_train = _to_tensor(X_train, dtype=torch.float32)\n    X_val = _to_tensor(X_val, dtype=torch.float32)\n    y_train = _normalize_targets(y_train, num_classes)\n    y_val = _normalize_targets(y_val, num_classes)\n\n    if X_train.ndim != 3:\n        raise ValueError(f'X_train must be 3D (N, C=6, L=6000). Got shape {tuple(X_train.shape)}')\n    if X_val.ndim != 3:\n        raise ValueError(f'X_val must be 3D (N, C=6, L=6000). Got shape {tuple(X_val.shape)}')\n    if X_train.shape[1] != 6 or X_val.shape[1] != 6:\n        raise ValueError('Expected 6 EEG channels.')\n    if X_train.shape[2] != 6000 or X_val.shape[2] != 6000:\n        raise ValueError('Expected sequence length 6000 per epoch.')\n    if y_train.ndim != 1 or y_val.ndim != 1:\n        raise ValueError('y_train and y_val must be 1D label tensors of shape (N,) after normalization.')\n    if X_train.shape[0] != y_train.shape[0] or X_val.shape[0] != y_val.shape[0]:\n        raise ValueError('Mismatched number of samples between X and y.')\n\n    # --------------------\n    # Datasets and DataLoaders (avoid custom multiprocessing context to reduce edge cases)\n    # --------------------\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=int(hp['batch_size']),\n        shuffle=True,\n        num_workers=int(hp['num_workers']),\n        pin_memory=True,\n        drop_last=False,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=max(1, int(hp['batch_size']) // 2),\n        shuffle=False,\n        num_workers=int(hp['num_workers']),\n        pin_memory=True,\n        drop_last=False,\n    )\n\n    # --------------------\n    # Model definition\n    # --------------------\n    class ChannelECA(nn.Module):\n        def __init__(self, channels: int, k: int = 3):\n            super().__init__()\n            k = max(1, k)\n            if k % 2 == 0:\n                k += 1\n            self.avg_pool = nn.AdaptiveAvgPool1d(1)\n            self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)\n            self.sig = nn.Sigmoid()\n\n        def forward(self, x):  # x: (B, C, L)\n            y = self.avg_pool(x)            # (B, C, 1)\n            y = y.permute(0, 2, 1)          # (B, 1, C)\n            y = self.conv(y)                # (B, 1, C)\n            y = self.sig(y).permute(0, 2, 1)  # (B, C, 1)\n            return x * y                    # broadcast over L\n\n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, in_ch: int, out_ch: int, k: int = 7, stride: int = 1, p: float = 0.0):\n            super().__init__()\n            pad = (k - 1) // 2\n            self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=stride, padding=pad, groups=in_ch, bias=False)\n            self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n        def forward(self, x):\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class ConvBlock(nn.Module):\n        def __init__(self, in_ch, out_ch, k=5, s=2, p: float = 0.0):\n            super().__init__()\n            pad = (k - 1) // 2\n            self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=pad, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n        def forward(self, x):\n            x = self.conv(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class ClassifierHead(nn.Module):\n        def __init__(self, in_dim, hidden, out_dim, p: float = 0.1):\n            super().__init__()\n            # Removed QuantStub/DeQuantStub to avoid static quantization issues\n            self.fc1 = nn.Linear(in_dim, hidden)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n            self.fc2 = nn.Linear(hidden, out_dim)\n        def forward(self, x):  # x: (B, in_dim)\n            x = self.fc1(x)\n            x = self.act(x)\n            x = self.drop(x)\n            x = self.fc2(x)\n            return x\n\n    class EEGSleepNet(nn.Module):\n        def __init__(self, base_channels=16, eca_k=3, pool_stride=4, dropout=0.1, num_classes=5):\n            super().__init__()\n            self.eca = ChannelECA(6, k=eca_k)\n            c1 = base_channels\n            c2 = base_channels * 2\n            c3 = base_channels * 4\n            self.conv1 = ConvBlock(6, c1, k=7, s=2, p=dropout)\n            self.conv2 = ConvBlock(c1, c2, k=5, s=2, p=dropout)\n            self.conv3 = ConvBlock(c2, c3, k=5, s=2, p=dropout)\n            self.ds = DepthwiseSeparableConv1d(c3, c3, k=7, stride=1, p=dropout)\n            self.pool = nn.AvgPool1d(kernel_size=pool_stride, stride=pool_stride)\n\n            d_model = c3\n            self.bigru = nn.GRU(input_size=d_model, hidden_size=d_model, num_layers=1, batch_first=False, bidirectional=True)\n\n            head_in = d_model * 2\n            head_hidden = max(32, d_model)\n            self.head = ClassifierHead(head_in, head_hidden, num_classes, p=dropout)\n\n        def forward(self, x):  # x: (B, 6, 6000)\n            x = self.eca(x)\n            x = self.conv1(x)\n            x = self.conv2(x)\n            x = self.conv3(x)\n            x = self.ds(x)\n            x = self.pool(x)  # (B, C, L')\n            x = x.transpose(1, 2)  # (B, L', C)\n            x = x.transpose(0, 1)  # (L', B, C)\n            out, _ = self.bigru(x)  # (L', B, 2C)\n            out = out.mean(dim=0)   # (B, 2C)\n            logits = self.head(out) # (B, num_classes)\n            return logits\n\n    # --------------------\n    # Instantiate model\n    # --------------------\n    model = EEGSleepNet(\n        base_channels=int(hp['base_channels']),\n        eca_k=int(hp['eca_kernel']),\n        pool_stride=int(hp['pool_stride']),\n        dropout=float(hp['dropout']),\n        num_classes=num_classes,\n    ).to(device)\n\n    # --------------------\n    # Optimizer, loss, scheduler\n    # --------------------\n    optimizer = optim.AdamW(model.parameters(), lr=float(hp['lr']), weight_decay=float(hp['weight_decay']))\n    criterion = nn.CrossEntropyLoss(label_smoothing=float(hp['label_smoothing'])).to(device)\n\n    scheduler = None\n    if int(hp['scheduler_step_size']) > 0:\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=int(hp['scheduler_step_size']), gamma=float(hp['scheduler_gamma']))\n\n    scaler = torch.cuda.amp.GradScaler(enabled=bool(hp['use_amp']))\n\n    # --------------------\n    # Training loop\n    # --------------------\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, int(hp['epochs']) + 1):\n        model.train()\n        epoch_loss = 0.0\n        n_train = 0\n        t0 = time.time()\n\n        for batch in train_loader:\n            xb, yb = _extract_xy(batch)\n\n            # Normalize targets\n            yb = _normalize_targets(yb, num_classes)\n\n            xb = xb.to(device, non_blocking=True).float()\n            yb = yb.to(device, non_blocking=True)\n\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=bool(hp['use_amp'])):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n\n            scaler.scale(loss).backward()\n            if hp['grad_clip_norm'] is not None and float(hp['grad_clip_norm']) > 0:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(hp['grad_clip_norm']))\n            scaler.step(optimizer)\n            scaler.update()\n\n            batch_size_now = xb.size(0)\n            epoch_loss += loss.detach().item() * batch_size_now\n            n_train += batch_size_now\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_loss = epoch_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _extract_xy(batch)\n                yb = _normalize_targets(yb, num_classes)\n\n                xb = xb.to(device, non_blocking=True).float()\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                pred = logits.argmax(dim=1)\n                correct += (pred == yb).sum().item()\n\n        val_loss = val_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} time={time.time()-t0:.1f}s')\n\n    # --------------------\n    # Post-Training Quantization (Dynamic quantization only for stability)\n    # --------------------\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n\n    qb = int(hp['quantization_bits'])\n    qw = bool(hp['quantize_weights'])\n\n    if qw and qb == 8:\n        # Dynamic quantization for Linear and GRU layers\n        model_cpu = tq.quantize_dynamic(\n            model_cpu,\n            {nn.Linear, nn.GRU},\n            dtype=torch.qint8,\n            inplace=True\n        )\n    elif qw and qb == 16:\n        # Half precision weights\n        model_cpu = model_cpu.half()\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return model_cpu, metrics\n"}
2025-10-12 09:40:42,471 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-10-12 09:40:42,471 - INFO - _models.training_function_executor - GPT suggested corrections: {"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **kwargs):\n    import torch\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch import nn, optim\n    import torch.nn.functional as F\n    import torch.ao.quantization as tq\n    import math\n    import time\n    import copy\n\n    # --------------------\n    # Hyperparameters with defaults (can be overridden via kwargs)\n    # --------------------\n    hp = {\n        'lr': 1e-3,\n        'weight_decay': 1e-4,\n        'epochs': 15,\n        'batch_size': 8,\n        'dropout': 0.1,\n        'base_channels': 16,\n        'eca_kernel': 3,\n        'pool_stride': 4,\n        'label_smoothing': 0.05,\n        'grad_clip_norm': 1.0,\n        'scheduler_step_size': 0,\n        'scheduler_gamma': 0.5,\n        'num_workers': 4,\n        'use_amp': True,\n        'quantization_bits': 8,\n        'quantize_weights': True,\n        'quantize_activations': True,  # ignored in dynamic path\n        'calibrate_batches': 4,        # ignored in dynamic path\n    }\n    hp.update(kwargs or {})\n\n    # --------------------\n    # Helpers\n    # --------------------\n    def _to_tensor(x, dtype=None):\n        if isinstance(x, torch.Tensor):\n            return x.to(dtype=dtype) if dtype is not None else x\n        return torch.as_tensor(x, dtype=dtype)\n\n    def _normalize_targets(y, num_classes):\n        # Robustly extract label tensor from possibly nested structures\n        while isinstance(y, (list, tuple)):\n            y = y[0]\n        if not isinstance(y, torch.Tensor):\n            y = torch.as_tensor(y)\n        if y.ndim > 1:\n            if y.size(-1) == num_classes:\n                y = y.argmax(dim=-1)\n            else:\n                y = y.squeeze()\n        return y.long()\n\n    def _extract_xy(batch):\n        # Robustly unpack (xb, yb) from possibly nested tuples/lists\n        if isinstance(batch, (list, tuple)):\n            xb = batch[0]\n            yb = batch[1] if len(batch) > 1 else None\n        else:\n            xb, yb = batch, None\n        while isinstance(xb, (list, tuple)):\n            xb = xb[0]\n        while isinstance(yb, (list, tuple)):\n            yb = yb[0]\n        return xb, yb\n\n    # --------------------\n    # Device handling and checks\n    # --------------------\n    device = torch.device(device)\n    if device.type != 'cuda':\n        raise RuntimeError('This training function requires a CUDA device. Pass device=\"cuda\" or torch.device(\"cuda\").')\n\n    torch.backends.cudnn.benchmark = True\n\n    num_classes = 5\n\n    # --------------------\n    # Convert inputs to tensors and normalize shapes/dtypes\n    # Expect X_*: (N, 6, 6000), y_*: (N,) with class indices 0..4 or 1-hot (N,5)\n    # --------------------\n    X_train = _to_tensor(X_train, dtype=torch.float32)\n    X_val = _to_tensor(X_val, dtype=torch.float32)\n    y_train = _normalize_targets(y_train, num_classes)\n    y_val = _normalize_targets(y_val, num_classes)\n\n    if X_train.ndim != 3:\n        raise ValueError(f'X_train must be 3D (N, C=6, L=6000). Got shape {tuple(X_train.shape)}')\n    if X_val.ndim != 3:\n        raise ValueError(f'X_val must be 3D (N, C=6, L=6000). Got shape {tuple(X_val.shape)}')\n    if X_train.shape[1] != 6 or X_val.shape[1] != 6:\n        raise ValueError('Expected 6 EEG channels.')\n    if X_train.shape[2] != 6000 or X_val.shape[2] != 6000:\n        raise ValueError('Expected sequence length 6000 per epoch.')\n    if y_train.ndim != 1 or y_val.ndim != 1:\n        raise ValueError('y_train and y_val must be 1D label tensors of shape (N,) after normalization.')\n    if X_train.shape[0] != y_train.shape[0] or X_val.shape[0] != y_val.shape[0]:\n        raise ValueError('Mismatched number of samples between X and y.')\n\n    # --------------------\n    # Datasets and DataLoaders (avoid custom multiprocessing context to reduce edge cases)\n    # --------------------\n    train_ds = TensorDataset(X_train, y_train)\n    val_ds = TensorDataset(X_val, y_val)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=int(hp['batch_size']),\n        shuffle=True,\n        num_workers=int(hp['num_workers']),\n        pin_memory=True,\n        drop_last=False,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=max(1, int(hp['batch_size']) // 2),\n        shuffle=False,\n        num_workers=int(hp['num_workers']),\n        pin_memory=True,\n        drop_last=False,\n    )\n\n    # --------------------\n    # Model definition\n    # --------------------\n    class ChannelECA(nn.Module):\n        def __init__(self, channels: int, k: int = 3):\n            super().__init__()\n            k = max(1, k)\n            if k % 2 == 0:\n                k += 1\n            self.avg_pool = nn.AdaptiveAvgPool1d(1)\n            self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)\n            self.sig = nn.Sigmoid()\n\n        def forward(self, x):  # x: (B, C, L)\n            y = self.avg_pool(x)            # (B, C, 1)\n            y = y.permute(0, 2, 1)          # (B, 1, C)\n            y = self.conv(y)                # (B, 1, C)\n            y = self.sig(y).permute(0, 2, 1)  # (B, C, 1)\n            return x * y                    # broadcast over L\n\n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, in_ch: int, out_ch: int, k: int = 7, stride: int = 1, p: float = 0.0):\n            super().__init__()\n            pad = (k - 1) // 2\n            self.dw = nn.Conv1d(in_ch, in_ch, kernel_size=k, stride=stride, padding=pad, groups=in_ch, bias=False)\n            self.pw = nn.Conv1d(in_ch, out_ch, kernel_size=1, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n        def forward(self, x):\n            x = self.dw(x)\n            x = self.pw(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class ConvBlock(nn.Module):\n        def __init__(self, in_ch, out_ch, k=5, s=2, p: float = 0.0):\n            super().__init__()\n            pad = (k - 1) // 2\n            self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=pad, bias=False)\n            self.bn = nn.BatchNorm1d(out_ch)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n        def forward(self, x):\n            x = self.conv(x)\n            x = self.bn(x)\n            x = self.act(x)\n            x = self.drop(x)\n            return x\n\n    class ClassifierHead(nn.Module):\n        def __init__(self, in_dim, hidden, out_dim, p: float = 0.1):\n            super().__init__()\n            # Removed QuantStub/DeQuantStub to avoid static quantization issues\n            self.fc1 = nn.Linear(in_dim, hidden)\n            self.act = nn.GELU()\n            self.drop = nn.Dropout(p)\n            self.fc2 = nn.Linear(hidden, out_dim)\n        def forward(self, x):  # x: (B, in_dim)\n            x = self.fc1(x)\n            x = self.act(x)\n            x = self.drop(x)\n            x = self.fc2(x)\n            return x\n\n    class EEGSleepNet(nn.Module):\n        def __init__(self, base_channels=16, eca_k=3, pool_stride=4, dropout=0.1, num_classes=5):\n            super().__init__()\n            self.eca = ChannelECA(6, k=eca_k)\n            c1 = base_channels\n            c2 = base_channels * 2\n            c3 = base_channels * 4\n            self.conv1 = ConvBlock(6, c1, k=7, s=2, p=dropout)\n            self.conv2 = ConvBlock(c1, c2, k=5, s=2, p=dropout)\n            self.conv3 = ConvBlock(c2, c3, k=5, s=2, p=dropout)\n            self.ds = DepthwiseSeparableConv1d(c3, c3, k=7, stride=1, p=dropout)\n            self.pool = nn.AvgPool1d(kernel_size=pool_stride, stride=pool_stride)\n\n            d_model = c3\n            self.bigru = nn.GRU(input_size=d_model, hidden_size=d_model, num_layers=1, batch_first=False, bidirectional=True)\n\n            head_in = d_model * 2\n            head_hidden = max(32, d_model)\n            self.head = ClassifierHead(head_in, head_hidden, num_classes, p=dropout)\n\n        def forward(self, x):  # x: (B, 6, 6000)\n            x = self.eca(x)\n            x = self.conv1(x)\n            x = self.conv2(x)\n            x = self.conv3(x)\n            x = self.ds(x)\n            x = self.pool(x)  # (B, C, L')\n            x = x.transpose(1, 2)  # (B, L', C)\n            x = x.transpose(0, 1)  # (L', B, C)\n            out, _ = self.bigru(x)  # (L', B, 2C)\n            out = out.mean(dim=0)   # (B, 2C)\n            logits = self.head(out) # (B, num_classes)\n            return logits\n\n    # --------------------\n    # Instantiate model\n    # --------------------\n    model = EEGSleepNet(\n        base_channels=int(hp['base_channels']),\n        eca_k=int(hp['eca_kernel']),\n        pool_stride=int(hp['pool_stride']),\n        dropout=float(hp['dropout']),\n        num_classes=num_classes,\n    ).to(device)\n\n    # --------------------\n    # Optimizer, loss, scheduler\n    # --------------------\n    optimizer = optim.AdamW(model.parameters(), lr=float(hp['lr']), weight_decay=float(hp['weight_decay']))\n    criterion = nn.CrossEntropyLoss(label_smoothing=float(hp['label_smoothing'])).to(device)\n\n    scheduler = None\n    if int(hp['scheduler_step_size']) > 0:\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=int(hp['scheduler_step_size']), gamma=float(hp['scheduler_gamma']))\n\n    scaler = torch.cuda.amp.GradScaler(enabled=bool(hp['use_amp']))\n\n    # --------------------\n    # Training loop\n    # --------------------\n    train_losses, val_losses, val_accs = [], [], []\n\n    for epoch in range(1, int(hp['epochs']) + 1):\n        model.train()\n        epoch_loss = 0.0\n        n_train = 0\n        t0 = time.time()\n\n        for batch in train_loader:\n            xb, yb = _extract_xy(batch)\n\n            # Normalize targets\n            yb = _normalize_targets(yb, num_classes)\n\n            xb = xb.to(device, non_blocking=True).float()\n            yb = yb.to(device, non_blocking=True)\n\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=bool(hp['use_amp'])):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n\n            scaler.scale(loss).backward()\n            if hp['grad_clip_norm'] is not None and float(hp['grad_clip_norm']) > 0:\n                scaler.unscale_(optimizer)\n                nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(hp['grad_clip_norm']))\n            scaler.step(optimizer)\n            scaler.update()\n\n            batch_size_now = xb.size(0)\n            epoch_loss += loss.detach().item() * batch_size_now\n            n_train += batch_size_now\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_loss = epoch_loss / max(1, n_train)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        n_val = 0\n        correct = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                xb, yb = _extract_xy(batch)\n                yb = _normalize_targets(yb, num_classes)\n\n                xb = xb.to(device, non_blocking=True).float()\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.detach().item() * xb.size(0)\n                n_val += xb.size(0)\n                pred = logits.argmax(dim=1)\n                correct += (pred == yb).sum().item()\n\n        val_loss = val_loss / max(1, n_val)\n        val_acc = correct / max(1, n_val)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f'Epoch {epoch:03d}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} time={time.time()-t0:.1f}s')\n\n    # --------------------\n    # Post-Training Quantization (Dynamic quantization only for stability)\n    # --------------------\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n\n    qb = int(hp['quantization_bits'])\n    qw = bool(hp['quantize_weights'])\n\n    if qw and qb == 8:\n        # Dynamic quantization for Linear and GRU layers\n        model_cpu = tq.quantize_dynamic(\n            model_cpu,\n            {nn.Linear, nn.GRU},\n            dtype=torch.qint8,\n            inplace=True\n        )\n    elif qw and qb == 16:\n        # Half precision weights\n        model_cpu = model_cpu.half()\n\n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs\n    }\n\n    return model_cpu, metrics\n"}
2025-10-12 09:40:42,471 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-10-12 09:40:42,471 - ERROR - _models.training_function_executor - BO training objective failed: 'tuple' object has no attribute 'numel'
2025-10-12 09:40:42,471 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 412.201s
2025-10-12 09:40:42,479 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: 'tuple' object has no attribute 'numel'
2025-10-12 09:40:42,479 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-10-12 09:40:45,481 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-10-12 09:40:45,481 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß Applying GPT fixes to original JSON file
2025-10-12 09:40:45,483 - INFO - evaluation.code_generation_pipeline_orchestrator - Applying fixes to: generated_training_functions/training_function_torch_tensor_CNN-ECA-BiGRU-ISRUC_1760279282.json
2025-10-12 09:40:45,483 - INFO - _models.training_function_executor - Loaded training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:40:45,483 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-10-12 09:40:45,483 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Updated training_code with GPT fix
2025-10-12 09:40:45,483 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ Saved GPT fixes back to: generated_training_functions/training_function_torch_tensor_CNN-ECA-BiGRU-ISRUC_1760279282.json
2025-10-12 09:40:45,483 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:40:45,483 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-10-12 09:40:45,809 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 2/4
2025-10-12 09:40:45,809 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 2: üì¶ Installing dependencies for GPT-generated training code...
2025-10-12 09:40:45,809 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-10-12 09:40:45,811 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-10-12 09:40:45,811 - INFO - package_installer - Available packages: {'torch'}
2025-10-12 09:40:45,811 - INFO - package_installer - Missing packages: set()
2025-10-12 09:40:45,811 - INFO - package_installer - ‚úÖ All required packages are already available
2025-10-12 09:40:45,811 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-10-12 09:40:45,811 - INFO - data_splitting - Using all 57140 training samples for BO
2025-10-12 09:40:45,811 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 57140 samples (using bo_sample_num=100000000000000)
2025-10-12 09:40:45,811 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'weight_decay', 'epochs', 'batch_size', 'dropout', 'base_channels', 'eca_kernel', 'pool_stride', 'label_smoothing', 'grad_clip_norm', 'scheduler_step_size', 'scheduler_gamma', 'num_workers', 'use_amp', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calibrate_batches']
2025-10-12 09:40:45,811 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-10-12 09:40:45,811 - INFO - data_splitting - Using all 57140 training samples for BO
2025-10-12 09:40:45,811 - INFO - _models.training_function_executor - Using BO subset for optimization: 57140 samples (bo_sample_num=100000000000000)
2025-10-12 09:40:46,458 - INFO - _models.training_function_executor - BO splits - Train: 45712, Val: 11428
2025-10-12 09:40:47,039 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-10-12 09:40:47,039 - INFO - bo.run_bo - Using GPT-generated search space
2025-10-12 09:40:47,040 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-10-12 09:40:47,041 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-10-12 09:40:47,041 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-12 09:40:47,041 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 09:40:47,041 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:40:47,041 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001412035543062636, 'weight_decay': 5.416754583247461e-06, 'epochs': 12, 'batch_size': 12, 'dropout': 0.07800932022121827, 'base_channels': 26, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03337086111390219, 'grad_clip_norm': 1.1429006806487336, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11832019992326419, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 09:40:47,043 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001412035543062636, 'weight_decay': 5.416754583247461e-06, 'epochs': 12, 'batch_size': 12, 'dropout': 0.07800932022121827, 'base_channels': 26, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03337086111390219, 'grad_clip_norm': 1.1429006806487336, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11832019992326419, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 09:41:00,111 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9447 val_loss=0.7309 val_acc=0.7426 time=13.1s
2025-10-12 09:41:13,063 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7436 val_loss=0.6854 val_acc=0.7694 time=12.9s
2025-10-12 09:41:26,042 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.6303 val_loss=0.6392 val_acc=0.7888 time=13.0s
2025-10-12 09:41:39,026 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6114 val_loss=0.6329 val_acc=0.7875 time=13.0s
2025-10-12 09:41:52,033 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.5943 val_loss=0.6197 val_acc=0.7958 time=13.0s
2025-10-12 09:42:05,032 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.5924 val_loss=0.6608 val_acc=0.7786 time=13.0s
2025-10-12 09:42:18,049 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.5882 val_loss=0.6405 val_acc=0.7885 time=13.0s
2025-10-12 09:42:31,095 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.5893 val_loss=0.6626 val_acc=0.7794 time=13.0s
2025-10-12 09:42:44,096 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.5902 val_loss=0.6447 val_acc=0.7870 time=13.0s
2025-10-12 09:42:57,133 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.5864 val_loss=0.6591 val_acc=0.7784 time=13.0s
2025-10-12 09:43:10,157 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.5886 val_loss=0.6355 val_acc=0.7894 time=13.0s
2025-10-12 09:43:23,182 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.5897 val_loss=0.6737 val_acc=0.7685 time=13.0s
2025-10-12 09:43:23,205 - INFO - _models.training_function_executor - Model: 47,011 parameters, 50.5KB storage
2025-10-12 09:43:23,205 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9446523024025479, 0.7436265354668986, 0.6302854172809791, 0.6113784875203084, 0.5942619586696586, 0.5923636076338837, 0.5882392651761668, 0.5893185642809666, 0.5902347158761887, 0.5864078483844517, 0.588612692248719, 0.5897496134253974], 'val_losses': [0.7309424397716541, 0.6854006363314309, 0.6391717468368494, 0.6329069051645125, 0.6197305253602885, 0.6607771677870954, 0.6404514985825993, 0.6625955457172279, 0.6447362816877243, 0.6590667152148174, 0.6354956435424947, 0.6736685275629888], 'val_acc': [0.7425621281064053, 0.7694259712985649, 0.7887644382219111, 0.7875393769688485, 0.795764788239412, 0.7786139306965348, 0.7885019250962548, 0.7794014700735037, 0.7870143507175359, 0.7783514175708786, 0.7893769688484424, 0.7684634231711586], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001412035543062636, 'weight_decay': 5.416754583247461e-06, 'epochs': 12, 'batch_size': 12, 'dropout': 0.07800932022121827, 'base_channels': 26, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03337086111390219, 'grad_clip_norm': 1.1429006806487336, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11832019992326419, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 4}, 'model_parameter_count': 47011, 'model_storage_size_kb': 50.500097656250006, 'model_size_validation': 'PASS'}
2025-10-12 09:43:23,205 - INFO - _models.training_function_executor - BO Objective: base=0.7685, size_penalty=0.0000, final=0.7685
2025-10-12 09:43:23,205 - INFO - _models.training_function_executor - Model: 47,011 parameters, 50.5KB (PASS 256KB limit)
2025-10-12 09:43:23,206 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 156.165s
2025-10-12 09:43:23,209 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7685
2025-10-12 09:43:23,209 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.001s
2025-10-12 09:43:23,209 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.001412035543062636, 'weight_decay': 5.416754583247461e-06, 'epochs': np.int64(12), 'batch_size': 12, 'dropout': 0.07800932022121827, 'base_channels': np.int64(26), 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03337086111390219, 'grad_clip_norm': 1.1429006806487336, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.11832019992326419, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': np.int64(4)}, value=0.7685
2025-10-12 09:43:23,209 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.001412035543062636, 'weight_decay': 5.416754583247461e-06, 'epochs': np.int64(12), 'batch_size': 12, 'dropout': 0.07800932022121827, 'base_channels': np.int64(26), 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03337086111390219, 'grad_clip_norm': 1.1429006806487336, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.11832019992326419, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': np.int64(4)} -> 0.7685
2025-10-12 09:43:23,210 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-10-12 09:43:23,210 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-12 09:43:23,210 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 09:43:23,210 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:43:23,210 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00044754172678939286, 'weight_decay': 1.0672476836323728e-06, 'epochs': 29, 'batch_size': 8, 'dropout': 0.3059264473611898, 'base_channels': 17, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.023277134043030428, 'grad_clip_norm': 0.9077289553976937, 'scheduler_step_size': 2, 'scheduler_gamma': 0.4403911722277749, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 2}
2025-10-12 09:43:23,214 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00044754172678939286, 'weight_decay': 1.0672476836323728e-06, 'epochs': 29, 'batch_size': 8, 'dropout': 0.3059264473611898, 'base_channels': 17, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.023277134043030428, 'grad_clip_norm': 0.9077289553976937, 'scheduler_step_size': 2, 'scheduler_gamma': 0.4403911722277749, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 2}
2025-10-12 09:43:40,163 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0462 val_loss=1.0019 val_acc=0.6320 time=16.9s
2025-10-12 09:43:56,449 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.8449 val_loss=0.7929 val_acc=0.7047 time=16.3s
2025-10-12 09:44:12,657 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7554 val_loss=0.7462 val_acc=0.7382 time=16.2s
2025-10-12 09:44:28,996 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7306 val_loss=0.8031 val_acc=0.7139 time=16.3s
2025-10-12 09:44:45,229 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.6989 val_loss=0.9111 val_acc=0.6923 time=16.2s
2025-10-12 09:45:01,416 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.6910 val_loss=0.8044 val_acc=0.7195 time=16.2s
2025-10-12 09:45:17,694 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.6744 val_loss=0.7824 val_acc=0.7232 time=16.3s
2025-10-12 09:45:33,884 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.6688 val_loss=0.8486 val_acc=0.7183 time=16.2s
2025-10-12 09:45:50,045 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.6642 val_loss=0.9820 val_acc=0.6594 time=16.2s
2025-10-12 09:46:06,273 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6612 val_loss=0.8192 val_acc=0.7241 time=16.2s
2025-10-12 09:46:22,508 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.6589 val_loss=0.8174 val_acc=0.7142 time=16.2s
2025-10-12 09:46:38,693 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.6620 val_loss=0.8571 val_acc=0.6940 time=16.2s
2025-10-12 09:46:54,694 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.6550 val_loss=0.8476 val_acc=0.6867 time=16.0s
2025-10-12 09:47:10,918 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.6611 val_loss=0.8206 val_acc=0.7181 time=16.2s
2025-10-12 09:47:27,037 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.6550 val_loss=0.8475 val_acc=0.7177 time=16.1s
2025-10-12 09:47:43,174 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.6563 val_loss=0.8279 val_acc=0.7118 time=16.1s
2025-10-12 09:47:59,306 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.6560 val_loss=0.8521 val_acc=0.7113 time=16.1s
2025-10-12 09:48:15,529 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.6571 val_loss=0.8776 val_acc=0.6839 time=16.2s
2025-10-12 09:48:31,702 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.6580 val_loss=0.8338 val_acc=0.7146 time=16.2s
2025-10-12 09:48:47,919 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.6508 val_loss=0.8463 val_acc=0.7160 time=16.2s
2025-10-12 09:49:03,982 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.6517 val_loss=0.8380 val_acc=0.7185 time=16.1s
2025-10-12 09:49:20,229 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.6605 val_loss=0.8593 val_acc=0.6873 time=16.2s
2025-10-12 09:49:36,345 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.6582 val_loss=0.8561 val_acc=0.7184 time=16.1s
2025-10-12 09:49:52,534 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.6566 val_loss=0.8335 val_acc=0.7143 time=16.2s
2025-10-12 09:50:08,745 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.6564 val_loss=0.8291 val_acc=0.7156 time=16.2s
2025-10-12 09:50:24,860 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.6560 val_loss=0.9199 val_acc=0.6817 time=16.1s
2025-10-12 09:50:40,972 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.6571 val_loss=0.8448 val_acc=0.7174 time=16.1s
2025-10-12 09:50:57,131 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.6547 val_loss=0.8552 val_acc=0.7157 time=16.2s
2025-10-12 09:51:13,252 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.6557 val_loss=0.8285 val_acc=0.7173 time=16.1s
2025-10-12 09:51:13,255 - INFO - _models.training_function_executor - Model: 86,606 parameters, 372.1KB storage
2025-10-12 09:51:13,255 - WARNING - _models.training_function_executor - Model storage 372.1KB exceeds 256KB limit!
2025-10-12 09:51:13,255 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0461898749061176, 0.844881247039291, 0.7554465256785237, 0.7305743430340503, 0.6988621868863363, 0.6910010071762759, 0.6743608812847502, 0.6687870843451382, 0.6642341237406986, 0.6612031296166792, 0.6588849601031685, 0.6620402593461625, 0.6549801775024494, 0.6610518865441166, 0.6549716470909778, 0.6563208717429058, 0.6560309118205616, 0.6570815633816822, 0.6580414011771962, 0.6508366913776903, 0.6517039567628523, 0.660497362659263, 0.6581963826861463, 0.6565828903085668, 0.6564394089389732, 0.6560024040372046, 0.6570830905655485, 0.6547336846324043, 0.6557151555655134], 'val_losses': [1.0019363834228305, 0.7929020849639067, 0.7462069314125747, 0.8031004858045132, 0.9111328872196484, 0.804352297085459, 0.7823519569606183, 0.8486213700198443, 0.9820339577282313, 0.8191912133490493, 0.8173887202080038, 0.8570713413488752, 0.847645499870979, 0.8206491938104027, 0.8474556094998449, 0.827919218098529, 0.8521003391141235, 0.8776197152764471, 0.8338246722873541, 0.8463373696852031, 0.8379819729412024, 0.8592581100368107, 0.8560921499048616, 0.833499850407321, 0.8290955837259227, 0.9198851934890043, 0.8447520774344193, 0.8552213599986866, 0.8284663872667489], 'val_acc': [0.6319565978298914, 0.7046727336366818, 0.7381869093454673, 0.7138606930346517, 0.6923346167308365, 0.7195484774238712, 0.7232236611830591, 0.7183234161708085, 0.6594329716485824, 0.7240987049352468, 0.7142107105355268, 0.693997199859993, 0.6867343367168358, 0.7180609030451522, 0.7177108855442772, 0.7118480924046202, 0.7113230661533076, 0.6839341967098355, 0.7146482324116206, 0.715960798039902, 0.718498424921246, 0.6872593629681484, 0.7184109205460273, 0.7142982149107455, 0.7156107805390269, 0.6816590829541477, 0.7173608680434022, 0.7156982849142457, 0.7172733636681834], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00044754172678939286, 'weight_decay': 1.0672476836323728e-06, 'epochs': 29, 'batch_size': 8, 'dropout': 0.3059264473611898, 'base_channels': 17, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.023277134043030428, 'grad_clip_norm': 0.9077289553976937, 'scheduler_step_size': 2, 'scheduler_gamma': 0.4403911722277749, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 2}, 'model_parameter_count': 86606, 'model_storage_size_kb': 372.13515625, 'model_size_validation': 'FAIL'}
2025-10-12 09:51:13,255 - INFO - _models.training_function_executor - BO Objective: base=0.7173, size_penalty=0.2268, final=0.4904
2025-10-12 09:51:13,255 - INFO - _models.training_function_executor - Model: 86,606 parameters, 372.1KB (FAIL 256KB limit)
2025-10-12 09:51:13,255 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 470.045s
2025-10-12 09:51:13,256 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.4904
2025-10-12 09:51:13,256 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-10-12 09:51:13,256 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 0.00044754172678939286, 'weight_decay': 1.0672476836323728e-06, 'epochs': np.int64(29), 'batch_size': 8, 'dropout': 0.3059264473611898, 'base_channels': np.int64(17), 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.023277134043030428, 'grad_clip_norm': 0.9077289553976937, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.4403911722277749, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': np.int64(2)}, value=0.4904
2025-10-12 09:51:13,256 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 0.00044754172678939286, 'weight_decay': 1.0672476836323728e-06, 'epochs': np.int64(29), 'batch_size': 8, 'dropout': 0.3059264473611898, 'base_channels': np.int64(17), 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.023277134043030428, 'grad_clip_norm': 0.9077289553976937, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.4403911722277749, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': np.int64(2)} -> 0.4904
2025-10-12 09:51:13,257 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-10-12 09:51:13,257 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-10-12 09:51:13,257 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 09:51:13,257 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:51:13,257 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0036392643453677957, 'weight_decay': 0.007286653737491051, 'epochs': 13, 'batch_size': 4, 'dropout': 0.11544691281107453, 'base_channels': 22, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.08331949117361645, 'grad_clip_norm': 1.2801409407849746, 'scheduler_step_size': 0, 'scheduler_gamma': 0.3303141836240151, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 7}
2025-10-12 09:51:13,258 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0036392643453677957, 'weight_decay': 0.007286653737491051, 'epochs': 13, 'batch_size': 4, 'dropout': 0.11544691281107453, 'base_channels': 22, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.08331949117361645, 'grad_clip_norm': 1.2801409407849746, 'scheduler_step_size': 0, 'scheduler_gamma': 0.3303141836240151, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 7}
2025-10-12 09:51:45,265 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.1555 val_loss=0.9801 val_acc=0.6861 time=32.0s
2025-10-12 09:52:16,380 - INFO - _models.training_function_executor - Epoch 002: train_loss=1.1077 val_loss=1.3266 val_acc=0.5592 time=31.1s
2025-10-12 09:52:47,581 - INFO - _models.training_function_executor - Epoch 003: train_loss=1.1303 val_loss=1.2121 val_acc=0.5288 time=31.2s
2025-10-12 09:53:18,776 - INFO - _models.training_function_executor - Epoch 004: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 09:53:49,884 - INFO - _models.training_function_executor - Epoch 005: train_loss=nan val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 09:54:20,785 - INFO - _models.training_function_executor - Epoch 006: train_loss=nan val_loss=nan val_acc=0.2325 time=30.9s
2025-10-12 09:54:52,021 - INFO - _models.training_function_executor - Epoch 007: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 09:55:23,023 - INFO - _models.training_function_executor - Epoch 008: train_loss=nan val_loss=nan val_acc=0.2325 time=31.0s
2025-10-12 09:55:54,072 - INFO - _models.training_function_executor - Epoch 009: train_loss=nan val_loss=nan val_acc=0.2325 time=31.0s
2025-10-12 09:56:24,915 - INFO - _models.training_function_executor - Epoch 010: train_loss=nan val_loss=nan val_acc=0.2325 time=30.8s
2025-10-12 09:56:56,031 - INFO - _models.training_function_executor - Epoch 011: train_loss=1.0930 val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 09:57:27,106 - INFO - _models.training_function_executor - Epoch 012: train_loss=nan val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 09:57:58,214 - INFO - _models.training_function_executor - Epoch 013: train_loss=nan val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 09:57:58,217 - INFO - _models.training_function_executor - Model: 143,978 parameters, 618.7KB storage
2025-10-12 09:57:58,217 - WARNING - _models.training_function_executor - Model storage 618.7KB exceeds 256KB limit!
2025-10-12 09:57:58,217 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1554775664339667, 1.1076521059673567, 1.130291173750281, nan, nan, nan, nan, nan, nan, nan, 1.0929796948669088, nan, nan], 'val_losses': [0.9800745994791435, 1.3266315001278521, 1.2120883687939474, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'val_acc': [0.6861218060903045, 0.5592404620231012, 0.5287889394469724, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0036392643453677957, 'weight_decay': 0.007286653737491051, 'epochs': 13, 'batch_size': 4, 'dropout': 0.11544691281107453, 'base_channels': 22, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.08331949117361645, 'grad_clip_norm': 1.2801409407849746, 'scheduler_step_size': 0, 'scheduler_gamma': 0.3303141836240151, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 7}, 'model_parameter_count': 143978, 'model_storage_size_kb': 618.6554687500001, 'model_size_validation': 'FAIL'}
2025-10-12 09:57:58,217 - INFO - _models.training_function_executor - BO Objective: base=0.2325, size_penalty=0.7083, final=-0.4758
2025-10-12 09:57:58,217 - INFO - _models.training_function_executor - Model: 143,978 parameters, 618.7KB (FAIL 256KB limit)
2025-10-12 09:57:58,217 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 404.960s
2025-10-12 09:57:58,303 - INFO - bo.run_bo - Updated RF surrogate model with observation: -0.4758
2025-10-12 09:57:58,303 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.085s
2025-10-12 09:57:58,303 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.0036392643453677957, 'weight_decay': 0.007286653737491051, 'epochs': np.int64(13), 'batch_size': 4, 'dropout': 0.11544691281107453, 'base_channels': np.int64(22), 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.08331949117361645, 'grad_clip_norm': 1.2801409407849746, 'scheduler_step_size': np.int64(0), 'scheduler_gamma': 0.3303141836240151, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': np.int64(7)}, value=-0.4758
2025-10-12 09:57:58,303 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.0036392643453677957, 'weight_decay': 0.007286653737491051, 'epochs': np.int64(13), 'batch_size': 4, 'dropout': 0.11544691281107453, 'base_channels': np.int64(22), 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.08331949117361645, 'grad_clip_norm': 1.2801409407849746, 'scheduler_step_size': np.int64(0), 'scheduler_gamma': 0.3303141836240151, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': np.int64(7)} -> -0.4758
2025-10-12 09:57:58,303 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-10-12 09:57:58,303 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 09:57:58,303 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 09:57:58,303 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 09:57:58,303 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001876295952593552, 'weight_decay': 0.0034633508958535042, 'epochs': 11, 'batch_size': 16, 'dropout': 0.03294865872715514, 'base_channels': 30, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.057965307840431836, 'grad_clip_norm': 4.630688056116182, 'scheduler_step_size': 2, 'scheduler_gamma': 0.3232014665057665, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 1}
2025-10-12 09:57:58,304 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001876295952593552, 'weight_decay': 0.0034633508958535042, 'epochs': 11, 'batch_size': 16, 'dropout': 0.03294865872715514, 'base_channels': 30, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.057965307840431836, 'grad_clip_norm': 4.630688056116182, 'scheduler_step_size': 2, 'scheduler_gamma': 0.3232014665057665, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 1}
2025-10-12 09:58:11,945 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9818 val_loss=0.8834 val_acc=0.7048 time=13.6s
2025-10-12 09:58:24,388 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7792 val_loss=0.7063 val_acc=0.7832 time=12.4s
2025-10-12 09:58:36,916 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.6861 val_loss=0.6666 val_acc=0.8026 time=12.5s
2025-10-12 09:58:49,367 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6622 val_loss=0.6768 val_acc=0.7957 time=12.5s
2025-10-12 09:59:01,815 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.6285 val_loss=0.6941 val_acc=0.7848 time=12.4s
2025-10-12 09:59:14,265 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.6167 val_loss=0.6366 val_acc=0.8130 time=12.5s
2025-10-12 09:59:26,753 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.6047 val_loss=0.6439 val_acc=0.8102 time=12.5s
2025-10-12 09:59:39,198 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.5997 val_loss=0.6383 val_acc=0.8147 time=12.4s
2025-10-12 09:59:51,659 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.5957 val_loss=0.6489 val_acc=0.8072 time=12.5s
2025-10-12 10:00:04,120 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.5948 val_loss=0.8448 val_acc=0.7154 time=12.5s
2025-10-12 10:00:16,619 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.5931 val_loss=0.6379 val_acc=0.8140 time=12.5s
2025-10-12 10:00:16,622 - INFO - _models.training_function_executor - Model: 265,930 parameters, 1142.7KB storage
2025-10-12 10:00:16,622 - WARNING - _models.training_function_executor - Model storage 1142.7KB exceeds 256KB limit!
2025-10-12 10:00:16,622 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9818367597144677, 0.7791820960024832, 0.6861319743464533, 0.6621556901769033, 0.6285099185739818, 0.616702415568035, 0.6046876477264715, 0.5997149119395495, 0.5956833420681878, 0.5947860736069998, 0.5931013689973281], 'val_losses': [0.8833680924811812, 0.7063186408799972, 0.6666445408745333, 0.6767953678908196, 0.6941160849853959, 0.6365656285570517, 0.643900684879341, 0.638338309288609, 0.6489303021381734, 0.8448366817995765, 0.6378583610579398], 'val_acc': [0.7048477423871193, 0.7831641582079104, 0.8025901295064753, 0.7956772838641932, 0.7848267413370669, 0.8130031501575079, 0.8102030101505076, 0.8146657332866644, 0.8072278613930697, 0.7154357717885894, 0.8139656982849143], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.001876295952593552, 'weight_decay': 0.0034633508958535042, 'epochs': 11, 'batch_size': 16, 'dropout': 0.03294865872715514, 'base_channels': 30, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.057965307840431836, 'grad_clip_norm': 4.630688056116182, 'scheduler_step_size': 2, 'scheduler_gamma': 0.3232014665057665, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 1}, 'model_parameter_count': 265930, 'model_storage_size_kb': 1142.66796875, 'model_size_validation': 'FAIL'}
2025-10-12 10:00:16,622 - INFO - _models.training_function_executor - BO Objective: base=0.8140, size_penalty=0.8000, final=0.0140
2025-10-12 10:00:16,622 - INFO - _models.training_function_executor - Model: 265,930 parameters, 1142.7KB (FAIL 256KB limit)
2025-10-12 10:00:16,622 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 138.319s
2025-10-12 10:00:16,703 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0140
2025-10-12 10:00:16,703 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.079s
2025-10-12 10:00:16,703 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 0.001876295952593552, 'weight_decay': 0.0034633508958535042, 'epochs': np.int64(11), 'batch_size': np.int64(16), 'dropout': 0.03294865872715514, 'base_channels': np.int64(30), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(4), 'label_smoothing': 0.057965307840431836, 'grad_clip_norm': 4.630688056116182, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.3232014665057665, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(1)}, value=0.0140
2025-10-12 10:00:16,703 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 0.001876295952593552, 'weight_decay': 0.0034633508958535042, 'epochs': np.int64(11), 'batch_size': np.int64(16), 'dropout': 0.03294865872715514, 'base_channels': np.int64(30), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(4), 'label_smoothing': 0.057965307840431836, 'grad_clip_norm': 4.630688056116182, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.3232014665057665, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(1)} -> 0.0140
2025-10-12 10:00:16,703 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-10-12 10:00:16,703 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:00:16,703 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:00:16,703 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:00:16,703 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.000810372261241655, 'weight_decay': 0.002546349773324225, 'epochs': 7, 'batch_size': 12, 'dropout': 0.3177245901607889, 'base_channels': 24, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.06049619744548198, 'grad_clip_norm': 1.2694363688856116, 'scheduler_step_size': 9, 'scheduler_gamma': 0.7055748554662118, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 8}
2025-10-12 10:00:16,704 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.000810372261241655, 'weight_decay': 0.002546349773324225, 'epochs': 7, 'batch_size': 12, 'dropout': 0.3177245901607889, 'base_channels': 24, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.06049619744548198, 'grad_clip_norm': 1.2694363688856116, 'scheduler_step_size': 9, 'scheduler_gamma': 0.7055748554662118, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 8}
2025-10-12 10:00:28,830 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0750 val_loss=1.1749 val_acc=0.5984 time=12.1s
2025-10-12 10:00:40,864 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.8814 val_loss=0.9190 val_acc=0.6720 time=12.0s
2025-10-12 10:00:52,885 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.8085 val_loss=1.0428 val_acc=0.6262 time=12.0s
2025-10-12 10:01:04,923 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7738 val_loss=0.7510 val_acc=0.7673 time=12.0s
2025-10-12 10:01:16,947 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.7520 val_loss=0.7068 val_acc=0.7909 time=12.0s
2025-10-12 10:01:28,958 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7370 val_loss=0.7173 val_acc=0.7800 time=12.0s
2025-10-12 10:01:41,006 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.7208 val_loss=0.8050 val_acc=0.7532 time=12.0s
2025-10-12 10:01:41,008 - INFO - _models.training_function_executor - Model: 170,984 parameters, 734.7KB storage
2025-10-12 10:01:41,008 - WARNING - _models.training_function_executor - Model storage 734.7KB exceeds 256KB limit!
2025-10-12 10:01:41,009 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0750228168405684, 0.881400550751157, 0.8084807376454541, 0.773757902887742, 0.7519623430319963, 0.7369635041423682, 0.7207899336625865], 'val_losses': [1.1749478528914352, 0.9190116597194125, 1.0428001332114032, 0.7510485235871575, 0.7067573630271528, 0.7173450420766099, 0.8050460956722112], 'val_acc': [0.5984424221211061, 0.672033601680084, 0.6261813090654532, 0.7673258662933147, 0.7908645432271614, 0.780014000700035, 0.7532376618830942], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.000810372261241655, 'weight_decay': 0.002546349773324225, 'epochs': 7, 'batch_size': 12, 'dropout': 0.3177245901607889, 'base_channels': 24, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.06049619744548198, 'grad_clip_norm': 1.2694363688856116, 'scheduler_step_size': 9, 'scheduler_gamma': 0.7055748554662118, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 8}, 'model_parameter_count': 170984, 'model_storage_size_kb': 734.6968750000001, 'model_size_validation': 'FAIL'}
2025-10-12 10:01:41,009 - INFO - _models.training_function_executor - BO Objective: base=0.7532, size_penalty=0.8000, final=-0.0468
2025-10-12 10:01:41,009 - INFO - _models.training_function_executor - Model: 170,984 parameters, 734.7KB (FAIL 256KB limit)
2025-10-12 10:01:41,009 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 84.305s
2025-10-12 10:01:41,090 - INFO - bo.run_bo - Updated RF surrogate model with observation: -0.0468
2025-10-12 10:01:41,090 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.079s
2025-10-12 10:01:41,090 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 0.000810372261241655, 'weight_decay': 0.002546349773324225, 'epochs': np.int64(7), 'batch_size': np.int64(12), 'dropout': 0.3177245901607889, 'base_channels': np.int64(24), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.06049619744548198, 'grad_clip_norm': 1.2694363688856116, 'scheduler_step_size': np.int64(9), 'scheduler_gamma': 0.7055748554662118, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(8)}, value=-0.0468
2025-10-12 10:01:41,090 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 0.000810372261241655, 'weight_decay': 0.002546349773324225, 'epochs': np.int64(7), 'batch_size': np.int64(12), 'dropout': 0.3177245901607889, 'base_channels': np.int64(24), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.06049619744548198, 'grad_clip_norm': 1.2694363688856116, 'scheduler_step_size': np.int64(9), 'scheduler_gamma': 0.7055748554662118, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(8)} -> -0.0468
2025-10-12 10:01:41,090 - INFO - bo.run_bo - üîçBO Trial 6: Using RF surrogate + Expected Improvement
2025-10-12 10:01:41,090 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:01:41,090 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:01:41,090 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:01:41,090 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0028456026565961026, 'weight_decay': 0.0012126769355838298, 'epochs': 25, 'batch_size': 8, 'dropout': 0.18068670134249573, 'base_channels': 30, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.09872160932935466, 'grad_clip_norm': 3.2787677022565176, 'scheduler_step_size': 5, 'scheduler_gamma': 0.49267111547044373, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 7}
2025-10-12 10:01:41,091 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0028456026565961026, 'weight_decay': 0.0012126769355838298, 'epochs': 25, 'batch_size': 8, 'dropout': 0.18068670134249573, 'base_channels': 30, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.09872160932935466, 'grad_clip_norm': 3.2787677022565176, 'scheduler_step_size': 5, 'scheduler_gamma': 0.49267111547044373, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 7}
2025-10-12 10:01:57,545 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0996 val_loss=0.9185 val_acc=0.7226 time=16.5s
2025-10-12 10:02:13,923 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.9535 val_loss=0.9348 val_acc=0.7238 time=16.4s
2025-10-12 10:02:30,389 - INFO - _models.training_function_executor - Epoch 003: train_loss=1.0255 val_loss=1.0617 val_acc=0.6524 time=16.5s
2025-10-12 10:02:46,862 - INFO - _models.training_function_executor - Epoch 004: train_loss=1.1482 val_loss=1.0548 val_acc=0.6495 time=16.5s
2025-10-12 10:03:03,222 - INFO - _models.training_function_executor - Epoch 005: train_loss=1.1535 val_loss=1.1980 val_acc=0.5859 time=16.4s
2025-10-12 10:03:19,636 - INFO - _models.training_function_executor - Epoch 006: train_loss=1.1378 val_loss=1.1165 val_acc=0.6210 time=16.4s
2025-10-12 10:03:36,057 - INFO - _models.training_function_executor - Epoch 007: train_loss=1.1028 val_loss=1.0152 val_acc=0.6739 time=16.4s
2025-10-12 10:03:52,444 - INFO - _models.training_function_executor - Epoch 008: train_loss=1.1229 val_loss=1.0652 val_acc=0.6410 time=16.4s
2025-10-12 10:04:08,935 - INFO - _models.training_function_executor - Epoch 009: train_loss=1.1376 val_loss=1.1213 val_acc=0.6022 time=16.5s
2025-10-12 10:04:25,316 - INFO - _models.training_function_executor - Epoch 010: train_loss=1.0411 val_loss=1.0215 val_acc=0.6693 time=16.4s
2025-10-12 10:04:41,714 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.9987 val_loss=1.0470 val_acc=0.6430 time=16.4s
2025-10-12 10:04:58,093 - INFO - _models.training_function_executor - Epoch 012: train_loss=1.0086 val_loss=0.9996 val_acc=0.6712 time=16.4s
2025-10-12 10:05:14,489 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.9954 val_loss=1.0147 val_acc=0.6730 time=16.4s
2025-10-12 10:05:30,897 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.9755 val_loss=0.9364 val_acc=0.7182 time=16.4s
2025-10-12 10:05:47,311 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.9650 val_loss=1.0006 val_acc=0.6714 time=16.4s
2025-10-12 10:06:03,768 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.9504 val_loss=1.0424 val_acc=0.6537 time=16.5s
2025-10-12 10:06:20,157 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.9413 val_loss=0.9491 val_acc=0.7085 time=16.4s
2025-10-12 10:06:36,558 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.9355 val_loss=0.9240 val_acc=0.7284 time=16.4s
2025-10-12 10:06:52,972 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.9235 val_loss=0.9519 val_acc=0.7076 time=16.4s
2025-10-12 10:07:09,361 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.9200 val_loss=0.9515 val_acc=0.7173 time=16.4s
2025-10-12 10:07:25,772 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.9085 val_loss=0.9135 val_acc=0.7272 time=16.4s
2025-10-12 10:07:42,202 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.9070 val_loss=0.9184 val_acc=0.7242 time=16.4s
2025-10-12 10:07:58,656 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.9034 val_loss=0.9386 val_acc=0.7100 time=16.5s
2025-10-12 10:08:15,051 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.9004 val_loss=0.9558 val_acc=0.6987 time=16.4s
2025-10-12 10:08:31,414 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.8975 val_loss=0.8917 val_acc=0.7407 time=16.4s
2025-10-12 10:08:31,417 - INFO - _models.training_function_executor - Model: 265,930 parameters, 1142.7KB storage
2025-10-12 10:08:31,417 - WARNING - _models.training_function_executor - Model storage 1142.7KB exceeds 256KB limit!
2025-10-12 10:08:31,418 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.099583493252922, 0.9535323401556426, 1.025486711166228, 1.1481928706148266, 1.153549696948923, 1.1378140345314895, 1.1027848656502144, 1.1228887204903593, 1.1376037484215489, 1.0411433936166814, 0.9987091731057118, 1.008568446907206, 0.9953992309261962, 0.9754805045347761, 0.9650000884299684, 0.9503990977449688, 0.9412812203387927, 0.9354997128821938, 0.9234562682748717, 0.9200349259687379, 0.9085269841878282, 0.9070488125357046, 0.9033597463707345, 0.9003574612687675, 0.8975398509672233], 'val_losses': [0.918458702871338, 0.9348252738506844, 1.061727856834493, 1.0548078327025405, 1.1980456868730978, 1.1165148352237277, 1.0152187819534304, 1.0652483913879274, 1.1212567270094171, 1.0215288687303095, 1.0470070497302022, 0.9995591691585951, 1.014676943812539, 0.9364148303683838, 1.0005596619950836, 1.042389737910894, 0.9491229005989846, 0.9240169349795753, 0.951944360021198, 0.951475678747836, 0.9135481872715481, 0.918354738090353, 0.9385789613707899, 0.9558450953189335, 0.8917345202569992], 'val_acc': [0.7226111305565278, 0.7238361918095905, 0.6524326216310815, 0.6495449772488624, 0.5859292964648233, 0.6210185509275464, 0.673871193559678, 0.6409695484774238, 0.6022051102555128, 0.6693209660483024, 0.6429821491074553, 0.6712460623031151, 0.6729961498074903, 0.7182359117955898, 0.6714210710535526, 0.6537451872593629, 0.7085229261463073, 0.728386419320966, 0.7075603780189009, 0.7172733636681834, 0.7272488624431221, 0.7241862093104655, 0.7100105005250262, 0.6987224361218061, 0.7407245362268113], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0028456026565961026, 'weight_decay': 0.0012126769355838298, 'epochs': 25, 'batch_size': 8, 'dropout': 0.18068670134249573, 'base_channels': 30, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.09872160932935466, 'grad_clip_norm': 3.2787677022565176, 'scheduler_step_size': 5, 'scheduler_gamma': 0.49267111547044373, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 7}, 'model_parameter_count': 265930, 'model_storage_size_kb': 1142.66796875, 'model_size_validation': 'FAIL'}
2025-10-12 10:08:31,418 - INFO - _models.training_function_executor - BO Objective: base=0.7407, size_penalty=0.8000, final=-0.0593
2025-10-12 10:08:31,418 - INFO - _models.training_function_executor - Model: 265,930 parameters, 1142.7KB (FAIL 256KB limit)
2025-10-12 10:08:31,418 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 410.327s
2025-10-12 10:08:31,500 - INFO - bo.run_bo - Updated RF surrogate model with observation: -0.0593
2025-10-12 10:08:31,500 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.079s
2025-10-12 10:08:31,500 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 0.0028456026565961026, 'weight_decay': 0.0012126769355838298, 'epochs': np.int64(25), 'batch_size': np.int64(8), 'dropout': 0.18068670134249573, 'base_channels': np.int64(30), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(4), 'label_smoothing': 0.09872160932935466, 'grad_clip_norm': 3.2787677022565176, 'scheduler_step_size': np.int64(5), 'scheduler_gamma': 0.49267111547044373, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(7)}, value=-0.0593
2025-10-12 10:08:31,500 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 0.0028456026565961026, 'weight_decay': 0.0012126769355838298, 'epochs': np.int64(25), 'batch_size': np.int64(8), 'dropout': 0.18068670134249573, 'base_channels': np.int64(30), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(4), 'label_smoothing': 0.09872160932935466, 'grad_clip_norm': 3.2787677022565176, 'scheduler_step_size': np.int64(5), 'scheduler_gamma': 0.49267111547044373, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(7)} -> -0.0593
2025-10-12 10:08:31,500 - INFO - bo.run_bo - üîçBO Trial 7: Using RF surrogate + Expected Improvement
2025-10-12 10:08:31,500 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:08:31,500 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:08:31,501 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:08:31,501 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.5430946291632846e-05, 'weight_decay': 1.6772767765876826e-05, 'epochs': 41, 'batch_size': 8, 'dropout': 0.4363885705372246, 'base_channels': 14, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.033589242006252434, 'grad_clip_norm': 3.9422578007474183, 'scheduler_step_size': 2, 'scheduler_gamma': 0.9530280897609378, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 4}
2025-10-12 10:08:31,502 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.5430946291632846e-05, 'weight_decay': 1.6772767765876826e-05, 'epochs': 41, 'batch_size': 8, 'dropout': 0.4363885705372246, 'base_channels': 14, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.033589242006252434, 'grad_clip_norm': 3.9422578007474183, 'scheduler_step_size': 2, 'scheduler_gamma': 0.9530280897609378, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 4}
2025-10-12 10:08:45,212 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.4435 val_loss=1.2931 val_acc=0.5343 time=13.7s
2025-10-12 10:08:58,487 - INFO - _models.training_function_executor - Epoch 002: train_loss=1.2441 val_loss=1.1325 val_acc=0.5526 time=13.3s
2025-10-12 10:09:11,812 - INFO - _models.training_function_executor - Epoch 003: train_loss=1.1803 val_loss=1.1383 val_acc=0.5442 time=13.3s
2025-10-12 10:09:25,157 - INFO - _models.training_function_executor - Epoch 004: train_loss=1.1431 val_loss=1.0794 val_acc=0.5698 time=13.3s
2025-10-12 10:09:38,444 - INFO - _models.training_function_executor - Epoch 005: train_loss=1.1082 val_loss=1.0286 val_acc=0.5991 time=13.3s
2025-10-12 10:09:51,791 - INFO - _models.training_function_executor - Epoch 006: train_loss=1.0844 val_loss=1.0601 val_acc=0.5977 time=13.3s
2025-10-12 10:10:05,069 - INFO - _models.training_function_executor - Epoch 007: train_loss=1.0580 val_loss=1.0338 val_acc=0.5851 time=13.3s
2025-10-12 10:10:18,461 - INFO - _models.training_function_executor - Epoch 008: train_loss=1.0442 val_loss=0.9850 val_acc=0.6377 time=13.4s
2025-10-12 10:10:31,839 - INFO - _models.training_function_executor - Epoch 009: train_loss=1.0312 val_loss=0.9935 val_acc=0.6140 time=13.4s
2025-10-12 10:10:45,190 - INFO - _models.training_function_executor - Epoch 010: train_loss=1.0145 val_loss=1.3184 val_acc=0.5061 time=13.4s
2025-10-12 10:10:58,533 - INFO - _models.training_function_executor - Epoch 011: train_loss=1.0041 val_loss=1.2290 val_acc=0.4698 time=13.3s
2025-10-12 10:11:11,924 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.9846 val_loss=0.9249 val_acc=0.6614 time=13.4s
2025-10-12 10:11:25,220 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.9715 val_loss=1.0719 val_acc=0.5735 time=13.3s
2025-10-12 10:11:38,577 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.9571 val_loss=1.1086 val_acc=0.5403 time=13.4s
2025-10-12 10:11:52,019 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.9494 val_loss=0.9940 val_acc=0.6152 time=13.4s
2025-10-12 10:12:05,398 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.9431 val_loss=0.9260 val_acc=0.6491 time=13.4s
2025-10-12 10:12:18,754 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.9323 val_loss=0.9807 val_acc=0.6362 time=13.4s
2025-10-12 10:12:32,049 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.9249 val_loss=0.9485 val_acc=0.6348 time=13.3s
2025-10-12 10:12:45,324 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.9200 val_loss=1.0951 val_acc=0.5588 time=13.3s
2025-10-12 10:12:58,709 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.9071 val_loss=0.9852 val_acc=0.6139 time=13.4s
2025-10-12 10:13:12,038 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.9077 val_loss=1.0094 val_acc=0.6242 time=13.3s
2025-10-12 10:13:25,511 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.8981 val_loss=1.0713 val_acc=0.5927 time=13.5s
2025-10-12 10:13:38,853 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.8900 val_loss=1.5406 val_acc=0.4517 time=13.3s
2025-10-12 10:13:52,221 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.8867 val_loss=0.9755 val_acc=0.6450 time=13.4s
2025-10-12 10:14:05,557 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.8854 val_loss=1.0722 val_acc=0.6086 time=13.3s
2025-10-12 10:14:18,816 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.8775 val_loss=1.8286 val_acc=0.3722 time=13.3s
2025-10-12 10:14:32,159 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.8689 val_loss=1.6313 val_acc=0.4359 time=13.3s
2025-10-12 10:14:45,403 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.8675 val_loss=1.0389 val_acc=0.6221 time=13.2s
2025-10-12 10:14:58,643 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.8633 val_loss=1.0255 val_acc=0.6282 time=13.2s
2025-10-12 10:15:11,827 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.8594 val_loss=1.0750 val_acc=0.6106 time=13.2s
2025-10-12 10:15:25,067 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.8533 val_loss=1.1104 val_acc=0.6143 time=13.2s
2025-10-12 10:15:38,474 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.8519 val_loss=1.0482 val_acc=0.6313 time=13.4s
2025-10-12 10:15:51,819 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.8459 val_loss=1.2590 val_acc=0.5955 time=13.3s
2025-10-12 10:16:05,177 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.8478 val_loss=1.9797 val_acc=0.3807 time=13.4s
2025-10-12 10:16:18,542 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.8378 val_loss=2.1953 val_acc=0.4077 time=13.4s
2025-10-12 10:16:31,893 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.8401 val_loss=1.2963 val_acc=0.5520 time=13.4s
2025-10-12 10:16:45,358 - INFO - _models.training_function_executor - Epoch 037: train_loss=0.8367 val_loss=1.0279 val_acc=0.6334 time=13.5s
2025-10-12 10:16:58,692 - INFO - _models.training_function_executor - Epoch 038: train_loss=0.8328 val_loss=1.0717 val_acc=0.6297 time=13.3s
2025-10-12 10:17:12,019 - INFO - _models.training_function_executor - Epoch 039: train_loss=0.8321 val_loss=1.0395 val_acc=0.6418 time=13.3s
2025-10-12 10:17:25,437 - INFO - _models.training_function_executor - Epoch 040: train_loss=0.8306 val_loss=1.0882 val_acc=0.6355 time=13.4s
2025-10-12 10:17:38,843 - INFO - _models.training_function_executor - Epoch 041: train_loss=0.8271 val_loss=1.0870 val_acc=0.6287 time=13.4s
2025-10-12 10:17:38,845 - INFO - _models.training_function_executor - Model: 59,144 parameters, 254.1KB storage
2025-10-12 10:17:38,846 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.443510580968306, 1.2441467131909516, 1.1802585800241916, 1.1431421969303686, 1.1082424099185813, 1.084431321904882, 1.0580419286600475, 1.0442450257252263, 1.0312433391757307, 1.0145284282887863, 1.0041447428241241, 0.9845579447166712, 0.9715436431841705, 0.957111497248934, 0.94943996057003, 0.9431227255230564, 0.9322644736243579, 0.9249363959951754, 0.9199936291346985, 0.9070574501891173, 0.9076652993761997, 0.8980860116496802, 0.8899947417535522, 0.8867426963329399, 0.8853715037940723, 0.8775073160418356, 0.8689101586339354, 0.8674943516846507, 0.86328617866729, 0.8594099849222588, 0.8532741767337033, 0.8519317752071766, 0.8459137293673454, 0.8478252860604361, 0.8378474320440514, 0.8401439284679562, 0.8367329459998588, 0.8328140856457583, 0.8321463245198282, 0.8305519842573676, 0.8271072712167185], 'val_losses': [1.2930546565868657, 1.13254872827126, 1.1383486786087285, 1.0794390458450644, 1.028642937102892, 1.060127078511517, 1.0338177187932218, 0.984980095458231, 0.9935485834388162, 1.3184490992466302, 1.228988905902463, 0.9249144318661241, 1.071861141147128, 1.1085980606268953, 0.994038146924088, 0.9260198421460747, 0.9806977419125937, 0.9485378138222893, 1.09514237292004, 0.9851790833220767, 1.0093876661320103, 1.0713163089524793, 1.540597123864192, 0.9755325145057168, 1.072150958154975, 1.8286136057635975, 1.6313252952608313, 1.0389439732170123, 1.0255109084301326, 1.075035561192983, 1.110417618398321, 1.0481505608815265, 1.2589817236917271, 1.979702447917564, 2.1952593922719363, 1.296294063183658, 1.02786252850267, 1.071714888889138, 1.0395235890039498, 1.0882476668571484, 1.0869513773442483], 'val_acc': [0.5343017150857543, 0.5525901295064753, 0.5441897094854743, 0.5698284914245713, 0.5991424571228562, 0.5976548827441373, 0.5851417570878544, 0.6377318865943297, 0.6140182009100456, 0.5061253062653133, 0.46981099054952746, 0.6613580679033951, 0.5735036751837592, 0.5403395169758488, 0.6152432621631082, 0.6491074553727686, 0.636156807840392, 0.6348442422121106, 0.5588029401470074, 0.6139306965348268, 0.6241687084354218, 0.5926671333566679, 0.45169758487924394, 0.6449947497374868, 0.6085929296464824, 0.37215610780539027, 0.43585929296464826, 0.6220686034301716, 0.6281939096954847, 0.6106055302765139, 0.6142807140357018, 0.6312565628281414, 0.5954672733636682, 0.38073153657682884, 0.4076828841442072, 0.551977598879944, 0.6334441722086104, 0.6296814840742037, 0.6417570878543927, 0.6354567728386419, 0.6287189359467973], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.5430946291632846e-05, 'weight_decay': 1.6772767765876826e-05, 'epochs': 41, 'batch_size': 8, 'dropout': 0.4363885705372246, 'base_channels': 14, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.033589242006252434, 'grad_clip_norm': 3.9422578007474183, 'scheduler_step_size': 2, 'scheduler_gamma': 0.9530280897609378, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 4}, 'model_parameter_count': 59144, 'model_storage_size_kb': 254.13437500000003, 'model_size_validation': 'PASS'}
2025-10-12 10:17:38,846 - INFO - _models.training_function_executor - BO Objective: base=0.6287, size_penalty=0.0000, final=0.6287
2025-10-12 10:17:38,846 - INFO - _models.training_function_executor - Model: 59,144 parameters, 254.1KB (PASS 256KB limit)
2025-10-12 10:17:38,846 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 547.345s
2025-10-12 10:17:38,928 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6287
2025-10-12 10:17:38,928 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.081s
2025-10-12 10:17:38,928 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 3.5430946291632846e-05, 'weight_decay': 1.6772767765876826e-05, 'epochs': np.int64(41), 'batch_size': np.int64(8), 'dropout': 0.4363885705372246, 'base_channels': np.int64(14), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.033589242006252434, 'grad_clip_norm': 3.9422578007474183, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.9530280897609378, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(4)}, value=0.6287
2025-10-12 10:17:38,928 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 3.5430946291632846e-05, 'weight_decay': 1.6772767765876826e-05, 'epochs': np.int64(41), 'batch_size': np.int64(8), 'dropout': 0.4363885705372246, 'base_channels': np.int64(14), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.033589242006252434, 'grad_clip_norm': 3.9422578007474183, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.9530280897609378, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(4)} -> 0.6287
2025-10-12 10:17:38,928 - INFO - bo.run_bo - üîçBO Trial 8: Using RF surrogate + Expected Improvement
2025-10-12 10:17:38,928 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:17:38,928 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:17:38,928 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:17:38,928 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002570808102044977, 'weight_decay': 3.391609098963755e-06, 'epochs': 30, 'batch_size': 12, 'dropout': 0.3569608924412171, 'base_channels': 29, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.032751458492801595, 'grad_clip_norm': 1.3370612178855952, 'scheduler_step_size': 1, 'scheduler_gamma': 0.6602871990823669, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 5}
2025-10-12 10:17:38,929 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002570808102044977, 'weight_decay': 3.391609098963755e-06, 'epochs': 30, 'batch_size': 12, 'dropout': 0.3569608924412171, 'base_channels': 29, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.032751458492801595, 'grad_clip_norm': 1.3370612178855952, 'scheduler_step_size': 1, 'scheduler_gamma': 0.6602871990823669, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 5}
2025-10-12 10:17:53,210 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0423 val_loss=0.8578 val_acc=0.7055 time=14.3s
2025-10-12 10:18:06,426 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.8202 val_loss=0.7448 val_acc=0.7415 time=13.2s
2025-10-12 10:18:19,739 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7499 val_loss=0.7460 val_acc=0.7536 time=13.3s
2025-10-12 10:18:32,970 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7084 val_loss=0.7487 val_acc=0.7457 time=13.2s
2025-10-12 10:18:46,275 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.6850 val_loss=0.6814 val_acc=0.7674 time=13.3s
2025-10-12 10:18:59,567 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.6659 val_loss=0.6834 val_acc=0.7680 time=13.3s
2025-10-12 10:19:12,862 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.6528 val_loss=0.6940 val_acc=0.7623 time=13.3s
2025-10-12 10:19:26,168 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.6443 val_loss=0.6988 val_acc=0.7637 time=13.3s
2025-10-12 10:19:39,374 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.6377 val_loss=0.6655 val_acc=0.7747 time=13.2s
2025-10-12 10:19:52,627 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6342 val_loss=0.7049 val_acc=0.7632 time=13.3s
2025-10-12 10:20:05,854 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.6313 val_loss=0.6775 val_acc=0.7720 time=13.2s
2025-10-12 10:20:19,156 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.6275 val_loss=0.6418 val_acc=0.7843 time=13.3s
2025-10-12 10:20:32,405 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.6305 val_loss=0.6958 val_acc=0.7657 time=13.2s
2025-10-12 10:20:45,672 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.6270 val_loss=0.6631 val_acc=0.7787 time=13.3s
2025-10-12 10:20:58,913 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.6256 val_loss=0.6884 val_acc=0.7677 time=13.2s
2025-10-12 10:21:12,172 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.6287 val_loss=0.6494 val_acc=0.7822 time=13.3s
2025-10-12 10:21:25,413 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.6242 val_loss=0.7305 val_acc=0.7556 time=13.2s
2025-10-12 10:21:38,700 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.6276 val_loss=0.6698 val_acc=0.7748 time=13.3s
2025-10-12 10:21:51,959 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.6253 val_loss=0.6850 val_acc=0.7693 time=13.3s
2025-10-12 10:22:05,262 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.6262 val_loss=0.6497 val_acc=0.7833 time=13.3s
2025-10-12 10:22:18,595 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.6277 val_loss=0.6550 val_acc=0.7788 time=13.3s
2025-10-12 10:22:31,876 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.6241 val_loss=1.3550 val_acc=0.5518 time=13.3s
2025-10-12 10:22:45,138 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.6259 val_loss=0.6837 val_acc=0.7672 time=13.3s
2025-10-12 10:22:58,425 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.6247 val_loss=0.6470 val_acc=0.7842 time=13.3s
2025-10-12 10:23:11,733 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.6228 val_loss=0.6620 val_acc=0.7763 time=13.3s
2025-10-12 10:23:25,041 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.6251 val_loss=0.6850 val_acc=0.7702 time=13.3s
2025-10-12 10:23:38,319 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.6258 val_loss=0.6724 val_acc=0.7742 time=13.3s
2025-10-12 10:23:51,551 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.6227 val_loss=0.6654 val_acc=0.7776 time=13.2s
2025-10-12 10:24:04,873 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.6278 val_loss=0.6434 val_acc=0.7846 time=13.3s
2025-10-12 10:24:18,139 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.6243 val_loss=0.6364 val_acc=0.7878 time=13.3s
2025-10-12 10:24:18,142 - INFO - _models.training_function_executor - Model: 248,654 parameters, 1068.4KB storage
2025-10-12 10:24:18,142 - WARNING - _models.training_function_executor - Model storage 1068.4KB exceeds 256KB limit!
2025-10-12 10:24:18,142 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0422744387653184, 0.8201796977585498, 0.7499207298202576, 0.7084420359047051, 0.6850089867164778, 0.6659418417621334, 0.6527503645337667, 0.6442831448616058, 0.6377112799730591, 0.6341597315316481, 0.6313278777867107, 0.6274710867409214, 0.6304510282414503, 0.6270102809816025, 0.6255763218271011, 0.6287356937029664, 0.6242192501803984, 0.6275858100233772, 0.6252972378539254, 0.626156472013908, 0.6277095715368299, 0.6240880631711001, 0.6259076561378482, 0.6246983293518241, 0.6228362384272135, 0.6251045589263221, 0.6258114421786464, 0.6226614604218047, 0.6278259398511522, 0.6243305475596765], 'val_losses': [0.8578102825731717, 0.7448064108354084, 0.7459740208648993, 0.7487115746961843, 0.6813924010108934, 0.6834098697125932, 0.6939998841318042, 0.6988215860830849, 0.6654558847360065, 0.7048946528881006, 0.6774877342710054, 0.641823090618104, 0.6958319357549723, 0.6630921067137338, 0.6884397856805097, 0.6494418389425021, 0.7305280062382064, 0.6698290666667657, 0.6849803346638542, 0.6496681391353119, 0.655029980761107, 1.3549673419822585, 0.6837461376872395, 0.6470257301935792, 0.6619757748292717, 0.685016520918563, 0.6724087863790226, 0.6654357844830018, 0.6434002181330594, 0.6363590658623771], 'val_acc': [0.7055477773888694, 0.7415120756037802, 0.7535876793839692, 0.7457122856142807, 0.7674133706685334, 0.7680259012950648, 0.7623381169058453, 0.7636506825341267, 0.7746762338116906, 0.7632131606580329, 0.771963598179909, 0.7843017150857543, 0.7656632831641582, 0.7787014350717536, 0.7676758837941897, 0.782201610080504, 0.7556002800140007, 0.7747637381869094, 0.7692509625481274, 0.7833391669583479, 0.7787889394469724, 0.5518025901295065, 0.7671508575428772, 0.7842142107105355, 0.776338816940847, 0.7702135106755338, 0.7742387119355968, 0.7775638781939097, 0.7845642282114106, 0.7878018900945047], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002570808102044977, 'weight_decay': 3.391609098963755e-06, 'epochs': 30, 'batch_size': 12, 'dropout': 0.3569608924412171, 'base_channels': 29, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.032751458492801595, 'grad_clip_norm': 1.3370612178855952, 'scheduler_step_size': 1, 'scheduler_gamma': 0.6602871990823669, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 5}, 'model_parameter_count': 248654, 'model_storage_size_kb': 1068.43515625, 'model_size_validation': 'FAIL'}
2025-10-12 10:24:18,142 - INFO - _models.training_function_executor - BO Objective: base=0.7878, size_penalty=0.8000, final=-0.0122
2025-10-12 10:24:18,142 - INFO - _models.training_function_executor - Model: 248,654 parameters, 1068.4KB (FAIL 256KB limit)
2025-10-12 10:24:18,142 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 399.214s
2025-10-12 10:24:18,227 - INFO - bo.run_bo - Updated RF surrogate model with observation: -0.0122
2025-10-12 10:24:18,227 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.083s
2025-10-12 10:24:18,227 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 0.002570808102044977, 'weight_decay': 3.391609098963755e-06, 'epochs': np.int64(30), 'batch_size': np.int64(12), 'dropout': 0.3569608924412171, 'base_channels': np.int64(29), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.032751458492801595, 'grad_clip_norm': 1.3370612178855952, 'scheduler_step_size': np.int64(1), 'scheduler_gamma': 0.6602871990823669, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(5)}, value=-0.0122
2025-10-12 10:24:18,227 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 0.002570808102044977, 'weight_decay': 3.391609098963755e-06, 'epochs': np.int64(30), 'batch_size': np.int64(12), 'dropout': 0.3569608924412171, 'base_channels': np.int64(29), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.032751458492801595, 'grad_clip_norm': 1.3370612178855952, 'scheduler_step_size': np.int64(1), 'scheduler_gamma': 0.6602871990823669, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(5)} -> -0.0122
2025-10-12 10:24:18,227 - INFO - bo.run_bo - üîçBO Trial 9: Using RF surrogate + Expected Improvement
2025-10-12 10:24:18,227 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:24:18,228 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:24:18,228 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:24:18,228 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.4081735171237766e-05, 'weight_decay': 6.626502938030894e-05, 'epochs': 42, 'batch_size': 12, 'dropout': 0.16890600982994128, 'base_channels': 14, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.04298585625898298, 'grad_clip_norm': 3.5607588982776264, 'scheduler_step_size': 10, 'scheduler_gamma': 0.9264491351077532, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 2}
2025-10-12 10:24:18,229 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.4081735171237766e-05, 'weight_decay': 6.626502938030894e-05, 'epochs': 42, 'batch_size': 12, 'dropout': 0.16890600982994128, 'base_channels': 14, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.04298585625898298, 'grad_clip_norm': 3.5607588982776264, 'scheduler_step_size': 10, 'scheduler_gamma': 0.9264491351077532, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 2}
2025-10-12 10:24:28,826 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.3516 val_loss=1.6845 val_acc=0.3586 time=10.6s
2025-10-12 10:24:39,287 - INFO - _models.training_function_executor - Epoch 002: train_loss=1.1814 val_loss=1.3020 val_acc=0.4673 time=10.5s
2025-10-12 10:24:49,831 - INFO - _models.training_function_executor - Epoch 003: train_loss=1.1282 val_loss=1.0766 val_acc=0.5862 time=10.5s
2025-10-12 10:25:00,374 - INFO - _models.training_function_executor - Epoch 004: train_loss=1.0862 val_loss=1.4879 val_acc=0.3773 time=10.5s
2025-10-12 10:25:10,847 - INFO - _models.training_function_executor - Epoch 005: train_loss=1.0527 val_loss=1.0212 val_acc=0.6040 time=10.5s
2025-10-12 10:25:21,307 - INFO - _models.training_function_executor - Epoch 006: train_loss=1.0262 val_loss=1.0981 val_acc=0.5496 time=10.5s
2025-10-12 10:25:31,811 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.9984 val_loss=0.9814 val_acc=0.6123 time=10.5s
2025-10-12 10:25:42,307 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.9782 val_loss=1.1090 val_acc=0.5480 time=10.5s
2025-10-12 10:25:52,797 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.9627 val_loss=0.8776 val_acc=0.6834 time=10.5s
2025-10-12 10:26:03,310 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.9412 val_loss=1.0070 val_acc=0.6235 time=10.5s
2025-10-12 10:26:13,822 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.9191 val_loss=1.0013 val_acc=0.6100 time=10.5s
2025-10-12 10:26:24,323 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.9075 val_loss=1.1056 val_acc=0.5522 time=10.5s
2025-10-12 10:26:34,796 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.8983 val_loss=1.2724 val_acc=0.4680 time=10.5s
2025-10-12 10:26:45,315 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.8836 val_loss=0.8927 val_acc=0.6880 time=10.5s
2025-10-12 10:26:55,830 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.8779 val_loss=1.3362 val_acc=0.4207 time=10.5s
2025-10-12 10:27:06,350 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.8668 val_loss=0.9693 val_acc=0.6436 time=10.5s
2025-10-12 10:27:16,845 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.8588 val_loss=0.8637 val_acc=0.7084 time=10.5s
2025-10-12 10:27:27,364 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.8498 val_loss=0.8520 val_acc=0.7095 time=10.5s
2025-10-12 10:27:37,818 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.8406 val_loss=0.8423 val_acc=0.7111 time=10.5s
2025-10-12 10:27:48,354 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.8401 val_loss=1.0302 val_acc=0.6263 time=10.5s
2025-10-12 10:27:58,863 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.8338 val_loss=0.9816 val_acc=0.6504 time=10.5s
2025-10-12 10:28:09,334 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.8274 val_loss=0.8680 val_acc=0.6951 time=10.5s
2025-10-12 10:28:19,769 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.8195 val_loss=1.1340 val_acc=0.5580 time=10.4s
2025-10-12 10:28:30,269 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.8152 val_loss=0.8789 val_acc=0.6967 time=10.5s
2025-10-12 10:28:40,782 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.8085 val_loss=0.9315 val_acc=0.6712 time=10.5s
2025-10-12 10:28:51,297 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.8019 val_loss=0.8434 val_acc=0.7055 time=10.5s
2025-10-12 10:29:01,834 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.8023 val_loss=0.8383 val_acc=0.7174 time=10.5s
2025-10-12 10:29:12,338 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.7997 val_loss=1.1083 val_acc=0.5901 time=10.5s
2025-10-12 10:29:22,836 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.7966 val_loss=0.9260 val_acc=0.6844 time=10.5s
2025-10-12 10:29:33,335 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.7895 val_loss=1.4020 val_acc=0.5094 time=10.5s
2025-10-12 10:29:43,803 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.7847 val_loss=0.8492 val_acc=0.7160 time=10.5s
2025-10-12 10:29:54,279 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.7813 val_loss=0.9341 val_acc=0.6609 time=10.5s
2025-10-12 10:30:04,780 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.7755 val_loss=1.6980 val_acc=0.4400 time=10.5s
2025-10-12 10:30:15,278 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.7747 val_loss=0.9022 val_acc=0.6696 time=10.5s
2025-10-12 10:30:25,753 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.7702 val_loss=0.9249 val_acc=0.6756 time=10.5s
2025-10-12 10:30:36,215 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.7676 val_loss=1.3528 val_acc=0.5457 time=10.5s
2025-10-12 10:30:46,736 - INFO - _models.training_function_executor - Epoch 037: train_loss=0.7653 val_loss=0.9814 val_acc=0.6373 time=10.5s
2025-10-12 10:30:57,245 - INFO - _models.training_function_executor - Epoch 038: train_loss=0.7622 val_loss=0.8374 val_acc=0.7017 time=10.5s
2025-10-12 10:31:07,767 - INFO - _models.training_function_executor - Epoch 039: train_loss=0.7586 val_loss=0.9337 val_acc=0.6831 time=10.5s
2025-10-12 10:31:18,257 - INFO - _models.training_function_executor - Epoch 040: train_loss=0.7559 val_loss=0.9156 val_acc=0.6650 time=10.5s
2025-10-12 10:31:28,768 - INFO - _models.training_function_executor - Epoch 041: train_loss=0.7536 val_loss=0.8670 val_acc=0.6944 time=10.5s
2025-10-12 10:31:39,267 - INFO - _models.training_function_executor - Epoch 042: train_loss=0.7529 val_loss=0.8682 val_acc=0.7003 time=10.5s
2025-10-12 10:31:39,280 - INFO - _models.training_function_executor - Model: 59,144 parameters, 127.1KB storage
2025-10-12 10:31:39,280 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.3516469402437574, 1.1814094597762559, 1.1282169421757273, 1.0861975343120152, 1.0526584721201986, 1.0261584317737373, 0.998373732599275, 0.9781519277246215, 0.9627185847987068, 0.9411604703646211, 0.9191312216636932, 0.907451740249934, 0.8983288693066073, 0.8835604721025065, 0.8779270790587742, 0.866831111838069, 0.8587504006377208, 0.8498182311278563, 0.8405788653681109, 0.8401454621635829, 0.8338039205062419, 0.8274322328375151, 0.8195296895980709, 0.8151754377123606, 0.8085218585446076, 0.8019362706762743, 0.8022767524275698, 0.7996590406636196, 0.7966037876212935, 0.7895328309706429, 0.7846614085281275, 0.7813439453799782, 0.7755321681442109, 0.7747121088136356, 0.7701659285113617, 0.7675588328963733, 0.7653077263456773, 0.7622405472604344, 0.758557860239803, 0.7558545650308642, 0.7535720619447672, 0.7528893298824475], 'val_losses': [1.684489759878967, 1.3019530292434796, 1.076557079358121, 1.4878712541726573, 1.0212269899235515, 1.0981079138082601, 0.9814248529109957, 1.1089689298958305, 0.8776297208924455, 1.0069584606784518, 1.0013339901026788, 1.105628880214808, 1.2724029746064365, 0.8926605437763262, 1.336153639867083, 0.9692902684258672, 0.8636824373869075, 0.8520491336389859, 0.8423484160082717, 1.030218632562535, 0.9816095259615621, 0.8680399937759257, 1.1339535564940462, 0.8788949774671444, 0.9315389549065646, 0.8433839645462308, 0.8383208324400578, 1.1083199032848017, 0.9259758504732822, 1.4019704516279476, 0.8492090503731139, 0.9341016696947916, 1.6979934468286455, 0.9022467239522876, 0.9248753360618984, 1.3528322377872917, 0.9813663633784757, 0.8373760044272229, 0.9336803851973189, 0.9155964278229726, 0.8670487063260482, 0.8681501836043619], 'val_acc': [0.3585929296464823, 0.4672733636681834, 0.5861918095904796, 0.37731886594329717, 0.6040427021351068, 0.5496149807490375, 0.6122681134056703, 0.547952397619881, 0.6834091704585229, 0.6234686734336717, 0.6099929996499825, 0.5522401120056003, 0.4679733986699335, 0.6879593979698985, 0.4207210360518026, 0.6435946797339867, 0.7084354217710885, 0.7094854742737137, 0.7111480574028701, 0.626268813440672, 0.65042002100105, 0.6951347567378369, 0.5580154007700385, 0.6967098354917746, 0.6712460623031151, 0.7055477773888694, 0.7174483724186209, 0.5901295064753238, 0.6843717185859293, 0.5093629681484074, 0.715960798039902, 0.6609205460273013, 0.43997199859992997, 0.6695834791739587, 0.6756212810640532, 0.5456772838641932, 0.6372943647182359, 0.7016975848792439, 0.6831466573328666, 0.6650332516625831, 0.6944347217360868, 0.7002975148757438], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.4081735171237766e-05, 'weight_decay': 6.626502938030894e-05, 'epochs': 42, 'batch_size': 12, 'dropout': 0.16890600982994128, 'base_channels': 14, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.04298585625898298, 'grad_clip_norm': 3.5607588982776264, 'scheduler_step_size': 10, 'scheduler_gamma': 0.9264491351077532, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 2}, 'model_parameter_count': 59144, 'model_storage_size_kb': 127.06718750000002, 'model_size_validation': 'PASS'}
2025-10-12 10:31:39,280 - INFO - _models.training_function_executor - BO Objective: base=0.7003, size_penalty=0.0000, final=0.7003
2025-10-12 10:31:39,280 - INFO - _models.training_function_executor - Model: 59,144 parameters, 127.1KB (PASS 256KB limit)
2025-10-12 10:31:39,280 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 441.052s
2025-10-12 10:31:39,366 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7003
2025-10-12 10:31:39,366 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.084s
2025-10-12 10:31:39,366 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 2.4081735171237766e-05, 'weight_decay': 6.626502938030894e-05, 'epochs': np.int64(42), 'batch_size': np.int64(12), 'dropout': 0.16890600982994128, 'base_channels': np.int64(14), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(3), 'label_smoothing': 0.04298585625898298, 'grad_clip_norm': 3.5607588982776264, 'scheduler_step_size': np.int64(10), 'scheduler_gamma': 0.9264491351077532, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(2)}, value=0.7003
2025-10-12 10:31:39,366 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 2.4081735171237766e-05, 'weight_decay': 6.626502938030894e-05, 'epochs': np.int64(42), 'batch_size': np.int64(12), 'dropout': 0.16890600982994128, 'base_channels': np.int64(14), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(3), 'label_smoothing': 0.04298585625898298, 'grad_clip_norm': 3.5607588982776264, 'scheduler_step_size': np.int64(10), 'scheduler_gamma': 0.9264491351077532, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(2)} -> 0.7003
2025-10-12 10:31:39,366 - INFO - bo.run_bo - üîçBO Trial 10: Using RF surrogate + Expected Improvement
2025-10-12 10:31:39,366 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:31:39,366 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:31:39,366 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:31:39,366 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 2.5383569237966734e-05, 'weight_decay': 1.4971205331574486e-05, 'epochs': 30, 'batch_size': 8, 'dropout': 0.22754470009273314, 'base_channels': 17, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.033209839303030196, 'grad_clip_norm': 0.7585306013922342, 'scheduler_step_size': 9, 'scheduler_gamma': 0.9853012057438603, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 3}
2025-10-12 10:31:39,367 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 2.5383569237966734e-05, 'weight_decay': 1.4971205331574486e-05, 'epochs': 30, 'batch_size': 8, 'dropout': 0.22754470009273314, 'base_channels': 17, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.033209839303030196, 'grad_clip_norm': 0.7585306013922342, 'scheduler_step_size': 9, 'scheduler_gamma': 0.9853012057438603, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 3}
2025-10-12 10:31:53,205 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.3217 val_loss=1.1314 val_acc=0.5654 time=13.8s
2025-10-12 10:32:07,058 - INFO - _models.training_function_executor - Epoch 002: train_loss=1.1736 val_loss=1.0742 val_acc=0.5572 time=13.9s
2025-10-12 10:32:20,876 - INFO - _models.training_function_executor - Epoch 003: train_loss=1.0989 val_loss=0.9928 val_acc=0.6306 time=13.8s
2025-10-12 10:32:34,669 - INFO - _models.training_function_executor - Epoch 004: train_loss=1.0445 val_loss=0.9690 val_acc=0.6285 time=13.8s
2025-10-12 10:32:48,519 - INFO - _models.training_function_executor - Epoch 005: train_loss=1.0002 val_loss=0.9491 val_acc=0.6265 time=13.8s
2025-10-12 10:33:02,370 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.9534 val_loss=0.9168 val_acc=0.6624 time=13.9s
2025-10-12 10:33:16,179 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.9205 val_loss=1.1843 val_acc=0.5253 time=13.8s
2025-10-12 10:33:29,955 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.8960 val_loss=1.1824 val_acc=0.5349 time=13.8s
2025-10-12 10:33:43,772 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.8817 val_loss=0.8970 val_acc=0.6601 time=13.8s
2025-10-12 10:33:57,562 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.8642 val_loss=0.8575 val_acc=0.6842 time=13.8s
2025-10-12 10:34:11,330 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.8500 val_loss=0.9104 val_acc=0.6530 time=13.8s
2025-10-12 10:34:25,168 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.8352 val_loss=1.2255 val_acc=0.5354 time=13.8s
2025-10-12 10:34:38,960 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.8285 val_loss=0.9926 val_acc=0.6245 time=13.8s
2025-10-12 10:34:52,787 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.8209 val_loss=0.9868 val_acc=0.6228 time=13.8s
2025-10-12 10:35:06,527 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.8089 val_loss=1.2080 val_acc=0.5509 time=13.7s
2025-10-12 10:35:20,349 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.8003 val_loss=0.8677 val_acc=0.6834 time=13.8s
2025-10-12 10:35:34,197 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.7945 val_loss=1.0120 val_acc=0.6221 time=13.8s
2025-10-12 10:35:48,019 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.7886 val_loss=0.9890 val_acc=0.6415 time=13.8s
2025-10-12 10:36:01,778 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.7828 val_loss=0.8856 val_acc=0.6758 time=13.8s
2025-10-12 10:36:15,557 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.7757 val_loss=0.8881 val_acc=0.6739 time=13.8s
2025-10-12 10:36:29,335 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.7700 val_loss=0.8131 val_acc=0.7050 time=13.8s
2025-10-12 10:36:43,082 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.7645 val_loss=1.0474 val_acc=0.6111 time=13.7s
2025-10-12 10:36:56,826 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.7588 val_loss=0.7950 val_acc=0.7152 time=13.7s
2025-10-12 10:37:10,711 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.7552 val_loss=1.2941 val_acc=0.5116 time=13.9s
2025-10-12 10:37:24,604 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.7490 val_loss=0.8437 val_acc=0.6975 time=13.9s
2025-10-12 10:37:38,441 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.7457 val_loss=0.8289 val_acc=0.6976 time=13.8s
2025-10-12 10:37:52,274 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.7399 val_loss=0.8257 val_acc=0.7065 time=13.8s
2025-10-12 10:38:06,154 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.7342 val_loss=1.0197 val_acc=0.6519 time=13.9s
2025-10-12 10:38:19,992 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.7314 val_loss=0.9406 val_acc=0.6644 time=13.8s
2025-10-12 10:38:33,774 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.7312 val_loss=0.9272 val_acc=0.6777 time=13.8s
2025-10-12 10:38:33,777 - INFO - _models.training_function_executor - Model: 86,606 parameters, 372.1KB storage
2025-10-12 10:38:33,777 - WARNING - _models.training_function_executor - Model storage 372.1KB exceeds 256KB limit!
2025-10-12 10:38:33,777 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.3216903536604299, 1.1735755619069768, 1.0989259150443813, 1.0444882022863007, 1.0002330094262941, 0.9533556844800513, 0.9204990899110116, 0.8960458527224273, 0.8817114844099392, 0.8641853530819198, 0.8500154320154526, 0.8351793203412923, 0.828494476054697, 0.8208548589252617, 0.8089492077077649, 0.8002903974521481, 0.7944875212740264, 0.7886002871944932, 0.7828368224336586, 0.7757293591976666, 0.7699876145639326, 0.7645411133442803, 0.7588290397963159, 0.7552225303959713, 0.7489839697743698, 0.7456799699687899, 0.7398665659246972, 0.7341598871056125, 0.7313861448681559, 0.7311637123194573], 'val_losses': [1.1314462504376053, 1.0742470096844996, 0.9928324618329525, 0.9690003099404393, 0.9490744731916094, 0.9167878739340221, 1.1843307532962069, 1.1824050336752816, 0.896996962239703, 0.8574970433417425, 0.9103523221598416, 1.2255207113256645, 0.992620017902393, 0.9868241234920139, 1.2079565710002491, 0.8677281586907734, 1.0119767896948388, 0.9889610636513844, 0.8855662331647459, 0.8881170774438726, 0.8131445774695737, 1.0473708596144433, 0.7950163285737276, 1.2941197247554423, 0.8436957429057866, 0.8288695482125764, 0.8256856860722701, 1.0197290115874673, 0.9405528395418775, 0.9272306955461532], 'val_acc': [0.5653657682884144, 0.5572278613930697, 0.6305565278263913, 0.628456422821141, 0.6265313265663283, 0.6624081204060203, 0.5252887644382219, 0.5349142457122856, 0.6601330066503325, 0.6841967098354917, 0.6530451522576128, 0.5354392719635982, 0.6245187259362969, 0.6227686384319217, 0.5509275463773189, 0.6834091704585229, 0.6220686034301716, 0.6414945747287364, 0.6757962898144907, 0.673871193559678, 0.7050227511375569, 0.6111305565278264, 0.7151732586629331, 0.5116380819040952, 0.6974973748687434, 0.6975848792439622, 0.7065103255162758, 0.651907595379769, 0.6644207210360518, 0.6777213860693034], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 2.5383569237966734e-05, 'weight_decay': 1.4971205331574486e-05, 'epochs': 30, 'batch_size': 8, 'dropout': 0.22754470009273314, 'base_channels': 17, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.033209839303030196, 'grad_clip_norm': 0.7585306013922342, 'scheduler_step_size': 9, 'scheduler_gamma': 0.9853012057438603, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 3}, 'model_parameter_count': 86606, 'model_storage_size_kb': 372.13515625, 'model_size_validation': 'FAIL'}
2025-10-12 10:38:33,777 - INFO - _models.training_function_executor - BO Objective: base=0.6777, size_penalty=0.2268, final=0.4509
2025-10-12 10:38:33,777 - INFO - _models.training_function_executor - Model: 86,606 parameters, 372.1KB (FAIL 256KB limit)
2025-10-12 10:38:33,777 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 414.411s
2025-10-12 10:38:33,866 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.4509
2025-10-12 10:38:33,867 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.088s
2025-10-12 10:38:33,867 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 2.5383569237966734e-05, 'weight_decay': 1.4971205331574486e-05, 'epochs': np.int64(30), 'batch_size': np.int64(8), 'dropout': 0.22754470009273314, 'base_channels': np.int64(17), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.033209839303030196, 'grad_clip_norm': 0.7585306013922342, 'scheduler_step_size': np.int64(9), 'scheduler_gamma': 0.9853012057438603, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(3)}, value=0.4509
2025-10-12 10:38:33,867 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 2.5383569237966734e-05, 'weight_decay': 1.4971205331574486e-05, 'epochs': np.int64(30), 'batch_size': np.int64(8), 'dropout': 0.22754470009273314, 'base_channels': np.int64(17), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.033209839303030196, 'grad_clip_norm': 0.7585306013922342, 'scheduler_step_size': np.int64(9), 'scheduler_gamma': 0.9853012057438603, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(3)} -> 0.4509
2025-10-12 10:38:33,867 - INFO - bo.run_bo - üîçBO Trial 11: Using RF surrogate + Expected Improvement
2025-10-12 10:38:33,867 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:38:33,867 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:38:33,867 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:38:33,867 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.6730043413239628e-05, 'weight_decay': 1.4419532382113912e-05, 'epochs': 37, 'batch_size': 12, 'dropout': 0.38445012080820684, 'base_channels': 20, 'eca_kernel': 5, 'pool_stride': 3, 'label_smoothing': 0.038019308229838226, 'grad_clip_norm': 3.480379163885794, 'scheduler_step_size': 5, 'scheduler_gamma': 0.9091234801647764, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 2}
2025-10-12 10:38:33,868 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.6730043413239628e-05, 'weight_decay': 1.4419532382113912e-05, 'epochs': 37, 'batch_size': 12, 'dropout': 0.38445012080820684, 'base_channels': 20, 'eca_kernel': 5, 'pool_stride': 3, 'label_smoothing': 0.038019308229838226, 'grad_clip_norm': 3.480379163885794, 'scheduler_step_size': 5, 'scheduler_gamma': 0.9091234801647764, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 2}
2025-10-12 10:38:46,443 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.4501 val_loss=1.2665 val_acc=0.5166 time=12.6s
2025-10-12 10:38:58,296 - INFO - _models.training_function_executor - Epoch 002: train_loss=1.2414 val_loss=1.2768 val_acc=0.4881 time=11.9s
2025-10-12 10:39:10,153 - INFO - _models.training_function_executor - Epoch 003: train_loss=1.1938 val_loss=1.3053 val_acc=0.4601 time=11.9s
2025-10-12 10:39:22,013 - INFO - _models.training_function_executor - Epoch 004: train_loss=1.1615 val_loss=1.2864 val_acc=0.5249 time=11.9s
2025-10-12 10:39:33,848 - INFO - _models.training_function_executor - Epoch 005: train_loss=1.1330 val_loss=1.3631 val_acc=0.4786 time=11.8s
2025-10-12 10:39:45,684 - INFO - _models.training_function_executor - Epoch 006: train_loss=1.1070 val_loss=1.1015 val_acc=0.5556 time=11.8s
2025-10-12 10:39:57,592 - INFO - _models.training_function_executor - Epoch 007: train_loss=1.0865 val_loss=1.0933 val_acc=0.5710 time=11.9s
2025-10-12 10:40:09,497 - INFO - _models.training_function_executor - Epoch 008: train_loss=1.0659 val_loss=1.1076 val_acc=0.5851 time=11.9s
2025-10-12 10:40:21,332 - INFO - _models.training_function_executor - Epoch 009: train_loss=1.0460 val_loss=1.0891 val_acc=0.5928 time=11.8s
2025-10-12 10:40:33,188 - INFO - _models.training_function_executor - Epoch 010: train_loss=1.0284 val_loss=1.1983 val_acc=0.5115 time=11.9s
2025-10-12 10:40:45,016 - INFO - _models.training_function_executor - Epoch 011: train_loss=1.0104 val_loss=1.0551 val_acc=0.6160 time=11.8s
2025-10-12 10:40:56,899 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.9941 val_loss=1.5202 val_acc=0.3110 time=11.9s
2025-10-12 10:41:08,777 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.9784 val_loss=1.0842 val_acc=0.5783 time=11.9s
2025-10-12 10:41:20,688 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.9633 val_loss=1.0386 val_acc=0.6041 time=11.9s
2025-10-12 10:41:32,590 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.9510 val_loss=1.2017 val_acc=0.5204 time=11.9s
2025-10-12 10:41:44,434 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.9412 val_loss=0.9587 val_acc=0.6504 time=11.8s
2025-10-12 10:41:56,309 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.9348 val_loss=1.0724 val_acc=0.5900 time=11.9s
2025-10-12 10:42:08,198 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.9237 val_loss=0.9308 val_acc=0.6446 time=11.9s
2025-10-12 10:42:20,078 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.9144 val_loss=1.1894 val_acc=0.5397 time=11.9s
2025-10-12 10:42:31,931 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.9093 val_loss=1.0586 val_acc=0.6063 time=11.9s
2025-10-12 10:42:43,816 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.8980 val_loss=1.0656 val_acc=0.6103 time=11.9s
2025-10-12 10:42:55,626 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.8917 val_loss=1.0559 val_acc=0.6033 time=11.8s
2025-10-12 10:43:07,498 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.8882 val_loss=1.2400 val_acc=0.5037 time=11.9s
2025-10-12 10:43:19,346 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.8805 val_loss=0.9775 val_acc=0.6451 time=11.8s
2025-10-12 10:43:31,207 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.8775 val_loss=1.0170 val_acc=0.6285 time=11.9s
2025-10-12 10:43:43,063 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.8705 val_loss=1.3575 val_acc=0.4632 time=11.9s
2025-10-12 10:43:54,961 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.8674 val_loss=1.4483 val_acc=0.4063 time=11.9s
2025-10-12 10:44:06,874 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.8584 val_loss=1.9636 val_acc=0.3188 time=11.9s
2025-10-12 10:44:18,715 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.8609 val_loss=0.9393 val_acc=0.6534 time=11.8s
2025-10-12 10:44:30,610 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.8557 val_loss=1.1140 val_acc=0.6074 time=11.9s
2025-10-12 10:44:42,544 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.8519 val_loss=1.0080 val_acc=0.6227 time=11.9s
2025-10-12 10:44:54,422 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.8477 val_loss=1.1405 val_acc=0.5923 time=11.9s
2025-10-12 10:45:06,270 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.8482 val_loss=1.0964 val_acc=0.6004 time=11.8s
2025-10-12 10:45:18,097 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.8450 val_loss=1.1885 val_acc=0.5623 time=11.8s
2025-10-12 10:45:30,036 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.8403 val_loss=1.0850 val_acc=0.6042 time=11.9s
2025-10-12 10:45:41,854 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.8367 val_loss=1.3046 val_acc=0.5268 time=11.8s
2025-10-12 10:45:53,689 - INFO - _models.training_function_executor - Epoch 037: train_loss=0.8380 val_loss=1.0671 val_acc=0.6115 time=11.8s
2025-10-12 10:45:53,692 - INFO - _models.training_function_executor - Model: 119,290 parameters, 256.3KB storage
2025-10-12 10:45:53,692 - WARNING - _models.training_function_executor - Model storage 256.3KB exceeds 256KB limit!
2025-10-12 10:45:53,692 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.4501355294794063, 1.24141218684585, 1.1937546909251828, 1.161531003681688, 1.1329655662611893, 1.1069763054798065, 1.0865165046515646, 1.0658713478906654, 1.0459769930707479, 1.028414754581005, 1.0103734710211725, 0.9941415834883772, 0.9784400668184282, 0.9632866669851748, 0.951004548643096, 0.9411794053802008, 0.9348343144196041, 0.9236661593162767, 0.9144158231229434, 0.9093001183301033, 0.8979808121846613, 0.8917184487626734, 0.8882246270833167, 0.8805275654167949, 0.8775170875447882, 0.8705416856855832, 0.8674335479475545, 0.8584335308325095, 0.8609337234699438, 0.8557059319887413, 0.8519097934152682, 0.8476694594725763, 0.848157232009868, 0.8449793924239494, 0.8403059983059457, 0.8367180534679863, 0.8380480037339092], 'val_losses': [1.2664801265899643, 1.276777953514487, 1.30528512065086, 1.2863740518579627, 1.363083533934876, 1.1015078748987404, 1.0932513450487202, 1.1075786793350881, 1.0891419558762712, 1.1983100892646688, 1.0550961381396213, 1.5201542240612818, 1.0842336982822436, 1.038642649633675, 1.201663017481576, 0.9587305821152388, 1.0723505787079386, 0.9307733407106523, 1.1893951531615672, 1.0585765150624387, 1.0655664275415009, 1.0559166695063331, 1.2399692397602464, 0.9775186291979584, 1.0170260658960348, 1.3575236596790359, 1.4483127662371538, 1.9635732820923193, 0.9392899415336124, 1.11400453599237, 1.0079831351188042, 1.140520285070464, 1.0964489412794882, 1.1885267866682163, 1.0850457695916185, 1.3045961960790289, 1.067115752180953], 'val_acc': [0.5166258312915646, 0.4880994049702485, 0.460098004900245, 0.5249387469373469, 0.4786489324466223, 0.5555652782639132, 0.5709660483024152, 0.5851417570878544, 0.5928421421071054, 0.5114630731536577, 0.616030801540077, 0.3109905495274764, 0.5783164158207911, 0.6041302065103256, 0.5203885194259713, 0.65042002100105, 0.5899544977248863, 0.6446447322366118, 0.5397269863493175, 0.6063178158907946, 0.6103430171508576, 0.6033426671333567, 0.503675183759188, 0.6450822541127056, 0.6285439271963598, 0.46316065803290163, 0.40628281414070705, 0.3187784389219461, 0.6533951697584879, 0.6073678683934197, 0.6226811340567029, 0.5923171158557928, 0.6003675183759188, 0.5623031151557578, 0.6042177108855443, 0.5267763388169409, 0.6114805740287015], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.6730043413239628e-05, 'weight_decay': 1.4419532382113912e-05, 'epochs': 37, 'batch_size': 12, 'dropout': 0.38445012080820684, 'base_channels': 20, 'eca_kernel': 5, 'pool_stride': 3, 'label_smoothing': 0.038019308229838226, 'grad_clip_norm': 3.480379163885794, 'scheduler_step_size': 5, 'scheduler_gamma': 0.9091234801647764, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 2}, 'model_parameter_count': 119290, 'model_storage_size_kb': 256.287109375, 'model_size_validation': 'FAIL'}
2025-10-12 10:45:53,692 - INFO - _models.training_function_executor - BO Objective: base=0.6115, size_penalty=0.0006, final=0.6109
2025-10-12 10:45:53,692 - INFO - _models.training_function_executor - Model: 119,290 parameters, 256.3KB (FAIL 256KB limit)
2025-10-12 10:45:53,692 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 439.825s
2025-10-12 10:45:53,783 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6109
2025-10-12 10:45:53,784 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.088s
2025-10-12 10:45:53,784 - INFO - bo.run_bo - Recorded observation #11: hparams={'lr': 1.6730043413239628e-05, 'weight_decay': 1.4419532382113912e-05, 'epochs': np.int64(37), 'batch_size': np.int64(12), 'dropout': 0.38445012080820684, 'base_channels': np.int64(20), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(3), 'label_smoothing': 0.038019308229838226, 'grad_clip_norm': 3.480379163885794, 'scheduler_step_size': np.int64(5), 'scheduler_gamma': 0.9091234801647764, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(2)}, value=0.6109
2025-10-12 10:45:53,784 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 11: {'lr': 1.6730043413239628e-05, 'weight_decay': 1.4419532382113912e-05, 'epochs': np.int64(37), 'batch_size': np.int64(12), 'dropout': 0.38445012080820684, 'base_channels': np.int64(20), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(3), 'label_smoothing': 0.038019308229838226, 'grad_clip_norm': 3.480379163885794, 'scheduler_step_size': np.int64(5), 'scheduler_gamma': 0.9091234801647764, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(2)} -> 0.6109
2025-10-12 10:45:53,784 - INFO - bo.run_bo - üîçBO Trial 12: Using RF surrogate + Expected Improvement
2025-10-12 10:45:53,784 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:45:53,784 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:45:53,784 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:45:53,784 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008982074567237212, 'weight_decay': 1.1433741009255374e-05, 'epochs': 10, 'batch_size': 16, 'dropout': 0.17057003187498457, 'base_channels': 16, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.05018640531682004, 'grad_clip_norm': 0.8895256211809951, 'scheduler_step_size': 0, 'scheduler_gamma': 0.8420615163715778, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 10:45:53,785 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008982074567237212, 'weight_decay': 1.1433741009255374e-05, 'epochs': 10, 'batch_size': 16, 'dropout': 0.17057003187498457, 'base_channels': 16, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.05018640531682004, 'grad_clip_norm': 0.8895256211809951, 'scheduler_step_size': 0, 'scheduler_gamma': 0.8420615163715778, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 10:46:02,857 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9978 val_loss=0.8561 val_acc=0.6967 time=9.1s
2025-10-12 10:46:11,542 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.8210 val_loss=0.8013 val_acc=0.7277 time=8.7s
2025-10-12 10:46:20,224 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7675 val_loss=0.7409 val_acc=0.7636 time=8.7s
2025-10-12 10:46:28,907 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7422 val_loss=0.8488 val_acc=0.7200 time=8.7s
2025-10-12 10:46:37,606 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.7219 val_loss=0.6981 val_acc=0.7817 time=8.7s
2025-10-12 10:46:46,308 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7074 val_loss=0.7150 val_acc=0.7758 time=8.7s
2025-10-12 10:46:55,022 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.6960 val_loss=0.6654 val_acc=0.7927 time=8.7s
2025-10-12 10:47:03,742 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.6835 val_loss=0.7711 val_acc=0.7514 time=8.7s
2025-10-12 10:47:12,445 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.6774 val_loss=0.7748 val_acc=0.7492 time=8.7s
2025-10-12 10:47:21,115 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6676 val_loss=0.6875 val_acc=0.7838 time=8.7s
2025-10-12 10:47:21,118 - INFO - _models.training_function_executor - Model: 76,872 parameters, 330.3KB storage
2025-10-12 10:47:21,118 - WARNING - _models.training_function_executor - Model storage 330.3KB exceeds 256KB limit!
2025-10-12 10:47:21,118 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9977565056842902, 0.820968025405381, 0.7675234362050124, 0.7422278635373347, 0.7219437859068394, 0.7073956635247004, 0.6960294610414215, 0.683541942016203, 0.6773515139524863, 0.6675625898473364], 'val_losses': [0.8561477087618047, 0.8012688193948777, 0.7409490638151426, 0.8488355067421349, 0.6980880598818292, 0.7149693268994414, 0.6653767659253577, 0.7711273604103599, 0.7748202399269534, 0.6875108756257973], 'val_acc': [0.6967098354917746, 0.727686384319216, 0.763563178158908, 0.719985999299965, 0.7816765838291915, 0.7758137906895345, 0.7927021351067554, 0.7514000700035002, 0.7492124606230312, 0.7837766888344417], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0008982074567237212, 'weight_decay': 1.1433741009255374e-05, 'epochs': 10, 'batch_size': 16, 'dropout': 0.17057003187498457, 'base_channels': 16, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.05018640531682004, 'grad_clip_norm': 0.8895256211809951, 'scheduler_step_size': 0, 'scheduler_gamma': 0.8420615163715778, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 4}, 'model_parameter_count': 76872, 'model_storage_size_kb': 330.30937500000005, 'model_size_validation': 'FAIL'}
2025-10-12 10:47:21,118 - INFO - _models.training_function_executor - BO Objective: base=0.7838, size_penalty=0.1451, final=0.6386
2025-10-12 10:47:21,118 - INFO - _models.training_function_executor - Model: 76,872 parameters, 330.3KB (FAIL 256KB limit)
2025-10-12 10:47:21,118 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 87.334s
2025-10-12 10:47:21,209 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6386
2025-10-12 10:47:21,209 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.089s
2025-10-12 10:47:21,209 - INFO - bo.run_bo - Recorded observation #12: hparams={'lr': 0.0008982074567237212, 'weight_decay': 1.1433741009255374e-05, 'epochs': np.int64(10), 'batch_size': np.int64(16), 'dropout': 0.17057003187498457, 'base_channels': np.int64(16), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.05018640531682004, 'grad_clip_norm': 0.8895256211809951, 'scheduler_step_size': np.int64(0), 'scheduler_gamma': 0.8420615163715778, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(4)}, value=0.6386
2025-10-12 10:47:21,209 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 12: {'lr': 0.0008982074567237212, 'weight_decay': 1.1433741009255374e-05, 'epochs': np.int64(10), 'batch_size': np.int64(16), 'dropout': 0.17057003187498457, 'base_channels': np.int64(16), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.05018640531682004, 'grad_clip_norm': 0.8895256211809951, 'scheduler_step_size': np.int64(0), 'scheduler_gamma': 0.8420615163715778, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(4)} -> 0.6386
2025-10-12 10:47:21,209 - INFO - bo.run_bo - üîçBO Trial 13: Using RF surrogate + Expected Improvement
2025-10-12 10:47:21,210 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:47:21,210 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:47:21,210 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:47:21,210 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007739618616737286, 'weight_decay': 0.0001522882160404798, 'epochs': 28, 'batch_size': 12, 'dropout': 0.17480448896379994, 'base_channels': 22, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.03539443862026371, 'grad_clip_norm': 3.0733157071303214, 'scheduler_step_size': 1, 'scheduler_gamma': 0.9020840629760757, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 3}
2025-10-12 10:47:21,211 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007739618616737286, 'weight_decay': 0.0001522882160404798, 'epochs': 28, 'batch_size': 12, 'dropout': 0.17480448896379994, 'base_channels': 22, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.03539443862026371, 'grad_clip_norm': 3.0733157071303214, 'scheduler_step_size': 1, 'scheduler_gamma': 0.9020840629760757, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 3}
2025-10-12 10:47:33,792 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9819 val_loss=0.7625 val_acc=0.7388 time=12.6s
2025-10-12 10:47:46,309 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7762 val_loss=0.8973 val_acc=0.6684 time=12.5s
2025-10-12 10:47:58,778 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7216 val_loss=0.7508 val_acc=0.7407 time=12.5s
2025-10-12 10:48:11,251 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6883 val_loss=0.7747 val_acc=0.7143 time=12.5s
2025-10-12 10:48:23,732 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.6639 val_loss=0.6721 val_acc=0.7697 time=12.5s
2025-10-12 10:48:36,264 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.6446 val_loss=0.6394 val_acc=0.7850 time=12.5s
2025-10-12 10:48:48,784 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.6285 val_loss=0.6299 val_acc=0.7954 time=12.5s
2025-10-12 10:49:01,320 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.6127 val_loss=0.8599 val_acc=0.6923 time=12.5s
2025-10-12 10:49:13,855 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.6055 val_loss=0.6065 val_acc=0.8014 time=12.5s
2025-10-12 10:49:26,376 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.5962 val_loss=0.6538 val_acc=0.7860 time=12.5s
2025-10-12 10:49:38,862 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.5850 val_loss=0.6357 val_acc=0.7882 time=12.5s
2025-10-12 10:49:51,379 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.5806 val_loss=0.6632 val_acc=0.7763 time=12.5s
2025-10-12 10:50:03,903 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.5711 val_loss=0.6367 val_acc=0.7882 time=12.5s
2025-10-12 10:50:16,406 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.5675 val_loss=0.6212 val_acc=0.7985 time=12.5s
2025-10-12 10:50:28,921 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.5638 val_loss=0.6566 val_acc=0.7872 time=12.5s
2025-10-12 10:50:41,459 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.5569 val_loss=0.6026 val_acc=0.8015 time=12.5s
2025-10-12 10:50:54,001 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.5526 val_loss=0.6290 val_acc=0.7947 time=12.5s
2025-10-12 10:51:06,527 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.5497 val_loss=0.6136 val_acc=0.8007 time=12.5s
2025-10-12 10:51:19,059 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.5437 val_loss=0.6098 val_acc=0.7959 time=12.5s
2025-10-12 10:51:31,538 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.5418 val_loss=0.6225 val_acc=0.7942 time=12.5s
2025-10-12 10:51:44,067 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.5385 val_loss=0.6102 val_acc=0.8023 time=12.5s
2025-10-12 10:51:56,599 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.5350 val_loss=0.6099 val_acc=0.8008 time=12.5s
2025-10-12 10:52:09,075 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.5350 val_loss=0.6102 val_acc=0.8012 time=12.5s
2025-10-12 10:52:21,570 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.5326 val_loss=0.6158 val_acc=0.8022 time=12.5s
2025-10-12 10:52:34,074 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.5294 val_loss=0.6233 val_acc=0.7953 time=12.5s
2025-10-12 10:52:46,554 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.5282 val_loss=0.6225 val_acc=0.7927 time=12.5s
2025-10-12 10:52:59,056 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.5281 val_loss=0.6686 val_acc=0.7724 time=12.5s
2025-10-12 10:53:11,561 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.5252 val_loss=0.6364 val_acc=0.7889 time=12.5s
2025-10-12 10:53:11,566 - INFO - _models.training_function_executor - Model: 33,971 parameters, 36.5KB storage
2025-10-12 10:53:11,566 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9818981854746422, 0.7761520891677554, 0.7216251754170329, 0.6882745446607935, 0.6639260855908572, 0.6446200398089984, 0.6284731381009165, 0.6127423254859731, 0.6054593746008929, 0.5962203689308111, 0.5850242368086761, 0.58057418398593, 0.5710845231647399, 0.5675168439033964, 0.5637596349642958, 0.5568633934181075, 0.5525863222910843, 0.549670698262665, 0.5436983820902412, 0.5418236288713082, 0.5384588848889438, 0.534994787496466, 0.5350152276662374, 0.5325517254155605, 0.5294357644296899, 0.5282204995557045, 0.5281345611454454, 0.5252323940391094], 'val_losses': [0.7625040871032185, 0.8972531143585034, 0.7508486833763698, 0.774713560784973, 0.6720613052623231, 0.639404087040885, 0.6298515478975087, 0.8598964401854265, 0.6065161592077032, 0.6538011715348091, 0.6356625066047203, 0.6631738604220804, 0.6366818275434344, 0.6211618104852452, 0.6565574176445222, 0.6025554017119154, 0.6290298687580336, 0.6135510634924299, 0.6098208237709305, 0.6224661485106535, 0.6101970233630792, 0.6099360531282708, 0.6102252704986835, 0.6157559555338152, 0.6233125952342027, 0.6225192617542852, 0.6686373902448917, 0.6363817428844685], 'val_acc': [0.7387994399719986, 0.668358417920896, 0.7407245362268113, 0.7142982149107455, 0.7696884844242212, 0.7850017500875044, 0.795414770738537, 0.6923346167308365, 0.8013650682534127, 0.7859642982149108, 0.7881519075953798, 0.7762513125656283, 0.7882394119705985, 0.7984774238711936, 0.7871893594679734, 0.8014525726286315, 0.7947147357367869, 0.8006650332516626, 0.7959397969898495, 0.7941897094854743, 0.8023276163808191, 0.8007525376268814, 0.8011900595029752, 0.8021526076303815, 0.7953272663633182, 0.7927021351067554, 0.7724011200560028, 0.7888519425971299], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0007739618616737286, 'weight_decay': 0.0001522882160404798, 'epochs': 28, 'batch_size': 12, 'dropout': 0.17480448896379994, 'base_channels': 22, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.03539443862026371, 'grad_clip_norm': 3.0733157071303214, 'scheduler_step_size': 1, 'scheduler_gamma': 0.9020840629760757, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 3}, 'model_parameter_count': 33971, 'model_storage_size_kb': 36.492285156250006, 'model_size_validation': 'PASS'}
2025-10-12 10:53:11,566 - INFO - _models.training_function_executor - BO Objective: base=0.7889, size_penalty=0.0000, final=0.7889
2025-10-12 10:53:11,566 - INFO - _models.training_function_executor - Model: 33,971 parameters, 36.5KB (PASS 256KB limit)
2025-10-12 10:53:11,566 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 350.357s
2025-10-12 10:53:11,660 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7889
2025-10-12 10:53:11,660 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.091s
2025-10-12 10:53:11,660 - INFO - bo.run_bo - Recorded observation #13: hparams={'lr': 0.0007739618616737286, 'weight_decay': 0.0001522882160404798, 'epochs': np.int64(28), 'batch_size': np.int64(12), 'dropout': 0.17480448896379994, 'base_channels': np.int64(22), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(3), 'label_smoothing': 0.03539443862026371, 'grad_clip_norm': 3.0733157071303214, 'scheduler_step_size': np.int64(1), 'scheduler_gamma': 0.9020840629760757, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(3)}, value=0.7889
2025-10-12 10:53:11,660 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 13: {'lr': 0.0007739618616737286, 'weight_decay': 0.0001522882160404798, 'epochs': np.int64(28), 'batch_size': np.int64(12), 'dropout': 0.17480448896379994, 'base_channels': np.int64(22), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(3), 'label_smoothing': 0.03539443862026371, 'grad_clip_norm': 3.0733157071303214, 'scheduler_step_size': np.int64(1), 'scheduler_gamma': 0.9020840629760757, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(3)} -> 0.7889
2025-10-12 10:53:11,660 - INFO - bo.run_bo - üîçBO Trial 14: Using RF surrogate + Expected Improvement
2025-10-12 10:53:11,660 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:53:11,660 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:53:11,660 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:53:11,660 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 3.516302465198941e-05, 'weight_decay': 0.00010609794181856222, 'epochs': 35, 'batch_size': 12, 'dropout': 0.14794674355578372, 'base_channels': 8, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.07874420878792557, 'grad_clip_norm': 1.5390212216862735, 'scheduler_step_size': 0, 'scheduler_gamma': 0.9893854971802761, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 4}
2025-10-12 10:53:11,662 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 3.516302465198941e-05, 'weight_decay': 0.00010609794181856222, 'epochs': 35, 'batch_size': 12, 'dropout': 0.14794674355578372, 'base_channels': 8, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.07874420878792557, 'grad_clip_norm': 1.5390212216862735, 'scheduler_step_size': 0, 'scheduler_gamma': 0.9893854971802761, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 4}
2025-10-12 10:53:21,480 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.4146 val_loss=1.2525 val_acc=0.5288 time=9.8s
2025-10-12 10:53:30,890 - INFO - _models.training_function_executor - Epoch 002: train_loss=1.2474 val_loss=1.1904 val_acc=0.5257 time=9.4s
2025-10-12 10:53:40,401 - INFO - _models.training_function_executor - Epoch 003: train_loss=1.1955 val_loss=1.1262 val_acc=0.5682 time=9.5s
2025-10-12 10:53:49,812 - INFO - _models.training_function_executor - Epoch 004: train_loss=1.1674 val_loss=1.1438 val_acc=0.5656 time=9.4s
2025-10-12 10:53:59,311 - INFO - _models.training_function_executor - Epoch 005: train_loss=1.1381 val_loss=1.0757 val_acc=0.5964 time=9.5s
2025-10-12 10:54:08,833 - INFO - _models.training_function_executor - Epoch 006: train_loss=1.1104 val_loss=1.1627 val_acc=0.5459 time=9.5s
2025-10-12 10:54:18,283 - INFO - _models.training_function_executor - Epoch 007: train_loss=1.0819 val_loss=1.0295 val_acc=0.6481 time=9.5s
2025-10-12 10:54:27,811 - INFO - _models.training_function_executor - Epoch 008: train_loss=1.0610 val_loss=1.0535 val_acc=0.6290 time=9.5s
2025-10-12 10:54:37,258 - INFO - _models.training_function_executor - Epoch 009: train_loss=1.0384 val_loss=1.0196 val_acc=0.6432 time=9.4s
2025-10-12 10:54:46,758 - INFO - _models.training_function_executor - Epoch 010: train_loss=1.0145 val_loss=1.2005 val_acc=0.5486 time=9.5s
2025-10-12 10:54:56,204 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.9989 val_loss=0.9965 val_acc=0.6537 time=9.4s
2025-10-12 10:55:05,655 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.9857 val_loss=1.1664 val_acc=0.5514 time=9.5s
2025-10-12 10:55:15,104 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.9731 val_loss=1.1189 val_acc=0.5936 time=9.4s
2025-10-12 10:55:24,622 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.9637 val_loss=1.0078 val_acc=0.6477 time=9.5s
2025-10-12 10:55:34,077 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.9489 val_loss=1.0104 val_acc=0.6517 time=9.5s
2025-10-12 10:55:43,606 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.9434 val_loss=0.9779 val_acc=0.6602 time=9.5s
2025-10-12 10:55:53,040 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.9285 val_loss=0.9432 val_acc=0.6915 time=9.4s
2025-10-12 10:56:02,492 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.9223 val_loss=1.0017 val_acc=0.6556 time=9.5s
2025-10-12 10:56:11,945 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.9177 val_loss=1.0224 val_acc=0.6376 time=9.5s
2025-10-12 10:56:21,360 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.9098 val_loss=0.9862 val_acc=0.6623 time=9.4s
2025-10-12 10:56:30,872 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.9034 val_loss=0.9666 val_acc=0.6697 time=9.5s
2025-10-12 10:56:40,359 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.8987 val_loss=1.2120 val_acc=0.5282 time=9.5s
2025-10-12 10:56:49,809 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.8965 val_loss=1.0412 val_acc=0.6407 time=9.4s
2025-10-12 10:56:59,328 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.8928 val_loss=1.0251 val_acc=0.6572 time=9.5s
2025-10-12 10:57:08,884 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.8910 val_loss=1.2057 val_acc=0.5259 time=9.6s
2025-10-12 10:57:18,342 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.8823 val_loss=1.1553 val_acc=0.5833 time=9.5s
2025-10-12 10:57:27,807 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.8830 val_loss=0.9715 val_acc=0.6811 time=9.5s
2025-10-12 10:57:37,263 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.8765 val_loss=1.0971 val_acc=0.6358 time=9.5s
2025-10-12 10:57:46,648 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.8749 val_loss=0.9803 val_acc=0.6813 time=9.4s
2025-10-12 10:57:56,078 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.8723 val_loss=0.9863 val_acc=0.6784 time=9.4s
2025-10-12 10:58:05,581 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.8709 val_loss=1.0724 val_acc=0.6392 time=9.5s
2025-10-12 10:58:15,067 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.8682 val_loss=0.9873 val_acc=0.6735 time=9.5s
2025-10-12 10:58:24,511 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.8638 val_loss=1.0341 val_acc=0.6681 time=9.4s
2025-10-12 10:58:33,957 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.8603 val_loss=0.9616 val_acc=0.6840 time=9.4s
2025-10-12 10:58:43,394 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.8582 val_loss=1.0084 val_acc=0.6747 time=9.4s
2025-10-12 10:58:43,399 - INFO - _models.training_function_executor - Model: 4,965 parameters, 5.3KB storage
2025-10-12 10:58:43,399 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.4145624165012154, 1.2474218741408754, 1.1954836462933074, 1.1673994202702707, 1.1380613930875454, 1.1104430020074558, 1.081915874022431, 1.0610007016776448, 1.0384023416843455, 1.01449606411412, 0.9989211088216593, 0.9857218737759955, 0.9730605421484163, 0.9636934321552337, 0.9488932410340482, 0.9433956040257877, 0.9284717506374727, 0.922342614940634, 0.9177125754538307, 0.9097945850015229, 0.9033531938810886, 0.8986958887481505, 0.8965068006504029, 0.8928392919828143, 0.8909743439522205, 0.8823221818624487, 0.8830107632720642, 0.8765333263287605, 0.8748879165557827, 0.8722699335015108, 0.8709257154259016, 0.8681629193796117, 0.8638062406908936, 0.8603264852774114, 0.8581836025772038], 'val_losses': [1.2525407308709342, 1.1903851531187803, 1.1261547867301298, 1.143818395003014, 1.0757051714815624, 1.1627434492163362, 1.0295083466519581, 1.053470055378731, 1.0196349542427614, 1.200473908453627, 0.9965394258102754, 1.1663931643301513, 1.1188577714346488, 1.007771976143487, 1.0104430656688466, 0.977912408976151, 0.9431893211855603, 1.0017214568972712, 1.0224088369277045, 0.9861784592964535, 0.966636480679452, 1.2119913790655337, 1.0412192057751675, 1.025050940137457, 1.2056671758819637, 1.1552615296623696, 0.9714797887563872, 1.0971020160905958, 0.9803102371218657, 0.9863424838830592, 1.072358329510163, 0.9873171451419728, 1.0341061673612497, 0.9616343356958419, 1.0083641115293078], 'val_acc': [0.5287889394469724, 0.5257262863143157, 0.5681659082954148, 0.5656282814140707, 0.5964298214910746, 0.5458522926146308, 0.6481449072453622, 0.6289814490724536, 0.6431571578578928, 0.5485649282464123, 0.6536576828841442, 0.5513650682534127, 0.5936296814840742, 0.6477073853692684, 0.6517325866293314, 0.6602205110255512, 0.6914595729786489, 0.6555827791389569, 0.6375568778438921, 0.6623206160308015, 0.6696709835491774, 0.528176408820441, 0.6407070353517675, 0.6572453622681134, 0.5259012950647532, 0.5833041652082605, 0.6811340567028351, 0.6358067903395169, 0.6813090654532726, 0.6784214210710535, 0.6392194609730486, 0.6735211760588029, 0.6680959047952397, 0.6840217010850542, 0.6747462373118656], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 3.516302465198941e-05, 'weight_decay': 0.00010609794181856222, 'epochs': 35, 'batch_size': 12, 'dropout': 0.14794674355578372, 'base_channels': 8, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.07874420878792557, 'grad_clip_norm': 1.5390212216862735, 'scheduler_step_size': 0, 'scheduler_gamma': 0.9893854971802761, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 4}, 'model_parameter_count': 4965, 'model_storage_size_kb': 5.33349609375, 'model_size_validation': 'PASS'}
2025-10-12 10:58:43,399 - INFO - _models.training_function_executor - BO Objective: base=0.6747, size_penalty=0.0000, final=0.6747
2025-10-12 10:58:43,399 - INFO - _models.training_function_executor - Model: 4,965 parameters, 5.3KB (PASS 256KB limit)
2025-10-12 10:58:43,399 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 331.739s
2025-10-12 10:58:43,493 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6747
2025-10-12 10:58:43,493 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.093s
2025-10-12 10:58:43,493 - INFO - bo.run_bo - Recorded observation #14: hparams={'lr': 3.516302465198941e-05, 'weight_decay': 0.00010609794181856222, 'epochs': np.int64(35), 'batch_size': np.int64(12), 'dropout': 0.14794674355578372, 'base_channels': np.int64(8), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.07874420878792557, 'grad_clip_norm': 1.5390212216862735, 'scheduler_step_size': np.int64(0), 'scheduler_gamma': 0.9893854971802761, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(4)}, value=0.6747
2025-10-12 10:58:43,494 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 14: {'lr': 3.516302465198941e-05, 'weight_decay': 0.00010609794181856222, 'epochs': np.int64(35), 'batch_size': np.int64(12), 'dropout': 0.14794674355578372, 'base_channels': np.int64(8), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.07874420878792557, 'grad_clip_norm': 1.5390212216862735, 'scheduler_step_size': np.int64(0), 'scheduler_gamma': 0.9893854971802761, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(4)} -> 0.6747
2025-10-12 10:58:43,494 - INFO - bo.run_bo - üîçBO Trial 15: Using RF surrogate + Expected Improvement
2025-10-12 10:58:43,494 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 10:58:43,494 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 10:58:43,494 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 10:58:43,494 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0006664783191368236, 'weight_decay': 0.0007722901145133004, 'epochs': 38, 'batch_size': 12, 'dropout': 0.007476398047571333, 'base_channels': 22, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.09056097355199394, 'grad_clip_norm': 0.6714650711279913, 'scheduler_step_size': 3, 'scheduler_gamma': 0.9317460383851168, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 10:58:43,495 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0006664783191368236, 'weight_decay': 0.0007722901145133004, 'epochs': 38, 'batch_size': 12, 'dropout': 0.007476398047571333, 'base_channels': 22, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.09056097355199394, 'grad_clip_norm': 0.6714650711279913, 'scheduler_step_size': 3, 'scheduler_gamma': 0.9317460383851168, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 4}
2025-10-12 10:58:55,076 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9974 val_loss=0.9132 val_acc=0.7085 time=11.6s
2025-10-12 10:59:06,677 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.8435 val_loss=0.8374 val_acc=0.7472 time=11.6s
2025-10-12 10:59:18,236 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7961 val_loss=0.8668 val_acc=0.7298 time=11.6s
2025-10-12 10:59:29,779 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7664 val_loss=0.7588 val_acc=0.7924 time=11.5s
2025-10-12 10:59:41,311 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.7481 val_loss=0.7364 val_acc=0.8050 time=11.5s
2025-10-12 10:59:52,846 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7321 val_loss=0.7607 val_acc=0.7939 time=11.5s
2025-10-12 11:00:04,404 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.7207 val_loss=0.7243 val_acc=0.8148 time=11.6s
2025-10-12 11:00:15,966 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.7118 val_loss=0.7216 val_acc=0.8141 time=11.6s
2025-10-12 11:00:27,522 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.7039 val_loss=0.8618 val_acc=0.7427 time=11.6s
2025-10-12 11:00:39,119 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6913 val_loss=0.8181 val_acc=0.7687 time=11.6s
2025-10-12 11:00:50,676 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.6799 val_loss=0.7347 val_acc=0.8085 time=11.6s
2025-10-12 11:01:02,219 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.6732 val_loss=0.7354 val_acc=0.8131 time=11.5s
2025-10-12 11:01:13,778 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.6598 val_loss=0.7350 val_acc=0.8127 time=11.6s
2025-10-12 11:01:25,361 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.6533 val_loss=0.7646 val_acc=0.8036 time=11.6s
2025-10-12 11:01:36,978 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.6442 val_loss=0.7542 val_acc=0.8050 time=11.6s
2025-10-12 11:01:48,549 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.6322 val_loss=0.7672 val_acc=0.7974 time=11.6s
2025-10-12 11:02:00,118 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.6245 val_loss=0.7814 val_acc=0.7975 time=11.6s
2025-10-12 11:02:11,669 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.6161 val_loss=0.7605 val_acc=0.8090 time=11.6s
2025-10-12 11:02:23,243 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.6038 val_loss=0.7804 val_acc=0.8082 time=11.6s
2025-10-12 11:02:34,833 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.5969 val_loss=0.7839 val_acc=0.8069 time=11.6s
2025-10-12 11:02:46,390 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.5885 val_loss=0.7886 val_acc=0.8002 time=11.6s
2025-10-12 11:02:58,006 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.5783 val_loss=0.7811 val_acc=0.8092 time=11.6s
2025-10-12 11:03:09,590 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.5696 val_loss=0.8053 val_acc=0.8029 time=11.6s
2025-10-12 11:03:21,172 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.5656 val_loss=0.8064 val_acc=0.8019 time=11.6s
2025-10-12 11:03:32,749 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.5552 val_loss=0.8228 val_acc=0.7939 time=11.6s
2025-10-12 11:03:44,324 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.5477 val_loss=0.8479 val_acc=0.7913 time=11.6s
2025-10-12 11:03:55,898 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.5473 val_loss=0.8139 val_acc=0.8051 time=11.6s
2025-10-12 11:04:07,472 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.5376 val_loss=0.8291 val_acc=0.8032 time=11.6s
2025-10-12 11:04:19,036 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.5311 val_loss=0.9413 val_acc=0.7475 time=11.6s
2025-10-12 11:04:30,620 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.5316 val_loss=0.8341 val_acc=0.8036 time=11.6s
2025-10-12 11:04:42,244 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.5228 val_loss=0.8399 val_acc=0.8036 time=11.6s
2025-10-12 11:04:53,862 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.5159 val_loss=0.8564 val_acc=0.7964 time=11.6s
2025-10-12 11:05:05,459 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.5139 val_loss=0.8627 val_acc=0.7987 time=11.6s
2025-10-12 11:05:17,005 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.5070 val_loss=0.8621 val_acc=0.7965 time=11.5s
2025-10-12 11:05:28,618 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.5035 val_loss=0.8652 val_acc=0.8008 time=11.6s
2025-10-12 11:05:40,247 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.4992 val_loss=0.8718 val_acc=0.8015 time=11.6s
2025-10-12 11:05:51,798 - INFO - _models.training_function_executor - Epoch 037: train_loss=0.4953 val_loss=0.8903 val_acc=0.7943 time=11.6s
2025-10-12 11:06:03,360 - INFO - _models.training_function_executor - Epoch 038: train_loss=0.4944 val_loss=0.8845 val_acc=0.7925 time=11.6s
2025-10-12 11:06:03,363 - INFO - _models.training_function_executor - Model: 143,978 parameters, 618.7KB storage
2025-10-12 11:06:03,363 - WARNING - _models.training_function_executor - Model storage 618.7KB exceeds 256KB limit!
2025-10-12 11:06:03,363 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9973513494797591, 0.8435020674910627, 0.796076314635301, 0.7663941924278662, 0.7480911330134459, 0.7321108074237468, 0.7206664995235249, 0.7118496648343994, 0.7039071230135016, 0.6913369547733487, 0.6799319304048619, 0.6732005616757558, 0.6598199883728207, 0.6532859501458567, 0.6441698314574451, 0.6322395209670275, 0.6244721356694583, 0.6161230036693517, 0.6038249332352164, 0.5968525917078592, 0.588534166896243, 0.5782770226765939, 0.5695758552391879, 0.5656189664854092, 0.5552285758294048, 0.5476962663481024, 0.5473075237361167, 0.5375654016643251, 0.5310546418102338, 0.5315927321682621, 0.5228317930360836, 0.5159330970947043, 0.5139451534661706, 0.5070342847025807, 0.5035370887108186, 0.4991897140291549, 0.4952687473435021, 0.4943792507688733], 'val_losses': [0.9132234552783235, 0.8373543081278979, 0.8668126817355842, 0.7588300712049362, 0.7363599058700935, 0.7606800310118114, 0.7243208249663823, 0.7216446874001496, 0.8618483828692992, 0.818111981749326, 0.7346823653339881, 0.7354144081551586, 0.7350395801791704, 0.7645647781511516, 0.7541679062280447, 0.7671553739612237, 0.781385213809494, 0.7604608804164835, 0.78042227851268, 0.7839458685187055, 0.788647945637798, 0.7810720892627631, 0.8052893042522666, 0.8064195702949687, 0.822828717996576, 0.8479195764933093, 0.8138572396798529, 0.8291250696476832, 0.9412699559788876, 0.8341484032062788, 0.8398779560666757, 0.8563884357378372, 0.8627387180501946, 0.8621425716754728, 0.865176849367112, 0.8718342359640985, 0.8903234380032994, 0.8845368218058925], 'val_acc': [0.7085229261463073, 0.7471998599929996, 0.7297864893244662, 0.7924396219810991, 0.8050402520126007, 0.793927196359818, 0.8148407420371019, 0.8141407070353518, 0.7427371368568428, 0.7687259362968148, 0.8085404270213511, 0.8130906545327267, 0.8126531326566329, 0.8035526776338817, 0.8049527476373819, 0.7974273713685684, 0.7975148757437872, 0.8089779488974449, 0.808190409520476, 0.8068778438921946, 0.8002275113755688, 0.8092404620231012, 0.8028526426321316, 0.8018900945047253, 0.793927196359818, 0.7913020651032552, 0.8051277563878194, 0.8032026601330067, 0.7474623731186559, 0.8036401820091005, 0.8035526776338817, 0.7963773188659433, 0.7986524326216311, 0.7964648232411621, 0.8008400420021001, 0.8014525726286315, 0.794277213860693, 0.7925271263563178], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0006664783191368236, 'weight_decay': 0.0007722901145133004, 'epochs': 38, 'batch_size': 12, 'dropout': 0.007476398047571333, 'base_channels': 22, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.09056097355199394, 'grad_clip_norm': 0.6714650711279913, 'scheduler_step_size': 3, 'scheduler_gamma': 0.9317460383851168, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 4}, 'model_parameter_count': 143978, 'model_storage_size_kb': 618.6554687500001, 'model_size_validation': 'FAIL'}
2025-10-12 11:06:03,363 - INFO - _models.training_function_executor - BO Objective: base=0.7925, size_penalty=0.7083, final=0.0842
2025-10-12 11:06:03,363 - INFO - _models.training_function_executor - Model: 143,978 parameters, 618.7KB (FAIL 256KB limit)
2025-10-12 11:06:03,363 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 439.869s
2025-10-12 11:06:03,458 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0842
2025-10-12 11:06:03,458 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.093s
2025-10-12 11:06:03,458 - INFO - bo.run_bo - Recorded observation #15: hparams={'lr': 0.0006664783191368236, 'weight_decay': 0.0007722901145133004, 'epochs': np.int64(38), 'batch_size': np.int64(12), 'dropout': 0.007476398047571333, 'base_channels': np.int64(22), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(4), 'label_smoothing': 0.09056097355199394, 'grad_clip_norm': 0.6714650711279913, 'scheduler_step_size': np.int64(3), 'scheduler_gamma': 0.9317460383851168, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(4)}, value=0.0842
2025-10-12 11:06:03,458 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 15: {'lr': 0.0006664783191368236, 'weight_decay': 0.0007722901145133004, 'epochs': np.int64(38), 'batch_size': np.int64(12), 'dropout': 0.007476398047571333, 'base_channels': np.int64(22), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(4), 'label_smoothing': 0.09056097355199394, 'grad_clip_norm': 0.6714650711279913, 'scheduler_step_size': np.int64(3), 'scheduler_gamma': 0.9317460383851168, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(4)} -> 0.0842
2025-10-12 11:06:03,459 - INFO - bo.run_bo - üîçBO Trial 16: Using RF surrogate + Expected Improvement
2025-10-12 11:06:03,459 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 11:06:03,459 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 11:06:03,459 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 11:06:03,459 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0002361157914100513, 'weight_decay': 1.5778233946903882e-05, 'epochs': 10, 'batch_size': 12, 'dropout': 0.04963707416774406, 'base_channels': 11, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.04330192931126272, 'grad_clip_norm': 3.6626502940899597, 'scheduler_step_size': 3, 'scheduler_gamma': 0.2613013381368354, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 6}
2025-10-12 11:06:03,460 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0002361157914100513, 'weight_decay': 1.5778233946903882e-05, 'epochs': 10, 'batch_size': 12, 'dropout': 0.04963707416774406, 'base_channels': 11, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.04330192931126272, 'grad_clip_norm': 3.6626502940899597, 'scheduler_step_size': 3, 'scheduler_gamma': 0.2613013381368354, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 6}
2025-10-12 11:06:15,928 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0974 val_loss=0.8755 val_acc=0.6881 time=12.5s
2025-10-12 11:06:27,584 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.8642 val_loss=0.8324 val_acc=0.7055 time=11.7s
2025-10-12 11:06:39,179 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.8001 val_loss=0.8430 val_acc=0.7034 time=11.6s
2025-10-12 11:06:50,725 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7470 val_loss=0.9503 val_acc=0.6593 time=11.5s
2025-10-12 11:07:02,284 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.7399 val_loss=0.7304 val_acc=0.7488 time=11.6s
2025-10-12 11:07:13,944 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7294 val_loss=1.0943 val_acc=0.6198 time=11.7s
2025-10-12 11:07:25,547 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.7140 val_loss=0.8025 val_acc=0.7195 time=11.6s
2025-10-12 11:07:37,127 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.7141 val_loss=0.7735 val_acc=0.7296 time=11.6s
2025-10-12 11:07:48,710 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.7126 val_loss=0.7580 val_acc=0.7356 time=11.6s
2025-10-12 11:08:00,287 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.7067 val_loss=0.8812 val_acc=0.6926 time=11.6s
2025-10-12 11:08:00,291 - INFO - _models.training_function_executor - Model: 9,003 parameters, 9.7KB storage
2025-10-12 11:08:00,292 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.097447446260913, 0.8641707620156116, 0.8000778370296617, 0.7469839989528775, 0.7399385670499239, 0.7293930019365769, 0.7140106277101558, 0.7141359885297207, 0.712619722465827, 0.7067211660803261], 'val_losses': [0.8754680881633348, 0.8324238130256226, 0.8429697569685254, 0.9502565781861324, 0.7303666283680001, 1.0942551956456943, 0.8024528749498363, 0.7735334767597849, 0.7579846339268329, 0.8811678021760265], 'val_acc': [0.688134406720336, 0.7054602730136507, 0.7033601680084004, 0.6592579628981449, 0.7487749387469373, 0.6197934896744838, 0.7195484774238712, 0.7296114805740287, 0.7355617780889044, 0.6925971298564928], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0002361157914100513, 'weight_decay': 1.5778233946903882e-05, 'epochs': 10, 'batch_size': 12, 'dropout': 0.04963707416774406, 'base_channels': 11, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.04330192931126272, 'grad_clip_norm': 3.6626502940899597, 'scheduler_step_size': 3, 'scheduler_gamma': 0.2613013381368354, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 6}, 'model_parameter_count': 9003, 'model_storage_size_kb': 9.671191406250001, 'model_size_validation': 'PASS'}
2025-10-12 11:08:00,292 - INFO - _models.training_function_executor - BO Objective: base=0.6926, size_penalty=0.0000, final=0.6926
2025-10-12 11:08:00,292 - INFO - _models.training_function_executor - Model: 9,003 parameters, 9.7KB (PASS 256KB limit)
2025-10-12 11:08:00,292 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 116.833s
2025-10-12 11:08:00,386 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.6926
2025-10-12 11:08:00,386 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.094s
2025-10-12 11:08:00,387 - INFO - bo.run_bo - Recorded observation #16: hparams={'lr': 0.0002361157914100513, 'weight_decay': 1.5778233946903882e-05, 'epochs': np.int64(10), 'batch_size': np.int64(12), 'dropout': 0.04963707416774406, 'base_channels': np.int64(11), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.04330192931126272, 'grad_clip_norm': 3.6626502940899597, 'scheduler_step_size': np.int64(3), 'scheduler_gamma': 0.2613013381368354, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(6)}, value=0.6926
2025-10-12 11:08:00,387 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 16: {'lr': 0.0002361157914100513, 'weight_decay': 1.5778233946903882e-05, 'epochs': np.int64(10), 'batch_size': np.int64(12), 'dropout': 0.04963707416774406, 'base_channels': np.int64(11), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.04330192931126272, 'grad_clip_norm': 3.6626502940899597, 'scheduler_step_size': np.int64(3), 'scheduler_gamma': 0.2613013381368354, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(6)} -> 0.6926
2025-10-12 11:08:00,387 - INFO - bo.run_bo - üîçBO Trial 17: Using RF surrogate + Expected Improvement
2025-10-12 11:08:00,387 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 11:08:00,387 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 11:08:00,387 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 11:08:00,387 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00028796960615400265, 'weight_decay': 0.0003140588202932991, 'epochs': 22, 'batch_size': 16, 'dropout': 0.11548966509266881, 'base_channels': 10, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.034413850669089995, 'grad_clip_norm': 3.9088797778308733, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5602975571949111, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 6}
2025-10-12 11:08:00,388 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00028796960615400265, 'weight_decay': 0.0003140588202932991, 'epochs': 22, 'batch_size': 16, 'dropout': 0.11548966509266881, 'base_channels': 10, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.034413850669089995, 'grad_clip_norm': 3.9088797778308733, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5602975571949111, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 6}
2025-10-12 11:08:10,760 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0839 val_loss=0.9544 val_acc=0.6186 time=10.4s
2025-10-12 11:08:20,629 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.9032 val_loss=1.1942 val_acc=0.5102 time=9.9s
2025-10-12 11:08:30,496 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.8271 val_loss=1.0478 val_acc=0.5918 time=9.9s
2025-10-12 11:08:40,363 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7880 val_loss=0.9108 val_acc=0.6769 time=9.9s
2025-10-12 11:08:50,207 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.7657 val_loss=0.8658 val_acc=0.6959 time=9.8s
2025-10-12 11:09:00,051 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7437 val_loss=0.8665 val_acc=0.6803 time=9.8s
2025-10-12 11:09:09,955 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.7275 val_loss=0.9378 val_acc=0.6583 time=9.9s
2025-10-12 11:09:19,819 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.7172 val_loss=0.8573 val_acc=0.6992 time=9.9s
2025-10-12 11:09:29,687 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.7013 val_loss=1.0493 val_acc=0.6703 time=9.9s
2025-10-12 11:09:39,543 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6932 val_loss=0.8239 val_acc=0.7270 time=9.9s
2025-10-12 11:09:49,376 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.6722 val_loss=0.9076 val_acc=0.6794 time=9.8s
2025-10-12 11:09:59,245 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.6667 val_loss=0.8734 val_acc=0.6894 time=9.9s
2025-10-12 11:10:09,132 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.6617 val_loss=1.1065 val_acc=0.5911 time=9.9s
2025-10-12 11:10:19,004 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.6590 val_loss=0.8599 val_acc=0.7048 time=9.9s
2025-10-12 11:10:28,905 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.6522 val_loss=0.9055 val_acc=0.7067 time=9.9s
2025-10-12 11:10:38,769 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.6474 val_loss=0.8354 val_acc=0.7291 time=9.9s
2025-10-12 11:10:48,654 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.6452 val_loss=1.0536 val_acc=0.6276 time=9.9s
2025-10-12 11:10:58,551 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.6435 val_loss=0.8166 val_acc=0.7301 time=9.9s
2025-10-12 11:11:08,443 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.6421 val_loss=0.7995 val_acc=0.7321 time=9.9s
2025-10-12 11:11:18,287 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.6383 val_loss=0.8999 val_acc=0.7038 time=9.8s
2025-10-12 11:11:28,172 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.6261 val_loss=0.8705 val_acc=0.7128 time=9.9s
2025-10-12 11:11:38,055 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.6247 val_loss=0.8283 val_acc=0.7224 time=9.9s
2025-10-12 11:11:38,060 - INFO - _models.training_function_executor - Model: 7,523 parameters, 8.1KB storage
2025-10-12 11:11:38,060 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0839361685074247, 0.9032103558040642, 0.827121702062946, 0.7880275636796648, 0.7657249785508565, 0.7437247883868793, 0.7274957723579166, 0.7171980155632551, 0.7012574279333355, 0.6932251601420293, 0.6721567110160785, 0.6667051649944825, 0.6617082313699325, 0.6589938244141187, 0.6522392654360529, 0.6474447536654183, 0.6452026658326043, 0.6434976803305101, 0.6421258088943214, 0.638277328124946, 0.6261112687103307, 0.6246502057609752], 'val_losses': [0.9543944458298889, 1.194210461836659, 1.0478387211726852, 0.9107695734392399, 0.8657870275764813, 0.8665243300433588, 0.9377665822953246, 0.857251678213691, 1.0492945089179984, 0.8238789805028253, 0.907577028506529, 0.8733995168890033, 1.1065236059234909, 0.859852763366816, 0.9055155840189589, 0.8354034001567708, 1.0536125561533418, 0.8166024138304083, 0.79946041742446, 0.8999024488978508, 0.8705318389153373, 0.8282739178133796], 'val_acc': [0.6185684284214211, 0.510238011900595, 0.5917920896044803, 0.6769338466923346, 0.6959222961148057, 0.6802590129506475, 0.6582954147707385, 0.6992474623731186, 0.6702835141757088, 0.7269863493174659, 0.6793839691984599, 0.6894469723486174, 0.5910920546027302, 0.7048477423871193, 0.7066853342667133, 0.7290864543227161, 0.6275813790689534, 0.7301365068253413, 0.732061603080154, 0.7037976898844942, 0.7128106405320266, 0.7224361218060903], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00028796960615400265, 'weight_decay': 0.0003140588202932991, 'epochs': 22, 'batch_size': 16, 'dropout': 0.11548966509266881, 'base_channels': 10, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.034413850669089995, 'grad_clip_norm': 3.9088797778308733, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5602975571949111, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 6}, 'model_parameter_count': 7523, 'model_storage_size_kb': 8.081347656250001, 'model_size_validation': 'PASS'}
2025-10-12 11:11:38,060 - INFO - _models.training_function_executor - BO Objective: base=0.7224, size_penalty=0.0000, final=0.7224
2025-10-12 11:11:38,060 - INFO - _models.training_function_executor - Model: 7,523 parameters, 8.1KB (PASS 256KB limit)
2025-10-12 11:11:38,060 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 217.673s
2025-10-12 11:11:38,157 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7224
2025-10-12 11:11:38,157 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.096s
2025-10-12 11:11:38,157 - INFO - bo.run_bo - Recorded observation #17: hparams={'lr': 0.00028796960615400265, 'weight_decay': 0.0003140588202932991, 'epochs': np.int64(22), 'batch_size': np.int64(16), 'dropout': 0.11548966509266881, 'base_channels': np.int64(10), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(3), 'label_smoothing': 0.034413850669089995, 'grad_clip_norm': 3.9088797778308733, 'scheduler_step_size': np.int64(10), 'scheduler_gamma': 0.5602975571949111, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(6)}, value=0.7224
2025-10-12 11:11:38,157 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 17: {'lr': 0.00028796960615400265, 'weight_decay': 0.0003140588202932991, 'epochs': np.int64(22), 'batch_size': np.int64(16), 'dropout': 0.11548966509266881, 'base_channels': np.int64(10), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(3), 'label_smoothing': 0.034413850669089995, 'grad_clip_norm': 3.9088797778308733, 'scheduler_step_size': np.int64(10), 'scheduler_gamma': 0.5602975571949111, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(6)} -> 0.7224
2025-10-12 11:11:38,157 - INFO - bo.run_bo - üîçBO Trial 18: Using RF surrogate + Expected Improvement
2025-10-12 11:11:38,157 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 11:11:38,157 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 11:11:38,157 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 11:11:38,157 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004947581664189955, 'weight_decay': 5.7989994062940114e-05, 'epochs': 42, 'batch_size': 16, 'dropout': 0.09707147905151617, 'base_channels': 8, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.03904057929546953, 'grad_clip_norm': 4.780675376438337, 'scheduler_step_size': 7, 'scheduler_gamma': 0.3498876486390067, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 7}
2025-10-12 11:11:38,159 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004947581664189955, 'weight_decay': 5.7989994062940114e-05, 'epochs': 42, 'batch_size': 16, 'dropout': 0.09707147905151617, 'base_channels': 8, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.03904057929546953, 'grad_clip_norm': 4.780675376438337, 'scheduler_step_size': 7, 'scheduler_gamma': 0.3498876486390067, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 7}
2025-10-12 11:11:48,037 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0029 val_loss=0.7936 val_acc=0.7266 time=9.9s
2025-10-12 11:11:57,409 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.8239 val_loss=0.8919 val_acc=0.6806 time=9.4s
2025-10-12 11:12:06,783 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7705 val_loss=0.7562 val_acc=0.7417 time=9.4s
2025-10-12 11:12:16,174 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7510 val_loss=0.7416 val_acc=0.7410 time=9.4s
2025-10-12 11:12:25,522 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.7281 val_loss=0.9474 val_acc=0.6664 time=9.3s
2025-10-12 11:12:34,893 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7176 val_loss=0.7051 val_acc=0.7700 time=9.4s
2025-10-12 11:12:44,230 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.7097 val_loss=0.6695 val_acc=0.7762 time=9.3s
2025-10-12 11:12:53,565 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.6632 val_loss=0.7362 val_acc=0.7594 time=9.3s
2025-10-12 11:13:02,937 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.6500 val_loss=0.6550 val_acc=0.7865 time=9.4s
2025-10-12 11:13:12,357 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6424 val_loss=0.6658 val_acc=0.7840 time=9.4s
2025-10-12 11:13:21,717 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.6388 val_loss=0.7096 val_acc=0.7619 time=9.4s
2025-10-12 11:13:31,064 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.6333 val_loss=0.7091 val_acc=0.7699 time=9.3s
2025-10-12 11:13:40,438 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.6302 val_loss=0.6769 val_acc=0.7757 time=9.4s
2025-10-12 11:13:49,770 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.6282 val_loss=0.7333 val_acc=0.7546 time=9.3s
2025-10-12 11:13:59,148 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.6077 val_loss=0.7381 val_acc=0.7561 time=9.4s
2025-10-12 11:14:08,484 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.6067 val_loss=0.6461 val_acc=0.7889 time=9.3s
2025-10-12 11:14:17,826 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.6028 val_loss=0.7089 val_acc=0.7641 time=9.3s
2025-10-12 11:14:27,200 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.6010 val_loss=0.6725 val_acc=0.7785 time=9.4s
2025-10-12 11:14:36,542 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.5980 val_loss=0.6905 val_acc=0.7700 time=9.3s
2025-10-12 11:14:45,887 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.5984 val_loss=0.6337 val_acc=0.7969 time=9.3s
2025-10-12 11:14:55,284 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.5980 val_loss=0.6888 val_acc=0.7737 time=9.4s
2025-10-12 11:15:04,680 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.5889 val_loss=0.6639 val_acc=0.7831 time=9.4s
2025-10-12 11:15:14,028 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.5895 val_loss=0.6790 val_acc=0.7793 time=9.3s
2025-10-12 11:15:23,432 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.5908 val_loss=0.6570 val_acc=0.7868 time=9.4s
2025-10-12 11:15:32,795 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.5875 val_loss=0.7027 val_acc=0.7626 time=9.4s
2025-10-12 11:15:42,171 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.5876 val_loss=0.6575 val_acc=0.7872 time=9.4s
2025-10-12 11:15:51,553 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.5871 val_loss=0.6671 val_acc=0.7836 time=9.4s
2025-10-12 11:16:00,890 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.5838 val_loss=0.6865 val_acc=0.7796 time=9.3s
2025-10-12 11:16:10,280 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.5827 val_loss=0.6680 val_acc=0.7840 time=9.4s
2025-10-12 11:16:19,586 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.5825 val_loss=0.6627 val_acc=0.7843 time=9.3s
2025-10-12 11:16:28,990 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.5804 val_loss=0.6403 val_acc=0.7933 time=9.4s
2025-10-12 11:16:38,384 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.5815 val_loss=0.6518 val_acc=0.7908 time=9.4s
2025-10-12 11:16:47,754 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.5804 val_loss=0.6402 val_acc=0.7929 time=9.4s
2025-10-12 11:16:57,129 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.5814 val_loss=0.6599 val_acc=0.7865 time=9.4s
2025-10-12 11:17:06,476 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.5856 val_loss=0.6558 val_acc=0.7862 time=9.3s
2025-10-12 11:17:15,853 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.5795 val_loss=0.6632 val_acc=0.7841 time=9.4s
2025-10-12 11:17:25,267 - INFO - _models.training_function_executor - Epoch 037: train_loss=0.5819 val_loss=0.6661 val_acc=0.7861 time=9.4s
2025-10-12 11:17:34,669 - INFO - _models.training_function_executor - Epoch 038: train_loss=0.5807 val_loss=0.6703 val_acc=0.7847 time=9.4s
2025-10-12 11:17:44,010 - INFO - _models.training_function_executor - Epoch 039: train_loss=0.5797 val_loss=0.6617 val_acc=0.7854 time=9.3s
2025-10-12 11:17:53,351 - INFO - _models.training_function_executor - Epoch 040: train_loss=0.5781 val_loss=0.6792 val_acc=0.7778 time=9.3s
2025-10-12 11:18:02,698 - INFO - _models.training_function_executor - Epoch 041: train_loss=0.5810 val_loss=0.6741 val_acc=0.7816 time=9.3s
2025-10-12 11:18:12,075 - INFO - _models.training_function_executor - Epoch 042: train_loss=0.5814 val_loss=0.6619 val_acc=0.7841 time=9.4s
2025-10-12 11:18:12,080 - INFO - _models.training_function_executor - Model: 4,965 parameters, 5.3KB storage
2025-10-12 11:18:12,080 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0028578762746083, 0.8238552041119698, 0.7704972077446631, 0.7510245379802436, 0.728117566218024, 0.7176049954310388, 0.7096635121793483, 0.663157744365094, 0.6500397502833578, 0.6423603069815638, 0.6388301939179024, 0.6332742398566286, 0.630153918458233, 0.6281589796629076, 0.6076709349327751, 0.6066626468591854, 0.602828236452591, 0.6010081602699239, 0.5980190914078614, 0.5983538169532462, 0.5979939738446948, 0.5889164615144753, 0.5894961837583962, 0.5907955775216697, 0.5875215299080667, 0.5875630313162208, 0.5870805945133971, 0.5838092299537472, 0.5827319092075851, 0.5825125191088193, 0.5803954374249849, 0.5815386382128168, 0.5804114995050313, 0.5814017107228445, 0.5855509188882864, 0.5794818236834527, 0.5819366422407186, 0.5807357992498366, 0.5796743263501908, 0.5780706151167245, 0.5810088690006529, 0.5813650779887685], 'val_losses': [0.7935808793852536, 0.8919221226009907, 0.756234360640392, 0.7416017852465423, 0.9474468243510147, 0.7051433237503574, 0.6694841369866574, 0.7361637349307307, 0.6550031953084122, 0.6657885504761984, 0.7095530561875913, 0.7090602061695931, 0.6769181471168056, 0.7332589570504617, 0.7381274161803626, 0.646149215777103, 0.7089011287893663, 0.6725159808393538, 0.6904508482963421, 0.633653081773585, 0.6887876449200421, 0.6639188627367216, 0.6790171293662401, 0.6569995563223014, 0.7027392160195673, 0.6575211744063126, 0.6671263831121587, 0.6865044544637725, 0.6679976010681409, 0.6627110871331525, 0.6402526169045221, 0.6518216851732302, 0.6402044686500868, 0.6599469100694619, 0.6558162983184851, 0.6631518616975084, 0.6661318784770659, 0.6703060079922741, 0.6616894266052099, 0.6791519357747534, 0.674118921927401, 0.6619230257200536], 'val_acc': [0.7266363318165908, 0.6806090304515225, 0.7416870843542177, 0.7409870493524676, 0.6664333216660833, 0.7699509975498775, 0.7761638081904095, 0.7593629681484074, 0.7864893244662233, 0.784039201960098, 0.7619005950297515, 0.7698634931746587, 0.7757262863143157, 0.7546377318865943, 0.7561253062653133, 0.7888519425971299, 0.7640882044102205, 0.7785264263213161, 0.7700385019250963, 0.7969023451172559, 0.7737136856842842, 0.7830766538326916, 0.7793139656982849, 0.7867518375918796, 0.7626006300315016, 0.7871893594679734, 0.7836016800840042, 0.7795764788239412, 0.7839516975848793, 0.7843017150857543, 0.7933146657332867, 0.7907770388519426, 0.7928771438571929, 0.7864893244662233, 0.786226811340567, 0.7841267063353168, 0.7860518025901295, 0.7846517325866293, 0.7853517675883794, 0.777826391319566, 0.7815890794539727, 0.7841267063353168], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004947581664189955, 'weight_decay': 5.7989994062940114e-05, 'epochs': 42, 'batch_size': 16, 'dropout': 0.09707147905151617, 'base_channels': 8, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.03904057929546953, 'grad_clip_norm': 4.780675376438337, 'scheduler_step_size': 7, 'scheduler_gamma': 0.3498876486390067, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 7}, 'model_parameter_count': 4965, 'model_storage_size_kb': 5.33349609375, 'model_size_validation': 'PASS'}
2025-10-12 11:18:12,080 - INFO - _models.training_function_executor - BO Objective: base=0.7841, size_penalty=0.0000, final=0.7841
2025-10-12 11:18:12,080 - INFO - _models.training_function_executor - Model: 4,965 parameters, 5.3KB (PASS 256KB limit)
2025-10-12 11:18:12,080 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 393.923s
2025-10-12 11:18:12,178 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7841
2025-10-12 11:18:12,178 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.097s
2025-10-12 11:18:12,178 - INFO - bo.run_bo - Recorded observation #18: hparams={'lr': 0.004947581664189955, 'weight_decay': 5.7989994062940114e-05, 'epochs': np.int64(42), 'batch_size': np.int64(16), 'dropout': 0.09707147905151617, 'base_channels': np.int64(8), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.03904057929546953, 'grad_clip_norm': 4.780675376438337, 'scheduler_step_size': np.int64(7), 'scheduler_gamma': 0.3498876486390067, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(7)}, value=0.7841
2025-10-12 11:18:12,178 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 18: {'lr': 0.004947581664189955, 'weight_decay': 5.7989994062940114e-05, 'epochs': np.int64(42), 'batch_size': np.int64(16), 'dropout': 0.09707147905151617, 'base_channels': np.int64(8), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.03904057929546953, 'grad_clip_norm': 4.780675376438337, 'scheduler_step_size': np.int64(7), 'scheduler_gamma': 0.3498876486390067, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(7)} -> 0.7841
2025-10-12 11:18:12,178 - INFO - bo.run_bo - üîçBO Trial 19: Using RF surrogate + Expected Improvement
2025-10-12 11:18:12,178 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 11:18:12,178 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 11:18:12,178 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 11:18:12,178 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 4.225603982221672e-05, 'weight_decay': 9.48120362207438e-05, 'epochs': 37, 'batch_size': 12, 'dropout': 0.03418400247371201, 'base_channels': 23, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.03559598066107902, 'grad_clip_norm': 3.214459590056314, 'scheduler_step_size': 6, 'scheduler_gamma': 0.7592504451032909, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_batches': 5}
2025-10-12 11:18:12,179 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 4.225603982221672e-05, 'weight_decay': 9.48120362207438e-05, 'epochs': 37, 'batch_size': 12, 'dropout': 0.03418400247371201, 'base_channels': 23, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.03559598066107902, 'grad_clip_norm': 3.214459590056314, 'scheduler_step_size': 6, 'scheduler_gamma': 0.7592504451032909, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_batches': 5}
2025-10-12 11:18:23,644 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.1506 val_loss=1.0052 val_acc=0.6089 time=11.5s
2025-10-12 11:18:35,048 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.9397 val_loss=0.8622 val_acc=0.6768 time=11.4s
2025-10-12 11:18:46,340 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.8546 val_loss=0.7838 val_acc=0.7322 time=11.3s
2025-10-12 11:18:57,742 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.8024 val_loss=0.7742 val_acc=0.7239 time=11.4s
2025-10-12 11:19:09,088 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.7679 val_loss=0.7084 val_acc=0.7557 time=11.3s
2025-10-12 11:19:20,433 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7464 val_loss=0.7189 val_acc=0.7501 time=11.3s
2025-10-12 11:19:31,814 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.7261 val_loss=0.7314 val_acc=0.7419 time=11.4s
2025-10-12 11:19:43,225 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.7125 val_loss=0.9136 val_acc=0.6644 time=11.4s
2025-10-12 11:19:54,654 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.7071 val_loss=0.7461 val_acc=0.7367 time=11.4s
2025-10-12 11:20:06,083 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6973 val_loss=0.7531 val_acc=0.7410 time=11.4s
2025-10-12 11:20:17,500 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.6925 val_loss=0.6765 val_acc=0.7708 time=11.4s
2025-10-12 11:20:28,877 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.6830 val_loss=0.8862 val_acc=0.6830 time=11.4s
2025-10-12 11:20:40,331 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.6745 val_loss=0.7814 val_acc=0.7258 time=11.5s
2025-10-12 11:20:51,757 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.6715 val_loss=0.7055 val_acc=0.7568 time=11.4s
2025-10-12 11:21:03,154 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.6653 val_loss=0.7155 val_acc=0.7485 time=11.4s
2025-10-12 11:21:14,533 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.6632 val_loss=0.7694 val_acc=0.7284 time=11.4s
2025-10-12 11:21:25,906 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.6592 val_loss=0.7039 val_acc=0.7580 time=11.4s
2025-10-12 11:21:37,322 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.6542 val_loss=0.9996 val_acc=0.6590 time=11.4s
2025-10-12 11:21:48,722 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.6491 val_loss=0.6973 val_acc=0.7585 time=11.4s
2025-10-12 11:22:00,058 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.6457 val_loss=1.0727 val_acc=0.6178 time=11.3s
2025-10-12 11:22:11,409 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.6450 val_loss=0.8566 val_acc=0.6935 time=11.4s
2025-10-12 11:22:22,846 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.6413 val_loss=0.7364 val_acc=0.7455 time=11.4s
2025-10-12 11:22:34,220 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.6389 val_loss=0.6733 val_acc=0.7723 time=11.4s
2025-10-12 11:22:45,579 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.6347 val_loss=0.8111 val_acc=0.7381 time=11.4s
2025-10-12 11:22:56,982 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.6327 val_loss=0.9853 val_acc=0.6618 time=11.4s
2025-10-12 11:23:08,409 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.6295 val_loss=0.7300 val_acc=0.7524 time=11.4s
2025-10-12 11:23:19,861 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.6269 val_loss=0.7912 val_acc=0.7308 time=11.5s
2025-10-12 11:23:31,248 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.6251 val_loss=0.7304 val_acc=0.7534 time=11.4s
2025-10-12 11:23:42,652 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.6247 val_loss=0.6655 val_acc=0.7707 time=11.4s
2025-10-12 11:23:54,043 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.6236 val_loss=0.7382 val_acc=0.7482 time=11.4s
2025-10-12 11:24:05,487 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.6195 val_loss=0.7008 val_acc=0.7655 time=11.4s
2025-10-12 11:24:16,908 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.6152 val_loss=0.8188 val_acc=0.7104 time=11.4s
2025-10-12 11:24:28,327 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.6183 val_loss=0.8336 val_acc=0.7055 time=11.4s
2025-10-12 11:24:39,716 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.6149 val_loss=0.7449 val_acc=0.7503 time=11.4s
2025-10-12 11:24:51,129 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.6142 val_loss=0.8385 val_acc=0.7062 time=11.4s
2025-10-12 11:25:02,538 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.6130 val_loss=0.7401 val_acc=0.7585 time=11.4s
2025-10-12 11:25:13,917 - INFO - _models.training_function_executor - Epoch 037: train_loss=0.6081 val_loss=0.7136 val_acc=0.7664 time=11.4s
2025-10-12 11:25:13,920 - INFO - _models.training_function_executor - Model: 157,190 parameters, 675.4KB storage
2025-10-12 11:25:13,920 - WARNING - _models.training_function_executor - Model storage 675.4KB exceeds 256KB limit!
2025-10-12 11:25:13,920 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.1506379345965878, 0.9396567594678534, 0.8545537089807068, 0.802422281898399, 0.7678955224742621, 0.7464250455296536, 0.7260707777670246, 0.71249505658176, 0.7071222250575071, 0.6973237285209903, 0.6924580172666583, 0.6830277395052304, 0.6745011416590836, 0.6715070938037583, 0.6653435630661898, 0.6631992941589926, 0.6592325418900684, 0.6542368188701208, 0.6491057810523229, 0.6457087633193144, 0.6449889551142755, 0.6413036591699689, 0.6389275037239889, 0.6346654243803771, 0.6326868295340968, 0.6295427326380288, 0.626863689617882, 0.6250521066276853, 0.6247326678873193, 0.6235833424771003, 0.619482448545575, 0.6151945388305112, 0.6182931963188751, 0.614922207080957, 0.6141863294561238, 0.6129686862457182, 0.608143739952547], 'val_losses': [1.0051825808809067, 0.8621738652562111, 0.7837927300596429, 0.774202018921447, 0.7083719007965564, 0.7189436738265533, 0.731365813532408, 0.9135739714700023, 0.7461307993354938, 0.7531152814574537, 0.6764556837767248, 0.8862355069593404, 0.7813566681603932, 0.7054558747066958, 0.7154526325567936, 0.7693573080073673, 0.7039390560741562, 0.9996262654782092, 0.6973319744073848, 1.0727372122032057, 0.8566088723028544, 0.7363517184278578, 0.6732615644766143, 0.8110752683321955, 0.9853442940583252, 0.7300112103905801, 0.7911993349150298, 0.7304389810057257, 0.6655327528767291, 0.7381936513359496, 0.7007991851007225, 0.8188173852451498, 0.8335536244812026, 0.7449035436452207, 0.8384570618833868, 0.7400996266914115, 0.7135832991645309], 'val_acc': [0.6088554427721387, 0.6768463423171158, 0.7322366118305915, 0.7239236961848092, 0.7556877843892195, 0.7500875043752188, 0.741949597479874, 0.6644207210360518, 0.7366993349667483, 0.7409870493524676, 0.7708260413020651, 0.6829716485824291, 0.7257612880644032, 0.7568253412670634, 0.7485124256212811, 0.728386419320966, 0.7579628981449072, 0.6589954497724886, 0.7584879243962198, 0.6177808890444523, 0.6934721736086804, 0.7455372768638432, 0.772313615680784, 0.7380994049702485, 0.6617955897794889, 0.7523626181309065, 0.7308365418270913, 0.7534126706335317, 0.7706510325516276, 0.748162408120406, 0.7654882744137207, 0.7103605180259013, 0.7055477773888694, 0.7502625131256563, 0.7061603080154008, 0.7584879243962198, 0.7663633181659083], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 4.225603982221672e-05, 'weight_decay': 9.48120362207438e-05, 'epochs': 37, 'batch_size': 12, 'dropout': 0.03418400247371201, 'base_channels': 23, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.03559598066107902, 'grad_clip_norm': 3.214459590056314, 'scheduler_step_size': 6, 'scheduler_gamma': 0.7592504451032909, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_batches': 5}, 'model_parameter_count': 157190, 'model_storage_size_kb': 675.42578125, 'model_size_validation': 'FAIL'}
2025-10-12 11:25:13,920 - INFO - _models.training_function_executor - BO Objective: base=0.7664, size_penalty=0.8000, final=-0.0336
2025-10-12 11:25:13,920 - INFO - _models.training_function_executor - Model: 157,190 parameters, 675.4KB (FAIL 256KB limit)
2025-10-12 11:25:13,920 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 421.742s
2025-10-12 11:25:14,020 - INFO - bo.run_bo - Updated RF surrogate model with observation: -0.0336
2025-10-12 11:25:14,020 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-10-12 11:25:14,020 - INFO - bo.run_bo - Recorded observation #19: hparams={'lr': 4.225603982221672e-05, 'weight_decay': 9.48120362207438e-05, 'epochs': np.int64(37), 'batch_size': np.int64(12), 'dropout': 0.03418400247371201, 'base_channels': np.int64(23), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(5), 'label_smoothing': 0.03559598066107902, 'grad_clip_norm': 3.214459590056314, 'scheduler_step_size': np.int64(6), 'scheduler_gamma': 0.7592504451032909, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(5)}, value=-0.0336
2025-10-12 11:25:14,020 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 19: {'lr': 4.225603982221672e-05, 'weight_decay': 9.48120362207438e-05, 'epochs': np.int64(37), 'batch_size': np.int64(12), 'dropout': 0.03418400247371201, 'base_channels': np.int64(23), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(5), 'label_smoothing': 0.03559598066107902, 'grad_clip_norm': 3.214459590056314, 'scheduler_step_size': np.int64(6), 'scheduler_gamma': 0.7592504451032909, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(5)} -> -0.0336
2025-10-12 11:25:14,020 - INFO - bo.run_bo - üîçBO Trial 20: Using RF surrogate + Expected Improvement
2025-10-12 11:25:14,020 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 11:25:14,020 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 11:25:14,020 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 11:25:14,021 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015019151986460566, 'weight_decay': 0.0004469400664677491, 'epochs': 36, 'batch_size': 16, 'dropout': 0.07586048632958338, 'base_channels': 15, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.035783499987152065, 'grad_clip_norm': 3.3551781036932766, 'scheduler_step_size': 9, 'scheduler_gamma': 0.5011359930052235, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 8}
2025-10-12 11:25:14,022 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015019151986460566, 'weight_decay': 0.0004469400664677491, 'epochs': 36, 'batch_size': 16, 'dropout': 0.07586048632958338, 'base_channels': 15, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.035783499987152065, 'grad_clip_norm': 3.3551781036932766, 'scheduler_step_size': 9, 'scheduler_gamma': 0.5011359930052235, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 8}
2025-10-12 11:25:22,593 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9404 val_loss=0.8268 val_acc=0.6945 time=8.6s
2025-10-12 11:25:31,058 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7542 val_loss=0.7536 val_acc=0.7431 time=8.5s
2025-10-12 11:25:39,566 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7017 val_loss=0.7804 val_acc=0.7308 time=8.5s
2025-10-12 11:25:48,022 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6720 val_loss=0.6360 val_acc=0.7913 time=8.5s
2025-10-12 11:25:56,506 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.6525 val_loss=0.7891 val_acc=0.7222 time=8.5s
2025-10-12 11:26:04,997 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.6334 val_loss=0.6212 val_acc=0.8006 time=8.5s
2025-10-12 11:26:13,494 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.6222 val_loss=0.6423 val_acc=0.7824 time=8.5s
2025-10-12 11:26:21,988 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.6098 val_loss=0.6482 val_acc=0.7798 time=8.5s
2025-10-12 11:26:30,481 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.6023 val_loss=0.6466 val_acc=0.7897 time=8.5s
2025-10-12 11:26:38,895 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.5697 val_loss=0.6090 val_acc=0.8029 time=8.4s
2025-10-12 11:26:47,430 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.5610 val_loss=0.6315 val_acc=0.7946 time=8.5s
2025-10-12 11:26:55,889 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.5561 val_loss=0.6115 val_acc=0.8030 time=8.5s
2025-10-12 11:27:04,358 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.5515 val_loss=0.6522 val_acc=0.7882 time=8.5s
2025-10-12 11:27:12,812 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.5485 val_loss=0.5929 val_acc=0.8071 time=8.5s
2025-10-12 11:27:21,257 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.5423 val_loss=0.6050 val_acc=0.8061 time=8.4s
2025-10-12 11:27:29,703 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.5411 val_loss=0.5959 val_acc=0.8090 time=8.4s
2025-10-12 11:27:38,171 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.5353 val_loss=0.6102 val_acc=0.8048 time=8.5s
2025-10-12 11:27:46,575 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.5310 val_loss=0.6035 val_acc=0.8047 time=8.4s
2025-10-12 11:27:55,006 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.5130 val_loss=0.5931 val_acc=0.8116 time=8.4s
2025-10-12 11:28:03,489 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.5088 val_loss=0.8696 val_acc=0.7090 time=8.5s
2025-10-12 11:28:11,955 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.5076 val_loss=0.5826 val_acc=0.8183 time=8.5s
2025-10-12 11:28:20,461 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.5055 val_loss=0.6215 val_acc=0.7950 time=8.5s
2025-10-12 11:28:28,923 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.4994 val_loss=0.5884 val_acc=0.8126 time=8.5s
2025-10-12 11:28:37,398 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.4965 val_loss=0.5805 val_acc=0.8171 time=8.5s
2025-10-12 11:28:45,849 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.4976 val_loss=0.5956 val_acc=0.8116 time=8.4s
2025-10-12 11:28:54,338 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.4933 val_loss=0.6044 val_acc=0.8113 time=8.5s
2025-10-12 11:29:02,806 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.4924 val_loss=0.5958 val_acc=0.8093 time=8.5s
2025-10-12 11:29:11,301 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.4818 val_loss=0.6748 val_acc=0.7832 time=8.5s
2025-10-12 11:29:19,807 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.4770 val_loss=0.5923 val_acc=0.8167 time=8.5s
2025-10-12 11:29:28,244 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.4763 val_loss=0.6006 val_acc=0.8099 time=8.4s
2025-10-12 11:29:36,724 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.4765 val_loss=0.6332 val_acc=0.7947 time=8.5s
2025-10-12 11:29:45,191 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.4728 val_loss=0.6057 val_acc=0.8092 time=8.5s
2025-10-12 11:29:53,670 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.4722 val_loss=0.6044 val_acc=0.8073 time=8.5s
2025-10-12 11:30:02,128 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.4689 val_loss=0.5988 val_acc=0.8165 time=8.5s
2025-10-12 11:30:10,620 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.4698 val_loss=0.5997 val_acc=0.8121 time=8.5s
2025-10-12 11:30:19,058 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.4698 val_loss=0.6043 val_acc=0.8108 time=8.4s
2025-10-12 11:30:19,063 - INFO - _models.training_function_executor - Model: 16,233 parameters, 17.4KB storage
2025-10-12 11:30:19,063 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9404154981209091, 0.7542017252035478, 0.701665477659638, 0.6719931887921062, 0.6525096806104591, 0.6334026052290883, 0.6222485352749586, 0.6098033317557406, 0.6022559519639414, 0.5696689210959903, 0.5610265399521366, 0.5561327652775578, 0.5515438652159578, 0.5485393291259087, 0.5423404887229945, 0.5411048116104396, 0.5352650207627433, 0.5310311786071545, 0.5129747555092744, 0.5087604881490825, 0.5076457028846704, 0.5054984697573328, 0.4993683093644373, 0.4964883590771011, 0.4976480164893407, 0.49329703114731227, 0.49235138130179645, 0.4817590526467151, 0.47703689881813954, 0.47631843674420726, 0.4764606523313512, 0.47278277404374247, 0.47221715634608546, 0.46893257595856874, 0.4698026112359347, 0.46976894904860345], 'val_losses': [0.8267849804660763, 0.7535522053573029, 0.7803602948717024, 0.635981228068278, 0.7890930114757944, 0.6211880278787623, 0.6423046557186448, 0.648238551962989, 0.6466439601149975, 0.6090066969019156, 0.6314995743759371, 0.6114796525350517, 0.6522423823187724, 0.5929173577549494, 0.6049640386681228, 0.5959342147426084, 0.610179846723638, 0.6035069080971319, 0.593096530871413, 0.8695914523769268, 0.5825991901455954, 0.6214754820381131, 0.5883651079833612, 0.5804906033388918, 0.5955861879158654, 0.6044102225910336, 0.5958141247772194, 0.6748431165150758, 0.5923239484528457, 0.6006085446634449, 0.6331864269642634, 0.6056614599425684, 0.6044119204523933, 0.5987764433480077, 0.599747011502556, 0.6042907739127896], 'val_acc': [0.6945222261113055, 0.7430871543577179, 0.7308365418270913, 0.7913020651032552, 0.722173608680434, 0.8005775288764438, 0.7823766188309416, 0.7798389919495975, 0.7897269863493175, 0.8028526426321316, 0.7946272313615681, 0.8030276513825692, 0.7881519075953798, 0.8070528526426322, 0.8060903045152258, 0.8089779488974449, 0.8047777388869444, 0.8046902345117256, 0.8116030801540077, 0.7090479523976199, 0.8182534126706336, 0.7949772488624431, 0.8125656282814141, 0.8171158557927897, 0.8116030801540077, 0.8112530626531327, 0.8093279663983199, 0.7831641582079104, 0.8166783339166959, 0.8098529926496325, 0.7947147357367869, 0.8092404620231012, 0.8073153657682884, 0.8165033251662583, 0.8121281064053203, 0.8108155407770389], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0015019151986460566, 'weight_decay': 0.0004469400664677491, 'epochs': 36, 'batch_size': 16, 'dropout': 0.07586048632958338, 'base_channels': 15, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.035783499987152065, 'grad_clip_norm': 3.3551781036932766, 'scheduler_step_size': 9, 'scheduler_gamma': 0.5011359930052235, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 8}, 'model_parameter_count': 16233, 'model_storage_size_kb': 17.437792968750003, 'model_size_validation': 'PASS'}
2025-10-12 11:30:19,063 - INFO - _models.training_function_executor - BO Objective: base=0.8108, size_penalty=0.0000, final=0.8108
2025-10-12 11:30:19,063 - INFO - _models.training_function_executor - Model: 16,233 parameters, 17.4KB (PASS 256KB limit)
2025-10-12 11:30:19,063 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 305.043s
2025-10-12 11:30:19,164 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8108
2025-10-12 11:30:19,164 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.098s
2025-10-12 11:30:19,164 - INFO - bo.run_bo - Recorded observation #20: hparams={'lr': 0.0015019151986460566, 'weight_decay': 0.0004469400664677491, 'epochs': np.int64(36), 'batch_size': np.int64(16), 'dropout': 0.07586048632958338, 'base_channels': np.int64(15), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(5), 'label_smoothing': 0.035783499987152065, 'grad_clip_norm': 3.3551781036932766, 'scheduler_step_size': np.int64(9), 'scheduler_gamma': 0.5011359930052235, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(8)}, value=0.8108
2025-10-12 11:30:19,164 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 20: {'lr': 0.0015019151986460566, 'weight_decay': 0.0004469400664677491, 'epochs': np.int64(36), 'batch_size': np.int64(16), 'dropout': 0.07586048632958338, 'base_channels': np.int64(15), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(5), 'label_smoothing': 0.035783499987152065, 'grad_clip_norm': 3.3551781036932766, 'scheduler_step_size': np.int64(9), 'scheduler_gamma': 0.5011359930052235, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(8)} -> 0.8108
2025-10-12 11:30:19,164 - INFO - bo.run_bo - üîçBO Trial 21: Using RF surrogate + Expected Improvement
2025-10-12 11:30:19,164 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 11:30:19,164 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 11:30:19,164 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 11:30:19,164 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0011536903187377462, 'weight_decay': 0.0005760778696930033, 'epochs': 15, 'batch_size': 8, 'dropout': 0.07856597224445717, 'base_channels': 14, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.03559445480582137, 'grad_clip_norm': 1.4344441423465266, 'scheduler_step_size': 7, 'scheduler_gamma': 0.6218601088619103, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 7}
2025-10-12 11:30:19,165 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0011536903187377462, 'weight_decay': 0.0005760778696930033, 'epochs': 15, 'batch_size': 8, 'dropout': 0.07856597224445717, 'base_channels': 14, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.03559445480582137, 'grad_clip_norm': 1.4344441423465266, 'scheduler_step_size': 7, 'scheduler_gamma': 0.6218601088619103, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 7}
2025-10-12 11:30:32,540 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9750 val_loss=0.8072 val_acc=0.7051 time=13.4s
2025-10-12 11:30:45,809 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7674 val_loss=0.7940 val_acc=0.7291 time=13.3s
2025-10-12 11:30:59,283 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7124 val_loss=0.6923 val_acc=0.7765 time=13.5s
2025-10-12 11:31:12,657 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6827 val_loss=0.6776 val_acc=0.7761 time=13.4s
2025-10-12 11:31:25,985 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.6638 val_loss=0.6879 val_acc=0.7650 time=13.3s
2025-10-12 11:31:39,398 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.6495 val_loss=0.6583 val_acc=0.7842 time=13.4s
2025-10-12 11:31:52,850 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.6377 val_loss=0.6896 val_acc=0.7747 time=13.5s
2025-10-12 11:32:06,193 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.6065 val_loss=0.7374 val_acc=0.7584 time=13.3s
2025-10-12 11:32:19,656 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.5963 val_loss=0.6043 val_acc=0.8069 time=13.5s
2025-10-12 11:32:33,088 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.5914 val_loss=0.6068 val_acc=0.8078 time=13.4s
2025-10-12 11:32:46,370 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.5837 val_loss=0.6009 val_acc=0.8067 time=13.3s
2025-10-12 11:32:59,775 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.5823 val_loss=0.6713 val_acc=0.7832 time=13.4s
2025-10-12 11:33:13,248 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.5771 val_loss=0.7433 val_acc=0.7604 time=13.5s
2025-10-12 11:33:26,612 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.5705 val_loss=0.6773 val_acc=0.7803 time=13.4s
2025-10-12 11:33:39,952 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.5548 val_loss=0.7636 val_acc=0.7354 time=13.3s
2025-10-12 11:33:39,957 - INFO - _models.training_function_executor - Model: 14,229 parameters, 15.3KB storage
2025-10-12 11:33:39,957 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.975022284085199, 0.7673843240467744, 0.7124249756565701, 0.6826702421117088, 0.6638006927627428, 0.649534495778859, 0.637658546158785, 0.6064898545091328, 0.5963183553784428, 0.5914041006459737, 0.5837438195203207, 0.5822777031840334, 0.5770595588877478, 0.5705149340090278, 0.5547637952961703], 'val_losses': [0.8071827144526, 0.793964380708073, 0.692304012854282, 0.6776470176596553, 0.687850439490077, 0.6583461654709902, 0.6895734458264664, 0.7374481529748478, 0.6042689720796546, 0.6067958399366573, 0.6008812078738655, 0.67133129722679, 0.7432976405197897, 0.6773331476323956, 0.7635662688850564], 'val_acc': [0.7051102555127756, 0.7290864543227161, 0.7765138256912846, 0.7760763038151908, 0.7649632481624081, 0.7842142107105355, 0.7746762338116906, 0.758400420021001, 0.8068778438921946, 0.807840392019601, 0.8067028351417571, 0.7831641582079104, 0.7604130206510326, 0.7802765138256913, 0.7353867693384669], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0011536903187377462, 'weight_decay': 0.0005760778696930033, 'epochs': 15, 'batch_size': 8, 'dropout': 0.07856597224445717, 'base_channels': 14, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.03559445480582137, 'grad_clip_norm': 1.4344441423465266, 'scheduler_step_size': 7, 'scheduler_gamma': 0.6218601088619103, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 7}, 'model_parameter_count': 14229, 'model_storage_size_kb': 15.285058593750001, 'model_size_validation': 'PASS'}
2025-10-12 11:33:39,957 - INFO - _models.training_function_executor - BO Objective: base=0.7354, size_penalty=0.0000, final=0.7354
2025-10-12 11:33:39,957 - INFO - _models.training_function_executor - Model: 14,229 parameters, 15.3KB (PASS 256KB limit)
2025-10-12 11:33:39,957 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 200.793s
2025-10-12 11:33:40,058 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7354
2025-10-12 11:33:40,058 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-10-12 11:33:40,058 - INFO - bo.run_bo - Recorded observation #21: hparams={'lr': 0.0011536903187377462, 'weight_decay': 0.0005760778696930033, 'epochs': np.int64(15), 'batch_size': np.int64(8), 'dropout': 0.07856597224445717, 'base_channels': np.int64(14), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.03559445480582137, 'grad_clip_norm': 1.4344441423465266, 'scheduler_step_size': np.int64(7), 'scheduler_gamma': 0.6218601088619103, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(7)}, value=0.7354
2025-10-12 11:33:40,058 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 21: {'lr': 0.0011536903187377462, 'weight_decay': 0.0005760778696930033, 'epochs': np.int64(15), 'batch_size': np.int64(8), 'dropout': 0.07856597224445717, 'base_channels': np.int64(14), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.03559445480582137, 'grad_clip_norm': 1.4344441423465266, 'scheduler_step_size': np.int64(7), 'scheduler_gamma': 0.6218601088619103, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(7)} -> 0.7354
2025-10-12 11:33:40,058 - INFO - bo.run_bo - üîçBO Trial 22: Using RF surrogate + Expected Improvement
2025-10-12 11:33:40,058 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 11:33:40,058 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 11:33:40,058 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 11:33:40,058 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0014324711360093985, 'weight_decay': 0.00011169571562604382, 'epochs': 49, 'batch_size': 4, 'dropout': 0.10934934288960754, 'base_channels': 14, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.003963856506386299, 'grad_clip_norm': 3.7740801610105663, 'scheduler_step_size': 5, 'scheduler_gamma': 0.3808601325808343, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_batches': 7}
2025-10-12 11:33:40,060 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0014324711360093985, 'weight_decay': 0.00011169571562604382, 'epochs': 49, 'batch_size': 4, 'dropout': 0.10934934288960754, 'base_channels': 14, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.003963856506386299, 'grad_clip_norm': 3.7740801610105663, 'scheduler_step_size': 5, 'scheduler_gamma': 0.3808601325808343, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_batches': 7}
2025-10-12 11:34:11,172 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9691 val_loss=0.7268 val_acc=0.7214 time=31.1s
2025-10-12 11:34:41,782 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7226 val_loss=0.9410 val_acc=0.6515 time=30.6s
2025-10-12 11:35:12,408 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.6527 val_loss=0.6317 val_acc=0.7628 time=30.6s
2025-10-12 11:35:42,987 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6198 val_loss=0.6746 val_acc=0.7411 time=30.6s
2025-10-12 11:36:13,454 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.5955 val_loss=0.7307 val_acc=0.7272 time=30.5s
2025-10-12 11:36:43,986 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.5373 val_loss=0.6000 val_acc=0.7767 time=30.5s
2025-10-12 11:37:14,690 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.5255 val_loss=0.5690 val_acc=0.7877 time=30.7s
2025-10-12 11:37:45,126 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.5146 val_loss=0.7361 val_acc=0.7544 time=30.4s
2025-10-12 11:38:15,703 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.5096 val_loss=0.6402 val_acc=0.7634 time=30.6s
2025-10-12 11:38:46,059 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.5020 val_loss=0.6403 val_acc=0.7617 time=30.4s
2025-10-12 11:39:16,525 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.4760 val_loss=0.6021 val_acc=0.7781 time=30.5s
2025-10-12 11:39:47,038 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.4717 val_loss=0.6425 val_acc=0.7712 time=30.5s
2025-10-12 11:40:17,546 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.4703 val_loss=0.6025 val_acc=0.7860 time=30.5s
2025-10-12 11:40:48,075 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.4651 val_loss=0.5838 val_acc=0.7910 time=30.5s
2025-10-12 11:41:18,663 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.4646 val_loss=0.5484 val_acc=0.8008 time=30.6s
2025-10-12 11:41:49,347 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.4533 val_loss=0.5954 val_acc=0.7895 time=30.7s
2025-10-12 11:42:19,795 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.4507 val_loss=0.5849 val_acc=0.7879 time=30.4s
2025-10-12 11:42:50,434 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.4511 val_loss=0.6654 val_acc=0.7595 time=30.6s
2025-10-12 11:43:20,978 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.4473 val_loss=0.5829 val_acc=0.7901 time=30.5s
2025-10-12 11:43:51,441 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.4448 val_loss=0.6570 val_acc=0.7741 time=30.5s
2025-10-12 11:44:21,822 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.4434 val_loss=0.6397 val_acc=0.7840 time=30.4s
2025-10-12 11:44:52,430 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.4405 val_loss=0.5988 val_acc=0.7876 time=30.6s
2025-10-12 11:45:22,843 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.4417 val_loss=0.6214 val_acc=0.7862 time=30.4s
2025-10-12 11:45:53,360 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.4405 val_loss=0.6228 val_acc=0.7844 time=30.5s
2025-10-12 11:46:23,788 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.4392 val_loss=0.7331 val_acc=0.7584 time=30.4s
2025-10-12 11:46:54,175 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.4355 val_loss=0.6181 val_acc=0.7882 time=30.4s
2025-10-12 11:47:24,655 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.4391 val_loss=0.6144 val_acc=0.7888 time=30.5s
2025-10-12 11:47:55,417 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.4393 val_loss=0.6344 val_acc=0.7868 time=30.8s
2025-10-12 11:48:25,974 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.4369 val_loss=0.6251 val_acc=0.7877 time=30.6s
2025-10-12 11:48:56,420 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.4382 val_loss=0.6254 val_acc=0.7886 time=30.4s
2025-10-12 11:49:26,785 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.4377 val_loss=0.6225 val_acc=0.7899 time=30.4s
2025-10-12 11:49:57,518 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.4353 val_loss=0.6333 val_acc=0.7707 time=30.7s
2025-10-12 11:50:28,007 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.4352 val_loss=0.6632 val_acc=0.7740 time=30.5s
2025-10-12 11:50:58,449 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.4353 val_loss=0.6728 val_acc=0.7781 time=30.4s
2025-10-12 11:51:28,886 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.4352 val_loss=0.5982 val_acc=0.7950 time=30.4s
2025-10-12 11:51:59,673 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.4354 val_loss=0.6308 val_acc=0.7875 time=30.8s
2025-10-12 11:52:30,318 - INFO - _models.training_function_executor - Epoch 037: train_loss=0.4355 val_loss=0.6241 val_acc=0.7902 time=30.6s
2025-10-12 11:53:00,777 - INFO - _models.training_function_executor - Epoch 038: train_loss=0.4369 val_loss=0.6353 val_acc=0.7838 time=30.5s
2025-10-12 11:53:31,312 - INFO - _models.training_function_executor - Epoch 039: train_loss=0.4339 val_loss=0.7883 val_acc=0.7292 time=30.5s
2025-10-12 11:54:01,829 - INFO - _models.training_function_executor - Epoch 040: train_loss=0.4379 val_loss=0.5988 val_acc=0.7938 time=30.5s
2025-10-12 11:54:32,402 - INFO - _models.training_function_executor - Epoch 041: train_loss=0.4357 val_loss=0.6016 val_acc=0.7906 time=30.6s
2025-10-12 11:55:02,877 - INFO - _models.training_function_executor - Epoch 042: train_loss=0.4350 val_loss=0.6081 val_acc=0.7860 time=30.5s
2025-10-12 11:55:33,233 - INFO - _models.training_function_executor - Epoch 043: train_loss=0.4346 val_loss=0.6098 val_acc=0.7918 time=30.4s
2025-10-12 11:56:03,677 - INFO - _models.training_function_executor - Epoch 044: train_loss=0.4373 val_loss=0.6310 val_acc=0.7858 time=30.4s
2025-10-12 11:56:34,240 - INFO - _models.training_function_executor - Epoch 045: train_loss=0.4366 val_loss=0.6108 val_acc=0.7867 time=30.6s
2025-10-12 11:57:04,950 - INFO - _models.training_function_executor - Epoch 046: train_loss=0.4339 val_loss=0.6181 val_acc=0.7823 time=30.7s
2025-10-12 11:57:35,374 - INFO - _models.training_function_executor - Epoch 047: train_loss=0.4357 val_loss=0.6155 val_acc=0.7868 time=30.4s
2025-10-12 11:58:05,960 - INFO - _models.training_function_executor - Epoch 048: train_loss=0.4372 val_loss=1.0204 val_acc=0.6934 time=30.6s
2025-10-12 11:58:36,615 - INFO - _models.training_function_executor - Epoch 049: train_loss=0.4340 val_loss=0.6270 val_acc=0.7887 time=30.7s
2025-10-12 11:58:36,618 - INFO - _models.training_function_executor - Model: 59,146 parameters, 254.1KB storage
2025-10-12 11:58:36,618 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9691476306761003, 0.7226215356016857, 0.6527294505819141, 0.6197756988322053, 0.5954955461434042, 0.5372601458807709, 0.5254915258716786, 0.5145657571812712, 0.5095991820243986, 0.5020188770990716, 0.47598912496977713, 0.47165262819674303, 0.47027959750484744, 0.4650625973694759, 0.4646170621641348, 0.45325157759658335, 0.45067193777270104, 0.45110361495648216, 0.4473085780104829, 0.4448438079036397, 0.443404474103837, 0.4404855752290456, 0.44169682179817893, 0.4405455334649262, 0.4391880599230314, 0.43550165004015756, 0.43909271782877607, 0.4393225161974489, 0.43692012855213946, 0.4382275750384316, 0.4377295249053738, 0.43526947118180886, 0.4351821462110358, 0.4353485524579087, 0.4352309721723162, 0.4353568698240442, 0.43545381135447747, 0.43690660101204126, 0.4338504214587566, 0.43787062375770086, 0.4357246858233082, 0.4349886659396852, 0.4345632334171977, 0.4372925478950375, 0.4365668746246378, 0.4339360272840101, 0.43567863937746376, 0.4372233846358496, 0.43399549005367316], 'val_losses': [0.7267859749004406, 0.9410051971401446, 0.6317063931148861, 0.6745577610857683, 0.7306927643725463, 0.5999606454006267, 0.568958572265886, 0.7360572035652271, 0.6402228338116475, 0.6402936466827097, 0.6020770563600903, 0.6425218261529958, 0.6025298795815053, 0.5837943150042851, 0.5484492436623788, 0.5953603238815954, 0.5849066675084196, 0.6653856543464196, 0.5828507775877624, 0.6569988357407159, 0.6397396920426198, 0.5987842626348764, 0.6213804760707903, 0.6227827282282786, 0.7331429166096045, 0.6180561365354982, 0.6143868014996817, 0.6344058066762992, 0.6251293664480637, 0.6254426031948794, 0.6224657145322626, 0.6333444329616925, 0.663240373950213, 0.6727834375103167, 0.5982430312028862, 0.630796558675146, 0.6240569187473606, 0.6352697963251657, 0.7882586819601223, 0.5988376544117615, 0.6016187367339494, 0.6081243730256106, 0.6097543287153872, 0.6310425879150802, 0.6107879873977353, 0.6180894849192785, 0.6154607649573761, 1.0204307471219767, 0.6270448940773167], 'val_acc': [0.7213860693034652, 0.6514700735036751, 0.7627756387819391, 0.7410745537276864, 0.7271613580679034, 0.7766888344417221, 0.787714385719286, 0.754375218760938, 0.7633881694084704, 0.761725586279314, 0.7780889044452223, 0.7711760588029402, 0.7859642982149108, 0.7909520476023801, 0.8007525376268814, 0.7894644732236612, 0.7878893944697235, 0.7594504725236262, 0.7900770038501925, 0.7740637031851593, 0.7839516975848793, 0.7876268813440672, 0.786226811340567, 0.7843892194609731, 0.758400420021001, 0.7881519075953798, 0.7887644382219111, 0.7868393419670984, 0.787714385719286, 0.7885894294714736, 0.789901995099755, 0.7706510325516276, 0.7739761988099405, 0.7780889044452223, 0.7949772488624431, 0.7874518725936297, 0.7901645082254113, 0.7837766888344417, 0.7291739586979349, 0.7937521876093805, 0.7906020301015051, 0.7859642982149108, 0.7918270913545677, 0.7857892894644732, 0.7866643332166608, 0.7822891144557228, 0.7868393419670984, 0.6933846692334616, 0.7886769338466924], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0014324711360093985, 'weight_decay': 0.00011169571562604382, 'epochs': 49, 'batch_size': 4, 'dropout': 0.10934934288960754, 'base_channels': 14, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.003963856506386299, 'grad_clip_norm': 3.7740801610105663, 'scheduler_step_size': 5, 'scheduler_gamma': 0.3808601325808343, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False, 'calibrate_batches': 7}, 'model_parameter_count': 59146, 'model_storage_size_kb': 254.14296875000002, 'model_size_validation': 'PASS'}
2025-10-12 11:58:36,618 - INFO - _models.training_function_executor - BO Objective: base=0.7887, size_penalty=0.0000, final=0.7887
2025-10-12 11:58:36,618 - INFO - _models.training_function_executor - Model: 59,146 parameters, 254.1KB (PASS 256KB limit)
2025-10-12 11:58:36,618 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 1496.560s
2025-10-12 11:58:36,718 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7887
2025-10-12 11:58:36,718 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-10-12 11:58:36,718 - INFO - bo.run_bo - Recorded observation #22: hparams={'lr': 0.0014324711360093985, 'weight_decay': 0.00011169571562604382, 'epochs': np.int64(49), 'batch_size': np.int64(4), 'dropout': 0.10934934288960754, 'base_channels': np.int64(14), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.003963856506386299, 'grad_clip_norm': 3.7740801610105663, 'scheduler_step_size': np.int64(5), 'scheduler_gamma': 0.3808601325808343, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(7)}, value=0.7887
2025-10-12 11:58:36,718 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 22: {'lr': 0.0014324711360093985, 'weight_decay': 0.00011169571562604382, 'epochs': np.int64(49), 'batch_size': np.int64(4), 'dropout': 0.10934934288960754, 'base_channels': np.int64(14), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.003963856506386299, 'grad_clip_norm': 3.7740801610105663, 'scheduler_step_size': np.int64(5), 'scheduler_gamma': 0.3808601325808343, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(7)} -> 0.7887
2025-10-12 11:58:36,719 - INFO - bo.run_bo - üîçBO Trial 23: Using RF surrogate + Expected Improvement
2025-10-12 11:58:36,719 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 11:58:36,719 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 11:58:36,719 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 11:58:36,719 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004819846283745847, 'weight_decay': 0.0006086158717586903, 'epochs': 39, 'batch_size': 8, 'dropout': 0.09233799360371688, 'base_channels': 8, 'eca_kernel': 5, 'pool_stride': 3, 'label_smoothing': 0.024292172402651607, 'grad_clip_norm': 4.076607453752, 'scheduler_step_size': 3, 'scheduler_gamma': 0.42350958098226543, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 8}
2025-10-12 11:58:36,720 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004819846283745847, 'weight_decay': 0.0006086158717586903, 'epochs': 39, 'batch_size': 8, 'dropout': 0.09233799360371688, 'base_channels': 8, 'eca_kernel': 5, 'pool_stride': 3, 'label_smoothing': 0.024292172402651607, 'grad_clip_norm': 4.076607453752, 'scheduler_step_size': 3, 'scheduler_gamma': 0.42350958098226543, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 8}
2025-10-12 11:58:55,370 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9906 val_loss=0.9829 val_acc=0.6306 time=18.6s
2025-10-12 11:59:13,423 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.8067 val_loss=0.7567 val_acc=0.7261 time=18.1s
2025-10-12 11:59:31,442 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7687 val_loss=0.8659 val_acc=0.6744 time=18.0s
2025-10-12 11:59:49,541 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7546 val_loss=0.8889 val_acc=0.6759 time=18.1s
2025-10-12 12:00:07,632 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.7331 val_loss=0.7735 val_acc=0.7353 time=18.1s
2025-10-12 12:00:25,762 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7768 val_loss=0.8041 val_acc=0.7148 time=18.1s
2025-10-12 12:00:43,804 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.7394 val_loss=0.7991 val_acc=0.7167 time=18.0s
2025-10-12 12:01:01,921 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.7219 val_loss=0.7824 val_acc=0.7237 time=18.1s
2025-10-12 12:01:20,016 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.7164 val_loss=0.7561 val_acc=0.7315 time=18.1s
2025-10-12 12:01:38,144 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6965 val_loss=0.7562 val_acc=0.7332 time=18.1s
2025-10-12 12:01:56,236 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.6898 val_loss=0.7982 val_acc=0.7127 time=18.1s
2025-10-12 12:02:14,317 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.6860 val_loss=0.7222 val_acc=0.7454 time=18.1s
2025-10-12 12:02:32,390 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.6772 val_loss=0.7249 val_acc=0.7433 time=18.1s
2025-10-12 12:02:50,553 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.6768 val_loss=0.7303 val_acc=0.7406 time=18.2s
2025-10-12 12:03:08,706 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.6702 val_loss=0.7224 val_acc=0.7474 time=18.2s
2025-10-12 12:03:26,865 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.6681 val_loss=0.7539 val_acc=0.7346 time=18.2s
2025-10-12 12:03:44,948 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.6658 val_loss=0.7279 val_acc=0.7443 time=18.1s
2025-10-12 12:04:02,979 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.6645 val_loss=0.7232 val_acc=0.7447 time=18.0s
2025-10-12 12:04:21,130 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.6659 val_loss=0.7146 val_acc=0.7463 time=18.2s
2025-10-12 12:04:39,203 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.6656 val_loss=0.7129 val_acc=0.7486 time=18.1s
2025-10-12 12:04:57,253 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.6699 val_loss=0.7268 val_acc=0.7405 time=18.1s
2025-10-12 12:05:15,337 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.6647 val_loss=0.7263 val_acc=0.7446 time=18.1s
2025-10-12 12:05:33,415 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.6653 val_loss=0.7269 val_acc=0.7437 time=18.1s
2025-10-12 12:05:51,557 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.6642 val_loss=0.7091 val_acc=0.7512 time=18.1s
2025-10-12 12:06:09,629 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.6637 val_loss=0.7300 val_acc=0.7412 time=18.1s
2025-10-12 12:06:27,748 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.6643 val_loss=0.7349 val_acc=0.7405 time=18.1s
2025-10-12 12:06:45,792 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.6639 val_loss=0.7666 val_acc=0.7234 time=18.0s
2025-10-12 12:07:03,880 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.6614 val_loss=0.8599 val_acc=0.6936 time=18.1s
2025-10-12 12:07:22,010 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.6659 val_loss=0.7204 val_acc=0.7453 time=18.1s
2025-10-12 12:07:40,092 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.6613 val_loss=0.7678 val_acc=0.7235 time=18.1s
2025-10-12 12:07:58,165 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.6642 val_loss=0.7232 val_acc=0.7455 time=18.1s
2025-10-12 12:08:16,301 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.6662 val_loss=0.7137 val_acc=0.7483 time=18.1s
2025-10-12 12:08:34,385 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.6627 val_loss=0.7270 val_acc=0.7427 time=18.1s
2025-10-12 12:08:52,486 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.6666 val_loss=0.7188 val_acc=0.7461 time=18.1s
2025-10-12 12:09:10,535 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.6647 val_loss=0.7061 val_acc=0.7504 time=18.0s
2025-10-12 12:09:28,641 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.6664 val_loss=0.7790 val_acc=0.7192 time=18.1s
2025-10-12 12:09:46,769 - INFO - _models.training_function_executor - Epoch 037: train_loss=0.6621 val_loss=0.7155 val_acc=0.7462 time=18.1s
2025-10-12 12:10:04,805 - INFO - _models.training_function_executor - Epoch 038: train_loss=0.6656 val_loss=0.7166 val_acc=0.7465 time=18.0s
2025-10-12 12:10:22,865 - INFO - _models.training_function_executor - Epoch 039: train_loss=0.6624 val_loss=0.8348 val_acc=0.7056 time=18.1s
2025-10-12 12:10:22,870 - INFO - _models.training_function_executor - Model: 4,965 parameters, 5.3KB storage
2025-10-12 12:10:22,870 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9906276014245887, 0.8067280475805179, 0.7687086549580702, 0.7545738397860469, 0.7330793926963699, 0.7768426704049987, 0.7394178388421878, 0.7218518514012723, 0.7164142975204885, 0.696531453440771, 0.6898089338734219, 0.6860017971059723, 0.6772370629578485, 0.6768248597558328, 0.6701740647686041, 0.6680801018530104, 0.665756911673056, 0.6644618722486629, 0.6658999999065186, 0.6655579925490961, 0.6698702825562579, 0.6646575894093322, 0.6653027835011608, 0.6642015925473294, 0.6636507051325695, 0.6642834832008419, 0.6639012664733835, 0.6613538594232559, 0.6658788445788035, 0.6613317842474341, 0.6642377168268727, 0.6662245349274635, 0.6627195615518289, 0.6666428657039337, 0.6646633854212151, 0.6664033852410683, 0.6620868896771487, 0.6655727669800083, 0.6623637872895746], 'val_losses': [0.9828573681849636, 0.7566942216311462, 0.8659377206673687, 0.8889487625357568, 0.7735404947607677, 0.804076673446688, 0.799144192632988, 0.7824163989802528, 0.756097674088166, 0.7562460644776311, 0.7981753918750745, 0.7221517517229122, 0.7248561966135487, 0.7303167168894639, 0.7224118181895158, 0.7538746246611024, 0.7279308764801853, 0.7232235426674534, 0.7146043387803324, 0.7128868735114052, 0.7268350228203064, 0.7262536838030039, 0.7268523480149888, 0.7091125320534961, 0.7299711729132883, 0.7348657960882544, 0.7666022538784129, 0.859894244856337, 0.7203805330002437, 0.7677653311879893, 0.7231950184390976, 0.7136802127259487, 0.7270113366923634, 0.7188159453053136, 0.7060501883037865, 0.7789792965186155, 0.7154627112781914, 0.7165548825414189, 0.834778111950893], 'val_acc': [0.6305565278263913, 0.7261113055652783, 0.6743962198109905, 0.6758837941897095, 0.7352992649632482, 0.7148232411620581, 0.7166608330416521, 0.7237486874343717, 0.7315365768288414, 0.7331991599579979, 0.7127231361568078, 0.7454497724886244, 0.7433496674833742, 0.7406370318515926, 0.7473748687434372, 0.7345992299614981, 0.7443122156107805, 0.7447497374868743, 0.746324816240812, 0.7485999299964998, 0.7405495274763738, 0.7445747287364368, 0.7436996849842492, 0.7512250612530627, 0.7411620581029051, 0.740462023101155, 0.7233986699334967, 0.6936471823591179, 0.7452747637381869, 0.7234861743087154, 0.7455372768638432, 0.7483374168708435, 0.7427371368568428, 0.7461498074903745, 0.7504375218760938, 0.7191984599229961, 0.7462373118655933, 0.7464998249912496, 0.7056352817640882], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004819846283745847, 'weight_decay': 0.0006086158717586903, 'epochs': 39, 'batch_size': 8, 'dropout': 0.09233799360371688, 'base_channels': 8, 'eca_kernel': 5, 'pool_stride': 3, 'label_smoothing': 0.024292172402651607, 'grad_clip_norm': 4.076607453752, 'scheduler_step_size': 3, 'scheduler_gamma': 0.42350958098226543, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 8}, 'model_parameter_count': 4965, 'model_storage_size_kb': 5.33349609375, 'model_size_validation': 'PASS'}
2025-10-12 12:10:22,870 - INFO - _models.training_function_executor - BO Objective: base=0.7056, size_penalty=0.0000, final=0.7056
2025-10-12 12:10:22,870 - INFO - _models.training_function_executor - Model: 4,965 parameters, 5.3KB (PASS 256KB limit)
2025-10-12 12:10:22,870 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 706.152s
2025-10-12 12:10:22,971 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7056
2025-10-12 12:10:22,971 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.100s
2025-10-12 12:10:22,971 - INFO - bo.run_bo - Recorded observation #23: hparams={'lr': 0.004819846283745847, 'weight_decay': 0.0006086158717586903, 'epochs': np.int64(39), 'batch_size': np.int64(8), 'dropout': 0.09233799360371688, 'base_channels': np.int64(8), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(3), 'label_smoothing': 0.024292172402651607, 'grad_clip_norm': 4.076607453752, 'scheduler_step_size': np.int64(3), 'scheduler_gamma': 0.42350958098226543, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(8)}, value=0.7056
2025-10-12 12:10:22,971 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 23: {'lr': 0.004819846283745847, 'weight_decay': 0.0006086158717586903, 'epochs': np.int64(39), 'batch_size': np.int64(8), 'dropout': 0.09233799360371688, 'base_channels': np.int64(8), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(3), 'label_smoothing': 0.024292172402651607, 'grad_clip_norm': 4.076607453752, 'scheduler_step_size': np.int64(3), 'scheduler_gamma': 0.42350958098226543, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(8)} -> 0.7056
2025-10-12 12:10:22,971 - INFO - bo.run_bo - üîçBO Trial 24: Using RF surrogate + Expected Improvement
2025-10-12 12:10:22,971 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 12:10:22,972 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 12:10:22,972 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 12:10:22,972 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.004325191349821037, 'weight_decay': 0.00015374504592603262, 'epochs': 8, 'batch_size': 16, 'dropout': 0.0832328766668581, 'base_channels': 20, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.07952523017774082, 'grad_clip_norm': 2.653533112489955, 'scheduler_step_size': 2, 'scheduler_gamma': 0.15164795118860852, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 3}
2025-10-12 12:10:22,973 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.004325191349821037, 'weight_decay': 0.00015374504592603262, 'epochs': 8, 'batch_size': 16, 'dropout': 0.0832328766668581, 'base_channels': 20, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.07952523017774082, 'grad_clip_norm': 2.653533112489955, 'scheduler_step_size': 2, 'scheduler_gamma': 0.15164795118860852, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 3}
2025-10-12 12:10:32,784 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0560 val_loss=1.1161 val_acc=0.6232 time=9.8s
2025-10-12 12:10:42,255 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.9069 val_loss=1.1877 val_acc=0.5525 time=9.5s
2025-10-12 12:10:51,770 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.9449 val_loss=0.8649 val_acc=0.7282 time=9.5s
2025-10-12 12:11:01,281 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.8725 val_loss=0.8923 val_acc=0.7156 time=9.5s
2025-10-12 12:11:10,834 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.8555 val_loss=0.8772 val_acc=0.7207 time=9.6s
2025-10-12 12:11:20,326 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.8491 val_loss=0.8237 val_acc=0.7464 time=9.5s
2025-10-12 12:11:29,833 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.8460 val_loss=0.8317 val_acc=0.7447 time=9.5s
2025-10-12 12:11:39,291 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.8458 val_loss=0.8905 val_acc=0.7146 time=9.5s
2025-10-12 12:11:39,295 - INFO - _models.training_function_executor - Model: 28,245 parameters, 30.3KB storage
2025-10-12 12:11:39,296 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.0559726679579629, 0.9069476871378654, 0.9448574503944351, 0.8724995664900151, 0.8555230976069019, 0.8491142148482537, 0.8460492474406092, 0.8457904828254017], 'val_losses': [1.1161326296186798, 1.1877347535923473, 0.8648970961195211, 0.8922500081065655, 0.8772285660121839, 0.8237163969633419, 0.8316604826347703, 0.8905339408555063], 'val_acc': [0.6232061603080155, 0.5525026251312566, 0.7282114105705285, 0.7156107805390269, 0.7206860343017151, 0.7464123206160308, 0.7446622331116556, 0.7146482324116206], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.004325191349821037, 'weight_decay': 0.00015374504592603262, 'epochs': 8, 'batch_size': 16, 'dropout': 0.0832328766668581, 'base_channels': 20, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.07952523017774082, 'grad_clip_norm': 2.653533112489955, 'scheduler_step_size': 2, 'scheduler_gamma': 0.15164795118860852, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 3}, 'model_parameter_count': 28245, 'model_storage_size_kb': 30.341308593750004, 'model_size_validation': 'PASS'}
2025-10-12 12:11:39,296 - INFO - _models.training_function_executor - BO Objective: base=0.7146, size_penalty=0.0000, final=0.7146
2025-10-12 12:11:39,296 - INFO - _models.training_function_executor - Model: 28,245 parameters, 30.3KB (PASS 256KB limit)
2025-10-12 12:11:39,296 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 76.324s
2025-10-12 12:11:39,399 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7146
2025-10-12 12:11:39,400 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.101s
2025-10-12 12:11:39,400 - INFO - bo.run_bo - Recorded observation #24: hparams={'lr': 0.004325191349821037, 'weight_decay': 0.00015374504592603262, 'epochs': np.int64(8), 'batch_size': np.int64(16), 'dropout': 0.0832328766668581, 'base_channels': np.int64(20), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.07952523017774082, 'grad_clip_norm': 2.653533112489955, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.15164795118860852, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(3)}, value=0.7146
2025-10-12 12:11:39,400 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 24: {'lr': 0.004325191349821037, 'weight_decay': 0.00015374504592603262, 'epochs': np.int64(8), 'batch_size': np.int64(16), 'dropout': 0.0832328766668581, 'base_channels': np.int64(20), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.07952523017774082, 'grad_clip_norm': 2.653533112489955, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.15164795118860852, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(3)} -> 0.7146
2025-10-12 12:11:39,400 - INFO - bo.run_bo - üîçBO Trial 25: Using RF surrogate + Expected Improvement
2025-10-12 12:11:39,400 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 12:11:39,400 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 12:11:39,400 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 12:11:39,400 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0025081727114233776, 'weight_decay': 0.0006961816374668264, 'epochs': 26, 'batch_size': 12, 'dropout': 0.040533873329526966, 'base_channels': 10, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03299875802365176, 'grad_clip_norm': 1.21456416901548, 'scheduler_step_size': 10, 'scheduler_gamma': 0.42373363797177255, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 1}
2025-10-12 12:11:39,401 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0025081727114233776, 'weight_decay': 0.0006961816374668264, 'epochs': 26, 'batch_size': 12, 'dropout': 0.040533873329526966, 'base_channels': 10, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03299875802365176, 'grad_clip_norm': 1.21456416901548, 'scheduler_step_size': 10, 'scheduler_gamma': 0.42373363797177255, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 1}
2025-10-12 12:11:51,840 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9750 val_loss=0.7432 val_acc=0.7394 time=12.4s
2025-10-12 12:12:03,357 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7696 val_loss=0.7794 val_acc=0.7220 time=11.5s
2025-10-12 12:12:14,947 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.7132 val_loss=0.6751 val_acc=0.7679 time=11.6s
2025-10-12 12:12:26,534 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6878 val_loss=0.7288 val_acc=0.7515 time=11.6s
2025-10-12 12:12:38,158 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.6612 val_loss=0.7356 val_acc=0.7539 time=11.6s
2025-10-12 12:12:49,723 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.6479 val_loss=0.6275 val_acc=0.7898 time=11.6s
2025-10-12 12:13:01,279 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.6397 val_loss=0.6278 val_acc=0.7917 time=11.6s
2025-10-12 12:13:12,859 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.6276 val_loss=0.6880 val_acc=0.7697 time=11.6s
2025-10-12 12:13:24,422 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.6222 val_loss=0.6119 val_acc=0.8046 time=11.6s
2025-10-12 12:13:35,970 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6131 val_loss=0.7471 val_acc=0.7411 time=11.5s
2025-10-12 12:13:47,568 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.5739 val_loss=0.5888 val_acc=0.8082 time=11.6s
2025-10-12 12:13:59,119 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.5657 val_loss=0.6103 val_acc=0.8031 time=11.6s
2025-10-12 12:14:10,682 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.5620 val_loss=0.6070 val_acc=0.8026 time=11.6s
2025-10-12 12:14:22,217 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.5564 val_loss=0.6029 val_acc=0.8050 time=11.5s
2025-10-12 12:14:33,790 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.5540 val_loss=0.6049 val_acc=0.8039 time=11.6s
2025-10-12 12:14:45,232 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.5488 val_loss=0.6430 val_acc=0.7891 time=11.4s
2025-10-12 12:14:56,763 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.5482 val_loss=0.7478 val_acc=0.7409 time=11.5s
2025-10-12 12:15:08,315 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.5458 val_loss=0.6289 val_acc=0.7948 time=11.6s
2025-10-12 12:15:19,935 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.5403 val_loss=0.6171 val_acc=0.7959 time=11.6s
2025-10-12 12:15:31,567 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.5404 val_loss=0.6201 val_acc=0.8040 time=11.6s
2025-10-12 12:15:43,119 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.5216 val_loss=0.6002 val_acc=0.8082 time=11.6s
2025-10-12 12:15:54,609 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.5173 val_loss=0.6054 val_acc=0.8071 time=11.5s
2025-10-12 12:16:06,216 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.5158 val_loss=0.6074 val_acc=0.8075 time=11.6s
2025-10-12 12:16:17,753 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.5140 val_loss=0.6194 val_acc=0.8007 time=11.5s
2025-10-12 12:16:29,291 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.5111 val_loss=0.6020 val_acc=0.8093 time=11.5s
2025-10-12 12:16:40,769 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.5111 val_loss=0.6170 val_acc=0.8029 time=11.5s
2025-10-12 12:16:40,774 - INFO - _models.training_function_executor - Model: 7,523 parameters, 8.1KB storage
2025-10-12 12:16:40,774 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9750304310538571, 0.7695514694714216, 0.7131531997435486, 0.6878093308842512, 0.6612226672282806, 0.6479123768468065, 0.6397047659599326, 0.6275598980019591, 0.6222357888010714, 0.6130953725658183, 0.5738939200995746, 0.5656567222817215, 0.5620380299238212, 0.5564258216489413, 0.5540375736720357, 0.548816458711653, 0.548167104426332, 0.5458232414565451, 0.5402656770593268, 0.5404060748378047, 0.521626278679814, 0.5173397624515073, 0.5157641337171984, 0.5140288876259415, 0.5111212303700942, 0.5111075288188762], 'val_losses': [0.743240771137796, 0.7794213125943136, 0.675123487459349, 0.7288007840675869, 0.7355741016569362, 0.6275318664298802, 0.6278355176663916, 0.6879555324485638, 0.6119031103358512, 0.7470885792608273, 0.5887566399315582, 0.6103463875358657, 0.607006555713262, 0.6028572817652176, 0.6048976771499588, 0.6429798947265818, 0.7478163287768549, 0.6288918127697959, 0.6170944276601232, 0.6201495157517584, 0.6001521387314479, 0.6053835012469836, 0.6073828317749154, 0.619402641823628, 0.6019649447356231, 0.6169849812911448], 'val_acc': [0.7394119705985299, 0.7219985999299965, 0.7678508925446272, 0.7514875743787189, 0.7539376968848442, 0.7898144907245362, 0.791739586979349, 0.7696884844242212, 0.8046027301365068, 0.7410745537276864, 0.808190409520476, 0.8031151557577879, 0.8025901295064753, 0.8050402520126007, 0.8039026951347568, 0.7891144557227862, 0.7408995449772489, 0.7948022401120056, 0.7959397969898495, 0.8039901995099755, 0.808190409520476, 0.8071403570178509, 0.807490374518726, 0.8006650332516626, 0.8093279663983199, 0.8028526426321316], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0025081727114233776, 'weight_decay': 0.0006961816374668264, 'epochs': 26, 'batch_size': 12, 'dropout': 0.040533873329526966, 'base_channels': 10, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03299875802365176, 'grad_clip_norm': 1.21456416901548, 'scheduler_step_size': 10, 'scheduler_gamma': 0.42373363797177255, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 1}, 'model_parameter_count': 7523, 'model_storage_size_kb': 8.081347656250001, 'model_size_validation': 'PASS'}
2025-10-12 12:16:40,774 - INFO - _models.training_function_executor - BO Objective: base=0.8029, size_penalty=0.0000, final=0.8029
2025-10-12 12:16:40,774 - INFO - _models.training_function_executor - Model: 7,523 parameters, 8.1KB (PASS 256KB limit)
2025-10-12 12:16:40,774 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 301.374s
2025-10-12 12:16:40,877 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8029
2025-10-12 12:16:40,878 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.102s
2025-10-12 12:16:40,878 - INFO - bo.run_bo - Recorded observation #25: hparams={'lr': 0.0025081727114233776, 'weight_decay': 0.0006961816374668264, 'epochs': np.int64(26), 'batch_size': np.int64(12), 'dropout': 0.040533873329526966, 'base_channels': np.int64(10), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.03299875802365176, 'grad_clip_norm': 1.21456416901548, 'scheduler_step_size': np.int64(10), 'scheduler_gamma': 0.42373363797177255, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(1)}, value=0.8029
2025-10-12 12:16:40,878 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 25: {'lr': 0.0025081727114233776, 'weight_decay': 0.0006961816374668264, 'epochs': np.int64(26), 'batch_size': np.int64(12), 'dropout': 0.040533873329526966, 'base_channels': np.int64(10), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.03299875802365176, 'grad_clip_norm': 1.21456416901548, 'scheduler_step_size': np.int64(10), 'scheduler_gamma': 0.42373363797177255, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(1)} -> 0.8029
2025-10-12 12:16:40,878 - INFO - bo.run_bo - üîçBO Trial 26: Using RF surrogate + Expected Improvement
2025-10-12 12:16:40,878 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 12:16:40,878 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 12:16:40,878 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 12:16:40,878 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.003569650973383609, 'weight_decay': 0.000649966514124665, 'epochs': 38, 'batch_size': 8, 'dropout': 0.03509768527797392, 'base_channels': 22, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.02914301442156792, 'grad_clip_norm': 2.072440322458, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11457813716224226, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 2}
2025-10-12 12:16:40,879 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.003569650973383609, 'weight_decay': 0.000649966514124665, 'epochs': 38, 'batch_size': 8, 'dropout': 0.03509768527797392, 'base_channels': 22, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.02914301442156792, 'grad_clip_norm': 2.072440322458, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11457813716224226, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 2}
2025-10-12 12:16:55,646 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.9598 val_loss=0.7906 val_acc=0.7055 time=14.8s
2025-10-12 12:17:10,329 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.7847 val_loss=0.8167 val_acc=0.7067 time=14.7s
2025-10-12 12:17:24,989 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.6810 val_loss=0.7788 val_acc=0.7269 time=14.7s
2025-10-12 12:17:39,672 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.6527 val_loss=0.6519 val_acc=0.7757 time=14.7s
2025-10-12 12:17:54,397 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.6375 val_loss=0.6435 val_acc=0.7823 time=14.7s
2025-10-12 12:18:09,140 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.6289 val_loss=0.7872 val_acc=0.7165 time=14.7s
2025-10-12 12:18:23,871 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.6302 val_loss=0.6373 val_acc=0.7846 time=14.7s
2025-10-12 12:18:38,541 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.6292 val_loss=0.6444 val_acc=0.7805 time=14.7s
2025-10-12 12:18:53,228 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.6300 val_loss=0.6592 val_acc=0.7737 time=14.7s
2025-10-12 12:19:07,890 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6286 val_loss=0.6367 val_acc=0.7855 time=14.7s
2025-10-12 12:19:22,552 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.6276 val_loss=0.6447 val_acc=0.7799 time=14.7s
2025-10-12 12:19:37,233 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.6275 val_loss=0.7002 val_acc=0.7519 time=14.7s
2025-10-12 12:19:51,935 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.6301 val_loss=0.6363 val_acc=0.7857 time=14.7s
2025-10-12 12:20:06,634 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.6256 val_loss=0.6351 val_acc=0.7857 time=14.7s
2025-10-12 12:20:21,344 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.6262 val_loss=0.6430 val_acc=0.7833 time=14.7s
2025-10-12 12:20:36,075 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.6300 val_loss=0.6387 val_acc=0.7846 time=14.7s
2025-10-12 12:20:50,778 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.6299 val_loss=0.6874 val_acc=0.7605 time=14.7s
2025-10-12 12:21:05,476 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.6295 val_loss=0.7781 val_acc=0.7209 time=14.7s
2025-10-12 12:21:20,154 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.6298 val_loss=0.6323 val_acc=0.7872 time=14.7s
2025-10-12 12:21:34,896 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.6278 val_loss=0.6404 val_acc=0.7846 time=14.7s
2025-10-12 12:21:49,572 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.6290 val_loss=0.6384 val_acc=0.7851 time=14.7s
2025-10-12 12:22:04,221 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.6274 val_loss=0.6453 val_acc=0.7825 time=14.6s
2025-10-12 12:22:18,815 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.6265 val_loss=0.6386 val_acc=0.7833 time=14.6s
2025-10-12 12:22:33,540 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.6303 val_loss=0.6372 val_acc=0.7857 time=14.7s
2025-10-12 12:22:48,262 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.6284 val_loss=0.6398 val_acc=0.7809 time=14.7s
2025-10-12 12:23:02,888 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.6273 val_loss=0.6586 val_acc=0.7777 time=14.6s
2025-10-12 12:23:17,614 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.6303 val_loss=0.6371 val_acc=0.7847 time=14.7s
2025-10-12 12:23:32,234 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.6275 val_loss=0.7677 val_acc=0.7266 time=14.6s
2025-10-12 12:23:46,905 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.6272 val_loss=0.6339 val_acc=0.7875 time=14.7s
2025-10-12 12:24:01,518 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.6288 val_loss=0.6433 val_acc=0.7840 time=14.6s
2025-10-12 12:24:16,189 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.6289 val_loss=0.6397 val_acc=0.7835 time=14.7s
2025-10-12 12:24:30,970 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.6288 val_loss=0.6718 val_acc=0.7693 time=14.8s
2025-10-12 12:24:45,627 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.6273 val_loss=0.6549 val_acc=0.7753 time=14.7s
2025-10-12 12:25:00,401 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.6292 val_loss=0.6736 val_acc=0.7679 time=14.8s
2025-10-12 12:25:15,055 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.6260 val_loss=0.6453 val_acc=0.7805 time=14.7s
2025-10-12 12:25:29,738 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.6267 val_loss=0.6457 val_acc=0.7775 time=14.7s
2025-10-12 12:25:44,370 - INFO - _models.training_function_executor - Epoch 037: train_loss=0.6279 val_loss=0.6690 val_acc=0.7678 time=14.6s
2025-10-12 12:25:59,046 - INFO - _models.training_function_executor - Epoch 038: train_loss=0.6265 val_loss=0.6376 val_acc=0.7845 time=14.7s
2025-10-12 12:25:59,051 - INFO - _models.training_function_executor - Model: 33,973 parameters, 36.5KB storage
2025-10-12 12:25:59,051 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.9598090547806197, 0.7846823323584537, 0.6809609621752798, 0.6526581335435975, 0.6375310455569864, 0.6289090815518217, 0.630199157199661, 0.6291665748403253, 0.6299542870332112, 0.6286418706981878, 0.627614794589128, 0.6275355144265986, 0.6301041311621248, 0.6255680877620163, 0.6261797196977948, 0.6300271988400096, 0.629905451945817, 0.6294654960065068, 0.62976223555298, 0.6277920924567826, 0.6290245668263047, 0.62736306645662, 0.6265001522626833, 0.630313007871004, 0.6284317102232386, 0.6272811163158153, 0.6303327763814963, 0.6274581384050667, 0.6271589131967814, 0.628826310710136, 0.6288549854104903, 0.6288128940973366, 0.6272819404173114, 0.6291583496940315, 0.6259779485870293, 0.6267109867009325, 0.6279419848540884, 0.6264534624809146], 'val_losses': [0.790627515468307, 0.8167104658898583, 0.7788465825419014, 0.6518651876607724, 0.6435348954668486, 0.7872323985174063, 0.6372984554494725, 0.6443865204925591, 0.6591575507167172, 0.6366870795612282, 0.6446858594154286, 0.7002334438882426, 0.6363219993386521, 0.6350572363903442, 0.6429824327823788, 0.6387047668645797, 0.6874115229967303, 0.7780937817929547, 0.6323030471415798, 0.6404128739591407, 0.6384025460195825, 0.6453272370888213, 0.6385544486914976, 0.6371932482026781, 0.6397766150771466, 0.6585723546770097, 0.6371410509736378, 0.7677228262750344, 0.6338729903879056, 0.6432648052910971, 0.6396645153626555, 0.6718333054243955, 0.6548593334689355, 0.6736378243670916, 0.6453496595081183, 0.6457026640737562, 0.6690308544804343, 0.6375983712355657], 'val_acc': [0.7054602730136507, 0.7066853342667133, 0.7268988449422471, 0.7757262863143157, 0.7822891144557228, 0.7164858242912145, 0.7845642282114106, 0.7804515225761288, 0.7737136856842842, 0.785526776338817, 0.7799264963248163, 0.7519250962548127, 0.7857017850892545, 0.7857017850892545, 0.7833391669583479, 0.7845642282114106, 0.7605005250262513, 0.7208610430521526, 0.7871893594679734, 0.7845642282114106, 0.7850892544627232, 0.7824641232061603, 0.7833391669583479, 0.7857017850892545, 0.7808890444522226, 0.7777388869443472, 0.7847392369618481, 0.7266363318165908, 0.7875393769688485, 0.784039201960098, 0.7835141757087855, 0.7693384669233462, 0.7752887644382219, 0.7678508925446272, 0.7804515225761288, 0.777476373818691, 0.7677633881694085, 0.7844767238361918], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.003569650973383609, 'weight_decay': 0.000649966514124665, 'epochs': 38, 'batch_size': 8, 'dropout': 0.03509768527797392, 'base_channels': 22, 'eca_kernel': 5, 'pool_stride': 4, 'label_smoothing': 0.02914301442156792, 'grad_clip_norm': 2.072440322458, 'scheduler_step_size': 2, 'scheduler_gamma': 0.11457813716224226, 'num_workers': 4, 'use_amp': False, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 2}, 'model_parameter_count': 33973, 'model_storage_size_kb': 36.49443359375, 'model_size_validation': 'PASS'}
2025-10-12 12:25:59,051 - INFO - _models.training_function_executor - BO Objective: base=0.7845, size_penalty=0.0000, final=0.7845
2025-10-12 12:25:59,051 - INFO - _models.training_function_executor - Model: 33,973 parameters, 36.5KB (PASS 256KB limit)
2025-10-12 12:25:59,051 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 558.173s
2025-10-12 12:25:59,155 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7845
2025-10-12 12:25:59,155 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.103s
2025-10-12 12:25:59,155 - INFO - bo.run_bo - Recorded observation #26: hparams={'lr': 0.003569650973383609, 'weight_decay': 0.000649966514124665, 'epochs': np.int64(38), 'batch_size': np.int64(8), 'dropout': 0.03509768527797392, 'base_channels': np.int64(22), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(4), 'label_smoothing': 0.02914301442156792, 'grad_clip_norm': 2.072440322458, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.11457813716224226, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(2)}, value=0.7845
2025-10-12 12:25:59,155 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 26: {'lr': 0.003569650973383609, 'weight_decay': 0.000649966514124665, 'epochs': np.int64(38), 'batch_size': np.int64(8), 'dropout': 0.03509768527797392, 'base_channels': np.int64(22), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(4), 'label_smoothing': 0.02914301442156792, 'grad_clip_norm': 2.072440322458, 'scheduler_step_size': np.int64(2), 'scheduler_gamma': 0.11457813716224226, 'num_workers': np.int64(4), 'use_amp': np.False_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(2)} -> 0.7845
2025-10-12 12:25:59,156 - INFO - bo.run_bo - üîçBO Trial 27: Using RF surrogate + Expected Improvement
2025-10-12 12:25:59,156 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 12:25:59,156 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 12:25:59,156 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 12:25:59,156 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00460659270402966, 'weight_decay': 6.211959821326813e-05, 'epochs': 32, 'batch_size': 4, 'dropout': 0.1393999961764887, 'base_channels': 22, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.016041562078395778, 'grad_clip_norm': 1.868198008546652, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5651240135550971, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 5}
2025-10-12 12:25:59,157 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00460659270402966, 'weight_decay': 6.211959821326813e-05, 'epochs': 32, 'batch_size': 4, 'dropout': 0.1393999961764887, 'base_channels': 22, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.016041562078395778, 'grad_clip_norm': 1.868198008546652, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5651240135550971, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 5}
2025-10-12 12:26:30,864 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.2459 val_loss=1.0947 val_acc=0.5755 time=31.7s
2025-10-12 12:27:02,036 - INFO - _models.training_function_executor - Epoch 002: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 12:27:33,372 - INFO - _models.training_function_executor - Epoch 003: train_loss=nan val_loss=nan val_acc=0.2325 time=31.3s
2025-10-12 12:28:04,501 - INFO - _models.training_function_executor - Epoch 004: train_loss=nan val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 12:28:35,884 - INFO - _models.training_function_executor - Epoch 005: train_loss=nan val_loss=nan val_acc=0.2325 time=31.4s
2025-10-12 12:29:07,127 - INFO - _models.training_function_executor - Epoch 006: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 12:29:38,599 - INFO - _models.training_function_executor - Epoch 007: train_loss=nan val_loss=nan val_acc=0.2325 time=31.5s
2025-10-12 12:30:09,748 - INFO - _models.training_function_executor - Epoch 008: train_loss=nan val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 12:30:40,760 - INFO - _models.training_function_executor - Epoch 009: train_loss=nan val_loss=nan val_acc=0.2325 time=31.0s
2025-10-12 12:31:12,084 - INFO - _models.training_function_executor - Epoch 010: train_loss=nan val_loss=nan val_acc=0.2325 time=31.3s
2025-10-12 12:31:43,289 - INFO - _models.training_function_executor - Epoch 011: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 12:32:14,485 - INFO - _models.training_function_executor - Epoch 012: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 12:32:45,717 - INFO - _models.training_function_executor - Epoch 013: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 12:33:16,962 - INFO - _models.training_function_executor - Epoch 014: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 12:33:48,315 - INFO - _models.training_function_executor - Epoch 015: train_loss=nan val_loss=nan val_acc=0.2325 time=31.4s
2025-10-12 12:34:19,343 - INFO - _models.training_function_executor - Epoch 016: train_loss=nan val_loss=nan val_acc=0.2325 time=31.0s
2025-10-12 12:34:50,642 - INFO - _models.training_function_executor - Epoch 017: train_loss=nan val_loss=nan val_acc=0.2325 time=31.3s
2025-10-12 12:35:21,757 - INFO - _models.training_function_executor - Epoch 018: train_loss=nan val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 12:35:53,013 - INFO - _models.training_function_executor - Epoch 019: train_loss=nan val_loss=nan val_acc=0.2325 time=31.3s
2025-10-12 12:36:24,153 - INFO - _models.training_function_executor - Epoch 020: train_loss=nan val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 12:36:55,391 - INFO - _models.training_function_executor - Epoch 021: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 12:37:26,315 - INFO - _models.training_function_executor - Epoch 022: train_loss=nan val_loss=nan val_acc=0.2325 time=30.9s
2025-10-12 12:37:57,551 - INFO - _models.training_function_executor - Epoch 023: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 12:38:28,598 - INFO - _models.training_function_executor - Epoch 024: train_loss=nan val_loss=nan val_acc=0.2325 time=31.0s
2025-10-12 12:38:59,953 - INFO - _models.training_function_executor - Epoch 025: train_loss=nan val_loss=nan val_acc=0.2325 time=31.4s
2025-10-12 12:39:31,169 - INFO - _models.training_function_executor - Epoch 026: train_loss=nan val_loss=nan val_acc=0.2325 time=31.2s
2025-10-12 12:40:02,266 - INFO - _models.training_function_executor - Epoch 027: train_loss=nan val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 12:40:33,276 - INFO - _models.training_function_executor - Epoch 028: train_loss=nan val_loss=nan val_acc=0.2325 time=31.0s
2025-10-12 12:41:04,585 - INFO - _models.training_function_executor - Epoch 029: train_loss=nan val_loss=nan val_acc=0.2325 time=31.3s
2025-10-12 12:41:35,925 - INFO - _models.training_function_executor - Epoch 030: train_loss=nan val_loss=nan val_acc=0.2325 time=31.3s
2025-10-12 12:42:07,312 - INFO - _models.training_function_executor - Epoch 031: train_loss=nan val_loss=nan val_acc=0.2325 time=31.4s
2025-10-12 12:42:38,217 - INFO - _models.training_function_executor - Epoch 032: train_loss=nan val_loss=nan val_acc=0.2325 time=30.9s
2025-10-12 12:42:38,222 - INFO - _models.training_function_executor - Model: 33,971 parameters, 36.5KB storage
2025-10-12 12:42:38,222 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.2458577984393082, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'val_losses': [1.0946652139421313, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'val_acc': [0.5755162758137907, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00460659270402966, 'weight_decay': 6.211959821326813e-05, 'epochs': 32, 'batch_size': 4, 'dropout': 0.1393999961764887, 'base_channels': 22, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.016041562078395778, 'grad_clip_norm': 1.868198008546652, 'scheduler_step_size': 10, 'scheduler_gamma': 0.5651240135550971, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 5}, 'model_parameter_count': 33971, 'model_storage_size_kb': 36.492285156250006, 'model_size_validation': 'PASS'}
2025-10-12 12:42:38,222 - INFO - _models.training_function_executor - BO Objective: base=0.2325, size_penalty=0.0000, final=0.2325
2025-10-12 12:42:38,222 - INFO - _models.training_function_executor - Model: 33,971 parameters, 36.5KB (PASS 256KB limit)
2025-10-12 12:42:38,222 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 999.066s
2025-10-12 12:42:38,325 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2325
2025-10-12 12:42:38,325 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.103s
2025-10-12 12:42:38,325 - INFO - bo.run_bo - Recorded observation #27: hparams={'lr': 0.00460659270402966, 'weight_decay': 6.211959821326813e-05, 'epochs': np.int64(32), 'batch_size': np.int64(4), 'dropout': 0.1393999961764887, 'base_channels': np.int64(22), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(5), 'label_smoothing': 0.016041562078395778, 'grad_clip_norm': 1.868198008546652, 'scheduler_step_size': np.int64(10), 'scheduler_gamma': 0.5651240135550971, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(5)}, value=0.2325
2025-10-12 12:42:38,326 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 27: {'lr': 0.00460659270402966, 'weight_decay': 6.211959821326813e-05, 'epochs': np.int64(32), 'batch_size': np.int64(4), 'dropout': 0.1393999961764887, 'base_channels': np.int64(22), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(5), 'label_smoothing': 0.016041562078395778, 'grad_clip_norm': 1.868198008546652, 'scheduler_step_size': np.int64(10), 'scheduler_gamma': 0.5651240135550971, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(5)} -> 0.2325
2025-10-12 12:42:38,326 - INFO - bo.run_bo - üîçBO Trial 28: Using RF surrogate + Expected Improvement
2025-10-12 12:42:38,326 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 12:42:38,326 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 12:42:38,326 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 12:42:38,326 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0018825435344418245, 'weight_decay': 0.0008287161307726618, 'epochs': 20, 'batch_size': 16, 'dropout': 0.007697616196014058, 'base_channels': 21, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.016822787231011296, 'grad_clip_norm': 2.342573920977913, 'scheduler_step_size': 1, 'scheduler_gamma': 0.3597919287700696, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 6}
2025-10-12 12:42:38,327 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0018825435344418245, 'weight_decay': 0.0008287161307726618, 'epochs': 20, 'batch_size': 16, 'dropout': 0.007697616196014058, 'base_channels': 21, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.016822787231011296, 'grad_clip_norm': 2.342573920977913, 'scheduler_step_size': 1, 'scheduler_gamma': 0.3597919287700696, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 6}
2025-10-12 12:42:48,921 - INFO - _models.training_function_executor - Epoch 001: train_loss=0.8172 val_loss=0.6861 val_acc=0.7491 time=10.6s
2025-10-12 12:42:59,058 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.6012 val_loss=0.6610 val_acc=0.7653 time=10.1s
2025-10-12 12:43:09,203 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.5523 val_loss=0.5617 val_acc=0.8031 time=10.1s
2025-10-12 12:43:19,287 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.5291 val_loss=0.5476 val_acc=0.8051 time=10.1s
2025-10-12 12:43:29,348 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.5210 val_loss=0.5392 val_acc=0.8106 time=10.1s
2025-10-12 12:43:39,455 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.5182 val_loss=0.5589 val_acc=0.8021 time=10.1s
2025-10-12 12:43:49,533 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.5161 val_loss=0.5482 val_acc=0.8030 time=10.1s
2025-10-12 12:43:59,610 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.5190 val_loss=0.5347 val_acc=0.8106 time=10.1s
2025-10-12 12:44:09,650 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.5150 val_loss=0.5353 val_acc=0.8123 time=10.0s
2025-10-12 12:44:19,720 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.5158 val_loss=0.5383 val_acc=0.8087 time=10.1s
2025-10-12 12:44:29,810 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.5156 val_loss=0.5319 val_acc=0.8113 time=10.1s
2025-10-12 12:44:39,916 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.5155 val_loss=0.5340 val_acc=0.8113 time=10.1s
2025-10-12 12:44:50,014 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.5158 val_loss=0.5341 val_acc=0.8132 time=10.1s
2025-10-12 12:45:00,123 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.5161 val_loss=0.5340 val_acc=0.8113 time=10.1s
2025-10-12 12:45:10,229 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.5166 val_loss=0.5302 val_acc=0.8121 time=10.1s
2025-10-12 12:45:20,298 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.5167 val_loss=0.5348 val_acc=0.8126 time=10.1s
2025-10-12 12:45:30,335 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.5167 val_loss=0.6224 val_acc=0.7707 time=10.0s
2025-10-12 12:45:40,391 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.5173 val_loss=0.5320 val_acc=0.8124 time=10.1s
2025-10-12 12:45:50,461 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.5159 val_loss=0.5425 val_acc=0.8075 time=10.1s
2025-10-12 12:46:00,557 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.5174 val_loss=0.6153 val_acc=0.7786 time=10.1s
2025-10-12 12:46:00,560 - INFO - _models.training_function_executor - Model: 131,342 parameters, 282.2KB storage
2025-10-12 12:46:00,560 - WARNING - _models.training_function_executor - Model storage 282.2KB exceeds 256KB limit!
2025-10-12 12:46:00,560 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.8171920332722535, 0.6012198763697724, 0.552257838587599, 0.5291096008153574, 0.520972260007668, 0.5182370528980365, 0.5160552457282708, 0.5189641259707143, 0.5149724271838215, 0.515816753290106, 0.5156127992765779, 0.5155233159468313, 0.5158095879126229, 0.5160651344298732, 0.5166160503091368, 0.5167488440542651, 0.5166693293822993, 0.5172733383648955, 0.5159083864292066, 0.517359763123082], 'val_losses': [0.6860942335386647, 0.6610051737742947, 0.5616925270731815, 0.5476022873844432, 0.5392031765324192, 0.5588517897209172, 0.548232111118872, 0.5346939259521353, 0.5352625091738495, 0.5383045552575509, 0.5318546790863027, 0.5340213188298482, 0.5340612917606506, 0.5340271068069957, 0.5302364955342312, 0.5348133159958486, 0.6223973760268552, 0.5320437419318552, 0.5424741275000914, 0.6152792272969981], 'val_acc': [0.7491249562478124, 0.7653132656632832, 0.8031151557577879, 0.8051277563878194, 0.8105530276513826, 0.8020651032551628, 0.8030276513825692, 0.8106405320266014, 0.8123031151557578, 0.8087154357717886, 0.8112530626531327, 0.8113405670283514, 0.8131781589079454, 0.8113405670283514, 0.8121281064053203, 0.8125656282814141, 0.7707385369268464, 0.8123906195309766, 0.807490374518726, 0.7786139306965348], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0018825435344418245, 'weight_decay': 0.0008287161307726618, 'epochs': 20, 'batch_size': 16, 'dropout': 0.007697616196014058, 'base_channels': 21, 'eca_kernel': 3, 'pool_stride': 5, 'label_smoothing': 0.016822787231011296, 'grad_clip_norm': 2.342573920977913, 'scheduler_step_size': 1, 'scheduler_gamma': 0.3597919287700696, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True, 'calibrate_batches': 6}, 'model_parameter_count': 131342, 'model_storage_size_kb': 282.180078125, 'model_size_validation': 'FAIL'}
2025-10-12 12:46:00,560 - INFO - _models.training_function_executor - BO Objective: base=0.7786, size_penalty=0.0511, final=0.7275
2025-10-12 12:46:00,560 - INFO - _models.training_function_executor - Model: 131,342 parameters, 282.2KB (FAIL 256KB limit)
2025-10-12 12:46:00,560 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 202.234s
2025-10-12 12:46:00,791 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7275
2025-10-12 12:46:00,792 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.230s
2025-10-12 12:46:00,792 - INFO - bo.run_bo - Recorded observation #28: hparams={'lr': 0.0018825435344418245, 'weight_decay': 0.0008287161307726618, 'epochs': np.int64(20), 'batch_size': np.int64(16), 'dropout': 0.007697616196014058, 'base_channels': np.int64(21), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(5), 'label_smoothing': 0.016822787231011296, 'grad_clip_norm': 2.342573920977913, 'scheduler_step_size': np.int64(1), 'scheduler_gamma': 0.3597919287700696, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(6)}, value=0.7275
2025-10-12 12:46:00,792 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 28: {'lr': 0.0018825435344418245, 'weight_decay': 0.0008287161307726618, 'epochs': np.int64(20), 'batch_size': np.int64(16), 'dropout': 0.007697616196014058, 'base_channels': np.int64(21), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(5), 'label_smoothing': 0.016822787231011296, 'grad_clip_norm': 2.342573920977913, 'scheduler_step_size': np.int64(1), 'scheduler_gamma': 0.3597919287700696, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.True_, 'calibrate_batches': np.int64(6)} -> 0.7275
2025-10-12 12:46:00,792 - INFO - bo.run_bo - üîçBO Trial 29: Using RF surrogate + Expected Improvement
2025-10-12 12:46:00,792 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 12:46:00,792 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 12:46:00,792 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 12:46:00,792 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0046008085189395585, 'weight_decay': 0.0009921281096808422, 'epochs': 46, 'batch_size': 4, 'dropout': 0.1065463769190467, 'base_channels': 16, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03665682658659703, 'grad_clip_norm': 2.7925686711090165, 'scheduler_step_size': 9, 'scheduler_gamma': 0.44579979131946623, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 2}
2025-10-12 12:46:00,796 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0046008085189395585, 'weight_decay': 0.0009921281096808422, 'epochs': 46, 'batch_size': 4, 'dropout': 0.1065463769190467, 'base_channels': 16, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03665682658659703, 'grad_clip_norm': 2.7925686711090165, 'scheduler_step_size': 9, 'scheduler_gamma': 0.44579979131946623, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 2}
2025-10-12 12:46:32,017 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.2173 val_loss=1.3371 val_acc=0.4130 time=31.2s
2025-10-12 12:47:02,777 - INFO - _models.training_function_executor - Epoch 002: train_loss=nan val_loss=nan val_acc=0.2325 time=30.8s
2025-10-12 12:47:34,273 - INFO - _models.training_function_executor - Epoch 003: train_loss=nan val_loss=nan val_acc=0.2325 time=31.5s
2025-10-12 12:48:05,806 - INFO - _models.training_function_executor - Epoch 004: train_loss=nan val_loss=nan val_acc=0.2325 time=31.5s
2025-10-12 12:48:37,158 - INFO - _models.training_function_executor - Epoch 005: train_loss=nan val_loss=nan val_acc=0.2325 time=31.4s
2025-10-12 12:49:08,147 - INFO - _models.training_function_executor - Epoch 006: train_loss=nan val_loss=nan val_acc=0.2325 time=31.0s
2025-10-12 12:49:39,099 - INFO - _models.training_function_executor - Epoch 007: train_loss=nan val_loss=nan val_acc=0.2325 time=31.0s
2025-10-12 12:50:10,013 - INFO - _models.training_function_executor - Epoch 008: train_loss=nan val_loss=nan val_acc=0.2325 time=30.9s
2025-10-12 12:50:41,135 - INFO - _models.training_function_executor - Epoch 009: train_loss=nan val_loss=nan val_acc=0.2325 time=31.1s
2025-10-12 12:51:11,913 - INFO - _models.training_function_executor - Epoch 010: train_loss=nan val_loss=nan val_acc=0.2325 time=30.8s
2025-10-12 12:51:43,285 - INFO - _models.training_function_executor - Epoch 011: train_loss=nan val_loss=nan val_acc=0.2325 time=31.4s
2025-10-12 12:52:14,120 - INFO - _models.training_function_executor - Epoch 012: train_loss=nan val_loss=nan val_acc=0.2325 time=30.8s
2025-10-12 12:52:44,685 - INFO - _models.training_function_executor - Epoch 013: train_loss=nan val_loss=nan val_acc=0.2325 time=30.6s
2025-10-12 12:53:13,139 - INFO - _models.training_function_executor - Epoch 014: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 12:53:41,587 - INFO - _models.training_function_executor - Epoch 015: train_loss=nan val_loss=nan val_acc=0.2325 time=28.4s
2025-10-12 12:54:09,892 - INFO - _models.training_function_executor - Epoch 016: train_loss=nan val_loss=nan val_acc=0.2325 time=28.3s
2025-10-12 12:54:38,205 - INFO - _models.training_function_executor - Epoch 017: train_loss=nan val_loss=nan val_acc=0.2325 time=28.3s
2025-10-12 12:55:06,543 - INFO - _models.training_function_executor - Epoch 018: train_loss=nan val_loss=nan val_acc=0.2325 time=28.3s
2025-10-12 12:55:35,161 - INFO - _models.training_function_executor - Epoch 019: train_loss=nan val_loss=nan val_acc=0.2325 time=28.6s
2025-10-12 12:56:03,710 - INFO - _models.training_function_executor - Epoch 020: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 12:56:32,178 - INFO - _models.training_function_executor - Epoch 021: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 12:57:00,645 - INFO - _models.training_function_executor - Epoch 022: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 12:57:29,275 - INFO - _models.training_function_executor - Epoch 023: train_loss=nan val_loss=nan val_acc=0.2325 time=28.6s
2025-10-12 12:57:57,685 - INFO - _models.training_function_executor - Epoch 024: train_loss=nan val_loss=nan val_acc=0.2325 time=28.4s
2025-10-12 12:58:26,186 - INFO - _models.training_function_executor - Epoch 025: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 12:58:54,652 - INFO - _models.training_function_executor - Epoch 026: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 12:59:23,093 - INFO - _models.training_function_executor - Epoch 027: train_loss=nan val_loss=nan val_acc=0.2325 time=28.4s
2025-10-12 12:59:51,430 - INFO - _models.training_function_executor - Epoch 028: train_loss=nan val_loss=nan val_acc=0.2325 time=28.3s
2025-10-12 13:00:19,963 - INFO - _models.training_function_executor - Epoch 029: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 13:00:48,231 - INFO - _models.training_function_executor - Epoch 030: train_loss=nan val_loss=nan val_acc=0.2325 time=28.3s
2025-10-12 13:01:16,510 - INFO - _models.training_function_executor - Epoch 031: train_loss=nan val_loss=nan val_acc=0.2325 time=28.3s
2025-10-12 13:01:45,107 - INFO - _models.training_function_executor - Epoch 032: train_loss=nan val_loss=nan val_acc=0.2325 time=28.6s
2025-10-12 13:02:13,524 - INFO - _models.training_function_executor - Epoch 033: train_loss=nan val_loss=nan val_acc=0.2325 time=28.4s
2025-10-12 13:02:41,949 - INFO - _models.training_function_executor - Epoch 034: train_loss=nan val_loss=nan val_acc=0.2325 time=28.4s
2025-10-12 13:03:10,494 - INFO - _models.training_function_executor - Epoch 035: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 13:03:38,922 - INFO - _models.training_function_executor - Epoch 036: train_loss=nan val_loss=nan val_acc=0.2325 time=28.4s
2025-10-12 13:04:07,434 - INFO - _models.training_function_executor - Epoch 037: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 13:04:35,833 - INFO - _models.training_function_executor - Epoch 038: train_loss=nan val_loss=nan val_acc=0.2325 time=28.4s
2025-10-12 13:05:04,276 - INFO - _models.training_function_executor - Epoch 039: train_loss=nan val_loss=nan val_acc=0.2325 time=28.4s
2025-10-12 13:05:32,743 - INFO - _models.training_function_executor - Epoch 040: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 13:06:01,332 - INFO - _models.training_function_executor - Epoch 041: train_loss=nan val_loss=nan val_acc=0.2325 time=28.6s
2025-10-12 13:06:29,974 - INFO - _models.training_function_executor - Epoch 042: train_loss=nan val_loss=nan val_acc=0.2325 time=28.6s
2025-10-12 13:06:58,470 - INFO - _models.training_function_executor - Epoch 043: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 13:07:26,975 - INFO - _models.training_function_executor - Epoch 044: train_loss=nan val_loss=nan val_acc=0.2325 time=28.5s
2025-10-12 13:07:55,536 - INFO - _models.training_function_executor - Epoch 045: train_loss=nan val_loss=nan val_acc=0.2325 time=28.6s
2025-10-12 13:08:24,091 - INFO - _models.training_function_executor - Epoch 046: train_loss=nan val_loss=nan val_acc=0.2325 time=28.6s
2025-10-12 13:08:24,094 - INFO - _models.training_function_executor - Model: 76,872 parameters, 165.2KB storage
2025-10-12 13:08:24,095 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.2173192714441061, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'val_losses': [1.3371482051551697, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'val_acc': [0.41302065103255164, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478, 0.2324991249562478], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0046008085189395585, 'weight_decay': 0.0009921281096808422, 'epochs': 46, 'batch_size': 4, 'dropout': 0.1065463769190467, 'base_channels': 16, 'eca_kernel': 3, 'pool_stride': 4, 'label_smoothing': 0.03665682658659703, 'grad_clip_norm': 2.7925686711090165, 'scheduler_step_size': 9, 'scheduler_gamma': 0.44579979131946623, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 2}, 'model_parameter_count': 76872, 'model_storage_size_kb': 165.15468750000002, 'model_size_validation': 'PASS'}
2025-10-12 13:08:24,095 - INFO - _models.training_function_executor - BO Objective: base=0.2325, size_penalty=0.0000, final=0.2325
2025-10-12 13:08:24,095 - INFO - _models.training_function_executor - Model: 76,872 parameters, 165.2KB (PASS 256KB limit)
2025-10-12 13:08:24,095 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 1343.303s
2025-10-12 13:08:24,200 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2325
2025-10-12 13:08:24,200 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-10-12 13:08:24,201 - INFO - bo.run_bo - Recorded observation #29: hparams={'lr': 0.0046008085189395585, 'weight_decay': 0.0009921281096808422, 'epochs': np.int64(46), 'batch_size': np.int64(4), 'dropout': 0.1065463769190467, 'base_channels': np.int64(16), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.03665682658659703, 'grad_clip_norm': 2.7925686711090165, 'scheduler_step_size': np.int64(9), 'scheduler_gamma': 0.44579979131946623, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(2)}, value=0.2325
2025-10-12 13:08:24,201 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 29: {'lr': 0.0046008085189395585, 'weight_decay': 0.0009921281096808422, 'epochs': np.int64(46), 'batch_size': np.int64(4), 'dropout': 0.1065463769190467, 'base_channels': np.int64(16), 'eca_kernel': np.int64(3), 'pool_stride': np.int64(4), 'label_smoothing': 0.03665682658659703, 'grad_clip_norm': 2.7925686711090165, 'scheduler_step_size': np.int64(9), 'scheduler_gamma': 0.44579979131946623, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(2)} -> 0.2325
2025-10-12 13:08:24,201 - INFO - bo.run_bo - üîçBO Trial 30: Using RF surrogate + Expected Improvement
2025-10-12 13:08:24,201 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 13:08:24,201 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 13:08:24,201 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 13:08:24,201 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0014170576185058507, 'weight_decay': 0.0003505908785346465, 'epochs': 36, 'batch_size': 16, 'dropout': 0.09482716930493115, 'base_channels': 15, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.08611213260829995, 'grad_clip_norm': 2.2977602709701843, 'scheduler_step_size': 7, 'scheduler_gamma': 0.13460551267275914, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 1}
2025-10-12 13:08:24,202 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0014170576185058507, 'weight_decay': 0.0003505908785346465, 'epochs': 36, 'batch_size': 16, 'dropout': 0.09482716930493115, 'base_channels': 15, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.08611213260829995, 'grad_clip_norm': 2.2977602709701843, 'scheduler_step_size': 7, 'scheduler_gamma': 0.13460551267275914, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 1}
2025-10-12 13:08:34,189 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0422 val_loss=1.0339 val_acc=0.6393 time=10.0s
2025-10-12 13:08:43,565 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.8603 val_loss=0.8379 val_acc=0.7493 time=9.4s
2025-10-12 13:08:52,979 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.8154 val_loss=0.9474 val_acc=0.7075 time=9.4s
2025-10-12 13:09:02,356 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.7892 val_loss=0.9124 val_acc=0.7052 time=9.4s
2025-10-12 13:09:11,804 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.7712 val_loss=0.7527 val_acc=0.7921 time=9.4s
2025-10-12 13:09:21,199 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7609 val_loss=0.7530 val_acc=0.7883 time=9.4s
2025-10-12 13:09:30,604 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.7524 val_loss=0.7572 val_acc=0.7843 time=9.4s
2025-10-12 13:09:40,014 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.7087 val_loss=0.7376 val_acc=0.7944 time=9.4s
2025-10-12 13:09:49,367 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.7042 val_loss=0.7225 val_acc=0.8057 time=9.4s
2025-10-12 13:09:58,806 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.6999 val_loss=0.8656 val_acc=0.7269 time=9.4s
2025-10-12 13:10:08,167 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.6960 val_loss=0.7264 val_acc=0.8043 time=9.4s
2025-10-12 13:10:17,522 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.6926 val_loss=0.7166 val_acc=0.8074 time=9.4s
2025-10-12 13:10:26,935 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.6913 val_loss=0.7221 val_acc=0.8036 time=9.4s
2025-10-12 13:10:36,299 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.6892 val_loss=0.7086 val_acc=0.8098 time=9.4s
2025-10-12 13:10:45,693 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.6808 val_loss=0.7272 val_acc=0.8011 time=9.4s
2025-10-12 13:10:55,108 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.6814 val_loss=0.7273 val_acc=0.8001 time=9.4s
2025-10-12 13:11:04,560 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.6801 val_loss=0.7433 val_acc=0.7922 time=9.5s
2025-10-12 13:11:13,987 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.6798 val_loss=0.7270 val_acc=0.8018 time=9.4s
2025-10-12 13:11:23,360 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.6792 val_loss=0.7360 val_acc=0.7974 time=9.4s
2025-10-12 13:11:32,753 - INFO - _models.training_function_executor - Epoch 020: train_loss=0.6804 val_loss=0.7247 val_acc=0.8015 time=9.4s
2025-10-12 13:11:42,155 - INFO - _models.training_function_executor - Epoch 021: train_loss=0.6772 val_loss=0.7288 val_acc=0.8018 time=9.4s
2025-10-12 13:11:51,580 - INFO - _models.training_function_executor - Epoch 022: train_loss=0.6767 val_loss=0.8170 val_acc=0.7540 time=9.4s
2025-10-12 13:12:00,989 - INFO - _models.training_function_executor - Epoch 023: train_loss=0.6763 val_loss=0.7404 val_acc=0.7924 time=9.4s
2025-10-12 13:12:10,328 - INFO - _models.training_function_executor - Epoch 024: train_loss=0.6783 val_loss=0.7272 val_acc=0.8017 time=9.3s
2025-10-12 13:12:19,688 - INFO - _models.training_function_executor - Epoch 025: train_loss=0.6783 val_loss=0.7521 val_acc=0.7875 time=9.4s
2025-10-12 13:12:29,075 - INFO - _models.training_function_executor - Epoch 026: train_loss=0.6754 val_loss=0.7320 val_acc=0.7999 time=9.4s
2025-10-12 13:12:38,422 - INFO - _models.training_function_executor - Epoch 027: train_loss=0.6774 val_loss=0.7313 val_acc=0.8002 time=9.3s
2025-10-12 13:12:47,833 - INFO - _models.training_function_executor - Epoch 028: train_loss=0.6783 val_loss=0.7338 val_acc=0.8021 time=9.4s
2025-10-12 13:12:57,228 - INFO - _models.training_function_executor - Epoch 029: train_loss=0.6756 val_loss=0.7301 val_acc=0.7992 time=9.4s
2025-10-12 13:13:06,625 - INFO - _models.training_function_executor - Epoch 030: train_loss=0.6771 val_loss=0.7280 val_acc=0.7993 time=9.4s
2025-10-12 13:13:15,979 - INFO - _models.training_function_executor - Epoch 031: train_loss=0.6767 val_loss=0.7320 val_acc=0.8000 time=9.4s
2025-10-12 13:13:25,419 - INFO - _models.training_function_executor - Epoch 032: train_loss=0.6779 val_loss=0.7640 val_acc=0.7842 time=9.4s
2025-10-12 13:13:34,837 - INFO - _models.training_function_executor - Epoch 033: train_loss=0.6769 val_loss=0.7337 val_acc=0.7992 time=9.4s
2025-10-12 13:13:44,357 - INFO - _models.training_function_executor - Epoch 034: train_loss=0.6751 val_loss=0.7555 val_acc=0.7854 time=9.5s
2025-10-12 13:13:53,964 - INFO - _models.training_function_executor - Epoch 035: train_loss=0.6770 val_loss=0.7297 val_acc=0.8001 time=9.6s
2025-10-12 13:14:03,490 - INFO - _models.training_function_executor - Epoch 036: train_loss=0.6769 val_loss=0.7305 val_acc=0.7994 time=9.5s
2025-10-12 13:14:03,494 - INFO - _models.training_function_executor - Model: 16,235 parameters, 17.4KB storage
2025-10-12 13:14:03,495 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.042242685981593, 0.8603040942642545, 0.8154097200170196, 0.7892074580788404, 0.7712294069523989, 0.7608698786327294, 0.7523664867998964, 0.7087080634929602, 0.7041650272496397, 0.6998597550504809, 0.6959878083267619, 0.6925539762188515, 0.691322021762683, 0.6892207667758342, 0.6808126556380962, 0.681359982016957, 0.6800705143255414, 0.6798339968467034, 0.6791621197613545, 0.6803510283564286, 0.677207516307467, 0.6766530748718804, 0.6762592947311988, 0.6783193022190043, 0.6782526333181016, 0.6753652061684214, 0.6773555078913709, 0.6783334395561679, 0.6756206108612491, 0.6770784253000921, 0.6766543856524987, 0.6778716064538411, 0.6768786301657083, 0.6751180254561858, 0.6770226206965575, 0.6768820093339], 'val_losses': [1.0338645829660391, 0.8379302216151981, 0.9473611970944216, 0.9123999452232104, 0.7527470252586488, 0.752976903078395, 0.7572448956912157, 0.7375574321608298, 0.7224633504679289, 0.8656429692002331, 0.7263623292436957, 0.716595399721878, 0.7220784223158959, 0.7086227813466346, 0.7272459456125339, 0.7273185440907329, 0.7432974597395489, 0.7269504115762601, 0.7360020924749431, 0.7247341061164168, 0.7288162205283526, 0.8169799050355984, 0.7403906888589602, 0.7271909340094719, 0.7520985173770168, 0.7319683095438313, 0.7313210779359636, 0.7337997314351649, 0.7300616336694997, 0.7279659934005497, 0.7319790924976537, 0.7639629781809477, 0.7336744849482407, 0.7555316871577809, 0.7296988497174283, 0.7304725856433851], 'val_acc': [0.6393069653482674, 0.7492999649982499, 0.7074728736436822, 0.7051977598879944, 0.792089604480224, 0.7883269163458173, 0.7843017150857543, 0.7943647182359118, 0.805652782639132, 0.7268988449422471, 0.8042527126356318, 0.8074028701435072, 0.8035526776338817, 0.8097654882744137, 0.8011025551277564, 0.80014000700035, 0.7921771088554428, 0.8018025901295065, 0.7974273713685684, 0.8015400770038502, 0.8018025901295065, 0.754025201260063, 0.7924396219810991, 0.8017150857542877, 0.7874518725936297, 0.7998774938746938, 0.8002275113755688, 0.8020651032551628, 0.7991774588729437, 0.7992649632481624, 0.7999649982499125, 0.7842142107105355, 0.7991774588729437, 0.7854392719635982, 0.80014000700035, 0.7994399719986], 'model_name': 'CNN-ECA-BiGRU-ISRUC', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0014170576185058507, 'weight_decay': 0.0003505908785346465, 'epochs': 36, 'batch_size': 16, 'dropout': 0.09482716930493115, 'base_channels': 15, 'eca_kernel': 5, 'pool_stride': 5, 'label_smoothing': 0.08611213260829995, 'grad_clip_norm': 2.2977602709701843, 'scheduler_step_size': 7, 'scheduler_gamma': 0.13460551267275914, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calibrate_batches': 1}, 'model_parameter_count': 16235, 'model_storage_size_kb': 17.43994140625, 'model_size_validation': 'PASS'}
2025-10-12 13:14:03,495 - INFO - _models.training_function_executor - BO Objective: base=0.7994, size_penalty=0.0000, final=0.7994
2025-10-12 13:14:03,495 - INFO - _models.training_function_executor - Model: 16,235 parameters, 17.4KB (PASS 256KB limit)
2025-10-12 13:14:03,495 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 339.294s
2025-10-12 13:14:03,601 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7994
2025-10-12 13:14:03,602 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.105s
2025-10-12 13:14:03,602 - INFO - bo.run_bo - Recorded observation #30: hparams={'lr': 0.0014170576185058507, 'weight_decay': 0.0003505908785346465, 'epochs': np.int64(36), 'batch_size': np.int64(16), 'dropout': 0.09482716930493115, 'base_channels': np.int64(15), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.08611213260829995, 'grad_clip_norm': 2.2977602709701843, 'scheduler_step_size': np.int64(7), 'scheduler_gamma': 0.13460551267275914, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(1)}, value=0.7994
2025-10-12 13:14:03,602 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 30: {'lr': 0.0014170576185058507, 'weight_decay': 0.0003505908785346465, 'epochs': np.int64(36), 'batch_size': np.int64(16), 'dropout': 0.09482716930493115, 'base_channels': np.int64(15), 'eca_kernel': np.int64(5), 'pool_stride': np.int64(5), 'label_smoothing': 0.08611213260829995, 'grad_clip_norm': 2.2977602709701843, 'scheduler_step_size': np.int64(7), 'scheduler_gamma': 0.13460551267275914, 'num_workers': np.int64(4), 'use_amp': np.True_, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_, 'calibrate_batches': np.int64(1)} -> 0.7994
2025-10-12 13:14:03,602 - INFO - bo.run_bo - üîçBO Trial 31: Using RF surrogate + Expected Improvement
2025-10-12 13:14:03,602 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-10-12 13:14:03,602 - INFO - _models.training_function_executor - Using device: cuda
2025-10-12 13:14:03,602 - INFO - _models.training_function_executor - Executing training function: CNN-ECA-BiGRU-ISRUC
2025-10-12 13:14:03,602 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00042783847542258606, 'weight_decay': 0.00018453525424897836, 'epochs': 23, 'batch_size': 16, 'dropout': 0.0681526240022259, 'base_channels': 13, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.09273576966530019, 'grad_clip_norm': 3.202156509195424, 'scheduler_step_size': 10, 'scheduler_gamma': 0.34992285646881827, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 3}
2025-10-12 13:14:03,604 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00042783847542258606, 'weight_decay': 0.00018453525424897836, 'epochs': 23, 'batch_size': 16, 'dropout': 0.0681526240022259, 'base_channels': 13, 'eca_kernel': 3, 'pool_stride': 3, 'label_smoothing': 0.09273576966530019, 'grad_clip_norm': 3.202156509195424, 'scheduler_step_size': 10, 'scheduler_gamma': 0.34992285646881827, 'num_workers': 4, 'use_amp': True, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calibrate_batches': 3}
2025-10-12 13:14:13,880 - INFO - _models.training_function_executor - Epoch 001: train_loss=1.0894 val_loss=0.9073 val_acc=0.7226 time=10.3s
2025-10-12 13:14:23,585 - INFO - _models.training_function_executor - Epoch 002: train_loss=0.9039 val_loss=0.8699 val_acc=0.7423 time=9.7s
2025-10-12 13:14:33,349 - INFO - _models.training_function_executor - Epoch 003: train_loss=0.8604 val_loss=0.8460 val_acc=0.7495 time=9.8s
2025-10-12 13:14:43,038 - INFO - _models.training_function_executor - Epoch 004: train_loss=0.8354 val_loss=0.8143 val_acc=0.7685 time=9.7s
2025-10-12 13:14:52,709 - INFO - _models.training_function_executor - Epoch 005: train_loss=0.8154 val_loss=0.9152 val_acc=0.7136 time=9.7s
2025-10-12 13:15:02,396 - INFO - _models.training_function_executor - Epoch 006: train_loss=0.7938 val_loss=0.8253 val_acc=0.7574 time=9.7s
2025-10-12 13:15:12,110 - INFO - _models.training_function_executor - Epoch 007: train_loss=0.7856 val_loss=0.8838 val_acc=0.7358 time=9.7s
2025-10-12 13:15:21,825 - INFO - _models.training_function_executor - Epoch 008: train_loss=0.7722 val_loss=0.7585 val_acc=0.7986 time=9.7s
2025-10-12 13:15:31,480 - INFO - _models.training_function_executor - Epoch 009: train_loss=0.7667 val_loss=0.7568 val_acc=0.7984 time=9.7s
2025-10-12 13:15:41,174 - INFO - _models.training_function_executor - Epoch 010: train_loss=0.7566 val_loss=0.7925 val_acc=0.7772 time=9.7s
2025-10-12 13:15:50,846 - INFO - _models.training_function_executor - Epoch 011: train_loss=0.7293 val_loss=0.7651 val_acc=0.7898 time=9.7s
2025-10-12 13:16:00,558 - INFO - _models.training_function_executor - Epoch 012: train_loss=0.7279 val_loss=0.8043 val_acc=0.7744 time=9.7s
2025-10-12 13:16:10,274 - INFO - _models.training_function_executor - Epoch 013: train_loss=0.7254 val_loss=0.7882 val_acc=0.7828 time=9.7s
2025-10-12 13:16:19,997 - INFO - _models.training_function_executor - Epoch 014: train_loss=0.7202 val_loss=0.7476 val_acc=0.7994 time=9.7s
2025-10-12 13:16:29,669 - INFO - _models.training_function_executor - Epoch 015: train_loss=0.7219 val_loss=0.8191 val_acc=0.7665 time=9.7s
2025-10-12 13:16:39,358 - INFO - _models.training_function_executor - Epoch 016: train_loss=0.7177 val_loss=0.8289 val_acc=0.7693 time=9.7s
2025-10-12 13:16:49,038 - INFO - _models.training_function_executor - Epoch 017: train_loss=0.7155 val_loss=0.7943 val_acc=0.7831 time=9.7s
2025-10-12 13:16:58,801 - INFO - _models.training_function_executor - Epoch 018: train_loss=0.7139 val_loss=0.8930 val_acc=0.7379 time=9.8s
2025-10-12 13:17:08,653 - INFO - _models.training_function_executor - Epoch 019: train_loss=0.7121 val_loss=0.7861 val_acc=0.7804 time=9.9s
