2025-09-22 18:39:38,904 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 18:39:39,011 - INFO - __main__ - Logging system initialized successfully
2025-09-22 18:39:39,011 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-22 18:39:39,011 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-22 18:39:39,012 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-22 18:39:39,012 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-22 18:39:39,012 - INFO - __main__ - Attempting to load: X.npy
2025-09-22 18:39:39,054 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-22 18:39:39,092 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-22 18:39:39,093 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-22 18:39:39,093 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-22 18:39:39,093 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-22 18:39:39,095 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-22 18:39:39,095 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-22 18:39:39,095 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-22 18:39:39,095 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-22 18:39:39,095 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-22 18:39:39,095 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-22 18:39:39,095 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-22 18:39:39,095 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-22 18:39:39,095 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-22 18:39:39,196 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-22 18:39:39,196 - INFO - class_balancing - Class imbalance analysis:
2025-09-22 18:39:39,196 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-22 18:39:39,196 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-22 18:39:39,196 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-22 18:39:39,196 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-22 18:39:39,196 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-22 18:39:39,196 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-22 18:39:39,197 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-22 18:39:39,197 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-22 18:39:39,367 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-22 18:39:39,368 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-22 18:39:39,368 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-22 18:39:39,368 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-22 18:39:39,368 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-22 18:39:39,368 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-22 18:39:39,368 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-22 18:43:15,888 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:43:16,102 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 18:43:16,102 - INFO - _models.ai_code_generator - Prompt length: 3314 characters
2025-09-22 18:43:16,102 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 18:43:16,102 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 18:43:16,102 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 18:45:26,021 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:45:26,022 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 18:45:26,022 - INFO - _models.ai_code_generator - AI generated training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:26,022 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-09-22 18:45:26,022 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.74)
2025-09-22 18:45:26,022 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:26,022 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'd_model', 'n_heads', 'n_layers', 'patch_size', 'stem_channels', 'ffn_mult', 'gamma', 'grad_clip_norm', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'use_rr']
2025-09-22 18:45:26,022 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-09-22 18:45:26,023 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-22 18:45:26,023 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions/training_function_torch_tensor_TinyConvTransformer-ECG__‚â§256KB_1758584726.json
2025-09-22 18:45:26,023 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions/training_function_torch_tensor_TinyConvTransformer-ECG__‚â§256KB_1758584726.json
2025-09-22 18:45:26,023 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-22 18:45:26,023 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:26,023 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 18:45:26,024 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 18:45:26,026 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-22 18:45:26,026 - INFO - package_installer - Available packages: {'torch'}
2025-09-22 18:45:26,026 - INFO - package_installer - Missing packages: set()
2025-09-22 18:45:26,026 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-22 18:45:26,026 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 18:45:26,026 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:45:26,026 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 18:45:26,026 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'd_model', 'n_heads', 'n_layers', 'patch_size', 'stem_channels', 'ffn_mult', 'gamma', 'grad_clip_norm', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'use_rr']
2025-09-22 18:45:26,026 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 18:45:26,026 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:45:26,026 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 18:45:26,061 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 18:45:26,190 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-22 18:45:26,190 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 18:45:26,190 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 18:45:26,191 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 18:45:26,191 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 18:45:26,191 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 18:45:26,191 - INFO - _models.training_function_executor - Executing training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:26,191 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:45:26,192 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:45:26,193 - ERROR - _models.training_function_executor - Training execution failed: Sequence length must be divisible by patch_size
2025-09-22 18:45:26,193 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 18:45:26,193 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 18:45:26,193 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 18:45:26,193 - INFO - _models.ai_code_generator - Prompt length: 19699 characters
2025-09-22 18:45:26,193 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 18:45:26,193 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 18:45:26,193 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 18:45:35,953 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:45:35,954 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 18:45:35,954 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250922_184535_attempt1.txt
2025-09-22 18:45:35,954 - INFO - _models.ai_code_generator - GPT suggested correction: {"bo_config": {"patch_size": 20}}
2025-09-22 18:45:35,954 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-22 18:45:35,954 - INFO - _models.training_function_executor - GPT suggested corrections: {"bo_config": {"patch_size": 20}}
2025-09-22 18:45:35,954 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-22 18:45:35,954 - ERROR - _models.training_function_executor - BO training objective failed: Sequence length must be divisible by patch_size
2025-09-22 18:45:35,954 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 9.763s
2025-09-22 18:45:35,954 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: Sequence length must be divisible by patch_size
2025-09-22 18:45:35,954 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-22 18:45:38,957 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-22 18:45:38,957 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß GPT has fixed the training function, reloading from JSON file
2025-09-22 18:45:38,957 - INFO - evaluation.code_generation_pipeline_orchestrator - Reloading fixed training function from: generated_training_functions/training_function_torch_tensor_TinyConvTransformer-ECG__‚â§256KB_1758584726.json
2025-09-22 18:45:38,957 - INFO - _models.training_function_executor - Loaded training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:38,957 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-22 18:45:38,957 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:38,957 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-22 18:45:38,958 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 1/4
2025-09-22 18:45:38,958 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 1: üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 18:45:38,958 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 18:45:38,960 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-22 18:45:38,960 - INFO - package_installer - Available packages: {'torch'}
2025-09-22 18:45:38,960 - INFO - package_installer - Missing packages: set()
2025-09-22 18:45:38,960 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-22 18:45:38,960 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 18:45:38,960 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:45:38,960 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 18:45:38,960 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'd_model', 'n_heads', 'n_layers', 'patch_size', 'stem_channels', 'ffn_mult', 'gamma', 'grad_clip_norm', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'use_rr']
2025-09-22 18:45:38,960 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 18:45:38,960 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:45:38,960 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 18:45:38,996 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 18:45:39,019 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-22 18:45:39,020 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 18:45:39,020 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 18:45:39,021 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 18:45:39,021 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 18:45:39,021 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 18:45:39,021 - INFO - _models.training_function_executor - Executing training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:39,021 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:45:39,022 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:45:39,023 - ERROR - _models.training_function_executor - Training execution failed: Sequence length must be divisible by patch_size
2025-09-22 18:45:39,023 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 18:45:39,023 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 18:45:39,023 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 18:45:39,023 - INFO - _models.ai_code_generator - Prompt length: 19699 characters
2025-09-22 18:45:39,023 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 18:45:39,023 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 18:45:39,023 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 18:45:50,596 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:45:50,597 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 18:45:50,597 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250922_184550_attempt1.txt
2025-09-22 18:45:50,597 - INFO - _models.ai_code_generator - GPT suggested correction: {"bo_config": {"patch_size": 20}}
2025-09-22 18:45:50,597 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-22 18:45:50,597 - INFO - _models.training_function_executor - GPT suggested corrections: {"bo_config": {"patch_size": 20}}
2025-09-22 18:45:50,597 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-22 18:45:50,597 - ERROR - _models.training_function_executor - BO training objective failed: Sequence length must be divisible by patch_size
2025-09-22 18:45:50,597 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 11.576s
2025-09-22 18:45:50,597 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: Sequence length must be divisible by patch_size
2025-09-22 18:45:50,597 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-22 18:45:53,600 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-22 18:45:53,600 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß GPT has fixed the training function, reloading from JSON file
2025-09-22 18:45:53,600 - INFO - evaluation.code_generation_pipeline_orchestrator - Reloading fixed training function from: generated_training_functions/training_function_torch_tensor_TinyConvTransformer-ECG__‚â§256KB_1758584726.json
2025-09-22 18:45:53,600 - INFO - _models.training_function_executor - Loaded training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:53,601 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-22 18:45:53,601 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:53,601 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-22 18:45:53,601 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 2/4
2025-09-22 18:45:53,601 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 2: üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 18:45:53,601 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 18:45:53,603 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-22 18:45:53,603 - INFO - package_installer - Available packages: {'torch'}
2025-09-22 18:45:53,603 - INFO - package_installer - Missing packages: set()
2025-09-22 18:45:53,603 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-22 18:45:53,603 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 18:45:53,603 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:45:53,603 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 18:45:53,603 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'd_model', 'n_heads', 'n_layers', 'patch_size', 'stem_channels', 'ffn_mult', 'gamma', 'grad_clip_norm', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'use_rr']
2025-09-22 18:45:53,603 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 18:45:53,603 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:45:53,603 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 18:45:53,639 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 18:45:53,662 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-22 18:45:53,662 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 18:45:53,663 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 18:45:53,663 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 18:45:53,663 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 18:45:53,663 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 18:45:53,663 - INFO - _models.training_function_executor - Executing training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:45:53,664 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:45:53,665 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:45:53,665 - ERROR - _models.training_function_executor - Training execution failed: Sequence length must be divisible by patch_size
2025-09-22 18:45:53,665 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 18:45:53,665 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 18:45:53,665 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 18:45:53,666 - INFO - _models.ai_code_generator - Prompt length: 19699 characters
2025-09-22 18:45:53,666 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 18:45:53,666 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 18:45:53,666 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 18:46:07,800 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:46:07,801 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 18:46:07,802 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250922_184607_attempt1.txt
2025-09-22 18:46:07,802 - INFO - _models.ai_code_generator - GPT suggested correction: {"bo_config": {"patch_size": 20}}
2025-09-22 18:46:07,802 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-22 18:46:07,802 - INFO - _models.training_function_executor - GPT suggested corrections: {"bo_config": {"patch_size": 20}}
2025-09-22 18:46:07,802 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-22 18:46:07,802 - ERROR - _models.training_function_executor - BO training objective failed: Sequence length must be divisible by patch_size
2025-09-22 18:46:07,802 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 14.138s
2025-09-22 18:46:07,802 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: Sequence length must be divisible by patch_size
2025-09-22 18:46:07,802 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-22 18:46:10,805 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-22 18:46:10,805 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß GPT has fixed the training function, reloading from JSON file
2025-09-22 18:46:10,805 - INFO - evaluation.code_generation_pipeline_orchestrator - Reloading fixed training function from: generated_training_functions/training_function_torch_tensor_TinyConvTransformer-ECG__‚â§256KB_1758584726.json
2025-09-22 18:46:10,806 - INFO - _models.training_function_executor - Loaded training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:46:10,806 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-22 18:46:10,806 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:46:10,806 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-22 18:46:10,806 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 3/4
2025-09-22 18:46:10,806 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 3: üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 18:46:10,806 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 18:46:10,808 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-22 18:46:10,808 - INFO - package_installer - Available packages: {'torch'}
2025-09-22 18:46:10,808 - INFO - package_installer - Missing packages: set()
2025-09-22 18:46:10,808 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-22 18:46:10,808 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 18:46:10,808 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:46:10,808 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 18:46:10,808 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'd_model', 'n_heads', 'n_layers', 'patch_size', 'stem_channels', 'ffn_mult', 'gamma', 'grad_clip_norm', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'use_rr']
2025-09-22 18:46:10,808 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 18:46:10,808 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:46:10,808 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 18:46:10,844 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 18:46:10,874 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-22 18:46:10,874 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 18:46:10,874 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 18:46:10,875 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 18:46:10,875 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-22 18:46:10,875 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 18:46:10,875 - INFO - _models.training_function_executor - Executing training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:46:10,875 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:46:10,876 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:46:10,876 - ERROR - _models.training_function_executor - Training execution failed: Sequence length must be divisible by patch_size
2025-09-22 18:46:10,876 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 18:46:10,877 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 18:46:10,877 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 18:46:10,877 - INFO - _models.ai_code_generator - Prompt length: 19699 characters
2025-09-22 18:46:10,877 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 18:46:10,877 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 18:46:10,877 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 18:46:28,894 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:46:28,895 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 18:46:28,895 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250922_184628_attempt1.txt
2025-09-22 18:46:28,895 - INFO - _models.ai_code_generator - GPT suggested correction: {"bo_config": {"patch_size": 20}}
2025-09-22 18:46:28,895 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-22 18:46:28,895 - INFO - _models.training_function_executor - GPT suggested corrections: {"bo_config": {"patch_size": 20}}
2025-09-22 18:46:28,895 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-22 18:46:28,895 - ERROR - _models.training_function_executor - BO training objective failed: Sequence length must be divisible by patch_size
2025-09-22 18:46:28,895 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 18.020s
2025-09-22 18:46:28,895 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: Sequence length must be divisible by patch_size
2025-09-22 18:46:28,895 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-22 18:46:31,897 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-22 18:46:31,897 - INFO - evaluation.code_generation_pipeline_orchestrator - üîß GPT has fixed the training function, reloading from JSON file
2025-09-22 18:46:31,898 - INFO - evaluation.code_generation_pipeline_orchestrator - Reloading fixed training function from: generated_training_functions/training_function_torch_tensor_TinyConvTransformer-ECG__‚â§256KB_1758584726.json
2025-09-22 18:46:31,898 - INFO - _models.training_function_executor - Loaded training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:46:31,898 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-22 18:46:31,898 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Reloaded fixed training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:46:31,898 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ Applied GPT fixes, restarting BO from trial 0
2025-09-22 18:46:31,898 - INFO - evaluation.code_generation_pipeline_orchestrator - üîÑ BO Restart attempt 4/4
2025-09-22 18:46:31,898 - INFO - evaluation.code_generation_pipeline_orchestrator - Session 4: üì¶ Installing dependencies for GPT-generated training code...
2025-09-22 18:46:31,898 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-22 18:46:31,901 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-22 18:46:31,901 - INFO - package_installer - Available packages: {'torch'}
2025-09-22 18:46:31,901 - INFO - package_installer - Missing packages: set()
2025-09-22 18:46:31,901 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-22 18:46:31,901 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-22 18:46:31,901 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:46:31,901 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 39904 samples (using bo_sample_num=70000)
2025-09-22 18:46:31,901 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'd_model', 'n_heads', 'n_layers', 'patch_size', 'stem_channels', 'ffn_mult', 'gamma', 'grad_clip_norm', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'use_rr']
2025-09-22 18:46:31,901 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti
2025-09-22 18:46:31,901 - INFO - data_splitting - Using all 39904 training samples for BO
2025-09-22 18:46:31,901 - INFO - _models.training_function_executor - Using BO subset for optimization: 39904 samples (bo_sample_num=70000)
2025-09-22 18:46:31,937 - INFO - _models.training_function_executor - BO splits - Train: 31923, Val: 7981
2025-09-22 18:46:31,970 - INFO - bo.run_bo - Converted GPT search space: 18 parameters
2025-09-22 18:46:31,970 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-22 18:46:31,971 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-22 18:46:31,971 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-22 18:46:31,971 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-22 18:46:31,971 - INFO - _models.training_function_executor - Using device: cuda
2025-09-22 18:46:31,971 - INFO - _models.training_function_executor - Executing training function: TinyConvTransformer-ECG (‚â§256KB)
2025-09-22 18:46:31,971 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:46:31,973 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001412035543062636, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.046805592132730965, 'd_model': 18, 'n_heads': 2, 'n_layers': 1, 'patch_size': 19, 'stem_channels': 8, 'ffn_mult': 4, 'gamma': 1.041168988591605, 'grad_clip_norm': 4.852558275593773, 'seed': 718315, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False, 'use_rr': False}
2025-09-22 18:46:31,973 - ERROR - _models.training_function_executor - Training execution failed: Sequence length must be divisible by patch_size
2025-09-22 18:46:31,973 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-22 18:46:31,973 - INFO - _models.ai_code_generator - Calling GPT to debug training error (attempt 1/4)
2025-09-22 18:46:31,973 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-22 18:46:31,973 - INFO - _models.ai_code_generator - Prompt length: 19699 characters
2025-09-22 18:46:31,973 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-22 18:46:31,973 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-22 18:46:31,973 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-22 18:46:44,972 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-22 18:46:44,973 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-22 18:46:44,973 - INFO - _models.ai_code_generator - Saved GPT debug response to: gpt_debug_responses/gpt_debug_training_error_20250922_184644_attempt1.txt
2025-09-22 18:46:44,973 - INFO - _models.ai_code_generator - GPT suggested correction: {"bo_config": {"patch_size": 20}}
2025-09-22 18:46:44,973 - INFO - _models.ai_code_generator - Debug successful on attempt 1
2025-09-22 18:46:44,973 - INFO - _models.training_function_executor - GPT suggested corrections: {"bo_config": {"patch_size": 20}}
2025-09-22 18:46:44,973 - INFO - _models.training_function_executor - Stored corrections for BO process
2025-09-22 18:46:44,973 - ERROR - _models.training_function_executor - BO training objective failed: Sequence length must be divisible by patch_size
2025-09-22 18:46:44,973 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 13.002s
2025-09-22 18:46:44,973 - ERROR - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 FAILED with error: Sequence length must be divisible by patch_size
2025-09-22 18:46:44,973 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚è≥ Waiting for GPT to finish debugging...
2025-09-22 18:46:47,976 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ GPT provided fixes after 3s - requesting BO restart
2025-09-22 18:46:47,977 - ERROR - __main__ - Unhandled exception: Exception: GPT_FIXES_AVAILABLE
Traceback (most recent call last):
  File "/home/shiyuanduan/Documents/Projects/GPTLAB/IMUFigureSets/main.py", line 484, in <module>
    processed_real_data = process_real_data()
  File "/home/shiyuanduan/Documents/Projects/GPTLAB/IMUFigureSets/main.py", line 342, in process_real_data
    result = process_data_with_ai_enhanced_evaluation(
  File "/home/shiyuanduan/Documents/Projects/GPTLAB/IMUFigureSets/main.py", line 136, in process_data_with_ai_enhanced_evaluation
    result = train_with_iterative_selection(data, labels, device=device, **kwargs)
  File "/home/shiyuanduan/Documents/Projects/GPTLAB/IMUFigureSets/main.py", line 85, in train_with_iterative_selection
    best_model, pipeline_results = orchestrator.run_complete_pipeline(
  File "/home/shiyuanduan/Documents/Projects/GPTLAB/IMUFigureSets/evaluation/code_generation_pipeline_orchestrator.py", line 90, in run_complete_pipeline
    bo_results = self._run_bayesian_optimization(X, y, device, code_rec)
  File "/home/shiyuanduan/Documents/Projects/GPTLAB/IMUFigureSets/evaluation/code_generation_pipeline_orchestrator.py", line 189, in _run_bayesian_optimization
    return self._run_single_bo_session(X, y, device, code_rec, restart_attempt)
  File "/home/shiyuanduan/Documents/Projects/GPTLAB/IMUFigureSets/evaluation/code_generation_pipeline_orchestrator.py", line 296, in _run_single_bo_session
    raise Exception("GPT_FIXES_AVAILABLE")
Exception: GPT_FIXES_AVAILABLE

