2025-09-19 10:42:49,882 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-19 10:42:50,201 - INFO - __main__ - Logging system initialized successfully
2025-09-19 10:42:50,202 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-19 10:42:50,202 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-19 10:42:50,203 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-19 10:42:50,203 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-19 10:42:50,204 - INFO - __main__ - Attempting to load: X.npy
2025-09-19 10:42:50,327 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-19 10:42:50,413 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-19 10:42:50,413 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-19 10:42:50,413 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-19 10:42:50,413 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-19 10:42:50,416 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-19 10:42:50,416 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-19 10:42:50,416 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-19 10:42:50,416 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-19 10:42:50,416 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-19 10:42:50,416 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-19 10:42:50,416 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-19 10:42:50,416 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-19 10:42:50,416 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-19 10:42:50,622 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-19 10:42:50,622 - INFO - class_balancing - Class imbalance analysis:
2025-09-19 10:42:50,622 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-19 10:42:50,622 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-19 10:42:50,622 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-19 10:42:50,622 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-19 10:42:50,623 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-19 10:42:50,623 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-19 10:42:50,623 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-19 10:42:50,623 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-19 10:42:51,319 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-19 10:42:51,327 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-19 10:42:51,327 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-19 10:42:51,327 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-19 10:42:51,327 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-19 10:42:51,327 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-19 10:42:51,327 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-19 10:46:59,467 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-19 10:46:59,581 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-19 10:46:59,581 - INFO - _models.ai_code_generator - Prompt length: 3099 characters
2025-09-19 10:46:59,581 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-19 10:46:59,581 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-19 10:46:59,581 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-19 10:51:56,245 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-19 10:51:56,294 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-19 10:51:56,294 - INFO - _models.ai_code_generator - AI generated training function: ECGTransMini-MSCNN-BiT-PTQ
2025-09-19 10:51:56,294 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-09-19 10:51:56,294 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.73)
2025-09-19 10:51:56,294 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: ECGTransMini-MSCNN-BiT-PTQ
2025-09-19 10:51:56,294 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'd_model', 'nhead', 'num_layers', 'dim_feedforward', 'conv_channels', 'se_ratio', 'downsample_stride', 'focal_gamma', 'label_smoothing', 'class_weight_mode', 'max_grad_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calib_batches']
2025-09-19 10:51:56,294 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-09-19 10:51:56,295 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-19 10:51:56,298 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions\training_function_torch_tensor_ECGTransMini-MSCNN-BiT-PTQ_1758297116.json
2025-09-19 10:51:56,298 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions\training_function_torch_tensor_ECGTransMini-MSCNN-BiT-PTQ_1758297116.json
2025-09-19 10:51:56,301 - INFO - _models.training_function_executor - Training function validation passed
2025-09-19 10:51:56,301 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-19 10:51:56,301 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: ECGTransMini-MSCNN-BiT-PTQ
2025-09-19 10:51:56,301 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-19 10:51:56,302 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-19 10:51:56,307 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-19 10:51:56,308 - INFO - package_installer - Available packages: {'torch'}
2025-09-19 10:51:56,308 - INFO - package_installer - Missing packages: set()
2025-09-19 10:51:56,308 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-19 10:51:56,308 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-19 10:51:56,308 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 62352 samples (using full dataset)
2025-09-19 10:51:56,308 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'd_model', 'nhead', 'num_layers', 'dim_feedforward', 'conv_channels', 'se_ratio', 'downsample_stride', 'focal_gamma', 'label_smoothing', 'class_weight_mode', 'max_grad_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations', 'calib_batches']
2025-09-19 10:51:56,308 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-19 10:51:56,308 - INFO - _models.training_function_executor - Using centralized data splits for BO objective
2025-09-19 10:51:56,634 - INFO - bo.run_bo - Converted GPT search space: 20 parameters
2025-09-19 10:51:56,635 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-19 10:51:56,635 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-19 10:51:56,637 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-19 10:51:56,637 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 10:51:56,637 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 10:51:56,637 - INFO - _models.training_function_executor - Executing training function: ECGTransMini-MSCNN-BiT-PTQ
2025-09-19 10:51:56,637 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'd_model': 114, 'nhead': 7, 'num_layers': 1, 'dim_feedforward': 151, 'conv_channels': 28, 'se_ratio': 7, 'downsample_stride': 4, 'focal_gamma': 1.041168988591605, 'label_smoothing': 0.09699098521619945, 'class_weight_mode': 'none', 'max_grad_norm': 1.061695553391381, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 31}
2025-09-19 10:51:56,640 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0008838704519727185, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'd_model': 114, 'nhead': 7, 'num_layers': 1, 'dim_feedforward': 151, 'conv_channels': 28, 'se_ratio': 7, 'downsample_stride': 4, 'focal_gamma': 1.041168988591605, 'label_smoothing': 0.09699098521619945, 'class_weight_mode': 'none', 'max_grad_norm': 1.061695553391381, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': 31}
2025-09-19 10:54:35,683 - ERROR - _models.training_function_executor - Training execution failed: empty_strided not supported on quantized tensors yet see https://github.com/pytorch/pytorch/issues/74540
2025-09-19 10:54:35,683 - ERROR - _models.training_function_executor - Training code: import copy
import math
from typing import Dict, Any

def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):
    import torch
    import torch.nn as nn
    import torch.nn.functional ...
2025-09-19 10:54:35,683 - ERROR - _models.training_function_executor - BO training objective failed: empty_strided not supported on quantized tensors yet see https://github.com/pytorch/pytorch/issues/74540
2025-09-19 10:54:35,683 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 159.046s
2025-09-19 10:54:35,684 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 10:54:35,684 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.001s
2025-09-19 10:54:35,684 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.0008838704519727185, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'd_model': np.int64(114), 'nhead': np.int64(7), 'num_layers': np.int64(1), 'dim_feedforward': np.int64(151), 'conv_channels': np.int64(28), 'se_ratio': np.int64(7), 'downsample_stride': np.int64(4), 'focal_gamma': 1.041168988591605, 'label_smoothing': 0.09699098521619945, 'class_weight_mode': 'none', 'max_grad_norm': 1.061695553391381, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': np.int64(31)}, value=0.0000
2025-09-19 10:54:35,684 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.0008838704519727185, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'dropout': 0.07800932022121827, 'd_model': np.int64(114), 'nhead': np.int64(7), 'num_layers': np.int64(1), 'dim_feedforward': np.int64(151), 'conv_channels': np.int64(28), 'se_ratio': np.int64(7), 'downsample_stride': np.int64(4), 'focal_gamma': 1.041168988591605, 'label_smoothing': 0.09699098521619945, 'class_weight_mode': 'none', 'max_grad_norm': 1.061695553391381, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True, 'calib_batches': np.int64(31)} -> 0.0000
2025-09-19 10:54:35,685 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-09-19 10:54:35,686 - INFO - bo.run_bo - [PROFILE] suggest() took 0.002s
2025-09-19 10:54:35,686 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 10:54:35,686 - INFO - _models.training_function_executor - Executing training function: ECGTransMini-MSCNN-BiT-PTQ
2025-09-19 10:54:35,686 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.0620330965627432e-06, 'batch_size': 32, 'epochs': 31, 'weight_decay': 0.0002801635158716261, 'dropout': 0.06974693032602093, 'd_model': 123, 'nhead': 4, 'num_layers': 2, 'dim_feedforward': 78, 'conv_channels': 22, 'se_ratio': 6, 'downsample_stride': 5, 'focal_gamma': 2.0284688768272234, 'label_smoothing': 0.05924145688620426, 'class_weight_mode': 'inverse', 'max_grad_norm': 3.0377242595071925, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 69}
2025-09-19 10:54:35,689 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.0620330965627432e-06, 'batch_size': 32, 'epochs': 31, 'weight_decay': 0.0002801635158716261, 'dropout': 0.06974693032602093, 'd_model': 123, 'nhead': 4, 'num_layers': 2, 'dim_feedforward': 78, 'conv_channels': 22, 'se_ratio': 6, 'downsample_stride': 5, 'focal_gamma': 2.0284688768272234, 'label_smoothing': 0.05924145688620426, 'class_weight_mode': 'inverse', 'max_grad_norm': 3.0377242595071925, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': 69}
2025-09-19 10:54:35,711 - ERROR - _models.training_function_executor - Training execution failed: The expanded size of the tensor (61) must match the existing size (62) at non-singleton dimension 1.  Target sizes: [2000, 61].  Tensor sizes: [2000, 62]
2025-09-19 10:54:35,711 - ERROR - _models.training_function_executor - Training code: import copy
import math
from typing import Dict, Any

def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):
    import torch
    import torch.nn as nn
    import torch.nn.functional ...
2025-09-19 10:54:35,711 - ERROR - _models.training_function_executor - BO training objective failed: The expanded size of the tensor (61) must match the existing size (62) at non-singleton dimension 1.  Target sizes: [2000, 61].  Tensor sizes: [2000, 62]
2025-09-19 10:54:35,711 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.025s
2025-09-19 10:54:35,711 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 10:54:35,711 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-19 10:54:35,712 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 1.0620330965627432e-06, 'batch_size': 32, 'epochs': np.int64(31), 'weight_decay': 0.0002801635158716261, 'dropout': 0.06974693032602093, 'd_model': np.int64(123), 'nhead': np.int64(4), 'num_layers': np.int64(2), 'dim_feedforward': np.int64(78), 'conv_channels': np.int64(22), 'se_ratio': np.int64(6), 'downsample_stride': np.int64(5), 'focal_gamma': 2.0284688768272234, 'label_smoothing': 0.05924145688620426, 'class_weight_mode': 'inverse', 'max_grad_norm': 3.0377242595071925, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': np.int64(69)}, value=0.0000
2025-09-19 10:54:35,712 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 1.0620330965627432e-06, 'batch_size': 32, 'epochs': np.int64(31), 'weight_decay': 0.0002801635158716261, 'dropout': 0.06974693032602093, 'd_model': np.int64(123), 'nhead': np.int64(4), 'num_layers': np.int64(2), 'dim_feedforward': np.int64(78), 'conv_channels': np.int64(22), 'se_ratio': np.int64(6), 'downsample_stride': np.int64(5), 'focal_gamma': 2.0284688768272234, 'label_smoothing': 0.05924145688620426, 'class_weight_mode': 'inverse', 'max_grad_norm': 3.0377242595071925, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False, 'calib_batches': np.int64(69)} -> 0.0000
2025-09-19 10:54:35,713 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-09-19 10:54:35,713 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 10:54:35,714 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 10:54:35,714 - INFO - _models.training_function_executor - Executing training function: ECGTransMini-MSCNN-BiT-PTQ
2025-09-19 10:54:35,714 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00012122333327045812, 'batch_size': 64, 'epochs': 30, 'weight_decay': 2.4586032763280047e-06, 'dropout': 0.3421165132560785, 'd_model': 91, 'nhead': 7, 'num_layers': 2, 'dim_feedforward': 71, 'conv_channels': 22, 'se_ratio': 6, 'downsample_stride': 7, 'focal_gamma': 1.7821212151464818, 'label_smoothing': 0.018223608778806237, 'class_weight_mode': 'none', 'max_grad_norm': 2.1257793724562237, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 155}
2025-09-19 10:54:35,717 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00012122333327045812, 'batch_size': 64, 'epochs': 30, 'weight_decay': 2.4586032763280047e-06, 'dropout': 0.3421165132560785, 'd_model': 91, 'nhead': 7, 'num_layers': 2, 'dim_feedforward': 71, 'conv_channels': 22, 'se_ratio': 6, 'downsample_stride': 7, 'focal_gamma': 1.7821212151464818, 'label_smoothing': 0.018223608778806237, 'class_weight_mode': 'none', 'max_grad_norm': 2.1257793724562237, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 155}
2025-09-19 10:54:35,723 - ERROR - _models.training_function_executor - Training execution failed: The expanded size of the tensor (45) must match the existing size (46) at non-singleton dimension 1.  Target sizes: [2000, 45].  Tensor sizes: [2000, 46]
2025-09-19 10:54:35,723 - ERROR - _models.training_function_executor - Training code: import copy
import math
from typing import Dict, Any

def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):
    import torch
    import torch.nn as nn
    import torch.nn.functional ...
2025-09-19 10:54:35,723 - ERROR - _models.training_function_executor - BO training objective failed: The expanded size of the tensor (45) must match the existing size (46) at non-singleton dimension 1.  Target sizes: [2000, 45].  Tensor sizes: [2000, 46]
2025-09-19 10:54:35,723 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.009s
2025-09-19 10:54:35,955 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 10:54:35,955 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.232s
2025-09-19 10:54:35,955 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.00012122333327045812, 'batch_size': 64, 'epochs': np.int64(30), 'weight_decay': 2.4586032763280047e-06, 'dropout': 0.3421165132560785, 'd_model': np.int64(91), 'nhead': np.int64(7), 'num_layers': np.int64(2), 'dim_feedforward': np.int64(71), 'conv_channels': np.int64(22), 'se_ratio': np.int64(6), 'downsample_stride': np.int64(7), 'focal_gamma': 1.7821212151464818, 'label_smoothing': 0.018223608778806237, 'class_weight_mode': 'none', 'max_grad_norm': 2.1257793724562237, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': np.int64(155)}, value=0.0000
2025-09-19 10:54:35,955 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.00012122333327045812, 'batch_size': 64, 'epochs': np.int64(30), 'weight_decay': 2.4586032763280047e-06, 'dropout': 0.3421165132560785, 'd_model': np.int64(91), 'nhead': np.int64(7), 'num_layers': np.int64(2), 'dim_feedforward': np.int64(71), 'conv_channels': np.int64(22), 'se_ratio': np.int64(6), 'downsample_stride': np.int64(7), 'focal_gamma': 1.7821212151464818, 'label_smoothing': 0.018223608778806237, 'class_weight_mode': 'none', 'max_grad_norm': 2.1257793724562237, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': np.int64(155)} -> 0.0000
2025-09-19 10:54:35,955 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-09-19 10:54:35,955 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 10:54:35,955 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 10:54:35,955 - INFO - _models.training_function_executor - Executing training function: ECGTransMini-MSCNN-BiT-PTQ
2025-09-19 10:54:35,955 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0007365344466688374, 'batch_size': 64, 'epochs': 8, 'weight_decay': 0.0003397954840289545, 'dropout': 0.3971142002927063, 'd_model': 35, 'nhead': 5, 'num_layers': 1, 'dim_feedforward': 202, 'conv_channels': 14, 'se_ratio': 15, 'downsample_stride': 6, 'focal_gamma': 2.260668222934651, 'label_smoothing': 0.052804742239333624, 'class_weight_mode': np.str_('balanced'), 'max_grad_norm': 1.1835949467344977, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 133}
2025-09-19 10:54:35,957 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0007365344466688374, 'batch_size': 64, 'epochs': 8, 'weight_decay': 0.0003397954840289545, 'dropout': 0.3971142002927063, 'd_model': 35, 'nhead': 5, 'num_layers': 1, 'dim_feedforward': 202, 'conv_channels': 14, 'se_ratio': 15, 'downsample_stride': 6, 'focal_gamma': 2.260668222934651, 'label_smoothing': 0.052804742239333624, 'class_weight_mode': np.str_('balanced'), 'max_grad_norm': 1.1835949467344977, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False, 'calib_batches': 133}
2025-09-19 10:54:35,962 - ERROR - _models.training_function_executor - Training execution failed: The expanded size of the tensor (17) must match the existing size (18) at non-singleton dimension 1.  Target sizes: [2000, 17].  Tensor sizes: [2000, 18]
2025-09-19 10:54:35,962 - ERROR - _models.training_function_executor - Training code: import copy
import math
from typing import Dict, Any

def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):
    import torch
    import torch.nn as nn
    import torch.nn.functional ...
2025-09-19 10:54:35,962 - ERROR - _models.training_function_executor - BO training objective failed: The expanded size of the tensor (17) must match the existing size (18) at non-singleton dimension 1.  Target sizes: [2000, 17].  Tensor sizes: [2000, 18]
2025-09-19 10:54:35,962 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.007s
2025-09-19 10:54:36,178 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 10:54:36,178 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.216s
2025-09-19 10:54:36,178 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 0.0007365344466688374, 'batch_size': np.int64(64), 'epochs': np.int64(8), 'weight_decay': 0.0003397954840289545, 'dropout': 0.3971142002927063, 'd_model': np.int64(35), 'nhead': np.int64(5), 'num_layers': np.int64(1), 'dim_feedforward': np.int64(202), 'conv_channels': np.int64(14), 'se_ratio': np.int64(15), 'downsample_stride': np.int64(6), 'focal_gamma': 2.260668222934651, 'label_smoothing': 0.052804742239333624, 'class_weight_mode': np.str_('balanced'), 'max_grad_norm': 1.1835949467344977, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(133)}, value=0.0000
2025-09-19 10:54:36,178 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 0.0007365344466688374, 'batch_size': np.int64(64), 'epochs': np.int64(8), 'weight_decay': 0.0003397954840289545, 'dropout': 0.3971142002927063, 'd_model': np.int64(35), 'nhead': np.int64(5), 'num_layers': np.int64(1), 'dim_feedforward': np.int64(202), 'conv_channels': np.int64(14), 'se_ratio': np.int64(15), 'downsample_stride': np.int64(6), 'focal_gamma': 2.260668222934651, 'label_smoothing': 0.052804742239333624, 'class_weight_mode': np.str_('balanced'), 'max_grad_norm': 1.1835949467344977, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_, 'calib_batches': np.int64(133)} -> 0.0000
2025-09-19 10:54:36,178 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-09-19 10:54:36,178 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 10:54:36,178 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 10:54:36,178 - INFO - _models.training_function_executor - Executing training function: ECGTransMini-MSCNN-BiT-PTQ
2025-09-19 10:54:36,178 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0015979629500056552, 'batch_size': 32, 'epochs': 6, 'weight_decay': 0.00022462331923754332, 'dropout': 0.4054625004151639, 'd_model': 54, 'nhead': 6, 'num_layers': 2, 'dim_feedforward': 203, 'conv_channels': 30, 'se_ratio': 11, 'downsample_stride': 2, 'focal_gamma': 1.4411086926567713, 'label_smoothing': 0.06776994066244284, 'class_weight_mode': np.str_('balanced'), 'max_grad_norm': 3.833840001606798, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 69}
2025-09-19 10:54:36,181 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0015979629500056552, 'batch_size': 32, 'epochs': 6, 'weight_decay': 0.00022462331923754332, 'dropout': 0.4054625004151639, 'd_model': 54, 'nhead': 6, 'num_layers': 2, 'dim_feedforward': 203, 'conv_channels': 30, 'se_ratio': 11, 'downsample_stride': 2, 'focal_gamma': 1.4411086926567713, 'label_smoothing': 0.06776994066244284, 'class_weight_mode': np.str_('balanced'), 'max_grad_norm': 3.833840001606798, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True, 'calib_batches': 69}
