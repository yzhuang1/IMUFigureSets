2025-09-20 14:55:36,256 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 14:55:36,925 - INFO - __main__ - Logging system initialized successfully
2025-09-20 14:55:36,925 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-20 14:55:36,925 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-20 14:55:36,926 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-20 14:55:36,926 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-20 14:55:36,927 - INFO - __main__ - Attempting to load: X.npy
2025-09-20 14:55:37,161 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-20 14:55:37,245 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-20 14:55:37,246 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-20 14:55:37,246 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-20 14:55:37,246 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-20 14:55:37,250 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 14:55:37,250 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-20 14:55:37,252 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-20 14:55:37,252 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-20 14:55:37,252 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-20 14:55:37,252 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-20 14:55:37,252 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-20 14:55:37,252 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-20 14:55:37,252 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-20 14:55:37,468 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-20 14:55:37,468 - INFO - class_balancing - Class imbalance analysis:
2025-09-20 14:55:37,469 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-20 14:55:37,469 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-20 14:55:37,469 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-20 14:55:37,469 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-20 14:55:37,469 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-20 14:55:37,469 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-20 14:55:37,469 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-20 14:55:37,470 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-20 14:55:38,155 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-20 14:55:38,164 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-20 14:55:38,164 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-20 14:55:38,164 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-20 14:55:38,164 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-20 14:55:38,164 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-20 14:55:38,164 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-20 14:58:13,942 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-20 14:58:14,076 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-20 14:58:14,076 - INFO - _models.ai_code_generator - Prompt length: 3393 characters
2025-09-20 14:58:14,076 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-20 14:58:14,076 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-20 14:58:14,076 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-20 14:59:38,629 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-20 14:59:38,635 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-20 14:59:38,635 - INFO - _models.ai_code_generator - AI generated training function: TinyViT1D-RR
2025-09-20 14:59:38,635 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-09-20 14:59:38,635 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.76)
2025-09-20 14:59:38,635 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: TinyViT1D-RR
2025-09-20 14:59:38,635 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'weight_decay', 'patch_size', 'stride', 'embed_dim', 'depth', 'num_heads', 'mlp_ratio', 'dropout', 'attn_dropout', 'head_hidden_dim', 'pooling', 'augment_prob', 'noise_std', 'wander_amp', 'loss_type', 'focal_gamma', 'label_smoothing', 'lr_scheduler', 'step_lr_gamma', 'step_lr_step_size', 'gradient_clip_norm', 'early_stop_patience', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-20 14:59:38,635 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-09-20 14:59:38,637 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-20 14:59:38,640 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions\training_function_torch_tensor_TinyViT1D-RR_1758398378.json
2025-09-20 14:59:38,640 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions\training_function_torch_tensor_TinyViT1D-RR_1758398378.json
2025-09-20 14:59:38,640 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-20 14:59:38,640 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: TinyViT1D-RR
2025-09-20 14:59:38,640 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-20 14:59:38,642 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-20 14:59:38,642 - WARNING - package_installer - Could not parse code for imports due to syntax error: unexpected character after line continuation character (<unknown>, line 1)
2025-09-20 14:59:38,642 - INFO - package_installer - Extracted imports from code: set()
2025-09-20 14:59:38,642 - INFO - package_installer - ‚úÖ No external packages required
2025-09-20 14:59:38,642 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-20 14:59:38,643 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 62352 samples (using full dataset)
2025-09-20 14:59:38,643 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'patch_size', 'stride', 'embed_dim', 'depth', 'num_heads', 'mlp_ratio', 'dropout', 'attn_dropout', 'head_hidden_dim', 'pooling', 'augment_prob', 'noise_std', 'wander_amp', 'loss_type', 'focal_gamma', 'label_smoothing', 'lr_scheduler', 'step_lr_gamma', 'step_lr_step_size', 'gradient_clip_norm', 'early_stop_patience', 'seed', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-20 14:59:38,643 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 14:59:38,643 - INFO - _models.training_function_executor - Using centralized data splits for BO objective
2025-09-20 14:59:39,562 - INFO - bo.run_bo - Converted GPT search space: 29 parameters
2025-09-20 14:59:39,562 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-20 14:59:39,563 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-20 14:59:39,565 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-20 14:59:39,565 - INFO - bo.run_bo - [PROFILE] suggest() took 0.002s
2025-09-20 14:59:39,566 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 14:59:39,566 - INFO - _models.training_function_executor - Executing training function: TinyViT1D-RR
2025-09-20 14:59:39,566 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'patch_size': 16, 'stride': 8, 'embed_dim': 55, 'depth': 3, 'num_heads': 1, 'mlp_ratio': 3.924774630404986, 'dropout': 0.24973279224012657, 'attn_dropout': 0.04246782213565524, 'head_hidden_dim': 95, 'pooling': 'max', 'augment_prob': 0.4322370567394016, 'noise_std': 0.03058265802441405, 'wander_amp': 0.0007066305219717408, 'loss_type': 'weighted_ce', 'focal_gamma': 2.0495493205167783, 'label_smoothing': 0.07997219434305111, 'lr_scheduler': 'none', 'step_lr_gamma': 0.8790044150731675, 'step_lr_step_size': 3, 'gradient_clip_norm': 1.9123099563358141, 'early_stop_patience': 3, 'seed': 1529, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 14:59:39,568 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'patch_size': 16, 'stride': 8, 'embed_dim': 55, 'depth': 3, 'num_heads': 1, 'mlp_ratio': 3.924774630404986, 'dropout': 0.24973279224012657, 'attn_dropout': 0.04246782213565524, 'head_hidden_dim': 95, 'pooling': 'max', 'augment_prob': 0.4322370567394016, 'noise_std': 0.03058265802441405, 'wander_amp': 0.0007066305219717408, 'loss_type': 'weighted_ce', 'focal_gamma': 2.0495493205167783, 'label_smoothing': 0.07997219434305111, 'lr_scheduler': 'none', 'step_lr_gamma': 0.8790044150731675, 'step_lr_step_size': 3, 'gradient_clip_norm': 1.9123099563358141, 'early_stop_patience': 3, 'seed': 1529, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 14:59:39,627 - ERROR - _models.training_function_executor - Training execution failed: The expanded size of the tensor (27) must match the existing size (28) at non-singleton dimension 1.  Target sizes: [134, 27].  Tensor sizes: [134, 28]
2025-09-20 14:59:39,627 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Optional, Dict, Any, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch....
2025-09-20 14:59:39,627 - ERROR - _models.training_function_executor - BO training objective failed: The expanded size of the tensor (27) must match the existing size (28) at non-singleton dimension 1.  Target sizes: [134, 27].  Tensor sizes: [134, 28]
2025-09-20 14:59:39,627 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.062s
2025-09-20 14:59:39,627 - WARNING - evaluation.code_generation_pipeline_orchestrator - BO Trial 1 failed with error: The expanded size of the tensor (27) must match the existing size (28) at non-singleton dimension 1.  Target sizes: [134, 27].  Tensor sizes: [134, 28]
2025-09-20 14:59:39,627 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-20 14:59:39,627 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 14:59:39,627 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'patch_size': 16, 'stride': 8, 'embed_dim': np.int64(55), 'depth': np.int64(3), 'num_heads': 1, 'mlp_ratio': 3.924774630404986, 'dropout': 0.24973279224012657, 'attn_dropout': 0.04246782213565524, 'head_hidden_dim': np.int64(95), 'pooling': 'max', 'augment_prob': 0.4322370567394016, 'noise_std': 0.03058265802441405, 'wander_amp': 0.0007066305219717408, 'loss_type': 'weighted_ce', 'focal_gamma': 2.0495493205167783, 'label_smoothing': 0.07997219434305111, 'lr_scheduler': 'none', 'step_lr_gamma': 0.8790044150731675, 'step_lr_step_size': np.int64(3), 'gradient_clip_norm': 1.9123099563358141, 'early_stop_patience': np.int64(3), 'seed': np.int64(1529), 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, value=0.0000
2025-09-20 14:59:39,627 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'patch_size': 16, 'stride': 8, 'embed_dim': np.int64(55), 'depth': np.int64(3), 'num_heads': 1, 'mlp_ratio': 3.924774630404986, 'dropout': 0.24973279224012657, 'attn_dropout': 0.04246782213565524, 'head_hidden_dim': np.int64(95), 'pooling': 'max', 'augment_prob': 0.4322370567394016, 'noise_std': 0.03058265802441405, 'wander_amp': 0.0007066305219717408, 'loss_type': 'weighted_ce', 'focal_gamma': 2.0495493205167783, 'label_smoothing': 0.07997219434305111, 'lr_scheduler': 'none', 'step_lr_gamma': 0.8790044150731675, 'step_lr_step_size': np.int64(3), 'gradient_clip_norm': 1.9123099563358141, 'early_stop_patience': np.int64(3), 'seed': np.int64(1529), 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True} -> 0.0000
2025-09-20 14:59:39,629 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-09-20 14:59:39,629 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 14:59:39,629 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 14:59:39,629 - INFO - _models.training_function_executor - Executing training function: TinyViT1D-RR
2025-09-20 14:59:39,629 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.8205657658407253e-06, 'batch_size': 256, 'epochs': 18, 'weight_decay': 0.0017123375973164003, 'patch_size': 16, 'stride': 8, 'embed_dim': 91, 'depth': 3, 'num_heads': 2, 'mlp_ratio': 3.5829872793404114, 'dropout': 0.05200939605233163, 'attn_dropout': 0.07821212151464817, 'head_hidden_dim': 81, 'pooling': 'max', 'augment_prob': 0.2181977532625877, 'noise_std': 0.026003401058890544, 'wander_amp': 0.05467102793432797, 'loss_type': 'weighted_ce', 'focal_gamma': 2.9391692555291176, 'label_smoothing': 0.15502656467222295, 'lr_scheduler': 'step', 'step_lr_gamma': 0.8158618803421192, 'step_lr_step_size': 8, 'gradient_clip_norm': 2.8522198720269976, 'early_stop_patience': 8, 'seed': 7870, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-20 14:59:39,631 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.8205657658407253e-06, 'batch_size': 256, 'epochs': 18, 'weight_decay': 0.0017123375973164003, 'patch_size': 16, 'stride': 8, 'embed_dim': 91, 'depth': 3, 'num_heads': 2, 'mlp_ratio': 3.5829872793404114, 'dropout': 0.05200939605233163, 'attn_dropout': 0.07821212151464817, 'head_hidden_dim': 81, 'pooling': 'max', 'augment_prob': 0.2181977532625877, 'noise_std': 0.026003401058890544, 'wander_amp': 0.05467102793432797, 'loss_type': 'weighted_ce', 'focal_gamma': 2.9391692555291176, 'label_smoothing': 0.15502656467222295, 'lr_scheduler': 'step', 'step_lr_gamma': 0.8158618803421192, 'step_lr_step_size': 8, 'gradient_clip_norm': 2.8522198720269976, 'early_stop_patience': 8, 'seed': 7870, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-20 14:59:39,634 - ERROR - _models.training_function_executor - Training execution failed: The expanded size of the tensor (45) must match the existing size (46) at non-singleton dimension 1.  Target sizes: [134, 45].  Tensor sizes: [134, 46]
2025-09-20 14:59:39,634 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Optional, Dict, Any, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch....
2025-09-20 14:59:39,634 - ERROR - _models.training_function_executor - BO training objective failed: The expanded size of the tensor (45) must match the existing size (46) at non-singleton dimension 1.  Target sizes: [134, 45].  Tensor sizes: [134, 46]
2025-09-20 14:59:39,634 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.006s
2025-09-20 14:59:39,634 - WARNING - evaluation.code_generation_pipeline_orchestrator - BO Trial 2 failed with error: The expanded size of the tensor (45) must match the existing size (46) at non-singleton dimension 1.  Target sizes: [134, 45].  Tensor sizes: [134, 46]
2025-09-20 14:59:39,634 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-20 14:59:39,634 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 14:59:39,634 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 1.8205657658407253e-06, 'batch_size': 256, 'epochs': np.int64(18), 'weight_decay': 0.0017123375973164003, 'patch_size': 16, 'stride': 8, 'embed_dim': np.int64(91), 'depth': np.int64(3), 'num_heads': 2, 'mlp_ratio': 3.5829872793404114, 'dropout': 0.05200939605233163, 'attn_dropout': 0.07821212151464817, 'head_hidden_dim': np.int64(81), 'pooling': 'max', 'augment_prob': 0.2181977532625877, 'noise_std': 0.026003401058890544, 'wander_amp': 0.05467102793432797, 'loss_type': 'weighted_ce', 'focal_gamma': 2.9391692555291176, 'label_smoothing': 0.15502656467222295, 'lr_scheduler': 'step', 'step_lr_gamma': 0.8158618803421192, 'step_lr_step_size': np.int64(8), 'gradient_clip_norm': 2.8522198720269976, 'early_stop_patience': np.int64(8), 'seed': np.int64(7870), 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, value=0.0000
2025-09-20 14:59:39,634 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 1.8205657658407253e-06, 'batch_size': 256, 'epochs': np.int64(18), 'weight_decay': 0.0017123375973164003, 'patch_size': 16, 'stride': 8, 'embed_dim': np.int64(91), 'depth': np.int64(3), 'num_heads': 2, 'mlp_ratio': 3.5829872793404114, 'dropout': 0.05200939605233163, 'attn_dropout': 0.07821212151464817, 'head_hidden_dim': np.int64(81), 'pooling': 'max', 'augment_prob': 0.2181977532625877, 'noise_std': 0.026003401058890544, 'wander_amp': 0.05467102793432797, 'loss_type': 'weighted_ce', 'focal_gamma': 2.9391692555291176, 'label_smoothing': 0.15502656467222295, 'lr_scheduler': 'step', 'step_lr_gamma': 0.8158618803421192, 'step_lr_step_size': np.int64(8), 'gradient_clip_norm': 2.8522198720269976, 'early_stop_patience': np.int64(8), 'seed': np.int64(7870), 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True} -> 0.0000
2025-09-20 14:59:39,635 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-09-20 14:59:39,636 - INFO - bo.run_bo - [PROFILE] suggest() took 0.002s
2025-09-20 14:59:39,636 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 14:59:39,636 - INFO - _models.training_function_executor - Executing training function: TinyViT1D-RR
2025-09-20 14:59:39,636 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002065142557895928, 'batch_size': 64, 'epochs': 49, 'weight_decay': 3.6618192203924283e-06, 'patch_size': 32, 'stride': 8, 'embed_dim': 40, 'depth': 4, 'num_heads': 2, 'mlp_ratio': 2.2337204367950956, 'dropout': 0.004223946814525337, 'attn_dropout': 0.03976848081776104, 'head_hidden_dim': 39, 'pooling': 'max', 'augment_prob': 0.539889242680162, 'noise_std': 0.003702232586704519, 'wander_amp': 0.035846572854427265, 'loss_type': 'weighted_ce', 'focal_gamma': 2.7262068517511873, 'label_smoothing': 0.1246596253655116, 'lr_scheduler': 'none', 'step_lr_gamma': 0.15084668022881892, 'step_lr_step_size': 7, 'gradient_clip_norm': 3.344206263318037, 'early_stop_patience': 4, 'seed': 2732, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 14:59:39,639 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002065142557895928, 'batch_size': 64, 'epochs': 49, 'weight_decay': 3.6618192203924283e-06, 'patch_size': 32, 'stride': 8, 'embed_dim': 40, 'depth': 4, 'num_heads': 2, 'mlp_ratio': 2.2337204367950956, 'dropout': 0.004223946814525337, 'attn_dropout': 0.03976848081776104, 'head_hidden_dim': 39, 'pooling': 'max', 'augment_prob': 0.539889242680162, 'noise_std': 0.003702232586704519, 'wander_amp': 0.035846572854427265, 'loss_type': 'weighted_ce', 'focal_gamma': 2.7262068517511873, 'label_smoothing': 0.1246596253655116, 'lr_scheduler': 'none', 'step_lr_gamma': 0.15084668022881892, 'step_lr_step_size': 7, 'gradient_clip_norm': 3.344206263318037, 'early_stop_patience': 4, 'seed': 2732, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 14:59:39,714 - INFO - _models.ai_code_generator - Starting debug attempts (max: 4)
2025-09-20 14:59:41,606 - ERROR - _models.training_function_executor - Training execution failed: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
2025-09-20 14:59:41,606 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Optional, Dict, Any, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch....
2025-09-20 14:59:41,606 - ERROR - _models.training_function_executor - BO training objective failed: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
2025-09-20 14:59:41,606 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 1.970s
2025-09-20 14:59:41,606 - WARNING - evaluation.code_generation_pipeline_orchestrator - BO Trial 3 failed with error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
2025-09-20 14:59:41,932 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-20 14:59:41,932 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.326s
2025-09-20 14:59:41,932 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.002065142557895928, 'batch_size': 64, 'epochs': np.int64(49), 'weight_decay': 3.6618192203924283e-06, 'patch_size': 32, 'stride': 8, 'embed_dim': np.int64(40), 'depth': np.int64(4), 'num_heads': 2, 'mlp_ratio': 2.2337204367950956, 'dropout': 0.004223946814525337, 'attn_dropout': 0.03976848081776104, 'head_hidden_dim': np.int64(39), 'pooling': 'max', 'augment_prob': 0.539889242680162, 'noise_std': 0.003702232586704519, 'wander_amp': 0.035846572854427265, 'loss_type': 'weighted_ce', 'focal_gamma': 2.7262068517511873, 'label_smoothing': 0.1246596253655116, 'lr_scheduler': 'none', 'step_lr_gamma': 0.15084668022881892, 'step_lr_step_size': np.int64(7), 'gradient_clip_norm': 3.344206263318037, 'early_stop_patience': np.int64(4), 'seed': np.int64(2732), 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, value=0.0000
2025-09-20 14:59:41,932 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.002065142557895928, 'batch_size': 64, 'epochs': np.int64(49), 'weight_decay': 3.6618192203924283e-06, 'patch_size': 32, 'stride': 8, 'embed_dim': np.int64(40), 'depth': np.int64(4), 'num_heads': 2, 'mlp_ratio': 2.2337204367950956, 'dropout': 0.004223946814525337, 'attn_dropout': 0.03976848081776104, 'head_hidden_dim': np.int64(39), 'pooling': 'max', 'augment_prob': 0.539889242680162, 'noise_std': 0.003702232586704519, 'wander_amp': 0.035846572854427265, 'loss_type': 'weighted_ce', 'focal_gamma': 2.7262068517511873, 'label_smoothing': 0.1246596253655116, 'lr_scheduler': 'none', 'step_lr_gamma': 0.15084668022881892, 'step_lr_step_size': np.int64(7), 'gradient_clip_norm': 3.344206263318037, 'early_stop_patience': np.int64(4), 'seed': np.int64(2732), 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True} -> 0.0000
2025-09-20 14:59:41,932 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.0000
2025-09-20 14:59:41,932 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'patch_size': 16, 'stride': 8, 'embed_dim': np.int64(55), 'depth': np.int64(3), 'num_heads': 1, 'mlp_ratio': 3.924774630404986, 'dropout': 0.24973279224012657, 'attn_dropout': 0.04246782213565524, 'head_hidden_dim': np.int64(95), 'pooling': 'max', 'augment_prob': 0.4322370567394016, 'noise_std': 0.03058265802441405, 'wander_amp': 0.0007066305219717408, 'loss_type': 'weighted_ce', 'focal_gamma': 2.0495493205167783, 'label_smoothing': 0.07997219434305111, 'lr_scheduler': 'none', 'step_lr_gamma': 0.8790044150731675, 'step_lr_step_size': np.int64(3), 'gradient_clip_norm': 1.9123099563358141, 'early_stop_patience': np.int64(3), 'seed': np.int64(1529), 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 14:59:41,933 - INFO - visualization - Generating BO visualization charts with 3 trials...
2025-09-20 14:59:44,125 - INFO - visualization - BO summary saved to: charts\BO_TinyViT1D-RR_20250920_145941\bo_summary.txt
2025-09-20 14:59:44,126 - INFO - visualization - BO charts saved to: charts\BO_TinyViT1D-RR_20250920_145941
2025-09-20 14:59:44,126 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä BO charts saved to: charts\BO_TinyViT1D-RR_20250920_145941
2025-09-20 14:59:44,126 - INFO - evaluation.code_generation_pipeline_orchestrator - üöÄ STEP 4: Final Training Execution
2025-09-20 14:59:44,126 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-20 14:59:44,301 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 14:59:44,308 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 14:59:44,318 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 14:59:44,327 - INFO - _models.training_function_executor - Loaded training function: TinyViT1D-RR
2025-09-20 14:59:44,327 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-20 14:59:44,327 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'patch_size': 16, 'stride': 8, 'embed_dim': np.int64(55), 'depth': np.int64(3), 'num_heads': 1, 'mlp_ratio': 3.924774630404986, 'dropout': 0.24973279224012657, 'attn_dropout': 0.04246782213565524, 'head_hidden_dim': np.int64(95), 'pooling': 'max', 'augment_prob': 0.4322370567394016, 'noise_std': 0.03058265802441405, 'wander_amp': 0.0007066305219717408, 'loss_type': 'weighted_ce', 'focal_gamma': 2.0495493205167783, 'label_smoothing': 0.07997219434305111, 'lr_scheduler': 'none', 'step_lr_gamma': 0.8790044150731675, 'step_lr_step_size': np.int64(3), 'gradient_clip_norm': 1.9123099563358141, 'early_stop_patience': np.int64(3), 'seed': np.int64(1529), 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 14:59:44,327 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 14:59:44,371 - INFO - _models.training_function_executor - Executing training function: TinyViT1D-RR
2025-09-20 14:59:44,371 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 0.0002481040974867812, 'patch_size': 16, 'stride': 8, 'embed_dim': np.int64(55), 'depth': np.int64(3), 'num_heads': 1, 'mlp_ratio': 3.924774630404986, 'dropout': 0.24973279224012657, 'attn_dropout': 0.04246782213565524, 'head_hidden_dim': np.int64(95), 'pooling': 'max', 'augment_prob': 0.4322370567394016, 'noise_std': 0.03058265802441405, 'wander_amp': 0.0007066305219717408, 'loss_type': 'weighted_ce', 'focal_gamma': 2.0495493205167783, 'label_smoothing': 0.07997219434305111, 'lr_scheduler': 'none', 'step_lr_gamma': 0.8790044150731675, 'step_lr_step_size': np.int64(3), 'gradient_clip_norm': 1.9123099563358141, 'early_stop_patience': np.int64(3), 'seed': np.int64(1529), 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 14:59:44,374 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.001535224694197351, 'batch_size': 32, 'epochs': 12, 'weight_decay': 0.0002481040974867812, 'patch_size': 16, 'stride': 8, 'embed_dim': 55, 'depth': 3, 'num_heads': 1, 'mlp_ratio': 3.924774630404986, 'dropout': 0.24973279224012657, 'attn_dropout': 0.04246782213565524, 'head_hidden_dim': 95, 'pooling': 'max', 'augment_prob': 0.4322370567394016, 'noise_std': 0.03058265802441405, 'wander_amp': 0.0007066305219717408, 'loss_type': 'weighted_ce', 'focal_gamma': 2.0495493205167783, 'label_smoothing': 0.07997219434305111, 'lr_scheduler': 'none', 'step_lr_gamma': 0.8790044150731675, 'step_lr_step_size': 3, 'gradient_clip_norm': 1.9123099563358141, 'early_stop_patience': 3, 'seed': 1529, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 14:59:44,394 - ERROR - _models.training_function_executor - Training execution failed: The expanded size of the tensor (27) must match the existing size (28) at non-singleton dimension 1.  Target sizes: [134, 27].  Tensor sizes: [134, 28]
2025-09-20 14:59:44,394 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Optional, Dict, Any, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch....
2025-09-20 14:59:44,394 - ERROR - __main__ - Unhandled exception: RuntimeError: The expanded size of the tensor (27) must match the existing size (28) at non-singleton dimension 1.  Target sizes: [134, 27].  Tensor sizes: [134, 28]
Traceback (most recent call last):
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 484, in <module>
    processed_real_data = process_real_data()
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 342, in process_real_data
    result = process_data_with_ai_enhanced_evaluation(
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 136, in process_data_with_ai_enhanced_evaluation
    result = train_with_iterative_selection(data, labels, device=device, **kwargs)
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 85, in train_with_iterative_selection
    best_model, pipeline_results = orchestrator.run_complete_pipeline(
  File "D:\_A\GPT_research\ml_pipeline\evaluation\code_generation_pipeline_orchestrator.py", line 98, in run_complete_pipeline
    final_model, training_results = self._execute_final_training(
  File "D:\_A\GPT_research\ml_pipeline\evaluation\code_generation_pipeline_orchestrator.py", line 408, in _execute_final_training
    trained_model, training_metrics = training_executor.execute_training_function(
  File "D:\_A\GPT_research\ml_pipeline\_models\training_function_executor.py", line 187, in execute_training_function
    model, metrics = train_model(
  File "<string>", line 274, in train_model
  File "<string>", line 126, in __init__
  File "<string>", line 79, in __init__
RuntimeError: The expanded size of the tensor (27) must match the existing size (28) at non-singleton dimension 1.  Target sizes: [134, 27].  Tensor sizes: [134, 28]

