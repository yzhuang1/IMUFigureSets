2025-09-20 02:11:46,944 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 02:11:47,315 - INFO - __main__ - Logging system initialized successfully
2025-09-20 02:11:47,315 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-20 02:11:47,316 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-20 02:11:47,317 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-20 02:11:47,317 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-20 02:11:47,319 - INFO - __main__ - Attempting to load: X.npy
2025-09-20 02:11:47,467 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-20 02:11:47,558 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-20 02:11:47,559 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-20 02:11:47,559 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-20 02:11:47,559 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-20 02:11:47,562 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 02:11:47,562 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-20 02:11:47,562 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-20 02:11:47,562 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-20 02:11:47,562 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-20 02:11:47,562 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-20 02:11:47,563 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-20 02:11:47,563 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-20 02:11:47,563 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-20 02:11:47,800 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-20 02:11:47,800 - INFO - class_balancing - Class imbalance analysis:
2025-09-20 02:11:47,800 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-20 02:11:47,800 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-20 02:11:47,800 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-20 02:11:47,800 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-20 02:11:47,800 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-20 02:11:47,800 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-20 02:11:47,800 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-20 02:11:47,800 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-20 02:11:48,507 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-20 02:11:48,517 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-20 02:11:48,517 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-20 02:11:48,517 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-20 02:11:48,517 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-20 02:11:48,517 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-20 02:11:48,517 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-20 02:16:10,337 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-20 02:16:10,456 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-20 02:16:10,456 - INFO - _models.ai_code_generator - Prompt length: 3357 characters
2025-09-20 02:16:10,456 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-20 02:16:10,456 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-20 02:16:10,456 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-20 02:18:38,752 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-20 02:18:38,764 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-20 02:18:38,766 - INFO - _models.ai_code_generator - AI generated training function: STCT_ECG_TinyTransformer
2025-09-20 02:18:38,766 - INFO - _models.ai_code_generator - Confidence: 0.90
2025-09-20 02:18:38,766 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-09-20 02:18:38,766 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: STCT_ECG_TinyTransformer
2025-09-20 02:18:38,766 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'embed_dim', 'stem_channels', 'num_heads', 'num_layers', 'mlp_ratio', 'kernel_size', 'stride1', 'stride2', 'use_cls_token', 'use_focal', 'focal_gamma', 'label_smoothing', 'grad_clip_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-20 02:18:38,766 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.90
2025-09-20 02:18:38,768 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-20 02:18:38,771 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions\training_function_torch_tensor_STCT_ECG_TinyTransformer_1758352718.json
2025-09-20 02:18:38,771 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions\training_function_torch_tensor_STCT_ECG_TinyTransformer_1758352718.json
2025-09-20 02:18:38,771 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-20 02:18:38,771 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: STCT_ECG_TinyTransformer
2025-09-20 02:18:38,771 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-20 02:18:38,772 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-20 02:18:38,776 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-20 02:18:38,777 - INFO - package_installer - Available packages: {'torch'}
2025-09-20 02:18:38,777 - INFO - package_installer - Missing packages: set()
2025-09-20 02:18:38,777 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-20 02:18:38,777 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-20 02:18:38,777 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 62352 samples (using full dataset)
2025-09-20 02:18:38,777 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['lr', 'batch_size', 'epochs', 'weight_decay', 'dropout', 'embed_dim', 'stem_channels', 'num_heads', 'num_layers', 'mlp_ratio', 'kernel_size', 'stride1', 'stride2', 'use_cls_token', 'use_focal', 'focal_gamma', 'label_smoothing', 'grad_clip_norm', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-20 02:18:38,777 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-20 02:18:38,777 - INFO - _models.training_function_executor - Using centralized data splits for BO objective
2025-09-20 02:18:39,179 - INFO - bo.run_bo - Converted GPT search space: 21 parameters
2025-09-20 02:18:39,179 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-20 02:18:39,180 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-20 02:18:39,181 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-20 02:18:39,181 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 02:18:39,181 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 02:18:39,181 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 02:18:39,181 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': 12, 'weight_decay': 6.251373574521747e-05, 'dropout': 0.07800932022121827, 'embed_dim': 50, 'stem_channels': 38, 'num_heads': 2, 'num_layers': 1, 'mlp_ratio': 4, 'kernel_size': 5, 'stride1': 2, 'stride2': 4, 'use_cls_token': False, 'use_focal': True, 'focal_gamma': 1.545474901621302, 'label_smoothing': 0.018340450985343384, 'grad_clip_norm': 0.6084844859190756, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-09-20 02:18:39,183 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': 12, 'weight_decay': 6.251373574521747e-05, 'dropout': 0.07800932022121827, 'embed_dim': 50, 'stem_channels': 38, 'num_heads': 2, 'num_layers': 1, 'mlp_ratio': 4, 'kernel_size': 5, 'stride1': 2, 'stride2': 4, 'use_cls_token': False, 'use_focal': True, 'focal_gamma': 1.545474901621302, 'label_smoothing': 0.018340450985343384, 'grad_clip_norm': 0.6084844859190756, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}
2025-09-20 02:20:01,842 - INFO - _models.training_function_executor - Model parameter count: 12,836
2025-09-20 02:20:01,842 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.6654189105303219, 0.45988087829593766, 0.3871050585900025, 0.3333118775153328, 0.297076018318032, 0.281188340280683, 0.24506034317882708, 0.24362405731455508, 0.21687238031734826, 0.19875133349965368, 0.1871968141173288, 0.1858052308744928], 'val_losses': [0.4608434794469361, 0.35621504044524116, 0.36810979619136824, 0.29415404420393443, 0.26810482197843577, 0.3436595517944556, 0.26290816523902394, 0.34177838386444104, 0.3005534633889554, 0.3188523980841511, 0.28931687106952925, 0.292751962007611], 'val_acc': [0.4563496040894056, 0.7250676556078982, 0.7004109451739, 0.7788914503357722, 0.8197855066653302, 0.8908489525909592, 0.8119675253082089, 0.9156058935551769, 0.8785205973739602, 0.9293374761952491, 0.8731081487421068, 0.8877418061541545], 'num_parameters': 33341, 'quantization_bits': 16, 'quantized_on': 'Linear (Transformer + head) via dynamic quantization', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': 12, 'weight_decay': 6.251373574521747e-05, 'dropout': 0.07800932022121827, 'embed_dim': 50, 'stem_channels': 38, 'num_heads': 2, 'num_layers': 1, 'mlp_ratio': 4, 'kernel_size': 5, 'stride1': 2, 'stride2': 4, 'use_cls_token': False, 'use_focal': True, 'focal_gamma': 1.545474901621302, 'label_smoothing': 0.018340450985343384, 'grad_clip_norm': 0.6084844859190756, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 12836, 'model_size_validation': 'PASS'}
2025-09-20 02:20:01,842 - INFO - _models.training_function_executor - BO Objective: base=0.8877, size_penalty=0.0000, final=0.8877
2025-09-20 02:20:01,842 - INFO - _models.training_function_executor - Model size: 12,836 parameters (PASS 256K limit)
2025-09-20 02:20:01,842 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 82.661s
2025-09-20 02:20:01,842 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8877
2025-09-20 02:20:01,842 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 02:20:01,842 - INFO - bo.run_bo - Recorded observation #1: hparams={'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 6.251373574521747e-05, 'dropout': 0.07800932022121827, 'embed_dim': np.int64(50), 'stem_channels': np.int64(38), 'num_heads': 2, 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'kernel_size': 5, 'stride1': np.int64(2), 'stride2': np.int64(4), 'use_cls_token': False, 'use_focal': True, 'focal_gamma': 1.545474901621302, 'label_smoothing': 0.018340450985343384, 'grad_clip_norm': 0.6084844859190756, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True}, value=0.8877
2025-09-20 02:20:01,842 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'lr': 0.002452612631133679, 'batch_size': 32, 'epochs': np.int64(12), 'weight_decay': 6.251373574521747e-05, 'dropout': 0.07800932022121827, 'embed_dim': np.int64(50), 'stem_channels': np.int64(38), 'num_heads': 2, 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'kernel_size': 5, 'stride1': np.int64(2), 'stride2': np.int64(4), 'use_cls_token': False, 'use_focal': True, 'focal_gamma': 1.545474901621302, 'label_smoothing': 0.018340450985343384, 'grad_clip_norm': 0.6084844859190756, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': True} -> 0.8877
2025-09-20 02:20:01,843 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-09-20 02:20:01,843 - INFO - bo.run_bo - [PROFILE] suggest() took 0.002s
2025-09-20 02:20:01,843 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 02:20:01,843 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 02:20:01,843 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0006847920095574785, 'batch_size': 32, 'epochs': 32, 'weight_decay': 0.0008341930294140779, 'dropout': 0.11638567021515214, 'embed_dim': 78, 'stem_channels': 59, 'num_heads': 4, 'num_layers': 1, 'mlp_ratio': 4, 'kernel_size': 11, 'stride1': 3, 'stride2': 1, 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 3.8266052670545587, 'label_smoothing': 0.05632882178455394, 'grad_clip_norm': 0.7708330050798324, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-20 02:20:01,845 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0006847920095574785, 'batch_size': 32, 'epochs': 32, 'weight_decay': 0.0008341930294140779, 'dropout': 0.11638567021515214, 'embed_dim': 78, 'stem_channels': 59, 'num_heads': 4, 'num_layers': 1, 'mlp_ratio': 4, 'kernel_size': 11, 'stride1': 3, 'stride2': 1, 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 3.8266052670545587, 'label_smoothing': 0.05632882178455394, 'grad_clip_norm': 0.7708330050798324, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-20 02:28:14,417 - INFO - _models.training_function_executor - Model parameter count: 29,457
2025-09-20 02:28:14,418 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.4173539660164508, 0.2411916222973744, 0.19485340446925334, 0.17182436992991984, 0.16430024335483118, 0.1462135069689705, 0.13751764774822714, 0.13065325543949113, 0.1168476004973532, 0.11073355907796484, 0.10788267651886874, 0.10329719838520954, 0.09916204496783652, 0.09338671376887797, 0.08840276003926702, 0.08011523908773747, 0.08468166769432436, 0.07969746575019808, 0.07671432918656573, 0.07153048349332534, 0.07324266078158384, 0.06581532546806994, 0.06706530890641223, 0.06498210744521363, 0.0609126372679018, 0.05911494878975001, 0.0577982928317377, 0.060813830008199746, 0.05817726772986766, 0.053491226809506166, 0.05234014601895324, 0.05094323380525469], 'val_losses': [0.2867667608082009, 0.2606883056321697, 0.1576070487956637, 0.20004250400993626, 0.17543434268827907, 0.15403265754798764, 0.19596120960214394, 0.1433737837256217, 0.19154025899123864, 0.14052020288498188, 0.12679185415041705, 0.12849997563782914, 0.11054396177235522, 0.13374320801167894, 0.13229768695814764, 0.11120463197161634, 0.14009039178670565, 0.11435293815779855, 0.0998800171380871, 0.11226023725459686, 0.09303538247298271, 0.11817157873320261, 0.1367979585257507, 0.12087740530057578, 0.14836769981095166, 0.12075649575416476, 0.11744191108953939, 0.1027125255278248, 0.12289096570846822, 0.131999091101821, 0.10863902671124702, 0.14642854842207143], 'val_acc': [0.31702916708429385, 0.515786308509572, 0.45705121780094216, 0.7850055126791621, 0.6579132003608299, 0.5106745514683773, 0.7194547459156059, 0.6470883030971234, 0.6500952190037085, 0.680364839129999, 0.6292472687180515, 0.6550065149844643, 0.6443820787811968, 0.6108048511576626, 0.7652600982259196, 0.7715746216297484, 0.743910995289165, 0.8629848651899369, 0.7676656309511878, 0.7882128896461862, 0.6135110754735893, 0.7942267214593565, 0.7310814874210685, 0.8799238247970331, 0.8240954194647689, 0.7747819985967725, 0.774280845945675, 0.8000400922120878, 0.8802245163876917, 0.8941565600882029, 0.8078580735692091, 0.7789916808659918], 'num_parameters': 76430, 'quantization_bits': 8, 'quantized_on': 'Linear (Transformer + head) via dynamic quantization', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0006847920095574785, 'batch_size': 32, 'epochs': 32, 'weight_decay': 0.0008341930294140779, 'dropout': 0.11638567021515214, 'embed_dim': 78, 'stem_channels': 59, 'num_heads': 4, 'num_layers': 1, 'mlp_ratio': 4, 'kernel_size': 11, 'stride1': 3, 'stride2': 1, 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 3.8266052670545587, 'label_smoothing': 0.05632882178455394, 'grad_clip_norm': 0.7708330050798324, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 29457, 'model_size_validation': 'PASS'}
2025-09-20 02:28:14,418 - INFO - _models.training_function_executor - BO Objective: base=0.7790, size_penalty=0.0000, final=0.7790
2025-09-20 02:28:14,418 - INFO - _models.training_function_executor - Model size: 29,457 parameters (PASS 256K limit)
2025-09-20 02:28:14,418 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 492.574s
2025-09-20 02:28:14,418 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7790
2025-09-20 02:28:14,418 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-20 02:28:14,418 - INFO - bo.run_bo - Recorded observation #2: hparams={'lr': 0.0006847920095574785, 'batch_size': 32, 'epochs': np.int64(32), 'weight_decay': 0.0008341930294140779, 'dropout': 0.11638567021515214, 'embed_dim': np.int64(78), 'stem_channels': np.int64(59), 'num_heads': 4, 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'kernel_size': 11, 'stride1': np.int64(3), 'stride2': np.int64(1), 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 3.8266052670545587, 'label_smoothing': 0.05632882178455394, 'grad_clip_norm': 0.7708330050798324, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, value=0.7790
2025-09-20 02:28:14,418 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'lr': 0.0006847920095574785, 'batch_size': 32, 'epochs': np.int64(32), 'weight_decay': 0.0008341930294140779, 'dropout': 0.11638567021515214, 'embed_dim': np.int64(78), 'stem_channels': np.int64(59), 'num_heads': 4, 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'kernel_size': 11, 'stride1': np.int64(3), 'stride2': np.int64(1), 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 3.8266052670545587, 'label_smoothing': 0.05632882178455394, 'grad_clip_norm': 0.7708330050798324, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True} -> 0.7790
2025-09-20 02:28:14,420 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-09-20 02:28:14,420 - INFO - bo.run_bo - [PROFILE] suggest() took 0.002s
2025-09-20 02:28:14,420 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 02:28:14,420 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 02:28:14,420 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': 12, 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': 67, 'stem_channels': 55, 'num_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'kernel_size': 5, 'stride1': 4, 'stride2': 2, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 02:28:14,422 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': 12, 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': 67, 'stem_channels': 55, 'num_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'kernel_size': 5, 'stride1': 4, 'stride2': 2, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 02:29:21,293 - INFO - _models.training_function_executor - Model parameter count: 88,116
2025-09-20 02:29:21,294 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.4297514981332549, 1.0785304538326064, 0.9903025857524673, 0.9465232295997638, 0.9157074154522673, 0.8875726588076367, 0.8690879153746457, 0.8518071435123037, 0.8487380786045322, 0.8363810980329537, 0.8249187331822938, 0.8118632066316765], 'val_losses': [1.0845210504541418, 1.0017013950076432, 0.9184797803242268, 0.9105519319961863, 0.8902895420721015, 0.8702647043777585, 0.8559390186988676, 0.8636330516804097, 0.862738401333961, 0.8615707797087468, 0.8673315665293423, 0.8636531181991178], 'val_acc': [0.6379673248471485, 0.7659617119374561, 0.7847048210885036, 0.7968327152450636, 0.8081587651598677, 0.8522601984564498, 0.8750125288162774, 0.8803247469179112, 0.8922521800140323, 0.9079883732584946, 0.8665931642778391, 0.8939560990277639], 'num_parameters': 88116, 'quantization_bits': 32, 'quantized_on': 'none', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': 12, 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': 67, 'stem_channels': 55, 'num_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'kernel_size': 5, 'stride1': 4, 'stride2': 2, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 88116, 'model_size_validation': 'PASS'}
2025-09-20 02:29:21,294 - INFO - _models.training_function_executor - BO Objective: base=0.8940, size_penalty=0.0000, final=0.8940
2025-09-20 02:29:21,294 - INFO - _models.training_function_executor - Model size: 88,116 parameters (PASS 256K limit)
2025-09-20 02:29:21,294 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 66.875s
2025-09-20 02:29:21,545 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8940
2025-09-20 02:29:21,545 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.250s
2025-09-20 02:29:21,546 - INFO - bo.run_bo - Recorded observation #3: hparams={'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': np.int64(12), 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': np.int64(67), 'stem_channels': np.int64(55), 'num_heads': 4, 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'kernel_size': 5, 'stride1': np.int64(4), 'stride2': np.int64(2), 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, value=0.8940
2025-09-20 02:29:21,546 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': np.int64(12), 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': np.int64(67), 'stem_channels': np.int64(55), 'num_heads': 4, 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'kernel_size': 5, 'stride1': np.int64(4), 'stride2': np.int64(2), 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False} -> 0.8940
2025-09-20 02:29:21,546 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-09-20 02:29:21,546 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-20 02:29:21,546 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 02:29:21,546 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 02:29:21,547 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00036517646487517594, 'batch_size': 256, 'epochs': 9, 'weight_decay': 0.0005914929847717894, 'dropout': 0.15843488182141865, 'embed_dim': 38, 'stem_channels': 19, 'num_heads': 2, 'num_layers': 2, 'mlp_ratio': 2, 'kernel_size': 7, 'stride1': 3, 'stride2': 1, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 1.7139895840597617, 'label_smoothing': 0.015788585691846903, 'grad_clip_norm': 1.2431289736249025, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 02:29:21,548 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00036517646487517594, 'batch_size': 256, 'epochs': 9, 'weight_decay': 0.0005914929847717894, 'dropout': 0.15843488182141865, 'embed_dim': 38, 'stem_channels': 19, 'num_heads': 2, 'num_layers': 2, 'mlp_ratio': 2, 'kernel_size': 7, 'stride1': 3, 'stride2': 1, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 1.7139895840597617, 'label_smoothing': 0.015788585691846903, 'grad_clip_norm': 1.2431289736249025, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 02:31:29,135 - INFO - _models.training_function_executor - Model parameter count: 13,265
2025-09-20 02:31:29,135 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.4514872314648908, 1.0303839215681663, 0.8440127363182013, 0.7595921307443712, 0.7156751769107155, 0.6834134516375678, 0.6519709102803455, 0.6272234663355322, 0.616970478293412], 'val_losses': [1.1303013931525618, 0.8544566786228278, 0.7234252610736636, 0.6693289614552974, 0.6496690583991374, 0.6186579376372495, 0.5947009203064068, 0.577605192322787, 0.5698909692490902], 'val_acc': [0.48702014633657414, 0.6181216798636865, 0.7649594066352611, 0.7982359426681367, 0.8205873509070863, 0.824897263706525, 0.8518592763355718, 0.8609802545855467, 0.8610804851157663], 'num_parameters': 25240, 'quantization_bits': 8, 'quantized_on': 'Linear (Transformer + head) via dynamic quantization', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00036517646487517594, 'batch_size': 256, 'epochs': 9, 'weight_decay': 0.0005914929847717894, 'dropout': 0.15843488182141865, 'embed_dim': 38, 'stem_channels': 19, 'num_heads': 2, 'num_layers': 2, 'mlp_ratio': 2, 'kernel_size': 7, 'stride1': 3, 'stride2': 1, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 1.7139895840597617, 'label_smoothing': 0.015788585691846903, 'grad_clip_norm': 1.2431289736249025, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 13265, 'model_size_validation': 'PASS'}
2025-09-20 02:31:29,135 - INFO - _models.training_function_executor - BO Objective: base=0.8611, size_penalty=0.0000, final=0.8611
2025-09-20 02:31:29,135 - INFO - _models.training_function_executor - Model size: 13,265 parameters (PASS 256K limit)
2025-09-20 02:31:29,135 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 127.589s
2025-09-20 02:31:29,370 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8611
2025-09-20 02:31:29,370 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.236s
2025-09-20 02:31:29,370 - INFO - bo.run_bo - Recorded observation #4: hparams={'lr': 0.00036517646487517594, 'batch_size': np.int64(256), 'epochs': np.int64(9), 'weight_decay': 0.0005914929847717894, 'dropout': 0.15843488182141865, 'embed_dim': np.int64(38), 'stem_channels': np.int64(19), 'num_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'kernel_size': np.int64(7), 'stride1': np.int64(3), 'stride2': np.int64(1), 'use_cls_token': np.True_, 'use_focal': np.False_, 'focal_gamma': 1.7139895840597617, 'label_smoothing': 0.015788585691846903, 'grad_clip_norm': 1.2431289736249025, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.8611
2025-09-20 02:31:29,370 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'lr': 0.00036517646487517594, 'batch_size': np.int64(256), 'epochs': np.int64(9), 'weight_decay': 0.0005914929847717894, 'dropout': 0.15843488182141865, 'embed_dim': np.int64(38), 'stem_channels': np.int64(19), 'num_heads': np.int64(2), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(2), 'kernel_size': np.int64(7), 'stride1': np.int64(3), 'stride2': np.int64(1), 'use_cls_token': np.True_, 'use_focal': np.False_, 'focal_gamma': 1.7139895840597617, 'label_smoothing': 0.015788585691846903, 'grad_clip_norm': 1.2431289736249025, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.8611
2025-09-20 02:31:29,370 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-09-20 02:31:29,370 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-20 02:31:29,371 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 02:31:29,371 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 02:31:29,371 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0006868638673706484, 'batch_size': 256, 'epochs': 14, 'weight_decay': 0.000494716146267866, 'dropout': 0.4127778276933724, 'embed_dim': 76, 'stem_channels': 44, 'num_heads': 2, 'num_layers': 1, 'mlp_ratio': 4, 'kernel_size': 7, 'stride1': 2, 'stride2': 3, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 3.40173043803309, 'label_smoothing': 0.061154746679415985, 'grad_clip_norm': 1.9890657063781751, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 02:31:29,373 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0006868638673706484, 'batch_size': 256, 'epochs': 14, 'weight_decay': 0.000494716146267866, 'dropout': 0.4127778276933724, 'embed_dim': 76, 'stem_channels': 44, 'num_heads': 2, 'num_layers': 1, 'mlp_ratio': 4, 'kernel_size': 7, 'stride1': 2, 'stride2': 3, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 3.40173043803309, 'label_smoothing': 0.061154746679415985, 'grad_clip_norm': 1.9890657063781751, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 02:32:33,689 - INFO - _models.training_function_executor - Model parameter count: 27,878
2025-09-20 02:32:33,689 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.696614614752836, 1.3941570898008615, 1.2955668953388524, 1.2412595006113734, 1.2104629461728962, 1.1862137624523406, 1.163283201151116, 1.1547208811248315, 1.1383705687886156, 1.1261678839456395, 1.1257260922154333, 1.1162003816942645, 1.1056947248308966, 1.1097717296627874], 'val_losses': [1.3733362543376209, 1.2161053762868923, 1.146220691985258, 1.1375102521398925, 1.1098210294607276, 1.08329269712048, 1.0736087895687112, 1.0656316580270089, 1.0658827498786685, 1.0711966002466589, 1.0509318676216635, 1.0661134694279035, 1.060051863509189, 1.0653755959712206], 'val_acc': [0.4017239651197755, 0.5228024456249374, 0.7553372757341886, 0.6905883532123885, 0.7517289766462865, 0.8032474691791119, 0.8240954194647689, 0.783802746316528, 0.8318131702916708, 0.8374260799839631, 0.8161772075774281, 0.8398316127092312, 0.7544352009622131, 0.8009421669840634], 'num_parameters': 74851, 'quantization_bits': 8, 'quantized_on': 'Linear (Transformer + head) via dynamic quantization', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0006868638673706484, 'batch_size': 256, 'epochs': 14, 'weight_decay': 0.000494716146267866, 'dropout': 0.4127778276933724, 'embed_dim': 76, 'stem_channels': 44, 'num_heads': 2, 'num_layers': 1, 'mlp_ratio': 4, 'kernel_size': 7, 'stride1': 2, 'stride2': 3, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 3.40173043803309, 'label_smoothing': 0.061154746679415985, 'grad_clip_norm': 1.9890657063781751, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 27878, 'model_size_validation': 'PASS'}
2025-09-20 02:32:33,689 - INFO - _models.training_function_executor - BO Objective: base=0.8009, size_penalty=0.0000, final=0.8009
2025-09-20 02:32:33,689 - INFO - _models.training_function_executor - Model size: 27,878 parameters (PASS 256K limit)
2025-09-20 02:32:33,689 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 64.318s
2025-09-20 02:32:33,932 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8009
2025-09-20 02:32:33,932 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.243s
2025-09-20 02:32:33,932 - INFO - bo.run_bo - Recorded observation #5: hparams={'lr': 0.0006868638673706484, 'batch_size': np.int64(256), 'epochs': np.int64(14), 'weight_decay': 0.000494716146267866, 'dropout': 0.4127778276933724, 'embed_dim': np.int64(76), 'stem_channels': np.int64(44), 'num_heads': np.int64(2), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'kernel_size': np.int64(7), 'stride1': np.int64(2), 'stride2': np.int64(3), 'use_cls_token': np.True_, 'use_focal': np.False_, 'focal_gamma': 3.40173043803309, 'label_smoothing': 0.061154746679415985, 'grad_clip_norm': 1.9890657063781751, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.8009
2025-09-20 02:32:33,932 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'lr': 0.0006868638673706484, 'batch_size': np.int64(256), 'epochs': np.int64(14), 'weight_decay': 0.000494716146267866, 'dropout': 0.4127778276933724, 'embed_dim': np.int64(76), 'stem_channels': np.int64(44), 'num_heads': np.int64(2), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(4), 'kernel_size': np.int64(7), 'stride1': np.int64(2), 'stride2': np.int64(3), 'use_cls_token': np.True_, 'use_focal': np.False_, 'focal_gamma': 3.40173043803309, 'label_smoothing': 0.061154746679415985, 'grad_clip_norm': 1.9890657063781751, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.8009
2025-09-20 02:32:33,932 - INFO - bo.run_bo - üîçBO Trial 6: Using RF surrogate + Expected Improvement
2025-09-20 02:32:33,932 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-20 02:32:33,933 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 02:32:33,933 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 02:32:33,933 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0019138538912211584, 'batch_size': 256, 'epochs': 44, 'weight_decay': 2.2914147877970487e-05, 'dropout': 0.32741776410701573, 'embed_dim': 62, 'stem_channels': 27, 'num_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'kernel_size': 11, 'stride1': 2, 'stride2': 1, 'use_cls_token': False, 'use_focal': True, 'focal_gamma': 3.7846513805210065, 'label_smoothing': 0.016747318751769283, 'grad_clip_norm': 0.62861666365858, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 02:32:33,935 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0019138538912211584, 'batch_size': 256, 'epochs': 44, 'weight_decay': 2.2914147877970487e-05, 'dropout': 0.32741776410701573, 'embed_dim': 62, 'stem_channels': 27, 'num_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'kernel_size': 11, 'stride1': 2, 'stride2': 1, 'use_cls_token': False, 'use_focal': True, 'focal_gamma': 3.7846513805210065, 'label_smoothing': 0.016747318751769283, 'grad_clip_norm': 0.62861666365858, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 04:38:14,679 - INFO - _models.training_function_executor - Model parameter count: 31,989
2025-09-20 04:38:14,679 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.5279501783149951, 0.20198464473080616, 0.15362137540840298, 0.12128620582192826, 0.11507746865248432, 0.09419077872608601, 0.08011149965770357, 0.08309818315525101, 0.07136156476299191, 0.07076510795972399, 0.06138878507179648, 0.05897488749459828, 0.0643894981291166, 0.05335215963094638, 0.05348647908332258, 0.04624478124775166, 0.046326898328912146, 0.04560596553673004, 0.04648493679946866, 0.04001500692932531, 0.04447867465448217, 0.04327316068668507, 0.03749676393352467, 0.0373305472983727, 0.0328887938189454, 0.04009455941996889, 0.034770953511175456, 0.03072496324048677, 0.02861519686880715, 0.034007525825820735, 0.02720472999604205, 0.02606392562622391, 0.02503175194634374, 0.023506229183223596, 0.020621428238534172, 0.02161093014677047, 0.025278441085903616, 0.026184001075643105, 0.021460132857854832, 0.022438316415538288, 0.024595567921464837, 0.020594477545956946, 0.019307390437181368, 0.016620757388370745], 'val_losses': [0.2960739088247257, 0.1633938728308312, 0.14646109670081, 0.12930160390030732, 0.14198535013004093, 0.15091580844921973, 0.11323857503985431, 0.11609192221977434, 0.10658067582445654, 0.10617588941518273, 0.12161601284850966, 0.11099473655170079, 0.1000926409443562, 0.09994698148655774, 0.13005516898468356, 0.14913977452434232, 0.13829516715703274, 0.15117903685084658, 0.1577657789687923, 0.1649724536224299, 0.11665386368375963, 0.10971605657658955, 0.14766752728243038, 0.1619707298766304, 0.1385702029657684, 0.1230630458022593, 0.13779015761584573, 0.1489092976742043, 0.13049147946957393, 0.1566085416307977, 0.12814871891713206, 0.1332775536419407, 0.10630449621157882, 0.1476607131279338, 0.16381169700414655, 0.12705781210355413, 0.1527571340559288, 0.14358332621450234, 0.16797870645198554, 0.15466048543768893, 0.1430720228989712, 0.12563780008987827, 0.1811123203511968, 0.142385457570966], 'val_acc': [0.3321639771474391, 0.3380775784303899, 0.2931743008920517, 0.4579532925729177, 0.4834118472486719, 0.40583341685877516, 0.5274130500150346, 0.49313420867996394, 0.47178510574320937, 0.5030570311716949, 0.5723163275533728, 0.47789916808659916, 0.6413751628746116, 0.6972035682068758, 0.6991079482810464, 0.5785306204269821, 0.710734689786509, 0.7461160669539942, 0.6021850255587852, 0.730379873709532, 0.6587150446025859, 0.7605492633056029, 0.637466172196051, 0.6356620226520998, 0.6421770071163676, 0.6957001102535832, 0.783602285256089, 0.7048210885035582, 0.7074270822892653, 0.6952991881327052, 0.6941966523002907, 0.7418061541545555, 0.7981357121379172, 0.8344191640773779, 0.8071564598576726, 0.6996091009321439, 0.7596471885336273, 0.7224616618221911, 0.8624837125388394, 0.794126490929137, 0.7886138117670642, 0.750425979753433, 0.8025458554675754, 0.7796932945775283], 'num_parameters': 75974, 'quantization_bits': 8, 'quantized_on': 'Linear (Transformer + head) via dynamic quantization', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0019138538912211584, 'batch_size': 256, 'epochs': 44, 'weight_decay': 2.2914147877970487e-05, 'dropout': 0.32741776410701573, 'embed_dim': 62, 'stem_channels': 27, 'num_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'kernel_size': 11, 'stride1': 2, 'stride2': 1, 'use_cls_token': False, 'use_focal': True, 'focal_gamma': 3.7846513805210065, 'label_smoothing': 0.016747318751769283, 'grad_clip_norm': 0.62861666365858, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 31989, 'model_size_validation': 'PASS'}
2025-09-20 04:38:14,679 - INFO - _models.training_function_executor - BO Objective: base=0.7797, size_penalty=0.0000, final=0.7797
2025-09-20 04:38:14,679 - INFO - _models.training_function_executor - Model size: 31,989 parameters (PASS 256K limit)
2025-09-20 04:38:14,679 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 7540.746s
2025-09-20 04:38:14,905 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7797
2025-09-20 04:38:14,905 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.225s
2025-09-20 04:38:14,905 - INFO - bo.run_bo - Recorded observation #6: hparams={'lr': 0.0019138538912211584, 'batch_size': np.int64(256), 'epochs': np.int64(44), 'weight_decay': 2.2914147877970487e-05, 'dropout': 0.32741776410701573, 'embed_dim': np.int64(62), 'stem_channels': np.int64(27), 'num_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'kernel_size': np.int64(11), 'stride1': np.int64(2), 'stride2': np.int64(1), 'use_cls_token': np.False_, 'use_focal': np.True_, 'focal_gamma': 3.7846513805210065, 'label_smoothing': 0.016747318751769283, 'grad_clip_norm': 0.62861666365858, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.7797
2025-09-20 04:38:14,905 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'lr': 0.0019138538912211584, 'batch_size': np.int64(256), 'epochs': np.int64(44), 'weight_decay': 2.2914147877970487e-05, 'dropout': 0.32741776410701573, 'embed_dim': np.int64(62), 'stem_channels': np.int64(27), 'num_heads': np.int64(4), 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'kernel_size': np.int64(11), 'stride1': np.int64(2), 'stride2': np.int64(1), 'use_cls_token': np.False_, 'use_focal': np.True_, 'focal_gamma': 3.7846513805210065, 'label_smoothing': 0.016747318751769283, 'grad_clip_norm': 0.62861666365858, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.7797
2025-09-20 04:38:14,905 - INFO - bo.run_bo - üîçBO Trial 7: Using RF surrogate + Expected Improvement
2025-09-20 04:38:14,905 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-20 04:38:14,906 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 04:38:14,906 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 04:38:14,906 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0019640046317910944, 'batch_size': 32, 'epochs': 5, 'weight_decay': 0.0005862886643402355, 'dropout': 0.11049840378007522, 'embed_dim': 76, 'stem_channels': 31, 'num_heads': 2, 'num_layers': 1, 'mlp_ratio': 2, 'kernel_size': 7, 'stride1': 3, 'stride2': 4, 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 2.3723111649925546, 'label_smoothing': 0.07303843133772002, 'grad_clip_norm': 0.501883742737317, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-20 04:38:14,907 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0019640046317910944, 'batch_size': 32, 'epochs': 5, 'weight_decay': 0.0005862886643402355, 'dropout': 0.11049840378007522, 'embed_dim': 76, 'stem_channels': 31, 'num_heads': 2, 'num_layers': 1, 'mlp_ratio': 2, 'kernel_size': 7, 'stride1': 3, 'stride2': 4, 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 2.3723111649925546, 'label_smoothing': 0.07303843133772002, 'grad_clip_norm': 0.501883742737317, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}
2025-09-20 04:38:44,865 - INFO - _models.training_function_executor - Model parameter count: 50,438
2025-09-20 04:38:44,865 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [0.622910815644367, 0.4359877658388021, 0.3613113050607467, 0.3200465958048653, 0.28905500506955323], 'val_losses': [0.3749629078897599, 0.37680392328063916, 0.31439242231424075, 0.2574325456996504, 0.2840698512400425], 'val_acc': [0.35030570311716946, 0.5037586448832314, 0.5794326951989576, 0.5853462964819084, 0.5986769570011026], 'num_parameters': 50438, 'quantization_bits': 32, 'quantized_on': 'none', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0019640046317910944, 'batch_size': 32, 'epochs': 5, 'weight_decay': 0.0005862886643402355, 'dropout': 0.11049840378007522, 'embed_dim': 76, 'stem_channels': 31, 'num_heads': 2, 'num_layers': 1, 'mlp_ratio': 2, 'kernel_size': 7, 'stride1': 3, 'stride2': 4, 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 2.3723111649925546, 'label_smoothing': 0.07303843133772002, 'grad_clip_norm': 0.501883742737317, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': True}, 'model_parameter_count': 50438, 'model_size_validation': 'PASS'}
2025-09-20 04:38:44,865 - INFO - _models.training_function_executor - BO Objective: base=0.5987, size_penalty=0.0000, final=0.5987
2025-09-20 04:38:44,865 - INFO - _models.training_function_executor - Model size: 50,438 parameters (PASS 256K limit)
2025-09-20 04:38:44,865 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 29.959s
2025-09-20 04:38:45,087 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.5987
2025-09-20 04:38:45,087 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.222s
2025-09-20 04:38:45,087 - INFO - bo.run_bo - Recorded observation #7: hparams={'lr': 0.0019640046317910944, 'batch_size': np.int64(32), 'epochs': np.int64(5), 'weight_decay': 0.0005862886643402355, 'dropout': 0.11049840378007522, 'embed_dim': np.int64(76), 'stem_channels': np.int64(31), 'num_heads': np.int64(2), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(2), 'kernel_size': np.int64(7), 'stride1': np.int64(3), 'stride2': np.int64(4), 'use_cls_token': np.True_, 'use_focal': np.True_, 'focal_gamma': 2.3723111649925546, 'label_smoothing': 0.07303843133772002, 'grad_clip_norm': 0.501883742737317, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.5987
2025-09-20 04:38:45,087 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 7: {'lr': 0.0019640046317910944, 'batch_size': np.int64(32), 'epochs': np.int64(5), 'weight_decay': 0.0005862886643402355, 'dropout': 0.11049840378007522, 'embed_dim': np.int64(76), 'stem_channels': np.int64(31), 'num_heads': np.int64(2), 'num_layers': np.int64(1), 'mlp_ratio': np.int64(2), 'kernel_size': np.int64(7), 'stride1': np.int64(3), 'stride2': np.int64(4), 'use_cls_token': np.True_, 'use_focal': np.True_, 'focal_gamma': 2.3723111649925546, 'label_smoothing': 0.07303843133772002, 'grad_clip_norm': 0.501883742737317, 'quantization_bits': np.int64(32), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.5987
2025-09-20 04:38:45,088 - INFO - bo.run_bo - üîçBO Trial 8: Using RF surrogate + Expected Improvement
2025-09-20 04:38:45,088 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 04:38:45,088 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 04:38:45,088 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 04:38:45,088 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.00018845213009362156, 'batch_size': 32, 'epochs': 5, 'weight_decay': 0.00010542171042829973, 'dropout': 0.13018647107255663, 'embed_dim': 35, 'stem_channels': 17, 'num_heads': 2, 'num_layers': 3, 'mlp_ratio': 3, 'kernel_size': 9, 'stride1': 2, 'stride2': 1, 'use_cls_token': False, 'use_focal': False, 'focal_gamma': 2.359678385776121, 'label_smoothing': 0.0237835277080688, 'grad_clip_norm': 1.1054959860287736, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-20 04:38:45,090 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.00018845213009362156, 'batch_size': 32, 'epochs': 5, 'weight_decay': 0.00010542171042829973, 'dropout': 0.13018647107255663, 'embed_dim': 35, 'stem_channels': 17, 'num_heads': 2, 'num_layers': 3, 'mlp_ratio': 3, 'kernel_size': 9, 'stride1': 2, 'stride2': 1, 'use_cls_token': False, 'use_focal': False, 'focal_gamma': 2.359678385776121, 'label_smoothing': 0.0237835277080688, 'grad_clip_norm': 1.1054959860287736, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-20 04:41:28,205 - INFO - _models.training_function_executor - Model parameter count: 37,006
2025-09-20 04:41:28,205 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.4105914209303898, 1.0385257765686597, 0.9046254738531977, 0.8403431564784566, 0.7943960156503829], 'val_losses': [1.1798401313230964, 0.8897739081866901, 0.823453567776494, 0.7729095194518262, 0.7440165916353784], 'val_acc': [0.5793324646687381, 0.8320136313521098, 0.8093615315225018, 0.8537636564097424, 0.8664929337476195], 'num_parameters': 37006, 'quantization_bits': 8, 'quantized_on': 'none', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.00018845213009362156, 'batch_size': 32, 'epochs': 5, 'weight_decay': 0.00010542171042829973, 'dropout': 0.13018647107255663, 'embed_dim': 35, 'stem_channels': 17, 'num_heads': 2, 'num_layers': 3, 'mlp_ratio': 3, 'kernel_size': 9, 'stride1': 2, 'stride2': 1, 'use_cls_token': False, 'use_focal': False, 'focal_gamma': 2.359678385776121, 'label_smoothing': 0.0237835277080688, 'grad_clip_norm': 1.1054959860287736, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 37006, 'model_size_validation': 'PASS'}
2025-09-20 04:41:28,205 - INFO - _models.training_function_executor - BO Objective: base=0.8665, size_penalty=0.0000, final=0.8665
2025-09-20 04:41:28,205 - INFO - _models.training_function_executor - Model size: 37,006 parameters (PASS 256K limit)
2025-09-20 04:41:28,205 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 163.117s
2025-09-20 04:41:28,440 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.8665
2025-09-20 04:41:28,440 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.235s
2025-09-20 04:41:28,440 - INFO - bo.run_bo - Recorded observation #8: hparams={'lr': 0.00018845213009362156, 'batch_size': np.int64(32), 'epochs': np.int64(5), 'weight_decay': 0.00010542171042829973, 'dropout': 0.13018647107255663, 'embed_dim': np.int64(35), 'stem_channels': np.int64(17), 'num_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'kernel_size': np.int64(9), 'stride1': np.int64(2), 'stride2': np.int64(1), 'use_cls_token': np.False_, 'use_focal': np.False_, 'focal_gamma': 2.359678385776121, 'label_smoothing': 0.0237835277080688, 'grad_clip_norm': 1.1054959860287736, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.8665
2025-09-20 04:41:28,440 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 8: {'lr': 0.00018845213009362156, 'batch_size': np.int64(32), 'epochs': np.int64(5), 'weight_decay': 0.00010542171042829973, 'dropout': 0.13018647107255663, 'embed_dim': np.int64(35), 'stem_channels': np.int64(17), 'num_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(3), 'kernel_size': np.int64(9), 'stride1': np.int64(2), 'stride2': np.int64(1), 'use_cls_token': np.False_, 'use_focal': np.False_, 'focal_gamma': 2.359678385776121, 'label_smoothing': 0.0237835277080688, 'grad_clip_norm': 1.1054959860287736, 'quantization_bits': np.int64(8), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.8665
2025-09-20 04:41:28,441 - INFO - bo.run_bo - üîçBO Trial 9: Using RF surrogate + Expected Improvement
2025-09-20 04:41:28,441 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-20 04:41:28,441 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 04:41:28,441 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 04:41:28,441 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 1.8974386927325864e-05, 'batch_size': 64, 'epochs': 7, 'weight_decay': 7.170163616701984e-05, 'dropout': 0.3982117114452452, 'embed_dim': 70, 'stem_channels': 56, 'num_heads': 4, 'num_layers': 3, 'mlp_ratio': 2, 'kernel_size': 11, 'stride1': 1, 'stride2': 2, 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 2.2799217418815054, 'label_smoothing': 0.01868879434990455, 'grad_clip_norm': 0.9698162423719281, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 04:41:28,444 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 1.8974386927325864e-05, 'batch_size': 64, 'epochs': 7, 'weight_decay': 7.170163616701984e-05, 'dropout': 0.3982117114452452, 'embed_dim': 70, 'stem_channels': 56, 'num_heads': 4, 'num_layers': 3, 'mlp_ratio': 2, 'kernel_size': 11, 'stride1': 1, 'stride2': 2, 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 2.2799217418815054, 'label_smoothing': 0.01868879434990455, 'grad_clip_norm': 0.9698162423719281, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}
2025-09-20 04:47:34,003 - INFO - _models.training_function_executor - Model parameter count: 118,559
2025-09-20 04:47:34,003 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.2500811323893959, 0.9892032905492959, 0.8394201006904639, 0.7507936996560911, 0.6988477429439282, 0.6278129027620161, 0.5615998138737086], 'val_losses': [0.9461217709315066, 0.7568808062241218, 0.6503695365845399, 0.6076062432215569, 0.554997470388554, 0.4960434838712747, 0.4433407412599774], 'val_acc': [0.13360729678259997, 0.16668337175503659, 0.16337576425779293, 0.1876315525709131, 0.19204169590057132, 0.22662122882630048, 0.24887240653503057], 'num_parameters': 118559, 'quantization_bits': 32, 'quantized_on': 'none', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 1.8974386927325864e-05, 'batch_size': 64, 'epochs': 7, 'weight_decay': 7.170163616701984e-05, 'dropout': 0.3982117114452452, 'embed_dim': 70, 'stem_channels': 56, 'num_heads': 4, 'num_layers': 3, 'mlp_ratio': 2, 'kernel_size': 11, 'stride1': 1, 'stride2': 2, 'use_cls_token': True, 'use_focal': True, 'focal_gamma': 2.2799217418815054, 'label_smoothing': 0.01868879434990455, 'grad_clip_norm': 0.9698162423719281, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': True}, 'model_parameter_count': 118559, 'model_size_validation': 'PASS'}
2025-09-20 04:47:34,003 - INFO - _models.training_function_executor - BO Objective: base=0.2489, size_penalty=0.0000, final=0.2489
2025-09-20 04:47:34,004 - INFO - _models.training_function_executor - Model size: 118,559 parameters (PASS 256K limit)
2025-09-20 04:47:34,004 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 365.563s
2025-09-20 04:47:34,254 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.2489
2025-09-20 04:47:34,254 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.250s
2025-09-20 04:47:34,254 - INFO - bo.run_bo - Recorded observation #9: hparams={'lr': 1.8974386927325864e-05, 'batch_size': np.int64(64), 'epochs': np.int64(7), 'weight_decay': 7.170163616701984e-05, 'dropout': 0.3982117114452452, 'embed_dim': np.int64(70), 'stem_channels': np.int64(56), 'num_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'kernel_size': np.int64(11), 'stride1': np.int64(1), 'stride2': np.int64(2), 'use_cls_token': np.True_, 'use_focal': np.True_, 'focal_gamma': 2.2799217418815054, 'label_smoothing': 0.01868879434990455, 'grad_clip_norm': 0.9698162423719281, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.2489
2025-09-20 04:47:34,254 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 9: {'lr': 1.8974386927325864e-05, 'batch_size': np.int64(64), 'epochs': np.int64(7), 'weight_decay': 7.170163616701984e-05, 'dropout': 0.3982117114452452, 'embed_dim': np.int64(70), 'stem_channels': np.int64(56), 'num_heads': np.int64(4), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(2), 'kernel_size': np.int64(11), 'stride1': np.int64(1), 'stride2': np.int64(2), 'use_cls_token': np.True_, 'use_focal': np.True_, 'focal_gamma': 2.2799217418815054, 'label_smoothing': 0.01868879434990455, 'grad_clip_norm': 0.9698162423719281, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.2489
2025-09-20 04:47:34,255 - INFO - bo.run_bo - üîçBO Trial 10: Using RF surrogate + Expected Improvement
2025-09-20 04:47:34,255 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-20 04:47:34,255 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 04:47:34,255 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 04:47:34,255 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0005666447237811246, 'batch_size': 128, 'epochs': 8, 'weight_decay': 3.7881882381055686e-06, 'dropout': 0.2224934323351413, 'embed_dim': 58, 'stem_channels': 39, 'num_heads': 2, 'num_layers': 3, 'mlp_ratio': 4, 'kernel_size': 11, 'stride1': 4, 'stride2': 4, 'use_cls_token': False, 'use_focal': False, 'focal_gamma': 1.9161324757978346, 'label_smoothing': 0.08622294453256109, 'grad_clip_norm': 1.2476872661713898, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-20 04:47:34,258 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0005666447237811246, 'batch_size': 128, 'epochs': 8, 'weight_decay': 3.7881882381055686e-06, 'dropout': 0.2224934323351413, 'embed_dim': 58, 'stem_channels': 39, 'num_heads': 2, 'num_layers': 3, 'mlp_ratio': 4, 'kernel_size': 11, 'stride1': 4, 'stride2': 4, 'use_cls_token': False, 'use_focal': False, 'focal_gamma': 1.9161324757978346, 'label_smoothing': 0.08622294453256109, 'grad_clip_norm': 1.2476872661713898, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}
2025-09-20 04:48:07,041 - INFO - _models.training_function_executor - Model parameter count: 126,732
2025-09-20 04:48:07,041 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.6459277827164223, 1.359947746050482, 1.2953438951955762, 1.2653148269882752, 1.2473165932997954, 1.223144753608306, 1.2176140891903386, 1.2071897865010914], 'val_losses': [1.4343156309037957, 1.3244409562232584, 1.2799378899112306, 1.274398630721853, 1.2647646991525754, 1.2691493773252487, 1.262059010471934, 1.2714822728205888], 'val_acc': [0.5059637165480605, 0.5365340282650095, 0.7197554375062644, 0.6994086398717049, 0.7554375062644081, 0.7503257492232134, 0.7994387090307707, 0.7221609702315325], 'num_parameters': 126732, 'quantization_bits': 32, 'quantized_on': 'none', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0005666447237811246, 'batch_size': 128, 'epochs': 8, 'weight_decay': 3.7881882381055686e-06, 'dropout': 0.2224934323351413, 'embed_dim': 58, 'stem_channels': 39, 'num_heads': 2, 'num_layers': 3, 'mlp_ratio': 4, 'kernel_size': 11, 'stride1': 4, 'stride2': 4, 'use_cls_token': False, 'use_focal': False, 'focal_gamma': 1.9161324757978346, 'label_smoothing': 0.08622294453256109, 'grad_clip_norm': 1.2476872661713898, 'quantization_bits': 32, 'quantize_weights': False, 'quantize_activations': False}, 'model_parameter_count': 126732, 'model_size_validation': 'PASS'}
2025-09-20 04:48:07,041 - INFO - _models.training_function_executor - BO Objective: base=0.7222, size_penalty=0.0000, final=0.7222
2025-09-20 04:48:07,041 - INFO - _models.training_function_executor - Model size: 126,732 parameters (PASS 256K limit)
2025-09-20 04:48:07,041 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) took 32.786s
2025-09-20 04:48:07,286 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.7222
2025-09-20 04:48:07,286 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.245s
2025-09-20 04:48:07,286 - INFO - bo.run_bo - Recorded observation #10: hparams={'lr': 0.0005666447237811246, 'batch_size': np.int64(128), 'epochs': np.int64(8), 'weight_decay': 3.7881882381055686e-06, 'dropout': 0.2224934323351413, 'embed_dim': np.int64(58), 'stem_channels': np.int64(39), 'num_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'kernel_size': np.int64(11), 'stride1': np.int64(4), 'stride2': np.int64(4), 'use_cls_token': np.False_, 'use_focal': np.False_, 'focal_gamma': 1.9161324757978346, 'label_smoothing': 0.08622294453256109, 'grad_clip_norm': 1.2476872661713898, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_}, value=0.7222
2025-09-20 04:48:07,286 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 10: {'lr': 0.0005666447237811246, 'batch_size': np.int64(128), 'epochs': np.int64(8), 'weight_decay': 3.7881882381055686e-06, 'dropout': 0.2224934323351413, 'embed_dim': np.int64(58), 'stem_channels': np.int64(39), 'num_heads': np.int64(2), 'num_layers': np.int64(3), 'mlp_ratio': np.int64(4), 'kernel_size': np.int64(11), 'stride1': np.int64(4), 'stride2': np.int64(4), 'use_cls_token': np.False_, 'use_focal': np.False_, 'focal_gamma': 1.9161324757978346, 'label_smoothing': 0.08622294453256109, 'grad_clip_norm': 1.2476872661713898, 'quantization_bits': np.int64(32), 'quantize_weights': np.False_, 'quantize_activations': np.False_} -> 0.7222
2025-09-20 04:48:07,287 - INFO - evaluation.code_generation_pipeline_orchestrator - BO completed - Best score: 0.8940
2025-09-20 04:48:07,287 - INFO - evaluation.code_generation_pipeline_orchestrator - Best params: {'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': np.int64(12), 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': np.int64(67), 'stem_channels': np.int64(55), 'num_heads': 4, 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'kernel_size': 5, 'stride1': np.int64(4), 'stride2': np.int64(2), 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 04:48:07,288 - INFO - visualization - Generating BO visualization charts with 10 trials...
2025-09-20 04:48:09,417 - INFO - visualization - BO summary saved to: charts\BO_STCT_ECG_TinyTransformer_20250920_044807\bo_summary.txt
2025-09-20 04:48:09,418 - INFO - visualization - BO charts saved to: charts\BO_STCT_ECG_TinyTransformer_20250920_044807
2025-09-20 04:48:09,418 - INFO - evaluation.code_generation_pipeline_orchestrator - üìä BO charts saved to: charts\BO_STCT_ECG_TinyTransformer_20250920_044807
2025-09-20 04:48:09,507 - INFO - evaluation.code_generation_pipeline_orchestrator - üöÄ STEP 4: Final Training Execution
2025-09-20 04:48:09,507 - INFO - evaluation.code_generation_pipeline_orchestrator - Using centralized splits - Train: (39904, 1000, 2), Val: (9977, 1000, 2), Test: (12471, 1000, 2)
2025-09-20 04:48:09,946 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 04:48:09,965 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 04:48:09,983 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-20 04:48:09,994 - INFO - _models.training_function_executor - Loaded training function: STCT_ECG_TinyTransformer
2025-09-20 04:48:09,994 - INFO - _models.training_function_executor - Reasoning: No reasoning provided
2025-09-20 04:48:09,994 - INFO - evaluation.code_generation_pipeline_orchestrator - Executing final training with optimized params: {'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': np.int64(12), 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': np.int64(67), 'stem_channels': np.int64(55), 'num_heads': 4, 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'kernel_size': 5, 'stride1': np.int64(4), 'stride2': np.int64(2), 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 04:48:09,994 - INFO - _models.training_function_executor - Using device: cuda
2025-09-20 04:48:10,026 - INFO - _models.training_function_executor - Executing training function: STCT_ECG_TinyTransformer
2025-09-20 04:48:10,026 - INFO - _models.training_function_executor - Hyperparameters: {'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': np.int64(12), 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': np.int64(67), 'stem_channels': np.int64(55), 'num_heads': 4, 'num_layers': np.int64(2), 'mlp_ratio': np.int64(3), 'kernel_size': 5, 'stride1': np.int64(4), 'stride2': np.int64(2), 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 04:48:10,027 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': 12, 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': 67, 'stem_channels': 55, 'num_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'kernel_size': 5, 'stride1': 4, 'stride2': 2, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}
2025-09-20 04:50:13,812 - INFO - _models.training_function_executor - Model parameter count: 88,116
2025-09-20 04:50:13,812 - INFO - _models.training_function_executor - Training completed successfully: {'train_losses': [1.35362940161536, 0.9962104291273483, 0.9254414523804579, 0.8865252792500646, 0.8685576006614789, 0.8425760433591646, 0.8367410258665597, 0.8198556136683454, 0.8080321050397663, 0.8009025149089198, 0.7954523387486779, 0.7910318908064291], 'val_losses': [0.9862201180381122, 0.9055376261117237, 0.8912137979446462, 0.8779049301429444, 0.8459536712752969, 0.8832016461141915, 0.9087233317571042, 0.8816767927161006, 0.8442570222419405, 0.8343363785249049, 0.8554494390509656, 0.8801925987665383], 'val_acc': [0.7418061541545555, 0.8006414753934048, 0.7141425278139721, 0.8293074070361832, 0.8758143730580334, 0.8539641174701814, 0.8108649894757943, 0.9486819685276136, 0.9133005913601283, 0.9146035882529818, 0.9205171895359326, 0.9212188032474692], 'num_parameters': 88116, 'quantization_bits': 32, 'quantized_on': 'none', 'model_name': 'STCT_ECG_TinyTransformer', 'training_function_source': 'ai_generated', 'hyperparameters_used': {'lr': 0.0011214774784159465, 'batch_size': 128, 'epochs': 12, 'weight_decay': 1.2681352169084594e-06, 'dropout': 0.45466020103939114, 'embed_dim': 67, 'stem_channels': 55, 'num_heads': 4, 'num_layers': 2, 'mlp_ratio': 3, 'kernel_size': 5, 'stride1': 4, 'stride2': 2, 'use_cls_token': True, 'use_focal': False, 'focal_gamma': 2.349262400109297, 'label_smoothing': 0.03951502360018145, 'grad_clip_norm': 1.8533177315875888, 'quantization_bits': 32, 'quantize_weights': True, 'quantize_activations': False}, 'model_parameter_count': 88116, 'model_size_validation': 'PASS'}
2025-09-20 04:50:13,846 - ERROR - __main__ - Unhandled exception: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA___conv_depthwise2d)
Traceback (most recent call last):
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 484, in <module>
    processed_real_data = process_real_data()
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 342, in process_real_data
    result = process_data_with_ai_enhanced_evaluation(
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 136, in process_data_with_ai_enhanced_evaluation
    result = train_with_iterative_selection(data, labels, device=device, **kwargs)
  File "D:\_A\GPT_research\ml_pipeline\main.py", line 85, in train_with_iterative_selection
    best_model, pipeline_results = orchestrator.run_complete_pipeline(
  File "D:\_A\GPT_research\ml_pipeline\evaluation\code_generation_pipeline_orchestrator.py", line 98, in run_complete_pipeline
    final_model, training_results = self._execute_final_training(
  File "D:\_A\GPT_research\ml_pipeline\evaluation\code_generation_pipeline_orchestrator.py", line 421, in _execute_final_training
    final_metrics = evaluate_model(trained_model, test_loader, device)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "D:\_A\GPT_research\ml_pipeline\evaluation\evaluate.py", line 27, in evaluate_model
    logits = model(X) if lengths is None else model(X, lengths=lengths)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "<string>", line 92, in forward
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\22447\.conda\envs\GPT\lib\site-packages\torch\nn\modules\conv.py", line 370, in _conv_forward
    return F.conv1d(
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA___conv_depthwise2d)

