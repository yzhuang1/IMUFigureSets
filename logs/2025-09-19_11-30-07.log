2025-09-19 11:30:08,958 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-19 11:30:09,293 - INFO - __main__ - Logging system initialized successfully
2025-09-19 11:30:09,293 - INFO - __main__ - Starting AI-Enhanced Machine Learning Pipeline
2025-09-19 11:30:09,294 - INFO - __main__ - Starting real data processing from data/ directory
2025-09-19 11:30:09,295 - INFO - __main__ - Found 5 data files: ['mitdb_ecg_sample.csv', 'X.npy', 'y.npy', 'mitdb_ecg_data.npz', 'mitdb_metadata.json']
2025-09-19 11:30:09,296 - INFO - __main__ - Found NPY files, prioritizing them over CSV
2025-09-19 11:30:09,296 - INFO - __main__ - Attempting to load: X.npy
2025-09-19 11:30:09,425 - INFO - __main__ - Successfully loaded NPY data: X(62352, 1000, 2), y(62352,)
2025-09-19 11:30:09,515 - INFO - __main__ - Data loaded successfully: X.npy + y.npy, shape: (62352, 1000, 2), device: cuda
2025-09-19 11:30:09,515 - INFO - __main__ - Starting AI-enhanced data processing with new pipeline flow...
2025-09-19 11:30:09,516 - INFO - __main__ - Starting AI-enhanced training (single attempt, fail fast)
2025-09-19 11:30:09,516 - INFO - __main__ - Flow: Code Generation ‚Üí BO ‚Üí Evaluation
2025-09-19 11:30:09,518 - INFO - adapters.universal_converter - Data conversion: numpy_array -> torch_tensor
2025-09-19 11:30:09,518 - INFO - __main__ - Data profile: {'data_type': 'torch_tensor', 'shape': (62352, 1000, 2), 'dtype': 'float32', 'feature_count': 2, 'sample_count': 62352, 'is_sequence': True, 'is_image': False, 'is_tabular': False, 'has_labels': True, 'label_count': 5, 'sequence_lengths': None, 'channels': None, 'height': None, 'width': None, 'metadata': {}}
2025-09-19 11:30:09,518 - INFO - evaluation.code_generation_pipeline_orchestrator - Code generation pipeline orchestrator initialized (no retry logic)
2025-09-19 11:30:09,518 - INFO - evaluation.code_generation_pipeline_orchestrator - Starting code generation pipeline execution (single attempt, fail fast)
2025-09-19 11:30:09,518 - INFO - evaluation.code_generation_pipeline_orchestrator - Flow: AI Code Generation ‚Üí JSON Storage ‚Üí BO ‚Üí Training Execution ‚Üí Evaluation
2025-09-19 11:30:09,518 - INFO - evaluation.code_generation_pipeline_orchestrator - Creating centralized data splits to prevent data leakage
2025-09-19 11:30:09,518 - INFO - data_splitting - Creating centralized data splits with test_size=0.2, val_size=0.2
2025-09-19 11:30:09,518 - INFO - data_splitting - Input data shape: X=(62352, 1000, 2), y=(62352,)
2025-09-19 11:30:09,518 - INFO - data_splitting - Class distribution: [44897  9551  1201  1239  5464]
2025-09-19 11:30:09,726 - INFO - data_splitting - Computed class weights: {np.int64(0): np.float64(0.27775728256708315), np.int64(1): np.float64(1.3057591623036648), np.int64(2): np.float64(10.37815344603381), np.int64(3): np.float64(10.0640605296343), np.int64(4): np.float64(2.282184729768373)}
2025-09-19 11:30:09,726 - INFO - class_balancing - Class imbalance analysis:
2025-09-19 11:30:09,726 - INFO - class_balancing -   Strategy: severe_imbalance
2025-09-19 11:30:09,726 - INFO - class_balancing -   Imbalance ratio: 37.36
2025-09-19 11:30:09,726 - INFO - class_balancing -   Recommendations: Use weighted loss + weighted sampling, Consider Focal Loss, Evaluate with balanced metrics
2025-09-19 11:30:09,726 - INFO - data_splitting - Final splits - Train: 39904, Val: 9977, Test: 12471
2025-09-19 11:30:09,726 - INFO - data_splitting - Train class distribution: [28733  6112   769   793  3497]
2025-09-19 11:30:09,726 - INFO - data_splitting - Val class distribution: [7184 1529  192  198  874]
2025-09-19 11:30:09,726 - INFO - data_splitting - Test class distribution: [8980 1910  240  248 1093]
2025-09-19 11:30:09,726 - INFO - data_splitting - Recommended balancing strategy: severe_imbalance
2025-09-19 11:30:10,406 - INFO - data_splitting - Computed standardization stats - mean shape: torch.Size([1, 2]), std shape: torch.Size([1, 2])
2025-09-19 11:30:10,415 - INFO - evaluation.code_generation_pipeline_orchestrator - Computed standardization statistics from training data only
2025-09-19 11:30:10,415 - INFO - evaluation.code_generation_pipeline_orchestrator - 
============================================================
2025-09-19 11:30:10,415 - INFO - evaluation.code_generation_pipeline_orchestrator - PIPELINE EXECUTION (SINGLE ATTEMPT)
2025-09-19 11:30:10,415 - INFO - evaluation.code_generation_pipeline_orchestrator - ============================================================
2025-09-19 11:30:10,415 - INFO - evaluation.code_generation_pipeline_orchestrator - ü§ñ STEP 1: AI Training Code Generation
2025-09-19 11:30:10,415 - INFO - _models.ai_code_generator - Conducting literature review before code generation...
2025-09-19 11:33:45,994 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-19 11:33:46,088 - INFO - _models.ai_code_generator - Making API call to gpt-5
2025-09-19 11:33:46,089 - INFO - _models.ai_code_generator - Prompt length: 3027 characters
2025-09-19 11:33:46,089 - INFO - _models.ai_code_generator - Using API base URL: https://api.openai.com/v1
2025-09-19 11:33:46,089 - INFO - _models.ai_code_generator - Calling self.client.responses.create...
2025-09-19 11:33:46,089 - INFO - _models.ai_code_generator - Model parameter: gpt-5
2025-09-19 11:36:04,982 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-09-19 11:36:05,038 - INFO - _models.ai_code_generator - Successfully extracted response content
2025-09-19 11:36:05,038 - INFO - _models.ai_code_generator - AI generated training function: Tiny-ECG-Transformer-1D
2025-09-19 11:36:05,038 - INFO - _models.ai_code_generator - Confidence: 0.86
2025-09-19 11:36:05,038 - INFO - _models.ai_code_generator - Literature review informed code generation (confidence: 0.78)
2025-09-19 11:36:05,038 - INFO - evaluation.code_generation_pipeline_orchestrator - Generated training function: Tiny-ECG-Transformer-1D
2025-09-19 11:36:05,038 - INFO - evaluation.code_generation_pipeline_orchestrator - BO parameters: ['seed', 'lr', 'weight_decay', 'batch_size', 'epochs', 'embed_dim', 'num_heads', 'depth', 'mlp_ratio', 'dropout', 'attn_dropout', 'drop_path', 'patch_size', 'label_smoothing', 'use_focal', 'focal_gamma', 'max_grad_norm', 'aug_noise_std', 'aug_time_warp_ratio', 'aug_baseline_amp', 'aug_baseline_freq', 'aug_lead_dropout_prob', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-19 11:36:05,038 - INFO - evaluation.code_generation_pipeline_orchestrator - Confidence: 0.86
2025-09-19 11:36:05,040 - INFO - evaluation.code_generation_pipeline_orchestrator - üíæ STEP 2: Save Training Function to JSON
2025-09-19 11:36:05,043 - INFO - _models.ai_code_generator - Training function saved to: generated_training_functions\training_function_torch_tensor_Tiny-ECG-Transformer-1D_1758299765.json
2025-09-19 11:36:05,043 - INFO - evaluation.code_generation_pipeline_orchestrator - Training function saved to: generated_training_functions\training_function_torch_tensor_Tiny-ECG-Transformer-1D_1758299765.json
2025-09-19 11:36:05,046 - INFO - _models.training_function_executor - Training function validation passed
2025-09-19 11:36:05,047 - INFO - evaluation.code_generation_pipeline_orchestrator - üîç STEP 3: Bayesian Optimization
2025-09-19 11:36:05,047 - INFO - evaluation.code_generation_pipeline_orchestrator - Running BO for generated training function: Tiny-ECG-Transformer-1D
2025-09-19 11:36:05,047 - INFO - evaluation.code_generation_pipeline_orchestrator - üì¶ Installing dependencies for GPT-generated training code...
2025-09-19 11:36:05,051 - INFO - package_installer - üîç Analyzing GPT-generated code for package dependencies...
2025-09-19 11:36:05,057 - INFO - package_installer - Extracted imports from code: {'torch'}
2025-09-19 11:36:05,058 - INFO - package_installer - Available packages: {'torch'}
2025-09-19 11:36:05,058 - INFO - package_installer - Missing packages: set()
2025-09-19 11:36:05,058 - INFO - package_installer - ‚úÖ All required packages are already available
2025-09-19 11:36:05,058 - INFO - evaluation.code_generation_pipeline_orchestrator - ‚úÖ All dependencies installed successfully
2025-09-19 11:36:05,058 - INFO - evaluation.code_generation_pipeline_orchestrator - BO dataset size: 62352 samples (using full dataset)
2025-09-19 11:36:05,058 - INFO - evaluation.code_generation_pipeline_orchestrator - BO will optimize: ['seed', 'lr', 'weight_decay', 'batch_size', 'epochs', 'embed_dim', 'num_heads', 'depth', 'mlp_ratio', 'dropout', 'attn_dropout', 'drop_path', 'patch_size', 'label_smoothing', 'use_focal', 'focal_gamma', 'max_grad_norm', 'aug_noise_std', 'aug_time_warp_ratio', 'aug_baseline_amp', 'aug_baseline_freq', 'aug_lead_dropout_prob', 'quantization_bits', 'quantize_weights', 'quantize_activations']
2025-09-19 11:36:05,058 - INFO - _models.training_function_executor - GPU available: NVIDIA GeForce RTX 3070 Ti Laptop GPU
2025-09-19 11:36:05,058 - INFO - _models.training_function_executor - Using centralized data splits for BO objective
2025-09-19 11:36:05,385 - INFO - bo.run_bo - Converted GPT search space: 25 parameters
2025-09-19 11:36:05,385 - INFO - bo.run_bo - Using GPT-generated search space
2025-09-19 11:36:05,386 - INFO - bo.run_bo - Initialized Random Forest Bayesian Optimizer
2025-09-19 11:36:05,388 - INFO - bo.run_bo - üîçBO Trial 1: Initial random exploration
2025-09-19 11:36:05,388 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 11:36:05,388 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 11:36:05,388 - INFO - _models.training_function_executor - Executing training function: Tiny-ECG-Transformer-1D
2025-09-19 11:36:05,388 - INFO - _models.training_function_executor - Hyperparameters: {'seed': 860, 'lr': 3.1266912307463734e-05, 'weight_decay': 0.0013145103232150136, 'batch_size': 256, 'epochs': 23, 'embed_dim': 87, 'num_heads': 4, 'depth': 3, 'mlp_ratio': 1.5846173685406504, 'dropout': 0.21659963168004745, 'attn_dropout': 0.18771054180315008, 'drop_path': 7.787658410143285e-05, 'patch_size': 31, 'label_smoothing': 0.018340450985343384, 'use_focal': True, 'focal_gamma': 2.049512863264476, 'max_grad_norm': 2.216530591346368, 'aug_noise_std': 0.014561457009902098, 'aug_time_warp_ratio': 0.12237057894447592, 'aug_baseline_amp': 0.027898772130408374, 'aug_baseline_freq': 0.1814650918408482, 'aug_lead_dropout_prob': 0.10990855298810753, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-19 11:36:05,392 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'seed': 860, 'lr': 3.1266912307463734e-05, 'weight_decay': 0.0013145103232150136, 'batch_size': 256, 'epochs': 23, 'embed_dim': 87, 'num_heads': 4, 'depth': 3, 'mlp_ratio': 1.5846173685406504, 'dropout': 0.21659963168004745, 'attn_dropout': 0.18771054180315008, 'drop_path': 7.787658410143285e-05, 'patch_size': 31, 'label_smoothing': 0.018340450985343384, 'use_focal': True, 'focal_gamma': 2.049512863264476, 'max_grad_norm': 2.216530591346368, 'aug_noise_std': 0.014561457009902098, 'aug_time_warp_ratio': 0.12237057894447592, 'aug_baseline_amp': 0.027898772130408374, 'aug_baseline_freq': 0.1814650918408482, 'aug_lead_dropout_prob': 0.10990855298810753, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-19 11:39:43,697 - ERROR - _models.training_function_executor - Training execution failed: 'str' object has no attribute 'update'
2025-09-19 11:39:43,697 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# -------------------------...
2025-09-19 11:39:43,697 - ERROR - _models.training_function_executor - BO training objective failed: 'str' object has no attribute 'update'
2025-09-19 11:39:43,697 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 218.309s
2025-09-19 11:39:43,697 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 11:39:43,697 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-19 11:39:43,697 - INFO - bo.run_bo - Recorded observation #1: hparams={'seed': np.int64(860), 'lr': 3.1266912307463734e-05, 'weight_decay': 0.0013145103232150136, 'batch_size': 256, 'epochs': np.int64(23), 'embed_dim': np.int64(87), 'num_heads': np.int64(4), 'depth': np.int64(3), 'mlp_ratio': 1.5846173685406504, 'dropout': 0.21659963168004745, 'attn_dropout': 0.18771054180315008, 'drop_path': 7.787658410143285e-05, 'patch_size': np.int64(31), 'label_smoothing': 0.018340450985343384, 'use_focal': True, 'focal_gamma': 2.049512863264476, 'max_grad_norm': 2.216530591346368, 'aug_noise_std': 0.014561457009902098, 'aug_time_warp_ratio': 0.12237057894447592, 'aug_baseline_amp': 0.027898772130408374, 'aug_baseline_freq': 0.1814650918408482, 'aug_lead_dropout_prob': 0.10990855298810753, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}, value=0.0000
2025-09-19 11:39:43,697 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 1: {'seed': np.int64(860), 'lr': 3.1266912307463734e-05, 'weight_decay': 0.0013145103232150136, 'batch_size': 256, 'epochs': np.int64(23), 'embed_dim': np.int64(87), 'num_heads': np.int64(4), 'depth': np.int64(3), 'mlp_ratio': 1.5846173685406504, 'dropout': 0.21659963168004745, 'attn_dropout': 0.18771054180315008, 'drop_path': 7.787658410143285e-05, 'patch_size': np.int64(31), 'label_smoothing': 0.018340450985343384, 'use_focal': True, 'focal_gamma': 2.049512863264476, 'max_grad_norm': 2.216530591346368, 'aug_noise_std': 0.014561457009902098, 'aug_time_warp_ratio': 0.12237057894447592, 'aug_baseline_amp': 0.027898772130408374, 'aug_baseline_freq': 0.1814650918408482, 'aug_lead_dropout_prob': 0.10990855298810753, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True} -> 0.0000
2025-09-19 11:39:43,698 - INFO - bo.run_bo - üîçBO Trial 2: Initial random exploration
2025-09-19 11:39:43,698 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 11:39:43,698 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 11:39:43,698 - INFO - _models.training_function_executor - Executing training function: Tiny-ECG-Transformer-1D
2025-09-19 11:39:43,698 - INFO - _models.training_function_executor - Hyperparameters: {'seed': 7734, 'lr': 0.004505164654025286, 'weight_decay': 7.362945281639229e-05, 'batch_size': 512, 'epochs': 11, 'embed_dim': 84, 'num_heads': 2, 'depth': 4, 'mlp_ratio': 1.5198974417398, 'dropout': 0.2826605267054559, 'attn_dropout': 0.11265764356910787, 'drop_path': 0.038541650253991616, 'patch_size': 29, 'label_smoothing': 0.009767211400638388, 'use_focal': False, 'focal_gamma': 1.8803049874792028, 'max_grad_norm': 0.6979873507394164, 'aug_noise_std': 0.024758845505563513, 'aug_time_warp_ratio': 0.006877704223043681, 'aug_baseline_amp': 0.18186408041575644, 'aug_baseline_freq': 0.16645099172000763, 'aug_lead_dropout_prob': 0.1987566853061946, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-19 11:39:43,701 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'seed': 7734, 'lr': 0.004505164654025286, 'weight_decay': 7.362945281639229e-05, 'batch_size': 512, 'epochs': 11, 'embed_dim': 84, 'num_heads': 2, 'depth': 4, 'mlp_ratio': 1.5198974417398, 'dropout': 0.2826605267054559, 'attn_dropout': 0.11265764356910787, 'drop_path': 0.038541650253991616, 'patch_size': 29, 'label_smoothing': 0.009767211400638388, 'use_focal': False, 'focal_gamma': 1.8803049874792028, 'max_grad_norm': 0.6979873507394164, 'aug_noise_std': 0.024758845505563513, 'aug_time_warp_ratio': 0.006877704223043681, 'aug_baseline_amp': 0.18186408041575644, 'aug_baseline_freq': 0.16645099172000763, 'aug_lead_dropout_prob': 0.1987566853061946, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}
2025-09-19 11:41:30,557 - ERROR - _models.training_function_executor - Training execution failed: 'str' object has no attribute 'update'
2025-09-19 11:41:30,557 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# -------------------------...
2025-09-19 11:41:30,557 - ERROR - _models.training_function_executor - BO training objective failed: 'str' object has no attribute 'update'
2025-09-19 11:41:30,557 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 106.859s
2025-09-19 11:41:30,557 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 11:41:30,557 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.000s
2025-09-19 11:41:30,557 - INFO - bo.run_bo - Recorded observation #2: hparams={'seed': np.int64(7734), 'lr': 0.004505164654025286, 'weight_decay': 7.362945281639229e-05, 'batch_size': 512, 'epochs': np.int64(11), 'embed_dim': np.int64(84), 'num_heads': np.int64(2), 'depth': np.int64(4), 'mlp_ratio': 1.5198974417398, 'dropout': 0.2826605267054559, 'attn_dropout': 0.11265764356910787, 'drop_path': 0.038541650253991616, 'patch_size': np.int64(29), 'label_smoothing': 0.009767211400638388, 'use_focal': False, 'focal_gamma': 1.8803049874792028, 'max_grad_norm': 0.6979873507394164, 'aug_noise_std': 0.024758845505563513, 'aug_time_warp_ratio': 0.006877704223043681, 'aug_baseline_amp': 0.18186408041575644, 'aug_baseline_freq': 0.16645099172000763, 'aug_lead_dropout_prob': 0.1987566853061946, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False}, value=0.0000
2025-09-19 11:41:30,557 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 2: {'seed': np.int64(7734), 'lr': 0.004505164654025286, 'weight_decay': 7.362945281639229e-05, 'batch_size': 512, 'epochs': np.int64(11), 'embed_dim': np.int64(84), 'num_heads': np.int64(2), 'depth': np.int64(4), 'mlp_ratio': 1.5198974417398, 'dropout': 0.2826605267054559, 'attn_dropout': 0.11265764356910787, 'drop_path': 0.038541650253991616, 'patch_size': np.int64(29), 'label_smoothing': 0.009767211400638388, 'use_focal': False, 'focal_gamma': 1.8803049874792028, 'max_grad_norm': 0.6979873507394164, 'aug_noise_std': 0.024758845505563513, 'aug_time_warp_ratio': 0.006877704223043681, 'aug_baseline_amp': 0.18186408041575644, 'aug_baseline_freq': 0.16645099172000763, 'aug_lead_dropout_prob': 0.1987566853061946, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': False} -> 0.0000
2025-09-19 11:41:30,558 - INFO - bo.run_bo - üîçBO Trial 3: Initial random exploration
2025-09-19 11:41:30,558 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 11:41:30,559 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 11:41:30,559 - INFO - _models.training_function_executor - Executing training function: Tiny-ECG-Transformer-1D
2025-09-19 11:41:30,559 - INFO - _models.training_function_executor - Hyperparameters: {'seed': 7989, 'lr': 1.214824766007251e-05, 'weight_decay': 0.0023395864551222488, 'batch_size': 128, 'epochs': 14, 'embed_dim': 125, 'num_heads': 5, 'depth': 3, 'mlp_ratio': 2.5909079937846315, 'dropout': 0.09796223064175062, 'attn_dropout': 0.1140887948810799, 'drop_path': 0.052083426002582374, 'patch_size': 27, 'label_smoothing': 0.08445338486781516, 'use_focal': False, 'focal_gamma': 2.0793842647781595, 'max_grad_norm': 2.9750807117528573, 'aug_noise_std': 0.04826276536320691, 'aug_time_warp_ratio': 0.12140684953733696, 'aug_baseline_amp': 0.055199836404508686, 'aug_baseline_freq': 0.18332307756683708, 'aug_lead_dropout_prob': 0.049580081718900756, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-19 11:41:30,562 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'seed': 7989, 'lr': 1.214824766007251e-05, 'weight_decay': 0.0023395864551222488, 'batch_size': 128, 'epochs': 14, 'embed_dim': 125, 'num_heads': 5, 'depth': 3, 'mlp_ratio': 2.5909079937846315, 'dropout': 0.09796223064175062, 'attn_dropout': 0.1140887948810799, 'drop_path': 0.052083426002582374, 'patch_size': 27, 'label_smoothing': 0.08445338486781516, 'use_focal': False, 'focal_gamma': 2.0793842647781595, 'max_grad_norm': 2.9750807117528573, 'aug_noise_std': 0.04826276536320691, 'aug_time_warp_ratio': 0.12140684953733696, 'aug_baseline_amp': 0.055199836404508686, 'aug_baseline_freq': 0.18332307756683708, 'aug_lead_dropout_prob': 0.049580081718900756, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-19 11:41:30,568 - ERROR - _models.training_function_executor - Training execution failed: Model has 446599 parameters which exceeds the 256K limit. Reduce embed_dim/depth/heads.
2025-09-19 11:41:30,568 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# -------------------------...
2025-09-19 11:41:30,568 - ERROR - _models.training_function_executor - BO training objective failed: Model has 446599 parameters which exceeds the 256K limit. Reduce embed_dim/depth/heads.
2025-09-19 11:41:30,568 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.009s
2025-09-19 11:41:30,778 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 11:41:30,778 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.210s
2025-09-19 11:41:30,778 - INFO - bo.run_bo - Recorded observation #3: hparams={'seed': np.int64(7989), 'lr': 1.214824766007251e-05, 'weight_decay': 0.0023395864551222488, 'batch_size': 128, 'epochs': np.int64(14), 'embed_dim': np.int64(125), 'num_heads': np.int64(5), 'depth': np.int64(3), 'mlp_ratio': 2.5909079937846315, 'dropout': 0.09796223064175062, 'attn_dropout': 0.1140887948810799, 'drop_path': 0.052083426002582374, 'patch_size': np.int64(27), 'label_smoothing': 0.08445338486781516, 'use_focal': False, 'focal_gamma': 2.0793842647781595, 'max_grad_norm': 2.9750807117528573, 'aug_noise_std': 0.04826276536320691, 'aug_time_warp_ratio': 0.12140684953733696, 'aug_baseline_amp': 0.055199836404508686, 'aug_baseline_freq': 0.18332307756683708, 'aug_lead_dropout_prob': 0.049580081718900756, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}, value=0.0000
2025-09-19 11:41:30,778 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 3: {'seed': np.int64(7989), 'lr': 1.214824766007251e-05, 'weight_decay': 0.0023395864551222488, 'batch_size': 128, 'epochs': np.int64(14), 'embed_dim': np.int64(125), 'num_heads': np.int64(5), 'depth': np.int64(3), 'mlp_ratio': 2.5909079937846315, 'dropout': 0.09796223064175062, 'attn_dropout': 0.1140887948810799, 'drop_path': 0.052083426002582374, 'patch_size': np.int64(27), 'label_smoothing': 0.08445338486781516, 'use_focal': False, 'focal_gamma': 2.0793842647781595, 'max_grad_norm': 2.9750807117528573, 'aug_noise_std': 0.04826276536320691, 'aug_time_warp_ratio': 0.12140684953733696, 'aug_baseline_amp': 0.055199836404508686, 'aug_baseline_freq': 0.18332307756683708, 'aug_lead_dropout_prob': 0.049580081718900756, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True} -> 0.0000
2025-09-19 11:41:30,779 - INFO - bo.run_bo - üîçBO Trial 4: Using RF surrogate + Expected Improvement
2025-09-19 11:41:30,779 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 11:41:30,779 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 11:41:30,779 - INFO - _models.training_function_executor - Executing training function: Tiny-ECG-Transformer-1D
2025-09-19 11:41:30,779 - INFO - _models.training_function_executor - Hyperparameters: {'seed': 2695, 'lr': 0.00049884942266866, 'weight_decay': 3.898469255456132e-06, 'batch_size': 512, 'epochs': 14, 'embed_dim': 126, 'num_heads': 7, 'depth': 2, 'mlp_ratio': 2.960943511647524, 'dropout': 0.0904968698968807, 'attn_dropout': 0.07043493953604084, 'drop_path': 0.06981463061210556, 'patch_size': 25, 'label_smoothing': 0.07394242888387194, 'use_focal': True, 'focal_gamma': 1.524169023254145, 'max_grad_norm': 0.517458339746004, 'aug_noise_std': 0.031157613534318375, 'aug_time_warp_ratio': 0.08936790599567081, 'aug_baseline_amp': 0.009199581355787979, 'aug_baseline_freq': 0.4140273021802005, 'aug_lead_dropout_prob': 0.03002001973425586, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-19 11:41:30,781 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'seed': 2695, 'lr': 0.00049884942266866, 'weight_decay': 3.898469255456132e-06, 'batch_size': 512, 'epochs': 14, 'embed_dim': 126, 'num_heads': 7, 'depth': 2, 'mlp_ratio': 2.960943511647524, 'dropout': 0.0904968698968807, 'attn_dropout': 0.07043493953604084, 'drop_path': 0.06981463061210556, 'patch_size': 25, 'label_smoothing': 0.07394242888387194, 'use_focal': True, 'focal_gamma': 1.524169023254145, 'max_grad_norm': 0.517458339746004, 'aug_noise_std': 0.031157613534318375, 'aug_time_warp_ratio': 0.08936790599567081, 'aug_baseline_amp': 0.009199581355787979, 'aug_baseline_freq': 0.4140273021802005, 'aug_lead_dropout_prob': 0.03002001973425586, 'quantization_bits': 16, 'quantize_weights': True, 'quantize_activations': False}
2025-09-19 11:41:30,786 - ERROR - _models.training_function_executor - Training execution failed: Model has 330619 parameters which exceeds the 256K limit. Reduce embed_dim/depth/heads.
2025-09-19 11:41:30,786 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# -------------------------...
2025-09-19 11:41:30,786 - ERROR - _models.training_function_executor - BO training objective failed: Model has 330619 parameters which exceeds the 256K limit. Reduce embed_dim/depth/heads.
2025-09-19 11:41:30,786 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.007s
2025-09-19 11:41:30,990 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 11:41:30,990 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.204s
2025-09-19 11:41:30,990 - INFO - bo.run_bo - Recorded observation #4: hparams={'seed': np.int64(2695), 'lr': 0.00049884942266866, 'weight_decay': 3.898469255456132e-06, 'batch_size': np.int64(512), 'epochs': np.int64(14), 'embed_dim': np.int64(126), 'num_heads': np.int64(7), 'depth': np.int64(2), 'mlp_ratio': 2.960943511647524, 'dropout': 0.0904968698968807, 'attn_dropout': 0.07043493953604084, 'drop_path': 0.06981463061210556, 'patch_size': np.int64(25), 'label_smoothing': 0.07394242888387194, 'use_focal': np.True_, 'focal_gamma': 1.524169023254145, 'max_grad_norm': 0.517458339746004, 'aug_noise_std': 0.031157613534318375, 'aug_time_warp_ratio': 0.08936790599567081, 'aug_baseline_amp': 0.009199581355787979, 'aug_baseline_freq': 0.4140273021802005, 'aug_lead_dropout_prob': 0.03002001973425586, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_}, value=0.0000
2025-09-19 11:41:30,990 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 4: {'seed': np.int64(2695), 'lr': 0.00049884942266866, 'weight_decay': 3.898469255456132e-06, 'batch_size': np.int64(512), 'epochs': np.int64(14), 'embed_dim': np.int64(126), 'num_heads': np.int64(7), 'depth': np.int64(2), 'mlp_ratio': 2.960943511647524, 'dropout': 0.0904968698968807, 'attn_dropout': 0.07043493953604084, 'drop_path': 0.06981463061210556, 'patch_size': np.int64(25), 'label_smoothing': 0.07394242888387194, 'use_focal': np.True_, 'focal_gamma': 1.524169023254145, 'max_grad_norm': 0.517458339746004, 'aug_noise_std': 0.031157613534318375, 'aug_time_warp_ratio': 0.08936790599567081, 'aug_baseline_amp': 0.009199581355787979, 'aug_baseline_freq': 0.4140273021802005, 'aug_lead_dropout_prob': 0.03002001973425586, 'quantization_bits': np.int64(16), 'quantize_weights': np.True_, 'quantize_activations': np.False_} -> 0.0000
2025-09-19 11:41:30,990 - INFO - bo.run_bo - üîçBO Trial 5: Using RF surrogate + Expected Improvement
2025-09-19 11:41:30,990 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 11:41:30,990 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 11:41:30,990 - INFO - _models.training_function_executor - Executing training function: Tiny-ECG-Transformer-1D
2025-09-19 11:41:30,991 - INFO - _models.training_function_executor - Hyperparameters: {'seed': 2762, 'lr': 0.004662306628711941, 'weight_decay': 1.701839500005624e-05, 'batch_size': 64, 'epochs': 8, 'embed_dim': 125, 'num_heads': 2, 'depth': 3, 'mlp_ratio': 1.954567554650501, 'dropout': 0.1895460659258956, 'attn_dropout': 0.051332184641420475, 'drop_path': 0.030028642273506306, 'patch_size': 22, 'label_smoothing': 0.013168623653689352, 'use_focal': False, 'focal_gamma': 2.3370183186944935, 'max_grad_norm': 0.2926655625989844, 'aug_noise_std': 0.01851068693092238, 'aug_time_warp_ratio': 0.04553662398451913, 'aug_baseline_amp': 0.11644926238008527, 'aug_baseline_freq': 0.17994316051389875, 'aug_lead_dropout_prob': 0.1758842063906307, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-19 11:41:30,993 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'seed': 2762, 'lr': 0.004662306628711941, 'weight_decay': 1.701839500005624e-05, 'batch_size': 64, 'epochs': 8, 'embed_dim': 125, 'num_heads': 2, 'depth': 3, 'mlp_ratio': 1.954567554650501, 'dropout': 0.1895460659258956, 'attn_dropout': 0.051332184641420475, 'drop_path': 0.030028642273506306, 'patch_size': 22, 'label_smoothing': 0.013168623653689352, 'use_focal': False, 'focal_gamma': 2.3370183186944935, 'max_grad_norm': 0.2926655625989844, 'aug_noise_std': 0.01851068693092238, 'aug_time_warp_ratio': 0.04553662398451913, 'aug_baseline_amp': 0.11644926238008527, 'aug_baseline_freq': 0.17994316051389875, 'aug_lead_dropout_prob': 0.1758842063906307, 'quantization_bits': 16, 'quantize_weights': False, 'quantize_activations': True}
2025-09-19 11:41:30,999 - ERROR - _models.training_function_executor - Training execution failed: Model has 387112 parameters which exceeds the 256K limit. Reduce embed_dim/depth/heads.
2025-09-19 11:41:30,999 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# -------------------------...
2025-09-19 11:41:30,999 - ERROR - _models.training_function_executor - BO training objective failed: Model has 387112 parameters which exceeds the 256K limit. Reduce embed_dim/depth/heads.
2025-09-19 11:41:30,999 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 0.009s
2025-09-19 11:41:31,201 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 11:41:31,201 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.203s
2025-09-19 11:41:31,201 - INFO - bo.run_bo - Recorded observation #5: hparams={'seed': np.int64(2762), 'lr': 0.004662306628711941, 'weight_decay': 1.701839500005624e-05, 'batch_size': np.int64(64), 'epochs': np.int64(8), 'embed_dim': np.int64(125), 'num_heads': np.int64(2), 'depth': np.int64(3), 'mlp_ratio': 1.954567554650501, 'dropout': 0.1895460659258956, 'attn_dropout': 0.051332184641420475, 'drop_path': 0.030028642273506306, 'patch_size': np.int64(22), 'label_smoothing': 0.013168623653689352, 'use_focal': np.False_, 'focal_gamma': 2.3370183186944935, 'max_grad_norm': 0.2926655625989844, 'aug_noise_std': 0.01851068693092238, 'aug_time_warp_ratio': 0.04553662398451913, 'aug_baseline_amp': 0.11644926238008527, 'aug_baseline_freq': 0.17994316051389875, 'aug_lead_dropout_prob': 0.1758842063906307, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_}, value=0.0000
2025-09-19 11:41:31,201 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 5: {'seed': np.int64(2762), 'lr': 0.004662306628711941, 'weight_decay': 1.701839500005624e-05, 'batch_size': np.int64(64), 'epochs': np.int64(8), 'embed_dim': np.int64(125), 'num_heads': np.int64(2), 'depth': np.int64(3), 'mlp_ratio': 1.954567554650501, 'dropout': 0.1895460659258956, 'attn_dropout': 0.051332184641420475, 'drop_path': 0.030028642273506306, 'patch_size': np.int64(22), 'label_smoothing': 0.013168623653689352, 'use_focal': np.False_, 'focal_gamma': 2.3370183186944935, 'max_grad_norm': 0.2926655625989844, 'aug_noise_std': 0.01851068693092238, 'aug_time_warp_ratio': 0.04553662398451913, 'aug_baseline_amp': 0.11644926238008527, 'aug_baseline_freq': 0.17994316051389875, 'aug_lead_dropout_prob': 0.1758842063906307, 'quantization_bits': np.int64(16), 'quantize_weights': np.False_, 'quantize_activations': np.True_} -> 0.0000
2025-09-19 11:41:31,201 - INFO - bo.run_bo - üîçBO Trial 6: Using RF surrogate + Expected Improvement
2025-09-19 11:41:31,201 - INFO - bo.run_bo - [PROFILE] suggest() took 0.000s
2025-09-19 11:41:31,202 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 11:41:31,202 - INFO - _models.training_function_executor - Executing training function: Tiny-ECG-Transformer-1D
2025-09-19 11:41:31,202 - INFO - _models.training_function_executor - Hyperparameters: {'seed': 7307, 'lr': 0.000246563687922462, 'weight_decay': 3.146337368073077e-05, 'batch_size': 512, 'epochs': 13, 'embed_dim': 87, 'num_heads': 4, 'depth': 4, 'mlp_ratio': 1.9693177331782719, 'dropout': 0.13964203174878684, 'attn_dropout': 0.17504280233480612, 'drop_path': 0.0461600614032031, 'patch_size': 24, 'label_smoothing': 0.07755711856280188, 'use_focal': False, 'focal_gamma': 2.0098104863831825, 'max_grad_norm': 3.8215578687123894, 'aug_noise_std': 0.00018155734308115682, 'aug_time_warp_ratio': 0.08954876983909689, 'aug_baseline_amp': 0.024587191065478315, 'aug_baseline_freq': 0.29240341334113806, 'aug_lead_dropout_prob': 0.06955862398476673, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-19 11:41:31,205 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'seed': 7307, 'lr': 0.000246563687922462, 'weight_decay': 3.146337368073077e-05, 'batch_size': 512, 'epochs': 13, 'embed_dim': 87, 'num_heads': 4, 'depth': 4, 'mlp_ratio': 1.9693177331782719, 'dropout': 0.13964203174878684, 'attn_dropout': 0.17504280233480612, 'drop_path': 0.0461600614032031, 'patch_size': 24, 'label_smoothing': 0.07755711856280188, 'use_focal': False, 'focal_gamma': 2.0098104863831825, 'max_grad_norm': 3.8215578687123894, 'aug_noise_std': 0.00018155734308115682, 'aug_time_warp_ratio': 0.08954876983909689, 'aug_baseline_amp': 0.024587191065478315, 'aug_baseline_freq': 0.29240341334113806, 'aug_lead_dropout_prob': 0.06955862398476673, 'quantization_bits': 8, 'quantize_weights': True, 'quantize_activations': True}
2025-09-19 11:43:44,405 - ERROR - _models.training_function_executor - Training execution failed: 'str' object has no attribute 'update'
2025-09-19 11:43:44,405 - ERROR - _models.training_function_executor - Training code: import math
import random
from typing import Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# -------------------------...
2025-09-19 11:43:44,405 - ERROR - _models.training_function_executor - BO training objective failed: 'str' object has no attribute 'update'
2025-09-19 11:43:44,405 - INFO - _models.training_function_executor - [PROFILE] objective(train+eval) FAILED took 133.203s
2025-09-19 11:43:44,609 - INFO - bo.run_bo - Updated RF surrogate model with observation: 0.0000
2025-09-19 11:43:44,609 - INFO - bo.run_bo - [PROFILE] observe()->tell took 0.204s
2025-09-19 11:43:44,609 - INFO - bo.run_bo - Recorded observation #6: hparams={'seed': np.int64(7307), 'lr': 0.000246563687922462, 'weight_decay': 3.146337368073077e-05, 'batch_size': np.int64(512), 'epochs': np.int64(13), 'embed_dim': np.int64(87), 'num_heads': np.int64(4), 'depth': np.int64(4), 'mlp_ratio': 1.9693177331782719, 'dropout': 0.13964203174878684, 'attn_dropout': 0.17504280233480612, 'drop_path': 0.0461600614032031, 'patch_size': np.int64(24), 'label_smoothing': 0.07755711856280188, 'use_focal': np.False_, 'focal_gamma': 2.0098104863831825, 'max_grad_norm': 3.8215578687123894, 'aug_noise_std': 0.00018155734308115682, 'aug_time_warp_ratio': 0.08954876983909689, 'aug_baseline_amp': 0.024587191065478315, 'aug_baseline_freq': 0.29240341334113806, 'aug_lead_dropout_prob': 0.06955862398476673, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_}, value=0.0000
2025-09-19 11:43:44,609 - INFO - evaluation.code_generation_pipeline_orchestrator - BO Trial 6: {'seed': np.int64(7307), 'lr': 0.000246563687922462, 'weight_decay': 3.146337368073077e-05, 'batch_size': np.int64(512), 'epochs': np.int64(13), 'embed_dim': np.int64(87), 'num_heads': np.int64(4), 'depth': np.int64(4), 'mlp_ratio': 1.9693177331782719, 'dropout': 0.13964203174878684, 'attn_dropout': 0.17504280233480612, 'drop_path': 0.0461600614032031, 'patch_size': np.int64(24), 'label_smoothing': 0.07755711856280188, 'use_focal': np.False_, 'focal_gamma': 2.0098104863831825, 'max_grad_norm': 3.8215578687123894, 'aug_noise_std': 0.00018155734308115682, 'aug_time_warp_ratio': 0.08954876983909689, 'aug_baseline_amp': 0.024587191065478315, 'aug_baseline_freq': 0.29240341334113806, 'aug_lead_dropout_prob': 0.06955862398476673, 'quantization_bits': np.int64(8), 'quantize_weights': np.True_, 'quantize_activations': np.True_} -> 0.0000
2025-09-19 11:43:44,610 - INFO - bo.run_bo - üîçBO Trial 7: Using RF surrogate + Expected Improvement
2025-09-19 11:43:44,610 - INFO - bo.run_bo - [PROFILE] suggest() took 0.001s
2025-09-19 11:43:44,610 - INFO - _models.training_function_executor - Using device: cuda
2025-09-19 11:43:44,610 - INFO - _models.training_function_executor - Executing training function: Tiny-ECG-Transformer-1D
2025-09-19 11:43:44,610 - INFO - _models.training_function_executor - Hyperparameters: {'seed': 3634, 'lr': 0.0003102988386730896, 'weight_decay': 0.00017557537132222763, 'batch_size': 64, 'epochs': 18, 'embed_dim': 83, 'num_heads': 3, 'depth': 3, 'mlp_ratio': 2.0585330844312963, 'dropout': 0.14605263259999715, 'attn_dropout': 0.1590899577741733, 'drop_path': 0.06561451866371841, 'patch_size': 31, 'label_smoothing': 0.06746516904703369, 'use_focal': True, 'focal_gamma': 1.3207595598569546, 'max_grad_norm': 1.0936622942367986, 'aug_noise_std': 0.021202972899328362, 'aug_time_warp_ratio': 0.18721749709379462, 'aug_baseline_amp': 0.04711131393880922, 'aug_baseline_freq': 0.2898881625639969, 'aug_lead_dropout_prob': 0.09001971018434249, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
2025-09-19 11:43:44,612 - INFO - _models.training_function_executor - Starting training with hyperparameters: {'seed': 3634, 'lr': 0.0003102988386730896, 'weight_decay': 0.00017557537132222763, 'batch_size': 64, 'epochs': 18, 'embed_dim': 83, 'num_heads': 3, 'depth': 3, 'mlp_ratio': 2.0585330844312963, 'dropout': 0.14605263259999715, 'attn_dropout': 0.1590899577741733, 'drop_path': 0.06561451866371841, 'patch_size': 31, 'label_smoothing': 0.06746516904703369, 'use_focal': True, 'focal_gamma': 1.3207595598569546, 'max_grad_norm': 1.0936622942367986, 'aug_noise_std': 0.021202972899328362, 'aug_time_warp_ratio': 0.18721749709379462, 'aug_baseline_amp': 0.04711131393880922, 'aug_baseline_freq': 0.2898881625639969, 'aug_lead_dropout_prob': 0.09001971018434249, 'quantization_bits': 8, 'quantize_weights': False, 'quantize_activations': True}
