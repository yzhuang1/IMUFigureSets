=== GPT DEBUG RESPONSE ===
Timestamp: 20250922_191832
Attempt: 1/4
Error Type: training_error
Original Error: The expanded size of the tensor (25) must match the existing size (26) at non-singleton dimension 1.  Target sizes: [1000, 25].  Tensor sizes: [1000, 26]
=== RAW GPT RESPONSE ===
{"training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hparams):\n    import io\n    import math\n    import copy\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader\n    from typing import List, Tuple\n    \n    # ---------------------------\n    # Defaults & Hyperparameters\n    # ---------------------------\n    defaults = {\n        # optimization\n        'epochs': 20,\n        'batch_size': 256,\n        'lr': 3e-4,\n        'weight_decay': 1e-4,\n        'optimizer': 'adamw',  # ['adam','adamw']\n        'grad_clip_norm': 1.0,\n        # model\n        'tcn_channels': 12,            # base channels after stem\n        'num_tcn_blocks': 2,          # number of residual MB-TCN blocks\n        'dropout': 0.1,\n        'd_model': 32,\n        'num_heads': 4,\n        'num_encoder_layers': 1,      # 1-2 as recommended\n        # loss & imbalance\n        'use_focal_loss': True,\n        'focal_gamma': 2.0,\n        'label_smoothing': 0.0,\n        'class_weighting': 'auto',    # ['none','auto']\n        'use_smote_tomek': False,\n        'smote_k_neighbors': 5,\n        # quantization\n        'quantization_bits': 8,       # [8,16,32]\n        'quantize_weights': True,\n        'quantize_activations': False,\n        'calib_batches': 4,           # used if static calibration is enabled\n    }\n    # Merge provided hyperparameters\n    for k, v in defaults.items():\n        if k not in hparams:\n            hparams[k] = v\n    \n    # ---------------------------\n    # Assertions & Setup\n    # ---------------------------\n    if not isinstance(device, torch.device):\n        device = torch.device(device)\n    assert device.type == 'cuda', 'Training must run on GPU (cuda).'\n    torch.backends.cudnn.benchmark = True\n    \n    num_classes = 5\n    seq_len = 1000\n    in_channels = 2\n    \n    # Ensure heads divide d_model\n    def _fix_heads(d_model, num_heads):\n        # ensure num_heads is a divisor of d_model\n        if d_model % num_heads == 0:\n            return num_heads\n        # pick the largest divisor <= num_heads, else fallback to 1\n        divisors = [d for d in range(min(d_model, num_heads), 0, -1) if d_model % d == 0]\n        return divisors[0] if len(divisors) > 0 else 1\n    hparams['num_heads'] = _fix_heads(int(hparams['d_model']), int(hparams['num_heads']))\n    \n    # ---------------------------\n    # Optional SMOTE+Tomek on train only (before DataLoader)\n    # ---------------------------\n    def maybe_smote_tomek(Xt, yt, use_smote, k_neighbors):\n        if not use_smote:\n            return Xt, yt\n        try:\n            from imblearn.combine import SMOTETomek\n        except Exception:\n            return Xt, yt  # fallback silently if imblearn not available\n        # Expect Xt shape: (N, 2, 1000) or (N, 1000, 2)\n        Xt_cpu = Xt.detach().cpu()\n        if Xt_cpu.dim() != 3:\n            raise ValueError('Expected X tensor with 3 dims: (N, C, L) or (N, L, C)')\n        if Xt_cpu.shape[1] == seq_len and Xt_cpu.shape[2] == in_channels:\n            Xt_cpu = Xt_cpu.permute(0, 2, 1)  # (N, 2, 1000)\n        # Flatten to 2D for SMOTE\n        N = Xt_cpu.shape[0]\n        X_flat = Xt_cpu.reshape(N, -1).numpy()\n        y_np = yt.detach().cpu().numpy()\n        smt = SMOTETomek(k_neighbors=max(1, int(k_neighbors)))\n        X_res, y_res = smt.fit_resample(X_flat, y_np)\n        X_res_t = torch.from_numpy(X_res).float().reshape(-1, in_channels, seq_len)\n        y_res_t = torch.from_numpy(y_res).long()\n        return X_res_t, y_res_t\n    \n    # ---------------------------\n    # Dataset & Dataloaders\n    # ---------------------------\n    class ECGDataset(Dataset):\n        def __init__(self, X, y):\n            super().__init__()\n            self.X = X\n            self.y = y\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            # Ensure shape (C, L) for Conv1d\n            if x.dim() == 2 and x.shape[0] == seq_len and x.shape[1] == in_channels:\n                x = x.permute(1, 0)  # (2, 1000)\n            elif x.dim() == 2 and x.shape[0] == in_channels and x.shape[1] == seq_len:\n                pass\n            else:\n                raise ValueError('Each sample must have shape (1000, 2) or (2, 1000).')\n            y = self.y[idx]\n            return x, y\n    \n    # Apply optional SMOTE/Tomek to training set only (on CPU) before loaders\n    X_train_bal, y_train_bal = maybe_smote_tomek(X_train, y_train, bool(hparams['use_smote_tomek']), int(hparams['smote_k_neighbors']))\n    \n    train_ds = ECGDataset(X_train_bal, y_train_bal)\n    val_ds = ECGDataset(X_val, y_val)\n    \n    train_loader = DataLoader(train_ds, batch_size=int(hparams['batch_size']), shuffle=True, num_workers=0, pin_memory=False)\n    val_loader = DataLoader(val_ds, batch_size=int(hparams['batch_size']), shuffle=False, num_workers=0, pin_memory=False)\n    \n    # ---------------------------\n    # Model Definition: MB-MHA-TCN Tiny\n    # ---------------------------\n    class PositionalEncoding(nn.Module):\n        def __init__(self, d_model: int, max_len: int = 5000):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            if d_model % 2 == 1:\n                pe[:, 1::2] = torch.cos(position * div_term[:-1])\n            else:\n                pe[:, 1::2] = torch.cos(position * div_term)\n            pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n            self.register_buffer('pe', pe)\n        def forward(self, x):  # x: (B, L, d_model)\n            L = x.size(1)\n            return x + self.pe[:, :L, :]\n    \n    class DepthwiseSeparableConv1d(nn.Module):\n        def __init__(self, channels: int, kernel_size: int, dilation: int, dropout: float):\n            super().__init__()\n            padding = (kernel_size - 1) // 2 * dilation\n            self.dw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size, padding=padding, dilation=dilation, groups=channels, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.pw = nn.Sequential(\n                nn.Conv1d(channels, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n                nn.ReLU(inplace=True),\n            )\n            self.do = nn.Dropout(p=dropout)\n        def forward(self, x):\n            x = self.dw[x.__class__] if False else self.dw(x)\n            x = self.pw(x)\n            x = self.do(x)\n            return x\n    \n    class MBTCNBlock(nn.Module):\n        def __init__(self, channels: int, dropout: float):\n            super().__init__()\n            ks = [7, 15, 31]\n            dil = [1, 2, 4]\n            self.branches = nn.ModuleList([\n                DepthwiseSeparableConv1d(channels, ks[0], dil[0], dropout),\n                DepthwiseSeparableConv1d(channels, ks[1], dil[1], dropout),\n                DepthwiseSeparableConv1d(channels, ks[2], dil[2], dropout),\n            ])\n            self.merge = nn.Sequential(\n                nn.Conv1d(channels * 3, channels, kernel_size=1, bias=False),\n                nn.BatchNorm1d(channels),\n            )\n            self.act = nn.ReLU(inplace=True)\n        def forward(self, x):\n            outs = [b(x) for b in self.branches]\n            y = torch.cat(outs, dim=1)\n            y = self.merge(y)\n            y = y + x  # residual\n            return self.act(y)\n    \n    class TransformerEncoderBlock(nn.Module):\n        def __init__(self, d_model: int, nhead: int, dropout: float):\n            super().__init__()\n            self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n            self.ln1 = nn.LayerNorm(d_model)\n            self.ffn = nn.Sequential(\n                nn.Linear(d_model, d_model * 2),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout),\n                nn.Linear(d_model * 2, d_model),\n            )\n            self.ln2 = nn.LayerNorm(d_model)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, x):  # x: (B, L, D)\n            attn_out, _ = self.mha(x, x, x, need_weights=False)\n            x = self.ln1(x + self.drop(attn_out))\n            x = self.ln2(x + self.drop(self.ffn(x)))\n            return x\n    \n    class MB_MHA_TCN_Tiny(nn.Module):\n        def __init__(self, in_ch: int, base_ch: int, d_model: int, nhead: int, num_layers: int, dropout: float, num_classes: int, use_act_quant: bool = False):\n            super().__init__()\n            self.use_act_quant = use_act_quant\n            if use_act_quant:\n                from torch.ao.quantization import QuantStub, DeQuantStub\n                self.quant = QuantStub()\n                self.dequant = DeQuantStub()\n            else:\n                self.quant = None\n                self.dequant = None\n            # Stem\n            self.stem = nn.Sequential(\n                nn.Conv1d(in_ch, base_ch, kernel_size=7, padding=3, bias=False),\n                nn.BatchNorm1d(base_ch),\n                nn.ReLU(inplace=True),\n            )\n            # MB-TCN blocks\n            self.tcn_blocks = nn.Sequential(*[MBTCNBlock(base_ch, dropout) for _ in range(int(hparams['num_tcn_blocks']))])\n            # Projection to d_model\n            self.proj = nn.Sequential(\n                nn.Conv1d(base_ch, d_model, kernel_size=1, bias=False),\n                nn.BatchNorm1d(d_model),\n                nn.ReLU(inplace=True),\n            )\n            # Transformer encoder(s)\n            self.posenc = PositionalEncoding(d_model, max_len=seq_len)\n            self.encoder = nn.Sequential(*[TransformerEncoderBlock(d_model, nhead, dropout) for _ in range(int(hparams['num_encoder_layers']))])\n            # Head\n            self.pool = nn.AdaptiveAvgPool1d(1)\n            self.fc = nn.Linear(d_model, num_classes)\n        def forward(self, x):  # x: (B, C, L)\n            if self.quant is not None:\n                x = self.quant(x)\n            x = self.stem(x)\n            x = self.tcn_blocks(x)\n            x = self.proj(x)            # (B, D, L)\n            x = x.transpose(1, 2)       # (B, L, D)\n            x = self.posenc(x)\n            x = self.encoder(x)\n            x = x.transpose(1, 2)       # (B, D, L)\n            x = self.pool(x).squeeze(-1)  # (B, D)\n            x = self.fc(x)              # (B, num_classes)\n            if self.dequant is not None:\n                x = self.dequant(x)\n            return x\n        def fuse_for_quant(self):\n            # Try to fuse typical conv-bn-relu patterns\n            for name, module in self.named_modules():\n                pass  # kept simple; prepare/convert will still quantize supported ops\n    \n    # ---------------------------\n    # Loss functions\n    # ---------------------------\n    def compute_class_weights(y: torch.Tensor, num_classes: int) -> torch.Tensor:\n        with torch.no_grad():\n            counts = torch.bincount(y.view(-1).cpu(), minlength=num_classes).float()\n            counts[counts == 0] = 1.0\n            inv = 1.0 / counts\n            w = inv / inv.sum() * num_classes\n            return w\n    \n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0):\n            super().__init__()\n            self.gamma = gamma\n            self.weight = weight\n            self.label_smoothing = label_smoothing\n        def forward(self, logits, targets):\n            ce = F.cross_entropy(logits, targets, weight=self.weight, reduction='none', label_smoothing=self.label_smoothing)\n            pt = torch.softmax(logits, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n            pt = pt.clamp(1e-6, 1 - 1e-6)\n            loss = ((1 - pt) ** self.gamma) * ce\n            return loss.mean()\n    \n    # ---------------------------\n    # Build model, optimizer, criterion\n    # ---------------------------\n    model = MB_MHA_TCN_Tiny(\n        in_ch=in_channels,\n        base_ch=int(hparams['tcn_channels']),\n        d_model=int(hparams['d_model']),\n        nhead=int(hparams['num_heads']),\n        num_layers=int(hparams['num_encoder_layers']),\n        dropout=float(hparams['dropout']),\n        num_classes=num_classes,\n        use_act_quant=bool(hparams['quantize_activations']) and int(hparams['quantization_bits']) == 8,\n    ).to(device)\n    \n    # Class weights\n    if hparams['class_weighting'] == 'auto':\n        cls_w = compute_class_weights(y_train_bal, num_classes).to(device)\n    else:\n        cls_w = None\n    \n    # Criterion\n    if bool(hparams['use_focal_loss']):\n        criterion = FocalLoss(gamma=float(hparams['focal_gamma']), weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    else:\n        criterion = nn.CrossEntropyLoss(weight=cls_w, label_smoothing=float(hparams['label_smoothing']))\n    \n    # Optimizer\n    if hparams['optimizer'].lower() == 'adamw':\n        optimizer = optim.AdamW(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    else:\n        optimizer = optim.Adam(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))\n    \n    # ---------------------------\n    # Training Loop\n    # ---------------------------\n    train_losses: List[float] = []\n    val_losses: List[float] = []\n    val_accs: List[float] = []\n    \n    for epoch in range(int(hparams['epochs'])):\n        model.train()\n        running_loss = 0.0\n        n_train = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=False).float()\n            yb = yb.to(device, non_blocking=False).long()\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if float(hparams['grad_clip_norm']) > 0:\n                nn.utils.clip_grad_norm_(model.parameters(), float(hparams['grad_clip_norm']))\n            optimizer.step()\n            bs = xb.size(0)\n            running_loss += loss.detach().item() * bs\n            n_train += bs\n        train_loss_epoch = running_loss / max(1, n_train)\n        train_losses.append(train_loss_epoch)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=False).float()\n                yb = yb.to(device, non_blocking=False).long()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss += loss.item() * xb.size(0)\n                preds = torch.argmax(logits, dim=1)\n                correct += (preds == yb).sum().item()\n                total += xb.size(0)\n        val_loss_epoch = val_loss / max(1, total)\n        val_acc_epoch = correct / max(1, total)\n        val_losses.append(val_loss_epoch)\n        val_accs.append(val_acc_epoch)\n        print(f\"Epoch {epoch+1}/{hparams['epochs']} - train_loss: {train_loss_epoch:.6f} - val_loss: {val_loss_epoch:.6f} - val_acc: {val_acc_epoch:.4f}\")\n    \n    # ---------------------------\n    # Post-Training Quantization (on CPU)\n    # ---------------------------\n    model_cpu = copy.deepcopy(model).to('cpu')\n    model_cpu.eval()\n    quant_bits = int(hparams['quantization_bits'])\n    q_weights = bool(hparams['quantize_weights'])\n    q_acts = bool(hparams['quantize_activations'])\n    \n    quantized_model = model_cpu\n    \n    if quant_bits == 32 or not q_weights:\n        # No quantization (float32)\n        pass\n    elif quant_bits == 16:\n        # FP16 cast (weights + activations during inference if run in fp16-capable device)\n        quantized_model = model_cpu.half()\n    elif quant_bits == 8:\n        if q_acts:\n            # Static quantization with calibration\n            from torch.ao.quantization import get_default_qconfig\n            from torch.ao.quantization import prepare, convert\n            torch.backends.quantized.engine = 'fbgemm'\n            quantized_model.fuse_for_quant()\n            quantized_model.qconfig = get_default_qconfig('fbgemm')\n            prepare(quantized_model, inplace=True)\n            # Calibration on a few batches from training data (CPU)\n            calib_batches = max(1, int(hparams['calib_batches']))\n            quantized_model.eval()\n            with torch.inference_mode():\n                seen = 0\n                for xb, _ in DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=0, pin_memory=False):\n                    xb = xb.float()\n                    _ = quantized_model(xb)\n                    seen += 1\n                    if seen >= calib_batches:\n                        break\n            convert(quantized_model, inplace=True)\n        else:\n            # Dynamic quantization for Linear layers (covers MHA and FFN); convs remain FP32\n            from torch.ao.quantization import quantize_dynamic\n            quantized_model = quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)\n    else:\n        pass\n    \n    # ---------------------------\n    # Compute serialized size (KB)\n    # ---------------------------\n    with io.BytesIO() as f:\n        try:\n            torch.save(quantized_model.state_dict(), f)\n        except Exception:\n            # fallback to full model if state_dict fails\n            torch.save(quantized_model, f)\n        size_bytes = f.tell()\n    size_kb = size_bytes / 1024.0\n    \n    # Enforce size constraint by design; print warning if exceeded\n    if size_kb > 256.0:\n        print(f\"Warning: Quantized model size {size_kb:.2f} KB exceeds 256KB. Consider reducing d_model/tcn_channels.\")\n    \n    metrics = {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'val_acc': val_accs,\n        'final_model_size_kb': size_kb,\n        'used_hparams': hparams\n    }\n    return quantized_model, metrics\n"}
=== PROMPT USED ===
CRITICAL: You MUST respond with ONLY valid JSON. No text before or after the JSON.

Analyze this PyTorch training error and provide either hyperparameter corrections OR fixed training code.

PyTorch Version: 2.8.0+cu128
Training Error: The expanded size of the tensor (25) must match the existing size (26) at non-singleton dimension 1.  Target sizes: [1000, 25].  Tensor sizes: [1000, 26]
BO Config: {'epochs': {'default': 20, 'type': 'Integer', 'low': 5, 'high': 200}, 'batch_size': {'default': 256, 'type': 'Categorical', 'categories': [64, 128, 256, 512]}, 'lr': {'default': 0.0003, 'type': 'Real', 'low': 1e-06, 'high': 0.01, 'prior': 'log-uniform'}, 'weight_decay': {'default': 0.0001, 'type': 'Real', 'low': 1e-08, 'high': 0.01, 'prior': 'log-uniform'}, 'optimizer': {'default': 'adamw', 'type': 'Categorical', 'categories': ['adam', 'adamw']}, 'grad_clip_norm': {'default': 1.0, 'type': 'Real', 'low': 0.0, 'high': 5.0}, 'tcn_channels': {'default': 12, 'type': 'Integer', 'low': 8, 'high': 24}, 'num_tcn_blocks': {'default': 2, 'type': 'Integer', 'low': 1, 'high': 4}, 'dropout': {'default': 0.1, 'type': 'Real', 'low': 0.0, 'high': 0.5}, 'd_model': {'default': 32, 'type': 'Integer', 'low': 16, 'high': 64}, 'num_heads': {'default': 4, 'type': 'Integer', 'low': 2, 'high': 8}, 'num_encoder_layers': {'default': 1, 'type': 'Integer', 'low': 1, 'high': 2}, 'use_focal_loss': {'default': True, 'type': 'Categorical', 'categories': [True, False]}, 'focal_gamma': {'default': 2.0, 'type': 'Real', 'low': 1.0, 'high': 5.0}, 'label_smoothing': {'default': 0.0, 'type': 'Real', 'low': 0.0, 'high': 0.2}, 'class_weighting': {'default': 'auto', 'type': 'Categorical', 'categories': ['none', 'auto']}, 'use_smote_tomek': {'default': False, 'type': 'Categorical', 'categories': [True, False]}, 'smote_k_neighbors': {'default': 5, 'type': 'Integer', 'low': 1, 'high': 10}, 'quantization_bits': {'default': 8, 'type': 'Categorical', 'categories': [8, 16, 32]}, 'quantize_weights': {'default': True, 'type': 'Categorical', 'categories': [True, False]}, 'quantize_activations': {'default': False, 'type': 'Categorical', 'categories': [True, False]}, 'calib_batches': {'default': 4, 'type': 'Integer', 'low': 1, 'high': 32}}
Training Code: def train_model(X_train, y_train, X_val, y_val, device, **hparams):
    import io
    import math
    import copy
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    from typing import List, Tuple
    
    # ---------------------------
    # Defaults & Hyperparameters
    # ---------------------------
    defaults = {
        # optimization
        'epochs': 20,
        'batch_size': 256,
        'lr': 3e-4,
        'weight_decay': 1e-4,
        'optimizer': 'adamw',  # ['adam','adamw']
        'grad_clip_norm': 1.0,
        # model
        'tcn_channels': 12,            # base channels after stem
        'num_tcn_blocks': 2,          # number of residual MB-TCN blocks
        'dropout': 0.1,
        'd_model': 32,
        'num_heads': 4,
        'num_encoder_layers': 1,      # 1-2 as recommended
        # loss & imbalance
        'use_focal_loss': True,
        'focal_gamma': 2.0,
        'label_smoothing': 0.0,
        'class_weighting': 'auto',    # ['none','auto']
        'use_smote_tomek': False,
        'smote_k_neighbors': 5,
        # quantization
        'quantization_bits': 8,       # [8,16,32]
        'quantize_weights': True,
        'quantize_activations': False,
        'calib_batches': 4,           # used if static calibration is enabled
    }
    # Merge provided hyperparameters
    for k, v in defaults.items():
        if k not in hparams:
            hparams[k] = v
    
    # ---------------------------
    # Assertions & Setup
    # ---------------------------
    if not isinstance(device, torch.device):
        device = torch.device(device)
    assert device.type == 'cuda', 'Training must run on GPU (cuda).'
    torch.backends.cudnn.benchmark = True
    
    num_classes = 5
    seq_len = 1000
    in_channels = 2
    
    # Ensure heads divide d_model
    def _fix_heads(d_model, num_heads):
        # ensure num_heads is a divisor of d_model
        if d_model % num_heads == 0:
            return num_heads
        # pick the largest divisor <= num_heads, else fallback to 1
        divisors = [d for d in range(min(d_model, num_heads), 0, -1) if d_model % d == 0]
        return divisors[0] if len(divisors) > 0 else 1
    hparams['num_heads'] = _fix_heads(int(hparams['d_model']), int(hparams['num_heads']))
    
    # ---------------------------
    # Optional SMOTE+Tomek on train only (before DataLoader)
    # ---------------------------
    def maybe_smote_tomek(Xt, yt, use_smote, k_neighbors):
        if not use_smote:
            return Xt, yt
        try:
            from imblearn.combine import SMOTETomek
        except Exception:
            return Xt, yt  # fallback silently if imblearn not available
        # Expect Xt shape: (N, 2, 1000) or (N, 1000, 2)
        Xt_cpu = Xt.detach().cpu()
        if Xt_cpu.dim() != 3:
            raise ValueError('Expected X tensor with 3 dims: (N, C, L) or (N, L, C)')
        if Xt_cpu.shape[1] == seq_len and Xt_cpu.shape[2] == in_channels:
            Xt_cpu = Xt_cpu.permute(0, 2, 1)  # (N, 2, 1000)
        # Flatten to 2D for SMOTE
        N = Xt_cpu.shape[0]
        X_flat = Xt_cpu.reshape(N, -1).numpy()
        y_np = yt.detach().cpu().numpy()
        smt = SMOTETomek(k_neighbors=max(1, int(k_neighbors)))
        X_res, y_res = smt.fit_resample(X_flat, y_np)
        X_res_t = torch.from_numpy(X_res).float().reshape(-1, in_channels, seq_len)
        y_res_t = torch.from_numpy(y_res).long()
        return X_res_t, y_res_t
    
    # ---------------------------
    # Dataset & Dataloaders
    # ---------------------------
    class ECGDataset(Dataset):
        def __init__(self, X, y):
            super().__init__()
            self.X = X
            self.y = y
        def __len__(self):
            return self.X.shape[0]
        def __getitem__(self, idx):
            x = self.X[idx]
            # Ensure shape (C, L) for Conv1d
            if x.dim() == 2 and x.shape[0] == seq_len and x.shape[1] == in_channels:
                x = x.permute(1, 0)  # (2, 1000)
            elif x.dim() == 2 and x.shape[0] == in_channels and x.shape[1] == seq_len:
                pass
            else:
                raise ValueError('Each sample must have shape (1000, 2) or (2, 1000).')
            y = self.y[idx]
            return x, y
    
    # Apply optional SMOTE/Tomek to training set only (on CPU) before loaders
    X_train_bal, y_train_bal = maybe_smote_tomek(X_train, y_train, bool(hparams['use_smote_tomek']), int(hparams['smote_k_neighbors']))
    
    train_ds = ECGDataset(X_train_bal, y_train_bal)
    val_ds = ECGDataset(X_val, y_val)
    
    train_loader = DataLoader(train_ds, batch_size=int(hparams['batch_size']), shuffle=True, num_workers=0, pin_memory=False)
    val_loader = DataLoader(val_ds, batch_size=int(hparams['batch_size']), shuffle=False, num_workers=0, pin_memory=False)
    
    # ---------------------------
    # Model Definition: MB-MHA-TCN Tiny
    # ---------------------------
    class PositionalEncoding(nn.Module):
        def __init__(self, d_model: int, max_len: int = 5000):
            super().__init__()
            pe = torch.zeros(max_len, d_model)
            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            pe = pe.unsqueeze(0)  # (1, max_len, d_model)
            self.register_buffer('pe', pe)
        def forward(self, x):  # x: (B, L, d_model)
            L = x.size(1)
            return x + self.pe[:, :L, :]
    
    class DepthwiseSeparableConv1d(nn.Module):
        def __init__(self, channels: int, kernel_size: int, dilation: int, dropout: float):
            super().__init__()
            padding = (kernel_size - 1) // 2 * dilation
            self.dw = nn.Sequential(
                nn.Conv1d(channels, channels, kernel_size, padding=padding, dilation=dilation, groups=channels, bias=False),
                nn.BatchNorm1d(channels),
                nn.ReLU(inplace=True),
            )
            self.pw = nn.Sequential(
                nn.Conv1d(channels, channels, kernel_size=1, bias=False),
                nn.BatchNorm1d(channels),
                nn.ReLU(inplace=True),
            )
            self.do = nn.Dropout(p=dropout)
        def forward(self, x):
            x = self.dw[x.__class__] if False else self.dw(x)
            x = self.pw(x)
            x = self.do(x)
            return x
    
    class MBTCNBlock(nn.Module):
        def __init__(self, channels: int, dropout: float):
            super().__init__()
            ks = [7, 15, 31]
            dil = [1, 2, 4]
            self.branches = nn.ModuleList([
                DepthwiseSeparableConv1d(channels, ks[0], dil[0], dropout),
                DepthwiseSeparableConv1d(channels, ks[1], dil[1], dropout),
                DepthwiseSeparableConv1d(channels, ks[2], dil[2], dropout),
            ])
            self.merge = nn.Sequential(
                nn.Conv1d(channels * 3, channels, kernel_size=1, bias=False),
                nn.BatchNorm1d(channels),
            )
            self.act = nn.ReLU(inplace=True)
        def forward(self, x):
            outs = [b(x) for b in self.branches]
            y = torch.cat(outs, dim=1)
            y = self.merge(y)
            y = y + x  # residual
            return self.act(y)
    
    class TransformerEncoderBlock(nn.Module):
        def __init__(self, d_model: int, nhead: int, dropout: float):
            super().__init__()
            self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
            self.ln1 = nn.LayerNorm(d_model)
            self.ffn = nn.Sequential(
                nn.Linear(d_model, d_model * 2),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(d_model * 2, d_model),
            )
            self.ln2 = nn.LayerNorm(d_model)
            self.drop = nn.Dropout(dropout)
        def forward(self, x):  # x: (B, L, D)
            attn_out, _ = self.mha(x, x, x, need_weights=False)
            x = self.ln1(x + self.drop(attn_out))
            x = self.ln2(x + self.drop(self.ffn(x)))
            return x
    
    class MB_MHA_TCN_Tiny(nn.Module):
        def __init__(self, in_ch: int, base_ch: int, d_model: int, nhead: int, num_layers: int, dropout: float, num_classes: int, use_act_quant: bool = False):
            super().__init__()
            self.use_act_quant = use_act_quant
            if use_act_quant:
                from torch.ao.quantization import QuantStub, DeQuantStub
                self.quant = QuantStub()
                self.dequant = DeQuantStub()
            else:
                self.quant = None
                self.dequant = None
            # Stem
            self.stem = nn.Sequential(
                nn.Conv1d(in_ch, base_ch, kernel_size=7, padding=3, bias=False),
                nn.BatchNorm1d(base_ch),
                nn.ReLU(inplace=True),
            )
            # MB-TCN blocks
            self.tcn_blocks = nn.Sequential(*[MBTCNBlock(base_ch, dropout) for _ in range(int(hparams['num_tcn_blocks']))])
            # Projection to d_model
            self.proj = nn.Sequential(
                nn.Conv1d(base_ch, d_model, kernel_size=1, bias=False),
                nn.BatchNorm1d(d_model),
                nn.ReLU(inplace=True),
            )
            # Transformer encoder(s)
            self.posenc = PositionalEncoding(d_model, max_len=seq_len)
            self.encoder = nn.Sequential(*[TransformerEncoderBlock(d_model, nhead, dropout) for _ in range(int(hparams['num_encoder_layers']))])
            # Head
            self.pool = nn.AdaptiveAvgPool1d(1)
            self.fc = nn.Linear(d_model, num_classes)
        def forward(self, x):  # x: (B, C, L)
            if self.quant is not None:
                x = self.quant(x)
            x = self.stem(x)
            x = self.tcn_blocks(x)
            x = self.proj(x)            # (B, D, L)
            x = x.transpose(1, 2)       # (B, L, D)
            x = self.posenc(x)
            x = self.encoder(x)
            x = x.transpose(1, 2)       # (B, D, L)
            x = self.pool(x).squeeze(-1)  # (B, D)
            x = self.fc(x)              # (B, num_classes)
            if self.dequant is not None:
                x = self.dequant(x)
            return x
        def fuse_for_quant(self):
            # Try to fuse typical conv-bn-relu patterns
            for name, module in self.named_modules():
                pass  # kept simple; prepare/convert will still quantize supported ops
    
    # ---------------------------
    # Loss functions
    # ---------------------------
    def compute_class_weights(y: torch.Tensor, num_classes: int) -> torch.Tensor:
        with torch.no_grad():
            counts = torch.bincount(y.view(-1).cpu(), minlength=num_classes).float()
            counts[counts == 0] = 1.0
            inv = 1.0 / counts
            w = inv / inv.sum() * num_classes
            return w
    
    class FocalLoss(nn.Module):
        def __init__(self, gamma=2.0, weight=None, label_smoothing=0.0):
            super().__init__()
            self.gamma = gamma
            self.weight = weight
            self.label_smoothing = label_smoothing
        def forward(self, logits, targets):
            ce = F.cross_entropy(logits, targets, weight=self.weight, reduction='none', label_smoothing=self.label_smoothing)
            pt = torch.softmax(logits, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)
            pt = pt.clamp(1e-6, 1 - 1e-6)
            loss = ((1 - pt) ** self.gamma) * ce
            return loss.mean()
    
    # ---------------------------
    # Build model, optimizer, criterion
    # ---------------------------
    model = MB_MHA_TCN_Tiny(
        in_ch=in_channels,
        base_ch=int(hparams['tcn_channels']),
        d_model=int(hparams['d_model']),
        nhead=int(hparams['num_heads']),
        num_layers=int(hparams['num_encoder_layers']),
        dropout=float(hparams['dropout']),
        num_classes=num_classes,
        use_act_quant=bool(hparams['quantize_activations']) and int(hparams['quantization_bits']) == 8,
    ).to(device)
    
    # Class weights
    if hparams['class_weighting'] == 'auto':
        cls_w = compute_class_weights(y_train_bal, num_classes).to(device)
    else:
        cls_w = None
    
    # Criterion
    if bool(hparams['use_focal_loss']):
        criterion = FocalLoss(gamma=float(hparams['focal_gamma']), weight=cls_w, label_smoothing=float(hparams['label_smoothing']))
    else:
        criterion = nn.CrossEntropyLoss(weight=cls_w, label_smoothing=float(hparams['label_smoothing']))
    
    # Optimizer
    if hparams['optimizer'].lower() == 'adamw':
        optimizer = optim.AdamW(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))
    else:
        optimizer = optim.Adam(model.parameters(), lr=float(hparams['lr']), weight_decay=float(hparams['weight_decay']))
    
    # ---------------------------
    # Training Loop
    # ---------------------------
    train_losses: List[float] = []
    val_losses: List[float] = []
    val_accs: List[float] = []
    
    for epoch in range(int(hparams['epochs'])):
        model.train()
        running_loss = 0.0
        n_train = 0
        for xb, yb in train_loader:
            xb = xb.to(device, non_blocking=False).float()
            yb = yb.to(device, non_blocking=False).long()
            optimizer.zero_grad(set_to_none=True)
            logits = model(xb)
            loss = criterion(logits, yb)
            loss.backward()
            if float(hparams['grad_clip_norm']) > 0:
                nn.utils.clip_grad_norm_(model.parameters(), float(hparams['grad_clip_norm']))
            optimizer.step()
            bs = xb.size(0)
            running_loss += loss.detach().item() * bs
            n_train += bs
        train_loss_epoch = running_loss / max(1, n_train)
        train_losses.append(train_loss_epoch)
        
        # Validation
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device, non_blocking=False).float()
                yb = yb.to(device, non_blocking=False).long()
                logits = model(xb)
                loss = criterion(logits, yb)
                val_loss += loss.item() * xb.size(0)
                preds = torch.argmax(logits, dim=1)
                correct += (preds == yb).sum().item()
                total += xb.size(0)
        val_loss_epoch = val_loss / max(1, total)
        val_acc_epoch = correct / max(1, total)
        val_losses.append(val_loss_epoch)
        val_accs.append(val_acc_epoch)
        print(f"Epoch {epoch+1}/{hparams['epochs']} - train_loss: {train_loss_epoch:.6f} - val_loss: {val_loss_epoch:.6f} - val_acc: {val_acc_epoch:.4f}")
    
    # ---------------------------
    # Post-Training Quantization (on CPU)
    # ---------------------------
    model_cpu = copy.deepcopy(model).to('cpu')
    model_cpu.eval()
    quant_bits = int(hparams['quantization_bits'])
    q_weights = bool(hparams['quantize_weights'])
    q_acts = bool(hparams['quantize_activations'])
    
    quantized_model = model_cpu
    
    if quant_bits == 32 or not q_weights:
        # No quantization (float32)
        pass
    elif quant_bits == 16:
        # FP16 cast (weights + activations during inference if run in fp16-capable device)
        quantized_model = model_cpu.half()
    elif quant_bits == 8:
        if q_acts:
            # Static quantization with calibration
            from torch.ao.quantization import get_default_qconfig
            from torch.ao.quantization import prepare, convert
            torch.backends.quantized.engine = 'fbgemm'
            quantized_model.fuse_for_quant()
            quantized_model.qconfig = get_default_qconfig('fbgemm')
            prepare(quantized_model, inplace=True)
            # Calibration on a few batches from training data (CPU)
            calib_batches = max(1, int(hparams['calib_batches']))
            quantized_model.eval()
            with torch.inference_mode():
                seen = 0
                for xb, _ in DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=0, pin_memory=False):
                    xb = xb.float()
                    _ = quantized_model(xb)
                    seen += 1
                    if seen >= calib_batches:
                        break
            convert(quantized_model, inplace=True)
        else:
            # Dynamic quantization for Linear layers (covers MHA and FFN); convs remain FP32
            from torch.ao.quantization import quantize_dynamic
            quantized_model = quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)
    else:
        pass
    
    # ---------------------------
    # Compute serialized size (KB)
    # ---------------------------
    with io.BytesIO() as f:
        try:
            torch.save(quantized_model.state_dict(), f)
        except Exception:
            # fallback to full model if state_dict fails
            torch.save(quantized_model, f)
        size_bytes = f.tell()
    size_kb = size_bytes / 1024.0
    
    # Enforce size constraint by design; print warning if exceeded
    if size_kb > 256.0:
        print(f"Warning: Quantized model size {size_kb:.2f} KB exceeds 256KB. Consider reducing d_model/tcn_channels.")
    
    metrics = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_acc': val_accs,
        'final_model_size_kb': size_kb,
        'used_hparams': hparams
    }
    return quantized_model, metrics


RESPONSE OPTIONS:
1. HYPERPARAMETER FIX: If error can be fixed by changing hyperparameters
   Output: {"bo_config": {"param_name": new_value, "param2": new_value}}

2. CODE FIX: If error requires fixing bugs in the training code
   Output: {"training_code": "complete_corrected_training_function_code"}

3. CANNOT FIX: If error cannot be resolved
   Output: {}

RESPONSE FORMAT REQUIREMENTS:
1. Output ONLY a JSON object with either "bo_config" OR "training_code" field
2. No explanations, no markdown, no ```json``` blocks
3. Start with { and end with }
4. For training_code fixes, include the COMPLETE corrected function

CORRECTION EXAMPLES:
- "Model has X KB storage, exceeds 256KB limit" → {"bo_config": {"d_model": 64, "hidden_size": 128}}
- "'str' object has no attribute 'type'" → {"training_code": "def train_model(...):\n    # fixed implementation"}
- "Quantization bug in code" → {"training_code": "corrected_training_function"}
- "mat1 and mat2 shapes cannot be multiplied" → {"bo_config": {"d_model": 128}}

OUTPUT ONLY THE JSON OBJECT: