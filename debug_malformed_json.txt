Original JSON:
{
    "model_name": "ECGConvTransformerTiny",
    "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    \"\"\"\n    Train a lightweight 1D CNN + Transformer encoder for 5-class ECG classification.\n    - Input tensors: X_* shape [N, 1000, 2] or [N, 2, 1000]; y_* shape [N] with class ids 0..4\n    - MIT-BIH note: use an inter-patient split (e.g., AAMI DS1→DS2) outside this function to avoid leakage.\n    \"\"\"\n    import math\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import Dataset, DataLoader\n\n    # -------------------- defaults & hyperparams --------------------\n    cfg = {\n        'n_classes': 5,\n        'epochs': 20,\n        'batch_size': 128,\n        'lr': 3e-4,\n        'weight_decay': 1e-4,\n        'hidden_size': 64,      # Transformer d_model (kept small for <256K params)\n        'n_heads': 4,\n        'n_layers': 2,\n        'dropout': 0.1,\n        'label_smoothing': 0.05,\n        'grad_clip': 1.0,\n        'num_workers': 0\n    }\n    cfg.update(hyperparams or {})\n\n    # -------------------- Dataset & DataLoaders --------------------\n    class ECGDataset(Dataset):\n        def __init__(self, X, y):\n            self.X = X\n            self.y = y\n            assert self.X.shape[0] == self.y.shape[0], 'Mismatched X/y sizes'\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            y = self.y[idx]\n            # Expect [L, C] or [C, L]; convert to [C, L] for Conv1d\n            if x.dim() == 2:\n                if x.shape[-1] <= 4:  # likely [L, C]\n                    x = x.transpose(0, 1)\n            x = x.float()\n            return x, y.long()\n\n    # pin_memory only if tensors live on CPU\n    pin_mem = (X_train.device.type == 'cpu' and y_train.device.type == 'cpu')\n\n    train_ds = ECGDataset(X_train, y_train)\n    val_ds = ECGDataset(X_val, y_val)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg['batch_size'],\n        shuffle=True,\n        num_workers=cfg['num_workers'],\n        pin_memory=pin_mem,\n        drop_last=False\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg['batch_size'],\n        shuffle=False,\n        num_workers=cfg['num_workers'],\n        pin_memory=pin_mem,\n        drop_last=False\n    )\n\n    # -------------------- Model --------------------\n    class SinusoidalPositionalEncoding(nn.Module):\n        def __init__(self, d_model, max_len=2000):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            self.register_buffer('pe', pe)  # [max_len, d_model]\n        def forward(self, x):\n            # x: [S, B, E]\n            S = x.size(0)\n            return x + self.pe[:S].unsqueeze(1)\n\n    class ConvTransformerECG(nn.Module):\n        def __init__(self, n_classes=5, d_model=64, n_heads=4, n_layers=2, dropout=0.1):\n            super().__init__()\n            # Conv stem: downsample 1000 -> 250 (stride 2 twice)\n            self.stem = nn.Sequential(\n                nn.Conv1d(2, 64, kernel_size=7, stride=2, padding=3),\n                nn.BatchNorm1d(64),\n                nn.ReLU(inplace=True),\n                nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n                nn.BatchNorm1d(128),\n                nn.ReLU(inplace=True),\n                nn.Conv1d(128, d_model, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm1d(d_model),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout)\n            )\n            self.pos = SinusoidalPositionalEncoding(d_model)\n            enc_layer = nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=n_heads,\n                dim_feedforward=4 * d_model,\n                dropout=dropout,\n                batch_first=False,  # expect [S, B, E]\n                activation='gelu',\n                norm_first=True\n            )\n            self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n            self.head = nn.Sequential(\n                nn.LayerNorm(d_model),\n                nn.Dropout(dropout),\n                nn.Linear(d_model, n_classes)\n            )\n        def forward(self, x):\n            # x: [B, C, L]\n            z = self.stem(x)              # [B, d_model, L']\n            z = z.transpose(1, 2)         # [B, L', d_model]\n            z = z.transpose(0, 1)         # [L', B, d_model]\n            z = self.pos(z)               # add PE\n            z = self.encoder(z)           # [L', B, d_model]\n            z = z.mean(dim=0)             # global average over time -> [B, d_model]\n            logits = self.head(z)         # [B, n_classes]\n            return logits\n\n    model = ConvTransformerECG(\n        n_classes=cfg['n_classes'],\n        d_model=cfg['hidden_size'],\n        n_heads=cfg['n_heads'],\n        n_layers=cfg['n_layers'],\n        dropout=cfg['dropout']\n    ).to(device)\n\n    # -------------------- Loss, Optimizer --------------------\n    # Class weights (balanced): N / (C * count_c)\n    with torch.no_grad():\n        counts = torch.bincount(y_train.detach().cpu(), minlength=cfg['n_classes']).float()\n        balanced_w = (len(y_train) / (cfg['n_classes'] * counts.clamp_min(1.0))).to(device)\n    criterion = nn.CrossEntropyLoss(weight=balanced_w, label_smoothing=cfg['label_smoothing'])\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n\n    # -------------------- Metrics helpers --------------------\n    def macro_f1_from_preds(preds_cpu, t_cpu, num_classes):\n        f1_sum = 0.0\n        for c in range(num_classes):\n            tp = ((preds_cpu == c) & (t_cpu == c)).sum().item()\n            fp = ((preds_cpu == c) & (t_cpu != c)).sum().item()\n            fn = ((preds_cpu != c) & (t_cpu == c)).sum().item()\n            prec = tp / (tp + fp + 1e-9)\n            rec = tp / (tp + fn + 1e-9)\n            f1 = 0.0 if (prec + rec) == 0 else (2 * prec * rec) / (prec + rec + 1e-9)\n            f1_sum += f1\n        return f1_sum / num_classes\n\n    def evaluate():\n        model.eval()\n        total, correct = 0, 0\n        val_loss_sum = 0.0\n        all_preds = []\n        all_targets = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=True)\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss_sum += loss.item() * yb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += yb.size(0)\n                all_preds.append(preds.detach().cpu())\n                all_targets.append(yb.detach().cpu())\n        val_loss = val_loss_sum / max(1, total)\n        all_preds = torch.cat(all_preds, dim=0)\n        all_targets = torch.cat(all_targets, dim=0)\n        acc = correct / max(1, total)\n        f1 = macro_f1_from_preds(all_preds, all_targets, cfg['n_classes'])\n        return val_loss, acc, f1\n\n    # -------------------- Training loop --------------------\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'val_acc': [],\n        'val_f1_macro': [],\n        'class_weights': balanced_w.detach().cpu().tolist()\n    }\n\n    for epoch in range(cfg['epochs']):\n        model.train()\n        running = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if cfg['grad_clip'] and cfg['grad_clip'] > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['grad_clip'])\n            optimizer.step()\n            bs = yb.size(0)\n            running += loss.item() * bs\n            seen += bs\n        train_loss = running / max(1, seen)\n        val_loss, val_acc, val_f1 = evaluate()\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['val_f1_macro'].append(val_f1)\n\n    return model, history\n",
    "hyperparameters": {
        "lr": 0.0003,
        "epochs": 20,
        "batch_size": 128
    },
    "reasoning": "Implements a compact local-global encoder: a 1D CNN stem downsamples the two-lead ECG to capture morphology (QRS/P/T) and reduce sequence length (1000→250), followed by a small Transformer encoder (d_model=64, 2 layers, 4 heads) to model rhythm/context across beats. This hybrid matches literature showing CNN+attention effectiveness on MIT-BIH while staying lightweight (<256K params). Class-imbalance is addressed with balanced loss weights and label smoothing; evaluation reports accuracy and macro-F1. Pin memory is enabled only when tensors reside on CPU as required.",
    "confidence": 0.9,
    "bo_parameters": [
        "lr",
        "batch_size",
        "epochs",
        "hidden_size",
        "dropout"
    ],
    "bo_search_space": {
        "lr": {
            "type": "Real",
            "low": 1e-05,
            "high": 0.1,
            "prior": "log-uniform"
        },
        "batch_size": {
            "type": "Categorical",
            "categories": [
                8,
                16,
                32,
                64,
                128
            ]
        },
        "epochs": {
            "type": "Integer",
            "low": 5,
            "high": 50
        },
        "hidden_size": {
            "type": "Integer",
            "low": 32,
            "high": 512"
        },
        "dropout": {
            "type": "Real",
            "low": 0.0,
            "high": 0.7
        }
    }
}

Fixed JSON:
{
    "model_name": "ECGConvTransformerTiny",
    "training_code": "def train_model(X_train, y_train, X_val, y_val, device, **hyperparams):\n    \"\"\"\n    Train a lightweight 1D CNN + Transformer encoder for 5-class ECG classification.\n    - Input tensors: X_* shape [N, 1000, 2] or [N, 2, 1000]; y_* shape [N] with class ids 0..4\n    - MIT-BIH note: use an inter-patient split (e.g., AAMI DS1→DS2) outside this function to avoid leakage.\n    \"\"\"\n    import math\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import Dataset, DataLoader\n\n    # -------------------- defaults & hyperparams --------------------\n    cfg = {\n        'n_classes': 5,\n        'epochs': 20,\n        'batch_size': 128,\n        'lr': 3e-4,\n        'weight_decay': 1e-4,\n        'hidden_size': 64,      # Transformer d_model (kept small for <256K params)\n        'n_heads': 4,\n        'n_layers': 2,\n        'dropout': 0.1,\n        'label_smoothing': 0.05,\n        'grad_clip': 1.0,\n        'num_workers': 0\n    }\n    cfg.update(hyperparams or {})\n\n    # -------------------- Dataset & DataLoaders --------------------\n    class ECGDataset(Dataset):\n        def __init__(self, X, y):\n            self.X = X\n            self.y = y\n            assert self.X.shape[0] == self.y.shape[0], 'Mismatched X/y sizes'\n        def __len__(self):\n            return self.X.shape[0]\n        def __getitem__(self, idx):\n            x = self.X[idx]\n            y = self.y[idx]\n            # Expect [L, C] or [C, L]; convert to [C, L] for Conv1d\n            if x.dim() == 2:\n                if x.shape[-1] <= 4:  # likely [L, C]\n                    x = x.transpose(0, 1)\n            x = x.float()\n            return x, y.long()\n\n    # pin_memory only if tensors live on CPU\n    pin_mem = (X_train.device.type == 'cpu' and y_train.device.type == 'cpu')\n\n    train_ds = ECGDataset(X_train, y_train)\n    val_ds = ECGDataset(X_val, y_val)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg['batch_size'],\n        shuffle=True,\n        num_workers=cfg['num_workers'],\n        pin_memory=pin_mem,\n        drop_last=False\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg['batch_size'],\n        shuffle=False,\n        num_workers=cfg['num_workers'],\n        pin_memory=pin_mem,\n        drop_last=False\n    )\n\n    # -------------------- Model --------------------\n    class SinusoidalPositionalEncoding(nn.Module):\n        def __init__(self, d_model, max_len=2000):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n            self.register_buffer('pe', pe)  # [max_len, d_model]\n        def forward(self, x):\n            # x: [S, B, E]\n            S = x.size(0)\n            return x + self.pe[:S].unsqueeze(1)\n\n    class ConvTransformerECG(nn.Module):\n        def __init__(self, n_classes=5, d_model=64, n_heads=4, n_layers=2, dropout=0.1):\n            super().__init__()\n            # Conv stem: downsample 1000 -> 250 (stride 2 twice)\n            self.stem = nn.Sequential(\n                nn.Conv1d(2, 64, kernel_size=7, stride=2, padding=3),\n                nn.BatchNorm1d(64),\n                nn.ReLU(inplace=True),\n                nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n                nn.BatchNorm1d(128),\n                nn.ReLU(inplace=True),\n                nn.Conv1d(128, d_model, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm1d(d_model),\n                nn.ReLU(inplace=True),\n                nn.Dropout(dropout)\n            )\n            self.pos = SinusoidalPositionalEncoding(d_model)\n            enc_layer = nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=n_heads,\n                dim_feedforward=4 * d_model,\n                dropout=dropout,\n                batch_first=False,  # expect [S, B, E]\n                activation='gelu',\n                norm_first=True\n            )\n            self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n            self.head = nn.Sequential(\n                nn.LayerNorm(d_model),\n                nn.Dropout(dropout),\n                nn.Linear(d_model, n_classes)\n            )\n        def forward(self, x):\n            # x: [B, C, L]\n            z = self.stem(x)              # [B, d_model, L']\n            z = z.transpose(1, 2)         # [B, L', d_model]\n            z = z.transpose(0, 1)         # [L', B, d_model]\n            z = self.pos(z)               # add PE\n            z = self.encoder(z)           # [L', B, d_model]\n            z = z.mean(dim=0)             # global average over time -> [B, d_model]\n            logits = self.head(z)         # [B, n_classes]\n            return logits\n\n    model = ConvTransformerECG(\n        n_classes=cfg['n_classes'],\n        d_model=cfg['hidden_size'],\n        n_heads=cfg['n_heads'],\n        n_layers=cfg['n_layers'],\n        dropout=cfg['dropout']\n    ).to(device)\n\n    # -------------------- Loss, Optimizer --------------------\n    # Class weights (balanced): N / (C * count_c)\n    with torch.no_grad():\n        counts = torch.bincount(y_train.detach().cpu(), minlength=cfg['n_classes']).float()\n        balanced_w = (len(y_train) / (cfg['n_classes'] * counts.clamp_min(1.0))).to(device)\n    criterion = nn.CrossEntropyLoss(weight=balanced_w, label_smoothing=cfg['label_smoothing'])\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n\n    # -------------------- Metrics helpers --------------------\n    def macro_f1_from_preds(preds_cpu, t_cpu, num_classes):\n        f1_sum = 0.0\n        for c in range(num_classes):\n            tp = ((preds_cpu == c) & (t_cpu == c)).sum().item()\n            fp = ((preds_cpu == c) & (t_cpu != c)).sum().item()\n            fn = ((preds_cpu != c) & (t_cpu == c)).sum().item()\n            prec = tp / (tp + fp + 1e-9)\n            rec = tp / (tp + fn + 1e-9)\n            f1 = 0.0 if (prec + rec) == 0 else (2 * prec * rec) / (prec + rec + 1e-9)\n            f1_sum += f1\n        return f1_sum / num_classes\n\n    def evaluate():\n        model.eval()\n        total, correct = 0, 0\n        val_loss_sum = 0.0\n        all_preds = []\n        all_targets = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb = xb.to(device, non_blocking=True)\n                yb = yb.to(device, non_blocking=True)\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                val_loss_sum += loss.item() * yb.size(0)\n                preds = logits.argmax(dim=1)\n                correct += (preds == yb).sum().item()\n                total += yb.size(0)\n                all_preds.append(preds.detach().cpu())\n                all_targets.append(yb.detach().cpu())\n        val_loss = val_loss_sum / max(1, total)\n        all_preds = torch.cat(all_preds, dim=0)\n        all_targets = torch.cat(all_targets, dim=0)\n        acc = correct / max(1, total)\n        f1 = macro_f1_from_preds(all_preds, all_targets, cfg['n_classes'])\n        return val_loss, acc, f1\n\n    # -------------------- Training loop --------------------\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'val_acc': [],\n        'val_f1_macro': [],\n        'class_weights': balanced_w.detach().cpu().tolist()\n    }\n\n    for epoch in range(cfg['epochs']):\n        model.train()\n        running = 0.0\n        seen = 0\n        for xb, yb in train_loader:\n            xb = xb.to(device, non_blocking=True)\n            yb = yb.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            loss.backward()\n            if cfg['grad_clip'] and cfg['grad_clip'] > 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['grad_clip'])\n            optimizer.step()\n            bs = yb.size(0)\n            running += loss.item() * bs\n            seen += bs\n        train_loss = running / max(1, seen)\n        val_loss, val_acc, val_f1 = evaluate()\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['val_f1_macro'].append(val_f1)\n\n    return model, history\n",
    "hyperparameters": {
        "lr": 0.0003,
        "epochs": 20,
        "batch_size": 128
    },
    "reasoning": "Implements a compact local-global encoder: a 1D CNN stem downsamples the two-lead ECG to capture morphology (QRS/P/T) and reduce sequence length (1000→250), followed by a small Transformer encoder (d_model=64, 2 layers, 4 heads) to model rhythm/context across beats. This hybrid matches literature showing CNN+attention effectiveness on MIT-BIH while staying lightweight (<256K params). Class-imbalance is addressed with balanced loss weights and label smoothing; evaluation reports accuracy and macro-F1. Pin memory is enabled only when tensors reside on CPU as required.",
    "confidence": 0.9,
    "bo_parameters": [
        "lr",
        "batch_size",
        "epochs",
        "hidden_size",
        "dropout"
],
    "bo_search_space": {
        "lr": {
            "type": "Real",
            "low": 1e-05,
            "high": 0.1,
            "prior": "log-uniform"
},
        "batch_size": {
            "type": "Categorical",
            "categories": [
                8,
                16,
                32,
                64,
                128
            ]
        },
        "epochs": {
            "type": "Integer",
            "low": 5,
            "high": 50
        },
        "hidden_size": {
            "type": "Integer",
            "low": 32,
            "high": 512"
},
        "dropout": {
            "type": "Real",
            "low": 0.0,
            "high": 0.7
        }
    }
}